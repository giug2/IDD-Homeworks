<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Show and Guide: Instructional-Plan Grounded Vision and Language Model</title>
<!--Generated on Wed Oct  2 20:46:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.19074v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S1" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S2" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Multimodal Plan-Grounded LM</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS1" title="In 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS2" title="In 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span> MM-PlanLLM Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS2.SSS0.Px1" title="In 3.2 MM-PlanLLM Learning ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Plan-Grounded Answer Generation (PGAG).</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS2.SSS0.Px2" title="In 3.2 MM-PlanLLM Learning ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Conversational Video Moment Retrieval (CVMR).</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS2.SSS0.Px3" title="In 3.2 MM-PlanLLM Learning ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Visually-Informed Step Generation (VSG).</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS3" title="In 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Model Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS3.SSS0.Px1" title="In 3.3 Model Architecture ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">a) V&amp;L Model Backbone.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS3.SSS0.Px2" title="In 3.3 Model Architecture ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">b) Video Encoder.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS3.SSS0.Px3" title="In 3.3 Model Architecture ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">c) Task-specific layers.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS4" title="In 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multi-stage Multimodal Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS4.SSS0.Px1" title="In 3.4 Multi-stage Multimodal Training ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Stage 1. Visual Projection Layers.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS4.SSS0.Px2" title="In 3.4 Multi-stage Multimodal Training ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Stage 2. Task Data Specialization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS4.SSS0.Px3" title="In 3.4 Multi-stage Multimodal Training ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Stage 3. Multimodal Plan-Grounded Dialogue.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS5" title="In 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Synthetic Multimodal Plan-oriented Training Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS5.SSS0.Px1" title="In 3.5 Synthetic Multimodal Plan-oriented Training Data ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">CVMR Requests.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS5.SSS0.Px2" title="In 3.5 Synthetic Multimodal Plan-oriented Training Data ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">VSG Requests.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS1" title="In 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Instructional Tasks Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS1.SSS0.Px1" title="In 4.1 Instructional Tasks Datasets ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">TastyVidDial.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS1.SSS0.Px2" title="In 4.1 Instructional Tasks Datasets ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Simulated Alexa TaskBot.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS2" title="In 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS2.SSS0.Px1" title="In 4.2 Methodology ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Backbone Models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS2.SSS0.Px2" title="In 4.2 Methodology ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Metrics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS2.SSS0.Px3" title="In 4.2 Methodology ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Protocol.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS2.SSS0.Px4" title="In 4.2 Methodology ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Baselines.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS1" title="In 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Plan Grounding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS1.SSS0.Px1" title="In 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Plan-Grounded Answer Generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS1.SSS0.Px2" title="In 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Conversational Video Moment Retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS1.SSS0.Px3" title="In 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Visually-Informed Step Generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS2" title="In 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Multimodal Plan Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS2.SSS0.Px1" title="In 5.2 Multimodal Plan Alignment ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Text to Visual Plan.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS2.SSS0.Px2" title="In 5.2 Multimodal Plan Alignment ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Image to Text Plan.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS3" title="In 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS3.SSS0.Px1" title="In 5.3 Ablation Study ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">Training Stages.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS3.SSS0.Px2" title="In 5.3 Ablation Study ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title">LLM Backbone.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S6" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A1" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A2" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>LM Backbone Ablation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A3" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Frame Similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A4" title="In Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>CVMR and VSG Examples</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Show and Guide: 
<br class="ltx_break"/>Instructional-Plan Grounded Vision and Language Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Diogo Glória-Silva, David Semedo, João Magalhães 
<br class="ltx_break"/>NOVA LINCS, NOVA School of Science and Technology, Portugal 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">dmgc.silva@campus.fct.unl.pt
<br class="ltx_break"/>{df.semedo, jmag}@fct.unl.pt
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Guiding users through complex procedural plans is an inherently multimodal task in which having visually illustrated plan steps is crucial to deliver an effective plan guidance.
However, existing works on plan-following language models (LMs) often are not capable of multimodal input and output. In this work, we present MM-PlanLLM, the first multimodal LLM designed to assist users in executing instructional tasks by leveraging both textual plans and visual information. Specifically, we bring cross-modality through two key tasks: Conversational Video Moment Retrieval, where the model retrieves relevant step-video segments based on user queries, and Visually-Informed Step Generation, where the model generates the next step in a plan, conditioned on an image of the user’s current progress. MM-PlanLLM is trained using a novel multitask-multistage approach, designed to gradually expose the model to multimodal instructional-plans semantic layers, achieving strong performance on both multimodal and textual dialogue in a plan-grounded setting. Furthermore, we show that the model delivers cross-modal temporal and plan-structure representations aligned between textual plan steps and instructional video moments. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The model, code, and non-personal data will be made publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dmgcsilva/mmplanllm" title="">https://github.com/dmgcsilva/mmplanllm</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="842" id="S1.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of a plan-grounded multimodal dialogue. The proposed model has the ability to understand and respond to multimodal input, provide relevant information from multiple knowledge sources, and guide the user through a complex task while adhering to a structured plan.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The research of Large Language Models (LLMs) in assisting users with complex procedural plans, such as cooking or DIY projects, presents an exciting new frontier in NLP research <cite class="ltx_cite ltx_citemacro_cite">Choi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib8" title="">2022</a>)</cite>.
However, while LLMs can excel at text-based conversational interactions <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib7" title="">2023</a>)</cite>, procedural plans are inherently multimodal, often accompanied by illustrative images or instructional videos <cite class="ltx_cite ltx_citemacro_cite">Sener and Yao (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib39" title="">2019</a>); Marin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib33" title="">2019</a>)</cite>.
Thus, to interact in a reliable and trustworthy manner, it is crucial that these models not only comprehend procedural plans, but also ground the dialogue on these plans and align them with the visual domain, understanding images and videos to accurately assess progress and provide helpful guidance (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we tackle this challenge and propose a multimodal LLM that is deeply grounded in both procedural text-plan and the accompanying visual-plan. Specifically, we focus on jointly learning three fundamental tasks: <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">Plan-following</span> capabilities where the LLM can generate and skip steps of the plan (label 1 of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>), <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">Conversational Video Moment Retrieval</span> to retrieve a relevant step-video moment that accurately describes the current plan step (label 2 of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>), and <span class="ltx_text ltx_font_bold" id="S1.p2.1.3">Visually-Informed Step Generation</span>, where, the goal is to, based on visual user input describing their current progress, generates the appropriate follow-up plan step (label 3 of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>).
To address these challenges, we propose <span class="ltx_text ltx_font_bold" id="S1.p2.1.4">M</span>ulti<span class="ltx_text ltx_font_bold" id="S1.p2.1.5">M</span>odal <span class="ltx_text ltx_font_bold" id="S1.p2.1.6">Plan</span> <span class="ltx_text ltx_font_bold" id="S1.p2.1.7">LLM</span> (MM-PlanLLM), a dedicated model architecture capable of guiding users through a complex task plan, while supporting textual and visual plan information, both as input and output. In particular, we extend an LLM backbone with task-specific projection layers. These allow capturing video semantic and temporal information and supporting flexible decode-time multimodal retrieval, conditioned on task plans.
For training, we devise a novel multitask, multistage training approach designed to progressively instill the desired multimodal capabilities while preserving or improving on previously learned ones.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">MM-PlanLLM, the main contribution of this paper, is a model capable of guiding users through complex tasks, while adhering to the user requests, grounding the plan progress on user-uploaded images through visually-informed step retrieval, and performing conversational step-video moment retrieval.
In particular, its groundbreaking multimodal plan-guiding capabilities, lets it align image inputs with the correct step of the instructional plan, perform step-video moment retrieval, producing step-aligned cross-modal representations, with limited performance drop on text-only requests.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A thorough evaluation shows MM-PlanLLM’s competitive performance on text-only tasks against task-specific baselines, and substantial improvements over existing approaches on multimodal tasks.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In recent years, with the release of large open source foundational models such as OPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib60" title="">2022</a>)</cite>, Llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib48" title="">2023a</a>)</cite> and others <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib37" title="">2019</a>); Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib3" title="">2020a</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib21" title="">2023a</a>)</cite>, the field of Large Language Models (LLMs) for conversational settings has received significant attention. Due to this, the contributions have been diverse, with work focusing on improving training data <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib7" title="">2023</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib49" title="">2023b</a>)</cite>, scaling model size <cite class="ltx_cite ltx_citemacro_cite">Chowdhery et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib9" title="">2022</a>)</cite>, and adopting a Mixture of Experts (MoE) architecture <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib22" title="">2024</a>); Shen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib41" title="">2023</a>)</cite>.
The applications of these models are varied, such as instruction following <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib4" title="">2020b</a>); Taori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib45" title="">2023</a>); Mishra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib34" title="">2022</a>)</cite>, conversational dialogue <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib62" title="">2020c</a>); Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib7" title="">2023</a>)</cite>, and other task-specific applications <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib38" title="">2020</a>); Hosseini-Asl et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib16" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Researching models capable of understanding multimodal input has also been a topic of great interest.
A common approach has been the usage of pretrained LLMs and Visual Encoders to achieve efficient and effective Large Vision-Language Models (LVLMs) with limited resources; however, the way these models interface has been varied. Some approaches, such as the LLaVa models <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib30" title="">2023</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib44" title="">2023</a>)</cite> and FROMAGe <cite class="ltx_cite ltx_citemacro_cite">Koh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib24" title="">2023</a>)</cite> have found that linear projections are enough. Others deploy larger "interpretation" modules such as the Q-Fromer in BLIP <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib27" title="">2023</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib10" title="">2023</a>)</cite>, the Visual Abstractor in mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib56" title="">2023a</a>)</cite>, or the Perceiver <cite class="ltx_cite ltx_citemacro_cite">Jaegle et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib18" title="">2021</a>)</cite> employed in Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib1" title="">2022</a>)</cite>. Another interesting approach is the modification of the internal Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib51" title="">2017</a>)</cite> attention blocks such as the visual expert in CogVLM <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib53" title="">2023a</a>)</cite> and the Modality-Adaptive Module in mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib57" title="">2023b</a>)</cite>.
Some work has also been done on training multimodal models from scratch such as PaLi <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib6" title="">2023</a>)</cite>, Gemini <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib46" title="">2023</a>)</cite>, and Large World Model <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib29" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Video Moment Retrieval (VMR) is the task of, given a video and textual prompt that describes an action or event that occurs in a video, retrieving a video clip from within said video that best matches the provided textual prompt. Proposal-driven approaches focus on identifying candidate proposals and then ranking them to find the most relevant one <cite class="ltx_cite ltx_citemacro_cite">Gao and Xu (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib14" title="">2021</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib52" title="">2022</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib31" title="">2018</a>)</cite>. In contrast, others opt for a proposal-free approach that predicts the target moment directly from the video-prompt feature mappings <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib54" title="">2023b</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib59" title="">2020a</a>); Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib58" title="">2018</a>)</cite> often relying on cross-modal attention modules or on learnable query embeddings such as EaTR <cite class="ltx_cite ltx_citemacro_cite">Jang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib19" title="">2023</a>)</cite> or MH-DETR <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib55" title="">2023</a>)</cite>.
A common problem in VMR is the need to do extensive and expensive temporal annotations,
an alternative is point-level VMR where the annotation is a single frame point <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib23" title="">2023b</a>)</cite> or a small segment <cite class="ltx_cite ltx_citemacro_cite">Ji et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib20" title="">2023</a>)</cite>.
Recently, several approaches have been adopting a Detection Transformers <cite class="ltx_cite ltx_citemacro_cite">Carion et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib5" title="">2020</a>)</cite>, as it does away with the need for many hand-designed components, and tackling the problem as a direct set prediction <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib25" title="">2021a</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib43" title="">2024</a>); Moon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib35" title="">2023</a>); Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib26" title="">2021b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multimodal Plan-Grounded LM</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present the main elements of this work:
we start by formalizing the problem, then we describe MM-PlanLLM, its architecture, and the multi-stage training process used. We end by detailing how the supporting synthetic training dataset is generated.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Definition</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.20">Let <math alttext="D=\left\langle P,T,V\right\rangle" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.3"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.4" xref="S3.SS1.p1.1.m1.3.4.cmml"><mi id="S3.SS1.p1.1.m1.3.4.2" xref="S3.SS1.p1.1.m1.3.4.2.cmml">D</mi><mo id="S3.SS1.p1.1.m1.3.4.1" xref="S3.SS1.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.3.4.3.2" xref="S3.SS1.p1.1.m1.3.4.3.1.cmml"><mo id="S3.SS1.p1.1.m1.3.4.3.2.1" xref="S3.SS1.p1.1.m1.3.4.3.1.cmml">⟨</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">P</mi><mo id="S3.SS1.p1.1.m1.3.4.3.2.2" xref="S3.SS1.p1.1.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">T</mi><mo id="S3.SS1.p1.1.m1.3.4.3.2.3" xref="S3.SS1.p1.1.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml">V</mi><mo id="S3.SS1.p1.1.m1.3.4.3.2.4" xref="S3.SS1.p1.1.m1.3.4.3.1.cmml">⟩</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><apply id="S3.SS1.p1.1.m1.3.4.cmml" xref="S3.SS1.p1.1.m1.3.4"><eq id="S3.SS1.p1.1.m1.3.4.1.cmml" xref="S3.SS1.p1.1.m1.3.4.1"></eq><ci id="S3.SS1.p1.1.m1.3.4.2.cmml" xref="S3.SS1.p1.1.m1.3.4.2">𝐷</ci><list id="S3.SS1.p1.1.m1.3.4.3.1.cmml" xref="S3.SS1.p1.1.m1.3.4.3.2"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑃</ci><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">𝑇</ci><ci id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">𝑉</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">D=\left\langle P,T,V\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.3d">italic_D = ⟨ italic_P , italic_T , italic_V ⟩</annotation></semantics></math> be a dialogue that consists of a procedural plan <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_P</annotation></semantics></math> composed of <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_k</annotation></semantics></math> sequential steps <math alttext="P=\{s_{1},\cdots,s_{k}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.3"><semantics id="S3.SS1.p1.4.m4.3a"><mrow id="S3.SS1.p1.4.m4.3.3" xref="S3.SS1.p1.4.m4.3.3.cmml"><mi id="S3.SS1.p1.4.m4.3.3.4" xref="S3.SS1.p1.4.m4.3.3.4.cmml">P</mi><mo id="S3.SS1.p1.4.m4.3.3.3" xref="S3.SS1.p1.4.m4.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.4.m4.3.3.2.2" xref="S3.SS1.p1.4.m4.3.3.2.3.cmml"><mo id="S3.SS1.p1.4.m4.3.3.2.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.3.3.2.3.cmml">{</mo><msub id="S3.SS1.p1.4.m4.2.2.1.1.1" xref="S3.SS1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS1.p1.4.m4.2.2.1.1.1.2.cmml">s</mi><mn id="S3.SS1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS1.p1.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.4.m4.3.3.2.2.4" xref="S3.SS1.p1.4.m4.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p1.4.m4.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.4.m4.3.3.2.2.5" xref="S3.SS1.p1.4.m4.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.4.m4.3.3.2.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.3.3.2.2.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.2.2.cmml">s</mi><mi id="S3.SS1.p1.4.m4.3.3.2.2.2.3" xref="S3.SS1.p1.4.m4.3.3.2.2.2.3.cmml">k</mi></msub><mo id="S3.SS1.p1.4.m4.3.3.2.2.6" stretchy="false" xref="S3.SS1.p1.4.m4.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.3b"><apply id="S3.SS1.p1.4.m4.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3"><eq id="S3.SS1.p1.4.m4.3.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3.3"></eq><ci id="S3.SS1.p1.4.m4.3.3.4.cmml" xref="S3.SS1.p1.4.m4.3.3.4">𝑃</ci><set id="S3.SS1.p1.4.m4.3.3.2.3.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2"><apply id="S3.SS1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1.2">𝑠</ci><cn id="S3.SS1.p1.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">⋯</ci><apply id="S3.SS1.p1.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2.2">𝑠</ci><ci id="S3.SS1.p1.4.m4.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2.3">𝑘</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.3c">P=\{s_{1},\cdots,s_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.3d">italic_P = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math>, and a set of <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_n</annotation></semantics></math> user-system interaction turns <math alttext="T=\left\{t_{1},\cdots,t_{n}\right\}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.3"><semantics id="S3.SS1.p1.6.m6.3a"><mrow id="S3.SS1.p1.6.m6.3.3" xref="S3.SS1.p1.6.m6.3.3.cmml"><mi id="S3.SS1.p1.6.m6.3.3.4" xref="S3.SS1.p1.6.m6.3.3.4.cmml">T</mi><mo id="S3.SS1.p1.6.m6.3.3.3" xref="S3.SS1.p1.6.m6.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.6.m6.3.3.2.2" xref="S3.SS1.p1.6.m6.3.3.2.3.cmml"><mo id="S3.SS1.p1.6.m6.3.3.2.2.3" xref="S3.SS1.p1.6.m6.3.3.2.3.cmml">{</mo><msub id="S3.SS1.p1.6.m6.2.2.1.1.1" xref="S3.SS1.p1.6.m6.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.2.2.1.1.1.2" xref="S3.SS1.p1.6.m6.2.2.1.1.1.2.cmml">t</mi><mn id="S3.SS1.p1.6.m6.2.2.1.1.1.3" xref="S3.SS1.p1.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.6.m6.3.3.2.2.4" xref="S3.SS1.p1.6.m6.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.6.m6.1.1" mathvariant="normal" xref="S3.SS1.p1.6.m6.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.6.m6.3.3.2.2.5" xref="S3.SS1.p1.6.m6.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.6.m6.3.3.2.2.2" xref="S3.SS1.p1.6.m6.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.6.m6.3.3.2.2.2.2" xref="S3.SS1.p1.6.m6.3.3.2.2.2.2.cmml">t</mi><mi id="S3.SS1.p1.6.m6.3.3.2.2.2.3" xref="S3.SS1.p1.6.m6.3.3.2.2.2.3.cmml">n</mi></msub><mo id="S3.SS1.p1.6.m6.3.3.2.2.6" xref="S3.SS1.p1.6.m6.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.3b"><apply id="S3.SS1.p1.6.m6.3.3.cmml" xref="S3.SS1.p1.6.m6.3.3"><eq id="S3.SS1.p1.6.m6.3.3.3.cmml" xref="S3.SS1.p1.6.m6.3.3.3"></eq><ci id="S3.SS1.p1.6.m6.3.3.4.cmml" xref="S3.SS1.p1.6.m6.3.3.4">𝑇</ci><set id="S3.SS1.p1.6.m6.3.3.2.3.cmml" xref="S3.SS1.p1.6.m6.3.3.2.2"><apply id="S3.SS1.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.2.2.1.1.1.2">𝑡</ci><cn id="S3.SS1.p1.6.m6.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.6.m6.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">⋯</ci><apply id="S3.SS1.p1.6.m6.3.3.2.2.2.cmml" xref="S3.SS1.p1.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.6.m6.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.3.3.2.2.2.2">𝑡</ci><ci id="S3.SS1.p1.6.m6.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.3.3.2.2.2.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.3c">T=\left\{t_{1},\cdots,t_{n}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.3d">italic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>, where a turn <math alttext="t_{i}=\left\langle U_{i},R_{i},I_{i}^{*}\right\rangle" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.3"><semantics id="S3.SS1.p1.7.m7.3a"><mrow id="S3.SS1.p1.7.m7.3.3" xref="S3.SS1.p1.7.m7.3.3.cmml"><msub id="S3.SS1.p1.7.m7.3.3.5" xref="S3.SS1.p1.7.m7.3.3.5.cmml"><mi id="S3.SS1.p1.7.m7.3.3.5.2" xref="S3.SS1.p1.7.m7.3.3.5.2.cmml">t</mi><mi id="S3.SS1.p1.7.m7.3.3.5.3" xref="S3.SS1.p1.7.m7.3.3.5.3.cmml">i</mi></msub><mo id="S3.SS1.p1.7.m7.3.3.4" xref="S3.SS1.p1.7.m7.3.3.4.cmml">=</mo><mrow id="S3.SS1.p1.7.m7.3.3.3.3" xref="S3.SS1.p1.7.m7.3.3.3.4.cmml"><mo id="S3.SS1.p1.7.m7.3.3.3.3.4" xref="S3.SS1.p1.7.m7.3.3.3.4.cmml">⟨</mo><msub id="S3.SS1.p1.7.m7.1.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.2.cmml">U</mi><mi id="S3.SS1.p1.7.m7.1.1.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.7.m7.3.3.3.3.5" xref="S3.SS1.p1.7.m7.3.3.3.4.cmml">,</mo><msub id="S3.SS1.p1.7.m7.2.2.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.7.m7.2.2.2.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.2.2.2.cmml">R</mi><mi id="S3.SS1.p1.7.m7.2.2.2.2.2.3" xref="S3.SS1.p1.7.m7.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.7.m7.3.3.3.3.6" xref="S3.SS1.p1.7.m7.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.7.m7.3.3.3.3.3" xref="S3.SS1.p1.7.m7.3.3.3.3.3.cmml"><mi id="S3.SS1.p1.7.m7.3.3.3.3.3.2.2" xref="S3.SS1.p1.7.m7.3.3.3.3.3.2.2.cmml">I</mi><mi id="S3.SS1.p1.7.m7.3.3.3.3.3.2.3" xref="S3.SS1.p1.7.m7.3.3.3.3.3.2.3.cmml">i</mi><mo id="S3.SS1.p1.7.m7.3.3.3.3.3.3" xref="S3.SS1.p1.7.m7.3.3.3.3.3.3.cmml">∗</mo></msubsup><mo id="S3.SS1.p1.7.m7.3.3.3.3.7" xref="S3.SS1.p1.7.m7.3.3.3.4.cmml">⟩</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.3b"><apply id="S3.SS1.p1.7.m7.3.3.cmml" xref="S3.SS1.p1.7.m7.3.3"><eq id="S3.SS1.p1.7.m7.3.3.4.cmml" xref="S3.SS1.p1.7.m7.3.3.4"></eq><apply id="S3.SS1.p1.7.m7.3.3.5.cmml" xref="S3.SS1.p1.7.m7.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.3.3.5.1.cmml" xref="S3.SS1.p1.7.m7.3.3.5">subscript</csymbol><ci id="S3.SS1.p1.7.m7.3.3.5.2.cmml" xref="S3.SS1.p1.7.m7.3.3.5.2">𝑡</ci><ci id="S3.SS1.p1.7.m7.3.3.5.3.cmml" xref="S3.SS1.p1.7.m7.3.3.5.3">𝑖</ci></apply><list id="S3.SS1.p1.7.m7.3.3.3.4.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3"><apply id="S3.SS1.p1.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.2">𝑈</ci><ci id="S3.SS1.p1.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p1.7.m7.2.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.2.2">𝑅</ci><ci id="S3.SS1.p1.7.m7.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS1.p1.7.m7.3.3.3.3.3.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3">superscript</csymbol><apply id="S3.SS1.p1.7.m7.3.3.3.3.3.2.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.3.3.3.3.3.2.1.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.7.m7.3.3.3.3.3.2.2.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3.2.2">𝐼</ci><ci id="S3.SS1.p1.7.m7.3.3.3.3.3.2.3.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3.2.3">𝑖</ci></apply><times id="S3.SS1.p1.7.m7.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.7.m7.3.3.3.3.3.3"></times></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.3c">t_{i}=\left\langle U_{i},R_{i},I_{i}^{*}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.3d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ⟨ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ⟩</annotation></semantics></math> is composed of a user request <math alttext="U_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">U</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝑈</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">U_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, a system response <math alttext="R_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">R</mi><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝑅</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">R_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and, optionally, a user-uploaded image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">𝐼</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><mi id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><ci id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">italic_V</annotation></semantics></math> a video that demonstrates how to follow the plan <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">italic_P</annotation></semantics></math>. <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13.1"><semantics id="S3.SS1.p1.13.m13.1a"><mi id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><ci id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.13.m13.1d">italic_V</annotation></semantics></math> is composed of <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m14.1"><semantics id="S3.SS1.p1.14.m14.1a"><mi id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><ci id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.14.m14.1d">italic_i</annotation></semantics></math> frames <math alttext="V=\{f_{1},\cdots,f_{l}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.15.m15.3"><semantics id="S3.SS1.p1.15.m15.3a"><mrow id="S3.SS1.p1.15.m15.3.3" xref="S3.SS1.p1.15.m15.3.3.cmml"><mi id="S3.SS1.p1.15.m15.3.3.4" xref="S3.SS1.p1.15.m15.3.3.4.cmml">V</mi><mo id="S3.SS1.p1.15.m15.3.3.3" xref="S3.SS1.p1.15.m15.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.15.m15.3.3.2.2" xref="S3.SS1.p1.15.m15.3.3.2.3.cmml"><mo id="S3.SS1.p1.15.m15.3.3.2.2.3" stretchy="false" xref="S3.SS1.p1.15.m15.3.3.2.3.cmml">{</mo><msub id="S3.SS1.p1.15.m15.2.2.1.1.1" xref="S3.SS1.p1.15.m15.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.15.m15.2.2.1.1.1.2" xref="S3.SS1.p1.15.m15.2.2.1.1.1.2.cmml">f</mi><mn id="S3.SS1.p1.15.m15.2.2.1.1.1.3" xref="S3.SS1.p1.15.m15.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.15.m15.3.3.2.2.4" xref="S3.SS1.p1.15.m15.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.15.m15.1.1" mathvariant="normal" xref="S3.SS1.p1.15.m15.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.15.m15.3.3.2.2.5" xref="S3.SS1.p1.15.m15.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.15.m15.3.3.2.2.2" xref="S3.SS1.p1.15.m15.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.15.m15.3.3.2.2.2.2" xref="S3.SS1.p1.15.m15.3.3.2.2.2.2.cmml">f</mi><mi id="S3.SS1.p1.15.m15.3.3.2.2.2.3" xref="S3.SS1.p1.15.m15.3.3.2.2.2.3.cmml">l</mi></msub><mo id="S3.SS1.p1.15.m15.3.3.2.2.6" stretchy="false" xref="S3.SS1.p1.15.m15.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.3b"><apply id="S3.SS1.p1.15.m15.3.3.cmml" xref="S3.SS1.p1.15.m15.3.3"><eq id="S3.SS1.p1.15.m15.3.3.3.cmml" xref="S3.SS1.p1.15.m15.3.3.3"></eq><ci id="S3.SS1.p1.15.m15.3.3.4.cmml" xref="S3.SS1.p1.15.m15.3.3.4">𝑉</ci><set id="S3.SS1.p1.15.m15.3.3.2.3.cmml" xref="S3.SS1.p1.15.m15.3.3.2.2"><apply id="S3.SS1.p1.15.m15.2.2.1.1.1.cmml" xref="S3.SS1.p1.15.m15.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.15.m15.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m15.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.15.m15.2.2.1.1.1.2">𝑓</ci><cn id="S3.SS1.p1.15.m15.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.15.m15.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">⋯</ci><apply id="S3.SS1.p1.15.m15.3.3.2.2.2.cmml" xref="S3.SS1.p1.15.m15.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.15.m15.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.15.m15.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.15.m15.3.3.2.2.2.2">𝑓</ci><ci id="S3.SS1.p1.15.m15.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.15.m15.3.3.2.2.2.3">𝑙</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.3c">V=\{f_{1},\cdots,f_{l}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.15.m15.3d">italic_V = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }</annotation></semantics></math>.
Here, a plan step is a sequence of words <math alttext="s=\{w_{1},w_{2},\cdots\}" class="ltx_Math" display="inline" id="S3.SS1.p1.16.m16.3"><semantics id="S3.SS1.p1.16.m16.3a"><mrow id="S3.SS1.p1.16.m16.3.3" xref="S3.SS1.p1.16.m16.3.3.cmml"><mi id="S3.SS1.p1.16.m16.3.3.4" xref="S3.SS1.p1.16.m16.3.3.4.cmml">s</mi><mo id="S3.SS1.p1.16.m16.3.3.3" xref="S3.SS1.p1.16.m16.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.16.m16.3.3.2.2" xref="S3.SS1.p1.16.m16.3.3.2.3.cmml"><mo id="S3.SS1.p1.16.m16.3.3.2.2.3" stretchy="false" xref="S3.SS1.p1.16.m16.3.3.2.3.cmml">{</mo><msub id="S3.SS1.p1.16.m16.2.2.1.1.1" xref="S3.SS1.p1.16.m16.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.16.m16.2.2.1.1.1.2" xref="S3.SS1.p1.16.m16.2.2.1.1.1.2.cmml">w</mi><mn id="S3.SS1.p1.16.m16.2.2.1.1.1.3" xref="S3.SS1.p1.16.m16.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.16.m16.3.3.2.2.4" xref="S3.SS1.p1.16.m16.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.16.m16.3.3.2.2.2" xref="S3.SS1.p1.16.m16.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.16.m16.3.3.2.2.2.2" xref="S3.SS1.p1.16.m16.3.3.2.2.2.2.cmml">w</mi><mn id="S3.SS1.p1.16.m16.3.3.2.2.2.3" xref="S3.SS1.p1.16.m16.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.16.m16.3.3.2.2.5" xref="S3.SS1.p1.16.m16.3.3.2.3.cmml">,</mo><mi id="S3.SS1.p1.16.m16.1.1" mathvariant="normal" xref="S3.SS1.p1.16.m16.1.1.cmml">⋯</mi><mo id="S3.SS1.p1.16.m16.3.3.2.2.6" stretchy="false" xref="S3.SS1.p1.16.m16.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.3b"><apply id="S3.SS1.p1.16.m16.3.3.cmml" xref="S3.SS1.p1.16.m16.3.3"><eq id="S3.SS1.p1.16.m16.3.3.3.cmml" xref="S3.SS1.p1.16.m16.3.3.3"></eq><ci id="S3.SS1.p1.16.m16.3.3.4.cmml" xref="S3.SS1.p1.16.m16.3.3.4">𝑠</ci><set id="S3.SS1.p1.16.m16.3.3.2.3.cmml" xref="S3.SS1.p1.16.m16.3.3.2.2"><apply id="S3.SS1.p1.16.m16.2.2.1.1.1.cmml" xref="S3.SS1.p1.16.m16.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.16.m16.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.16.m16.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.16.m16.2.2.1.1.1.2">𝑤</ci><cn id="S3.SS1.p1.16.m16.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.16.m16.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p1.16.m16.3.3.2.2.2.cmml" xref="S3.SS1.p1.16.m16.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.16.m16.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.16.m16.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.16.m16.3.3.2.2.2.2">𝑤</ci><cn id="S3.SS1.p1.16.m16.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.16.m16.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1">⋯</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.3c">s=\{w_{1},w_{2},\cdots\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.16.m16.3d">italic_s = { italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ }</annotation></semantics></math>, and a video moment <math alttext="m_{V}" class="ltx_Math" display="inline" id="S3.SS1.p1.17.m17.1"><semantics id="S3.SS1.p1.17.m17.1a"><msub id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml"><mi id="S3.SS1.p1.17.m17.1.1.2" xref="S3.SS1.p1.17.m17.1.1.2.cmml">m</mi><mi id="S3.SS1.p1.17.m17.1.1.3" xref="S3.SS1.p1.17.m17.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><apply id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.17.m17.1.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p1.17.m17.1.1.2.cmml" xref="S3.SS1.p1.17.m17.1.1.2">𝑚</ci><ci id="S3.SS1.p1.17.m17.1.1.3.cmml" xref="S3.SS1.p1.17.m17.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">m_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.17.m17.1d">italic_m start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> is the sequence of video frames denoted by its starting and ending frame <math alttext="m_{V}=\{f_{s},f_{e}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.18.m18.2"><semantics id="S3.SS1.p1.18.m18.2a"><mrow id="S3.SS1.p1.18.m18.2.2" xref="S3.SS1.p1.18.m18.2.2.cmml"><msub id="S3.SS1.p1.18.m18.2.2.4" xref="S3.SS1.p1.18.m18.2.2.4.cmml"><mi id="S3.SS1.p1.18.m18.2.2.4.2" xref="S3.SS1.p1.18.m18.2.2.4.2.cmml">m</mi><mi id="S3.SS1.p1.18.m18.2.2.4.3" xref="S3.SS1.p1.18.m18.2.2.4.3.cmml">V</mi></msub><mo id="S3.SS1.p1.18.m18.2.2.3" xref="S3.SS1.p1.18.m18.2.2.3.cmml">=</mo><mrow id="S3.SS1.p1.18.m18.2.2.2.2" xref="S3.SS1.p1.18.m18.2.2.2.3.cmml"><mo id="S3.SS1.p1.18.m18.2.2.2.2.3" stretchy="false" xref="S3.SS1.p1.18.m18.2.2.2.3.cmml">{</mo><msub id="S3.SS1.p1.18.m18.1.1.1.1.1" xref="S3.SS1.p1.18.m18.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.18.m18.1.1.1.1.1.2" xref="S3.SS1.p1.18.m18.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.18.m18.1.1.1.1.1.3" xref="S3.SS1.p1.18.m18.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.SS1.p1.18.m18.2.2.2.2.4" xref="S3.SS1.p1.18.m18.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.18.m18.2.2.2.2.2" xref="S3.SS1.p1.18.m18.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.18.m18.2.2.2.2.2.2" xref="S3.SS1.p1.18.m18.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS1.p1.18.m18.2.2.2.2.2.3" xref="S3.SS1.p1.18.m18.2.2.2.2.2.3.cmml">e</mi></msub><mo id="S3.SS1.p1.18.m18.2.2.2.2.5" stretchy="false" xref="S3.SS1.p1.18.m18.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m18.2b"><apply id="S3.SS1.p1.18.m18.2.2.cmml" xref="S3.SS1.p1.18.m18.2.2"><eq id="S3.SS1.p1.18.m18.2.2.3.cmml" xref="S3.SS1.p1.18.m18.2.2.3"></eq><apply id="S3.SS1.p1.18.m18.2.2.4.cmml" xref="S3.SS1.p1.18.m18.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m18.2.2.4.1.cmml" xref="S3.SS1.p1.18.m18.2.2.4">subscript</csymbol><ci id="S3.SS1.p1.18.m18.2.2.4.2.cmml" xref="S3.SS1.p1.18.m18.2.2.4.2">𝑚</ci><ci id="S3.SS1.p1.18.m18.2.2.4.3.cmml" xref="S3.SS1.p1.18.m18.2.2.4.3">𝑉</ci></apply><set id="S3.SS1.p1.18.m18.2.2.2.3.cmml" xref="S3.SS1.p1.18.m18.2.2.2.2"><apply id="S3.SS1.p1.18.m18.1.1.1.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m18.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.18.m18.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.18.m18.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.18.m18.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.18.m18.1.1.1.1.1.3">𝑠</ci></apply><apply id="S3.SS1.p1.18.m18.2.2.2.2.2.cmml" xref="S3.SS1.p1.18.m18.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m18.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.18.m18.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.18.m18.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.18.m18.2.2.2.2.2.2">𝑓</ci><ci id="S3.SS1.p1.18.m18.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.18.m18.2.2.2.2.2.3">𝑒</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m18.2c">m_{V}=\{f_{s},f_{e}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.18.m18.2d">italic_m start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT = { italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT }</annotation></semantics></math> with <math alttext="f_{s},f_{e}\in V" class="ltx_Math" display="inline" id="S3.SS1.p1.19.m19.2"><semantics id="S3.SS1.p1.19.m19.2a"><mrow id="S3.SS1.p1.19.m19.2.2" xref="S3.SS1.p1.19.m19.2.2.cmml"><mrow id="S3.SS1.p1.19.m19.2.2.2.2" xref="S3.SS1.p1.19.m19.2.2.2.3.cmml"><msub id="S3.SS1.p1.19.m19.1.1.1.1.1" xref="S3.SS1.p1.19.m19.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.19.m19.1.1.1.1.1.2" xref="S3.SS1.p1.19.m19.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.19.m19.1.1.1.1.1.3" xref="S3.SS1.p1.19.m19.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.SS1.p1.19.m19.2.2.2.2.3" xref="S3.SS1.p1.19.m19.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.19.m19.2.2.2.2.2" xref="S3.SS1.p1.19.m19.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.19.m19.2.2.2.2.2.2" xref="S3.SS1.p1.19.m19.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS1.p1.19.m19.2.2.2.2.2.3" xref="S3.SS1.p1.19.m19.2.2.2.2.2.3.cmml">e</mi></msub></mrow><mo id="S3.SS1.p1.19.m19.2.2.3" xref="S3.SS1.p1.19.m19.2.2.3.cmml">∈</mo><mi id="S3.SS1.p1.19.m19.2.2.4" xref="S3.SS1.p1.19.m19.2.2.4.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m19.2b"><apply id="S3.SS1.p1.19.m19.2.2.cmml" xref="S3.SS1.p1.19.m19.2.2"><in id="S3.SS1.p1.19.m19.2.2.3.cmml" xref="S3.SS1.p1.19.m19.2.2.3"></in><list id="S3.SS1.p1.19.m19.2.2.2.3.cmml" xref="S3.SS1.p1.19.m19.2.2.2.2"><apply id="S3.SS1.p1.19.m19.1.1.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.19.m19.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.19.m19.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.19.m19.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.19.m19.1.1.1.1.1.3">𝑠</ci></apply><apply id="S3.SS1.p1.19.m19.2.2.2.2.2.cmml" xref="S3.SS1.p1.19.m19.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.19.m19.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.19.m19.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.19.m19.2.2.2.2.2.2">𝑓</ci><ci id="S3.SS1.p1.19.m19.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.19.m19.2.2.2.2.2.3">𝑒</ci></apply></list><ci id="S3.SS1.p1.19.m19.2.2.4.cmml" xref="S3.SS1.p1.19.m19.2.2.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m19.2c">f_{s},f_{e}\in V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.19.m19.2d">italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ∈ italic_V</annotation></semantics></math> and <math alttext="s,e\leq l" class="ltx_Math" display="inline" id="S3.SS1.p1.20.m20.2"><semantics id="S3.SS1.p1.20.m20.2a"><mrow id="S3.SS1.p1.20.m20.2.3" xref="S3.SS1.p1.20.m20.2.3.cmml"><mrow id="S3.SS1.p1.20.m20.2.3.2.2" xref="S3.SS1.p1.20.m20.2.3.2.1.cmml"><mi id="S3.SS1.p1.20.m20.1.1" xref="S3.SS1.p1.20.m20.1.1.cmml">s</mi><mo id="S3.SS1.p1.20.m20.2.3.2.2.1" xref="S3.SS1.p1.20.m20.2.3.2.1.cmml">,</mo><mi id="S3.SS1.p1.20.m20.2.2" xref="S3.SS1.p1.20.m20.2.2.cmml">e</mi></mrow><mo id="S3.SS1.p1.20.m20.2.3.1" xref="S3.SS1.p1.20.m20.2.3.1.cmml">≤</mo><mi id="S3.SS1.p1.20.m20.2.3.3" xref="S3.SS1.p1.20.m20.2.3.3.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.20.m20.2b"><apply id="S3.SS1.p1.20.m20.2.3.cmml" xref="S3.SS1.p1.20.m20.2.3"><leq id="S3.SS1.p1.20.m20.2.3.1.cmml" xref="S3.SS1.p1.20.m20.2.3.1"></leq><list id="S3.SS1.p1.20.m20.2.3.2.1.cmml" xref="S3.SS1.p1.20.m20.2.3.2.2"><ci id="S3.SS1.p1.20.m20.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1">𝑠</ci><ci id="S3.SS1.p1.20.m20.2.2.cmml" xref="S3.SS1.p1.20.m20.2.2">𝑒</ci></list><ci id="S3.SS1.p1.20.m20.2.3.3.cmml" xref="S3.SS1.p1.20.m20.2.3.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.20.m20.2c">s,e\leq l</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.20.m20.2d">italic_s , italic_e ≤ italic_l</annotation></semantics></math>. Each video moment represents a plan step or part of it.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Based on the user-request type, our approach simultaneously adapts and performs interleaved multimodal plan-grounded tasks. In particular, three key features are supported: general plan-grounded answer generation, conversational video moment retrieval, and visually-informed step generation.
These key features are delivered by extending a vision and language model, in a multi-task setting, through a multi-stage training scheme.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span> MM-PlanLLM Learning</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Plan-Grounded Answer Generation (PGAG).</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.6">In this task, given a dialogue <math alttext="D_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">D_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and the latest user request <math alttext="U_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">U</mi><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝑈</ci><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><plus id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">U_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_U start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> the objective is to generate <math alttext="R_{i+1}=\{w^{r}_{1},\ldots,w_{n}^{r}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.3"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.3a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml"><msub id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.2.cmml">R</mi><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml">{</mo><msubsup id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.2.cmml">w</mi><mn id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.3.cmml">1</mn><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.3.cmml">r</mi></msubsup><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.4" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.5" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.2.cmml">w</mi><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.3.cmml">n</mi><mi id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.3.cmml">r</mi></msubsup><mo id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.6" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.3b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3"><eq id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.3"></eq><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.2">𝑅</ci><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3"><plus id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.1"></plus><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.4.3.3">1</cn></apply></apply><set id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.2">𝑤</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.2.3">𝑟</ci></apply><cn id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">…</ci><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2">superscript</csymbol><apply id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.2">𝑤</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.2.3">𝑛</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.3">𝑟</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.3c">R_{i+1}=\{w^{r}_{1},\ldots,w_{n}^{r}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.3d">italic_R start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = { italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }</annotation></semantics></math> that adequately answers the user request, while conditioning on the previous turns <math alttext="T_{i-c:i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">T</mi><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1.cmml">−</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.3.cmml">c</mi></mrow><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.1.cmml">:</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3"><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.1">:</ci><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2"><minus id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.1"></minus><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.2">𝑖</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.2.3">𝑐</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">T_{i-c:i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_i - italic_c : italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.1a"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.1b"><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m5.1d">italic_c</annotation></semantics></math> being the context size and <math alttext="1\leq i&lt;i+1" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px1.p1.6.m6.1a"><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml">≤</mo><mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.4" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.4.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.5" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.5.cmml">&lt;</mo><mrow id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.cmml"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.3" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1"><and id="S3.SS2.SSS0.Px1.p1.6.m6.1.1a.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1"></and><apply id="S3.SS2.SSS0.Px1.p1.6.m6.1.1b.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1"><leq id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3"></leq><cn id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2">1</cn><ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.4.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.4">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.6.m6.1.1c.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1"><lt id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.5.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.5"></lt><share href="https://arxiv.org/html/2409.19074v2#S3.SS2.SSS0.Px1.p1.6.m6.1.1.4.cmml" id="S3.SS2.SSS0.Px1.p1.6.m6.1.1d.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1"></share><apply id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6"><plus id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.1"></plus><ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.2">𝑖</ci><cn id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.6.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m6.1c">1\leq i&lt;i+1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.6.m6.1d">1 ≤ italic_i &lt; italic_i + 1</annotation></semantics></math>. The objective is formulated as a plan-grounded cross-entropy loss,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{pgag}=-\sum_{t=1}^{T}\log P(w_{t}^{r}|w^{r}_{1:t-1},%
\bm{U_{i+1}},D_{j})" class="ltx_Math" display="inline" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.3.3.2.cmml">p</mi><mo id="S3.Ex1.m1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.3.3.3.cmml">g</mi><mo id="S3.Ex1.m1.1.1.3.3.1a" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.4" xref="S3.Ex1.m1.1.1.3.3.4.cmml">a</mi><mo id="S3.Ex1.m1.1.1.3.3.1b" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.5" xref="S3.Ex1.m1.1.1.3.3.5.cmml">g</mi></mrow></msub><mo id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1a" xref="S3.Ex1.m1.1.1.1.cmml">−</mo><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><munderover id="S3.Ex1.m1.1.1.1.1.2a" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex1.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.3" xref="S3.Ex1.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.3.2" xref="S3.Ex1.m1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.Ex1.m1.1.1.1.1.2.2.3.1" xref="S3.Ex1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex1.m1.1.1.1.1.2.2.3.3" xref="S3.Ex1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">T</mi></munderover></mstyle><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S3.Ex1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.Ex1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.2.cmml">w</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.3.cmml">t</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.3.cmml">r</mi></msubsup><mo fence="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.4.cmml"><msubsup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mrow><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">r</mi></msubsup><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝑼</mi><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">𝒊</mi><mo class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.1" mathvariant="bold" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">+</mo><mn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">𝟏</mn></mrow></msub><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.5" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">D</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">j</mi></msub></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"></eq><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">ℒ</ci><apply id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3"><times id="S3.Ex1.m1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.3.3.1"></times><ci id="S3.Ex1.m1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.3.3.2">𝑝</ci><ci id="S3.Ex1.m1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3.3">𝑔</ci><ci id="S3.Ex1.m1.1.1.3.3.4.cmml" xref="S3.Ex1.m1.1.1.3.3.4">𝑎</ci><ci id="S3.Ex1.m1.1.1.3.3.5.cmml" xref="S3.Ex1.m1.1.1.3.3.5">𝑔</ci></apply></apply><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><minus id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1"></minus><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1"><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex1.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2"></sum><apply id="S3.Ex1.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.3"><eq id="S3.Ex1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.3.2">𝑡</ci><cn id="S3.Ex1.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3"><log id="S3.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.1"></log><ci id="S3.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.2">𝑤</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.2.3">𝑡</ci></apply><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.5.3">𝑟</ci></apply><list id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3"><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑟</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">1</cn><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><cn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" type="integer" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝑼</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3"><plus id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.1"></plus><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.2">𝒊</ci><cn id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.2">𝐷</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.3.3">𝑗</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle\mathcal{L}_{pgag}=-\sum_{t=1}^{T}\log P(w_{t}^{r}|w^{r}_{1:t-1},%
\bm{U_{i+1}},D_{j})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_p italic_g italic_a italic_g end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_log italic_P ( italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT | italic_w start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_t - 1 end_POSTSUBSCRIPT , bold_italic_U start_POSTSUBSCRIPT bold_italic_i bold_+ bold_1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Conversational Video Moment Retrieval (CVMR).</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.15">This task seeks to retrieve a video moment that illustrates the current step of the task plan. Namely, given a textual user video request <math alttext="U_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">U</mi><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">𝑈</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3"><plus id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">U_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">italic_U start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math>, it seeks to retrieve the relevant video moment from a video <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.1d">italic_V</annotation></semantics></math>, given a dialogue <math alttext="D_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">D_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, considering only the previous turns <math alttext="T_{i-c:i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml">T</mi><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.1.cmml">−</mo><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.3.cmml">c</mi></mrow><mo id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.1.cmml">:</mo><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.1">:</ci><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2"><minus id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.1"></minus><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.2">𝑖</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.3">𝑐</ci></apply><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">T_{i-c:i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_i - italic_c : italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.5.m5.1d">italic_c</annotation></semantics></math> being the context size. To formulate the retrieval problem, MM-PlanLLM generates a system response <math alttext="R_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">R</mi><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2">𝑅</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3"><plus id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">R_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.6.m6.1d">italic_R start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> and locates the corresponding video moment <math alttext="m_{V}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><msub id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml">m</mi><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2">𝑚</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">m_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.7.m7.1d">italic_m start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> within <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.8.m8.1"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.1a"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.1b"><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.8.m8.1d">italic_V</annotation></semantics></math>. For tractability, we focus on retrieving a single keyframe <math alttext="f_{m}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.9.m9.1"><semantics id="S3.SS2.SSS0.Px2.p1.9.m9.1a"><msub id="S3.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml">f</mi><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m9.1b"><apply id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.2">𝑓</ci><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m9.1c">f_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.9.m9.1d">italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> that represents moment <math alttext="m_{v}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.10.m10.1"><semantics id="S3.SS2.SSS0.Px2.p1.10.m10.1a"><msub id="S3.SS2.SSS0.Px2.p1.10.m10.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml">m</mi><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2">𝑚</ci><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m10.1c">m_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.10.m10.1d">italic_m start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>. We define <math alttext="f_{m}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.11.m11.1"><semantics id="S3.SS2.SSS0.Px2.p1.11.m11.1a"><msub id="S3.SS2.SSS0.Px2.p1.11.m11.1.1" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2.cmml">f</mi><mi id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.11.m11.1b"><apply id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2">𝑓</ci><ci id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.11.m11.1c">f_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.11.m11.1d">italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> as the relevant segment’s middle frame, with <math alttext="m=\lfloor\frac{e-s}{2}\rceil" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS0.Px2.p1.12.m12.1"><semantics id="S3.SS2.SSS0.Px2.p1.12.m12.1a"><mrow id="S3.SS2.SSS0.Px2.p1.12.m12.1b"><mi id="S3.SS2.SSS0.Px2.p1.12.m12.1.1">m</mi><mo id="S3.SS2.SSS0.Px2.p1.12.m12.1.2">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.12.m12.1.3"><mo id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.1" stretchy="false">⌊</mo><mfrac id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2"><mrow id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2.2"><mi id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2.2.2">e</mi><mo id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2.2.1">−</mo><mi id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2.2.3">s</mi></mrow><mn id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.2.3">2</mn></mfrac><mo id="S3.SS2.SSS0.Px2.p1.12.m12.1.3.3" stretchy="false">⌉</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.12.m12.1c">m=\lfloor\frac{e-s}{2}\rceil</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.12.m12.1d">italic_m = ⌊ divide start_ARG italic_e - italic_s end_ARG start_ARG 2 end_ARG ⌉</annotation></semantics></math>.
Recognizing the high similarity between consecutive frames (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A3" title="Appendix C Frame Similarity ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">C</span></a>), we formulate a video moment retrieval task by relaxing the retrieval target to consider a bidirectional context window of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.13.m13.1"><semantics id="S3.SS2.SSS0.Px2.p1.13.m13.1a"><mi id="S3.SS2.SSS0.Px2.p1.13.m13.1.1" xref="S3.SS2.SSS0.Px2.p1.13.m13.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.13.m13.1b"><ci id="S3.SS2.SSS0.Px2.p1.13.m13.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m13.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.13.m13.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.13.m13.1d">italic_N</annotation></semantics></math> adjacent frames. This translates to retrieving any frame in a window of <math alttext="2N+1" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.14.m14.1"><semantics id="S3.SS2.SSS0.Px2.p1.14.m14.1a"><mrow id="S3.SS2.SSS0.Px2.p1.14.m14.1.1" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.cmml"><mn id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.2.cmml">2</mn><mo id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.1" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.3.cmml">N</mi></mrow><mo id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.1" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.3" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.14.m14.1b"><apply id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1"><plus id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.1"></plus><apply id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2"><times id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.1"></times><cn id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.2">2</cn><ci id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.2.3">𝑁</ci></apply><cn id="S3.SS2.SSS0.Px2.p1.14.m14.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.14.m14.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.14.m14.1c">2N+1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.14.m14.1d">2 italic_N + 1</annotation></semantics></math> frames centered around <math alttext="f_{m}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.15.m15.1"><semantics id="S3.SS2.SSS0.Px2.p1.15.m15.1a"><msub id="S3.SS2.SSS0.Px2.p1.15.m15.1.1" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.2" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1.2.cmml">f</mi><mi id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.3" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.15.m15.1b"><apply id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1.2">𝑓</ci><ci id="S3.SS2.SSS0.Px2.p1.15.m15.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.15.m15.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.15.m15.1c">f_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.15.m15.1d">italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math>. Specifically, the two-component loss is formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx2">
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}&amp;\mathcal{L}_{ret}=-\sum_{k=m-N}^{m+N}\text{log}%
\left(\frac{P(f_{k}|D_{j},U_{i+1})}{2N+1}\right)\\
&amp;\mathcal{L}_{cvmr}=\mathcal{L}_{ret}+\mathcal{L}_{pgag}\end{split}" class="ltx_Math" display="inline" id="S3.Ex2.m1.19"><semantics id="S3.Ex2.m1.19a"><mtable columnspacing="0pt" id="S3.Ex2.m1.19.19a" rowspacing="0pt" xref="S3.Ex2.m1.19.20.1.cmml"><mtr id="S3.Ex2.m1.19.19aa" xref="S3.Ex2.m1.19.20.1.cmml"><mtd id="S3.Ex2.m1.19.19ab" xref="S3.Ex2.m1.19.20.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex2.m1.19.19ac" xref="S3.Ex2.m1.19.20.1.cmml"><mrow id="S3.Ex2.m1.11.11.11.11.11a" xref="S3.Ex2.m1.19.20.1.cmml"><msub id="S3.Ex2.m1.11.11.11.11.11a.12" xref="S3.Ex2.m1.19.20.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml">ℒ</mi><mrow id="S3.Ex2.m1.2.2.2.2.2.2.1" xref="S3.Ex2.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.Ex2.m1.2.2.2.2.2.2.1.2" xref="S3.Ex2.m1.2.2.2.2.2.2.1.2.cmml">r</mi><mo id="S3.Ex2.m1.2.2.2.2.2.2.1.1" xref="S3.Ex2.m1.2.2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.2.2.2.2.1.3" xref="S3.Ex2.m1.2.2.2.2.2.2.1.3.cmml">e</mi><mo id="S3.Ex2.m1.2.2.2.2.2.2.1.1a" xref="S3.Ex2.m1.2.2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.2.2.2.2.1.4" xref="S3.Ex2.m1.2.2.2.2.2.2.1.4.cmml">t</mi></mrow></msub><mo id="S3.Ex2.m1.3.3.3.3.3.3" xref="S3.Ex2.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.Ex2.m1.11.11.11.11.11a.13" xref="S3.Ex2.m1.19.20.1.cmml"><mo id="S3.Ex2.m1.11.11.11.11.11a.13a" xref="S3.Ex2.m1.19.20.1a.cmml">−</mo><mrow id="S3.Ex2.m1.11.11.11.11.11a.13.1" xref="S3.Ex2.m1.19.20.1.cmml"><mstyle displaystyle="true" id="S3.Ex2.m1.11.11.11.11.11a.13.1.1" xref="S3.Ex2.m1.19.20.1.cmml"><munderover id="S3.Ex2.m1.11.11.11.11.11a.13.1.1a" xref="S3.Ex2.m1.19.20.1.cmml"><mo id="S3.Ex2.m1.5.5.5.5.5.5" movablelimits="false" xref="S3.Ex2.m1.5.5.5.5.5.5.cmml">∑</mo><mrow id="S3.Ex2.m1.6.6.6.6.6.6.1" xref="S3.Ex2.m1.6.6.6.6.6.6.1.cmml"><mi id="S3.Ex2.m1.6.6.6.6.6.6.1.2" xref="S3.Ex2.m1.6.6.6.6.6.6.1.2.cmml">k</mi><mo id="S3.Ex2.m1.6.6.6.6.6.6.1.1" xref="S3.Ex2.m1.6.6.6.6.6.6.1.1.cmml">=</mo><mrow id="S3.Ex2.m1.6.6.6.6.6.6.1.3" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.cmml"><mi id="S3.Ex2.m1.6.6.6.6.6.6.1.3.2" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.2.cmml">m</mi><mo id="S3.Ex2.m1.6.6.6.6.6.6.1.3.1" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.1.cmml">−</mo><mi id="S3.Ex2.m1.6.6.6.6.6.6.1.3.3" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.3.cmml">N</mi></mrow></mrow><mrow id="S3.Ex2.m1.7.7.7.7.7.7.1" xref="S3.Ex2.m1.7.7.7.7.7.7.1.cmml"><mi id="S3.Ex2.m1.7.7.7.7.7.7.1.2" xref="S3.Ex2.m1.7.7.7.7.7.7.1.2.cmml">m</mi><mo id="S3.Ex2.m1.7.7.7.7.7.7.1.1" xref="S3.Ex2.m1.7.7.7.7.7.7.1.1.cmml">+</mo><mi id="S3.Ex2.m1.7.7.7.7.7.7.1.3" xref="S3.Ex2.m1.7.7.7.7.7.7.1.3.cmml">N</mi></mrow></munderover></mstyle><mrow id="S3.Ex2.m1.11.11.11.11.11a.13.1.2" xref="S3.Ex2.m1.19.20.1.cmml"><mtext id="S3.Ex2.m1.8.8.8.8.8.8" xref="S3.Ex2.m1.8.8.8.8.8.8a.cmml">log</mtext><mo id="S3.Ex2.m1.11.11.11.11.11a.13.1.2.1" xref="S3.Ex2.m1.19.20.1a.cmml">⁢</mo><mrow id="S3.Ex2.m1.11.11.11.11.11a.13.1.2.2" xref="S3.Ex2.m1.19.20.1.cmml"><mo id="S3.Ex2.m1.9.9.9.9.9.9" xref="S3.Ex2.m1.19.20.1a.cmml">(</mo><mstyle displaystyle="true" id="S3.Ex2.m1.10.10.10.10.10.10" xref="S3.Ex2.m1.10.10.10.10.10.10.cmml"><mfrac id="S3.Ex2.m1.10.10.10.10.10.10a" xref="S3.Ex2.m1.10.10.10.10.10.10.cmml"><mrow id="S3.Ex2.m1.10.10.10.10.10.10.1" xref="S3.Ex2.m1.10.10.10.10.10.10.1.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.3.cmml">P</mi><mo id="S3.Ex2.m1.10.10.10.10.10.10.1.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.2.cmml">⁢</mo><mrow id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.cmml"><mo id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.cmml"><msub id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.2.cmml">f</mi><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.3.cmml">k</mi></msub><mo fence="false" id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.3.cmml">|</mo><mrow id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.3.cmml"><msub id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.2.cmml">D</mi><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.3.cmml">,</mo><msub id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.2.cmml">U</mi><mrow id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.2" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.1" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.1.cmml">+</mo><mn id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.3" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.Ex2.m1.10.10.10.10.10.10.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.cmml"><mrow id="S3.Ex2.m1.10.10.10.10.10.10.3.2" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.cmml"><mn id="S3.Ex2.m1.10.10.10.10.10.10.3.2.2" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.2.cmml">2</mn><mo id="S3.Ex2.m1.10.10.10.10.10.10.3.2.1" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.1.cmml">⁢</mo><mi id="S3.Ex2.m1.10.10.10.10.10.10.3.2.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.3.cmml">N</mi></mrow><mo id="S3.Ex2.m1.10.10.10.10.10.10.3.1" xref="S3.Ex2.m1.10.10.10.10.10.10.3.1.cmml">+</mo><mn id="S3.Ex2.m1.10.10.10.10.10.10.3.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.cmml">1</mn></mrow></mfrac></mstyle><mo id="S3.Ex2.m1.11.11.11.11.11.11" xref="S3.Ex2.m1.19.20.1a.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.Ex2.m1.19.19ad" xref="S3.Ex2.m1.19.20.1.cmml"><mtd id="S3.Ex2.m1.19.19ae" xref="S3.Ex2.m1.19.20.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex2.m1.19.19af" xref="S3.Ex2.m1.19.20.1.cmml"><mrow id="S3.Ex2.m1.19.19.19.8.8a" xref="S3.Ex2.m1.19.20.1.cmml"><msub id="S3.Ex2.m1.19.19.19.8.8a.9" xref="S3.Ex2.m1.19.20.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.12.12.12.1.1.1" xref="S3.Ex2.m1.12.12.12.1.1.1.cmml">ℒ</mi><mrow id="S3.Ex2.m1.13.13.13.2.2.2.1" xref="S3.Ex2.m1.13.13.13.2.2.2.1.cmml"><mi id="S3.Ex2.m1.13.13.13.2.2.2.1.2" xref="S3.Ex2.m1.13.13.13.2.2.2.1.2.cmml">c</mi><mo id="S3.Ex2.m1.13.13.13.2.2.2.1.1" xref="S3.Ex2.m1.13.13.13.2.2.2.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.13.13.13.2.2.2.1.3" xref="S3.Ex2.m1.13.13.13.2.2.2.1.3.cmml">v</mi><mo id="S3.Ex2.m1.13.13.13.2.2.2.1.1a" xref="S3.Ex2.m1.13.13.13.2.2.2.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.13.13.13.2.2.2.1.4" xref="S3.Ex2.m1.13.13.13.2.2.2.1.4.cmml">m</mi><mo id="S3.Ex2.m1.13.13.13.2.2.2.1.1b" xref="S3.Ex2.m1.13.13.13.2.2.2.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.13.13.13.2.2.2.1.5" xref="S3.Ex2.m1.13.13.13.2.2.2.1.5.cmml">r</mi></mrow></msub><mo id="S3.Ex2.m1.14.14.14.3.3.3" xref="S3.Ex2.m1.14.14.14.3.3.3.cmml">=</mo><mrow id="S3.Ex2.m1.19.19.19.8.8a.10" xref="S3.Ex2.m1.19.20.1.cmml"><msub id="S3.Ex2.m1.19.19.19.8.8a.10.1" xref="S3.Ex2.m1.19.20.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.15.15.15.4.4.4" xref="S3.Ex2.m1.15.15.15.4.4.4.cmml">ℒ</mi><mrow id="S3.Ex2.m1.16.16.16.5.5.5.1" xref="S3.Ex2.m1.16.16.16.5.5.5.1.cmml"><mi id="S3.Ex2.m1.16.16.16.5.5.5.1.2" xref="S3.Ex2.m1.16.16.16.5.5.5.1.2.cmml">r</mi><mo id="S3.Ex2.m1.16.16.16.5.5.5.1.1" xref="S3.Ex2.m1.16.16.16.5.5.5.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.16.16.16.5.5.5.1.3" xref="S3.Ex2.m1.16.16.16.5.5.5.1.3.cmml">e</mi><mo id="S3.Ex2.m1.16.16.16.5.5.5.1.1a" xref="S3.Ex2.m1.16.16.16.5.5.5.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.16.16.16.5.5.5.1.4" xref="S3.Ex2.m1.16.16.16.5.5.5.1.4.cmml">t</mi></mrow></msub><mo id="S3.Ex2.m1.17.17.17.6.6.6" xref="S3.Ex2.m1.17.17.17.6.6.6.cmml">+</mo><msub id="S3.Ex2.m1.19.19.19.8.8a.10.2" xref="S3.Ex2.m1.19.20.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.18.18.18.7.7.7" xref="S3.Ex2.m1.18.18.18.7.7.7.cmml">ℒ</mi><mrow id="S3.Ex2.m1.19.19.19.8.8.8.1" xref="S3.Ex2.m1.19.19.19.8.8.8.1.cmml"><mi id="S3.Ex2.m1.19.19.19.8.8.8.1.2" xref="S3.Ex2.m1.19.19.19.8.8.8.1.2.cmml">p</mi><mo id="S3.Ex2.m1.19.19.19.8.8.8.1.1" xref="S3.Ex2.m1.19.19.19.8.8.8.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.19.19.19.8.8.8.1.3" xref="S3.Ex2.m1.19.19.19.8.8.8.1.3.cmml">g</mi><mo id="S3.Ex2.m1.19.19.19.8.8.8.1.1a" xref="S3.Ex2.m1.19.19.19.8.8.8.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.19.19.19.8.8.8.1.4" xref="S3.Ex2.m1.19.19.19.8.8.8.1.4.cmml">a</mi><mo id="S3.Ex2.m1.19.19.19.8.8.8.1.1b" xref="S3.Ex2.m1.19.19.19.8.8.8.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.19.19.19.8.8.8.1.5" xref="S3.Ex2.m1.19.19.19.8.8.8.1.5.cmml">g</mi></mrow></msub></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.19b"><apply id="S3.Ex2.m1.19.20.1.cmml" xref="S3.Ex2.m1.19.19a"><and id="S3.Ex2.m1.19.20.1a.cmml" xref="S3.Ex2.m1.19.19ab"></and><apply id="S3.Ex2.m1.19.20.1b.cmml" xref="S3.Ex2.m1.19.19a"><eq id="S3.Ex2.m1.3.3.3.3.3.3.cmml" xref="S3.Ex2.m1.3.3.3.3.3.3"></eq><apply id="S3.Ex2.m1.19.20.1.2.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.2.1.cmml" xref="S3.Ex2.m1.19.19ab">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1">ℒ</ci><apply id="S3.Ex2.m1.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1"><times id="S3.Ex2.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.Ex2.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1.2">𝑟</ci><ci id="S3.Ex2.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1.3">𝑒</ci><ci id="S3.Ex2.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1.4">𝑡</ci></apply></apply><apply id="S3.Ex2.m1.19.20.1.4.cmml" xref="S3.Ex2.m1.19.19a"><minus id="S3.Ex2.m1.4.4.4.4.4.4.cmml" xref="S3.Ex2.m1.19.19a"></minus><apply id="S3.Ex2.m1.19.20.1.4.2.cmml" xref="S3.Ex2.m1.19.19a"><apply id="S3.Ex2.m1.19.20.1.4.2.1.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.4.2.1.1.cmml" xref="S3.Ex2.m1.19.19ab">superscript</csymbol><apply id="S3.Ex2.m1.19.20.1.4.2.1.2.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.4.2.1.2.1.cmml" xref="S3.Ex2.m1.19.19ab">subscript</csymbol><sum id="S3.Ex2.m1.5.5.5.5.5.5.cmml" xref="S3.Ex2.m1.5.5.5.5.5.5"></sum><apply id="S3.Ex2.m1.6.6.6.6.6.6.1.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1"><eq id="S3.Ex2.m1.6.6.6.6.6.6.1.1.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.1"></eq><ci id="S3.Ex2.m1.6.6.6.6.6.6.1.2.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.2">𝑘</ci><apply id="S3.Ex2.m1.6.6.6.6.6.6.1.3.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3"><minus id="S3.Ex2.m1.6.6.6.6.6.6.1.3.1.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.1"></minus><ci id="S3.Ex2.m1.6.6.6.6.6.6.1.3.2.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.2">𝑚</ci><ci id="S3.Ex2.m1.6.6.6.6.6.6.1.3.3.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1.3.3">𝑁</ci></apply></apply></apply><apply id="S3.Ex2.m1.7.7.7.7.7.7.1.cmml" xref="S3.Ex2.m1.7.7.7.7.7.7.1"><plus id="S3.Ex2.m1.7.7.7.7.7.7.1.1.cmml" xref="S3.Ex2.m1.7.7.7.7.7.7.1.1"></plus><ci id="S3.Ex2.m1.7.7.7.7.7.7.1.2.cmml" xref="S3.Ex2.m1.7.7.7.7.7.7.1.2">𝑚</ci><ci id="S3.Ex2.m1.7.7.7.7.7.7.1.3.cmml" xref="S3.Ex2.m1.7.7.7.7.7.7.1.3">𝑁</ci></apply></apply><apply id="S3.Ex2.m1.19.20.1.4.2.2.cmml" xref="S3.Ex2.m1.19.19a"><times id="S3.Ex2.m1.19.20.1.4.2.2.1.cmml" xref="S3.Ex2.m1.19.19ab"></times><ci id="S3.Ex2.m1.8.8.8.8.8.8a.cmml" xref="S3.Ex2.m1.8.8.8.8.8.8"><mtext id="S3.Ex2.m1.8.8.8.8.8.8.cmml" xref="S3.Ex2.m1.8.8.8.8.8.8">log</mtext></ci><apply id="S3.Ex2.m1.10.10.10.10.10.10.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10"><divide id="S3.Ex2.m1.10.10.10.10.10.10.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10"></divide><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1"><times id="S3.Ex2.m1.10.10.10.10.10.10.1.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.2"></times><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.3">𝑃</ci><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.3">conditional</csymbol><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4">subscript</csymbol><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.2">𝑓</ci><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.4.3">𝑘</ci></apply><list id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2"><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.2">𝐷</ci><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.2">𝑈</ci><apply id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3"><plus id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.1"></plus><ci id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.2">𝑖</ci><cn id="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.Ex2.m1.10.10.10.10.10.10.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply><apply id="S3.Ex2.m1.10.10.10.10.10.10.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3"><plus id="S3.Ex2.m1.10.10.10.10.10.10.3.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.1"></plus><apply id="S3.Ex2.m1.10.10.10.10.10.10.3.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2"><times id="S3.Ex2.m1.10.10.10.10.10.10.3.2.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.1"></times><cn id="S3.Ex2.m1.10.10.10.10.10.10.3.2.2.cmml" type="integer" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.2">2</cn><ci id="S3.Ex2.m1.10.10.10.10.10.10.3.2.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.3">𝑁</ci></apply><cn id="S3.Ex2.m1.10.10.10.10.10.10.3.3.cmml" type="integer" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3">1</cn></apply></apply><apply id="S3.Ex2.m1.19.20.1.4.2.2.4.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.4.2.2.4.1.cmml" xref="S3.Ex2.m1.19.19ab">subscript</csymbol><ci id="S3.Ex2.m1.12.12.12.1.1.1.cmml" xref="S3.Ex2.m1.12.12.12.1.1.1">ℒ</ci><apply id="S3.Ex2.m1.13.13.13.2.2.2.1.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1"><times id="S3.Ex2.m1.13.13.13.2.2.2.1.1.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1.1"></times><ci id="S3.Ex2.m1.13.13.13.2.2.2.1.2.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1.2">𝑐</ci><ci id="S3.Ex2.m1.13.13.13.2.2.2.1.3.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1.3">𝑣</ci><ci id="S3.Ex2.m1.13.13.13.2.2.2.1.4.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1.4">𝑚</ci><ci id="S3.Ex2.m1.13.13.13.2.2.2.1.5.cmml" xref="S3.Ex2.m1.13.13.13.2.2.2.1.5">𝑟</ci></apply></apply></apply></apply></apply></apply><apply id="S3.Ex2.m1.19.20.1c.cmml" xref="S3.Ex2.m1.19.19a"><eq id="S3.Ex2.m1.14.14.14.3.3.3.cmml" xref="S3.Ex2.m1.14.14.14.3.3.3"></eq><share href="https://arxiv.org/html/2409.19074v2#S3.Ex2.m1.19.20.1.4.cmml" id="S3.Ex2.m1.19.20.1d.cmml" xref="S3.Ex2.m1.19.19ab"></share><apply id="S3.Ex2.m1.19.20.1.6.cmml" xref="S3.Ex2.m1.19.19a"><plus id="S3.Ex2.m1.17.17.17.6.6.6.cmml" xref="S3.Ex2.m1.17.17.17.6.6.6"></plus><apply id="S3.Ex2.m1.19.20.1.6.2.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.6.2.1.cmml" xref="S3.Ex2.m1.19.19ab">subscript</csymbol><ci id="S3.Ex2.m1.15.15.15.4.4.4.cmml" xref="S3.Ex2.m1.15.15.15.4.4.4">ℒ</ci><apply id="S3.Ex2.m1.16.16.16.5.5.5.1.cmml" xref="S3.Ex2.m1.16.16.16.5.5.5.1"><times id="S3.Ex2.m1.16.16.16.5.5.5.1.1.cmml" xref="S3.Ex2.m1.16.16.16.5.5.5.1.1"></times><ci id="S3.Ex2.m1.16.16.16.5.5.5.1.2.cmml" xref="S3.Ex2.m1.16.16.16.5.5.5.1.2">𝑟</ci><ci id="S3.Ex2.m1.16.16.16.5.5.5.1.3.cmml" xref="S3.Ex2.m1.16.16.16.5.5.5.1.3">𝑒</ci><ci id="S3.Ex2.m1.16.16.16.5.5.5.1.4.cmml" xref="S3.Ex2.m1.16.16.16.5.5.5.1.4">𝑡</ci></apply></apply><apply id="S3.Ex2.m1.19.20.1.6.3.cmml" xref="S3.Ex2.m1.19.19a"><csymbol cd="ambiguous" id="S3.Ex2.m1.19.20.1.6.3.1.cmml" xref="S3.Ex2.m1.19.19ab">subscript</csymbol><ci id="S3.Ex2.m1.18.18.18.7.7.7.cmml" xref="S3.Ex2.m1.18.18.18.7.7.7">ℒ</ci><apply id="S3.Ex2.m1.19.19.19.8.8.8.1.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1"><times id="S3.Ex2.m1.19.19.19.8.8.8.1.1.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1.1"></times><ci id="S3.Ex2.m1.19.19.19.8.8.8.1.2.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1.2">𝑝</ci><ci id="S3.Ex2.m1.19.19.19.8.8.8.1.3.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1.3">𝑔</ci><ci id="S3.Ex2.m1.19.19.19.8.8.8.1.4.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1.4">𝑎</ci><ci id="S3.Ex2.m1.19.19.19.8.8.8.1.5.cmml" xref="S3.Ex2.m1.19.19.19.8.8.8.1.5">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.19c">\displaystyle\begin{split}&amp;\mathcal{L}_{ret}=-\sum_{k=m-N}^{m+N}\text{log}%
\left(\frac{P(f_{k}|D_{j},U_{i+1})}{2N+1}\right)\\
&amp;\mathcal{L}_{cvmr}=\mathcal{L}_{ret}+\mathcal{L}_{pgag}\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.19d">start_ROW start_CELL end_CELL start_CELL caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_k = italic_m - italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m + italic_N end_POSTSUPERSCRIPT log ( divide start_ARG italic_P ( italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_U start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) end_ARG start_ARG 2 italic_N + 1 end_ARG ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL caligraphic_L start_POSTSUBSCRIPT italic_c italic_v italic_m italic_r end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_p italic_g italic_a italic_g end_POSTSUBSCRIPT end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Visually-Informed Step Generation (VSG).</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.7">In this last task, given a user request <math alttext="U_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">U</mi><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2">𝑈</ci><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3"><plus id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">U_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.1.m1.1d">italic_U start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> and a user-uploaded image <math alttext="I_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml">I</mi><mrow id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2">𝐼</ci><apply id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3"><plus id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">I_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math>, that visually depicts their current progress on the task being executed, the goal is to generate an appropriate system response <math alttext="R_{i+1}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px3.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml">R</mi><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2">𝑅</ci><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3"><plus id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.1"></plus><ci id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.3.m3.1c">R_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.3.m3.1d">italic_R start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> , that accurately copies the relevant plan step <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.4.m4.1d">italic_s</annotation></semantics></math>, while accounting for the conversational history <math alttext="D_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px3.p1.5.m5.1a"><msub id="S3.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml">D</mi><mi id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2">𝐷</ci><ci id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.5.m5.1c">D_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, considering only the previous turns <math alttext="T_{i-c:i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px3.p1.6.m6.1a"><msub id="S3.SS2.SSS0.Px3.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.2.cmml">T</mi><mrow id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.cmml"><mrow id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.2" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.2.cmml">i</mi><mo id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.1" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.1.cmml">−</mo><mi id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.3" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.3.cmml">c</mi></mrow><mo id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.1.cmml">:</mo><mi id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.3" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.2">𝑇</ci><apply id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3"><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.1">:</ci><apply id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2"><minus id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.1"></minus><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.2">𝑖</ci><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.2.3">𝑐</ci></apply><ci id="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px3.p1.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.6.m6.1c">T_{i-c:i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.6.m6.1d">italic_T start_POSTSUBSCRIPT italic_i - italic_c : italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, with <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px3.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px3.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.7.m7.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.7.m7.1d">italic_c</annotation></semantics></math> being the context size. The loss is formulated as a visually conditioned cross-entropy loss,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A4.EGx3">
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{vsg}=-\sum_{t=1}^{T}log~{}P(w_{t}^{r}|w_{t-1:1}^{r},%
\bm{I_{i+1}},D_{j},U_{i+1})." class="ltx_Math" display="inline" id="S3.Ex3.m1.1"><semantics id="S3.Ex3.m1.1a"><mrow id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml"><msub id="S3.Ex3.m1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.1.1.3.2" xref="S3.Ex3.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.Ex3.m1.1.1.1.1.3.3" xref="S3.Ex3.m1.1.1.1.1.3.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.3.3.2" xref="S3.Ex3.m1.1.1.1.1.3.3.2.cmml">v</mi><mo id="S3.Ex3.m1.1.1.1.1.3.3.1" xref="S3.Ex3.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex3.m1.1.1.1.1.3.3.3" xref="S3.Ex3.m1.1.1.1.1.3.3.3.cmml">s</mi><mo id="S3.Ex3.m1.1.1.1.1.3.3.1a" xref="S3.Ex3.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex3.m1.1.1.1.1.3.3.4" xref="S3.Ex3.m1.1.1.1.1.3.3.4.cmml">g</mi></mrow></msub><mo id="S3.Ex3.m1.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.Ex3.m1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.cmml"><mo id="S3.Ex3.m1.1.1.1.1.1a" xref="S3.Ex3.m1.1.1.1.1.1.cmml">−</mo><mrow id="S3.Ex3.m1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.Ex3.m1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.1.1.2.cmml"><munderover id="S3.Ex3.m1.1.1.1.1.1.1.2a" xref="S3.Ex3.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.Ex3.m1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.2.cmml">t</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex3.m1.1.1.1.1.1.1.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.2.3.cmml">T</mi></munderover></mstyle><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml">l</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.4" xref="S3.Ex3.m1.1.1.1.1.1.1.1.4.cmml">o</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.2a" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.5" xref="S3.Ex3.m1.1.1.1.1.1.1.1.5.cmml">g</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.2b" lspace="0.330em" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.6" xref="S3.Ex3.m1.1.1.1.1.1.1.1.6.cmml">P</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.2c" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.2.cmml">w</mi><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.3.cmml">t</mi><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.3.cmml">r</mi></msubsup><mo fence="false" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.5.cmml">|</mo><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.5.cmml"><msubsup id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">t</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.1.cmml">−</mo><mn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">1</mn></mrow><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">:</mo><mn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">r</mi></msubsup><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.5" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.5.cmml">,</mo><msub id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝑰</mi><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">𝒊</mi><mo class="ltx_mathvariant_bold" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1" mathvariant="bold" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">+</mo><mn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">𝟏</mn></mrow></msub><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.6" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.5.cmml">,</mo><msub id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">D</mi><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">j</mi></msub><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.7" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.5.cmml">,</mo><msub id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.2.cmml">U</mi><mrow id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.2" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.2.cmml">i</mi><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.1" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.1.cmml">+</mo><mn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.3" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex3.m1.1.1.1.2" lspace="0em" xref="S3.Ex3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1"><eq id="S3.Ex3.m1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.2"></eq><apply id="S3.Ex3.m1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.3.2">ℒ</ci><apply id="S3.Ex3.m1.1.1.1.1.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.3.3"><times id="S3.Ex3.m1.1.1.1.1.3.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.3.3.1"></times><ci id="S3.Ex3.m1.1.1.1.1.3.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.3.3.2">𝑣</ci><ci id="S3.Ex3.m1.1.1.1.1.3.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.3.3.3">𝑠</ci><ci id="S3.Ex3.m1.1.1.1.1.3.3.4.cmml" xref="S3.Ex3.m1.1.1.1.1.3.3.4">𝑔</ci></apply></apply><apply id="S3.Ex3.m1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1"><minus id="S3.Ex3.m1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1"></minus><apply id="S3.Ex3.m1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1"><apply id="S3.Ex3.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.2">𝑡</ci><cn id="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1"><times id="S3.Ex3.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.2"></times><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.3">𝑙</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.4">𝑜</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.5">𝑔</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.6.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.6">𝑃</ci><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.5">conditional</csymbol><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6">superscript</csymbol><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.2">𝑤</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.2.3">𝑡</ci></apply><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.6.3">𝑟</ci></apply><list id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.5.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4"><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1">:</ci><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2"><minus id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.1"></minus><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.2">𝑡</ci><cn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.3">1</cn></apply><cn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑟</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑰</ci><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3"><plus id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1"></plus><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2">𝒊</ci><cn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.2">𝐷</ci><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.3">𝑗</ci></apply><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4">subscript</csymbol><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.2">𝑈</ci><apply id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3"><plus id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.1.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.1"></plus><ci id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.2.cmml" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.2">𝑖</ci><cn id="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.1.1.1.1.1.1.1.1.4.4.4.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">\displaystyle\mathcal{L}_{vsg}=-\sum_{t=1}^{T}log~{}P(w_{t}^{r}|w_{t-1:1}^{r},%
\bm{I_{i+1}},D_{j},U_{i+1}).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex3.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_v italic_s italic_g end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_l italic_o italic_g italic_P ( italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT | italic_w start_POSTSUBSCRIPT italic_t - 1 : 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , bold_italic_I start_POSTSUBSCRIPT bold_italic_i bold_+ bold_1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_U start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Architecture</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="S3.F2.g1" src="x2.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comprehensive illustration of the MM-PlanLLM architecture, including the 3 training stages employed for model training. *Denotes the <span class="ltx_text ltx_font_typewriter" id="S3.F2.2.1">[RET]</span> token embedding representations and the Language Modeling Head of the LLM remain trainable.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The architecture of the proposed model, MM-PlanLLM, expands on the framework presented in FROMAGe <cite class="ltx_cite ltx_citemacro_cite">Koh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib24" title="">2023</a>)</cite> and is composed of three main component groups: <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">a) a language model backbone</span>, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">b) a vision encoder</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">c) task-specific projection layers</span>.
Each of these layers will be responsible for establishing an interface between the visual encoder and language model representations, while providing an efficient adaptation to new tasks, in a sequential or interleaved manner.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.F2" title="Figure 2 ‣ 3.3 Model Architecture ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of this architecture. This section describes these three main component groups:</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">a) V&amp;L Model Backbone.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">The vision and language backbone model, takes as input a multimodal sequence, comprised of a user request <math alttext="U_{n+1}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">U</mi><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2">𝑈</ci><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3"><plus id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1"></plus><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2">𝑛</ci><cn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">U_{n+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">italic_U start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the conversation history <math alttext="D" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">italic_D</annotation></semantics></math>, and an optional image <math alttext="I_{n+1}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml">I</mi><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">n</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.2">𝐼</ci><apply id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3"><plus id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.1"></plus><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.2">𝑛</ci><cn id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">I_{n+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and generates an appropriate system response.
For MM-PlanLLM’s backbone model we use a pretrained decoder-only Transformer model. We experiment with different backbone models as detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A2" title="Appendix B LM Backbone Ablation ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">B</span></a>.
The backbone LM model is trained with cross-entropy loss.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">b) Video Encoder.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.7">Given a frame <math alttext="I_{n}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">I</mi><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2">𝐼</ci><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">I_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> with resolution <math alttext="H" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.2.m2.1d">italic_H</annotation></semantics></math>x<math alttext="W" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.3.m3.1d">italic_W</annotation></semantics></math>, we leverage the ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib12" title="">2021</a>)</cite> architecture, such that the video encoder outputs a learnable <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.7.1">[CLS]</span> token that attends to the entire frame, and a sequence of <math alttext="N_{v}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2.cmml">N</mi><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.2">𝑁</ci><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">N_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.4.m4.1d">italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> visual tokens <math alttext="v_{i}\in\mathbb{R}^{d_{ve}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml"><msub id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.2.cmml">v</mi><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.2.cmml">d</mi><mrow id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.2.cmml">v</mi><mo id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.3.cmml">e</mi></mrow></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1"><in id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1"></in><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.2">𝑣</ci><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.2">𝑑</ci><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3"><times id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.1"></times><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.2">𝑣</ci><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">v_{i}\in\mathbb{R}^{d_{ve}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.5.m5.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="d_{ve}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml">d</mi><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.2.cmml">v</mi><mo id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2">𝑑</ci><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3"><times id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.1"></times><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.2">𝑣</ci><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">d_{ve}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.6.m6.1d">italic_d start_POSTSUBSCRIPT italic_v italic_e end_POSTSUBSCRIPT</annotation></semantics></math> being the visual token embedding dimension.
Each token is the result of attending to different non-overlapping patches of the frame, with <math alttext="\frac{H}{N_{v}}\times\frac{W}{N_{v}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS3.SSS0.Px2.p1.7.m7.1a"><mrow id="S3.SS3.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml"><mfrac id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.2.cmml">H</mi><msub id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.2.cmml">N</mi><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.3.cmml">v</mi></msub></mfrac><mo id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.1.cmml">×</mo><mfrac id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.2.cmml">W</mi><msub id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.2.cmml">N</mi><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.3.cmml">v</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1"><times id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.1"></times><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2"><divide id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2"></divide><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.2">𝐻</ci><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.2">𝑁</ci><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.3.3">𝑣</ci></apply></apply><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3"><divide id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3"></divide><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.2">𝑊</ci><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.2">𝑁</ci><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.7.m7.1c">\frac{H}{N_{v}}\times\frac{W}{N_{v}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.7.m7.1d">divide start_ARG italic_H end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG × divide start_ARG italic_W end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> resolution. In MM-PlanLLM this encoder remains frozen.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">c) Task-specific layers.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">Support for novel tasks is achieved in MM-PlanLLM through task-specific projection layers:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.2.1">VSG-specific layers.</span>
For VSG, and general Image-to-Text support, we learn a single linear mapping <math alttext="W_{c}\in\mathbb{R}^{d_{ve}\times d}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><msub id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.1.2.2.cmml">W</mi><mi id="S3.I1.i1.p1.1.m1.1.1.2.3" xref="S3.I1.i1.p1.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S3.I1.i1.p1.1.m1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.cmml"><msub id="S3.I1.i1.p1.1.m1.1.1.3.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.3.2.2" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.2.cmml">d</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.2.cmml">v</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.1" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.3.cmml">e</mi></mrow></msub><mo id="S3.I1.i1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.I1.i1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><in id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1"></in><apply id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.2">𝑊</ci><ci id="S3.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.3">𝑐</ci></apply><apply id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3"><times id="S3.I1.i1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.1"></times><apply id="S3.I1.i1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.3.3.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.2">𝑑</ci><apply id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3"><times id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.1"></times><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.2">𝑣</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.2.3.3">𝑒</ci></apply></apply><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">W_{c}\in\mathbb{R}^{d_{ve}\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v italic_e end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="d" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mi id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">italic_d</annotation></semantics></math> being the LLM hidden dimension, that maps the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.2.2">[CLS]</span>, obtained from the visual encoder, token to the embedding space of the language model, the resulting representation used to replace the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.2.3">[IMG]</span> text embedding in the LLM.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">CVMR-specific layers.</span>
For CVMR, the model needs to be able to retrieve the middle frame of the relevant video clip, for the current moment.
In our task, each textual step is annotated with a relevant video segment.
The fact that these textual steps are not directly describing the clip visual content, but rather the actions that the user has to perform, poses a greater challenge, compared to traditional VMR datasets where the captions offer a visual description of the clip.</p>
</div>
<div class="ltx_para" id="S3.I1.i2.p2">
<p class="ltx_p" id="S3.I1.i2.p2.5">We propose to address this challenge with a multi-stage multimodal plan-grounded training scheme, designed to close the visual<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.I1.i2.p2.1.m1.1"><semantics id="S3.I1.i2.p2.1.m1.1a"><mo id="S3.I1.i2.p2.1.m1.1.1" stretchy="false" xref="S3.I1.i2.p2.1.m1.1.1.cmml">⇔</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.1.m1.1b"><ci id="S3.I1.i2.p2.1.m1.1.1.cmml" xref="S3.I1.i2.p2.1.m1.1.1">⇔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.1.m1.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.1.m1.1d">⇔</annotation></semantics></math>plan step semantic gap. Originally, a <span class="ltx_text ltx_font_typewriter" id="S3.I1.i2.p2.5.1">[RET]</span> token is added to the language model’s vocabulary, and is then appended to the end of each retrieval request. Its decoder-output embedding is then mapped onto a cross-modal retrieval embedding space, using a trained linear mapping <math alttext="W_{t}\in\mathbb{R}^{d\times q}" class="ltx_Math" display="inline" id="S3.I1.i2.p2.2.m2.1"><semantics id="S3.I1.i2.p2.2.m2.1a"><mrow id="S3.I1.i2.p2.2.m2.1.1" xref="S3.I1.i2.p2.2.m2.1.1.cmml"><msub id="S3.I1.i2.p2.2.m2.1.1.2" xref="S3.I1.i2.p2.2.m2.1.1.2.cmml"><mi id="S3.I1.i2.p2.2.m2.1.1.2.2" xref="S3.I1.i2.p2.2.m2.1.1.2.2.cmml">W</mi><mi id="S3.I1.i2.p2.2.m2.1.1.2.3" xref="S3.I1.i2.p2.2.m2.1.1.2.3.cmml">t</mi></msub><mo id="S3.I1.i2.p2.2.m2.1.1.1" xref="S3.I1.i2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.I1.i2.p2.2.m2.1.1.3" xref="S3.I1.i2.p2.2.m2.1.1.3.cmml"><mi id="S3.I1.i2.p2.2.m2.1.1.3.2" xref="S3.I1.i2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.I1.i2.p2.2.m2.1.1.3.3" xref="S3.I1.i2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.I1.i2.p2.2.m2.1.1.3.3.2" xref="S3.I1.i2.p2.2.m2.1.1.3.3.2.cmml">d</mi><mo id="S3.I1.i2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.I1.i2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.I1.i2.p2.2.m2.1.1.3.3.3" xref="S3.I1.i2.p2.2.m2.1.1.3.3.3.cmml">q</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.2.m2.1b"><apply id="S3.I1.i2.p2.2.m2.1.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1"><in id="S3.I1.i2.p2.2.m2.1.1.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.1"></in><apply id="S3.I1.i2.p2.2.m2.1.1.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p2.2.m2.1.1.2.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.I1.i2.p2.2.m2.1.1.2.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.2.2">𝑊</ci><ci id="S3.I1.i2.p2.2.m2.1.1.2.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.2.3">𝑡</ci></apply><apply id="S3.I1.i2.p2.2.m2.1.1.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p2.2.m2.1.1.3.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.I1.i2.p2.2.m2.1.1.3.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.I1.i2.p2.2.m2.1.1.3.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.3"><times id="S3.I1.i2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.I1.i2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.3.2">𝑑</ci><ci id="S3.I1.i2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.I1.i2.p2.2.m2.1.1.3.3.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.2.m2.1c">W_{t}\in\mathbb{R}^{d\times q}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_q end_POSTSUPERSCRIPT</annotation></semantics></math>. A second linear layer is trained <math alttext="W_{i}\in\mathbb{R}^{d_{ve}\times q}" class="ltx_Math" display="inline" id="S3.I1.i2.p2.3.m3.1"><semantics id="S3.I1.i2.p2.3.m3.1a"><mrow id="S3.I1.i2.p2.3.m3.1.1" xref="S3.I1.i2.p2.3.m3.1.1.cmml"><msub id="S3.I1.i2.p2.3.m3.1.1.2" xref="S3.I1.i2.p2.3.m3.1.1.2.cmml"><mi id="S3.I1.i2.p2.3.m3.1.1.2.2" xref="S3.I1.i2.p2.3.m3.1.1.2.2.cmml">W</mi><mi id="S3.I1.i2.p2.3.m3.1.1.2.3" xref="S3.I1.i2.p2.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.I1.i2.p2.3.m3.1.1.1" xref="S3.I1.i2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.I1.i2.p2.3.m3.1.1.3" xref="S3.I1.i2.p2.3.m3.1.1.3.cmml"><mi id="S3.I1.i2.p2.3.m3.1.1.3.2" xref="S3.I1.i2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.I1.i2.p2.3.m3.1.1.3.3" xref="S3.I1.i2.p2.3.m3.1.1.3.3.cmml"><msub id="S3.I1.i2.p2.3.m3.1.1.3.3.2" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.cmml"><mi id="S3.I1.i2.p2.3.m3.1.1.3.3.2.2" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.2.cmml">d</mi><mrow id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.cmml"><mi id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.2" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.2.cmml">v</mi><mo id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.1" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.3" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.3.cmml">e</mi></mrow></msub><mo id="S3.I1.i2.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.I1.i2.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.I1.i2.p2.3.m3.1.1.3.3.3" xref="S3.I1.i2.p2.3.m3.1.1.3.3.3.cmml">q</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.3.m3.1b"><apply id="S3.I1.i2.p2.3.m3.1.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1"><in id="S3.I1.i2.p2.3.m3.1.1.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.1"></in><apply id="S3.I1.i2.p2.3.m3.1.1.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p2.3.m3.1.1.2.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.I1.i2.p2.3.m3.1.1.2.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.2.2">𝑊</ci><ci id="S3.I1.i2.p2.3.m3.1.1.2.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.I1.i2.p2.3.m3.1.1.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p2.3.m3.1.1.3.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.I1.i2.p2.3.m3.1.1.3.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.I1.i2.p2.3.m3.1.1.3.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3"><times id="S3.I1.i2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.1"></times><apply id="S3.I1.i2.p2.3.m3.1.1.3.3.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.I1.i2.p2.3.m3.1.1.3.3.2.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.I1.i2.p2.3.m3.1.1.3.3.2.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.2">𝑑</ci><apply id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3"><times id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.1.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.1"></times><ci id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.2.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.2">𝑣</ci><ci id="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.2.3.3">𝑒</ci></apply></apply><ci id="S3.I1.i2.p2.3.m3.1.1.3.3.3.cmml" xref="S3.I1.i2.p2.3.m3.1.1.3.3.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.3.m3.1c">W_{i}\in\mathbb{R}^{d_{ve}\times q}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v italic_e end_POSTSUBSCRIPT × italic_q end_POSTSUPERSCRIPT</annotation></semantics></math> to map the visual features onto the retrieval space. We leverage this approach, and use the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i2.p2.5.2">[RET]</span> token to retrieve the video moment.
For the training of these layers, we use the InfoNCE Loss <cite class="ltx_cite ltx_citemacro_cite">van den Oord et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib50" title="">2018</a>)</cite> as the <math alttext="\mathcal{L}_{ret}" class="ltx_Math" display="inline" id="S3.I1.i2.p2.4.m4.1"><semantics id="S3.I1.i2.p2.4.m4.1a"><msub id="S3.I1.i2.p2.4.m4.1.1" xref="S3.I1.i2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p2.4.m4.1.1.2" xref="S3.I1.i2.p2.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S3.I1.i2.p2.4.m4.1.1.3" xref="S3.I1.i2.p2.4.m4.1.1.3.cmml"><mi id="S3.I1.i2.p2.4.m4.1.1.3.2" xref="S3.I1.i2.p2.4.m4.1.1.3.2.cmml">r</mi><mo id="S3.I1.i2.p2.4.m4.1.1.3.1" xref="S3.I1.i2.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.4.m4.1.1.3.3" xref="S3.I1.i2.p2.4.m4.1.1.3.3.cmml">e</mi><mo id="S3.I1.i2.p2.4.m4.1.1.3.1a" xref="S3.I1.i2.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i2.p2.4.m4.1.1.3.4" xref="S3.I1.i2.p2.4.m4.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.4.m4.1b"><apply id="S3.I1.i2.p2.4.m4.1.1.cmml" xref="S3.I1.i2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p2.4.m4.1.1.1.cmml" xref="S3.I1.i2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.I1.i2.p2.4.m4.1.1.2.cmml" xref="S3.I1.i2.p2.4.m4.1.1.2">ℒ</ci><apply id="S3.I1.i2.p2.4.m4.1.1.3.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3"><times id="S3.I1.i2.p2.4.m4.1.1.3.1.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3.1"></times><ci id="S3.I1.i2.p2.4.m4.1.1.3.2.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3.2">𝑟</ci><ci id="S3.I1.i2.p2.4.m4.1.1.3.3.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3.3">𝑒</ci><ci id="S3.I1.i2.p2.4.m4.1.1.3.4.cmml" xref="S3.I1.i2.p2.4.m4.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.4.m4.1c">\mathcal{L}_{ret}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT</annotation></semantics></math> loss component, where we consider the middle clip frame, plus a bidirectional context-window of <math alttext="N" class="ltx_Math" display="inline" id="S3.I1.i2.p2.5.m5.1"><semantics id="S3.I1.i2.p2.5.m5.1a"><mi id="S3.I1.i2.p2.5.m5.1.1" xref="S3.I1.i2.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p2.5.m5.1b"><ci id="S3.I1.i2.p2.5.m5.1.1.cmml" xref="S3.I1.i2.p2.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p2.5.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p2.5.m5.1d">italic_N</annotation></semantics></math> consecutive frames, as targets (i.e. positives). To incorporate temporal information we use fixed Rotary Positional Embeddings (RoPE) <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib42" title="">2021</a>)</cite> and apply temporal position shifting, where each positional embedding is shifted according to the frame’s position within the video.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-stage Multimodal Training</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The model undergoes a multi-stage training scheme, in two core tasks: <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">image captioning</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.2">text-to-image retrieval</span>. We design a three-stage training approach tailored to our setting:</p>
</div>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Stage 1. Visual Projection Layers.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.3">This preliminary phase is focused on bootstrapping the model’s linear layers, <math alttext="W_{c}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS4.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2">𝑊</ci><ci id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p1.1.m1.1c">W_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px1.p1.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W_{t}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS4.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS4.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml">W</mi><mi id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1.2">𝑊</ci><ci id="S3.SS4.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p1.2.m2.1c">W_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px1.p1.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="W_{i}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS4.SSS0.Px1.p1.3.m3.1a"><msub id="S3.SS4.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml">W</mi><mi id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1.2">𝑊</ci><ci id="S3.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p1.3.m3.1c">W_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px1.p1.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, by training on the <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS0.Px1.p1.3.1">image-captioning</span> and <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS0.Px1.p1.3.2">image-text retrieval</span> tasks.
For both tasks, we use the CC3M <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib40" title="">2018</a>)</cite> dataset, while the LLM and Visual Encoder are kept static. Only the embedding for the introduced <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS0.Px1.p1.3.3">[RET]</span> token and the language modeling head are subject to training.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Stage 2. Task Data Specialization.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">The subsequent stage seeks to specialize the model in the target domain. The same previous two proxy tasks are considered, but instead of general-domain data, we use domain-specific videos and captions. Specifically, we leverage the annotations present in the Tasty Dataset <cite class="ltx_cite ltx_citemacro_cite">Sener and Yao (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib39" title="">2019</a>)</cite>. In this dataset, recipes are broken into actions, and these actions are then annotated with the start and end frame of the relevant video clip; we use these to create image-text pairs where the text is the action text and the image is the middle frame of the relevant clip.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Stage 3. Multimodal Plan-Grounded Dialogue.</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">The third, and most important, training stage aims to convey the necessary abilities to dialogue in the target plan-grounded dialogue setting on the recipes domain, attending to both uni- and multimodal user requests. To this extent, plan-following multimodal instructional data is used (see section <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S3.SS5" title="3.5 Synthetic Multimodal Plan-oriented Training Data ‣ 3 Multimodal Plan-Grounded LM ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">3.5</span></a>), covering dialogue interactions, with particular emphasis on the envisioned multimodal interactions.
To facilitate training, we start with text-only samples and then move to multimodal ones, for the latter we alternate between CVMR and VSG batches. During this phase, the LLM is fully trained, along with all of the additional linear layers.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Synthetic Multimodal Plan-oriented Training Data </h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">To prepare the model to cope with the wide range of user requests in plan-grounded dialogues, we resort to synthetic data generation. Namely, we build upon the methodology of PlanLLM, and further incorporate multimodal queries. This methodology follows a pipeline that utilizes real user-agent dialogues and, using an intent classifier, extracts a user policy and user utterances. To generate dialogues, user intents are selected for each turn, and a combination of templates, external knowledge bases, and generative models are used to create accurate system responses.
The incorporation of multimodal requests is accomplished by exploiting the Tasty Videos Dataset <cite class="ltx_cite ltx_citemacro_cite">Sener and Yao (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib39" title="">2019</a>)</cite>, which comprises culinary recipes, each accompanied by a video and annotations delineating each step into individual actions and signaling the start and end of the said actions within the video. Herein, we detail how these annotations were leveraged to integrate multimodal user requests into the pre-existing data generation pipeline.</p>
</div>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">CVMR Requests.</h4>
<div class="ltx_para" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">For the retrieval of specific video moments, the target clip corresponds to the one annotated for the current action. In instances where a plan step is composed of multiple actions, the first action is considered.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">VSG Requests.</h4>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">Regarding VSG queries, a step subsequent to the user’s current progress within the recipe is selected (e.g., if the user is at step 3, any step from 4 onwards is eligible), biasing to closer steps. The middle frame of the selected step is then used as the user-uploaded image that showcases the user’s progress, at that point in the dialogue.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p2.1">For both request types, the textual user requests and system responses are sampled from handwritten template lists. To improve diversity, an external generative model is prompted to extend the lists of possible user and system utterances. To this dataset we call <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS0.Px2.p2.1.1">Tasty</span> <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS0.Px2.p2.1.2">Vid</span>eo <span class="ltx_text ltx_font_bold" id="S3.SS5.SSS0.Px2.p2.1.3">Dial</span>ogue (TastyVidDial).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Instructional Tasks Datasets</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">TastyVidDial.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">To conduct our experiments, we propose a novel dataset for conversational multimodal dialogue over complex tasks.
We create a dataset of 50k generated dialogues, between a user and a multimodal agent, while following complex tasks, resulting in <math alttext="\approx 500k" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">500</mn><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><approx id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3"><times id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1"></times><cn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">500</cn><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">\approx 500k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">≈ 500 italic_k</annotation></semantics></math> dialogue turns.
We utilize a set of 1500 illustrated recipes obtained from the Tasty Videos Dataset <cite class="ltx_cite ltx_citemacro_cite">Sener and Yao (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib39" title="">2019</a>)</cite> to ground the generated dialogues.
To maximize dialogue quality, we only consider recipes with 5 to 10 steps, at least 6 ingredients, no more than 300 tokens, and at least 1 annotated video action for every step. To reduce frame count, we consider 1 for every 20 frames in the video.
For training, validation, and testing we use a 90/5/5 split.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Simulated Alexa TaskBot.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">For the evaluation of text-only requests, we use the PlanLLM dataset as described in <cite class="ltx_cite ltx_citemacro_citet">Glória-Silva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib15" title="">2024</a>)</cite>. We use this dataset version to avoid dialogue turns where the user request is text-only but one of the previous turns, present in the context, is multimodal.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Methodology</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Backbone Models.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">For the LM Backbone we use LLama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib49" title="">2023b</a>)</cite> (results with more models in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A2" title="Appendix B LM Backbone Ablation ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">B</span></a>). The visual encoder is CLIP ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib36" title="">2021</a>)</cite>. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A1" title="Appendix A Implementation Details ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">A</span></a> for more implementation details.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Metrics.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.4">For evaluation of CVMR turns, we follow recent works <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib13" title="">2023</a>); Diwan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib11" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib52" title="">2022</a>)</cite> and use <math alttext="R@n" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">R</mi><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3" mathvariant="normal" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml">@</mi><mo id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1a" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.4" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.4.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1"><times id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.2">𝑅</ci><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.3">@</ci><ci id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.4">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">R@n</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.1.m1.1d">italic_R @ italic_n</annotation></semantics></math>, mean Average Precision (mAP),
Step Accuracy to measure if the retrieved frame is inside the video moment for the relevant step, and Mean Normalized Frame Distance (MNFD)
<math alttext="\mathrm{MNFD}=\frac{1}{N}\sum_{i=1}^{N}\frac{|f_{\text{retrieved},i}-f_{\text{%
target},i}|}{F_{i}}," class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.2.m2.6"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.6a"><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.cmml"><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.cmml"><mi id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.2.cmml">MNFD</mi><mo id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.cmml"><mfrac id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.cmml"><mn id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.2.cmml">1</mn><mi id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.3.cmml">N</mi></mfrac><mo id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.1.cmml">⁢</mo><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.cmml"><msubsup id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.cmml"><mo id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.cmml"><mi id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.3.cmml">N</mi></msubsup><mfrac id="S4.SS2.SSS0.Px2.p1.2.m2.5.5" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.cmml"><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.6.cmml"><mo id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.2" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.6.1.cmml">|</mo><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.cmml"><msub id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.cmml"><mi id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.2.cmml">f</mi><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.4" xref="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml"><mtext id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1a.cmml">retrieved</mtext><mo id="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.4.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.1.cmml">−</mo><msub id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.cmml"><mi id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.2.cmml">f</mi><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.4" xref="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.3.cmml"><mtext id="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1a.cmml">target</mtext><mo id="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.4.1" xref="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.2.cmml">i</mi></mrow></msub></mrow><mo id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.3" stretchy="false" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.6.1.cmml">|</mo></mrow><msub id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.cmml"><mi id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.2.cmml">F</mi><mi id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.3" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.3.cmml">i</mi></msub></mfrac></mrow></mrow></mrow><mo id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.2" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.2.m2.6b"><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1"><eq id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.1"></eq><ci id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.2">MNFD</ci><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3"><times id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.1"></times><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2"><divide id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2"></divide><cn id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.2.cmml" type="integer" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.2">1</cn><ci id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.2.3">𝑁</ci></apply><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3"><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1">superscript</csymbol><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1">subscript</csymbol><sum id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.2"></sum><apply id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3"><eq id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.1"></eq><ci id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.2">𝑖</ci><cn id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.6.6.1.1.3.3.1.3">𝑁</ci></apply><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5"><divide id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.6.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5"></divide><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.6.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5"><abs id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.6.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.2"></abs><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1"><minus id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.1"></minus><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2">subscript</csymbol><ci id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.2.2">𝑓</ci><list id="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.4"><ci id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1a.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1"><mtext id="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.cmml" mathsize="50%" xref="S4.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.1">retrieved</mtext></ci><ci id="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.2">𝑖</ci></list></apply><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3">subscript</csymbol><ci id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.5.5.1.3.2">𝑓</ci><list id="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.4"><ci id="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1a.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1"><mtext id="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1.cmml" mathsize="50%" xref="S4.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.1">target</mtext></ci><ci id="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.2">𝑖</ci></list></apply></apply></apply><apply id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.1.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7">subscript</csymbol><ci id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.2.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.2">𝐹</ci><ci id="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.3.cmml" xref="S4.SS2.SSS0.Px2.p1.2.m2.5.5.7.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.6c">\mathrm{MNFD}=\frac{1}{N}\sum_{i=1}^{N}\frac{|f_{\text{retrieved},i}-f_{\text{%
target},i}|}{F_{i}},</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.2.m2.6d">roman_MNFD = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG | italic_f start_POSTSUBSCRIPT retrieved , italic_i end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT target , italic_i end_POSTSUBSCRIPT | end_ARG start_ARG italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math>
where <math alttext="f_{retrieved,i}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.3.m3.2"><semantics id="S4.SS2.SSS0.Px2.p1.3.m3.2a"><msub id="S4.SS2.SSS0.Px2.p1.3.m3.2.3" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.3.cmml"><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.3.2" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.3.2.cmml">f</mi><mrow id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.3.cmml"><mrow id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.cmml"><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.2" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.2.cmml">r</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.3" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.3.cmml">e</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1a" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.4" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.4.cmml">t</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1b" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.5" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.5.cmml">r</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1c" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.6" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.6.cmml">i</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1d" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.7" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.7.cmml">e</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1e" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.8" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.8.cmml">v</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1f" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.9" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.9.cmml">e</mi><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1g" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.10" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.10.cmml">d</mi></mrow><mo id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.2" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px2.p1.3.m3.1.1.1.1" xref="S4.SS2.SSS0.Px2.p1.3.m3.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.3.m3.2b"><apply id="S4.SS2.SSS0.Px2.p1.3.m3.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.3.m3.2.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.3">subscript</csymbol><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.3.2">𝑓</ci><list id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2"><apply id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1"><times id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.1"></times><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.2">𝑟</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.3">𝑒</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.4.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.4">𝑡</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.5.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.5">𝑟</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.6.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.6">𝑖</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.7.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.7">𝑒</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.8.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.8">𝑣</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.9.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.9">𝑒</ci><ci id="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.10.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.2.2.2.2.1.10">𝑑</ci></apply><ci id="S4.SS2.SSS0.Px2.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.3.m3.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.3.m3.2c">f_{retrieved,i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.3.m3.2d">italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r italic_i italic_e italic_v italic_e italic_d , italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{target,i}" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p1.4.m4.2"><semantics id="S4.SS2.SSS0.Px2.p1.4.m4.2a"><msub id="S4.SS2.SSS0.Px2.p1.4.m4.2.3" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.3.cmml"><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.3.2" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.3.2.cmml">f</mi><mrow id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.cmml"><mrow id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.cmml"><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.2" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.2.cmml">t</mi><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.3" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.3.cmml">a</mi><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1a" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.4" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.4.cmml">r</mi><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1b" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.5" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.5.cmml">g</mi><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1c" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.6" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.6.cmml">e</mi><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1d" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.7" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.7.cmml">t</mi></mrow><mo id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.2" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S4.SS2.SSS0.Px2.p1.4.m4.1.1.1.1" xref="S4.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.4.m4.2b"><apply id="S4.SS2.SSS0.Px2.p1.4.m4.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px2.p1.4.m4.2.3.1.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.3">subscript</csymbol><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.3.2.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.3.2">𝑓</ci><list id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2"><apply id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1"><times id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.1"></times><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.2.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.2">𝑡</ci><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.3.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.3">𝑎</ci><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.4.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.4">𝑟</ci><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.5.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.5">𝑔</ci><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.6.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.6">𝑒</ci><ci id="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.7.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.1.7">𝑡</ci></apply><ci id="S4.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.4.m4.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.4.m4.2c">f_{target,i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.4.m4.2d">italic_f start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t , italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the retrieved and target frame respectively.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">For the automatic evaluation of answer generation, we consider BERTScore(BS) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib61" title="">2020b</a>)</cite> and ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib28" title="">2004</a>)</cite>.
To measure the VSG performance, apart from ROUGE-L, we use Exact Match, which measures whether the target step is contained, or not, in the system’s response.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Protocol.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">Across all dialogue-based evaluations, we consider a context window of the 4 previous turns and pass the model the recipe steps along with the current step the user is on. The steps are included in the prompt in a numbered manner (eg, "Step 1 …, Step 2 …").
For CVMR, the model also sees the candidate system response and we extract the output embeddings for the position immediately after the generated <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.1.1">[RET]</span> token, if the model fails to generate the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.1.2">[RET]</span> token we use the output embeddings of the first generated token. We set <math alttext="N=2" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1"><eq id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.2">𝑁</ci><cn id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">N=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">italic_N = 2</annotation></semantics></math>.
For VSG and Answer Generation the model does not see any additional context. When evaluating a specific task, the model is not provided with any marker or information indicating the type of response wanted. For CVMR the candidate pool is composed of all of the frames of the recipe instructional video and the negative frames are the target frames for other samples in the same batch.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Baselines.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px4.p1.1">As a baseline, we compare our approach with FROMAGe on a zero-shot setting, as it was not fine-tuned for our domain specifically. We also compare against a random baseline that randomly retrieves a frame from the video for CVMR, and, for VSG, it randomly selects a plan step from the ones not yet completed by the user.
For textual requests, we compare against the PlanLLM model, with no further fine-tuning, to gauge performance variance on text-only dialogue turns.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Plan Grounding</h3>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Answer Gen.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">Plan-Navigation</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.2.2.1">ROUGE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T1.1.2.2.2">BS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.2.2.3">Explicit</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.2.2.4">Implicit</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.3.1.1">FROMAGe</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.2">29.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.3.1.3">63.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.4">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.3.1.5">—</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.4.2.1">PlanLLM</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.2.2.1">75.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.2.3.1">88.66</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.2.4.1">0.895</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.2.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.4.2.5.1">0.480</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.5.3.1">MM-PlanLLM</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.5.3.2">66.58</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.1.5.3.3">83.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.5.3.4">0.855</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.5.3.5">0.440</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Instructional plan following generation results, on automatic metrics. PlanLLM results as reported in  <cite class="ltx_cite ltx_citemacro_cite">Glória-Silva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib15" title="">2024</a>)</cite> </figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.2" rowspan="2"><span class="ltx_text" id="S5.T2.1.2.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T2.1.2.1.2.1.1">
<span class="ltx_tr" id="S5.T2.1.2.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.1.2.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.2.1.1.1.1.1">LLM Backbone</span></span></span>
<span class="ltx_tr" id="S5.T2.1.2.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.1.2.1.2.1.1.2.1">(# Params)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6" id="S5.T2.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.3.1">Conversational Video Moment Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T2.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.4.1">VSG</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1">
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.2">R@1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3">R@5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4">R@10</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.5">mAP</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6">Step Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.1.1">MNFD<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.7">Ex. Match</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.8">ROUGE</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.3.2.1">Random</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.3.2.2">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.3">1.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.4">8.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.5">17.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.6">7.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.7">16.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.2.8">32.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.9">28.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.10">37.51</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.4.3.1">FROMAGe</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.1.4.3.2">OPT (7B)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.3">3.08</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.4">11.75</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.5">22.76</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.6">10.17</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.7">25.09</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.4.3.8">26.11</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.9">0.34</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.10">7.31</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T2.1.5.4.1">MM-PlanLLM</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T2.1.5.4.2">Llama2 (7B)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.3.1">5.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.4.1">38.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.5.1">53.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.6.1">21.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.7.1">54.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.1.5.4.8"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.8.1">13.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.9"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.9.1">38.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.10"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.10.1">42.62</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results of our best-performing model MM-PlanLLM-Llama2, on multimodal tasks, against the baselines. For the CVMR and VSG tasks we used the TastyVidDial dataset.</figcaption>
</figure>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Plan-Grounded Answer Generation.</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">A key property of MM-PlanLLM is its strong plan-following capabilities.
To assess this, we evaluated MM-PlanLLM on text-only dialogues, mirroring the main evaluation setting of PlanLLM.
We utilized the original PlanLLM dataset, which exclusively comprises text-based conversations.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.T1" title="Table 1 ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a>, MM-PlanLLM achieves a BERTScore of 83.28, approximately 94% of PlanLLM’s performance (88.66), on answer generation in a text-only plan-grounded setting.
In contrast, FROMAGe demonstrates notably weaker performance in this setting.
To understand if this performance differential also reflects on MM-PlanLLM’s ability to guide users through tasks, we replicated the GPT-4-based Plan Navigation evaluation from <cite class="ltx_cite ltx_citemacro_citet">Glória-Silva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib15" title="">2024</a>)</cite>. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.T1" title="Table 1 ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">1</span></a> indicate that MM-PlanLLM remains competitive on this task having 85.5 accuracy on explicit navigational requests, a 4.0 accuracy loss over PlanLLM, reinforcing that it retained the ability to effectively follow instructional plans.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Conversational Video Moment Retrieval.</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">To assess CMVR performance, we evaluated MM-PlanLLM on all video moment retrieval requests within the TastyVidDial test set. We benchmarked MM-PlanLLM against FROMAGe, which is capable of general conversational image retrieval, and the random baseline (described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S4.SS1" title="4.1 Instructional Tasks Datasets ‣ 4 Experimental Setup ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">4.1</span></a>), to quantify the gains achieved through our task-specific training approach.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">The results shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.T2" title="Table 2 ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">2</span></a> highlight the efficacy of our focused training. MM-PlanLLM significantly outperforms FROMAGe across all metrics, demonstrating over 100% improvement in most cases. Whereas FROMAGe, in turn, demonstrates minimal improvement over the random baseline.
The performance gap between R@1 and R@5, coupled with a high Step Accuracy, suggests that while MM-PlanLLM consistently identifies the relevant video moment (evidenced by high Step Accuracy), the high visual similarity between adjacent frames within the same video moment proves a challenge for R@1 scores. This is explored in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.SS2" title="5.2 Multimodal Plan Alignment ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Visually-Informed Step Generation.</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">To evaluate MM-PlanLLM’s ability to interpret visual input and align it with instructional plans, we also evaluated solely in the VSG requests.
Results for this task are shown in the second column group of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.T2" title="Table 2 ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">2</span></a>. There is a stark contrast in performance between MM-PlanLLM and FROMAGe, with the latter rarely preserving the step text verbatim. Conversely, MM-PlanLLM achieves an Exact Match score of 38%.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="592" id="S5.F3.g1" src="x3.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Text-query to visual plan alignment. MM-PlanLLM effectively learns to align textual <span class="ltx_text ltx_font_typewriter" id="S5.F3.2.1">[RET]</span> token representations with that of the target step frames. We remove outliers for clarity.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="592" id="S5.F4.g1" src="x4.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Image-query to text plan alignment. Most similar plan step to the provided visual input, as measured by BS using the generated answer.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="6" id="S5.T3.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.1.2.1">Conversational Video Moment Retrieval</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T3.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.1.3.1">VSG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.1.4.1">Answer Gen.</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.3">R@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.4">R@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.5">R@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.6">mAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.7">Step Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1">MNFD<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.8">Ex. Match</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.9">ROUGE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.10">ROUGE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.1.1.11">BS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.3.1.1">LLaMa2 - Phase 1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.2">2.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.3">12.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.4">19.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.5">9.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.6">16.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.1.7">29.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.8">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.3.1.9">6.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.10">31.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.11">63.44</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.4.2.1">+ Phase 2</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.2">3.45</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.3">15.21</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.4">23.04</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.5">10.68</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.6">16.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.4.2.7">29.46</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.8">7.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.4.2.9">18.87</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.10">23.68</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.11">55.30</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.5.3.1">+ Phase 3</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.2">6.72</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.3">35.26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.4">48.69</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.5">20.80</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.6">52.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.3.7">14.03</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.8">37.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.3.9"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.3.9.1">42.84</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.10">66.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.11">83.03</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.6.4.1">   + Adj. Frames</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.6.4.2.1">7.46</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.3">30.50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.4">48.60</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.5">20.43</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.6">52.05</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.6.4.7">14.14</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.8">34.58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.6.4.9">40.59</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.10">65.78</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.4.11">82.95</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.1.7.5.1">   + Pos. Embs.</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.2">5.50</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.3.1">38.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.4.1">53.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.5.1">21.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.6.1">54.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.7.1">13.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.8.1">38.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.7.5.9">42.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.10"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.10.1">66.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.7.5.11"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.5.11.1">83.28</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Impact of the several training stages on model performance on the three main tasks.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Multimodal Plan Alignment</h3>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text to Visual Plan.</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.F3" title="Figure 3 ‣ Visually-Informed Step Generation. ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">3</span></a> we plot the average similarity between the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px1.p1.1.1">[RET]</span> token and all frames of each plan step in the plan video, ordered by their absolute distance to the target step.
The results demonstrate that MM-PlanLLM effectively learns to produce a representation that aligns closely with the video moment relevant to the target step. This is supported by the significantly higher similarity scores observed for frames within the target step (distance 0) compared to frames from other steps. Additionally, the gradual decline in similarity as the distance from the target step increases, further confirms the model’s ability to discriminate between relevant and irrelevant video moment frames based on the textual plan.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Image to Text Plan.</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">To assess MM-PlanLLM’s ability to align visual representations of steps with the corresponding textual descriptions, we used BERTScore to measure the similarity between generated answers and plan steps in the VSG task. Then, for each VSG instance, we identified the plan step with the highest BERTScore similarity to the generated answer and plotted its distance from the actual target step in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.F4" title="Figure 4 ‣ Visually-Informed Step Generation. ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p2.1">The distribution in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.F4" title="Figure 4 ‣ Visually-Informed Step Generation. ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">4</span></a> reinforces that MM-PlanLLM demonstrates a substantial capacity for aligning visual input with the corresponding textual step, achieving a success rate of 44.8% on the test set.
Moreover, 30.3% generated answers are most similar to steps immediately preceding or following the target step, highlighting the model’s ability to capture the sequential nature of instructions and identify steps closely related to the visual input.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conducted ablation studies to investigate the impact of each training stage and architectural choices on model performance across all tasks.</p>
</div>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Training Stages.</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">To train MM-PlanLLM, we devised a multi-stage approach to maximize performance gains and minimize catastrophic forgetting. This analysis can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5.T3" title="Table 3 ‣ Visually-Informed Step Generation. ‣ 5.1 Plan Grounding ‣ 5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">3</span></a>.
These results show that each of the three training stages contributed incrementally to improving the targeted capabilities. Stage 1, which focused on general image understanding, established a foundation for the model to outperform the random baseline in CVMR. Stage 2, which aimed to instill domain-specific multimodal understanding, further enhanced performance on both CVMR and Step Generation tasks, even before explicitly training on these tasks. Finally, Stage 3, where we integrated conversational abilities, led to substantial improvements across all three tasks, highlighting the importance of end-to-end task-specific training.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p2.1">Within the last stage, we also report the improvements provided both by the usage of adjacent frames as candidates and usage of positional embeddings for CVMR training.
Surprisingly, utilizing multiple candidate frames for CVMR training yielded minimal benefits. We hypothesize that this is due to the high similarity between consecutive frames in video moments. However, the addition of positional embeddings, which incorporate temporal information, significantly improved performance across the board, underscoring the model’s ability to leverage this additional context.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">LLM Backbone.</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">To understand how different LLM Backbones affect model performance we evaluated 8 LLMs on CVMR, VSG, and PGAG. The evaluated models ranged in size from 1.8B (Qwen-1.5 <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib2" title="">2023</a>)</cite>) to 7B parameters (Vicuna1.5 <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib7" title="">2023</a>)</cite>). We consider PlanLLM as a backbone but skip the text-only data training in Stage 3.
On the CVMR evaluation Llama2 achieves the best performance across most metrics, with the exception of R@1 that is led by PlanLLM with 6.16 R@1. For VSG, PlanLLM exhibited a substantial lead over Vicuna in Exact Match (42.76 vs. 40.37), while Mistral <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib21" title="">2023a</a>)</cite> achieved the highest ROUGE score (46.60).</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p2.1">In the answer generation task, most models demonstrated similar performance. Our approach generalizes well for smaller LLMs, such as Qwen-1.5 (1.8B Param.) and Phi-2 (2.7B Param.) which achieved, on average, 88% and 95% of Llama2’s performance, respectively.
The complete results are shown in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A2" title="Appendix B LM Backbone Ablation ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We propose MM-PlanLLM, a multimodal architecture that enables multimodal comprehension for LMs in plan-grounded conversational settings.
We follow a multistage training paradigm, coupled with task-specific synthetic data creation, that enables the model to slowly acquire the necessary abilities to understand multimodal input and generate multimodal outputs.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Experimental results demonstrates that MM-PlanLLM outperforms task-specific baselines, showcasing minimal performance loss in text-only dialogues, while being capable of aligning textual steps with video moments and user images with the plan steps. The ablation study further highlights the effectiveness of the multi-phase training methodology and the value of incorporating temporal information.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While MM-PlanLLM addresses two key multimodal request types (CVMR and VSG) crucial for plan-grounded dialogue, we acknowledge that a complete system would need broader multimodal support, including visual question answering.
Furthermore, long-term dialogue dependencies remain a challenge due to the limited context window of 4 turns during training (limited by the available hardware), hindering the model’s ability to effectively recall and utilize information from earlier turns in the conversation. This limitation may impact the model’s performance in extended interactions where maintaining context is essential.
We plan to address these limitations in future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported by the FCT Ph.D. scholarship grant Ref. PRT/BD/152810/2021 awarded by CMU Portugal Affiliated Ph.D. program, and by the FCT project NOVA LINCS Ref. (UIDB/04516/2020). Data collection was possible under the Alexa Prize Taskbot Challenge organized by Amazon Science.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf" title="">Flamingo: a visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pages 23716–23736. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.16609" title="">Qwen technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020a)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020b)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS’20, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-58452-8_13" title="">End-to-end object detection with transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I</em>, volume 12346 of <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Lecture Notes in Computer Science</em>, pages 213–229. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/pdf?id=mWVoBz4W0u" title="">Pali: A jointly-scaled multilingual language-image model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2022)</span>
<span class="ltx_bibblock">
Jason Ingyu Choi, Saar Kuzi, Nikhita Vedula, Jie Zhao, Giuseppe Castellucci, Marcus Collins, Shervin Malmasi, Oleg Rokhlenko, and Eugene Agichtein. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.coling-1.310" title="">Wizard of tasks: A novel conversational dataset for solving real-world tasks in conversational settings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022</em>, pages 3514–3529. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html" title="">Instructblip: Towards general-purpose vision-language models with instruction tuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diwan et al. (2022)</span>
<span class="ltx_bibblock">
Anuj Diwan, Puyuan Peng, and Raymond J Mooney. 2022.

</span>
<span class="ltx_bibblock">Zero-shot video moment retrieval with off-the-shelf models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">TL4NLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=YicbFdNTTy" title="">An image is worth 16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2023)</span>
<span class="ltx_bibblock">
Xiang Fang, Daizong Liu, Pan Zhou, and Yuchong Hu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TMM.2022.3222965" title="">Multi-modal cross-domain alignment network for video moment retrieval</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Multimedia</em>, 25:7517–7532.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao and Xu (2021)</span>
<span class="ltx_bibblock">
Junyu Gao and Changsheng Xu. 2021.

</span>
<span class="ltx_bibblock">Fast video moment retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 1503–1512. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glória-Silva et al. (2024)</span>
<span class="ltx_bibblock">
Diogo Glória-Silva, Rafael Ferreira, Diogo Tavares, David Semedo, and Joao Magalhaes. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.eacl-long.77" title="">Plan-grounded large language models for dual goal conversational settings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1271–1292, St. Julian’s, Malta. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini-Asl et al. (2020)</span>
<span class="ltx_bibblock">
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/e946209592563be0f01c844ab2170f0c-Abstract.html" title="">A simple language model for task-oriented dialogue</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaegle et al. (2021)</span>
<span class="ltx_bibblock">
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and João Carreira. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://proceedings.mlr.press/v139/jaegle21a.html" title="">Perceiver: General perception with iterative attention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">Proceedings of Machine Learning Research</em>, pages 4651–4664. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. (2023)</span>
<span class="ltx_bibblock">
Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, and Kwanghoon Sohn. 2023.

</span>
<span class="ltx_bibblock">Knowing where to focus: Event-aware transformer for video grounding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 13846–13856.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Wei Ji, Renjie Liang, Lizi Liao, Hao Fei, and Fuli Feng. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3581783.3612088" title="">Partial annotation-based video moment retrieval via iterative learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 31st ACM International Conference on Multimedia</em>, MM ’23, page 4330–4339, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.04088" title="">Mixtral of experts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Xun Jiang, Zailei Zhou, Xing Xu, Yang Yang, Guoqing Wang, and Heng Tao Shen. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3581783.3612394" title="">Faster video moment retrieval with point-level supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 31st ACM International Conference on Multimedia</em>, MM ’23, page 1334–1342, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh et al. (2023)</span>
<span class="ltx_bibblock">
Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/koh23a.html" title="">Grounding language models to images for multimodal inputs and outputs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Proceedings of Machine Learning Research</em>, pages 17283–17300. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2021a)</span>
<span class="ltx_bibblock">
Jie Lei, Tamara L Berg, and Mohit Bansal. 2021a.

</span>
<span class="ltx_bibblock">Detecting moments and highlights in videos via natural language queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</em>, 34:11846–11858.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2021b)</span>
<span class="ltx_bibblock">
Jie Lei, Tamara L. Berg, and Mohit Bansal. 2021b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:263868665" title="">Detecting moments and highlights in videos via natural language queries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/li23q.html" title="">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">Proceedings of Machine Learning Research</em>, pages 19730–19742. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:964287" title="">Rouge: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024.

</span>
<span class="ltx_bibblock">World model on Million-Length video and language with RingAttention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv [cs.LG]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.08485" title="">Visual instruction tuning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018)</span>
<span class="ltx_bibblock">
Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan Chen, and Tat-Seng Chua. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3240508.3240549" title="">Cross-modal moment localization in videos</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 26th ACM International Conference on Multimedia</em>, MM ’18, page 843–851, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled weight decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marin et al. (2019)</span>
<span class="ltx_bibblock">
Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2019.

</span>
<span class="ltx_bibblock">Recipe1m+: A dataset for learning cross-modal embeddings for cooking recipes and food images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2022)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-LONG.244" title="">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3470–3487. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2023)</span>
<span class="ltx_bibblock">
WonJun Moon, Sangeek Hyun, Sang shin Paldal-gu Suwon-city Park, Dongchan Park, and Jae-Pil Heo. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:257757326" title="">Query - dependent video representation for moment retrieval and highlight detection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 23023–23033.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://proceedings.mlr.press/v139/radford21a.html" title="">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:160025533" title="">Language models are unsupervised multitask learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sener and Yao (2019)</span>
<span class="ltx_bibblock">
Fadime Sener and Angela Yao. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV.2019.00095" title="">Zero-shot anticipation for instructional activities</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 862–871.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of ACL</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023)</span>
<span class="ltx_bibblock">
Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14705" title="">Mixture-of-experts meets instruction tuning:a winning combination for large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.09864" title="">Roformer: Enhanced transformer with rotary position embedding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Hao Sun, Mingyao Zhou, Wenjing Chen, and Wei Xie. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v38i5.28304" title="">Tr-detr: Task-reciprocal transformer for joint moment retrieval and highlight detection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(5):4998–5007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.14525" title="">Aligning large multimodal models with factually augmented RLHF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu,
Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson,
Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo
Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren
Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen,
Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby
Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent,
Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht,
Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, Rémi
Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer
Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan
Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto,
Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han
Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2024)</span>
<span class="ltx_bibblock">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, and et al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.34740/KAGGLE/M/3301" title="">Gemma</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al. (2018)</span>
<span class="ltx_bibblock">
Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1807.03748" title="">Representation learning with contrastive predictive coding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">CoRR</em>, abs/1807.03748.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Guolong Wang, Xun Wu, Zhaoyuan Liu, and Junchi Yan. 2022.

</span>
<span class="ltx_bibblock">Prompt-based zero-shot video moment retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 30th ACM International Conference on Multimedia</em>, MM ’22, pages 413–421, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.03079" title="">Cogvlm: Visual expert for pretrained language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">CoRR</em>, abs/2311.03079.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Yunxiao Wang, Meng Liu, Yinwei Wei, Zhiyong Cheng, Yinglong Wang, and Liqiang Nie. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TMM.2022.3168424" title="">Siamese alignment network for weakly supervised video moment retrieval</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">IEEE Transactions on Multimedia</em>, 25:3921–3933.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Yifang Xu, Yunzhuo Sun, Yang Li, Yilei Shi, Xiaoxia Zhu, and Sidan Du. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258427080" title="">Mh-detr: Video moment and highlight detection with cross-modal transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ArXiv</em>, abs/2305.00355.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023a)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.14178" title="">mPLUG-Owl: Modularization empowers large language models with multimodality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023b)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.04257" title="">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2018)</span>
<span class="ltx_bibblock">
Yitian Yuan, Tao Mei, and Wenwu Zhu. 2018.

</span>
<span class="ltx_bibblock">To find where you talk: Temporal sentence localization in video with attention based location regression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020a)</span>
<span class="ltx_bibblock">
Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.585" title="">Span-based localizing network for natural language video localization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 6543–6554, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:248496292" title="">Opt: Open pre-trained transformer language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">ArXiv</em>, abs/2205.01068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020b)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">Bertscore: Evaluating text generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020c)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020c.

</span>
<span class="ltx_bibblock">Dialogpt: Large-scale generative pre-training for conversational response generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">ACL, system demonstration</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.6">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A1.T4" title="Table 4 ‣ Appendix A Implementation Details ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">4</span></a> details some of the hyperparameters used.
Each model is trained for 10k, 5k, and 2k steps for each phase, using a batch size of 64 (and 16 on multimodal batches in phase 3) on a single A100 40GB GPU. For optimization, we use the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib32" title="">2019</a>)</cite> with <math alttext="\beta 1=0.9" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mrow id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mi id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">β</mi><mo id="A1.p1.1.m1.1.1.2.1" xref="A1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mn id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3.cmml">1</mn></mrow><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><eq id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></eq><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><times id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2.1"></times><ci id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">𝛽</ci><cn id="A1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.2.3">1</cn></apply><cn id="A1.p1.1.m1.1.1.3.cmml" type="float" xref="A1.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\beta 1=0.9</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_β 1 = 0.9</annotation></semantics></math>, <math alttext="\beta 2=0.95" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mrow id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml"><mi id="A1.p1.2.m2.1.1.2.2" xref="A1.p1.2.m2.1.1.2.2.cmml">β</mi><mo id="A1.p1.2.m2.1.1.2.1" xref="A1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mn id="A1.p1.2.m2.1.1.2.3" xref="A1.p1.2.m2.1.1.2.3.cmml">2</mn></mrow><mo id="A1.p1.2.m2.1.1.1" xref="A1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><eq id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></eq><apply id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2"><times id="A1.p1.2.m2.1.1.2.1.cmml" xref="A1.p1.2.m2.1.1.2.1"></times><ci id="A1.p1.2.m2.1.1.2.2.cmml" xref="A1.p1.2.m2.1.1.2.2">𝛽</ci><cn id="A1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="A1.p1.2.m2.1.1.2.3">2</cn></apply><cn id="A1.p1.2.m2.1.1.3.cmml" type="float" xref="A1.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\beta 2=0.95</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">italic_β 2 = 0.95</annotation></semantics></math>, and <math alttext="\epsilon=1*10^{-5}" class="ltx_Math" display="inline" id="A1.p1.3.m3.1"><semantics id="A1.p1.3.m3.1a"><mrow id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml"><mi id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2.cmml">ϵ</mi><mo id="A1.p1.3.m3.1.1.1" xref="A1.p1.3.m3.1.1.1.cmml">=</mo><mrow id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3.cmml"><mn id="A1.p1.3.m3.1.1.3.2" xref="A1.p1.3.m3.1.1.3.2.cmml">1</mn><mo id="A1.p1.3.m3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.3.m3.1.1.3.1.cmml">∗</mo><msup id="A1.p1.3.m3.1.1.3.3" xref="A1.p1.3.m3.1.1.3.3.cmml"><mn id="A1.p1.3.m3.1.1.3.3.2" xref="A1.p1.3.m3.1.1.3.3.2.cmml">10</mn><mrow id="A1.p1.3.m3.1.1.3.3.3" xref="A1.p1.3.m3.1.1.3.3.3.cmml"><mo id="A1.p1.3.m3.1.1.3.3.3a" xref="A1.p1.3.m3.1.1.3.3.3.cmml">−</mo><mn id="A1.p1.3.m3.1.1.3.3.3.2" xref="A1.p1.3.m3.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1"><eq id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1.1"></eq><ci id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2">italic-ϵ</ci><apply id="A1.p1.3.m3.1.1.3.cmml" xref="A1.p1.3.m3.1.1.3"><times id="A1.p1.3.m3.1.1.3.1.cmml" xref="A1.p1.3.m3.1.1.3.1"></times><cn id="A1.p1.3.m3.1.1.3.2.cmml" type="integer" xref="A1.p1.3.m3.1.1.3.2">1</cn><apply id="A1.p1.3.m3.1.1.3.3.cmml" xref="A1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="A1.p1.3.m3.1.1.3.3.1.cmml" xref="A1.p1.3.m3.1.1.3.3">superscript</csymbol><cn id="A1.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="A1.p1.3.m3.1.1.3.3.2">10</cn><apply id="A1.p1.3.m3.1.1.3.3.3.cmml" xref="A1.p1.3.m3.1.1.3.3.3"><minus id="A1.p1.3.m3.1.1.3.3.3.1.cmml" xref="A1.p1.3.m3.1.1.3.3.3"></minus><cn id="A1.p1.3.m3.1.1.3.3.3.2.cmml" type="integer" xref="A1.p1.3.m3.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">\epsilon=1*10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.3.m3.1d">italic_ϵ = 1 ∗ 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> for all runs. We used a constant learning rate of <math alttext="1*10^{-5}" class="ltx_Math" display="inline" id="A1.p1.4.m4.1"><semantics id="A1.p1.4.m4.1a"><mrow id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml"><mn id="A1.p1.4.m4.1.1.2" xref="A1.p1.4.m4.1.1.2.cmml">1</mn><mo id="A1.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.p1.4.m4.1.1.1.cmml">∗</mo><msup id="A1.p1.4.m4.1.1.3" xref="A1.p1.4.m4.1.1.3.cmml"><mn id="A1.p1.4.m4.1.1.3.2" xref="A1.p1.4.m4.1.1.3.2.cmml">10</mn><mrow id="A1.p1.4.m4.1.1.3.3" xref="A1.p1.4.m4.1.1.3.3.cmml"><mo id="A1.p1.4.m4.1.1.3.3a" xref="A1.p1.4.m4.1.1.3.3.cmml">−</mo><mn id="A1.p1.4.m4.1.1.3.3.2" xref="A1.p1.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><apply id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1"><times id="A1.p1.4.m4.1.1.1.cmml" xref="A1.p1.4.m4.1.1.1"></times><cn id="A1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.p1.4.m4.1.1.2">1</cn><apply id="A1.p1.4.m4.1.1.3.cmml" xref="A1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A1.p1.4.m4.1.1.3.1.cmml" xref="A1.p1.4.m4.1.1.3">superscript</csymbol><cn id="A1.p1.4.m4.1.1.3.2.cmml" type="integer" xref="A1.p1.4.m4.1.1.3.2">10</cn><apply id="A1.p1.4.m4.1.1.3.3.cmml" xref="A1.p1.4.m4.1.1.3.3"><minus id="A1.p1.4.m4.1.1.3.3.1.cmml" xref="A1.p1.4.m4.1.1.3.3"></minus><cn id="A1.p1.4.m4.1.1.3.3.2.cmml" type="integer" xref="A1.p1.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">1*10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.4.m4.1d">1 ∗ 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, for the third stage, with no warmup steps. All images are resized to fit a 224x224 image resolution. For phase 3, text-only training was separated from the multimodal training with the first 1k steps being text-only and the later 1k being multimodal.
The visual encoder used was CLIP ViT-L/14 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib36" title="">2021</a>)</cite>, the retrieval embedding dimension was set to 512, and the embedding dimension was kept the same as the LM Backbone so it varied from model to model.
For the 3rd phase we use LoRa <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib17" title="">2022</a>)</cite>, when training the LM Backbone, with a <math alttext="r=4" class="ltx_Math" display="inline" id="A1.p1.5.m5.1"><semantics id="A1.p1.5.m5.1a"><mrow id="A1.p1.5.m5.1.1" xref="A1.p1.5.m5.1.1.cmml"><mi id="A1.p1.5.m5.1.1.2" xref="A1.p1.5.m5.1.1.2.cmml">r</mi><mo id="A1.p1.5.m5.1.1.1" xref="A1.p1.5.m5.1.1.1.cmml">=</mo><mn id="A1.p1.5.m5.1.1.3" xref="A1.p1.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><apply id="A1.p1.5.m5.1.1.cmml" xref="A1.p1.5.m5.1.1"><eq id="A1.p1.5.m5.1.1.1.cmml" xref="A1.p1.5.m5.1.1.1"></eq><ci id="A1.p1.5.m5.1.1.2.cmml" xref="A1.p1.5.m5.1.1.2">𝑟</ci><cn id="A1.p1.5.m5.1.1.3.cmml" type="integer" xref="A1.p1.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">r=4</annotation><annotation encoding="application/x-llamapun" id="A1.p1.5.m5.1d">italic_r = 4</annotation></semantics></math> and <math alttext="\alpha=8" class="ltx_Math" display="inline" id="A1.p1.6.m6.1"><semantics id="A1.p1.6.m6.1a"><mrow id="A1.p1.6.m6.1.1" xref="A1.p1.6.m6.1.1.cmml"><mi id="A1.p1.6.m6.1.1.2" xref="A1.p1.6.m6.1.1.2.cmml">α</mi><mo id="A1.p1.6.m6.1.1.1" xref="A1.p1.6.m6.1.1.1.cmml">=</mo><mn id="A1.p1.6.m6.1.1.3" xref="A1.p1.6.m6.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.6.m6.1b"><apply id="A1.p1.6.m6.1.1.cmml" xref="A1.p1.6.m6.1.1"><eq id="A1.p1.6.m6.1.1.1.cmml" xref="A1.p1.6.m6.1.1.1"></eq><ci id="A1.p1.6.m6.1.1.2.cmml" xref="A1.p1.6.m6.1.1.2">𝛼</ci><cn id="A1.p1.6.m6.1.1.3.cmml" type="integer" xref="A1.p1.6.m6.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.6.m6.1c">\alpha=8</annotation><annotation encoding="application/x-llamapun" id="A1.p1.6.m6.1d">italic_α = 8</annotation></semantics></math> to reduce memory requirements.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For BERTScore calculations we utilize <span class="ltx_text ltx_font_typewriter" id="A1.p2.1.1">microsoft/deberta-xlarge-mnli</span>.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T4.4" style="width:433.6pt;height:543.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(94.8pt,-118.9pt) scale(1.77692577431907,1.77692577431907) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T4.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T4.4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.4.4.5.1.1.1">Stage</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.4.4.5.1.2.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.4.4.5.1.3.1">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.4.4.5.1.4"><span class="ltx_text ltx_font_bold" id="A1.T4.4.4.5.1.4.1">3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.4.4.6.2.1">Batch Size</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.6.2.2">64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.6.2.3">48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.6.2.4">1/4</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.7.3.1">Grad. Acc.</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.7.3.2">64</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.7.3.3">1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.7.3.4">64/4</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.8.4.1">Train Steps</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.8.4.2">10000</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.8.4.3">5000</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.8.4.4">2000</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.9.5.1">Val. Freq.</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.9.5.2">1000</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.9.5.3">1000</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.9.5.4">500</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.10.6.1">GPU #</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.10.6.2">1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.10.6.3">1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.10.6.4">1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.11.7.1">Seq. Max Len.</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.11.7.2">24</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.11.7.3">45</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.11.7.4">800</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.12.8.1">DType</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.12.8.2">BF16</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.12.8.3">BF16</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.12.8.4">BF16</td>
</tr>
<tr class="ltx_tr" id="A1.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.3.3.3.4">Learning Rate</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.1.1"><math alttext="5*10^{-4}" class="ltx_Math" display="inline" id="A1.T4.1.1.1.1.m1.1"><semantics id="A1.T4.1.1.1.1.m1.1a"><mrow id="A1.T4.1.1.1.1.m1.1.1" xref="A1.T4.1.1.1.1.m1.1.1.cmml"><mn id="A1.T4.1.1.1.1.m1.1.1.2" xref="A1.T4.1.1.1.1.m1.1.1.2.cmml">5</mn><mo id="A1.T4.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.T4.1.1.1.1.m1.1.1.1.cmml">∗</mo><msup id="A1.T4.1.1.1.1.m1.1.1.3" xref="A1.T4.1.1.1.1.m1.1.1.3.cmml"><mn id="A1.T4.1.1.1.1.m1.1.1.3.2" xref="A1.T4.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A1.T4.1.1.1.1.m1.1.1.3.3" xref="A1.T4.1.1.1.1.m1.1.1.3.3.cmml"><mo id="A1.T4.1.1.1.1.m1.1.1.3.3a" xref="A1.T4.1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="A1.T4.1.1.1.1.m1.1.1.3.3.2" xref="A1.T4.1.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.T4.1.1.1.1.m1.1b"><apply id="A1.T4.1.1.1.1.m1.1.1.cmml" xref="A1.T4.1.1.1.1.m1.1.1"><times id="A1.T4.1.1.1.1.m1.1.1.1.cmml" xref="A1.T4.1.1.1.1.m1.1.1.1"></times><cn id="A1.T4.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="A1.T4.1.1.1.1.m1.1.1.2">5</cn><apply id="A1.T4.1.1.1.1.m1.1.1.3.cmml" xref="A1.T4.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.T4.1.1.1.1.m1.1.1.3.1.cmml" xref="A1.T4.1.1.1.1.m1.1.1.3">superscript</csymbol><cn id="A1.T4.1.1.1.1.m1.1.1.3.2.cmml" type="integer" xref="A1.T4.1.1.1.1.m1.1.1.3.2">10</cn><apply id="A1.T4.1.1.1.1.m1.1.1.3.3.cmml" xref="A1.T4.1.1.1.1.m1.1.1.3.3"><minus id="A1.T4.1.1.1.1.m1.1.1.3.3.1.cmml" xref="A1.T4.1.1.1.1.m1.1.1.3.3"></minus><cn id="A1.T4.1.1.1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A1.T4.1.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.1.1.1.1.m1.1c">5*10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.T4.1.1.1.1.m1.1d">5 ∗ 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.2.2.2.2"><math alttext="1*10^{-4}" class="ltx_Math" display="inline" id="A1.T4.2.2.2.2.m1.1"><semantics id="A1.T4.2.2.2.2.m1.1a"><mrow id="A1.T4.2.2.2.2.m1.1.1" xref="A1.T4.2.2.2.2.m1.1.1.cmml"><mn id="A1.T4.2.2.2.2.m1.1.1.2" xref="A1.T4.2.2.2.2.m1.1.1.2.cmml">1</mn><mo id="A1.T4.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.T4.2.2.2.2.m1.1.1.1.cmml">∗</mo><msup id="A1.T4.2.2.2.2.m1.1.1.3" xref="A1.T4.2.2.2.2.m1.1.1.3.cmml"><mn id="A1.T4.2.2.2.2.m1.1.1.3.2" xref="A1.T4.2.2.2.2.m1.1.1.3.2.cmml">10</mn><mrow id="A1.T4.2.2.2.2.m1.1.1.3.3" xref="A1.T4.2.2.2.2.m1.1.1.3.3.cmml"><mo id="A1.T4.2.2.2.2.m1.1.1.3.3a" xref="A1.T4.2.2.2.2.m1.1.1.3.3.cmml">−</mo><mn id="A1.T4.2.2.2.2.m1.1.1.3.3.2" xref="A1.T4.2.2.2.2.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.T4.2.2.2.2.m1.1b"><apply id="A1.T4.2.2.2.2.m1.1.1.cmml" xref="A1.T4.2.2.2.2.m1.1.1"><times id="A1.T4.2.2.2.2.m1.1.1.1.cmml" xref="A1.T4.2.2.2.2.m1.1.1.1"></times><cn id="A1.T4.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="A1.T4.2.2.2.2.m1.1.1.2">1</cn><apply id="A1.T4.2.2.2.2.m1.1.1.3.cmml" xref="A1.T4.2.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="A1.T4.2.2.2.2.m1.1.1.3.1.cmml" xref="A1.T4.2.2.2.2.m1.1.1.3">superscript</csymbol><cn id="A1.T4.2.2.2.2.m1.1.1.3.2.cmml" type="integer" xref="A1.T4.2.2.2.2.m1.1.1.3.2">10</cn><apply id="A1.T4.2.2.2.2.m1.1.1.3.3.cmml" xref="A1.T4.2.2.2.2.m1.1.1.3.3"><minus id="A1.T4.2.2.2.2.m1.1.1.3.3.1.cmml" xref="A1.T4.2.2.2.2.m1.1.1.3.3"></minus><cn id="A1.T4.2.2.2.2.m1.1.1.3.3.2.cmml" type="integer" xref="A1.T4.2.2.2.2.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.2.2.2.2.m1.1c">1*10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.T4.2.2.2.2.m1.1d">1 ∗ 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.3.3.3.3"><math alttext="5*10^{-4}" class="ltx_Math" display="inline" id="A1.T4.3.3.3.3.m1.1"><semantics id="A1.T4.3.3.3.3.m1.1a"><mrow id="A1.T4.3.3.3.3.m1.1.1" xref="A1.T4.3.3.3.3.m1.1.1.cmml"><mn id="A1.T4.3.3.3.3.m1.1.1.2" xref="A1.T4.3.3.3.3.m1.1.1.2.cmml">5</mn><mo id="A1.T4.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.T4.3.3.3.3.m1.1.1.1.cmml">∗</mo><msup id="A1.T4.3.3.3.3.m1.1.1.3" xref="A1.T4.3.3.3.3.m1.1.1.3.cmml"><mn id="A1.T4.3.3.3.3.m1.1.1.3.2" xref="A1.T4.3.3.3.3.m1.1.1.3.2.cmml">10</mn><mrow id="A1.T4.3.3.3.3.m1.1.1.3.3" xref="A1.T4.3.3.3.3.m1.1.1.3.3.cmml"><mo id="A1.T4.3.3.3.3.m1.1.1.3.3a" xref="A1.T4.3.3.3.3.m1.1.1.3.3.cmml">−</mo><mn id="A1.T4.3.3.3.3.m1.1.1.3.3.2" xref="A1.T4.3.3.3.3.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.T4.3.3.3.3.m1.1b"><apply id="A1.T4.3.3.3.3.m1.1.1.cmml" xref="A1.T4.3.3.3.3.m1.1.1"><times id="A1.T4.3.3.3.3.m1.1.1.1.cmml" xref="A1.T4.3.3.3.3.m1.1.1.1"></times><cn id="A1.T4.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="A1.T4.3.3.3.3.m1.1.1.2">5</cn><apply id="A1.T4.3.3.3.3.m1.1.1.3.cmml" xref="A1.T4.3.3.3.3.m1.1.1.3"><csymbol cd="ambiguous" id="A1.T4.3.3.3.3.m1.1.1.3.1.cmml" xref="A1.T4.3.3.3.3.m1.1.1.3">superscript</csymbol><cn id="A1.T4.3.3.3.3.m1.1.1.3.2.cmml" type="integer" xref="A1.T4.3.3.3.3.m1.1.1.3.2">10</cn><apply id="A1.T4.3.3.3.3.m1.1.1.3.3.cmml" xref="A1.T4.3.3.3.3.m1.1.1.3.3"><minus id="A1.T4.3.3.3.3.m1.1.1.3.3.1.cmml" xref="A1.T4.3.3.3.3.m1.1.1.3.3"></minus><cn id="A1.T4.3.3.3.3.m1.1.1.3.3.2.cmml" type="integer" xref="A1.T4.3.3.3.3.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.3.3.3.3.m1.1c">5*10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.T4.3.3.3.3.m1.1d">5 ∗ 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.13.9.1">Scheduler</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.13.9.2">Constant</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.13.9.3">Constant</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.13.9.4">Constant</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.14.10.1">Optimizer</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.14.10.2">AdamW</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.14.10.3">AdamW</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.14.10.4">AdamW</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.15.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.15.11.1">T. Emb. Dropout</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.15.11.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.15.11.3">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.15.11.4">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.16.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.16.12.1">Ret. Dimension</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.16.12.2">512</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.16.12.3">512</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.16.12.4">512</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.17.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.4.4.17.13.1">LoRa DType</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.17.13.2">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.17.13.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.4.4.17.13.4">16 bits</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.18.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.18.14.1">LoRa Rank</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.18.14.2">—</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.18.14.3">—</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.18.14.4">4</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.4.4.4.1">LoRa <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.T4.4.4.4.1.m1.1"><semantics id="A1.T4.4.4.4.1.m1.1a"><mi id="A1.T4.4.4.4.1.m1.1.1" xref="A1.T4.4.4.4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.T4.4.4.4.1.m1.1b"><ci id="A1.T4.4.4.4.1.m1.1.1.cmml" xref="A1.T4.4.4.4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T4.4.4.4.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.T4.4.4.4.1.m1.1d">italic_α</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.4.2">—</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.4.3">—</td>
<td class="ltx_td ltx_align_center" id="A1.T4.4.4.4.4">8</td>
</tr>
<tr class="ltx_tr" id="A1.T4.4.4.19.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T4.4.4.19.15.1">LoRa Dropout</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.4.4.19.15.2">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.4.4.19.15.3">—</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.4.4.19.15.4">0.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Hyperparameters used to train MM-PlanLLM models across all three stages.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>LM Backbone Ablation</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We consider a comprehensive array of language model backbones in order to assess their impact on the overall model and select the best-performing one. In particular, we consider <span class="ltx_text ltx_font_bold" id="A2.p1.1.1">Qwen-1.5</span> <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib2" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.2">Phi-2</span>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.3">Gemma2b</span> <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib47" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.4">Mistral-v0.1</span> <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib21" title="">2023a</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.5">OPT</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib60" title="">2022</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.6">PlanLLM</span> <cite class="ltx_cite ltx_citemacro_cite">Glória-Silva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib15" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A2.p1.1.7">LLama2</span> <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib49" title="">2023b</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="A2.p1.1.8">Vicuna-7B</span> <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib7" title="">2023</a>)</cite>. As such, we cover LM backbones of different sizes, pre-training, and fine-tuning schemes.</p>
</div>
<figure class="ltx_table" id="A2.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A2.T5.1.2.1.1" rowspan="2"><span class="ltx_text" id="A2.T5.1.2.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="A2.T5.1.2.1.1.1.1">
<span class="ltx_tr" id="A2.T5.1.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T5.1.2.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T5.1.2.1.1.1.1.1.1.1">LM Backbone</span></span></span>
<span class="ltx_tr" id="A2.T5.1.2.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A2.T5.1.2.1.1.1.1.2.1">(# Params)</span></span>
</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6" id="A2.T5.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A2.T5.1.2.1.2.1">Conversational Video Moment Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A2.T5.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A2.T5.1.2.1.3.1">VSG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A2.T5.1.2.1.4"><span class="ltx_text ltx_font_bold" id="A2.T5.1.2.1.4.1">Answer Gen.</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1">
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.2">R@1</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.3">R@5</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.4">R@10</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.5">mAP</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.6">Step Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.1.1">MNFD<math alttext="\downarrow" class="ltx_Math" display="inline" id="A2.T5.1.1.1.m1.1"><semantics id="A2.T5.1.1.1.m1.1a"><mo id="A2.T5.1.1.1.m1.1.1" stretchy="false" xref="A2.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A2.T5.1.1.1.m1.1b"><ci id="A2.T5.1.1.1.m1.1.1.cmml" xref="A2.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.7">Ex. Match</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.1.8">ROUGE</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.9">ROUGE</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.1.10">BS</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.1.3.2.1">Qwen-1.5 (1.8B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.2">4.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.3">27.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.4">46.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.5">17.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.6">44.68</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.3.2.7">15.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.8">39.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.1.3.2.9">44.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.10">64.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.3.2.11">81.63</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.4.3.1">Gemma (2.5B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.2">3.08</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.3">24.44</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.4">47.39</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.5">15.69</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.6">46.46</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.4.3.7">15.45</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.8">37.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.4.3.9">42.30</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.10">63.33</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.4.3.11">82.07</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.5.4.1">Phi-2 (2.7B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.2">6.06</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.3">31.53</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.4">52.05</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.5">18.69</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.6">53.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.5.4.7">13.73</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.8">39.18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.5.4.9">43.44</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.10">54.35</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.5.4.11">77.97</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.6.5.1">OPT (7B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.2">4.48</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.3">31.44</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.4">52.61</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.5">18.80</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.6">50.84</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.6.5.7">15.34</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.8">35.78</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.6.5.9">43.15</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.10">38.52</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.6.5.11">70.65</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.7.6.1">Mistral-v0.1 (7B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.2">5.69</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.3">33.40</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.4">50.56</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.5">19.70</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.6">47.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.7.6.7">14.51</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.8">39.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.7.6.9"><span class="ltx_text ltx_font_bold" id="A2.T5.1.7.6.9.1">46.60</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.10">61.47</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.7.6.11">80.81</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.8.7.1">PlanLLM (7B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.2"><span class="ltx_text ltx_font_bold" id="A2.T5.1.8.7.2.1">6.16</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.3">33.30</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.4">52.33</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.5">20.20</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.6">44.68</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.8.7.7">14.06</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.8"><span class="ltx_text ltx_font_bold" id="A2.T5.1.8.7.8.1">42.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.8.7.9">44.67</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.10">66.64</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.8.7.11">83.13</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.1.9.8.1">Llama2 (7B)</th>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.2">5.50</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.3"><span class="ltx_text ltx_font_bold" id="A2.T5.1.9.8.3.1">38.53</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.4"><span class="ltx_text ltx_font_bold" id="A2.T5.1.9.8.4.1">53.82</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.5"><span class="ltx_text ltx_font_bold" id="A2.T5.1.9.8.5.1">21.52</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.6"><span class="ltx_text ltx_font_bold" id="A2.T5.1.9.8.6.1">54.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.9.8.7"><span class="ltx_text ltx_font_bold" id="A2.T5.1.9.8.7.1">13.26</span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.8">38.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.1.9.8.9">42.62</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.10">66.58</td>
<td class="ltx_td ltx_align_center" id="A2.T5.1.9.8.11">83.28</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A2.T5.1.10.9.1">Vicuna1.5 (7B)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.2">6.06</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.3">32.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.4">50.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.5">20.02</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.6">53.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T5.1.10.9.7">13.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.8">40.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T5.1.10.9.9">43.38</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.10"><span class="ltx_text ltx_font_bold" id="A2.T5.1.10.9.10.1">68.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.1.10.9.11"><span class="ltx_text ltx_font_bold" id="A2.T5.1.10.9.11.1">84.05</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Evaluation results of different LM Backbones for multimodal requests in the TastyVidDial dataset.</figcaption>
</figure>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">While we report our main evaluation results in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#S5" title="5 Results and Discussion ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">5</span></a> using Llama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib49" title="">2023b</a>)</cite> as the LM backbone, we trained a total of 8 models by varying the LM backbone. This sought to not only assert which was the best-performing model but also understand the impact of scaling the LM backbone on all tasks.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">The results from this analysis, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A2.T5" title="Table 5 ‣ Appendix B LM Backbone Ablation ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">5</span></a>, show a surprisingly low performance differential between models for all three tasks, with the only clear outlier being OPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib60" title="">2022</a>)</cite> on the answer generation task.
For Conversational Video Moment Retrieval we see a clear lead for Llama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib49" title="">2023b</a>)</cite> for most metrics, particularly for R@5, and a close second for Step Accuracy.
For Step Generation, PlanLLM <cite class="ltx_cite ltx_citemacro_cite">Glória-Silva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#bib.bib15" title="">2024</a>)</cite> outperforms the other models on Exact Match whereas Mistral holds a small lead on BertScore. On this task Llama2 underperforms indicating that there might be a performance tradeoff between this task and the previous.
For Answer Generation Vicuna performs the best likely due to its pretrain in a conversational setting, despite this both Llama2 and PlanLLM also perform closely to Vicuna.
Focusing on MM-PlanLLMPlanLLM, on phase 3 we skipped training on text-only samples as the model already had been trained on this setting, despite this the model is still competitive across all 3 tasks showing that our training approach seems to be agnostic to the models’ pertaining tasks.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Frame Similarity</h2>
<figure class="ltx_figure" id="A3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="991" id="A3.F5.g1" src="x5.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Average similarity of each frame against all other frames from the same video. It shows a clear bidirectional 3-frame window of higher similarity. </figcaption>
</figure>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">To investigate the degree of visual similarity between frames within recipe videos, we conducted an analysis using a subset of 1446 recipe videos from our dataset, each containing 100 or more frames. For each frame within the first 100 frames of a video, we computed its cosine similarity with all other frames in the same video using a CLIP image encoder, and averaged the similarity for each frame position across all videos.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">The resulting averaged similarity matrix, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A3.F5" title="Figure 5 ‣ Appendix C Frame Similarity ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">5</span></a>, confirms that frames exhibit exceptionally high similarity to their immediate neighbors (mostly on a bidirectional 3 to 5-frame window), with a gradual drop-off in similarity beyond that point. Interestingly, we also note how similar frames from the same video tend to be with most frames having at least 0.7 similarity score to every other frame in the video. This underscores the need of visual encoders capable of differentiating the subtle visual changes that separate frames relevant to different plan steps.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>CVMR and VSG Examples</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we include a few examples of CVMR (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A4.F6" title="Figure 6 ‣ Appendix D CVMR and VSG Examples ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">6</span></a>) and VSG (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A4.T6" title="Table 6 ‣ Appendix D CVMR and VSG Examples ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">6</span></a>) generations, extracted from the dataset test set. Additionally, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19074v2#A4.F7" title="Figure 7 ‣ Appendix D CVMR and VSG Examples ‣ Show and Guide: Instructional-Plan Grounded Vision and Language Model"><span class="ltx_text ltx_ref_tag">7</span></a>, we showcase two dialogues collected by having a volunteer interact with the system. These examples and dialogues, demonstrate the model’s performance on both single and multi-turn scenarios, showing that it can accurately answer to a wide range of requests in the target setting.</p>
</div>
<figure class="ltx_figure" id="A4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="569" id="A4.F6.g1" src="extracted/5897132/images/cvmr_template.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Five examples of CVMR results from the TastyVidDial test set. These examples demonstrate that the model is adept at identifying the key elements that should be characterized in the target frame. However, in some cases, the selected middle frame may not be ideal (3rd and 4th rows), and in others, alternative frames may more accurately represent the target plan step (1st row). The conversational dialogue context has been omitted for readability, and green boxes highlight the target frame amongst the retrieved ones.</figcaption>
</figure>
<figure class="ltx_table" id="A4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T6.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T6.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A4.T6.5.6.1.1"><span class="ltx_text ltx_font_bold" id="A4.T6.5.6.1.1.1">User Image</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A4.T6.5.6.1.2"></th>
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A4.T6.5.6.1.3"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T6.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.1.1.1" rowspan="2" style="padding-bottom:4.30554pt;"><span class="ltx_text" id="A4.T6.1.1.1.1" style="position:relative; bottom:0.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="A4.T6.1.1.1.1.1.g1" src="extracted/5897132/images/vsg/vsg_109.jpg" width="120"/></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.1.1.2" style="padding-bottom:4.30554pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.1.2.1">Target:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.1.3" style="padding-bottom:4.30554pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.1.3.1">
<span class="ltx_p" id="A4.T6.1.1.3.1.1" style="width:281.9pt;">Step 5: Pinch a piece of dough, about the size of a large egg, and roll into a ball. Using your hands, flatten the dough and spoon filling into the center. Wrap the dough into a pear shape and make sure there aren’t any holes.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.7.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T6.5.7.1.1"><span class="ltx_text ltx_font_bold" id="A4.T6.5.7.1.1.1">Gen:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T6.5.7.1.2">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.7.1.2.1">
<span class="ltx_p" id="A4.T6.5.7.1.2.1.1" style="width:281.9pt;">Continue with Step 5, Pinch a piece of dough, about the size of a large egg, and roll into a ball. Using your hands, flatten the dough and spoon filling into the center. Wrap the dough into a pear shape and make sure there aren’t any</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.2.2.1" rowspan="2" style="padding-bottom:17.22217pt;"><span class="ltx_text" id="A4.T6.2.2.1.1" style="position:relative; bottom:-0.9pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="A4.T6.2.2.1.1.1.g1" src="extracted/5897132/images/vsg/vsg_147.jpg" width="120"/></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.2.2.2" style="padding-bottom:17.22217pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.2.2.2.1">Target:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.2.2.3" style="padding-bottom:17.22217pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.2.2.3.1">
<span class="ltx_p" id="A4.T6.2.2.3.1.1" style="width:281.9pt;">Step 8: Bake or grill for 20-30 minutes or until chicken is no longer pink.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.8.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T6.5.8.2.1" style="padding-bottom:17.22217pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.5.8.2.1.1">Gen:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T6.5.8.2.2" style="padding-bottom:17.22217pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.8.2.2.1">
<span class="ltx_p" id="A4.T6.5.8.2.2.1.1" style="width:281.9pt;">Continue with Step 8, Bake or grill for 20-30 minutes or until chicken is no longer pink.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.3.3.1" rowspan="2" style="padding-bottom:21.52771pt;"><span class="ltx_text" id="A4.T6.3.3.1.1" style="position:relative; bottom:-0.9pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="A4.T6.3.3.1.1.1.g1" src="extracted/5897132/images/vsg/vsg_245.jpg" width="120"/></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.3.3.2" style="padding-bottom:21.52771pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.3.3.2.1">Target:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.3.3.3" style="padding-bottom:21.52771pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.3.3.3.1">
<span class="ltx_p" id="A4.T6.3.3.3.1.1" style="width:281.9pt;">Step 4: Add the rice, soy sauce, and peas, and mix thoroughly.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.9.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T6.5.9.3.1" style="padding-bottom:21.52771pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.5.9.3.1.1">Gen:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T6.5.9.3.2" style="padding-bottom:21.52771pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.9.3.2.1">
<span class="ltx_p" id="A4.T6.5.9.3.2.1.1" style="width:281.9pt;">Continue with Step 4, Add the rice, soy sauce, and peas, and mix thoroughly.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.4.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.4.4.1" rowspan="2"><span class="ltx_text" id="A4.T6.4.4.1.1" style="position:relative; bottom:0.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="A4.T6.4.4.1.1.1.g1" src="extracted/5897132/images/vsg/vsg_394.jpg" width="120"/></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.4.4.2"><span class="ltx_text ltx_font_bold" id="A4.T6.4.4.2.1">Target:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.4.4.3.1">
<span class="ltx_p" id="A4.T6.4.4.3.1.1" style="width:281.9pt;">Step 4: Melt 75g of the white chocolate and stir in some of the yellow food colouring until the desired colour is reached. If it seizes on you, add a little bit of oil and stir it until it reaches a runny consistency again.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.10.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T6.5.10.4.1"><span class="ltx_text ltx_font_bold" id="A4.T6.5.10.4.1.1">Gen:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T6.5.10.4.2">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.10.4.2.1">
<span class="ltx_p" id="A4.T6.5.10.4.2.1.1" style="width:281.9pt;">Continue with Step 4, Melt 75g of the white chocolate and stir in some of the yellow food colouring until the desired colour is reached. If it seizes on you, add a little bit of oil and stir it until it reaches a runny consistency again.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="A4.T6.5.5.1" rowspan="2" style="padding-bottom:17.22217pt;"><span class="ltx_text" id="A4.T6.5.5.1.1" style="position:relative; bottom:-0.8pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="120" id="A4.T6.5.5.1.1.1.g1" src="extracted/5897132/images/vsg/vsg_10.jpg" width="120"/></span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T6.5.5.2" style="padding-bottom:17.22217pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.5.5.2.1">Target:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.5.5.3" style="padding-bottom:17.22217pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.5.3.1">
<span class="ltx_p" id="A4.T6.5.5.3.1.1" style="width:281.9pt;">Step 5: Add the shrimp and let them cook for 3 minutes. Flip and cook for another 3 minutes.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.5.11.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A4.T6.5.11.5.1" style="padding-bottom:17.22217pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.5.11.5.1.1">Gen:</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A4.T6.5.11.5.2" style="padding-bottom:17.22217pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.5.11.5.2.1">
<span class="ltx_p" id="A4.T6.5.11.5.2.1.1" style="width:281.9pt;">Continue with Step 4, Add garlic, rosemary, thyme, smoked paprika, salt and pepper. Stir and cook an additional 2 minutes.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Five examples showcasing Visually-Informed Step Generation, from the test set. The model successfully identifies the appropriate next step in most cases, leveraging dialogue history and user-uploaded images. However, the difficulty increases when uploaded images lack clarity regarding the user’s progress, as seen in the final example where it is unclear if the ingredient from Step 4 have already been added. Dialogue history ommited for readability.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="608" id="A4.F7.g1" src="x6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Real multimodal dialogues carried out by a volunteer interacting with MM-PlanLLM. These dialogues showcase the model’s ability to carry out full conversations with interleaved multimodal requests, without hindering text-only performance.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 20:46:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
