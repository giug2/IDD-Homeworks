<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.05106] Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures</title><meta property="og:description" content="This paper presents an investigation into machine learning techniques for violence detection in videos and their adaptation to a federated learning context. The study includes experiments with spatio-temporal features …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.05106">

<!--Generated on Wed Feb 28 12:56:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
PAJON Quentin<sup id="id9.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">SERRE Swan<sup id="id10.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">WISSOCQ Hugo<sup id="id11.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">RABAUD Léo<sup id="id12.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">HAIDAR Siba<sup id="id13.5.id1" class="ltx_sup"><span id="id13.5.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>&amp;YAACOUB Antoun<sup id="id14.6.id2" class="ltx_sup"><span id="id14.6.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>
<sup id="id15.7.id3" class="ltx_sup">1</sup>ESIEA, Paris, France
<br class="ltx_break"><sup id="id16.8.id4" class="ltx_sup">2</sup>Learning, Data and Robotics (LDR) Lab, ESIEA, Paris, France
{pajon, sserre, wissocq, lrabaud}@et.esiea.fr, {siba.haidar, antoun.yaacoub}@esiea.fr
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">This paper presents an investigation into machine learning techniques for violence detection in videos and their adaptation to a federated learning context. The study includes experiments with spatio-temporal features extracted from benchmark video datasets, comparison of different methods, and proposal of a modified version of the ”Flow-Gated” architecture called ”Diff-Gated.” Additionally, various machine learning techniques, including super-convergence and transfer learning, are explored, and a method for adapting centralized datasets to a federated learning context is developed. The research achieves better accuracy results compared to state-of-the-art models by training the best violence detection model in a federated learning context.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Violence detection can be used in many contexts: soccer stadiums, surveillance cameras, video sharing services, etc.
Moreover, humans aren’t able to detect violence on this scale because of the huge quantity of data involved.
In the context of a CCTV center such AI can be used to inform the authorities and permit to intervene faster <cite class="ltx_cite ltx_citemacro_cite">Youssef <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning can play a crucial role in ensuring data privacy and security for violence detection in surveillance videos, especially in compliance with GDPR regulations <cite class="ltx_cite ltx_citemacro_cite">EU (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>. By keeping the training data and machine learning model on the devices where it was collected, the risk of sensitive information being intercepted on the network can be avoided <cite class="ltx_cite ltx_citemacro_cite">Gosselin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. Moreover, federated learning can leverage techniques like differential privacy <cite class="ltx_cite ltx_citemacro_cite">Hu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> to protect individual privacy and reduce the risk of model reverse-engineering <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>. This approach can be particularly useful in contexts such as CCTV centers, where timely detection of violence can inform authorities and help to intervene faster.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The ethical implications tied to the use of machine learning for violence detection in surveillance contexts are significant. Potential issues range from infringements of privacy rights <cite class="ltx_cite ltx_citemacro_cite">Crawford and
Schultz (<a href="#bib.bib7" title="" class="ltx_ref">2013</a>)</cite> to biases in the algorithm, potentially leading to discriminatory outcomes <cite class="ltx_cite ltx_citemacro_cite">Zou and Schiebinger (<a href="#bib.bib35" title="" class="ltx_ref">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Buolamwini and
Gebru (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>. These systems, while technologically innovative, need to be used judiciously to avoid misuse and ensure fairness and transparency <cite class="ltx_cite ltx_citemacro_cite">Mittelstadt <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Greene <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. They should not replace human judgement, but act as supportive tools for security personnel. Thus, it becomes critical to foster responsible AI practices, placing paramount importance on individual privacy and fairness.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose a deep learning architecture for violence detection that can be effectively trained using federated learning, with memory efficiency and reduced training time as key considerations. By exploring the use of spatio-temporal features and machine learning techniques such as super-convergence and transfer learning, we achieve better accuracy results compared to state-of-the-art models. Our approach can pave the way for privacy-preserving and efficient violence detection in a range of real-world scenarios.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we explore two important topics in the field of data science: violence detection and federated learning.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Violence Detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Violence detection is a sub-field of action recognition that consists of detecting specific actions in videos. The detection of violence or fight scenes has been an active research field for a long time. Classical methods for violence detection used hand-crafted features, such as ViF (Violent Flow) <cite class="ltx_cite ltx_citemacro_cite">Hassner <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2012</a>)</cite>, which detects changes in optical flow, and OViF (Oriented Violent Flow) <cite class="ltx_cite ltx_citemacro_cite">Gao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>, an improvement of ViF that makes better use of the orientation information of optical flows. Optical flow is the apparent changes in the pixels of two consecutive images of a video.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">More recently, deep learning methods have been developed, achieving better results and requiring less processing than classical methods, allowing the model to learn the violence patterns. To do this, several architectures have been experimented with over time. One method is to use 3D CNNs <cite class="ltx_cite ltx_citemacro_cite">Ding <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2014</a>)</cite>, which applies convolutions to videos. This method has improved over time, such as in <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, which applies the concepts of DenseNet <cite class="ltx_cite ltx_citemacro_cite">Huang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> to improve the model’s performance and reduce the number of parameters.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Another deep learning method is to use Conv-LSTM cells <cite class="ltx_cite ltx_citemacro_cite">Shi <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> and a traditional CNN. LSTM aggregates the features extracted by the CNN on the frames of the video to obtain temporal information. This is what is done by <cite class="ltx_cite ltx_citemacro_cite">Sudhakaran and
Lanz (<a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> proposed using two input channels instead of one corresponding to the video frames. The additional channel is the optical flow, which helps the model focus on the areas where there is movement and possibly violence.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Data governance has become an important consideration in violence detection research, particularly with regards to the ethical and legal implications of video data privacy following the implementation of the GDPR <cite class="ltx_cite ltx_citemacro_cite">EU (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>. In response, federated learning has emerged as a promising approach for training data science models without centralized data storage, with several algorithms such as FederatedAveraging <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2020b</a>)</cite>, MIME <cite class="ltx_cite ltx_citemacro_cite">Karimireddy <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, and FEDOPT <cite class="ltx_cite ltx_citemacro_cite">Reddi <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> proposed for this purpose.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In this paper, we adopt FederatedAveraging <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, a technique that allows each client to train a local model with their own data. The resulting models are then averaged to train a global model that is sent back to the clients after each round. FederatedAveraging has been demonstrated to be well-suited to Non-IID (independent and identically distributed) data distributions in <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Relatedly, <cite class="ltx_cite ltx_citemacro_cite">Silva <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> work in 2022 also leveraged federated learning for violence detection in videos, using various pre-trained convolutional neural networks on the AIRTLab Dataset and finding the best performance with MobileNet architecture, showcasing a high accuracy of 99.4% with only a marginal loss when compared to non-federated learning settings.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this subsection, we present the architectures of the models used for violence detection and how we adapted these models for the federated learning context. Our objective is to achieve state-of-the-art or higher accuracy while being resource and time-efficient since these models are intended to be deployed and trained in CCTV centers, which typically have less powerful computers than research environments. We also describe the methodology we used to develop and optimize our violence detection model, which involved addressing the limitations of classical classifiers using transfer learning, early stopping, and One-Cycle training. Additionally, we explore multi-channel input models using both optical flow and frame differences, and detail the specific model architectures we used to achieve better performance while reducing computation time. Overall, this section provides a comprehensive overview of our approach to developing a robust and efficient violence detection model.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Limitation of Classical Classifiers</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> utilized a pre-trained model on the Sports-1M dataset <cite class="ltx_cite ltx_citemacro_cite">Tran <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2015</a>)</cite> for feature extraction (see Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Limitation of Classical Classifiers ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and employed an SVM classifier to classify videos as violent or non-violent.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2308.05106/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Structure of the feature extraction architecture <cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We explored different classification methods for violence detection and evaluated their performance. In particular, we tested decision tree and random forest classifiers using a pre-trained model to extract video features. We optimized the hyperparameters of both classifiers using grid search and trained them on the extracted features. Despite being marginally faster than the SVM classifier, the decision tree and random forest classifiers did not meet our needs in terms of speed and accuracy on our dataset. For instance, it took around 5 minutes to extract features from 300 videos of 5 seconds in 30 fps and low resolution, followed by 10 seconds for the classifier training, which is not efficient for our use case.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">This experimentation revealed that classical classification methods on extracted features for violence detection are not well-suited for real-time applications.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transfer Learning and Early Stopping</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To improve training efficiency, it was essential to explore various transfer learning architectures.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> utilized a transfer learning architecture consisting of six pre-trained layers, with two fully connected layers of 4096 and 512 neurons respectively, and a final output layer (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transfer Learning and Early Stopping ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:867.2pt;height:187pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.6pt,14.6pt) scale(0.865176627938585,0.865176627938585) ;"><img src="/html/2308.05106/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="1070" height="231" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Structure of the transfer learning architecture <cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We tried many different architectures, from starting the transfer learning one layer earlier, to adding multiple training layers, each layer containing from 256 to 4096 neurons. The goal being to get a better training time or ROC curve than the model used in the previous paper, therefore, the time to beat was 12min30s and the AUC was 0,987.
Through trial and error, we obtained results ranging from 8 minutes of training time and an AUC of 0.941 to 17 minutes and and AUC of 0.991.
We noticed that the more neurons we used on the before last layer, the faster the network converged, but at the same time, the worse the accuracy became.
However, these results are not what we need since they are slower and less accurate than what a random forest classifier is able to produce (Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Classifiers Comparison ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The aim being to get the most efficient training possible due to material constraints in a federated context, we want to reduce training time as much as possible, which leads us to the next section.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Transfer Learning and One-Cycle Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To reduce training time without sacrificing results, we aimed to accelerate the convergence of our networks. We used a technique called cyclical learning rates, introduced by <cite class="ltx_cite ltx_citemacro_cite">Smith (<a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>, which involves gradually increasing the learning rate from a very low value to a high value and then decreasing it back to the starting value. This technique can help speed up convergence.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The super-convergence <cite class="ltx_cite ltx_citemacro_cite">Smith and
Topin (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> uses a single cycle of cyclical learning with optimal learning rates to train a model for a few epochs. This method is called One-Cycle training. In order to find these lower and higher bounds of learning rates, we trained the model for one single epoch with a learning rate gradually rising from <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="1e^{-15}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml"><mo id="S3.SS3.p2.1.m1.1.1.3.3a" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS3.p2.1.m1.1.1.3.3.2" xref="S3.SS3.p2.1.m1.1.1.3.3.2.cmml">15</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">1</cn><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3"><minus id="S3.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3.2">15</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">1e^{-15}</annotation></semantics></math> to 1 and plotted the loss on a graph as the Y-axis with the learning rate as the X-axis. The minimum and maximum learning rates were respectively located after a sharp drop and before a sharp rise in loss. We used this method to reduce training time while maintaining high accuracy and ROC curves with an Area Under Curve of over 0.96.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We, then, experimented with various architectures by varying the number of layers, number of neurons, and starting layer for transfer learning. We found that training time evened out completely, and that lighter architectures starting from the seventh layer tended to produce better results. Therefore, we chose an architecture with 1024 neurons on the seventh layer and a final fully connected layer (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Transfer Learning and Early Stopping ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This approach allows for faster training times than feature extraction methods while maintaining similar accuracy.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-Channel Input Models using Optical Flow</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We experimented with multi-input models, including the Flow-Gated architecture proposed by <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, which has a lightweight design (272,690 parameters) and uses both RGB frames and optical flow inputs to identify areas of movement and potential violence (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.4 Multi-Channel Input Models using Optical Flow ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The model consists of four blocks, including the RGB and Optical Flow Channels, a Merging Block, and a Fully Connected Layer, with all 3D CNN blocks using depth-wise separable convolutions from MobileNet <cite class="ltx_cite ltx_citemacro_cite">Howard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> and Pseudo-3D Residual Networks <cite class="ltx_cite ltx_citemacro_cite">Qiu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> to reduce parameters without sacrificing performance. However, the time required to compute optical flow is a significant drawback, with an average computation time of 9 seconds for a 5-second, 30fps video, making it unsuitable for near real-time violence detection and minimizing learning time.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:867.2pt;height:238.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-340.4pt,93.4pt) scale(0.560224080800493,0.560224080800493) ;"><img src="/html/2308.05106/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="1652" height="453" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Structure of the flow gated architecture <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Multi-Channel Input Models using Frame Differences</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">To reduce computation time required for calculating optical flow, we opted to use frame differences instead, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">Islam <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. This method requires significantly less computation time (0.065 seconds compared to 9 seconds for optical flow) and also reduces the number of input channels required. Unlike optical flow, which also provides direction of movement, frame differences only detect changes in the image.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">We retained the original Flow-Gated model architecture and modified the optical flow channel to use frame differences. We also added more dropout to the fully connected layer to prevent overfitting. This modified architecture, which we named ”Diff-Gated”, is lightweight (272,546 parameters) and achieves better results than the original Flow-Gated network, as shown in Table <a href="#S5.T6" title="Table 6 ‣ 5.3 Model Performance across Datasets and Configurations ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. These changes have reduced preprocessing and training time while improving accuracy.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe the experiments we conducted to develop and optimize our violence detection model. We begin by explaining our choice of dataset and the setup we used for our experiments. We then discuss how we adapted the dataset to the federated learning context and the challenges we faced. In the next section, we present our results for training previously tested models in a federated setting.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset Selection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Various datasets exist for violence detection, such as the Movies dataset <cite class="ltx_cite ltx_citemacro_cite">Bermejo Nievas <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2011</a>)</cite> consisting of fight scenes from movies or the Hockey Fight dataset <cite class="ltx_cite ltx_citemacro_cite">Bermejo Nievas <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2011</a>)</cite> composed of fights from hockey games. However, these datasets have specific video contexts and may not represent real-life situations. To choose the most relevant dataset, we looked for CCTV footage of real-life physical violence situations with a significant number of videos and that have been used in other studies.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We selected the RWF-2000 dataset <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, consisting of 2000 videos from various sources of CCTV cameras, with 1000 violent videos and 1000 non-violent videos. All videos are 5 seconds long and filmed at 30 fps. To ensure the reliability of our models in multiple situations, we also used three other datasets: the crowd violence dataset <cite class="ltx_cite ltx_citemacro_cite">Hassner <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2012</a>)</cite>, the AIRTLab dataset <cite class="ltx_cite ltx_citemacro_cite">Bianculli <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, and the hockey fights dataset <cite class="ltx_cite ltx_citemacro_cite">Bermejo Nievas <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2011</a>)</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">However, training the RWF-2000 dataset required a significant amount of RAM, even with high-memory resources like Google Colab (83.5GB RAM). To avoid memory issues, we limited ourselves to using 300 to 400 videos for training.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Setup: Hardware and Software</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">All our experiments were conducted using Google Colab. For the non-federated models, we used a high-RAM setup and a standard GPU (Tesla T4). For the federated model experiments, we used a high-RAM setup with a premium GPU (Nvidia V100 or A100), as the power and RAM associated with a premium GPU were necessary for federated learning. We needed to simulate multiple clients and a server on a single machine to create a federated learning environment.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Given the novelty of our engagement with Google Colab and video classification, coupled with the challenging timeline, our primary focus was directed towards other pivotal aspects of the project. Consequently, the optimization of memory usage presents an area for further exploration and improvement.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data Preparation for Federated Learning: Adapting Traditional Datasets</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The datasets we have chosen for violence detection consist of videos labelled either ”violent” or ”non-violent”. However, in a federated learning context, multiple ”clients” are needed, each simulating a different data source. This presents a challenge, as traditional datasets are not designed for federated learning.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To address this challenge, we propose and develop a method to simulate a federated-learning ready dataset from a traditional one. Our method involves a stratified split, which means that each video is associated with one and only one client, and each client has approximately the same number of violent and non-violent videos. This ensures that the data distribution is balanced across the different clients and reduces the risk of Non-IID data, making it easier to train a model.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Our proposed method offers several advantages. Firstly, it enables us to use traditional datasets for federated learning. Secondly, it helps to prevent the failure of models due to Non-IID data distribution. Lastly, it provides a more realistic representation of the data distribution in real-world scenarios, where different data sources may have varying proportions of violent and non-violent videos.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Federated Learning with Previously Tested Models</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We focused on training only one of our models in a federated context. Our choice was the model presented in <a href="#S3.SS3" title="3.3 Transfer Learning and One-Cycle Training ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> because it had the best accuracy/training time ratio while reducing the number of parameters.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We experimented with two frameworks: TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">TensorFlow (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> and Flower <cite class="ltx_cite ltx_citemacro_cite">Beutel <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. While achieving an accuracy of over 80% with TensorFlow Federated is extremely time-consuming, we found it easier to do so with Flower. As a result, we decided to focus our efforts on Flower.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Although we were satisfied with our metrics using Flower, we encountered issues because no memory was freed between training rounds. Despite this, we were able to successfully train the model presented in <a href="#S3.SS3" title="3.3 Transfer Learning and One-Cycle Training ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> using the FederatedAveraging algorithm <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">We observed the following:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">When training on a random sample of data sources each round, our metrics were slightly lower than those obtained outside of a federated context. We consider this result expected because the model was trained on fewer data per round.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">When training on every data source each round, the accuracy was higher than that obtained outside of a federated context. We believe this is due to the multiple rounds of training, corresponding to multiple complete training cycles, instead of two epochs used for training using the One-Cycle method.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">In conclusion, we were able to successfully adapt our non-federated models to a federated learning context using the Flower framework. We were able to train the model presented in Section <a href="#S3.SS3" title="3.3 Transfer Learning and One-Cycle Training ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> using the FederatedAveraging algorithm. Although we faced some challenges due to memory size issues in a federated context, we were still able to achieve good metrics for our federated model. Our results show that training on a random sample of data sources each round resulted in slightly lower accuracy, while training on every data source each round resulted in higher accuracy compared to training outside of a federated context. These findings suggest that federated learning has the potential to improve violence detection models while preserving privacy, and could be further explored in future work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present the results of our experiments on violence detection in videos using deep learning techniques. We first compare the accuracies of different classifiers following feature extraction on three hundred videos of the RWF-2000 database. We also report the preprocessing times of different datasets, which are influenced by video framerate, resolution, and length. Next, we provide the results of our proposed model trained on four different datasets, with different numbers of epochs and maximum learning rates. We also compare the accuracy and computation time of our Diff-Gated model with the original Flow-Gated architecture on the RWF-2000 dataset. Finally, we show the validation accuracy of our federated learning model on the RWF-2000 dataset for each round of training.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Classifiers Comparison</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Classifiers Comparison ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the accuracy and training times of various classifiers using the C3D model and different feature extraction methods on 300 videos of the RWF-2000 dataset <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. The feature extraction process takes approximately five minutes, which needs to be added to the training time to determine the overall time required for generating a classifier.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Classifier</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Accuracy</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Training time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<th id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">C3D + SVM(fc7)</th>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">99.8%</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">23s</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<th id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C3D + Decision Trees(fc7)</th>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_right">83.9%</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_right">10s</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<th id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C3D + Random Forest (fc7)</th>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_right">99.5%</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_right">10s</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<th id="S5.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">C3D + Decision Trees(fc6)</th>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_right">85.9%</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_right">10s</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<th id="S5.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">C3D + Random Forest(fc6)</th>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_right ltx_border_bb">99.5%</td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_right ltx_border_bb">10s</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy and training times of the extraction feature methods on 300 videos of the RWF-2000 dataset.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Establishing Baseline Model Performance for Subsequent Comparisons</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Although using decision trees instead of a SVM can save some time in training, the amount of time needed to extract features makes it not worthwhile to pursue this avenue in our context <cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. The preprocessing times shown in table <a href="#S5.T2" title="Table 2 ‣ 5.2 Establishing Baseline Model Performance for Subsequent Comparisons ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are influenced by several factors, such as the frame rate, resolution, and length of the videos used in the datasets. For instance, the Hockey Fights dataset takes less time to process because each video lasts only one second, is filmed at around 30fps, and has a resolution of 360p. On the other hand, the AIRTlab dataset has longer videos, with a duration of five seconds, filmed at 30fps and a resolution of 1080p.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Dataset</th>
<th id="S5.T2.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"># videos</th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">
<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><approx id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\approx</annotation></semantics></math> preprocessing time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">AIRTLab</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">350</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3m30s</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Crowd Violence</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">246</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">36s</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Hockey Fights</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1000</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1m08s</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">RWF-2000 (400 videos)</th>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">400</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">2m36s</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Number of videos per dataset and their approximate preprocessing time.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The results presented in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Establishing Baseline Model Performance for Subsequent Comparisons ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> can be used as a baseline for comparison with the subsequent models. It is important to note that the model used is the one presented in section <a href="#S3.SS2" title="3.2 Transfer Learning and Early Stopping ‣ 3 Methodology ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, with 512 neurons on its penultimate dense layer.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">C3DFC w/ Early Stopping</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Accuracy</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ROC AUC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AIRTLab</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">95.6%</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">0.9894</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Crowd Violence</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_right">99.0%</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_right">0.9994</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Hockey Fights</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_right">96.6%</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_right">0.9931</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">RWF-2000 (400 videos)</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_right ltx_border_bb">94.7%</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb">0.9922</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the model proposed by <cite class="ltx_cite ltx_citemacro_cite">Sernani <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> on different datasets.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model Performance across Datasets and Configurations</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The tables <a href="#S5.T4" title="Table 4 ‣ 5.3 Model Performance across Datasets and Configurations ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5.T5" title="Table 5 ‣ 5.3 Model Performance across Datasets and Configurations ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> display the results of training our model on four different databases, with the penultimate layer utilizing 1024 neurons.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.3.1.1" class="ltx_tr">
<th id="S5.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">C3DFC w/ One-Cycle</th>
<th id="S5.T4.3.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Training</th>
<th id="S5.T4.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">ACC</th>
<th id="S5.T4.3.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">ROC AUC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.3.2.1" class="ltx_tr">
<th id="S5.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AIRTLab</th>
<td id="S5.T4.3.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3m11s</td>
<td id="S5.T4.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">91.0%</td>
<td id="S5.T4.3.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.9972</td>
</tr>
<tr id="S5.T4.3.3.2" class="ltx_tr">
<th id="S5.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Crowd Violence</th>
<td id="S5.T4.3.3.2.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">59s</td>
<td id="S5.T4.3.3.2.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">98.4%</td>
<td id="S5.T4.3.3.2.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.9994</td>
</tr>
<tr id="S5.T4.3.4.3" class="ltx_tr">
<th id="S5.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Hockey Fights</th>
<td id="S5.T4.3.4.3.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">1m35s</td>
<td id="S5.T4.3.4.3.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">94.8%</td>
<td id="S5.T4.3.4.3.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.9828</td>
</tr>
<tr id="S5.T4.3.5.4" class="ltx_tr">
<th id="S5.T4.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">RWF-2000 (400 videos)</th>
<td id="S5.T4.3.5.4.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">3m03s</td>
<td id="S5.T4.3.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">94.9%</td>
<td id="S5.T4.3.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.9875</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of our model trained for <span id="S5.T4.5.1" class="ltx_text ltx_framed ltx_framed_underline">2 epochs</span> on four different datasets using super-convergence and a maximum learning rate of <math id="S5.T4.2.m1.1" class="ltx_Math" alttext="50e^{-2}" display="inline"><semantics id="S5.T4.2.m1.1b"><mrow id="S5.T4.2.m1.1.1" xref="S5.T4.2.m1.1.1.cmml"><mn id="S5.T4.2.m1.1.1.2" xref="S5.T4.2.m1.1.1.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S5.T4.2.m1.1.1.1" xref="S5.T4.2.m1.1.1.1.cmml">​</mo><msup id="S5.T4.2.m1.1.1.3" xref="S5.T4.2.m1.1.1.3.cmml"><mi id="S5.T4.2.m1.1.1.3.2" xref="S5.T4.2.m1.1.1.3.2.cmml">e</mi><mrow id="S5.T4.2.m1.1.1.3.3" xref="S5.T4.2.m1.1.1.3.3.cmml"><mo id="S5.T4.2.m1.1.1.3.3b" xref="S5.T4.2.m1.1.1.3.3.cmml">−</mo><mn id="S5.T4.2.m1.1.1.3.3.2" xref="S5.T4.2.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.m1.1c"><apply id="S5.T4.2.m1.1.1.cmml" xref="S5.T4.2.m1.1.1"><times id="S5.T4.2.m1.1.1.1.cmml" xref="S5.T4.2.m1.1.1.1"></times><cn type="integer" id="S5.T4.2.m1.1.1.2.cmml" xref="S5.T4.2.m1.1.1.2">50</cn><apply id="S5.T4.2.m1.1.1.3.cmml" xref="S5.T4.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T4.2.m1.1.1.3.1.cmml" xref="S5.T4.2.m1.1.1.3">superscript</csymbol><ci id="S5.T4.2.m1.1.1.3.2.cmml" xref="S5.T4.2.m1.1.1.3.2">𝑒</ci><apply id="S5.T4.2.m1.1.1.3.3.cmml" xref="S5.T4.2.m1.1.1.3.3"><minus id="S5.T4.2.m1.1.1.3.3.1.cmml" xref="S5.T4.2.m1.1.1.3.3"></minus><cn type="integer" id="S5.T4.2.m1.1.1.3.3.2.cmml" xref="S5.T4.2.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.m1.1d">50e^{-2}</annotation></semantics></math>. The table shows the training time in minutes, the accuracy (ACC), and the receiver operating characteristic area under the curve (ROC AUC) achieved on each dataset. The model has 1024 neurons in the penultimate layer.</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.3.1.1" class="ltx_tr">
<th id="S5.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">C3DFC w/ One-Cycle</th>
<th id="S5.T5.3.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Training</th>
<th id="S5.T5.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Acc</th>
<th id="S5.T5.3.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">ROC AUC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.3.2.1" class="ltx_tr">
<th id="S5.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AIRTLab</th>
<td id="S5.T5.3.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1m45</td>
<td id="S5.T5.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">89.8%</td>
<td id="S5.T5.3.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.9437</td>
</tr>
<tr id="S5.T5.3.3.2" class="ltx_tr">
<th id="S5.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Crowd Violence</th>
<td id="S5.T5.3.3.2.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">32s</td>
<td id="S5.T5.3.3.2.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">97.2%</td>
<td id="S5.T5.3.3.2.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.9627</td>
</tr>
<tr id="S5.T5.3.4.3" class="ltx_tr">
<th id="S5.T5.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">Hockey Fights</th>
<td id="S5.T5.3.4.3.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">59s</td>
<td id="S5.T5.3.4.3.3" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">94.8%</td>
<td id="S5.T5.3.4.3.4" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">0.9755</td>
</tr>
<tr id="S5.T5.3.5.4" class="ltx_tr">
<th id="S5.T5.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">RWF-2000 (400 videos)</th>
<td id="S5.T5.3.5.4.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">1m52s</td>
<td id="S5.T5.3.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">90.4%</td>
<td id="S5.T5.3.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.9669</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results of our model using super-convergence and <span id="S5.T5.5.1" class="ltx_text ltx_framed ltx_framed_underline">1 epoch</span> of training and a maximum learning rate of <math id="S5.T5.2.m1.1" class="ltx_Math" alttext="60e^{-2}" display="inline"><semantics id="S5.T5.2.m1.1b"><mrow id="S5.T5.2.m1.1.1" xref="S5.T5.2.m1.1.1.cmml"><mn id="S5.T5.2.m1.1.1.2" xref="S5.T5.2.m1.1.1.2.cmml">60</mn><mo lspace="0em" rspace="0em" id="S5.T5.2.m1.1.1.1" xref="S5.T5.2.m1.1.1.1.cmml">​</mo><msup id="S5.T5.2.m1.1.1.3" xref="S5.T5.2.m1.1.1.3.cmml"><mi id="S5.T5.2.m1.1.1.3.2" xref="S5.T5.2.m1.1.1.3.2.cmml">e</mi><mrow id="S5.T5.2.m1.1.1.3.3" xref="S5.T5.2.m1.1.1.3.3.cmml"><mo id="S5.T5.2.m1.1.1.3.3b" xref="S5.T5.2.m1.1.1.3.3.cmml">−</mo><mn id="S5.T5.2.m1.1.1.3.3.2" xref="S5.T5.2.m1.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.2.m1.1c"><apply id="S5.T5.2.m1.1.1.cmml" xref="S5.T5.2.m1.1.1"><times id="S5.T5.2.m1.1.1.1.cmml" xref="S5.T5.2.m1.1.1.1"></times><cn type="integer" id="S5.T5.2.m1.1.1.2.cmml" xref="S5.T5.2.m1.1.1.2">60</cn><apply id="S5.T5.2.m1.1.1.3.cmml" xref="S5.T5.2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T5.2.m1.1.1.3.1.cmml" xref="S5.T5.2.m1.1.1.3">superscript</csymbol><ci id="S5.T5.2.m1.1.1.3.2.cmml" xref="S5.T5.2.m1.1.1.3.2">𝑒</ci><apply id="S5.T5.2.m1.1.1.3.3.cmml" xref="S5.T5.2.m1.1.1.3.3"><minus id="S5.T5.2.m1.1.1.3.3.1.cmml" xref="S5.T5.2.m1.1.1.3.3"></minus><cn type="integer" id="S5.T5.2.m1.1.1.3.3.2.cmml" xref="S5.T5.2.m1.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.m1.1d">60e^{-2}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The table <a href="#S5.T6" title="Table 6 ‣ 5.3 Model Performance across Datasets and Configurations ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows us the accuracy of the multi-channel models we have experimented with during this research on the complete RWF-2000 dataset <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Context</th>
<th id="S5.T6.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Flow-gated</th>
<th id="S5.T6.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Diff-Gated</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.2.1" class="ltx_tr">
<th id="S5.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Accuracy</th>
<td id="S5.T6.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">87.25%</td>
<td id="S5.T6.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">89.75%</td>
</tr>
<tr id="S5.T6.1.3.2" class="ltx_tr">
<th id="S5.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Training time</th>
<td id="S5.T6.1.3.2.2" class="ltx_td ltx_align_right">5h30</td>
<td id="S5.T6.1.3.2.3" class="ltx_td ltx_align_right">5h00</td>
</tr>
<tr id="S5.T6.1.4.3" class="ltx_tr">
<th id="S5.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Processing time for a video</th>
<td id="S5.T6.1.4.3.2" class="ltx_td ltx_align_right ltx_border_bb">9s</td>
<td id="S5.T6.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb">0.065s</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Accuracy, training time, and processing time of our multi-channel input models on the RWF-2000 dataset.</figcaption>
</figure>
<figure id="S5.T7" class="ltx_table">
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Round</th>
<th id="S5.T7.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">0</th>
<th id="S5.T7.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">1</th>
<th id="S5.T7.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">2</th>
<th id="S5.T7.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">3</th>
<th id="S5.T7.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.2.1" class="ltx_tr">
<th id="S5.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Accuracy</th>
<td id="S5.T7.1.2.1.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">50.40%</td>
<td id="S5.T7.1.2.1.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">94.84%</td>
<td id="S5.T7.1.2.1.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">97.72%</td>
<td id="S5.T7.1.2.1.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">98.80%</td>
<td id="S5.T7.1.2.1.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">99.60%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Accuracy table of our federated model for each round</figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Our proposed model, referred to as <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Diff-Gated</span>, achieves higher accuracy while reducing computation in comparison to the original Flow-Gated architecture <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Model Performance across Datasets and Configurations ‣ 5 Results ‣ Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents the validation accuracy of our federated model for each round of training on 400 videos from the RWF-2000 dataset. Round 0 accuracy corresponds to the validation accuracy of the model before the first round of training.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we present an innovative examination of machine learning models for violence detection in videos, with a particular emphasis on Federated Learning and our proposed Diff-Gated architecture. We not only maintain or improve upon the accuracy of conventional methods but also reduce training times, enhancing model usability in practical applications.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Our work incorporated super-convergence for transfer learning and explored diverse classifiers for extracting spatio-temporal features from videos. Through modifications to the Flow-gated architecture proposed by <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, we were able to boost accuracy and cut down training and preprocessing time.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Moreover, we introduced a method for adapting centralized datasets to a federated learning context, using it to train our violence detection model. Despite demonstrating that deep learning models can be effectively trained using federated learning, we note the resource-intensive nature of federated learning, particularly with video data.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">While our research has made significant strides, the challenges inherent in federated learning, such as dealing with Non-IID and unevenly distributed data, cannot be overlooked <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2020a</a>)</cite>. Our future efforts will aim at exploring different federated learning strategies and studying the impact of unbalanced client data.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Despite these challenges, our work provides valuable insights for researchers and practitioners alike, underlining the potential of Federated Learning and other novel techniques in violence detection and other real-world applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bermejo Nievas <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2011]</span>
<span class="ltx_bibblock">
Enrique Bermejo Nievas, Oscar Deniz Suarez, Gloria Bueno García, and Rahul
Sukthankar.

</span>
<span class="ltx_bibblock">Violence detection in video using computer vision techniques.

</span>
<span class="ltx_bibblock">In Pedro Real, Daniel Diaz-Pernil, Helena Molina-Abril, Ainhoa
Berciano, and Walter Kropatsch, editors, <span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">Computer Analysis of Images and
Patterns</span>, volume 6855, pages 332–339. Springer Berlin Heidelberg, 2011.

</span>
<span class="ltx_bibblock">Series Title: Lecture Notes in Computer Science.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet,
Pedro PB de Gusmão, and Nicholas D Lane.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.14390</span>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bianculli <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Miriana Bianculli, Nicola Falcionelli, Paolo Sernani, Selene Tomassini, Paolo
Contardo, Mara Lombardi, and Aldo Franco Dragoni.

</span>
<span class="ltx_bibblock">A dataset for automatic violence detection in videos.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">Data in brief</span>, 33:106587, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and
Gebru [2018]</span>
<span class="ltx_bibblock">
Joy Buolamwini and Timnit Gebru.

</span>
<span class="ltx_bibblock">Gender shades: Intersectional accuracy disparities in commercial
gender classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Conference on fairness, accountability and transparency</span>,
pages 77–91. PMLR, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yong Cheng, Yang Liu, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Federated learning for privacy-preserving ai.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 63(12):33–36, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Ming Cheng, Kunjing Cai, and Ming Li.

</span>
<span class="ltx_bibblock">Rwf-2000: an open large scale video database for violence detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">2020 25th International Conference on Pattern Recognition
(ICPR)</span>, pages 4183–4190. IEEE, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crawford and
Schultz [2013]</span>
<span class="ltx_bibblock">
Kate Crawford and Jason Schultz.

</span>
<span class="ltx_bibblock">Big data and due process: Toward a framework to redress predictive
privacy harms.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Boston College Law Review</span>, 55:93, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Chunhui Ding, Shouke Fan, Ming Zhu, Weiguo Feng, and Baozhi Jia.

</span>
<span class="ltx_bibblock">Violence detection in video by using 3d convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">Advances in Visual Computing: 10th International Symposium,
ISVC 2014, Las Vegas, NV, USA, December 8-10, 2014, Proceedings, Part II 10</span>,
pages 551–558. Springer, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">EU [2016]</span>
<span class="ltx_bibblock">
EU.

</span>
<span class="ltx_bibblock">General data protection regulation (GDPR), https://gdprinfo.eu/,
2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Yuan Gao, Hong Liu, Xiaohu Sun, Can Wang, and Yi Liu.

</span>
<span class="ltx_bibblock">Violence detection using oriented violent flows.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">Image and vision computing</span>, 48:37–41, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gosselin <span id="bib.bib11.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Rémi Gosselin, Loïc Vieu, Faiza Loukil, and Alexandre Benoit.

</span>
<span class="ltx_bibblock">Privacy and security in federated learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 12(19):9901, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greene <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Daniel Greene, Anna Lauren Hoffmann, and Luke Stark.

</span>
<span class="ltx_bibblock">Better, nicer, clearer, fairer: A critical assessment of the movement
for ethical artificial intelligence and machine learning, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassner <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2012]</span>
<span class="ltx_bibblock">
Tal Hassner, Yossi Itcher, and Orit Kliper-Gross.

</span>
<span class="ltx_bibblock">Violent flows: Real-time detection of violent crowd behavior.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">2012 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition Workshops</span>, pages 1–6. IEEE, 2012.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong.

</span>
<span class="ltx_bibblock">Personalized federated learning with differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 7(10):9530–9539, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700–4708, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Islam <span id="bib.bib17.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Zahidul Islam, Mohammad Rukonuzzaman, Raiyan Ahmed, Md Hasanul Kabir, and
Moshiur Farazi.

</span>
<span class="ltx_bibblock">Efficient two-stream network for violence detection using separable
convolutional LSTM.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">2021 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy <span id="bib.bib18.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank
Reddi, Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Breaking the centralized barrier for cross-device federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
volume 34, pages 28663–28676. Curran Associates, Inc., 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib19.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Ji Li, Xinghao Jiang, Tanfeng Sun, and Ke Xu.

</span>
<span class="ltx_bibblock">Efficient violence detection using 3d convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">2019 16th IEEE International Conference on Advanced Video and
Signal Based Surveillance (AVSS)</span>, pages 1–8. IEEE, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib20.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020a]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50–60, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020b]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">Proceedings of Machine learning and systems</span>, 2:429–450, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittelstadt <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Brent Daniel Mittelstadt, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter,
and Luciano Floridi.

</span>
<span class="ltx_bibblock">The ethics of algorithms: Mapping the debate.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">Big Data &amp; Society</span>, 3(2), 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu <span id="bib.bib24.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Zhaofan Qiu, Ting Yao, and Tao Mei.

</span>
<span class="ltx_bibblock">Learning spatio-temporal representation with pseudo-3d residual
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.3.1" class="ltx_text ltx_font_italic">proceedings of the IEEE International Conference on Computer
Vision</span>, pages 5533–5541, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.00295</span>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sernani <span id="bib.bib26.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Paolo Sernani, Nicola Falcionelli, Selene Tomassini, Paolo Contardo, and
Aldo Franco Dragoni.

</span>
<span class="ltx_bibblock">Deep learning for automatic violence detection: Tests on the airtlab
dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 9:160580–160595, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and
Wang-chun Woo.

</span>
<span class="ltx_bibblock">Convolutional lstm network: A machine learning approach for
precipitation nowcasting.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva <span id="bib.bib28.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Victor E. De S. Silva, Tiago B. Lacerda, Péricles B.C. Miranda, André C.A.
Nascimento, and Ana Paula C. Furtado.

</span>
<span class="ltx_bibblock">Federated learning for physical violence detection in videos.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">2022 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith and
Topin [2019]</span>
<span class="ltx_bibblock">
Leslie N Smith and Nicholay Topin.

</span>
<span class="ltx_bibblock">Super-convergence: Very fast training of neural networks using large
learning rates.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and machine learning for multi-domain
operations applications</span>, volume 11006, pages 369–386. SPIE, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith [2017]</span>
<span class="ltx_bibblock">
Leslie N Smith.

</span>
<span class="ltx_bibblock">Cyclical learning rates for training neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">2017 IEEE winter conference on applications of computer
vision (WACV)</span>, pages 464–472. IEEE, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sudhakaran and
Lanz [2017]</span>
<span class="ltx_bibblock">
Swathikiran Sudhakaran and Oswald Lanz.

</span>
<span class="ltx_bibblock">Learning to detect violent videos using convolutional long short-term
memory.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2017 14th IEEE international conference on advanced video and
signal based surveillance (AVSS)</span>, pages 1–6. IEEE, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TensorFlow [2019]</span>
<span class="ltx_bibblock">
TensorFlow.

</span>
<span class="ltx_bibblock">Tensorflow federated, https://www.tensorflow.org/federated, February
2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran <span id="bib.bib33.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.

</span>
<span class="ltx_bibblock">Learning spatiotemporal features with 3d convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 4489–4497, 2015.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Youssef <span id="bib.bib34.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Wael F Youssef, Siba Haidar, and Philippe Joly.

</span>
<span class="ltx_bibblock">Automatic textual description of interactions between two objects in
surveillance videos.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic">SN Applied Sciences</span>, 3(7):695, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou and Schiebinger [2018]</span>
<span class="ltx_bibblock">
James Zou and Londa Schiebinger.

</span>
<span class="ltx_bibblock">Ai can be sexist and racist—it’s time to make it fair, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.05105" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.05106" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.05106">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.05106" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.05107" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:56:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
