<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models</title>
<!--Generated on Thu Aug 22 12:05:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Large Language Models (LLMs),  Hallucination mitigation,  Knowledge-Based tasks,  Medical domains
" lang="en" name="keywords"/>
<base href="/html/2408.13808v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S1" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2.SS1" title="In II Background ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Definition of Hallucination</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2.SS2" title="In II Background ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Causes of hallucination in Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2.SS2.SSS1" title="In II-B Causes of hallucination in Language Models ‣ II Background ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Hallucination from Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2.SS2.SSS2" title="In II-B Causes of hallucination in Language Models ‣ II Background ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Hallucination from Model Training and Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S2.SS2.SSS3" title="In II-B Causes of hallucination in Language Models ‣ II Background ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span>Snowball Hallucination</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Approach</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS1" title="In III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Research Questions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS2" title="In III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Search Strategy</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS2.SSS1" title="In III-B Search Strategy ‣ III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Manual Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS2.SSS2" title="In III-B Search Strategy ‣ III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Search String Formation and Automated Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS2.SSS3" title="In III-B Search Strategy ‣ III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>3 </span>Literature Review and Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S3.SS2.SSS4" title="In III-B Search Strategy ‣ III Approach ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>4 </span>Inclusion of Recent Literature</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization?</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS1" title="In IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Comprehensive Literature Reviews of Hallucination Mitigation Techniques</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2" title="In IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Retrieval-Augmented Generation (RAG)</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS1" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>Comprehensive Overview of RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS2" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>RAG Before Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS3" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>3 </span>RAG During Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS4" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>4 </span>RAG After Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS5" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>5 </span>RAG End-to-End</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS2.SSS6" title="In IV-B Retrieval-Augmented Generation (RAG) ‣ IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>6 </span>Optimizing Fidelity and Utilization in RAG Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS3" title="In IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Other Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S4.SS4" title="In IV RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Insights and Future Directions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S5" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">RQ2: How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization?</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S5.SS1" title="In V RQ2: How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Benchmark and Literature Review Relating to Medical Domain</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S5.SS2" title="In V RQ2: How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Hallucianation Mitigation Techniques in Medical Domain</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S5.SS3" title="In V RQ2: How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization? ‣ Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Insights and Future Directions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#S6" title="In Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Duy Khoa Pham
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Department of Computing Technologies</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Swinburne University of Technology</span>
<br class="ltx_break"/>Melbourne, Australia 
<br class="ltx_break"/>103515617@student.swin.edu.au
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bao Quoc Vo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Department of Computing Technologies</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">Swinburne University of Technology</span>
<br class="ltx_break"/>Melbourne, Australia 
<br class="ltx_break"/>bvo@swin.edu.au
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">The rapid advancement of large language models (LLMs) has significantly impacted various domains, including healthcare and biomedicine. However, the phenomenon of hallucination, where LLMs generate outputs that deviate from factual accuracy or context, poses a critical challenge, especially in high-stakes domains. This paper conducts a scoping study of existing techniques for mitigating hallucinations in knowledge-based task in general and especially for medical domains. Key methods covered in the paper include Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback loops, supervised fine-tuning, and prompt engineering. These techniques, while promising in general contexts, require further adaptation and optimization for the medical domain due to its unique demands for up-to-date, specialized knowledge and strict adherence to medical guidelines. Addressing these challenges is crucial for developing trustworthy AI systems that enhance clinical decision-making and patient safety as well as accuracy of biomedical scientific research.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large Language Models (LLMs), Hallucination mitigation, Knowledge-Based tasks, Medical domains

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have opened up new avenues for their application in knowledge-intensive tasks including medical domain. However, hallucinations, which outputs that deviate from factual accuracy, present a critical challenge, especially in high-stakes domains like healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib31" title="">31</a>]</cite>, where accuracy and reliability are paramount <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib27" title="">27</a>]</cite>. Hallucinations in healthcare AI systems can lead to severe consequences, including incorrect clinical decision-making, delayed or improper treatment, and compromised patient safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address this critical issue, researchers have explored various techniques to mitigate hallucinations in knowledge-based tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib10" title="">10</a>]</cite>, such as Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib29" title="">29</a>]</cite>, self-refinement via iterative feedback loops <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib16" title="">16</a>]</cite>, and supervised fine-tuning on factual data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib32" title="">32</a>]</cite>. While these techniques have shown promise in general domains, their effectiveness in the healthcare and biomedical contexts remains understudied, with only a few papers examining the issue. The medical domain presents unique challenges, including the need for up-to-date and specialized knowledge, strict adherence to established medical guidelines, and a deep understanding of the complex interplay between medical concepts and patient health outcomes.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper presents a scoping study of hallucination mitigation techniques, assessing their effectiveness in improving accuracy and reliability for QA and summarization tasks, with a focus on adapting these methods to the medical domain. By examining the breadth of research in this critical area, we aim to enhance understanding of current LLM limitations in healthcare applications and provide insights into addressing hallucinations. This study contributes to the advancement of LLMs in healthcare AI, supporting the development of more trustworthy and reliable systems for clinical decision support and biomedical research.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Definition of Hallucination</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the context of language models, particularly those trained for tasks like question answering, translation, summarization, and dialogue systems, “hallucination” refers to the generation of content that is either unfaithful or irrelevant to the given input or context. Maynez et al.’s seminal paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib21" title="">21</a>]</cite> has categorized hallucination into two types: <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">intrinsic hallucination</em> where the text generated by the model contradicts the facts or data provided in the input and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">extrinsic hallucination</em>, where the generation of information that cannot be verified or contradicted by the source input. Recently, Zhang et al.’s influential survey on hallucination in LLMs have provided a more granular categorization of hallucination within the context of LLMs  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib45" title="">45</a>]</cite> with three primary types of hallucination:</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">Input Conflicting:</span> The model’s response deviates from the user’s input (including task instructions and input).</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">Context Conflicting:</span> Generated content contradicts itself or previously generated content.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">Fact Conflicting:</span> Generated content contradicts established world knowledge, posing significant challenges in real-world applications, especially in high-stakes domains like healthcare.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">In medical domain, Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib11" title="">11</a>]</cite> classified problems with answers in medical QA tasks into three categories:
(i) <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.1">Fact Inconsistency</span>: Providing information inconsistent with established facts.
(ii) <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.2">Query Inconsistency:</span> Providing information unrelated to the user’s query.
(iii) <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.3">Tangentiality:</span> Providing related but not directly answering the query.
Among these, fact inconsistency is the most challenging and common form of hallucination in medical domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Causes of hallucination in Language Models</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Hallucinations in language models can critically undermine their utility, leading to outputs that are unfaithful or irrelevant to the input. This section explores the root causes of hallucination in language models.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.4.1.1">II-B</span>1 </span>Hallucination from Dataset</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The main reason for hallucination comes from the data source used to train the model. There are several factors for this issue:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p2.1.1">Data Collection Flaws:</span> Hallucinations frequently originate from flaws in data collection methods. In large-scale dataset construction, heuristic methods are frequently used to pair real sentences or tables as source and target <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib14" title="">14</a>]</cite>. However, this approach can introduce significant discrepancies. In particular, information in the target can be not verified by the source which is the factual data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib25" title="">25</a>]</cite>. This discrepancy means that the target reference may contain unsupported details, leading to hallucinations when the model generates text. Such mismatches between the source and target data introduce inconsistencies that models learn and replicate, resulting in hallucinated outputs.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p3.1.1">Duplicate Data:</span> When duplicates are not effectively removed from the dataset, the model may overfit to these repeated instances. This leads to a model that is prone to generate repetitive or overly similar outputs across different inputs, contributing to hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p4.1.1">Inherent Task Misalignment:</span> Certain language generation tasks are inherently misaligned with the objective of factual accuracy due to their very nature. Tasks such as creative writing or generating speculative content are not necessarily bound by strict factual constraints. This fundamental lack of alignment with truthfulness can condition models to become more tolerant of deviating from factual accuracy, thereby increasing the likelihood of hallucinations. For example, models may learn from the behavior of some authors who state claims based on their feelings or personal perspectives rather than verifiable facts. This unverified input data teaches the model that such subjective assertions can be considered “correct”. Consequently, the model may learn these behaviors, potentially causing it to extrinsically hallucinate.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.4.1.1">II-B</span>2 </span>Hallucination from Model Training and Inference</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Even with minimal factual divergence in the training data, hallucinations can still occur during generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib25" title="">25</a>]</cite>. This suggests that other factors related to the model’s training or the learning algorithm itself contribute to the production of hallucination:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p2.1.1">Encoder Limitations:</span> If the encoder component of a model does not effectively understand or encode the nuances of the input data, it may fail to capture important details that are crucial for generating accurate output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib25" title="">25</a>]</cite>. This could lead to the generation of irrelevant or incorrect information (hallucinations) compared to the input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p3.1.1">Decoding Problems:</span> There are primarily two reasons for hallucinations in language models: decoder misalignment and flaws in decoding strategies. Regarding decoder misalignment, the decoder can attend to the wrong part of the encoded input source, leading to erroneous generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib33" title="">33</a>]</cite>. This wrong association results in generated text with facts mixed up between similar entities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib6" title="">6</a>]</cite>. As for decoding strategy flaws, the design of the decoding strategy itself can contribute to hallucinations. Decoding strategies that improve generation diversity, such as top-k sampling, are positively correlated with increased hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib6" title="">6</a>]</cite>. The deliberate introduction of “randomness” by sampling from the top-k samples instead of choosing the most probable token may increase the unexpected nature of the generation, leading to a higher chance of containing hallucinated content.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p4">
<p class="ltx_p" id="S2.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p4.1.1">Parametric Knowledge Bias:</span> Large language models accumulate a broad base of “knowledge” encoded within their parameters during the pre-training process on massive datasets. While this parametric knowledge aids performance on downstream tasks, it also contributes to hallucinatory generations. The powerful generalisation ability of large pre-trained models for natural language generation comes at the cost of prioritizing their parametric knowledge over strictly adhering to provided input prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib19" title="">19</a>]</cite>. In other words, models favoring outputs aligned with their internal knowledge base rather than the specific input information are more prone to hallucinating excess or inconsistent content.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.4.1.1">II-B</span>3 </span>Snowball Hallucination</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">One phenomenon that can contribute to hallucinations in language models is known as “snowball hallucination” which is thoroughly examined by Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib44" title="">44</a>]</cite>. The paper investigates how initial minor inaccuracies in language models can lead to significant compounded errors, relating to discussion on mitigating hallucinations in QA tasks.
The paper questions whether knowledge gaps are the sole source of hallucination in LMs. The hypothesis is that LMs are prone to “hallucination snowballing”, a phenomenon where initial minor inaccuracies lead to significant compounded errors. This occurs because instruction-tuned LMs often commit to an answer in the first token and continue to provide a supportive explanation, regardless of the correctness of the initial answer. The paper identifies two key findings:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i1.p1.1.1">Initial Committal:</span> LMs, especially those tuned for instructions, tend to generate an answer before providing an explanation due to the design of instruction data. This design leads models to commit to a binary answer (e.g., Yes/No), and the subsequent explanation supports this answer to maintain coherence, even if the initial answer is wrong.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.1">Inherently Sequential Processing:</span> Transformer-based models process inputs and generate outputs one token at a time, making them ill-suited for tasks requiring multi-step reasoning. This limitation forces the model to provide answers to complex questions in a single step, increasing the likelihood of generating an incorrect answer followed by an incorrect justification.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">The study designed three datasets—Primality Test, Senator Search, and Graph Connectivity—to induce hallucination snowballing with yes/no questions that cannot be answered in a single step. The findings revealed that ChatGPT (2022) and GPT-4 (2023) gave correct yes/no answers 39.87% and 16.6% of the time, respectively. Despite their ability to detect incorrect claims (67.37% for ChatGPT and 87.03% for GPT-4), they often still generated wrong answers, demonstrating the existence of hallucination snowballing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib44" title="">44</a>]</cite>. When the prompt included “Let’s think step by step,” error rates were significantly reduced, showing that encouraging step-by-step reasoning helps mitigate snowball hallucinations.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Approach</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Research Questions</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To address the critical issue of hallucinations in large language models for knowledge-based tasks in the medical domain, this study aims to systematically investigate the following research questions:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ1:</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ2:</span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization?</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Search Strategy</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To conduct this scoping study, we utilize both manual and automated search techniques to ensure a comprehensive coverage of relevant literature.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>Manual Search</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The manual search process involves identifying relevant papers based on keywords derived from the research questions. These initial papers serve as a foundation for extracting search strings, which are subsequently used in the automated search process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.4.1.1">III-B</span>2 </span>Search String Formation and Automated Search</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">The search strings are constructed by extracting relevant terms from the manually identified papers. These search strings aim to encompass keywords pertinent to the research questions. Utilizing the formulated search strings, an automated search is conducted across various databases and scholarly repositories. This automated search process is followed by a snowballing approach, which involves examining the reference lists and citations of the initially retrieved papers to identify additional relevant literature.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS3.4.1.1">III-B</span>3 </span>Literature Review and Filtering</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">After obtaining the search results, the next step involves a thorough literature review process. This process involves extracting and documenting relevant information from each paper, such as the publication year, journal or conference, authors, keywords, mitigation techniques employed, problem addressed, key contributions, technical metrics or criteria used, and any identified issues or limitations. Subsequently, the collected papers are filtered to align with the specific research questions of the study.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS4.4.1.1">III-B</span>4 </span>Inclusion of Recent Literature</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">To ensure that the study incorporates the latest advancements and developments in the field, the search strategy also involves updating the literature review with recently published papers from open-access repositories such as arXiv. This step ensures that the systematic literature review remains current and captures the most recent research findings and perspectives.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">RQ1: How effective are current hallucination mitigation techniques for knowledge-based tasks such as QA and summarization?</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Comprehensive Literature Reviews of Hallucination Mitigation Techniques</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Many papers have focused on exploring and analysing various techniques for mitigating hallucination in LLMs. Among them, Tonmoy et al.’s survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib34" title="">34</a>]</cite> stands out by comprehensively categorizing 32 different techniques aimed at hallucination mitigation. This systematic survey introduces a taxonomy of hallucination mitigation techniques for LLMs by categorizing them into distinct phases and methodologies such as Retrieval-Augmented Generation, Prompt Engineering, and Developing Models. Furthermore, the taxonomy illustrates the temporal aspects of RAG techniques, such as “Before Generation”, “During Generation”, “After Generation”, and “End-to-End”, which provides insights into the stages at which these techniques are employed during the generation process. Inspired by this taxonomy, we further explore these techniques with a focus on knowledge-based tasks like QA or summarization, especially for medical domain.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Another significant contribution to the literature by Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib10" title="">10</a>]</cite> provides a comprehensive overview of the different types of hallucinations, categorized as intrinsic and extrinsic based on previous work. It delves into the causes of these hallucinations and explores various strategies to mitigate them. This paper serves as a foundational reference for understanding the diverse types of hallucinations and their implications in natural language generation. The insights gained from this survey are crucial for developing targeted mitigation strategies, especially in applications within the medical domain, where the accuracy and reliability of information are of paramount importance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Retrieval-Augmented Generation (RAG)</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">RAG has emerged as a promising technique to enhance the performance and reliability of LLMs by incorporating external knowledge during the text generation process. This approach addresses the limitations of LLMs in generating accurate and contextually relevant information, particularly in knowledge-intensive tasks such as medical question answering (QA) and summarization. The following sections provide a overview of RAG and discuss various studies categorized by the phase of RAG they try to enhance: before generation, during generation, after generation, and end-to-end training.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.4.1.1">IV-B</span>1 </span>Comprehensive Overview of RAG</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">RAG involves the integration of retrieval mechanisms with generative models to improve the quality of generated text. By fetching relevant information from external sources, RAG systems can ground their responses in factual data, thereby reducing the incidence of hallucinations. A recent survey by Hu and Lu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib9" title="">9</a>]</cite> presents an overview on Retrieval-Augmented Language Models (RALMs) which incorporate external information retrieval to enhance the capabilities of large language models across various NLP tasks. It provides a comprehensive survey of both RAG and RAU methods, detailing how these approaches improve the performance and reliability of language models in processing natural language. It gives detailed summary about aspect of RALMs such as: definition, LM, retriever, enhancement, application and so on.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.4.1.1">IV-B</span>2 </span>RAG Before Generation</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib26" title="">26</a>]</cite> explore the concept of generation-augmented retrieval for open-domain question answering (QA). This approach utilizes retrieved knowledge during the generation process, aiming to enhance the accuracy and reliability of the outputs produced by large language models.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib35" title="">35</a>]</cite>, Vu et al. investigate the potential of refreshing and updating the knowledge base of large language models with search engine data. This process aims to significantly improve the models’ accuracy and relevance. A key contribution of this study is the development of a practical approach for keeping medical models up-to-date with the latest research and clinical guidelines, which could potentially reduce the incidence of hallucinations in these models.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS3.4.1.1">IV-B</span>3 </span>RAG During Generation</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib4" title="">4</a>]</cite> propose a novel multi-stage question decomposition framework that leverages RAG to systematically refine the information retrieval process. The key innovation lies in ensuring that each stage of the question decomposition focuses on extracting only the most relevant and reliable information from external sources. This approach aims to constrain the reasoning process, preventing the inclusion of erroneous or irrelevant data, which is particularly critical for reducing hallucinations in QA systems. The stringent accuracy requirements in healthcare necessitate precise and trustworthy information, making this method a promising solution for mitigating hallucinations and enhancing the reliability of AI-powered decision support systems in healthcare.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib12" title="">12</a>]</cite> introduce a novel approach called EVER, which goes beyond traditional RAG methods. EVER employs a real-time validation and rectification process to address inaccuracies during the generation phase, aiming to enhance the reliability of language model outputs.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib20" title="">20</a>]</cite>, Mao et al. investigate the integration of generative models with retrieval processes to improve the depth and relevance of information accessed during question-answering tasks. The key contribution of this approach is that by augmenting retrieval with generation, it not only expands the pool of available information for answering questions but also ensures that the retrieved information is highly contextualized, thereby reducing the likelihood of hallucinations caused by out-of-context data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS4.4.1.1">IV-B</span>4 </span>RAG After Generation</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Rawte et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib29" title="">29</a>]</cite> introduce a unique framework for quantifying hallucinations in language models. It proposes both qualitative and quantitative methods for assessing hallucinations, emphasizing the need for a nuanced understanding of different hallucination types. Specifically, it categorizes hallucinations into intrinsic and extrinsic types, and further divides them into mild, moderate, and severe categories, each with specific implications and remediation strategies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS5.4.1.1">IV-B</span>5 </span>RAG End-to-End</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib17" title="">17</a>]</cite>, Lewis et al. investigate the integration of parametric memory (model knowledge) with non-parametric memory (external documents) using a RAG-based approach for knowledge-intensive NLP tasks like open-domain question answering. This method employs end-to-end training, where a retriever fetches relevant documents from a large external dataset, and a generator produces the final output based on both the input query and the retrieved documents, designed to work seamlessly together.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS6.4.1.1">IV-B</span>6 </span>Optimizing Fidelity and Utilization in RAG Models</h4>
<div class="ltx_para" id="S4.SS2.SSS6.p1">
<p class="ltx_p" id="S4.SS2.SSS6.p1.1">Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib37" title="">37</a>]</cite> analyze the effectiveness of RAG models in maintaining fidelity to source information, particularly examining how the internal biases of LLMs can affect the accuracy of retrieved data. It provides a quantitative analysis of the conflicts between the generative tendencies of LLMs and the corrective capabilities of RAG. The key contribution is offering insights into optimizing RAG to prioritize factual accuracy, which is essential for medical applications where misinformation can have severe consequences. The study emphasizes the need to balance the generative and retrieval aspects to enhance model reliability in sensitive healthcare environments.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS6.p2">
<p class="ltx_p" id="S4.SS2.SSS6.p2.1">To address the aforementioned concerns, Labruna et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib13" title="">13</a>]</cite> introduce ADAPT-LLM, a model trained to dynamically decide whether to use its parametric knowledge or seek external information via information retrieval (IR). The design rationale describes the necessity for LLMs to identify when to rely on stored knowledge versus when to retrieve additional information. ADAPT-LLM is trained to generate a special token <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS6.p2.1.1">&lt;RET&gt;</span> signaling the need for external data to improve question-answering capabilities. A key contribution is to demonstrate the feasibility of teaching LLMs to efficiently use external sources only when necessary.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Other Methods</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Si et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib30" title="">30</a>]</cite> focus on improving GPT-3’s performance through targeted prompting strategies, this study explores how effectively tailored prompts can reduce hallucinations and increase reliability across various tasks. By refining the prompt engineering process, the researchers were able to significantly enhance the model’s response quality, which is particularly vital in high-stakes environments where precise information is paramount. This method provides a scalable way to improve the utility and accuracy of language models in real-world applications, ensuring that they produce dependable and contextually appropriate outputs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Lei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib16" title="">16</a>]</cite> introduce a method that utilizes a chain of natural language inference tasks to verify each step of text generation, ensuring that every link in the chain is grounded in factual accuracy. This systematic verification significantly reduces the incidence of ungrounded hallucinations, where the model generates baseless claims. Such an approach is indispensable in domains where the veracity of information is crucial, such as in medical or legal applications, providing a robust framework for ensuring the reliability of automated text generation systems.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The UPRISE study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib5" title="">5</a>]</cite> enhances zero-shot task performance by utilizing dynamically retrieved prompts that guide the language model’s generation process. This innovative approach to prompt engineering uses a retrieval system to select the most effective prompts dynamically, improving the model’s performance and reducing the risk of hallucinatory outputs. By aligning prompt selection with specific task requirements, this method significantly bolsters the model’s ability to generate accurate and contextually relevant responses, showcasing a promising direction for enhancing the adaptability and effectiveness of language models in diverse applications.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">In the Inference-Time Intervention study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib18" title="">18</a>]</cite>, a novel decoding strategy is proposed that involves real-time interventions during the model’s inference process, guiding it towards more truthful and accurate outputs. This method is crucial for applications where precision is necessary, as it ensures the integrity of the information generated by the model. By focusing on veracity at the time of inference, this approach addresses the prevalent issue of hallucinations, enhancing the trustworthiness and reliability of responses in sectors such as healthcare and finance.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">The R-Tuning research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib43" title="">43</a>]</cite> explores techniques to train language models to recognize their limitations and refuse to answer when they lack sufficient information. By training models to refuse to respond rather than fabricating answers, the study aims to enhance the trustworthiness of AI systems, particularly in medical scenarios where incorrect information can result in detrimental outcomes. This refusal skill is a critical development towards creating more reliable and ethical AI systems, ensuring that they contribute positively in high-stakes environments.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">Large Language Model unlearning by Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib42" title="">42</a>]</cite> is an emerging technique aimed at addressing and rectifying specific undesirable behaviors in LLMs by effectively ”forgetting” or suppressing them. This innovative approach offers a promising solution for mitigating hallucinations by enabling targeted refinement of model outputs without necessitating comprehensive retraining. Unlearning stands out for its efficiency, requiring primarily negative examples, which makes it a cost-effective alternative to more resource-intensive methods like reinforcement learning from human feedback (RLHF).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Insights and Future Directions</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">Importance of Data Quality and Source Authority for RAG:</span> The reliability of external data sources is paramount for minimizing hallucinations for RAG. Models need to prioritize authoritative sources to ensure the generated content is accurate. Training models to recognize and validate the authority of these sources can significantly reduce the incidence of hallucinations. High-quality data should be clean, up-to-date, and relevant to the specific domain, particularly in sensitive areas like biomedicine where accuracy is critical. Future research should focus on developing more sophisticated retrieval mechanisms aiming at better prioritize authoritative sources and high-quality data.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p2.1.1">Dynamic Retrieval Decisions:</span> Models that can dynamically decide when to use internal knowledge versus external retrieval show potential promise. This decision-making process is crucial for maintaining the accuracy and relevance of the generated responses. Further exploration of techniques like ADAPT-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib13" title="">13</a>]</cite>, which train models to signal the need for external data dynamically, can improve the model’s ability to provide accurate answers by leveraging both internal and external knowledge effectively.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p3.1.1">Real-Time Validation and Rectification:</span> Real-time updates to the model’s knowledge base can significantly enhance its accuracy. Techniques that enable models to refresh their knowledge with the latest information are crucial, especially in fields like biomedicine where new research and guidelines are constantly emerging. Techniques like EVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib12" title="">12</a>]</cite> employ real-time checks to correct inaccuracies during the generation phase. More research is needed to develop efficient real-time validation mechanisms without significantly impacting model performance.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">Conflict Resolution in Pre-Trained Models:</span> Resolving conflicts between pre-trained model knowledge and retrieved information remains a challenge. Effective strategies are needed to resolve these conflicts, ensuring that the model can accurately decide when to rely on internal knowledge and when to seek external validation. Future work should explore methods to effectively integrate and prioritize different knowledge sources.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p5.1.1">Iterative Feedback and Self-Refinement:</span> Self-refinement techniques involving iterative feedback loops enable models to continuously evaluate and improve their outputs. A promising recent approach, such as Meta-Rewarding LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib38" title="">38</a>]</cite>, employs a three-role system—actor, judge, and meta-judge—which shows significant potential and may serve as an alternative to RLHF.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.p6.1.1">Ensemble Learning:</span> Ensemble methods, such as combining multiple models, can help mitigate individual biases and improve overall accuracy. Techniques like the LLM-Synergy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib41" title="">41</a>]</cite> leverage ensemble learning to enhance performance on medical QA systems. By combining different models and leveraging the strengths of each, ensemble methods can significantly reduce hallucinations in the medical domain. Researching deeper into ensemble learning for hallucination reduction involves exploring how to effectively integrate various models, each excelling in different aspects, to produce more reliable and accurate outputs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">RQ2: How effective are hallucination mitigation techniques in improving the accuracy and reliability of medical QA and summarization?</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Benchmark and Literature Review Relating to Medical Domain</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">BioMedLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib3" title="">3</a>]</cite> is a comprehensive language model consisting of 2.7 billion parameters, trained exclusively on biomedical texts from PubMed. This extensive training enables BioMedLM to effectively address the specific needs of the medical domain, making it a robust tool for researchers and practitioners. A key contribution of BioMedLM is its availability on platforms like Hugging Face, which makes it accessible and suitable for various biomedical research applications, providing a valuable resource for advancing medical research and applications.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib36" title="">36</a>]</cite> review the advancements in the use of pre-trained language models for biomedical applications, highlighting progress made while identifying existing gaps in current methodologies. The survey offers a broad overview of the landscape, providing essential context for applying specific hallucination mitigation techniques effectively. This comprehensive review is crucial for developing strategies to enhance the accuracy and reliability of biomedical language models, thus improving their practical utility in the medical field.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Med-HALT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib24" title="">24</a>]</cite> introduces a new benchmark dataset designed to evaluate medical hallucinations in large language models, covering a wide range of medical knowledge and reasoning/memory-based tests. The dataset aims to address the need for domain-specific hallucination evaluation in the medical field, as existing solutions are often too general. By reducing the reliance on human annotation and assessing both hallucinations and factual accuracy, Med-HALT provides a more efficient and targeted evaluation method tailored to medical contexts.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">This literature review by Wubineh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib39" title="">39</a>]</cite> systematically examines the ethical, social, privacy, and technological aspects of adopting artificial intelligence in healthcare. It identifies critical challenges, including data privacy concerns, bias mitigation needs, a lack of awareness about AI’s benefits and limitations, transparency issues with algorithms, and potential over-reliance on AI. Additionally, the review highlights opportunities such as enhanced decision support, technological advancements in diagnostics and drug development, and improved patient monitoring through AI technologies. These insights are essential for understanding the complex landscape of AI implementation in healthcare.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib8" title="">8</a>]</cite> study explores the use of ChatGPT (GPT-3.5 model) for identifying correct and incorrect drug-disease associations through prompting. The findings indicate that ChatGPT achieved an accuracy range of 74.6-83.5% for true associations and 96.2-97.6% for false ones. Providing additional disease context through prompts improved the accuracy of true associations but slightly decreased the accuracy for false associations. While ChatGPT performed better at ruling out incorrect associations, limitations include hallucinations and a lack of the latest biomedical knowledge, highlighting areas for improvement in biomedical information retrieval.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">Pushpanathan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib28" title="">28</a>]</cite> study evaluation for the performance of ChatGPT-3.5, ChatGPT-4.0, and Google Bard in accurately and comprehensively answering 37 common questions about eye and vision symptoms. The results show that ChatGPT-4.0 outperformed ChatGPT-3.5 and Google Bard in terms of accuracy, with all three models demonstrating high comprehensiveness scores for good responses. Notably, ChatGPT-3.5 frequently disclaimed its responses, while ChatGPT-4.0 and Bard often asserted accuracy even when incorrect, leading to hallucinations. This study underscores that although current LLMs have significant capabilities, they cannot fully replace the expertise of ophthalmologists.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Hallucianation Mitigation Techniques in Medical Domain</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib11" title="">11</a>]</cite> utilize iterative feedback loops to refine the accuracy of medical QA systems, focusing on reducing hallucinations through self-assessment and adjustment. This approach demonstrates a practical application of self-refinement techniques in the medical domain, where accuracy is critical for patient outcomes. By continuously evaluating and adjusting the model’s outputs, the technique aims to enhance the reliability and factual correctness of generated responses.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Another study by Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib32" title="">32</a>]</cite> explores supervised fine-tuning methods to improve the factual accuracy of language models, particularly for biography generation and medical QA tasks. The study employs automated factuality preference ranking and direct preference optimization (DPO) to train models without human labels, using the FactScore metric to evaluate the results. This fine-tuning process ensures that the models generate more factual and reliable content, which is vital for applications in the medical field.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">The paper by Ahmad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib1" title="">1</a>]</cite> discusses various methods for evaluating and measuring the trustworthiness of LLMs in healthcare, including both human and automated evaluation techniques. It emphasizes the need for robust regulatory and evaluation frameworks to ensure the reliability of AI systems in sensitive domains. The insights provided are crucial for developing and deploying AI technologies that adhere to principles of transparency, non-biased, and ethical considerations in healthcare.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib41" title="">41</a>]</cite> propose the LLM-Synergy pipeline, which combines multiple large language models using ensemble methods like Boosting-based Weighted Majority Vote and Cluster-based Dynamic Model Selection. This approach improves performance by mitigating individual model biases and weaknesses, offering scalability, flexibility, and computational efficiency. The ensemble method has shown superior performance on datasets like MedMCQA, PubMedQA, and MedQA-USMLE, making it a promising solution for enhancing the accuracy of medical QA systems.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib40" title="">40</a>]</cite> introduce a series of dense retrievers aimed at improving biomedical retrieval performance. The development of effective biomedical retrieval models faces challenges due to limited publicly annotated data and computational resources. BMRETRIEVER leverages a two-stage training process to address these challenges. The first stage involves unsupervised contrastive pre-training on large biomedical corpora, utilizing extensive and diverse biomedical data. This stage employs techniques for constructing positive and negative query-passage pairs to enhance the model’s ability to discern relevant information from irrelevant data. The second stage involves instruction fine-tuning using labeled datasets, where high-quality labeled datasets and synthetic data generation are used for fine-tuning. This process significantly impacts the model’s understanding and performance in biomedical retrieval tasks. Extensive experiments across multiple tasks and datasets demonstrate BMRETRIEVER’s efficiency and effectiveness, achieving superior performance compared to larger models with fewer parameters. This approach marks a significant advancement in the field of biomedical text retrieval, offering a robust solution to the challenges of limited annotated data and computational constraints.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">Insights and Future Directions</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">Medical Domain-Specific Challenges:</span> The medical domain presents unique challenges including the need for up-to-date and specialized knowledge, strict adherence to established medical guidelines, and a deep understanding of complex medical concepts. Hallucinations in this context can have severe consequences, including incorrect clinical decisions and compromised patient safety, making it imperative to develop robust mitigation techniques.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">Evaluation Metrics and Benchmarks:</span> Developing robust evaluation metrics and benchmarks specific to the medical domain is essential. These tools help assess the effectiveness of different hallucination mitigation techniques and guide improvements. Further research should explore more medical benchmarks like Med-HALT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib24" title="">24</a>]</cite>. Such domain-specific benchmarks evaluate medical hallucinations, reducing reliance on human annotation while focusing on factual accuracy and reliability.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1">Effectiveness of RAG in Medical QA:</span> RAG has shown promise in improving the accuracy and reliability of medical QA systems by grounding generated content in authoritative external sources. However, the effectiveness of RAG depends heavily on the quality and relevance of the retrieved data. Techniques like “FreshLLMs” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib35" title="">35</a>]</cite> that use search engine augmentation to keep models updated with the latest research can significantly enhance the performance of medical QA systems.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.1">Open Domain vs. Specific Domain Training:</span> The trade-offs between fine-tuning open-domain models for specific tasks and training domain-specific models from scratch require further investigation. Open-Domain models can offer flexibility and adaptability, allowing them to be fine-tuned for various tasks. Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib32" title="">32</a>]</cite> demonstrated the effectiveness of this approach in reducing hallucinations in the medical domain. Conversely, domain-specific models can provide higher initial accuracy for specialized tasks like BioMedLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib3" title="">3</a>]</cite> but may lack flexibility and adaptability to new data or tasks. Understanding the trade-offs between these approaches is essential for optimizing the training and deployment of models in the medical domain.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1"><span class="ltx_text ltx_font_italic" id="S5.SS3.p5.1.1">Iterative Feedback and Continuous Improvement:</span> Iterative feedback loops and real-time validation techniques are crucial for maintaining the accuracy and reliability of medical QA systems. Self-refinement methods, as demonstrated in studies by Ji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib11" title="">11</a>]</cite> or Niu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13808v1#bib.bib23" title="">23</a>]</cite> show significant potential in enhancing the reliability of generated responses. Continuous improvement and adaptation are necessary to keep up with the rapidly evolving medical knowledge, ensuring that the models remain relevant and accurate.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In conclusion, this scoping study has highlighted the essence of hallucination mitigation in LLMs, particularly for knowledge-intensive tasks in medical domain. While techniques like RAG, self-refinement and unlearning show promise, significant challenges remain. These include the need for high-quality, domain-specific data sources, robust evaluation metrics, and methods to handle the unique complexities of medical information. Future research should focus on refining these techniques, developing more sophisticated real-time validation mechanisms, and addressing the ethical implications of AI in healthcare. Ultimately, the goal is to create AI systems that can reliably support clinical decision-making and enhance patient care, while maintaining the highest standards of accuracy and safety.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Muhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy.

</span>
<span class="ltx_bibblock">Creating trustworthy llms: Dealing with hallucinations in healthcare ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2311.01463</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, and Ryan McDonald.

</span>
<span class="ltx_bibblock">Focus attention: Promoting faithfulness and diversity in summarization.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 6078–6095, Online, August 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et al.

</span>
<span class="ltx_bibblock">Biomedlm: A 2.7 b parameter language model trained on biomedical text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2403.18421</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Hejing Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, and Dongyan Zhao.

</span>
<span class="ltx_bibblock">A step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2311.07491</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang.

</span>
<span class="ltx_bibblock">UPRISE: Universal prompt retrieval for improving zero-shot evaluation.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 12318–12337, Singapore, December 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose.

</span>
<span class="ltx_bibblock">Neural path hunter: Reducing hallucination in dialogue systems via path grounding.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 2197–2214, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yang Feng, Wanying Xie, Shuhao Gu, Chenze Shao, Wen Zhang, Zhengxin Yang, and Dong Yu.

</span>
<span class="ltx_bibblock">Modeling fluency and faithfulness for diverse neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 34(01):59–66, Apr. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Zhenxiang Gao, Lingyao Li, Siyuan Ma, Qinyong Wang, Libby Hemphill, and Rong Xu.

</span>
<span class="ltx_bibblock">Examining the potential of chatgpt on biomedical information retrieval: Fact-checking drug-disease associations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Annals of Biomedical Engineering</span>, pages 1–9, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yucheng Hu and Yuxing Lu.

</span>
<span class="ltx_bibblock">Rag and rau: A survey on retrieval-augmented language model in natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2404.19543</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">ACM Computing Surveys</span>, 55(12):1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.

</span>
<span class="ltx_bibblock">Towards mitigating LLM hallucination via self reflection.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pages 1827–1843, Singapore, December 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Haoqiang Kang, Juntong Ni, and Huaxiu Yao.

</span>
<span class="ltx_bibblock">Ever: Mitigating hallucination in large language models through real-time verification and rectification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2311.09114</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tiziano Labruna, Jon Ander Campos, and Gorka Azkune.

</span>
<span class="ltx_bibblock">When to retrieve: Teaching llms to utilize information retrieval effectively.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2404.19705</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rémi Lebret, David Grangier, and Michael Auli.

</span>
<span class="ltx_bibblock">Neural text generation from structured data with application to the biography domain.

</span>
<span class="ltx_bibblock">In Jian Su, Kevin Duh, and Xavier Carreras, editors, <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</span>, pages 1203–1213, Austin, Texas, November 2016. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 8424–8445, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al.

</span>
<span class="ltx_bibblock">Chain of natural language inference for reducing large language model ungrounded hallucinations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2310.03951</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</span>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.

</span>
<span class="ltx_bibblock">Inference-time intervention: Eliciting truthful answers from a language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.

</span>
<span class="ltx_bibblock">Entity-based knowledge conflicts in question answering.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 7052–7063. Association for Computational Linguistics, November 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Generation-augmented retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 4089–4100, Online, August 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald.

</span>
<span class="ltx_bibblock">On faithfulness and factuality in abstractive summarization.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 1906–1919, Online, July 2020. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Jessica Morley, Caio C.V. Machado, Christopher Burr, Josh Cowls, Indra Joshi, Mariarosaria Taddeo, and Luciano Floridi.

</span>
<span class="ltx_bibblock">The ethics of ai in health care: A mapping review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Social Science &amp; Medicine</span>, 260:113172, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, and Fan Mo.

</span>
<span class="ltx_bibblock">Mitigating hallucinations in large language models via self-refinement-enhanced knowledge retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2405.06545</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">Med-halt: Medical domain hallucination test for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das.

</span>
<span class="ltx_bibblock">ToTTo: A controlled table-to-text generation dataset.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 1173–1186, Online, November 2020. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al.

</span>
<span class="ltx_bibblock">Check your facts and try again: Improving large language models with external knowledge and automated feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2302.12813</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Patrik Puchert, Poonam Poonam, Christian van Onzenoodt, and Timo Ropinski.

</span>
<span class="ltx_bibblock">Llmmaps–a visual metaphor for stratified evaluation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2304.00457</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Krithi Pushpanathan, Zhi Wei Lim, Samantha Min Er Yew, David Ziyou Chen, Hazel Anne Hui’En Lin, Jocelyn Hui Lin Goh, Wendy Meihua Wong, Xiaofei Wang, Marcus Chun Jin Tan, Victor Teck Chang Koh, et al.

</span>
<span class="ltx_bibblock">Popular large language model chatbots’ accuracy, comprehensiveness, and self-awareness in answering ocular symptom queries.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Iscience</span>, 26(11), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, and Amitava Das.

</span>
<span class="ltx_bibblock">The troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 2541–2573, Singapore, December 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Prompting gpt-3 to be reliable.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2210.09150</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al.

</span>
<span class="ltx_bibblock">Trustllm: Trustworthiness in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2401.05561</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Fine-tuning language models for factuality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2311.08401</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P Parikh.

</span>
<span class="ltx_bibblock">Sticking to the facts: Confident decoding for faithful data-to-text generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1910.08684</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das.

</span>
<span class="ltx_bibblock">A comprehensive survey of hallucination mitigation techniques in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2401.01313</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al.

</span>
<span class="ltx_bibblock">Freshllms: Refreshing large language models with search engine augmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2310.03214</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, and Jie Fu.

</span>
<span class="ltx_bibblock">Pre-trained language models in biomedical domain: A systematic survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">ACM Computing Surveys</span>, 56(3):1–52, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Kevin Wu, Eric Wu, and James Zou.

</span>
<span class="ltx_bibblock">How faithful are rag models? quantifying the tug-of-war between rag and llms’ internal prior.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2404.10198</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar.

</span>
<span class="ltx_bibblock">Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2407.19594</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Betelhem Zewdu Wubineh, Fitsum Gizachew Deriba, and Michael Melese Woldeyohannis.

</span>
<span class="ltx_bibblock">Exploring the opportunities and challenges of implementing artificial intelligence in healthcare: A systematic literature review.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Urologic Oncology: Seminars and Original Investigations</span>. Elsevier, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May D Wang, Joyce C Ho, Chao Zhang, and Carl Yang.

</span>
<span class="ltx_bibblock">Bmretriever: Tuning large language models as better biomedical text retrievers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2404.18443</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Han Yang, Mingchen Li, Huixue Zhou, Yongkang Xiao, Qian Fang, and Rui Zhang.

</span>
<span class="ltx_bibblock">One llm is not enough: Harnessing the power of ensemble learning for medical question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">medRxiv</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Yuanshun Yao, Xiaojun Xu, and Yang Liu.

</span>
<span class="ltx_bibblock">Large language model unlearning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2310.10683</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Hanning Zhang, Shizhe Diao, Yong Lin, Yi R Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang.

</span>
<span class="ltx_bibblock">R-tuning: Teaching large language models to refuse unknown questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2311.09677</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith.

</span>
<span class="ltx_bibblock">How language model hallucinations can snowball.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2305.13534</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: a survey on hallucination in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2309.01219</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 22 12:05:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
