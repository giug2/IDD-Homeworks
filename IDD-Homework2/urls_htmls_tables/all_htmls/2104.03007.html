<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.03007] Representative &amp; Fair Synthetic Data</title><meta property="og:description" content="Algorithms learn rules and associations based on the training data that they are exposed to. Yet, the very same data that teaches machines to understand and predict the world, contains societal and historic biases, res…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Representative &amp; Fair Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Representative &amp; Fair Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.03007">

<!--Generated on Sun Mar 17 01:11:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Representative &amp; Fair Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Tiwald 
<br class="ltx_break">MOSTLY AI 
<br class="ltx_break">Vienna, Austria 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">paul.tiwald@mostly.ai</span> 
<br class="ltx_break">&amp;Alexandra Ebert 
<br class="ltx_break">MOSTLY AI 
<br class="ltx_break">Vienna, Austria 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">alexandra.ebert@mostly.ai</span> 
<br class="ltx_break">&amp;Daniel T. Soukup 
<br class="ltx_break">Toronto, ON, Canada 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">daniel.t.soukup@gmail.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Algorithms learn rules and associations based on the training data that they are exposed to. Yet, the very same data that teaches machines to understand and predict the world, contains societal and historic biases, resulting in biased algorithms with the risk of further amplifying these once put into use for decision support. Synthetic data, on the other hand, emerges with the promise to provide an unlimited amount of representative, realistic training samples, that can be shared further without disclosing the privacy of individual subjects. We present a framework to incorporate fairness constraints into the self-supervised learning process, that allows to then simulate an unlimited amount of representative as well as fair synthetic data. This framework provides a handle to govern and control for privacy as well as for bias within AI at its very source: the training data. We demonstrate the proposed approach by amending an existing generative model architecture and generating a representative as well as fair version of the UCI Adult census data set. While the relationships between attributes are faithfully retained, the gender and racial biases inherent in the original data are controlled for. This is further validated by comparing propensity scores of downstream predictive models that are trained on the original data versus the fair synthetic data. We consider representative &amp; fair synthetic data a promising future building block to teach algorithms not on historic worlds, but rather on the worlds that we strive to live in.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Decision processes within public as well as private organizations are increasingly supported by machine learning algorithms. These come with the benefit of being fast, scalable, consistent, and oftentimes more accurate as well as objective<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>E.g., <cite class="ltx_cite ltx_citemacro_cite">Gates et al. (<a href="#bib.bib4" title="" class="ltx_ref">2002</a>)</cite> showed how the adoption of automated underwriting in mortgage lending contributed to the increase of approval rates for minority and low-income applicants by 30% while improving the overall accuracy of default predictions.</span></span></span> compared to human judgement. However, these algorithms depend on the provided training data to represent, understand and predict the world. And thus, any historic discrimination against groups based on their gender, race or sexual orientation, that is already present in the training data, influences the model as well <cite class="ltx_cite ltx_citemacro_citep">(Kearns &amp; Roth, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Barocas et al., <a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>. If not addressed, these systemic biases end up in data sets that decision-making algorithms are trained on. Subsequently, the biased algorithms make unfair decisions, perpetuating, and actually amplifying the biases in our society. More often than not, engineers or managers are not aware of the issue at hand, making unwanted bias one of the key challenges for establishing Ethical AI.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">AI-generated synthetic data emerges as a novel approach to safely share (training) data with third parties. As it strives to retain statistical information of an original data set, it does break the 1:1 relationship to actual records and thus can effectively prevent the re-identification of individuals. But the very same generative models, that are tasked to yield realistic, representative synthetic samples, can be amended to shift patterns and relationships in order to make the data also fair. For simplicity, we start out with adopting the concept of statistical parity for the purpose of this introduction, whereas the approach can also be expanded to other fairness definitions. Statistical parity requires that a classifier gives equal probability with respect to a target variable, independent of whether a subject is or is not contained within a protected group.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">There are three points in the machine learning life cycle where one can mitigate bias: at the source, by changing the training data; during the modeling phase by using additional fairness constraints; and as a post-processing step, by revising the algorithm’s decisions in favor of a sensitive group. Naive data-level techniques, such as oversampling methods, have the risk of skewing important data distributions when mitigating imbalances. Similarly, simply excluding the sensitive column, like race or gender, can have detrimental effects <cite class="ltx_cite ltx_citemacro_citep">(Besse et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, as the effect can still be present, yet less apparent, via proxy attributes.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Imagine we know which neighborhood a person lives in, the brand and model of the person’s mobile phone, the car this person drives, where this person buys her/his clothes, etc. Given some of the above information, we humans can make an educated guess on this person’s sex, skin color, and other attributes. And since algorithms are better in analyzing patterns like this, they will definitely detect these correlations and exploit them, leading again to unfair predictions and decisions. We could actually go one step further and say that leaving the “sex” column in the data set is better for fairness because it offers a clear handle to enforce fairness constraints such as statistical parity.</span></span></span> Thus, it requires a complete representation of the data structure to control for proxy variables in order to properly account for fairness. And for that reason, we propose to incorporate fairness constraints into the training data itself to govern for AI bias already at its source.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Generative deep neural networks are typically tasked to optimize an accuracy loss, in order to yield a model that provides truly new, synthetic samples, that are representative of the original records. That loss measures how well the fitted function matches the observed distributions of the real data. In order to get representative &amp; fair data, we propose to add an additional fairness loss to the loss function, which penalizes any violation of the statistical parity. That is, we optimize for a combined loss: a weighted sum of the accuracy loss and a fairness loss where the fairness loss is proportional to the deviation from the empirically estimated statistical parity. With all other things equal, this approach allows to explicitly trade off accuracy with fairness, by shifting weights from one loss component to the other.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>A very similar approach for training fair classifiers is described in <cite class="ltx_cite ltx_citemacro_cite">Manisha &amp; Gujar (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>, with an implementation available at <a target="_blank" href="https://github.com/yoshavit/fairml-farm" title="" class="ltx_ref ltx_href">https://github.com/yoshavit/fairml-farm</a>.</span></span></span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Empirical Demonstration</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Impact on Descriptive Statistics</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In order to demonstrate the effectiveness of the concept, we expanded an existing data synthesizer for tabular data with the additional loss component for fairness. We chose to synthesize the well-known Adult income data set <cite class="ltx_cite ltx_citemacro_citep">(Dua &amp; Graff, <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> end-to-end 50 times, each time training the generative model with a parity fairness constraint and treating gender as the protected attribute. Whereas the original data set exhibits a clear gender imbalance within the high-income class, that bias is successfully mitigated in the synthetic data, as is reported in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Impact on Descriptive Statistics ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. With regards to parity, it is common to compare not just the difference but the fraction of high-income male ratio to high-income female ratio. This fraction is called the disparate impact and it is an industry-standard to ask for at least 0.8, the so-called four-fifth rule. In the original data set, this fraction is roughly 10/30 = 0.33, a quite severe disparate impact violation but the bias-corrected synthetic data is at 22/25 = 0.88, well over the threshold.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2104.03007/assets/adult_gender.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="510" height="126" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The income gap is significantly mitigated in the synthetic data.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Equally important, the additional parity constraint during model training did not diminish the quality of the synthetic data itself (see Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Impact on Descriptive Statistics ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The univariate distributions of the original data remain perfectly preserved, including the population-wide male-to-female ratio, as well as the high-earner-to-low-earner ratios. Also, the bivariate correlations are in good agreement with the original data for all but one relationship. Given the statistical parity constraint “income must not depend on sex”, these two attributes should not be correlated. While in the original data, there is a clear “sex”-”income” correlation, this dependency is almost reduced to noise level in the representative &amp; fair synthetic data set.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2104.03007/assets/adult_descriptive.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="510" height="191" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of univariate and bivariate statistics for original and synthetic data.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">We conducted further experiments to study the impact of proxy attributes, and to see whether they can still introduce unfairness through a backdoor, as they are not explicitly accounted for in the parity constraint. We added an artificial feature to the Adult data set named “proxy”, that is strongly correlated with gender. For females, “proxy” equals to 1 in 90% of all cases and equals to 0 for the remaining 10%. For males, the percentages are swapped. A subset of the resulting correlation plots is displayed in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Impact on Descriptive Statistics ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The original data shows strong correlation between “sex” and “proxy”, as well as ”sex”/”proxy” and “income”. For the synthetic data, the correlation between “sex” and “proxy” remains intact, whereas the correlation of both of these to “income” is significantly reduced due to the induced fairness constraint. This demonstrates that the parity also successfully accounts for (hidden) proxy attributes.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2104.03007/assets/adult_proxy_fixed.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="510" height="100" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The parity-fairness constraint also accounts for proxy attributes for gender.</figcaption>
</figure>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">In the Adult data set, gender is not the only sensitive attribute: if the generative model is trained incorporating a fairness constraint for “race”, we can achieve similar results as reported for gender. But what if we want to account for fairness with respect to multiple attributes at the same time? In this case, one must be careful what ratios to optimize: if we were to simply put fairness losses independently on race and gender then the algorithm might fall into the mistake of “fairness gerrymandering” <cite class="ltx_cite ltx_citemacro_citep">(Kearns et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. That is, the new data set would look fair with respect to both gender and race individually, but we would see high imbalances when restricted to gender and race simultaneously. Taking this into account, our proposed approach is capable to yield synthetic data with significantly balanced high-income ratios across all four groups given by race and gender (see Figure <a href="#S2.F4" title="Figure 4 ‣ 2.1 Impact on Descriptive Statistics ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). It is apparent that we did not achieve complete parity but this difference can be further lowered by giving higher weight to the fairness loss against the accuracy loss.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2104.03007/assets/adult_race_gender_fixed.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="80" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mitigating bias with respect to gender and race simultaneously.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Impact on Downstream Models</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">We further investigated whether models trained on the de-biased data set yield more fair algorithms. To this end, we fit logistic regression classifiers to predict the income level using all other attributes as predictors. The models were fitted both on the original and the bias-corrected synthetic data and then both were tested on a holdout from the original data. Moreover, we repeated the model training procedure 50 times with independently generated synthetic data. Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Impact on Downstream Models ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the mean performance of the real and synthetic models over these experiments. The synthetically fitted models have very competitive performance and generalize well to the unseen real data.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">AUC</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">F1-Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">original</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">85.49%</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">91.17%</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">67.04%</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">synthetic</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center">84.13%</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center">88.34%</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center">59.8%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average predictive performance of downstream models for income.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Moreover, the models trained on the synthetic data treat the classes of the sensitive attribute (gender, in this case) nearly equally. Figure <a href="#S2.F5" title="Figure 5 ‣ 2.2 Impact on Downstream Models ‣ 2 Empirical Demonstration ‣ Representative &amp; Fair Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> visualizes the propensity scores of the corresponding predictive models. The model fitted on the original data assigns a significantly lower probability to women being in the high-income class when compared to male subjects. However, for the model fitted on synthetic data that discrepancy in score assignment is reduced, and the gender-specific distributions largely align. This is exactly the group fairness that parity is designed to capture. The important thing to keep in mind though is that the predictive-model training itself did not involve any type of optimization to fairness and the evaluation is also on the biased original data. So this fair outcome is solely due to using bias-corrected synthetic data for the training. Our results align with the findings of research conducted at Carnegie Mellon University into fair representations of data <cite class="ltx_cite ltx_citemacro_citep">(Zhao &amp; Gordon, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>. We see that our fairness-constrained synthetic data solution learns to represent data points in a way that removes the dependencies between the sensitive and target attribute while preserving other relationships.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2104.03007/assets/adult_pred_prob.png" id="S2.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Propensity distribution of downstream models for income.</figcaption>
</figure>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barocas et al. (2017)</span>
<span class="ltx_bibblock">
Solon Barocas, Moritz Hardt, and Arvind Narayanan.

</span>
<span class="ltx_bibblock">Fairness in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Nips tutorial</em>, 1:2, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besse et al. (2020)</span>
<span class="ltx_bibblock">
Philippe Besse, Eustasio del Barrio, Paula Gordaliza, Jean-Michel Loubes, and
Laurent Risser.

</span>
<span class="ltx_bibblock">A survey of bias in machine learning through the prism of statistical
parity for the adult data set.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.14263</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua &amp; Graff (2017)</span>
<span class="ltx_bibblock">
Dheeru Dua and Casey Graff.

</span>
<span class="ltx_bibblock">UCI machine learning repository, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://archive.ics.uci.edu/ml" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://archive.ics.uci.edu/ml</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gates et al. (2002)</span>
<span class="ltx_bibblock">
Susan Wharton Gates, Vanessa Gail Perry, and Peter M Zorn.

</span>
<span class="ltx_bibblock">Automated underwriting in mortgage lending: Good news for the
underserved?

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Housing Policy Debate</em>, 13(2):369–391,
2002.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kearns &amp; Roth (2019)</span>
<span class="ltx_bibblock">
Michael Kearns and Aaron Roth.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">The ethical algorithm: The science of socially aware algorithm
design</em>.

</span>
<span class="ltx_bibblock">Oxford University Press, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kearns et al. (2018)</span>
<span class="ltx_bibblock">
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu.

</span>
<span class="ltx_bibblock">Preventing fairness gerrymandering: Auditing and learning for
subgroup fairness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 2564–2572. PMLR, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manisha &amp; Gujar (2018)</span>
<span class="ltx_bibblock">
Padala Manisha and Sujit Gujar.

</span>
<span class="ltx_bibblock">Fnnc: Achieving fairness through neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.00247</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao &amp; Gordon (2019)</span>
<span class="ltx_bibblock">
Han Zhao and Geoffrey J Gordon.

</span>
<span class="ltx_bibblock">Inherent tradeoffs in learning fair representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.08386</em>, 2019.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">NOTE TO REVIEWERS: For camera-ready version we shall provide a link to download the representative &amp; fair datasets from the empirical section.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.03006" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.03007" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.03007">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.03007" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.03008" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 01:11:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
