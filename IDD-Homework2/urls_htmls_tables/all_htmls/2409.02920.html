<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</title>
<!--Generated on Mon Sep  2 14:11:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Dual-arm robotic benchmark Digital twin simulation" lang="en" name="keywords"/>
<base href="/html/2409.02920v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S1" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S2" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S2.SS1" title="In 2 Related Work ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Datasets and Benchmarks for Robotics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S2.SS2" title="In 2 Related Work ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Robot Manipulation Learning Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S3" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Real-to-sim transfer of the scene</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S3.SS1" title="In 3 Real-to-sim transfer of the scene ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Generative Digital Twin System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S3.SS2" title="In 3 Real-to-sim transfer of the scene ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Expert Data Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S4" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S5" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Real-world Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S6" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S7" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1" title="In RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1.SS1" title="In Appendix 0.A Appendix ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.1 </span>Licensing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1.SS2" title="In Appendix 0.A Appendix ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.2 </span>Benchmark Task Descriptions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1.SS3" title="In Appendix 0.A Appendix ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A.3 </span>Dataset Task Descriptions</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>The University of Hong Kong </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>AgileX Robotics </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Shanghai AI Laboratory </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Shenzhen University </span></span></span><span class="ltx_note ltx_role_institutetext" id="id5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Institute of Automation, Chinese Academy of Sciences 
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://robotwin-benchmark.github.io/early-version" title="">https://robotwin-benchmark.github.io/early-version</a></span></span></span>
<h1 class="ltx_title ltx_title_document">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yao Mu <sup class="ltx_sup" id="id6.2.id1"><span class="ltx_text ltx_font_italic" id="id6.2.id1.1">∗†</span></sup>
</span><span class="ltx_author_notes">11 3 3</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianxing Chen <sup class="ltx_sup" id="id7.2.id1"><span class="ltx_text ltx_font_italic" id="id7.2.id1.1">∗</span></sup>
</span><span class="ltx_author_notes">11 3 3 4 4</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shijia Peng <sup class="ltx_sup" id="id8.2.id1"><span class="ltx_text ltx_font_italic" id="id8.2.id1.1">∗</span></sup>
</span><span class="ltx_author_notes">22 4 4</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zanxin Chen <sup class="ltx_sup" id="id9.2.id1"><span class="ltx_text ltx_font_italic" id="id9.2.id1.1">∗</span></sup>
</span><span class="ltx_author_notes">22 4 4</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeyu Gao
</span><span class="ltx_author_notes">55</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yude Zou
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lunkai Lin
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiqiang Xie
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ping Luo <sup class="ltx_sup" id="id10.2.id1"><span class="ltx_text ltx_font_italic" id="id10.2.id1.1">†</span></sup>
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots’ ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data.
This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality.
Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Dual-arm robotic benchmark Digital twin simulation
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>* Equal Contributions. †Corresponding authors: Ping Luo (pluo.lhi@gmail.com) and Yao Mu (muyao@connect.hku.hk). Tianxing Chen completed this work while he was an intern at the University of Hong Kong.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the fast-evolving robotics field, the integration of dual-arm coordination and advanced tool use is crucial for developing sophisticated autonomous systems. These capabilities are essential for enabling robots to function effectively in diverse real-world settings such as manufacturing plants, healthcare centers, and homes. By using tools, robots can significantly expand their operational scope, adapting to a variety of tasks and challenges with greater flexibility.
However, the advancement in these areas is substantially hindered by the lack of specialized, high-quality training data. These activities, which often require tailored solutions, are difficult to standardize and are typically not well-represented in conventional datasets.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Addressing this critical gap, we introduce "RoboTwin", a comprehensive benchmark that includes both real-world teleoperated data and corresponding synthetic data generated by a digital twin. Specifically designed for scenarios involving dual-arm robotic tool use and human-robot interactions. RoboTwin features high-quality annotations and diversity of examples to ensure robust training and evaluation. To collect real-world data, we employ the open-source COBOT Magic platform developed by AgileX Robotics. This platform is outfitted with four AgileX Arms and four Intel Realsense D-435 RGBD cameras, mounted on a robust Tracer chassis. The data encompasses a variety of typical tasks, including tool usage and human-robot interaction.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="432" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">RoboTwin Benchmark.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Transforming from the collection of real-world data to its virtual replication, the challenge is to create accurate and cost-effective digital twins. Traditional methodologies often rely on expensive, high-fidelity sensors, limiting their widespread adoption. To circumvent these limitations, we have developed a novel, cost-effective approach using Artificial Intelligence Generated Content (AIGC) to construct 3D models from a single 2D RGB image. This method significantly reduces costs while providing lifelike visual representations and supporting physical simulations. Our process begins with converting a 2D image into a detailed 3D model, featuring complex geometry, surface textures, and accurate details, which are crucial for realistic visualizations and simulations. We further enhance the model by defining functional coordinate axes on the object parts, enabling the automated computation of grasp poses essential for robotic manipulations.
To enhance the utility and relevance of our dataset, we have also established an innovative pipeline leveraging large language models (LLMs) for the automatic generation of expert-level training data. This methodology not only enriches the dataset with high-quality, scenario-specific examples but also integrates the versatility of LLMs in synthesizing complex interactive sequences. Integrating the reasoning capabilities of GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib1" title="">1</a>]</cite>, we automate the generation of task-specific pose sequences, thereby increasing the precision of task executions. Moreover, we employ GPT4-generated scripts to activate trajectory planning tools, which streamline the programming efforts and expedite the deployment of robotic systems in various environments.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The core contributions of this work are: 1) The development of "RoboTwin", a benchmark that includes both real-world teleoperated data and high-fidelity synthetic data generated for corresponding scenarios. 2) The establishment of a convenient real-to-simulation pipeline that requires only a single RGB image from the real world to generate the 3D models of target objects and corresponding scenes. 3) The utilization of large language models (LLMs) combined with simulation environment information to generate code that automatically creates expert-level data. These advancements collectively aim to bridge the gap in robotic training data, significantly enhancing the potential for robots to learn and operate using tools in a manner that mimics human dexterity and interaction finesse.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets and Benchmarks for Robotics</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To enhance the collection of effective demonstrations for robotic tasks, human teleoperation has traditionally been employed. In this method, a human operator manually guides a robot through various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib14" title="">14</a>]</cite>. Recent advancements have extended this methodology by employing teams of human operators over prolonged periods to assemble substantial real-world datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib5" title="">5</a>]</cite>. An alternative method involves the use of algorithmic trajectory generators within simulations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib9" title="">9</a>]</cite>, which, while efficient, often depend on privileged information and hand-designed heuristics, making them labor-intensive for arbitrary tasks.
However, current systems often fail to produce high-fidelity expert simulation data that accurately mimics data from actual machine operations. Although initiatives like MimicGen<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib23" title="">23</a>]</cite> and RoboCaca<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib27" title="">27</a>]</cite> strive to generate simulated expert data using limited human demonstrations, they still heavily rely on predefined scenes and interactive objects.
To overcome these limitations, we introduce RoboTwin. This innovative system not only generates expert data and simulation scenes derived from real-world scenarios but also utilizes large language models (LLMs) to generate demonstration codes and expert data for similar tasks involving the same class of objects. This strategy significantly reduces the dependence on continuous human intervention, thereby streamlining the generation of reliable training data for robotic tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Robot Manipulation Learning Methods</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The adoption of human demonstrations to instruct robots in manipulation skills is a prevalent method in Robot Manipulation Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib6" title="">6</a>]</cite>. Among the techniques, Behavioral Cloning stands out for learning policies offline from these demonstrations. It replicates observed actions from a curated dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib16" title="">16</a>]</cite>. Conversely, Offline Reinforcement Learning enhances policy learning by optimizing actions based on a predefined reward function and exploiting large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib19" title="">19</a>]</cite>. The Action Chunking with Transformers (ACT) technique integrates a Transformer-based visuomotor policy with a conditional variational autoencoder to structure the learning of action sequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib30" title="">30</a>]</cite>. Recently, the Diffusion Policy method has gained prominence. It employs a conditional denoising diffusion process for visuomotor policy representation, effectively reducing the accumulative error in trajectory generation that is often observed in Transformer-based visuomotor policies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib8" title="">8</a>]</cite>. The 3D Diffusion Policy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib32" title="">32</a>]</cite> uses point clouds for environmental observations, enhancing spatial information utilization and managing various robotic tasks in both simulated and real environments with only a small number of demonstrations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Real-to-sim transfer of the scene</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generative Digital Twin System</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">AIGC &amp; Expert Data Generation pipeline. Automatic extraction of object segmentation and textual description from a single RGB photo, followed by the generation of 3D geometry, surface normals, Wireframe, and texture maps to create a high-fidelity simulation object. With the object’s surface normal and pose information, we can decompose and generate grasping postures, and leverage the large model’s capabilities to zero-shot generate expert data for tasks.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To synthesize high-fidelity data through simulation, a major challenge is the creation of accurate and cost-effective digital twins. Traditional methods often depend on costly high-precision sensors, which can hinder widespread adoption. In response, we have developed a more economical approach using Artificial Intelligence Generated Content (AIGC) to construct 3D models from simple 2D RGB images powered by Deemos’s Rodin platform<span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use Deemos’s 3D digital assert Generation Model (from text or image) Rodin: <a class="ltx_ref ltx_href" href="https://hyperhuman.deemos.com/rodin" title="">https://hyperhuman.deemos.com/rodin</a></span></span></span>. This technique significantly reduces the reliance on expensive sensors while achieving realistic visual effects and supporting physical simulations.
Our innovative pipeline commences with generating a detailed 3D mesh and texture of the target object involved in a robot’s task, created from a single real-world image. This capability ensures a high-fidelity recreation of real-world scenarios within a simulated environment. The process begins by transforming a single 2D image into a 3D model that encompasses detailed geometry, surface normals, wireframes, and textures. These features enhance the visual realism and ensure compatibility with physics engines for simulations.
Once the 3D model is ready, we assign specific coordinate axes to functional parts of objects within the model. For instance, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S3.F3" title="Figure 3 ‣ 3.2 Expert Data Generation ‣ 3 Real-to-sim transfer of the scene ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">3</span></a>, for a hammer, one axis is aligned with the hammerhead—identifying the functional part—while another axis indicates the approach direction. This strategic alignment is crucial for automating the calculation of grasp poses, which are essential for robotic manipulation and tool usage. Grasp poses are computed perpendicular to the surface normal of the functional part along the designated approach direction axis, facilitating correct and efficient tool use with minimal manual intervention.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Expert Data Generation</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Point for Function and Contact, Axis pointing to the functional part and approach direction</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We leverage the reasoning capabilities of GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib1" title="">1</a>]</cite> to write code that calculates the relationships between key poses and the functional coordinate axes of objects. GPT4-V analyzes task requirements and generates a sequence of poses that align with these requirements, ensuring precise task execution. We also generate code via GPT4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib2" title="">2</a>]</cite> to invoke trajectory planning tools based on the computed poses. This automation substantially decreases the time and labor associated with manual programming, facilitating the swift deployment of robotic systems across diverse applications. It also offers a scalable approach for generating high-quality data essential for robotic learning.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmark</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To further research and development in this area, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S4.F4" title="Figure 4 ‣ 4 Benchmark ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">4</span></a>, we introduce a comprehensive benchmark specifically designed to assess dual-arm robots in a variety of scenarios. This benchmark encompasses a diverse set of tasks, each presenting unique challenges that are critical for assessing the dexterity, coordination, and operational efficiency of robotic arms in a simulated environment. The tasks range from simple object manipulation to complex, coordinated actions requiring synchronized movements of both arms. Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1.SS2" title="0.A.2 Benchmark Task Descriptions ‣ Appendix 0.A Appendix ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">0.A.2</span></a> outlines the specific tasks and their descriptions, providing a clear framework for comparative analysis and further development of advanced robotic capabilities.
For each task, we provide a robust API that supports the generation of expert data across infinitely variable scenarios, such as different object placements and environmental conditions. This feature allows researchers to extensively test and refine the adaptability and precision of robotic systems under controlled yet varied conditions. Additionally, an offline dataset is available for each task, offering pre-generated expert data to facilitate offline training and benchmarking of algorithms. This benchmark aims to bridge the gap between theoretical robotic control models and their practical implementation, ensuring that the robotic systems can perform reliably in dynamic, real-world environments.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="879" id="S4.F4.g1" src="x4.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Task Execution of RoboTwin Benchmark.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Real-world Dataset</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">For the acquisition of real-world data, we employed the open-source Cobot Magic <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Platform Introduction: <a class="ltx_ref ltx_href" href="https://global.agilex.ai/products/cobot-magic" title="">https://global.agilex.ai/products/cobot-magic</a></span></span></span> platform from AgileX Robotics, which is equipped with four AgileX Arms and four Intel Realsense D-435 RGBD cameras and is built on the Tracer chassis. These cameras are strategically positioned: one on the high part of the stand for an expansive field of view, two on the wrists of the robot’s arms, and one on the low part of the stand which is optional for use. The front, left, and right cameras capture data simultaneously at a frequency of 30Hz, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S5.F5" title="Figure 5 ‣ 5 Real-world Dataset ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">5</span></a>. The data collection and alignment are facilitated by tools provided by the ARIO Data Alliance, available at our GitHub repository<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Tool for data alignment: <a class="ltx_ref ltx_href" href="https://github.com/ario-dataset/ario-tools" title="">https://github.com/ario-dataset/ario-tools</a></span></span></span>. Each captured frame consists of three images from the cameras, each providing an RGB and depth image at a resolution of 640<math alttext="\times" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><times id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">×</annotation></semantics></math>480 pixels. Additionally, the data includes the poses of the robotic arms’ joints and end-effectors for both master and slave configurations, encompassing both left and right arms. All data storage and formatting adhere to the unified standards established by the ARIO Data Alliance.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our dataset task design features two major highlights: a focus on human-robot interaction and tool usage. As shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#Pt0.A1.SS3" title="0.A.3 Dataset Task Descriptions ‣ Appendix 0.A Appendix ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">0.A.3</span></a>, we have designed 17 tasks, 9 of which emphasize tool usage, 5 involve interpersonal interactions and 6 tasks are dual-arm. We collected 30 trajectories for each task. During trajectory collection, we broke down the tasks into multiple stages and conducted slower data collection for key sub-trajectories that required precise operations, enhancing the detail of the trajectories for better model learning.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="281" id="S5.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Illustration of our robot platform, with the capabilities for teleoperation, mobility, and data acquisition.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiment</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our experimental aim is not to delve into the design choices of different strategy networks but to explore the correctness and effectiveness of our Benchmark expert data. Our experiments are intended to verify: a) the rationality of the COBOT Magic platform settings, and b) the effectiveness of the automatically generated expert data.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We utilized the 3D Diffusion Policy (DP3) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#bib.bib32" title="">32</a>]</cite> to test six tasks within the benchmark, with each task being tested using strategies trained from 10 sets, 20 sets, and 50 sets of expert data, respectively, to obtain the success rates. For the success criteria of each task, please refer to the appendix.</p>
</div>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T1.4.1.1" style="font-size:113%;">Table 1</span>: </span><span class="ltx_text" id="S6.T1.5.2" style="font-size:113%;">The testing results for DP3 on various tasks, trained with varying amounts of expert data.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T1.6.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.6.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T1.6.1.1.1.1" style="font-size:80%;">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.6.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T1.6.1.1.2.1" style="font-size:80%;">   10 demo.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.6.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T1.6.1.1.3.1" style="font-size:80%;">   20 demo.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T1.6.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T1.6.1.1.4.1" style="font-size:80%;">   50 demo.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.6.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.6.2.1.1"><span class="ltx_text" id="S6.T1.6.2.1.1.1" style="font-size:80%;">Block Hammer Beat</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.6.2.1.2"><span class="ltx_text" id="S6.T1.6.2.1.2.1" style="font-size:80%;">24%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.6.2.1.3"><span class="ltx_text" id="S6.T1.6.2.1.3.1" style="font-size:80%;">56%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.6.2.1.4"><span class="ltx_text" id="S6.T1.6.2.1.4.1" style="font-size:80%;">80%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.3.2">
<td class="ltx_td ltx_align_center" id="S6.T1.6.3.2.1"><span class="ltx_text" id="S6.T1.6.3.2.1.1" style="font-size:80%;">Empty Cup Place</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.3.2.2"><span class="ltx_text" id="S6.T1.6.3.2.2.1" style="font-size:80%;">10%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.3.2.3"><span class="ltx_text" id="S6.T1.6.3.2.3.1" style="font-size:80%;">60%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.3.2.4"><span class="ltx_text" id="S6.T1.6.3.2.4.1" style="font-size:80%;">96%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.4.3">
<td class="ltx_td ltx_align_center" id="S6.T1.6.4.3.1"><span class="ltx_text" id="S6.T1.6.4.3.1.1" style="font-size:80%;">Dual-Bottles Pick</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.4.3.2"><span class="ltx_text" id="S6.T1.6.4.3.2.1" style="font-size:80%;">10%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.4.3.3"><span class="ltx_text" id="S6.T1.6.4.3.3.1" style="font-size:80%;">42%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.4.3.4"><span class="ltx_text" id="S6.T1.6.4.3.4.1" style="font-size:80%;">74%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.5.4">
<td class="ltx_td ltx_align_center" id="S6.T1.6.5.4.1"><span class="ltx_text" id="S6.T1.6.5.4.1.1" style="font-size:80%;">Block Sweep</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.5.4.2"><span class="ltx_text" id="S6.T1.6.5.4.2.1" style="font-size:80%;">28%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.5.4.3"><span class="ltx_text" id="S6.T1.6.5.4.3.1" style="font-size:80%;">70%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.5.4.4"><span class="ltx_text" id="S6.T1.6.5.4.4.1" style="font-size:80%;">86%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.6.5">
<td class="ltx_td ltx_align_center" id="S6.T1.6.6.5.1"><span class="ltx_text" id="S6.T1.6.6.5.1.1" style="font-size:80%;">Apple Cabinet Storage</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.6.5.2"><span class="ltx_text" id="S6.T1.6.6.5.2.1" style="font-size:80%;">30%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.6.5.3"><span class="ltx_text" id="S6.T1.6.6.5.3.1" style="font-size:80%;">57%</span></td>
<td class="ltx_td ltx_align_center" id="S6.T1.6.6.5.4"><span class="ltx_text" id="S6.T1.6.6.5.4.1" style="font-size:80%;">64%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.7.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.6.7.6.1"><span class="ltx_text" id="S6.T1.6.7.6.1.1" style="font-size:80%;">Block Handover</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.6.7.6.2"><span class="ltx_text" id="S6.T1.6.7.6.2.1" style="font-size:80%;">50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.6.7.6.3"><span class="ltx_text" id="S6.T1.6.7.6.3.1" style="font-size:80%;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.6.7.6.4"><span class="ltx_text" id="S6.T1.6.7.6.4.1" style="font-size:80%;">98%</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The experimental results, as summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02920v1#S6.T1" title="Table 1 ‣ 6 Experiment ‣ RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)"><span class="ltx_text ltx_ref_tag">1</span></a>, demonstrate the performance of the 3D Diffusion Policy (DP3) across six tasks, each trained with varying quantities of expert demonstration data (10, 20, and 50 sets). Notably, for the "Block Hammer Beat" task, the success rate improved from 24% with 10 demonstrations to 80% with 50 demonstrations. The "Empty Cup Place" task saw success rates soar from 10% to 96% with increased demonstrations. The "Dual-Bottles Pick" task success rates climbed from 10% to 74% as demonstrations increased. The "Block Sweep" task success improved steadily from 28% to 86%, while the "Apple Cabinet Storage" task showed more modest gains, from 30% to 64%. The "Block Handover" task achieved the most significant improvement, reaching 98% success with 50 demonstrations, up from 50%.
These results suggest a strong correlation between the number of expert demonstrations and task success, highlighting the effectiveness of the automatically generated expert data in enhancing task performance on the COBOT Magic platform. The data further underscores the importance of ample training examples in the development of robust strategies for complex tasks.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we introduce RoboTwin, a benchmark integrating real-world and synthetic data to evaluate dual-arm robots, addressing the significant shortage of specialized training data in robotics. Our dataset, developed using the AgileX Robotics platform and enhanced through generative digital twins powered by Deemos’s Rodin platform, effectively accelerates the training of robotic systems, enabling performance improvements across diverse tasks. Our results demonstrate the potential of this hybrid data approach to refine robotic dexterity and efficiency, providing a scalable tool that could revolutionize robotic research and applications.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Gpt-4v(ision) system card (2023), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:263218031" title="">https://api.semanticscholar.org/CorpusID:263218031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al.: Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bahl, S., Mendonca, R., Chen, L., Jain, U., Pathak, D.: Affordances from human videos as a versatile representation for robotics. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13778–13790 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al.: RT-1: Robotics transformer for real-world control at scale. In: arXiv preprint arXiv:2212.06817 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al.: Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chebotar, Y., Vuong, Q., Hausman, K., Xia, F., Lu, Y., Irpan, A., Kumar, A., Yu, T., Herzog, A., Pertsch, K., et al.: Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In: Conference on Robot Learning. pp. 3909–3928. PMLR (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B., Song, S.: Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dalal, M., Mandlekar, A., Garrett, C., Handa, A., Salakhutdinov, R., Fox, D.: Imitating task and motion planning with visuomotor transformers. arXiv preprint arXiv:2305.16309 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Daniilidis, K., Finn, C., Levine, S.: Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Gu, J., Xiang, F., Li, X., Ling, Z., Liu, X., Mu, T., Tang, Y., Tao, S., Wei, X., Yao, Y., et al.: Maniskill2: A unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gürtler, N., Blaes, S., Kolev, P., Widmaier, F., Wüthrich, M., Bauer, S., Schölkopf, B., Martius, G.: Benchmarking offline reinforcement learning on real-robot hardware. arXiv preprint arXiv:2307.15690 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
James, S., Ma, Z., Arrojo, D.R., Davison, A.J.: Rlbench: The robot learning benchmark &amp; learning environment. IEEE Robotics and Automation Letters <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">5</span>(2), 3019–3026 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., Finn, C.: Bc-z: Zero-shot task generalization with robotic imitation learning. In: Conference on Robot Learning (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., Finn, C.: Bc-z: Zero-shot task generalization with robotic imitation learning. In: Conference on Robot Learning. pp. 991–1002. PMLR (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., Fan, L.: Vima: General robot manipulation with multimodal prompts. In: International Conference on Machine Learning (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn, C., Levine, S., Hausman, K.: Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kumar, A., Singh, A., Ebert, F., Nakamoto, M., Yang, Y., Finn, C., Levine, S.: Pre-training for robots: Offline rl enables learning new tasks from a handful of trials. arXiv preprint arXiv:2210.05178 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kumar, A., Singh, A., Tian, S., Finn, C., Levine, S.: A workflow for offline model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Levine, S., Kumar, A., Tucker, G., Fu, J.: Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., Florence, P.: Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mandlekar, A., Booher, J., Spero, M., Tung, A., Gupta, A., Zhu, Y., Garg, A., Savarese, S., Fei-Fei, L.: Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity. arXiv preprint arXiv:1911.04052 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Mandlekar, A., Nasiriany, S., Wen, B., Akinola, I., Narang, Y., Fan, L., Zhu, Y., Fox, D.: Mimicgen: A data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Mandlekar, A., Xu, D., Martín-Martín, R., Savarese, S., Fei-Fei, L.: Learning to generalize across long-horizon tasks from human demonstrations. In: Robotics: Science and Systems (RSS) (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Mandlekar, A., Xu, D., Martín-Martín, R., Zhu, Y., Fei-Fei, L., Savarese, S.: Human-in-the-loop imitation learning using remote teleoperation (2020). https://doi.org/10.48550/ARXIV.2012.06733, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2012.06733" title="">https://arxiv.org/abs/2012.06733</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M., Tung, A., Gao, J., Emmons, J., Gupta, A., Orbay, E., Savarese, S., Fei-Fei, L.: Roboturk: A crowdsourcing platform for robotic skill learning through imitation. In: Conference on Robot Learning (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Nasiriany, S., Maddukuri, A., Zhang, L., Parikh, A., Lo, A., Joshi, A., Mandlekar, A., Zhu, Y.: Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Pomerleau, D.A.: Alvinn: An autonomous land vehicle in a neural network. In: Advances in neural information processing systems. pp. 305–313 (1989)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Sharma, P., Mohan, L., Pinto, L., Gupta, A.: Multiple interactions made easy (mime): Large scale demonstrations data for imitation. In: Conference on robot learning. pp. 906–915. PMLR (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional generative models. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib30.1.1">28</span> (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ze, Y., Zhang, G., Zhang, K., Hu, C., Wang, M., Xu, H.: 3d diffusion policy. arXiv preprint arXiv:2403.03954 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., Lee, J.: Transporter networks: Rearranging the visual world for robotic manipulation. In: Conference on Robot Learning (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., Abbeel, P.: Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In: IEEE International Conference on Robotics and Automation (ICRA) (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zhao, T.Z., Kumar, V., Levine, S., Finn, C.: Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705 (2023)

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Appendix</h2>
<section class="ltx_subsection" id="Pt0.A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.1 </span>Licensing</h3>
<div class="ltx_para" id="Pt0.A1.SS1.p1">
<p class="ltx_p" id="Pt0.A1.SS1.p1.1">RoboTwin is released under the open-source MIT license.</p>
</div>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.2 </span>Benchmark Task Descriptions</h3>
<div class="ltx_para" id="Pt0.A1.SS2.p1">
<p class="ltx_p" id="Pt0.A1.SS2.p1.1">In this subsection, we present a comprehensive overview of six carefully designed simulation tasks aimed at evaluating the capabilities of robotic systems in various manipulation scenarios. These tasks are crafted to challenge different aspects of dexterous control, coordination between multiple limbs, and the ability to perform complex sequences of actions. Each task is intended to simulate real-world challenges that a robotic system might encounter, thus providing a robust benchmark for assessing the effectiveness of various robotic algorithms. Detailed descriptions of these tasks are provided in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:benchmark_description</span>, which outlines the specific objectives and required actions for each scenario.</p>
</div>
<figure class="ltx_table" id="Pt0.A1.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.3.2" style="font-size:90%;">Detailed descriptions of 6 simulation tasks we propose.</span></figcaption>
<table class="ltx_tabular" id="Pt0.A1.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A1.T2.6.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="Pt0.A1.T2.6.1.1.1" style="width:99.6pt;"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="Pt0.A1.T2.6.1.1.2"></th>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="Pt0.A1.T2.6.2.2.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.2.2.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.2.2.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Pt0.A1.T2.6.2.2.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="Pt0.A1.T2.6.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.2.2.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.2.2.2.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T2.6.2.2.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.T2.6.3.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T2.6.3.1.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.3.1.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.3.1.1.1.1.1">Block Hammer Beat</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T2.6.3.1.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.3.1.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.3.1.2.1.1" style="width:227.6pt;">A hammer and a red block are on the table. The right arm uses the hammer to strike the block.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.4.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T2.6.4.2.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.4.2.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.4.2.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.4.2.1.1.1.1">Empty Cup Place</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T2.6.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.4.2.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.4.2.2.1.1" style="width:227.6pt;">A cup (without liquid) is on the table. The right arm picks up the cup from above downwards and places it on a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.5.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T2.6.5.3.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.5.3.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.5.3.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.5.3.1.1.1.1">Dual-Bottles Pick</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T2.6.5.3.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.5.3.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.5.3.2.1.1" style="width:227.6pt;">The left arm grasps a cola can, while the right arm simultaneously picks up a sprite can.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.6.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T2.6.6.4.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.6.4.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.6.4.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.6.4.1.1.1.1">Block Sweep</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T2.6.6.4.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.6.4.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.6.4.2.1.1" style="width:227.6pt;">There is a red block on the table. The left arm of the robotic arm holds the dustpan and the right arm holds the brush. The two arms work together to sweep the block in.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.7.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T2.6.7.5.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.7.5.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.7.5.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.7.5.1.1.1.1">Apple Cabinet Storage</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T2.6.7.5.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.7.5.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.7.5.2.1.1" style="width:227.6pt;">On the table, there’s a cabinet and an apple. The left arm opens the cabinet, and the right arm picks up the apple, placing it inside.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T2.6.8.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="Pt0.A1.T2.6.8.6.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.8.6.1.1">
<span class="ltx_p" id="Pt0.A1.T2.6.8.6.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T2.6.8.6.1.1.1.1">Block Handover</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="Pt0.A1.T2.6.8.6.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T2.6.8.6.2.1">
<span class="ltx_p" id="Pt0.A1.T2.6.8.6.2.1.1" style="width:227.6pt;">The left arm picks up the red cuboid on the left side of the desk and hands it over to the right arm, which will then place the cuboid in the blue target area on the right side.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="Pt0.A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.3 </span>Dataset Task Descriptions</h3>
<div class="ltx_para" id="Pt0.A1.SS3.p1">
<p class="ltx_p" id="Pt0.A1.SS3.p1.1">Our dataset comprises 17 distinct, real-world robotic tasks designed to evaluate the dexterity, coordination, and contextual understanding of robotic systems in a controlled environment. Each task is uniquely structured to challenge various aspects of robotic manipulation, from simple object transfers to complex, dual-arm coordination tasks. Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:dataset_description</span> provides detailed descriptions of each task, outlining the specific actions, objects, and expected interactions. These descriptions are pivotal for replicating the tasks in different research settings, ensuring consistency in performance benchmarks across various robotic platforms. The diverse nature of these tasks enables comprehensive testing of the robots’ abilities to handle real-world scenarios, thereby advancing our understanding and development of more adaptive and capable robotic assistants.</p>
</div>
<figure class="ltx_table" id="Pt0.A1.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.3.2" style="font-size:90%;">Detailed descriptions of 17 real-world tasks we propose.</span></figcaption>
<table class="ltx_tabular" id="Pt0.A1.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A1.T3.6.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="Pt0.A1.T3.6.1.1.1" style="width:99.6pt;"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="Pt0.A1.T3.6.1.1.2"></th>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="Pt0.A1.T3.6.2.2.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.2.2.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.2.2.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Pt0.A1.T3.6.2.2.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="Pt0.A1.T3.6.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.2.2.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.2.2.2.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="Pt0.A1.T3.6.2.2.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.T3.6.3.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.3.1.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.3.1.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.3.1.1.1.1.1">Paddle Sweep</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.3.1.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.3.1.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.3.1.2.1.1" style="width:227.6pt;">On the table, there is a dustpan, a brush, and a ping pong ball. Using the left arm to grasp the dustpan and the right arm to grasp the brush, the two arms work together to sweep the ping pong ball into the dustpan.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.4.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.4.2.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.4.2.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.4.2.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.4.2.1.1.1.1">Mark Hammer Beat</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.4.2.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.4.2.2.1.1" style="width:227.6pt;">A hammer and a red mark are on the table. The right arm picks up the hammer and strikes the mark.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.5.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.5.3.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.5.3.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.5.3.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.5.3.1.1.1.1">Flour Scoop</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.5.3.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.5.3.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.5.3.2.1.1" style="width:227.6pt;">On the table, there is a bowl full of flour and an empty plate. The right arm initially holds a spoon, scoops a spoonful of flour from the bowl, and then pours it into the empty plate.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.6.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.6.4.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.6.4.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.6.4.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.6.4.1.1.1.1">Brush Adjust</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.6.4.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.6.4.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.6.4.2.1.1" style="width:227.6pt;">A brush is on the table. The left arm first picks up the brush, and the right arm adjusts the brush’s position.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.7.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.7.5.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.7.5.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.7.5.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.7.5.1.1.1.1">Plate Scrub</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.7.5.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.7.5.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.7.5.2.1.1" style="width:227.6pt;">A dishrag and a plate are on the table. The left arm picks up the plate, and the right arm picks up the dishrag. The two arms work together to scrub the plate with the dishrag for three circles.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.8.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.8.6.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.8.6.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.8.6.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.8.6.1.1.1.1">Pot Wire Scrub</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.8.6.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.8.6.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.8.6.2.1.1" style="width:227.6pt;">A stainless steel pot and a steel wool pad are on the table. The left arm adjusts the position of the pot, and the right arm picks up the steel wool pad.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.9.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.9.7.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.9.7.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.9.7.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.9.7.1.1.1.1">Cake Fork</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.9.7.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.9.7.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.9.7.2.1.1" style="width:227.6pt;">A small plate with a piece of cake is on the table. Initially, the right arm holds a fork, and it uses the fork to pierce the cake and lift it.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.10.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.10.8.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.10.8.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.10.8.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.10.8.1.1.1.1">Stain Clean</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.10.8.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.10.8.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.10.8.2.1.1" style="width:227.6pt;">There is a juice stain on the table with a rag. The right arm picks up the rag and wipes away the juice.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.11.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.11.9.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.11.9.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.11.9.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.11.9.1.1.1.1">Scissors Take</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.11.9.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.11.9.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.11.9.2.1.1" style="width:227.6pt;">A human is holding the head of the scissors. The right arm grabs the handle of the scissors and takes them over.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.12.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.12.10.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.12.10.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.12.10.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.12.10.1.1.1.1">Pot Handover</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.12.10.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.12.10.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.12.10.2.1.1" style="width:227.6pt;">A human is holding the pot with both hands. The robot arms work together to grab the pot’s handles and transfer the pot to the table.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.13.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.13.11.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.13.11.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.13.11.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.13.11.1.1.1.1">Empty Cup Place</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.13.11.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.13.11.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.13.11.2.1.1" style="width:227.6pt;">A cup (without liquid) is on the table. The right arm picks up the cup from above downwards and places it on a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.14.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.14.12.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.14.12.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.14.12.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.14.12.1.1.1.1">Empty Cup Handover</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.14.12.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.14.12.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.14.12.2.1.1" style="width:227.6pt;">A water cup (without liquid) is on the table. The right arm grasps the cup from above downwards and hands it over to a human.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.15.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.15.13.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.15.13.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.15.13.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.15.13.1.1.1.1">Empty Cup Transfer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.15.13.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.15.13.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.15.13.2.1.1" style="width:227.6pt;">A human is holding a cup (without liquid). The right arm picks up the cup from above downwards and transfers it to a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.16.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.16.14.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.16.14.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.16.14.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.16.14.1.1.1.1">Water Cup Place</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.16.14.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.16.14.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.16.14.2.1.1" style="width:227.6pt;">A cup with water is on the table. The right arm picks up the cup from the side and places it on a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.17.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.17.15.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.17.15.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.17.15.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.17.15.1.1.1.1">Juice Cup Transfer</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.17.15.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.17.15.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.17.15.2.1.1" style="width:227.6pt;">A human is holding a cup with juice. The right arm picks up the cup from the side and transfers it to a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.18.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="Pt0.A1.T3.6.18.16.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.18.16.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.18.16.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.18.16.1.1.1.1">Juice Cup Place</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Pt0.A1.T3.6.18.16.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.18.16.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.18.16.2.1.1" style="width:227.6pt;">A cup with juice is on the table. The right arm picks up the cup from the side and places it on a cup mat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T3.6.19.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="Pt0.A1.T3.6.19.17.1" style="width:99.6pt;">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.19.17.1.1">
<span class="ltx_p" id="Pt0.A1.T3.6.19.17.1.1.1"><span class="ltx_text ltx_font_italic" id="Pt0.A1.T3.6.19.17.1.1.1.1">Dual-Bottles Pick</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="Pt0.A1.T3.6.19.17.2">
<span class="ltx_inline-block ltx_align_top" id="Pt0.A1.T3.6.19.17.2.1">
<span class="ltx_p" id="Pt0.A1.T3.6.19.17.2.1.1" style="width:227.6pt;">The left arm grasps a cola can, while the right arm simultaneously picks up a sprite can.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  2 14:11:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
