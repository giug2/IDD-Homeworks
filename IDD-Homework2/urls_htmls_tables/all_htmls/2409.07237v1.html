<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Negative Sampling in Recommendation: A Survey and Future Directions</title>
<!--Generated on Wed Sep 11 12:47:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender System,  Negative Sample,  Information Retrieval,  Survey" lang="en" name="keywords"/>
<base href="/html/2409.07237v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S1" title="In Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2" title="In Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Necessity and Challenges of Negative sampling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS1" title="In 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Role of Negative Sampling in Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS2" title="In 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Challenges of Negative Sampling in Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS2.SSS1" title="In 2.2. Challenges of Negative Sampling in Recommendation ‣ 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>False Negative Problem</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS2.SSS2" title="In 2.2. Challenges of Negative Sampling in Recommendation ‣ 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Trade-off among Accuracy, Efficiency and Stability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS2.SSS3" title="In 2.2. Challenges of Negative Sampling in Recommendation ‣ 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Universality with Different Tasks, Objectives, and Datasets</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3" title="In Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Literature review of Negative Sampling in Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1" title="In 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Static Negative Sampling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1.SSS1" title="In 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Uniform SNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1.SSS2" title="In 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Predefined SNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1.SSS3" title="In 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Popularity-based SNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1.SSS4" title="In 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>Non-sampling SNS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2" title="In 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dynamic Negative Sampling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS1" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Universal DNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS2" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>User-similarity DNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS3" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Knowledge-aware DNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS4" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Distribution-based DNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS5" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>Interpolation DNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS6" title="In 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>Mixed DNS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS3" title="In 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Adversarial Negative Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS3.SSS1" title="In 3.3. Adversarial Negative Generation ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Generative ANG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS3.SSS2" title="In 3.3. Adversarial Negative Generation ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Sampled ANG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS4" title="In 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Importance Re-weighting</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS4.SSS1" title="In 3.4. Importance Re-weighting ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Attention-based IRW</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS4.SSS2" title="In 3.4. Importance Re-weighting ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Knowledge-based IRW</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS4.SSS3" title="In 3.4. Importance Re-weighting ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Debiased IRW</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS5" title="In 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Knowledge-enhanced Negative Sampling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS5.SSS1" title="In 3.5. Knowledge-enhanced Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>General KNS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS5.SSS2" title="In 3.5. Knowledge-enhanced Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>KG-based KNS</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4" title="In Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Negative Sampling in Multiple Practical Recommendation Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS1" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Introduction of Negative Feedback in Practical Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS2" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Collaborative-guided Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS3" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Sequential Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS4" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Multi-modal Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS5" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Multi-behavior Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS6" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Cross-domain Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS7" title="In 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>CL-enhanced Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5" title="In Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Direction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS1" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Further Explorations on False Negative Sample Issue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS2" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Curriculum Learning on Hard Negative Sampling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS3" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Causal Inference for Negative Sample Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS4" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Alleviating Biases in Negative Sampling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS5" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Incorporating Large Language Model into Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS6" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Understanding Negative Sampling with Theoretical Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S5.SS7" title="In 5. Conclusion and Future Direction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Exploring Sampled Metrics in Recommendation Evaluation</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Negative Sampling in Recommendation: A Survey and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haokai Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mahaokai@mail.sdu.edu.cn">mahaokai@mail.sdu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">Shandong University</span><span class="ltx_text ltx_affiliation_country" id="id4.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruobing Xie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ruobingxie@tencent.com">ruobingxie@tencent.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Tencent</span><span class="ltx_text ltx_affiliation_country" id="id6.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Meng<sup class="ltx_sup" id="id7.2.id1"><span class="ltx_text ltx_font_italic" id="id7.2.id1.1">∗</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lmeng@sdu.edu.cn">lmeng@sdu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.3.id1">Shandong Research Institute of Industrial Technology; Shandong University</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fuli Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fulifeng93@gmail.com">fulifeng93@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_country" id="id11.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoyu Du
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:duxy.me@gmail.com">duxy.me@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">Nanjing University of Science and Technology</span><span class="ltx_text ltx_affiliation_country" id="id13.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingwu Sun
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sunxingwu01@gmail.com">sunxingwu01@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">Tencent</span><span class="ltx_text ltx_affiliation_country" id="id15.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhanhui Kang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kegokang@tencent.com">kegokang@tencent.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Tencent</span><span class="ltx_text ltx_affiliation_country" id="id17.2.id2">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiangxu Meng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mxx@sdu.edu.cn">mxx@sdu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id18.1.id1">Shandong University</span><span class="ltx_text ltx_affiliation_country" id="id19.2.id2">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id20.id1">Recommender systems aim to capture users’ personalized preferences from the cast amount of user behaviors, making them pivotal in the era of information explosion. However, the presence of the dynamic preference, the “information cocoons”, and the inherent feedback loops in recommendation make users interact with a limited number of items. Conventional recommendation algorithms typically focus on the positive historical behaviors, while neglecting the essential role of negative feedback in user interest understanding. As a promising but easy-to-ignored area, negative sampling is proficients in revealing the genuine negative aspect inherent in user behaviors, emerging as an inescapable procedure in recommendation. In this survey, we first discuss the role of negative sampling in recommendation and thoroughly analyze challenges that consistently impede its progress. Then, we conduct an extensive literature review on the existing negative sampling strategies in recommendation and classify them into five categories with their discrepant techniques. Finally, we detail the insights of the tailored negative sampling strategies in diverse recommendation scenarios and outline an overview of the prospective research directions toward which the community may engage and benefit.</p>
</div>
<div class="ltx_keywords">Recommender System, Negative Sample, Information Retrieval, Survey
</div>
<div class="ltx_acknowledgements">
<sup class="ltx_sup" id="id21.id1"><span class="ltx_text ltx_font_italic" id="id21.id1.1">∗</span></sup> indicates corresponding author.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems (RS) have emerged as an effective solution to the information overloading issue, capable of capturing user preference from the massive behaviors and provide appropriate items to each user.
Presently, the extensive deployment of recommendation algorithms underscores their transformative power, shaping the way people engage and navigate the choices with digital content. These encompass electronic commerce platforms (Taobao<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.taobao.com/</span></span></span>, and Amazon<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.amazon.com/</span></span></span>), social network (WeChat<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://weixin.qq.com/</span></span></span>, and Facebook<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.facebook.com</span></span></span>), lifestyle applications (Meituan<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://www.meituan.com/</span></span></span> and Google Maps<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://maps.google.com</span></span></span>) and so on. These significant achievements across multiple platforms and applications underscore the continued significance of RS.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S1.F1.g1" src="x1.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Illustration of the training stage of a personalized recommender system.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In alignment with other supervised tasks, providing appropriate supervised signals (both positive and negative feedback) is indispensable in the training phase of recommendation algorithms. The absence of either of these feedback types can inevitably give rise to bias in model training <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. However, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>, explicit feedback remains conspicuously absent within most recommendation datasets. Even when encompassed within datasets of such information, certain studies <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite> have unearthed that these negative feedback indeed mirror user preferences to some extent. For instance, the behavior of assigning a low rating to a particular movie may paradoxically signify that the user has already viewed the movie, as this selection from an extensive movie pool subtly reveals the user’s underlying predilections <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib41" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib82" title="">2023</a>)</cite>. Consequently, the integral exploration of negative sampling strategies that are proficient in unveiling genuine negative aspects inherent in user preferences emerges as an inescapable procedure in RS.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The profound impact of distinct negative sampling strategies on the performance of recommendation algorithms in disparate recommendation scenarios underscores the intricacies inherent in this critical domain. Moreover, existing academic endeavors pertaining to negative sampling algorithms are diverse in RS. Varied resolutions are observed in addressing negative samples across distinct recommendation scenarios, indicating the absence of a universal algorithm capable of concurrently tackling all recommendation tasks <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib57" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib90" title="">2021a</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib192" title="">2022a</a>)</cite>. Finally, the predominant strategies for tackling matching and ranking tasks revolve around the random sampling strategies <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib207" title="">2016</a>)</cite> and the meticulous integration of genuine negative feedback <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib104" title="">2020a</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib135" title="">2021</a>; Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib60" title="">2021</a>)</cite> for the industrial recommendation systems. Currently, there lacks a comprehensive survey that systematically categorizes and presents the existing studies. Such a survey can serve as a handbook to offer a unified exploration of the varied landscape surrounding negative sampling algorithms in RS, thereby addressing the aforementioned issues.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Given the significance and popularity of RS, there are several recently published surveys also reviewed this area from different perspectives, including debiased recommendation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib20" title="">2023a</a>)</cite>, cross-domain recommendation <cite class="ltx_cite ltx_citemacro_citep">(Zang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib200" title="">2022</a>)</cite>, graph-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib49" title="">2023c</a>)</cite>, knowledge graph-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib63" title="">2020b</a>)</cite>, causal inference-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib50" title="">2022b</a>)</cite>, large language model-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib45" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib172" title="">2023</a>)</cite> and evaluation in recommendation <cite class="ltx_cite ltx_citemacro_citep">(Bauer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib5" title="">2023</a>)</cite>. As to negative sampling, there are surveys in other domains, including knowledge graph completion <cite class="ltx_cite ltx_citemacro_citep">(Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib9" title="">2022</a>)</cite>, deep metric learning <cite class="ltx_cite ltx_citemacro_citep">(Kaya and Bilge, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib88" title="">2019</a>)</cite>, neural semantic retrieval <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib61" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib29" title="">2022c</a>)</cite>, image retrieval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib29" title="">2022c</a>)</cite>, and representation learning on various types of data <cite class="ltx_cite ltx_citemacro_citep">(Liu and Tang, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib113" title="">2021</a>; Qian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib129" title="">2020</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib183" title="">2022</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib185" title="">2020b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib16" title="">2020a</a>; Kamigaito and Hayashi, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib85" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib157" title="">2021a</a>; Khoshraftar and An, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib89" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib154" title="">2017a</a>)</cite>. Compared to these research domains, negative sampling techniques employed in RS possess distinctive characteristics, necessitating a comprehensive review and summary. To bridge this gap, we review the negative sampling strategies in RS, classify them into six categories with the discrepant techniques and detail the insights of the tailored negative sampling strategies in diverse recommendation scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This survey encompasses over 180 papers from the top conferences and journals within the field of recommendation systems, which focus on the analysis of negative samples and the design of novel negative sampling techniques. We provide a comprehensive evaluation of negative sampling strategies in recommendation from various perspectives. In particular, we search related works from the top-tier conferences in recommendation, including SIGIR, WWW, KDD, ACL, AAAI, IJCAI, ICDM, ICDE, RecSys, WSDM, CIKM, and PAKDD, as well as the leading journals such as TOIS, TKDE, TKDD, KBS, TMM, etc, with the keywords ”recommendation”, ”collaborative filtering”, ”ranking” and ”matching” in conjunction with ”negative samples”, ”re-weighting” and ”sampling”. At the same time, to prevent omissions of relevant literature, we utilize Google Scholar, DBLP, and Arxiv to search for related studies. After getting the list of papers, we also check their references to retain the unobserved papers about negative sampling strategies. To facilitate this survey’s role as a handbook for negative sampling techniques in RS, we will release a GitHub repository to encompass all reviewed papers along with their corresponding codes at the following link: https://github.com/hulkima/NS4RS.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We believe this survey is beneficial for those following academic researchers and industry developers in RS: 1) who are a new to the negative sampling problem and seek a handbook with an overview of related works; 2) who are aspiring to delve into the investigation of negative sampling algorithms but are unsure of where to begin; 3) who are grappling with efficiency or performance issues in their recommendation algorithms and seeking solutions through negative sampling strategies. We summarize the contribution of this survey as follows.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We unify the achievements made in the communities of negative sampling and RS by presenting a comprehensive review of existing related studies, which is the pioneering survey of this promising but easy-to-ignored area.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We summarize the fundamental challenges of negative sampling in practical RS from three aspects, delineate existing research works into five categories regarding representative methods and characteristics.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We detail the effectiveness mechanism and meaningful insights of the tailored negative sampling methods in diverse recommendation scenarios and tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We outline further potential research directions for negative sampling in recommendation by intensively analyzing the most advanced technologies in recommendation and the fundamental challenges of negative sampling.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Necessity and Challenges of Negative sampling</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Role of Negative Sampling in Recommendation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Real-world recommendation systems frequently involve more than millions of users and items, rendering the integration of all corpus into the training process prohibitively expensive <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib214" title="">2020</a>)</cite>. Despite the presence of a substantial number of users and items, the existence of “Information Cocoons” and the unique feedback loops within recommendation result in the majority of users interacting with only a limited number of items in the corpus <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib99" title="">2022</a>; Lou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib114" title="">2019</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib19" title="">2021c</a>)</cite>. As a consequence, this gives rise to the well-documented <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">issue of data sparsity</span> in recommendations <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib116" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib10" title="">2022</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib180" title="">2022b</a>)</cite>. The <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.2">dynamic nature</span> of user interests also necessitates a continuous process of adaptation and updating for recommenders <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib207" title="">2016</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib182" title="">2021</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib72" title="">2016</a>)</cite>. Furthermore, the perpetual introduction of new users and items (both devoid of an adequate reservoir of historical interaction data) engenders the notorious <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.3">cold start problem</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib164" title="">2021</a>; Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Large-scale recommendation applications typically encompass multiple phases to narrow down the candidate items for the recommender, thereby alleviating the aforementioned issues and significantly reducing their computational complexity <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib82" title="">2023</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib41" title="">2023</a>)</cite>. These algorithms generally require both positive and negative examples to model the users’ personalized preferences. Negative sampling is the critical and irreplaceable element in recommendation that could potentially improve the modeling of dynamic user preferences with their sparse interactions. Its crucial secret lies in its ability to select samples from each user’s vast collection of unobserved items, specifically tailored to enhance the model’s optimization within its current state <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. To achieve this, it necessitates the deliberate selection of negative samples characterized by (1) heightened informativeness, (2) increased discriminative capacity, and (3) enhanced precision. Such samples are typically termed as <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS1.p2.1.1">Hard Negative Samples (HNS)</em>, which are derived from their tendency to encompass more comprehensive information compared to the randomly selected samples from the corpus. Incorporating them into the training process serves to balance the positive and negative information within the dataset, thereby ensuring an unbiased optimization of the recommender. Drawing from this foundational definition of HNS, their gradient directions and magnitudes tend to vary from other random negative samples. Theoretically, more HNS can not only expedite the recommender’s convergence but also rectify the optimization direction of the global gradient, thus making it computationally possible to predict the appropriate items for users from the sparse implicit interactions <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib202" title="">2021a</a>)</cite>. In the practical deployment, the inclusion of HNS enables recommenders to elevate their effectiveness and robustness simultaneously, which has been verified in many relevant studies.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Challenges of Negative Sampling in Recommendation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Real-world recommendation applications still face fundamental challenges with the false negative problem, the trade-off among accuracy, efficiency, and stability, and the universality across different tasks, goals, and datasets.</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">How to precisely identify negative feedback with the substantial knowledge in recommender systems?</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">How to balance the accuracy, efficiency and stability in model training?</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">How to handle multiple scenarios with diverse objectives and data availability in recommendation?</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="288" id="S2.F2.g1" src="x2.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>An illustration of diverse types of negative samples in the representation space (Left) and their corresponding toy examples (Right). (i) We utilize the red star, black circle, brown triangle, and pink cross to differentiate the user, positive sample, negative sample, and false negative sample in the representation space, where the brightness of color indicates the ”hardness” of each sample; (ii) Radios and compact discs that share the same category of electronics may be false negative samples (potential positive samples). Conversely, watches, tall glasses, and high-heeled shoes can serve as the negative samples with the ”hardness” decreasing, which exhibit less associated with electronics.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>False Negative Problem</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Traditional recommenders randomly select items that users have not interacted with as negative samples <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib109" title="">2021a</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib209" title="">2018</a>)</cite>. In addition to these potentially meaningless random samples, some recommendation algorithms attempt to sample informative yet not excessively challenging negative items from the random candidates <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. These samples compel the recommender to delve deeper into modeling the boundary between positive and negative samples in many complex scenarios, thereby improving the recommender’s ability to discriminate among the unobserved items. The pioneering negative sampling methods have categorized these items as hard negative samples in recommendation and presented their corresponding definitions as:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.I2.i1.p1.1.1">Negative Samples (NS)</em>: The complete set of all items in the corpus that user <math alttext="u" class="ltx_Math" display="inline" id="S2.I2.i1.p1.1.m1.1"><semantics id="S2.I2.i1.p1.1.m1.1a"><mi id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><ci id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">u</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.1.m1.1d">italic_u</annotation></semantics></math> has not interacted with, which serves as the fundamental candidate of all existing negative sampling strategies.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.2"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.I2.i2.p1.2.1">Hard Negative Samples (HNS)</em>: The samples that possess more information than the majority of <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.I2.i2.p1.2.2">NS</em>, such as those associated with user <math alttext="u" class="ltx_Math" display="inline" id="S2.I2.i2.p1.1.m1.1"><semantics id="S2.I2.i2.p1.1.m1.1a"><mi id="S2.I2.i2.p1.1.m1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.1.m1.1b"><ci id="S2.I2.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.1.m1.1c">u</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.1.m1.1d">italic_u</annotation></semantics></math>’s positive items or items engaged with user <math alttext="u" class="ltx_Math" display="inline" id="S2.I2.i2.p1.2.m2.1"><semantics id="S2.I2.i2.p1.2.m2.1a"><mi id="S2.I2.i2.p1.2.m2.1.1" xref="S2.I2.i2.p1.2.m2.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.2.m2.1b"><ci id="S2.I2.i2.p1.2.m2.1.1.cmml" xref="S2.I2.i2.p1.2.m2.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.2.m2.1c">u</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.2.m2.1d">italic_u</annotation></semantics></math>’s social connections.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.I2.i3.p1.1.1">False Negative Samples (FNS)</em>: The samples that are erroneously identified as negative samples and subsequently fed into the recommender optimization, which correspond to the real interest of user <math alttext="u" class="ltx_Math" display="inline" id="S2.I2.i3.p1.1.m1.1"><semantics id="S2.I2.i3.p1.1.m1.1a"><mi id="S2.I2.i3.p1.1.m1.1.1" xref="S2.I2.i3.p1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i3.p1.1.m1.1b"><ci id="S2.I2.i3.p1.1.m1.1.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i3.p1.1.m1.1c">u</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i3.p1.1.m1.1d">italic_u</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">The subsequent negative sampling strategies have generally focused on augmenting the ”hardness” of <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS2.SSS1.p2.1.1">HNS</em>. This includes but is not limited to introducing supplementary information <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib142" title="">2015</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>, enhancing the sampling weight of <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS2.SSS1.p2.1.2">HNS</em> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>, and employing positive representation interpolation <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib136" title="">2022</a>)</cite>. However, the excessively high ”hardness” of <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S2.SS2.SSS1.p2.1.3">NS</em> may introduce bias during the back-propagation process, potentially leading to the poor convergence speed and quickly falling into local optimum. Furthermore, the ”hardness” of samples varies significantly across different recommendation algorithms and datasets, resulting in substantial instability in parameter selection <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>. To summarize, the <span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.p2.1.4">False Negative Problem</span> stands as a prevalent and urgent challenge to be solved in RS.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Illustration of seven traditional negative sampling strategies with their sampling criteria, time complexities of candidate generation and negative selection, where <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S2.T1.10.m1.1"><semantics id="S2.T1.10.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.T1.10.m1.1.1" xref="S2.T1.10.m1.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.10.m1.1c"><ci id="S2.T1.10.m1.1.1.cmml" xref="S2.T1.10.m1.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.m1.1d">\mathcal{I}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.m1.1e">caligraphic_I</annotation></semantics></math> denotes the item set, <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.T1.11.m2.1"><semantics id="S2.T1.11.m2.1b"><mi class="ltx_font_mathcaligraphic" id="S2.T1.11.m2.1.1" xref="S2.T1.11.m2.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.11.m2.1c"><ci id="S2.T1.11.m2.1.1.cmml" xref="S2.T1.11.m2.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.m2.1d">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.m2.1e">caligraphic_R</annotation></semantics></math> denotes the interaction set, <math alttext="L" class="ltx_Math" display="inline" id="S2.T1.12.m3.1"><semantics id="S2.T1.12.m3.1b"><mi id="S2.T1.12.m3.1.1" xref="S2.T1.12.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.T1.12.m3.1c"><ci id="S2.T1.12.m3.1.1.cmml" xref="S2.T1.12.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.m3.1d">L</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.m3.1e">italic_L</annotation></semantics></math> denotes the number of GNN layers, <math alttext="C" class="ltx_Math" display="inline" id="S2.T1.13.m4.1"><semantics id="S2.T1.13.m4.1b"><mi id="S2.T1.13.m4.1.1" xref="S2.T1.13.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.T1.13.m4.1c"><ci id="S2.T1.13.m4.1.1.cmml" xref="S2.T1.13.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.m4.1d">C</annotation><annotation encoding="application/x-llamapun" id="S2.T1.13.m4.1e">italic_C</annotation></semantics></math> denotes the length of candidates, <math alttext="C_{a}" class="ltx_Math" display="inline" id="S2.T1.14.m5.1"><semantics id="S2.T1.14.m5.1b"><msub id="S2.T1.14.m5.1.1" xref="S2.T1.14.m5.1.1.cmml"><mi id="S2.T1.14.m5.1.1.2" xref="S2.T1.14.m5.1.1.2.cmml">C</mi><mi id="S2.T1.14.m5.1.1.3" xref="S2.T1.14.m5.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.14.m5.1c"><apply id="S2.T1.14.m5.1.1.cmml" xref="S2.T1.14.m5.1.1"><csymbol cd="ambiguous" id="S2.T1.14.m5.1.1.1.cmml" xref="S2.T1.14.m5.1.1">subscript</csymbol><ci id="S2.T1.14.m5.1.1.2.cmml" xref="S2.T1.14.m5.1.1.2">𝐶</ci><ci id="S2.T1.14.m5.1.1.3.cmml" xref="S2.T1.14.m5.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.m5.1d">C_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.14.m5.1e">italic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> denotes the additional length of memory candidates, <math alttext="T_{c}" class="ltx_Math" display="inline" id="S2.T1.15.m6.1"><semantics id="S2.T1.15.m6.1b"><msub id="S2.T1.15.m6.1.1" xref="S2.T1.15.m6.1.1.cmml"><mi id="S2.T1.15.m6.1.1.2" xref="S2.T1.15.m6.1.1.2.cmml">T</mi><mi id="S2.T1.15.m6.1.1.3" xref="S2.T1.15.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.15.m6.1c"><apply id="S2.T1.15.m6.1.1.cmml" xref="S2.T1.15.m6.1.1"><csymbol cd="ambiguous" id="S2.T1.15.m6.1.1.1.cmml" xref="S2.T1.15.m6.1.1">subscript</csymbol><ci id="S2.T1.15.m6.1.1.2.cmml" xref="S2.T1.15.m6.1.1.2">𝑇</ci><ci id="S2.T1.15.m6.1.1.3.cmml" xref="S2.T1.15.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.m6.1d">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.15.m6.1e">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> denotes the time complexity of computing an instance score, <math alttext="T_{s}" class="ltx_Math" display="inline" id="S2.T1.16.m7.1"><semantics id="S2.T1.16.m7.1b"><msub id="S2.T1.16.m7.1.1" xref="S2.T1.16.m7.1.1.cmml"><mi id="S2.T1.16.m7.1.1.2" xref="S2.T1.16.m7.1.1.2.cmml">T</mi><mi id="S2.T1.16.m7.1.1.3" xref="S2.T1.16.m7.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.16.m7.1c"><apply id="S2.T1.16.m7.1.1.cmml" xref="S2.T1.16.m7.1.1"><csymbol cd="ambiguous" id="S2.T1.16.m7.1.1.1.cmml" xref="S2.T1.16.m7.1.1">subscript</csymbol><ci id="S2.T1.16.m7.1.1.2.cmml" xref="S2.T1.16.m7.1.1.2">𝑇</ci><ci id="S2.T1.16.m7.1.1.3.cmml" xref="S2.T1.16.m7.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.m7.1d">T_{s}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.16.m7.1e">italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> denotes the time complexity of negative selection with a given sampling probability, <math alttext="T_{v}" class="ltx_Math" display="inline" id="S2.T1.17.m8.1"><semantics id="S2.T1.17.m8.1b"><msub id="S2.T1.17.m8.1.1" xref="S2.T1.17.m8.1.1.cmml"><mi id="S2.T1.17.m8.1.1.2" xref="S2.T1.17.m8.1.1.2.cmml">T</mi><mi id="S2.T1.17.m8.1.1.3" xref="S2.T1.17.m8.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.17.m8.1c"><apply id="S2.T1.17.m8.1.1.cmml" xref="S2.T1.17.m8.1.1"><csymbol cd="ambiguous" id="S2.T1.17.m8.1.1.1.cmml" xref="S2.T1.17.m8.1.1">subscript</csymbol><ci id="S2.T1.17.m8.1.1.2.cmml" xref="S2.T1.17.m8.1.1.2">𝑇</ci><ci id="S2.T1.17.m8.1.1.3.cmml" xref="S2.T1.17.m8.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.m8.1d">T_{v}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.17.m8.1e">italic_T start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> denotes the time complexity of computing the variance of each instance. Note that <math alttext="|\mathcal{R}|\gg|\mathcal{I}|\gg C\approx C_{a}" class="ltx_Math" display="inline" id="S2.T1.18.m9.2"><semantics id="S2.T1.18.m9.2b"><mrow id="S2.T1.18.m9.2.3" xref="S2.T1.18.m9.2.3.cmml"><mrow id="S2.T1.18.m9.2.3.2.2" xref="S2.T1.18.m9.2.3.2.1.cmml"><mo id="S2.T1.18.m9.2.3.2.2.1" stretchy="false" xref="S2.T1.18.m9.2.3.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.T1.18.m9.1.1" xref="S2.T1.18.m9.1.1.cmml">ℛ</mi><mo id="S2.T1.18.m9.2.3.2.2.2" stretchy="false" xref="S2.T1.18.m9.2.3.2.1.1.cmml">|</mo></mrow><mo id="S2.T1.18.m9.2.3.3" xref="S2.T1.18.m9.2.3.3.cmml">≫</mo><mrow id="S2.T1.18.m9.2.3.4.2" xref="S2.T1.18.m9.2.3.4.1.cmml"><mo id="S2.T1.18.m9.2.3.4.2.1" stretchy="false" xref="S2.T1.18.m9.2.3.4.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.T1.18.m9.2.2" xref="S2.T1.18.m9.2.2.cmml">ℐ</mi><mo id="S2.T1.18.m9.2.3.4.2.2" stretchy="false" xref="S2.T1.18.m9.2.3.4.1.1.cmml">|</mo></mrow><mo id="S2.T1.18.m9.2.3.5" xref="S2.T1.18.m9.2.3.5.cmml">≫</mo><mi id="S2.T1.18.m9.2.3.6" xref="S2.T1.18.m9.2.3.6.cmml">C</mi><mo id="S2.T1.18.m9.2.3.7" xref="S2.T1.18.m9.2.3.7.cmml">≈</mo><msub id="S2.T1.18.m9.2.3.8" xref="S2.T1.18.m9.2.3.8.cmml"><mi id="S2.T1.18.m9.2.3.8.2" xref="S2.T1.18.m9.2.3.8.2.cmml">C</mi><mi id="S2.T1.18.m9.2.3.8.3" xref="S2.T1.18.m9.2.3.8.3.cmml">a</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.18.m9.2c"><apply id="S2.T1.18.m9.2.3.cmml" xref="S2.T1.18.m9.2.3"><and id="S2.T1.18.m9.2.3a.cmml" xref="S2.T1.18.m9.2.3"></and><apply id="S2.T1.18.m9.2.3b.cmml" xref="S2.T1.18.m9.2.3"><csymbol cd="latexml" id="S2.T1.18.m9.2.3.3.cmml" xref="S2.T1.18.m9.2.3.3">much-greater-than</csymbol><apply id="S2.T1.18.m9.2.3.2.1.cmml" xref="S2.T1.18.m9.2.3.2.2"><abs id="S2.T1.18.m9.2.3.2.1.1.cmml" xref="S2.T1.18.m9.2.3.2.2.1"></abs><ci id="S2.T1.18.m9.1.1.cmml" xref="S2.T1.18.m9.1.1">ℛ</ci></apply><apply id="S2.T1.18.m9.2.3.4.1.cmml" xref="S2.T1.18.m9.2.3.4.2"><abs id="S2.T1.18.m9.2.3.4.1.1.cmml" xref="S2.T1.18.m9.2.3.4.2.1"></abs><ci id="S2.T1.18.m9.2.2.cmml" xref="S2.T1.18.m9.2.2">ℐ</ci></apply></apply><apply id="S2.T1.18.m9.2.3c.cmml" xref="S2.T1.18.m9.2.3"><csymbol cd="latexml" id="S2.T1.18.m9.2.3.5.cmml" xref="S2.T1.18.m9.2.3.5">much-greater-than</csymbol><share href="https://arxiv.org/html/2409.07237v1#S2.T1.18.m9.2.3.4.cmml" id="S2.T1.18.m9.2.3d.cmml" xref="S2.T1.18.m9.2.3"></share><ci id="S2.T1.18.m9.2.3.6.cmml" xref="S2.T1.18.m9.2.3.6">𝐶</ci></apply><apply id="S2.T1.18.m9.2.3e.cmml" xref="S2.T1.18.m9.2.3"><approx id="S2.T1.18.m9.2.3.7.cmml" xref="S2.T1.18.m9.2.3.7"></approx><share href="https://arxiv.org/html/2409.07237v1#S2.T1.18.m9.2.3.6.cmml" id="S2.T1.18.m9.2.3f.cmml" xref="S2.T1.18.m9.2.3"></share><apply id="S2.T1.18.m9.2.3.8.cmml" xref="S2.T1.18.m9.2.3.8"><csymbol cd="ambiguous" id="S2.T1.18.m9.2.3.8.1.cmml" xref="S2.T1.18.m9.2.3.8">subscript</csymbol><ci id="S2.T1.18.m9.2.3.8.2.cmml" xref="S2.T1.18.m9.2.3.8.2">𝐶</ci><ci id="S2.T1.18.m9.2.3.8.3.cmml" xref="S2.T1.18.m9.2.3.8.3">𝑎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.m9.2d">|\mathcal{R}|\gg|\mathcal{I}|\gg C\approx C_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.18.m9.2e">| caligraphic_R | ≫ | caligraphic_I | ≫ italic_C ≈ italic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.32" style="width:433.6pt;height:147.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.8pt,12.8pt) scale(0.851689355879078,0.851689355879078) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.32.14">
<tr class="ltx_tr" id="S2.T1.32.14.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.32.14.15.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.1.1" style="font-size:80%;">Algorithms</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.2.1" style="font-size:80%;">Venue</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.3" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.3.1" style="font-size:80%;">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.4.1" style="font-size:80%;">Scenarios</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.5.1" style="font-size:80%;">Sampling criteria</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.6" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.32.14.15.6.1">
<tr class="ltx_tr" id="S2.T1.32.14.15.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.32.14.15.6.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.6.1.1.1.1" style="font-size:80%;">Candidate</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.32.14.15.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.32.14.15.6.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.6.1.2.1.1" style="font-size:80%;">generation</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.32.14.15.7" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.32.14.15.7.1">
<tr class="ltx_tr" id="S2.T1.32.14.15.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.32.14.15.7.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.7.1.1.1.1" style="font-size:80%;">Negative</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.32.14.15.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.32.14.15.7.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.32.14.15.7.1.2.1.1" style="font-size:80%;">selection</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.20.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.20.2.2.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.20.2.2.3.1" style="font-size:80%;">RNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.20.2.2.3.2.1" style="font-size:80%;">(</span>Rendle et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.20.2.2.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a><span class="ltx_text" id="S2.T1.20.2.2.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.20.2.2.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.20.2.2.4.1" style="font-size:80%;">UAI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.20.2.2.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.20.2.2.5.1" style="font-size:80%;">2009</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.20.2.2.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.20.2.2.6.1" style="font-size:80%;">CF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.20.2.2.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.20.2.2.7.1" style="font-size:80%;">Random sampling</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.19.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O(|I|)}" class="ltx_Math" display="inline" id="S2.T1.19.1.1.1.m1.2"><semantics id="S2.T1.19.1.1.1.m1.2a"><mrow id="S2.T1.19.1.1.1.m1.2.2" xref="S2.T1.19.1.1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.19.1.1.1.m1.2.2.3" mathsize="80%" xref="S2.T1.19.1.1.1.m1.2.2.3.cmml">𝒪</mi><mo id="S2.T1.19.1.1.1.m1.2.2.2" xref="S2.T1.19.1.1.1.m1.2.2.2.cmml">⁢</mo><mrow id="S2.T1.19.1.1.1.m1.2.2.1.1" xref="S2.T1.19.1.1.1.m1.2.2.cmml"><mo id="S2.T1.19.1.1.1.m1.2.2.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.19.1.1.1.m1.2.2.cmml">(</mo><mrow id="S2.T1.19.1.1.1.m1.2.2.1.1.1.2" xref="S2.T1.19.1.1.1.m1.2.2.1.1.1.1.cmml"><mo id="S2.T1.19.1.1.1.m1.2.2.1.1.1.2.1" maxsize="80%" minsize="80%" xref="S2.T1.19.1.1.1.m1.2.2.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.T1.19.1.1.1.m1.1.1" mathsize="80%" xref="S2.T1.19.1.1.1.m1.1.1.cmml">ℐ</mi><mo id="S2.T1.19.1.1.1.m1.2.2.1.1.1.2.2" maxsize="80%" minsize="80%" xref="S2.T1.19.1.1.1.m1.2.2.1.1.1.1.1.cmml">|</mo></mrow><mo id="S2.T1.19.1.1.1.m1.2.2.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.19.1.1.1.m1.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.19.1.1.1.m1.2b"><apply id="S2.T1.19.1.1.1.m1.2.2.cmml" xref="S2.T1.19.1.1.1.m1.2.2"><times id="S2.T1.19.1.1.1.m1.2.2.2.cmml" xref="S2.T1.19.1.1.1.m1.2.2.2"></times><ci id="S2.T1.19.1.1.1.m1.2.2.3.cmml" xref="S2.T1.19.1.1.1.m1.2.2.3">𝒪</ci><apply id="S2.T1.19.1.1.1.m1.2.2.1.1.1.1.cmml" xref="S2.T1.19.1.1.1.m1.2.2.1.1.1.2"><abs id="S2.T1.19.1.1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.T1.19.1.1.1.m1.2.2.1.1.1.2.1"></abs><ci id="S2.T1.19.1.1.1.m1.1.1.cmml" xref="S2.T1.19.1.1.1.m1.1.1">ℐ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.1.1.1.m1.2c">\mathcal{O(|I|)}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.19.1.1.1.m1.2d">caligraphic_O ( | caligraphic_I | )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.20.2.2.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(T_{s})" class="ltx_Math" display="inline" id="S2.T1.20.2.2.2.m1.1"><semantics id="S2.T1.20.2.2.2.m1.1a"><mrow id="S2.T1.20.2.2.2.m1.1.1" xref="S2.T1.20.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.20.2.2.2.m1.1.1.3" mathsize="80%" xref="S2.T1.20.2.2.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.20.2.2.2.m1.1.1.2" xref="S2.T1.20.2.2.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.20.2.2.2.m1.1.1.1.1" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.20.2.2.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.cmml">(</mo><msub id="S2.T1.20.2.2.2.m1.1.1.1.1.1" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.cmml"><mi id="S2.T1.20.2.2.2.m1.1.1.1.1.1.2" mathsize="80%" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.2.cmml">T</mi><mi id="S2.T1.20.2.2.2.m1.1.1.1.1.1.3" mathsize="80%" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S2.T1.20.2.2.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.20.2.2.2.m1.1b"><apply id="S2.T1.20.2.2.2.m1.1.1.cmml" xref="S2.T1.20.2.2.2.m1.1.1"><times id="S2.T1.20.2.2.2.m1.1.1.2.cmml" xref="S2.T1.20.2.2.2.m1.1.1.2"></times><ci id="S2.T1.20.2.2.2.m1.1.1.3.cmml" xref="S2.T1.20.2.2.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.20.2.2.2.m1.1.1.1.1.1.cmml" xref="S2.T1.20.2.2.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T1.20.2.2.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.20.2.2.2.m1.1.1.1.1">subscript</csymbol><ci id="S2.T1.20.2.2.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.2">𝑇</ci><ci id="S2.T1.20.2.2.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.20.2.2.2.m1.1.1.1.1.1.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.2.2.2.m1.1c">\mathcal{O}(T_{s})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.20.2.2.2.m1.1d">caligraphic_O ( italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.22.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.22.4.4.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.22.4.4.3.1" style="font-size:80%;">DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.22.4.4.3.2.1" style="font-size:80%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.22.4.4.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a><span class="ltx_text" id="S2.T1.22.4.4.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.22.4.4.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.22.4.4.4.1" style="font-size:80%;">SIGIR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.22.4.4.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.22.4.4.5.1" style="font-size:80%;">2013</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.22.4.4.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.22.4.4.6.1" style="font-size:80%;">CF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.22.4.4.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.22.4.4.7.1" style="font-size:80%;">Top-ranked</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.21.3.3.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(C)" class="ltx_Math" display="inline" id="S2.T1.21.3.3.1.m1.1"><semantics id="S2.T1.21.3.3.1.m1.1a"><mrow id="S2.T1.21.3.3.1.m1.1.2" xref="S2.T1.21.3.3.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.21.3.3.1.m1.1.2.2" mathsize="80%" xref="S2.T1.21.3.3.1.m1.1.2.2.cmml">𝒪</mi><mo id="S2.T1.21.3.3.1.m1.1.2.1" xref="S2.T1.21.3.3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T1.21.3.3.1.m1.1.2.3.2" xref="S2.T1.21.3.3.1.m1.1.2.cmml"><mo id="S2.T1.21.3.3.1.m1.1.2.3.2.1" maxsize="80%" minsize="80%" xref="S2.T1.21.3.3.1.m1.1.2.cmml">(</mo><mi id="S2.T1.21.3.3.1.m1.1.1" mathsize="80%" xref="S2.T1.21.3.3.1.m1.1.1.cmml">C</mi><mo id="S2.T1.21.3.3.1.m1.1.2.3.2.2" maxsize="80%" minsize="80%" xref="S2.T1.21.3.3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.21.3.3.1.m1.1b"><apply id="S2.T1.21.3.3.1.m1.1.2.cmml" xref="S2.T1.21.3.3.1.m1.1.2"><times id="S2.T1.21.3.3.1.m1.1.2.1.cmml" xref="S2.T1.21.3.3.1.m1.1.2.1"></times><ci id="S2.T1.21.3.3.1.m1.1.2.2.cmml" xref="S2.T1.21.3.3.1.m1.1.2.2">𝒪</ci><ci id="S2.T1.21.3.3.1.m1.1.1.cmml" xref="S2.T1.21.3.3.1.m1.1.1">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.21.3.3.1.m1.1c">\mathcal{O}(C)</annotation><annotation encoding="application/x-llamapun" id="S2.T1.21.3.3.1.m1.1d">caligraphic_O ( italic_C )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.22.4.4.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(CT_{c})" class="ltx_Math" display="inline" id="S2.T1.22.4.4.2.m1.1"><semantics id="S2.T1.22.4.4.2.m1.1a"><mrow id="S2.T1.22.4.4.2.m1.1.1" xref="S2.T1.22.4.4.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.22.4.4.2.m1.1.1.3" mathsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.22.4.4.2.m1.1.1.2" xref="S2.T1.22.4.4.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.22.4.4.2.m1.1.1.1.1" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.22.4.4.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.22.4.4.2.m1.1.1.1.1.1" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.cmml"><mi id="S2.T1.22.4.4.2.m1.1.1.1.1.1.2" mathsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.2.cmml">C</mi><mo id="S2.T1.22.4.4.2.m1.1.1.1.1.1.1" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.3.cmml">c</mi></msub></mrow><mo id="S2.T1.22.4.4.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.22.4.4.2.m1.1b"><apply id="S2.T1.22.4.4.2.m1.1.1.cmml" xref="S2.T1.22.4.4.2.m1.1.1"><times id="S2.T1.22.4.4.2.m1.1.1.2.cmml" xref="S2.T1.22.4.4.2.m1.1.1.2"></times><ci id="S2.T1.22.4.4.2.m1.1.1.3.cmml" xref="S2.T1.22.4.4.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.22.4.4.2.m1.1.1.1.1.1.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1"><times id="S2.T1.22.4.4.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.1"></times><ci id="S2.T1.22.4.4.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.2">𝐶</ci><apply id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.3.cmml" xref="S2.T1.22.4.4.2.m1.1.1.1.1.1.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.22.4.4.2.m1.1c">\mathcal{O}(CT_{c})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.22.4.4.2.m1.1d">caligraphic_O ( italic_C italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.24.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.24.6.6.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.24.6.6.3.1" style="font-size:80%;">PNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.24.6.6.3.2.1" style="font-size:80%;">(</span>Rendle and Freudenthaler<span class="ltx_text" id="S2.T1.24.6.6.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a><span class="ltx_text" id="S2.T1.24.6.6.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.24.6.6.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.24.6.6.4.1" style="font-size:80%;">WSDM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.24.6.6.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.24.6.6.5.1" style="font-size:80%;">2014</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.24.6.6.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.24.6.6.6.1" style="font-size:80%;">CF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.24.6.6.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.24.6.6.7.1" style="font-size:80%;">Popularity</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.23.5.5.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O(|R|)}" class="ltx_Math" display="inline" id="S2.T1.23.5.5.1.m1.2"><semantics id="S2.T1.23.5.5.1.m1.2a"><mrow id="S2.T1.23.5.5.1.m1.2.2" xref="S2.T1.23.5.5.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.23.5.5.1.m1.2.2.3" mathsize="80%" xref="S2.T1.23.5.5.1.m1.2.2.3.cmml">𝒪</mi><mo id="S2.T1.23.5.5.1.m1.2.2.2" xref="S2.T1.23.5.5.1.m1.2.2.2.cmml">⁢</mo><mrow id="S2.T1.23.5.5.1.m1.2.2.1.1" xref="S2.T1.23.5.5.1.m1.2.2.cmml"><mo id="S2.T1.23.5.5.1.m1.2.2.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.23.5.5.1.m1.2.2.cmml">(</mo><mrow id="S2.T1.23.5.5.1.m1.2.2.1.1.1.2" xref="S2.T1.23.5.5.1.m1.2.2.1.1.1.1.cmml"><mo id="S2.T1.23.5.5.1.m1.2.2.1.1.1.2.1" maxsize="80%" minsize="80%" xref="S2.T1.23.5.5.1.m1.2.2.1.1.1.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.T1.23.5.5.1.m1.1.1" mathsize="80%" xref="S2.T1.23.5.5.1.m1.1.1.cmml">ℛ</mi><mo id="S2.T1.23.5.5.1.m1.2.2.1.1.1.2.2" maxsize="80%" minsize="80%" xref="S2.T1.23.5.5.1.m1.2.2.1.1.1.1.1.cmml">|</mo></mrow><mo id="S2.T1.23.5.5.1.m1.2.2.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.23.5.5.1.m1.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.23.5.5.1.m1.2b"><apply id="S2.T1.23.5.5.1.m1.2.2.cmml" xref="S2.T1.23.5.5.1.m1.2.2"><times id="S2.T1.23.5.5.1.m1.2.2.2.cmml" xref="S2.T1.23.5.5.1.m1.2.2.2"></times><ci id="S2.T1.23.5.5.1.m1.2.2.3.cmml" xref="S2.T1.23.5.5.1.m1.2.2.3">𝒪</ci><apply id="S2.T1.23.5.5.1.m1.2.2.1.1.1.1.cmml" xref="S2.T1.23.5.5.1.m1.2.2.1.1.1.2"><abs id="S2.T1.23.5.5.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.T1.23.5.5.1.m1.2.2.1.1.1.2.1"></abs><ci id="S2.T1.23.5.5.1.m1.1.1.cmml" xref="S2.T1.23.5.5.1.m1.1.1">ℛ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.23.5.5.1.m1.2c">\mathcal{O(|R|)}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.23.5.5.1.m1.2d">caligraphic_O ( | caligraphic_R | )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.24.6.6.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(T_{s})" class="ltx_Math" display="inline" id="S2.T1.24.6.6.2.m1.1"><semantics id="S2.T1.24.6.6.2.m1.1a"><mrow id="S2.T1.24.6.6.2.m1.1.1" xref="S2.T1.24.6.6.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.24.6.6.2.m1.1.1.3" mathsize="80%" xref="S2.T1.24.6.6.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.24.6.6.2.m1.1.1.2" xref="S2.T1.24.6.6.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.24.6.6.2.m1.1.1.1.1" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.24.6.6.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.cmml">(</mo><msub id="S2.T1.24.6.6.2.m1.1.1.1.1.1" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.cmml"><mi id="S2.T1.24.6.6.2.m1.1.1.1.1.1.2" mathsize="80%" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.2.cmml">T</mi><mi id="S2.T1.24.6.6.2.m1.1.1.1.1.1.3" mathsize="80%" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S2.T1.24.6.6.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.24.6.6.2.m1.1b"><apply id="S2.T1.24.6.6.2.m1.1.1.cmml" xref="S2.T1.24.6.6.2.m1.1.1"><times id="S2.T1.24.6.6.2.m1.1.1.2.cmml" xref="S2.T1.24.6.6.2.m1.1.1.2"></times><ci id="S2.T1.24.6.6.2.m1.1.1.3.cmml" xref="S2.T1.24.6.6.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.24.6.6.2.m1.1.1.1.1.1.cmml" xref="S2.T1.24.6.6.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.T1.24.6.6.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.24.6.6.2.m1.1.1.1.1">subscript</csymbol><ci id="S2.T1.24.6.6.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.2">𝑇</ci><ci id="S2.T1.24.6.6.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.24.6.6.2.m1.1.1.1.1.1.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.24.6.6.2.m1.1c">\mathcal{O}(T_{s})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.24.6.6.2.m1.1d">caligraphic_O ( italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.26.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.26.8.8.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.26.8.8.3.1" style="font-size:80%;">SRNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.26.8.8.3.2.1" style="font-size:80%;">(</span>Ding et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.26.8.8.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a><span class="ltx_text" id="S2.T1.26.8.8.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.26.8.8.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.26.8.8.4.1" style="font-size:80%;">NeuIPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.26.8.8.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.26.8.8.5.1" style="font-size:80%;">2020</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.26.8.8.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.26.8.8.6.1" style="font-size:80%;">CF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.26.8.8.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.26.8.8.7.1" style="font-size:80%;">Variance-based</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.25.7.7.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}((C+C_{a})T_{c}/E)" class="ltx_Math" display="inline" id="S2.T1.25.7.7.1.m1.1"><semantics id="S2.T1.25.7.7.1.m1.1a"><mrow id="S2.T1.25.7.7.1.m1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.25.7.7.1.m1.1.1.3" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.25.7.7.1.m1.1.1.2" xref="S2.T1.25.7.7.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.25.7.7.1.m1.1.1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.cmml"><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.25.7.7.1.m1.1.1.1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.cmml"><mrow id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.2" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">C</mi><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.1" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">a</mi></msub></mrow><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.2" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.2.cmml">⁢</mo><msub id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.3.cmml">c</mi></msub></mrow><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.1.2" maxsize="80%" minsize="80%" stretchy="true" symmetric="true" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.2.cmml">/</mo><mi id="S2.T1.25.7.7.1.m1.1.1.1.1.1.3" mathsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.3.cmml">E</mi></mrow><mo id="S2.T1.25.7.7.1.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.25.7.7.1.m1.1b"><apply id="S2.T1.25.7.7.1.m1.1.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1"><times id="S2.T1.25.7.7.1.m1.1.1.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.2"></times><ci id="S2.T1.25.7.7.1.m1.1.1.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.3">𝒪</ci><apply id="S2.T1.25.7.7.1.m1.1.1.1.1.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1"><divide id="S2.T1.25.7.7.1.m1.1.1.1.1.1.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.2"></divide><apply id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1"><times id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.2"></times><apply id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1"><plus id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.1"></plus><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.2">𝐶</ci><apply id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.1.1.1.3.3">𝑎</ci></apply></apply><apply id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.1.3.3">𝑐</ci></apply></apply><ci id="S2.T1.25.7.7.1.m1.1.1.1.1.1.3.cmml" xref="S2.T1.25.7.7.1.m1.1.1.1.1.1.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.25.7.7.1.m1.1c">\mathcal{O}((C+C_{a})T_{c}/E)</annotation><annotation encoding="application/x-llamapun" id="S2.T1.25.7.7.1.m1.1d">caligraphic_O ( ( italic_C + italic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT / italic_E )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.26.8.8.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(C(T_{c}+T_{v}))" class="ltx_Math" display="inline" id="S2.T1.26.8.8.2.m1.1"><semantics id="S2.T1.26.8.8.2.m1.1a"><mrow id="S2.T1.26.8.8.2.m1.1.1" xref="S2.T1.26.8.8.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.26.8.8.2.m1.1.1.3" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.26.8.8.2.m1.1.1.2" xref="S2.T1.26.8.8.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.26.8.8.2.m1.1.1.1.1" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.26.8.8.2.m1.1.1.1.1.1" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.cmml"><mi id="S2.T1.26.8.8.2.m1.1.1.1.1.1.3" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.3.cmml">C</mi><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.1.2" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.2" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mi id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.3" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.1" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.3.cmml">v</mi></msub></mrow><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.T1.26.8.8.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.26.8.8.2.m1.1b"><apply id="S2.T1.26.8.8.2.m1.1.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1"><times id="S2.T1.26.8.8.2.m1.1.1.2.cmml" xref="S2.T1.26.8.8.2.m1.1.1.2"></times><ci id="S2.T1.26.8.8.2.m1.1.1.3.cmml" xref="S2.T1.26.8.8.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.26.8.8.2.m1.1.1.1.1.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1"><times id="S2.T1.26.8.8.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.2"></times><ci id="S2.T1.26.8.8.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.3">𝐶</ci><apply id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1"><plus id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.T1.26.8.8.2.m1.1.1.1.1.1.1.1.1.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.26.8.8.2.m1.1c">\mathcal{O}(C(T_{c}+T_{v}))</annotation><annotation encoding="application/x-llamapun" id="S2.T1.26.8.8.2.m1.1d">caligraphic_O ( italic_C ( italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.28.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.28.10.10.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.28.10.10.3.1" style="font-size:80%;">MixGCF </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.28.10.10.3.2.1" style="font-size:80%;">(</span>Huang et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.28.10.10.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a><span class="ltx_text" id="S2.T1.28.10.10.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.28.10.10.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.28.10.10.4.1" style="font-size:80%;">KDD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.28.10.10.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.28.10.10.5.1" style="font-size:80%;">2021</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.28.10.10.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.28.10.10.6.1" style="font-size:80%;">GR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.28.10.10.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.28.10.10.7.1" style="font-size:80%;">Interpolation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.27.9.9.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(CL)" class="ltx_Math" display="inline" id="S2.T1.27.9.9.1.m1.1"><semantics id="S2.T1.27.9.9.1.m1.1a"><mrow id="S2.T1.27.9.9.1.m1.1.1" xref="S2.T1.27.9.9.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.27.9.9.1.m1.1.1.3" mathsize="80%" xref="S2.T1.27.9.9.1.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.27.9.9.1.m1.1.1.2" xref="S2.T1.27.9.9.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.27.9.9.1.m1.1.1.1.1" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.cmml"><mo id="S2.T1.27.9.9.1.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.27.9.9.1.m1.1.1.1.1.1" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.cmml"><mi id="S2.T1.27.9.9.1.m1.1.1.1.1.1.2" mathsize="80%" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.2.cmml">C</mi><mo id="S2.T1.27.9.9.1.m1.1.1.1.1.1.1" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.T1.27.9.9.1.m1.1.1.1.1.1.3" mathsize="80%" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.3.cmml">L</mi></mrow><mo id="S2.T1.27.9.9.1.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.27.9.9.1.m1.1b"><apply id="S2.T1.27.9.9.1.m1.1.1.cmml" xref="S2.T1.27.9.9.1.m1.1.1"><times id="S2.T1.27.9.9.1.m1.1.1.2.cmml" xref="S2.T1.27.9.9.1.m1.1.1.2"></times><ci id="S2.T1.27.9.9.1.m1.1.1.3.cmml" xref="S2.T1.27.9.9.1.m1.1.1.3">𝒪</ci><apply id="S2.T1.27.9.9.1.m1.1.1.1.1.1.cmml" xref="S2.T1.27.9.9.1.m1.1.1.1.1"><times id="S2.T1.27.9.9.1.m1.1.1.1.1.1.1.cmml" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.1"></times><ci id="S2.T1.27.9.9.1.m1.1.1.1.1.1.2.cmml" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.2">𝐶</ci><ci id="S2.T1.27.9.9.1.m1.1.1.1.1.1.3.cmml" xref="S2.T1.27.9.9.1.m1.1.1.1.1.1.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.27.9.9.1.m1.1c">\mathcal{O}(CL)</annotation><annotation encoding="application/x-llamapun" id="S2.T1.27.9.9.1.m1.1d">caligraphic_O ( italic_C italic_L )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.28.10.10.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(CLT_{c}+T_{s})" class="ltx_Math" display="inline" id="S2.T1.28.10.10.2.m1.1"><semantics id="S2.T1.28.10.10.2.m1.1a"><mrow id="S2.T1.28.10.10.2.m1.1.1" xref="S2.T1.28.10.10.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.28.10.10.2.m1.1.1.3" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.28.10.10.2.m1.1.1.2" xref="S2.T1.28.10.10.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.28.10.10.2.m1.1.1.1.1" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.28.10.10.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.28.10.10.2.m1.1.1.1.1.1" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.cmml"><mrow id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.cmml"><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.2" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.2.cmml">C</mi><mo id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.3" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.3.cmml">L</mi><mo id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1a" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.cmml"><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.2" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.2.cmml">T</mi><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.3" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.3.cmml">c</mi></msub></mrow><mo id="S2.T1.28.10.10.2.m1.1.1.1.1.1.1" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S2.T1.28.10.10.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.28.10.10.2.m1.1b"><apply id="S2.T1.28.10.10.2.m1.1.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1"><times id="S2.T1.28.10.10.2.m1.1.1.2.cmml" xref="S2.T1.28.10.10.2.m1.1.1.2"></times><ci id="S2.T1.28.10.10.2.m1.1.1.3.cmml" xref="S2.T1.28.10.10.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.28.10.10.2.m1.1.1.1.1.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1"><plus id="S2.T1.28.10.10.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.1"></plus><apply id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2"><times id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.1"></times><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.2.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.2">𝐶</ci><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.3.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.3">𝐿</ci><apply id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.2.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.2">𝑇</ci><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.3.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.2.4.3">𝑐</ci></apply></apply><apply id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.3.cmml" xref="S2.T1.28.10.10.2.m1.1.1.1.1.1.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.28.10.10.2.m1.1c">\mathcal{O}(CLT_{c}+T_{s})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.28.10.10.2.m1.1d">caligraphic_O ( italic_C italic_L italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.30.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.30.12.12.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.30.12.12.3.1" style="font-size:80%;">DNS* </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.30.12.12.3.2.1" style="font-size:80%;">(</span>Shi et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.30.12.12.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a><span class="ltx_text" id="S2.T1.30.12.12.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.30.12.12.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.30.12.12.4.1" style="font-size:80%;">WWW</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.30.12.12.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.30.12.12.5.1" style="font-size:80%;">2023</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.30.12.12.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.30.12.12.6.1" style="font-size:80%;">CF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.30.12.12.7" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.30.12.12.7.1" style="font-size:80%;">Highest-score scope</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.29.11.11.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(C)" class="ltx_Math" display="inline" id="S2.T1.29.11.11.1.m1.1"><semantics id="S2.T1.29.11.11.1.m1.1a"><mrow id="S2.T1.29.11.11.1.m1.1.2" xref="S2.T1.29.11.11.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.29.11.11.1.m1.1.2.2" mathsize="80%" xref="S2.T1.29.11.11.1.m1.1.2.2.cmml">𝒪</mi><mo id="S2.T1.29.11.11.1.m1.1.2.1" xref="S2.T1.29.11.11.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.T1.29.11.11.1.m1.1.2.3.2" xref="S2.T1.29.11.11.1.m1.1.2.cmml"><mo id="S2.T1.29.11.11.1.m1.1.2.3.2.1" maxsize="80%" minsize="80%" xref="S2.T1.29.11.11.1.m1.1.2.cmml">(</mo><mi id="S2.T1.29.11.11.1.m1.1.1" mathsize="80%" xref="S2.T1.29.11.11.1.m1.1.1.cmml">C</mi><mo id="S2.T1.29.11.11.1.m1.1.2.3.2.2" maxsize="80%" minsize="80%" xref="S2.T1.29.11.11.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.29.11.11.1.m1.1b"><apply id="S2.T1.29.11.11.1.m1.1.2.cmml" xref="S2.T1.29.11.11.1.m1.1.2"><times id="S2.T1.29.11.11.1.m1.1.2.1.cmml" xref="S2.T1.29.11.11.1.m1.1.2.1"></times><ci id="S2.T1.29.11.11.1.m1.1.2.2.cmml" xref="S2.T1.29.11.11.1.m1.1.2.2">𝒪</ci><ci id="S2.T1.29.11.11.1.m1.1.1.cmml" xref="S2.T1.29.11.11.1.m1.1.1">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.29.11.11.1.m1.1c">\mathcal{O}(C)</annotation><annotation encoding="application/x-llamapun" id="S2.T1.29.11.11.1.m1.1d">caligraphic_O ( italic_C )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.30.12.12.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(CT_{c}+T_{s})" class="ltx_Math" display="inline" id="S2.T1.30.12.12.2.m1.1"><semantics id="S2.T1.30.12.12.2.m1.1a"><mrow id="S2.T1.30.12.12.2.m1.1.1" xref="S2.T1.30.12.12.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.30.12.12.2.m1.1.1.3" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.30.12.12.2.m1.1.1.2" xref="S2.T1.30.12.12.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.30.12.12.2.m1.1.1.1.1" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.30.12.12.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.30.12.12.2.m1.1.1.1.1.1" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.cmml"><mrow id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.cmml"><mi id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.2" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.2.cmml">C</mi><mo id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.1" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.cmml"><mi id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.2" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.2.cmml">T</mi><mi id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.3" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.3.cmml">c</mi></msub></mrow><mo id="S2.T1.30.12.12.2.m1.1.1.1.1.1.1" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S2.T1.30.12.12.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.30.12.12.2.m1.1b"><apply id="S2.T1.30.12.12.2.m1.1.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1"><times id="S2.T1.30.12.12.2.m1.1.1.2.cmml" xref="S2.T1.30.12.12.2.m1.1.1.2"></times><ci id="S2.T1.30.12.12.2.m1.1.1.3.cmml" xref="S2.T1.30.12.12.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.30.12.12.2.m1.1.1.1.1.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1"><plus id="S2.T1.30.12.12.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.1"></plus><apply id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2"><times id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.1"></times><ci id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.2.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.2">𝐶</ci><apply id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.2.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.2">𝑇</ci><ci id="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.3.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.2.3.3">𝑐</ci></apply></apply><apply id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.3.cmml" xref="S2.T1.30.12.12.2.m1.1.1.1.1.1.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.30.12.12.2.m1.1c">\mathcal{O}(CT_{c}+T_{s})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.30.12.12.2.m1.1d">caligraphic_O ( italic_C italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T1.32.14.14">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.32.14.14.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_text" id="S2.T1.32.14.14.3.1" style="font-size:80%;">RealHNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S2.T1.32.14.14.3.2.1" style="font-size:80%;">(</span>Ma et al<span class="ltx_text">.</span><span class="ltx_text" id="S2.T1.32.14.14.3.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a><span class="ltx_text" id="S2.T1.32.14.14.3.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.32.14.14.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.32.14.14.4.1" style="font-size:80%;">Recsys</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.32.14.14.5" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.32.14.14.5.1" style="font-size:80%;">2023</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.32.14.14.6" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.32.14.14.6.1" style="font-size:80%;">CDR</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.32.14.14.7" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.32.14.14.7.1">
<tr class="ltx_tr" id="S2.T1.32.14.14.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.32.14.14.7.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S2.T1.32.14.14.7.1.1.1.1" style="font-size:80%;">Multi-granularity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.31.13.13.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(\mathcal{|I|}T_{c}/E)" class="ltx_Math" display="inline" id="S2.T1.31.13.13.1.m1.2"><semantics id="S2.T1.31.13.13.1.m1.2a"><mrow id="S2.T1.31.13.13.1.m1.2.2" xref="S2.T1.31.13.13.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.31.13.13.1.m1.2.2.3" mathsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.3.cmml">𝒪</mi><mo id="S2.T1.31.13.13.1.m1.2.2.2" xref="S2.T1.31.13.13.1.m1.2.2.2.cmml">⁢</mo><mrow id="S2.T1.31.13.13.1.m1.2.2.1.1" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.cmml"><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S2.T1.31.13.13.1.m1.2.2.1.1.1" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.cmml"><mrow id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.cmml"><mrow id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.2" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.1.cmml"><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.2.1" maxsize="80%" minsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.T1.31.13.13.1.m1.1.1" mathsize="80%" xref="S2.T1.31.13.13.1.m1.1.1.cmml">ℐ</mi><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.2.2" maxsize="80%" minsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.1.1.cmml">|</mo></mrow><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.1" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.1.cmml">⁢</mo><msub id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.cmml"><mi id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.2" mathsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.2.cmml">T</mi><mi id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.3" mathsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.3.cmml">c</mi></msub></mrow><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.1.1" maxsize="80%" minsize="80%" stretchy="true" symmetric="true" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.1.cmml">/</mo><mi id="S2.T1.31.13.13.1.m1.2.2.1.1.1.3" mathsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.3.cmml">E</mi></mrow><mo id="S2.T1.31.13.13.1.m1.2.2.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.31.13.13.1.m1.2b"><apply id="S2.T1.31.13.13.1.m1.2.2.cmml" xref="S2.T1.31.13.13.1.m1.2.2"><times id="S2.T1.31.13.13.1.m1.2.2.2.cmml" xref="S2.T1.31.13.13.1.m1.2.2.2"></times><ci id="S2.T1.31.13.13.1.m1.2.2.3.cmml" xref="S2.T1.31.13.13.1.m1.2.2.3">𝒪</ci><apply id="S2.T1.31.13.13.1.m1.2.2.1.1.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1"><divide id="S2.T1.31.13.13.1.m1.2.2.1.1.1.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.1"></divide><apply id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2"><times id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.1"></times><apply id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.2"><abs id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.1.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.2.2.1"></abs><ci id="S2.T1.31.13.13.1.m1.1.1.cmml" xref="S2.T1.31.13.13.1.m1.1.1">ℐ</ci></apply><apply id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.1.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3">subscript</csymbol><ci id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.2.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.2">𝑇</ci><ci id="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.3.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.2.3.3">𝑐</ci></apply></apply><ci id="S2.T1.31.13.13.1.m1.2.2.1.1.1.3.cmml" xref="S2.T1.31.13.13.1.m1.2.2.1.1.1.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.31.13.13.1.m1.2c">\mathcal{O}(\mathcal{|I|}T_{c}/E)</annotation><annotation encoding="application/x-llamapun" id="S2.T1.31.13.13.1.m1.2d">caligraphic_O ( | caligraphic_I | italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT / italic_E )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.32.14.14.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\mathcal{O}(CT_{c}+T_{s})" class="ltx_Math" display="inline" id="S2.T1.32.14.14.2.m1.1"><semantics id="S2.T1.32.14.14.2.m1.1a"><mrow id="S2.T1.32.14.14.2.m1.1.1" xref="S2.T1.32.14.14.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.32.14.14.2.m1.1.1.3" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.3.cmml">𝒪</mi><mo id="S2.T1.32.14.14.2.m1.1.1.2" xref="S2.T1.32.14.14.2.m1.1.1.2.cmml">⁢</mo><mrow id="S2.T1.32.14.14.2.m1.1.1.1.1" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.cmml"><mo id="S2.T1.32.14.14.2.m1.1.1.1.1.2" maxsize="80%" minsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.T1.32.14.14.2.m1.1.1.1.1.1" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.cmml"><mrow id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.cmml"><mi id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.2" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.2.cmml">C</mi><mo id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.1" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.cmml"><mi id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.2" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.2.cmml">T</mi><mi id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.3" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.3.cmml">c</mi></msub></mrow><mo id="S2.T1.32.14.14.2.m1.1.1.1.1.1.1" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.cmml"><mi id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.2" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.2.cmml">T</mi><mi id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.3" mathsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo id="S2.T1.32.14.14.2.m1.1.1.1.1.3" maxsize="80%" minsize="80%" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.32.14.14.2.m1.1b"><apply id="S2.T1.32.14.14.2.m1.1.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1"><times id="S2.T1.32.14.14.2.m1.1.1.2.cmml" xref="S2.T1.32.14.14.2.m1.1.1.2"></times><ci id="S2.T1.32.14.14.2.m1.1.1.3.cmml" xref="S2.T1.32.14.14.2.m1.1.1.3">𝒪</ci><apply id="S2.T1.32.14.14.2.m1.1.1.1.1.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1"><plus id="S2.T1.32.14.14.2.m1.1.1.1.1.1.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.1"></plus><apply id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2"><times id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.1"></times><ci id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.2.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.2">𝐶</ci><apply id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.2.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.2">𝑇</ci><ci id="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.3.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.2.3.3">𝑐</ci></apply></apply><apply id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.1.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.2.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.2">𝑇</ci><ci id="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.3.cmml" xref="S2.T1.32.14.14.2.m1.1.1.1.1.1.3.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.32.14.14.2.m1.1c">\mathcal{O}(CT_{c}+T_{s})</annotation><annotation encoding="application/x-llamapun" id="S2.T1.32.14.14.2.m1.1d">caligraphic_O ( italic_C italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )</annotation></semantics></math></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Trade-off among Accuracy, Efficiency and Stability</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">The ”information overload” phenomenon has compelled recommenders to grapple with the issue of striking an appropriate trade-off among accuracy, efficiency, and stability <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib20" title="">2023a</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib50" title="">2022b</a>)</cite>. Negative sampling is no exception to this challenge <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib160" title="">2020</a>)</cite>. Accuracy pertains to the precision of model’s predictions regarding user preferences, which is critical in evaluating the performance of recommenders. Efficiency focuses on how many queries the recommender can process within a unit of time, directly influencing its scalability and user experience. And stability concerns the consistent performance of the recommender across diverse datasets.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">Nonetheless, these three aspects are generally interrelated in RS, making it challenging to balance them simultaneously. We furnish the sampling criteria, as well as the time complexities of candidate generation and negative selection for seven classic negative sampling algorithms, arranged in chronological order in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.T1" title="Table 1 ‣ 2.2.1. False Negative Problem ‣ 2.2. Challenges of Negative Sampling in Recommendation ‣ 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>. This illustrates that with the advancement in the accuracy of negative sampling techniques, there is a synchronous increase in their time complexity, underscoring the delicate trade-off between accuracy, efficiency and stability <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>; Rendle and Freudenthaler, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>. This is due to the fact that enhancing accuracy may lead to increased computational complexity, resulting in decreased efficiency. Some efficiency-improving optimization techniques may exhibit greater sensitivity to data variations and neglect specific details, potentially impacting the model’s accuracy and stability. Simultaneously, the pursuit of stability may increase time and space complexity, conflicting with the balance between accuracy and efficiency. Therefore, achieving the optimal balance among these three factors within negative sampling is a complex and challenging task.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>Universality with Different Tasks, Objectives, and Datasets</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">In essence, the prevailing recommendation algorithms encompass different datasets with diverse distribution, distinct training objectives (e.g., matching and ranking), various recommendation scenarios (e.g., collaborative filtering, multi-modal recommendation, multi-behavior recommendation, sequence recommendation, and cross-domain recommendation), and diverse evaluation methods (random sampled testing, popularity-based sampled testing, and full testing). Although general negative sampling strategies (RNS <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>)</cite>, PNS <cite class="ltx_cite ltx_citemacro_citep">(Rendle and Freudenthaler, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a>)</cite>, DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite>) are available, some tailored negative sampling techniques designed for specific recommendation tasks can outperform these general methods <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib57" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib90" title="">2021a</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib192" title="">2022a</a>)</cite>. Upon a comprehensive examination of the current negative sampling algorithms, it is evident that none of these approaches can perfectly match all recommendation tasks. In practical implementation, employing different negative sampling strategies for various tasks presents a challenge to the efficiency and stability of the recommender. This challenge underscores the obstacles that large-scale recommendation systems must overcome when transitioning to the real-world online deployment.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="486" id="S2.F3.g1" src="x3.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Overall Ontology of Negative Sampling Strategies in Recommendation.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Literature review of Negative Sampling in Recommendation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.11">Let <math alttext="m" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_m</annotation></semantics></math> and <math alttext="n" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_n</annotation></semantics></math> be the number of users <math alttext="\mathcal{U}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">𝒰</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝒰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathcal{U}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">caligraphic_U</annotation></semantics></math> and items <math alttext="\mathcal{I}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathcal{I}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">caligraphic_I</annotation></semantics></math> respectively, we can define a user-item interaction matrix <math alttext="\bm{Y}\!\in\!\mathbb{R}^{m\times n}" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mrow id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">𝒀</mi><mo id="S3.p1.5.m5.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml"><mi id="S3.p1.5.m5.1.1.3.2" xref="S3.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p1.5.m5.1.1.3.3" xref="S3.p1.5.m5.1.1.3.3.cmml"><mi id="S3.p1.5.m5.1.1.3.3.2" xref="S3.p1.5.m5.1.1.3.3.2.cmml">m</mi><mo id="S3.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.p1.5.m5.1.1.3.3.3" xref="S3.p1.5.m5.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><in id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1"></in><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">𝒀</ci><apply id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.3.1.cmml" xref="S3.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.p1.5.m5.1.1.3.2.cmml" xref="S3.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.p1.5.m5.1.1.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3"><times id="S3.p1.5.m5.1.1.3.3.1.cmml" xref="S3.p1.5.m5.1.1.3.3.1"></times><ci id="S3.p1.5.m5.1.1.3.3.2.cmml" xref="S3.p1.5.m5.1.1.3.3.2">𝑚</ci><ci id="S3.p1.5.m5.1.1.3.3.3.cmml" xref="S3.p1.5.m5.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\bm{Y}\!\in\!\mathbb{R}^{m\times n}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">bold_italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> , where <math alttext="y_{u,i}=1" class="ltx_Math" display="inline" id="S3.p1.6.m6.2"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2.3" xref="S3.p1.6.m6.2.3.cmml"><msub id="S3.p1.6.m6.2.3.2" xref="S3.p1.6.m6.2.3.2.cmml"><mi id="S3.p1.6.m6.2.3.2.2" xref="S3.p1.6.m6.2.3.2.2.cmml">y</mi><mrow id="S3.p1.6.m6.2.2.2.4" xref="S3.p1.6.m6.2.2.2.3.cmml"><mi id="S3.p1.6.m6.1.1.1.1" xref="S3.p1.6.m6.1.1.1.1.cmml">u</mi><mo id="S3.p1.6.m6.2.2.2.4.1" xref="S3.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.p1.6.m6.2.2.2.2" xref="S3.p1.6.m6.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S3.p1.6.m6.2.3.1" xref="S3.p1.6.m6.2.3.1.cmml">=</mo><mn id="S3.p1.6.m6.2.3.3" xref="S3.p1.6.m6.2.3.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.2b"><apply id="S3.p1.6.m6.2.3.cmml" xref="S3.p1.6.m6.2.3"><eq id="S3.p1.6.m6.2.3.1.cmml" xref="S3.p1.6.m6.2.3.1"></eq><apply id="S3.p1.6.m6.2.3.2.cmml" xref="S3.p1.6.m6.2.3.2"><csymbol cd="ambiguous" id="S3.p1.6.m6.2.3.2.1.cmml" xref="S3.p1.6.m6.2.3.2">subscript</csymbol><ci id="S3.p1.6.m6.2.3.2.2.cmml" xref="S3.p1.6.m6.2.3.2.2">𝑦</ci><list id="S3.p1.6.m6.2.2.2.3.cmml" xref="S3.p1.6.m6.2.2.2.4"><ci id="S3.p1.6.m6.1.1.1.1.cmml" xref="S3.p1.6.m6.1.1.1.1">𝑢</ci><ci id="S3.p1.6.m6.2.2.2.2.cmml" xref="S3.p1.6.m6.2.2.2.2">𝑖</ci></list></apply><cn id="S3.p1.6.m6.2.3.3.cmml" type="integer" xref="S3.p1.6.m6.2.3.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">y_{u,i}=1</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.2d">italic_y start_POSTSUBSCRIPT italic_u , italic_i end_POSTSUBSCRIPT = 1</annotation></semantics></math> denotes the user <math alttext="u" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_u</annotation></semantics></math> has interacted with the item <math alttext="i" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><mi id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_i</annotation></semantics></math>, and <math alttext="y_{u,i}=0" class="ltx_Math" display="inline" id="S3.p1.9.m9.2"><semantics id="S3.p1.9.m9.2a"><mrow id="S3.p1.9.m9.2.3" xref="S3.p1.9.m9.2.3.cmml"><msub id="S3.p1.9.m9.2.3.2" xref="S3.p1.9.m9.2.3.2.cmml"><mi id="S3.p1.9.m9.2.3.2.2" xref="S3.p1.9.m9.2.3.2.2.cmml">y</mi><mrow id="S3.p1.9.m9.2.2.2.4" xref="S3.p1.9.m9.2.2.2.3.cmml"><mi id="S3.p1.9.m9.1.1.1.1" xref="S3.p1.9.m9.1.1.1.1.cmml">u</mi><mo id="S3.p1.9.m9.2.2.2.4.1" xref="S3.p1.9.m9.2.2.2.3.cmml">,</mo><mi id="S3.p1.9.m9.2.2.2.2" xref="S3.p1.9.m9.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S3.p1.9.m9.2.3.1" xref="S3.p1.9.m9.2.3.1.cmml">=</mo><mn id="S3.p1.9.m9.2.3.3" xref="S3.p1.9.m9.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.2b"><apply id="S3.p1.9.m9.2.3.cmml" xref="S3.p1.9.m9.2.3"><eq id="S3.p1.9.m9.2.3.1.cmml" xref="S3.p1.9.m9.2.3.1"></eq><apply id="S3.p1.9.m9.2.3.2.cmml" xref="S3.p1.9.m9.2.3.2"><csymbol cd="ambiguous" id="S3.p1.9.m9.2.3.2.1.cmml" xref="S3.p1.9.m9.2.3.2">subscript</csymbol><ci id="S3.p1.9.m9.2.3.2.2.cmml" xref="S3.p1.9.m9.2.3.2.2">𝑦</ci><list id="S3.p1.9.m9.2.2.2.3.cmml" xref="S3.p1.9.m9.2.2.2.4"><ci id="S3.p1.9.m9.1.1.1.1.cmml" xref="S3.p1.9.m9.1.1.1.1">𝑢</ci><ci id="S3.p1.9.m9.2.2.2.2.cmml" xref="S3.p1.9.m9.2.2.2.2">𝑖</ci></list></apply><cn id="S3.p1.9.m9.2.3.3.cmml" type="integer" xref="S3.p1.9.m9.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.2c">y_{u,i}=0</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.2d">italic_y start_POSTSUBSCRIPT italic_u , italic_i end_POSTSUBSCRIPT = 0</annotation></semantics></math> denotes <math alttext="u" class="ltx_Math" display="inline" id="S3.p1.10.m10.1"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m10.1d">italic_u</annotation></semantics></math> has never interacted with <math alttext="i" class="ltx_Math" display="inline" id="S3.p1.11.m11.1"><semantics id="S3.p1.11.m11.1a"><mi id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><ci id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m11.1d">italic_i</annotation></semantics></math>. Negative sampling aims to select the appropriate instances from the unobserved item candidates as the negative samples to support the recommender optimization. We conduct a comprehensive literature review of existing negative sampling strategies in recommendation and classify them into five categories according to their technical trajectories: 1) Static negative sampling typically samples negative instances with the static probability; 2) Dynamic negative sampling attempts to dynamically select negative instances with the pre-established sampling criterion; 3) Adversarial negative generation leverages the adversarial learning paradigm to sample or generate plausible items as the negative samples; 4) Importance re-weighting aims to identify the importance of each sample and assign diverse weights for them in a data-driven manner; and 5) Knowledge-enhanced negative sampling focuses on sampling negative instances with the mining of the implicit association from the auxiliary knowledge. In the following, we detail the effectiveness mechanisms and meaningful insights of these negative sampling strategies.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Static Negative Sampling</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Traditional approaches in the initial stages of deep learning for recommendation relied on the static negative sampling (SNS) method whereby several items are selected from users’ unobserved items. The primary objective of SNS is to offer a diverse range of informative negative samples and facilitate the acquisition of more comprehensive user preference patterns by the recommender.
We delineate existing SNS studies into four categories, Uniform SNS, Predefined SNS, Popularity-based SNS, and Non-sampling SNS. The in-depth investigation of behavior dependencies, advantages, and challenges of them are documented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.T2" title="Table 2 ‣ 3.1.1. Uniform SNS ‣ 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>. Among them, Uniform SNS emerges as the most widely employed SNS method with its easy-to-deploy and time-efficient character. However, the randomness inherent in sampling introduces variability to its recommendation performance. Predefined SNS leverages authentic user-centric ratings for negative sample selection. Despite its straightforward implementation, it is critically reliant on the accessibility of user behaviors. In the incorporation of the mainstream preferences of the majority of users into the training process, Popularity-based SNS also introduces the inherent popularity bias and conformity bias. Non-sampling SNS incorporates the unobserved items of each user from the whole training data. Maintaining visibility over the entire corpus of the dataset enhances the recommender’s performance, yet encountering challenges in computational efficiency.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Uniform SNS</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Uniform SNS randomly selects items from the unobserved interactions of the user as the negative sample <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib213" title="">2018</a>)</cite>.
This strategy is extensively employed across diverse recommendation scenarios (e.g., Collaborative Filtering <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib71" title="">2017</a>)</cite>, Graph-based Recommendaiton <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib158" title="">2019a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib70" title="">2023a</a>)</cite>, Sequential Recommendaiton <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib193" title="">2022b</a>; Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib220" title="">2024</a>)</cite>, Multi-modal Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib110" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib149" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib161" title="">2024</a>)</cite>, and Cross-domain Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib48" title="">2021a</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib10" title="">2022</a>)</cite>) and distinct recommendation applications (e.g., Point-of-Interest Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib184" title="">2017</a>; Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib106" title="">2014</a>)</cite>, News Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib165" title="">2019d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib166" title="">c</a>)</cite>, Song Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Tran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib147" title="">2019</a>)</cite> and API Recommendation <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib163" title="">2022</a>)</cite>). Primarily, its ease-to-deploy mechanism makes it well-suited for large-scale RS, as it obviates the necessity for additional computations. Additionally, it introduces a degree of diversity, mitigating the risk of over-fitting and consequently enhancing recommender’s generalization and robustness. Moreover, this strategy facilitates the exploration of items that users have not interacted with, thereby aiding the recommender in uncovering novel recommendation opportunities and unexplored areas of user preference.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Illustration of four types of commonly-used Static Negative Sampling algorithms.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T2.3" style="width:433.6pt;height:734.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.5pt,116.1pt) scale(0.759875869852929,0.759875869852929) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1" style="width:142.3pt;padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T2.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T2.1.1.1.1.1.p1.1.1"></span><span class="ltx_text" id="S3.T2.1.1.1.1.1.p1.1.2" style="font-size:80%;"> </span><span class="ltx_text" id="S3.T2.1.1.1.1.1.p1.1.3" style="font-size:80%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.1.1.p1.1.3.1">
<span class="ltx_tr" id="S3.T2.1.1.1.1.1.p1.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.1.1.p1.1.3.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.p1.1.3.1.1.1.1">Algorithms</span></span></span>
</span></span><span class="ltx_text" id="S3.T2.1.1.1.1.1.p1.1.4"></span><span class="ltx_text" id="S3.T2.1.1.1.1.1.p1.1.5" style="font-size:80%;"></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.2.1">
<tr class="ltx_tr" id="S3.T2.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.2.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1.1.1.1" style="font-size:80%;">Behavior</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.1.1.2.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1.2.1.1" style="font-size:80%;">dependency</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.3" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1" style="font-size:80%;">Advantages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1" style="font-size:80%;">Challenges</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.2.2.2.2" style="width:142.3pt;padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.2.2.2.1">
<span class="ltx_p" id="S3.T2.2.2.2.2.1.1"><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.1" style="font-size:80%;">Uniform SNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.2.1" style="font-size:80%;">(</span>Rendle et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>; Petrov and Macdonald<span class="ltx_text" id="S3.T2.2.2.2.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib127" title="">2022</a>; Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib149" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib161" title="">2024</a>; Zou et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib220" title="">2024</a><span class="ltx_text" id="S3.T2.2.2.2.2.1.1.4.3" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.2.2.2.1.m1.1"><semantics id="S3.T2.2.2.2.1.m1.1a"><mo id="S3.T2.2.2.2.1.m1.1.1" mathsize="80%" xref="S3.T2.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><times id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.2.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.2.2.2.3.1">
<tr class="ltx_tr" id="S3.T2.2.2.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.2.2.3.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.2.2.2.3.1.1.1.1" style="font-size:80%;">Easy-to-deploy;</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.2.2.2.3.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.2.2.2.3.1.2.1.1" style="font-size:80%;">Time-efficient</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.2.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.2.2.2.4.1" style="font-size:80%;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.3.3.4.1" style="width:142.3pt;padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.4.1.1">
<span class="ltx_p" id="S3.T2.3.3.4.1.1.1"><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.1" style="font-size:80%;">Predefined SNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.2.1" style="font-size:80%;">(</span>Song et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib139" title="">2015</a>; Yu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib198" title="">2018</a>; Zheng et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib210" title="">2018</a>; Lin et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib107" title="">2024</a>; Xi et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib176" title="">2023</a>; Dai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib37" title="">2023</a>; Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib100" title="">2023</a><span class="ltx_text" id="S3.T2.3.3.4.1.1.1.4.3" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.4.2" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.4.2.1" style="font-size:80%;">User-centric rating</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.4.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.4.3.1">
<tr class="ltx_tr" id="S3.T2.3.3.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.4.3.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.4.3.1.1.1.1" style="font-size:80%;">Authentic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.4.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.4.4.1" style="font-size:80%;">Data availability</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.3.3.5.1" style="width:142.3pt;padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.5.1.1">
<span class="ltx_p" id="S3.T2.3.3.5.1.1.1"><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.1" style="font-size:80%;">Popularity-based SNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.2.1" style="font-size:80%;">(</span>Gantner et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib46" title="">2012</a>; Cheng et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>; Togashi et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>; Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib151" title="">2019b</a>; Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib103" title="">2018</a>; He et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib72" title="">2016</a><span class="ltx_text" id="S3.T2.3.3.5.1.1.1.4.3" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.5.2" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.5.2.1">
<tr class="ltx_tr" id="S3.T2.3.3.5.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.2.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.2.1.1.1.1" style="font-size:80%;">Interaction</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.5.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.2.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.2.1.2.1.1" style="font-size:80%;">frequency</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.5.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.5.3.1">
<tr class="ltx_tr" id="S3.T2.3.3.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.3.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.3.1.1.1.1" style="font-size:80%;">Publicly</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.3.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.3.1.2.1.1" style="font-size:80%;">preferred</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.5.4" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.5.4.1">
<tr class="ltx_tr" id="S3.T2.3.3.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.4.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.4.1.1.1.1" style="font-size:80%;">Popularity bias;</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.5.4.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.5.4.1.2.1.1" style="font-size:80%;">Conformity bias</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.3.3.3.2" style="width:142.3pt;padding-top:1.2pt;padding-bottom:1.2pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.3.2.1">
<span class="ltx_p" id="S3.T2.3.3.3.2.1.1"><span class="ltx_text" id="S3.T2.3.3.3.2.1.1.1" style="font-size:80%;">Non-sampling SNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.3.3.3.2.1.1.2.1" style="font-size:80%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.3.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib15" title="">2020d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib14" title="">c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib13" title="">2019d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib12" title="">b</a>; Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.3.3.3.2.1.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a><span class="ltx_text" id="S3.T2.3.3.3.2.1.1.4.3" style="font-size:80%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.3.3.3.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><math alttext="\times" class="ltx_Math" display="inline" id="S3.T2.3.3.3.1.m1.1"><semantics id="S3.T2.3.3.3.1.m1.1a"><mo id="S3.T2.3.3.3.1.m1.1.1" mathsize="80%" xref="S3.T2.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.m1.1b"><times id="S3.T2.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.3.3.3.3" style="padding-top:1.2pt;padding-bottom:1.2pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.3.3.3.3.1">
<tr class="ltx_tr" id="S3.T2.3.3.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.3.3.1.1.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.3.3.1.1.1.1" style="font-size:80%;">High accuracy;</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.3.3.3.3.1.2.1" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.3.3.1.2.1.1" style="font-size:80%;">Corpus-visible</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.3.3.3.4" style="padding-top:1.2pt;padding-bottom:1.2pt;"><span class="ltx_text" id="S3.T2.3.3.3.4.1" style="font-size:80%;">Efficiency</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">It is noteworthy that TDM <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib217" title="">2018</a>)</cite> has made advancements in enhancing the uniform SNS method within the hierarchical tree structure. It denotes the positive sample set as the leaf node that has interacted with the user and their ancestor nodes, while randomly selecting nodes except positive ones in each level to constitute the set of negative samples. RSS <cite class="ltx_cite ltx_citemacro_citep">(Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib127" title="">2022</a>)</cite> also tries to propose a recency-based positive sampling strategy to generate multiple positive training samples out of a single user sequence simultaneously, which is orthogonal to negative sampling and can be applied independently. However, given that the primary focus of this subsection lies in the examination of the negative sampling strategies in recommendation, there has been limited elaboration on the positive-augmented sampling methods.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Predefined SNS</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.3">Predefined SNS incorporates the pre-defined negative samples from the dataset into the recommenders’s training process <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib139" title="">2015</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib198" title="">2018</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib143" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib204" title="">2024</a>)</cite>. For example, DRN <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib210" title="">2018</a>)</cite> employs real user behaviors (e.g., skipped, clicked, and ordered actions) within the dataset to delineate positive and negative samples. Additionally, a series of studies designate the ratings of <math alttext="(4,5)" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.1.m1.2"><semantics id="S3.SS1.SSS2.p1.1.m1.2a"><mrow id="S3.SS1.SSS2.p1.1.m1.2.3.2" xref="S3.SS1.SSS2.p1.1.m1.2.3.1.cmml"><mo id="S3.SS1.SSS2.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS1.SSS2.p1.1.m1.2.3.1.cmml">(</mo><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">4</mn><mo id="S3.SS1.SSS2.p1.1.m1.2.3.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.1.m1.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.cmml">5</mn><mo id="S3.SS1.SSS2.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS1.SSS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.2b"><interval closure="open" id="S3.SS1.SSS2.p1.1.m1.2.3.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.3.2"><cn id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.SSS2.p1.1.m1.1.1">4</cn><cn id="S3.SS1.SSS2.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS1.SSS2.p1.1.m1.2.2">5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.2c">(4,5)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.1.m1.2d">( 4 , 5 )</annotation></semantics></math> in the explicit user interactions as positive samples. Within this context, ELLR <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib139" title="">2015</a>)</cite> and IF-BPR <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib198" title="">2018</a>)</cite> segregate ratings of <math alttext="(1,2)" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.2.m2.2"><semantics id="S3.SS1.SSS2.p1.2.m2.2a"><mrow id="S3.SS1.SSS2.p1.2.m2.2.3.2" xref="S3.SS1.SSS2.p1.2.m2.2.3.1.cmml"><mo id="S3.SS1.SSS2.p1.2.m2.2.3.2.1" stretchy="false" xref="S3.SS1.SSS2.p1.2.m2.2.3.1.cmml">(</mo><mn id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.SSS2.p1.2.m2.2.3.2.2" xref="S3.SS1.SSS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.2.m2.2.2" xref="S3.SS1.SSS2.p1.2.m2.2.2.cmml">2</mn><mo id="S3.SS1.SSS2.p1.2.m2.2.3.2.3" stretchy="false" xref="S3.SS1.SSS2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.2b"><interval closure="open" id="S3.SS1.SSS2.p1.2.m2.2.3.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.2.3.2"><cn id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.SSS2.p1.2.m2.1.1">1</cn><cn id="S3.SS1.SSS2.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS1.SSS2.p1.2.m2.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.2c">(1,2)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.2.m2.2d">( 1 , 2 )</annotation></semantics></math> as negative samples, while SCoRe <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib130" title="">2020</a>)</cite>, ML-MGC <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib143" title="">2021</a>)</cite> and many LLM-based recommenders <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib107" title="">2024</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib176" title="">2023</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib37" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib100" title="">2023</a>)</cite> extend this categorization to include ratings of <math alttext="(1,2,3)" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p1.3.m3.3"><semantics id="S3.SS1.SSS2.p1.3.m3.3a"><mrow id="S3.SS1.SSS2.p1.3.m3.3.4.2" xref="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml"><mo id="S3.SS1.SSS2.p1.3.m3.3.4.2.1" stretchy="false" xref="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml">(</mo><mn id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.SSS2.p1.3.m3.3.4.2.2" xref="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.3.m3.2.2" xref="S3.SS1.SSS2.p1.3.m3.2.2.cmml">2</mn><mo id="S3.SS1.SSS2.p1.3.m3.3.4.2.3" xref="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml">,</mo><mn id="S3.SS1.SSS2.p1.3.m3.3.3" xref="S3.SS1.SSS2.p1.3.m3.3.3.cmml">3</mn><mo id="S3.SS1.SSS2.p1.3.m3.3.4.2.4" stretchy="false" xref="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.3b"><vector id="S3.SS1.SSS2.p1.3.m3.3.4.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.3.4.2"><cn id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.SSS2.p1.3.m3.1.1">1</cn><cn id="S3.SS1.SSS2.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS1.SSS2.p1.3.m3.2.2">2</cn><cn id="S3.SS1.SSS2.p1.3.m3.3.3.cmml" type="integer" xref="S3.SS1.SSS2.p1.3.m3.3.3">3</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.3c">(1,2,3)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p1.3.m3.3d">( 1 , 2 , 3 )</annotation></semantics></math> as negative samples.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Popularity-based SNS</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">Popularity-based SNS selects negative samples based on the popularity of items, that is, the more popular the item is, the more likely it is to be selected as the negative sample <cite class="ltx_cite ltx_citemacro_citep">(Gantner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib46" title="">2012</a>; Quadrana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib131" title="">2017</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib115" title="">2018</a>; Rendle and Freudenthaler, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a>; Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib151" title="">2019b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib103" title="">2018</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib72" title="">2016</a>)</cite>. Relying on the assumption that the popularity of items may demonstrate users’ global preferences, a series of popularity-based SNS methods (e.g., WBPR <cite class="ltx_cite ltx_citemacro_citep">(Gantner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib46" title="">2012</a>)</cite>, HRNN <cite class="ltx_cite ltx_citemacro_citep">(Quadrana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib131" title="">2017</a>)</cite>, NNCF <cite class="ltx_cite ltx_citemacro_citep">(Rendle and Freudenthaler, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a>)</cite>, BINN <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib103" title="">2018</a>)</cite>) typically assign sampling weights to items based on their frequency. This encourages the recommender to incorporate more popular items as negative samples into the training process. For quick updating the recommender parameters with the new incoming data, eALS <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib72" title="">2016</a>)</cite> assigns the weight of missing data based on the popularity of items by an incremental update strategy. SoftRec <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>)</cite> allows some unobserved items to have non-zero supervisory signals by generating soft targets with popularity distribution, thereby incorporating more unobserved knowledge into the training process. Similar to RSS <cite class="ltx_cite ltx_citemacro_citep">(Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib127" title="">2022</a>)</cite>, SAE-NAD <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib115" title="">2018</a>)</cite> adopts the general weighting scheme to assign higher confidence to each positive sample based on its frequency. In contrast, it assigns the weights of all negative examples to the same value (e.g., 1.0) for the Point-of-Interest recommendation task.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">Notably, KGPL <cite class="ltx_cite ltx_citemacro_citep">(Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>)</cite> and MKR <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib151" title="">2019b</a>)</cite> employ the Popularity-based SNS for the graph structure. In this context, KGPL <cite class="ltx_cite ltx_citemacro_citep">(Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>)</cite> directly formulates the sampling confidence of negative samples based on the interaction frequencies of each item across the entire dataset. Furthermore, it assigns reliable pseudo-labels to the interacted items of cold-start users by considering items that can be reached through meta-paths (i.e., the paths rooted at a user’s interacted items in KG). Different from the above studies, MKR <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib151" title="">2019b</a>)</cite> follows the negative sampling strategy of the classical work <cite class="ltx_cite ltx_citemacro_citep">(Mikolov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib121" title="">2013</a>)</cite> in NLP, which reduces the sampling probability for the high-frequency items and augments it for the low-frequency items. This is motivated by the observation that less probable words tend to be more pivotal, while higher probability words such as ‘the’, ‘a’, and ‘to’ bear less weight in NLP tasks. It actually deviates from the core assumptions in recommendation, thus MKR adopts such a sampling strategy primarily to enhance the computational efficiency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4. </span>Non-sampling SNS</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">Non-sampling SNS considers the unobserved instances from the whole training data for recommender learning, thus avoiding negative sampling <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib15" title="">2020d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib14" title="">c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib13" title="">2019d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib12" title="">b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>)</cite>. These related works argue that negative sampling strategies are highly sensitive to the data distribution and the number of negative samples, making them difficult to achieve the optimal performance in large-scale RS. To converge to a better optimum, EATNN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib13" title="">2019d</a>)</cite> first devises an efficient optimization method to learn from the whole training set without negative sampling, and reformulates a square loss function with rigorous mathematical reasoning for accelerating the training process in social-aware recommendation. ENMF <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib14" title="">2020c</a>)</cite> stands as an extended edition of EATNN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib13" title="">2019d</a>)</cite> that extends its efficient learning method with the item-based and alternating-based forms, enabling its application for implicit feedback. Due to the behavior characteristic in multi-bahevior recommendation, EHCF <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib15" title="">2020d</a>)</cite> links the prediction of each type of behavior in a transfer manner and applies non-sampling optimization for the recommender by considering all samples in each parameter update. Inspired by the memorization strategies, ENSFM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib12" title="">2020b</a>)</cite> employs the non-sampling technique to maintain the convergence into a better optimum in a stable way. JNSKR <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>)</cite> refine the existing non-sampling strategy into the modeling of the knowledge graph embeddings, which consists of entity-relation-entity triplets. Furthermore, it devises an efficient optimization approach to learn more precise entity embeddings and item representations within the knowledge graph, resulting in a discernible and stable enhancement of recommendation performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Dynamic Negative Sampling</h3>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Illustration of seven types of commonly-used Dynamic Negative Sampling algorithms.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.1" style="width:433.6pt;height:1074.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.0pt,210.4pt) scale(0.718477736527588,0.718477736527588) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T3.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T3.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T3.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.p1.1.1"></span><span class="ltx_text" id="S3.T3.1.1.1.1.1.p1.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.1.1.1.1.p1.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.1.1.p1.1.3.1">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.p1.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.p1.1.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.p1.1.3.1.1.1.1">Algorithms</span></span></span>
</span></span><span class="ltx_text" id="S3.T3.1.1.1.1.1.p1.1.4"></span><span class="ltx_text" id="S3.T3.1.1.1.1.1.p1.1.5" style="font-size:90%;"></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1" style="font-size:90%;">Sampling criteria</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.3.1" style="font-size:90%;">Advantages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.4" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.4.1" style="font-size:90%;">Challenges</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.2.1.1">
<span class="ltx_p" id="S3.T3.1.1.2.1.1.1"><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.1" style="font-size:90%;">Universal DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.2.1" style="font-size:90%;">(</span>Wu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Yang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib195" title="">2020a</a>; Shi et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Brin and Page<span class="ltx_text" id="S3.T3.1.1.2.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib8" title="">1998</a>; Ma et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a><span class="ltx_text" id="S3.T3.1.1.2.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.2.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.2.2.1.1.1.1" style="font-size:90%;">Relying solely on user</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.2.2.1.2.1.1" style="font-size:90%;">-item matching scores</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.3" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.2.3.1">
<tr class="ltx_tr" id="S3.T3.1.1.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.2.3.1.1.1.1" style="font-size:90%;">Easy-to-deploy;</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.3.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.2.3.1.2.1.1" style="font-size:90%;">Universal</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.4" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.2.4.1" style="font-size:90%;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.3.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.3.1.1">
<span class="ltx_p" id="S3.T3.1.1.3.1.1.1"><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.1" style="font-size:90%;">User-similarity DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.2.1" style="font-size:90%;">(</span>Wu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Giobergia<span class="ltx_text" id="S3.T3.1.1.3.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib56" title="">2022</a>; Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a><span class="ltx_text" id="S3.T3.1.1.3.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.3.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.2.1.1.1.1" style="font-size:90%;">Identifying user</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.2.1.2.1.1" style="font-size:90%;">similarity association</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.3.3.1">
<tr class="ltx_tr" id="S3.T3.1.1.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.3.1.1.1.1" style="font-size:90%;">Capturing user</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.3.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.3.1.2.1.1" style="font-size:90%;">behavior similarities</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.4" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.3.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.4.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.4.1.1.1.1" style="font-size:90%;">Relies on user</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.4.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.4.1.2.1.1" style="font-size:90%;">associations; Poor</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.4.1.3.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.3.4.1.3.1.1" style="font-size:90%;">performance for new users</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.4.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.4.1.1">
<span class="ltx_p" id="S3.T3.1.1.4.1.1.1"><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.1" style="font-size:90%;">Knowledge-aware DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.2.1" style="font-size:90%;">(</span>Lian et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>; Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>; Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib159" title="">2021d</a><span class="ltx_text" id="S3.T3.1.1.4.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.4.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.2.1.1.1.1" style="font-size:90%;">Emphasizing samples with</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.2.1.2.1.1" style="font-size:90%;">same attributes to positives</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.3" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.4.3.1">
<tr class="ltx_tr" id="S3.T3.1.1.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.3.1.1.1.1" style="font-size:90%;">Capturing item</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.3.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.3.1.2.1.1" style="font-size:90%;">content correlation</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.4.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.4.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.4.1.1.1.1" style="font-size:90%;">Additional knowledge</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.4.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.4.4.1.2.1.1" style="font-size:90%;">dependency</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.5.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.5.1.1">
<span class="ltx_p" id="S3.T3.1.1.5.1.1.1"><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.1" style="font-size:90%;">Distribution-based DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.2.1" style="font-size:90%;">(</span>Zhu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Ding et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Lai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib95" title="">2023</a><span class="ltx_text" id="S3.T3.1.1.5.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.5.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.5.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.2.1.1.1.1" style="font-size:90%;">Analyzing the inherent</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.2.1.2.1.1" style="font-size:90%;">distribution in dataset</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.3" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.5.3.1">
<tr class="ltx_tr" id="S3.T3.1.1.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.3.1.1.1.1" style="font-size:90%;">Focuses on</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.3.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.3.1.2.1.1" style="font-size:90%;">real negatives</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.4" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.5.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.5.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.4.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.4.1.1.1.1" style="font-size:90%;">Additional memory</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.4.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.5.4.1.2.1.1" style="font-size:90%;">dependency</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.6.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.6.1.1">
<span class="ltx_p" id="S3.T3.1.1.6.1.1.1"><span class="ltx_text" id="S3.T3.1.1.6.1.1.1.1" style="font-size:90%;">Interpolation DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.6.1.1.1.2.1" style="font-size:90%;">(</span>Huang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.6.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a>; Shi et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.6.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib136" title="">2022</a><span class="ltx_text" id="S3.T3.1.1.6.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.6.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.6.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.2.1.1.1.1" style="font-size:90%;">Injecting information</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.2.1.2.1.1" style="font-size:90%;">from positive samples</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.3" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.6.3.1">
<tr class="ltx_tr" id="S3.T3.1.1.6.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.3.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.3.1.1.1.1" style="font-size:90%;">Balance positive and</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.3.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.3.1.2.1.1" style="font-size:90%;">negative samples</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.4" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.6.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.6.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.4.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.4.1.1.1.1" style="font-size:90%;">Over-smoothing neglects</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.4.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.6.4.1.2.1.1" style="font-size:90%;">crucial samples</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.7.1" style="width:128.0pt;padding-top:1.35pt;padding-bottom:1.35pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.7.1.1">
<span class="ltx_p" id="S3.T3.1.1.7.1.1.1"><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.1" style="font-size:90%;">Mixed DNS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.2.1" style="font-size:90%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib28" title="">2017</a>; Lei et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib98" title="">2020</a>; Hidasi and Karatzoglou<span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib75" title="">2018</a>; Hidasi et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib76" title="">2015</a>; Huang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib83" title="">2020</a>; Yang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib188" title="">2020c</a>; Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib182" title="">2021</a><span class="ltx_text" id="S3.T3.1.1.7.1.1.1.4.3" style="font-size:90%;">)</span></cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.7.2" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.7.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.7.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.2.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.2.1.1.1.1" style="font-size:90%;">Combining multiple above</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.2.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.2.1.2.1.1" style="font-size:90%;">DNS strategies</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.7.3" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.3.1" style="font-size:90%;">Flexible</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.7.4" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.7.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.7.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.4.1.1.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.4.1.1.1.1" style="font-size:90%;">Complex hyperparameter</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.4.1.2.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.4.1.2.1.1" style="font-size:90%;">tuning; Substantial</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.4.1.3.1" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span class="ltx_text" id="S3.T3.1.1.7.4.1.3.1.1" style="font-size:90%;">computational cost</span></td>
</tr>
</table>
</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">With the aim to capture the relative informative negative samples in a universal manner, a wave of Dynamic Negative Sampling (DNS) strategies has sprung up in recent years. DNS refers to selecting the item that is more relevant to the positive sample representations (or user representations) as the negative samples from the dynamically selected item candidates. For example, the pioneering dynamic negative sampling strategy <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite> generally selects the hardest item from the randomly selected item candidates. The subsequent work primarily focuses on modeling the relationship between users and items, or precisely tuning the sampling difficulty of the candidate set. Particularly, DNS have outperformed most of the existing negative sampling methods in different recommendation objectives and scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We categorize existing DNS strategies into six groups: Universal DNS, User-similarity DNS, Knowledge-aware DNS, Distribution-based DNS, Interpolation DNS and Mixed DNS. The comprehensive examination of their points of emphasis, advantages, and challenges is detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.T3" title="Table 3 ‣ 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>. Among these methodologies, Universal DNS stands out as an easy-to-deploy and universal method. However, its reliance on user-item matching scores poses a challenge to accuracy. User-similarity DNS prioritizes identifying user similarity associations and capturing user behavior similarities, while facing challenges in user association dependency, particularly for new users. Knowledge-aware DNS emphasizes samples with similar attributes to positives to capture their content correlations, introducing an additional dependency on knowledge. Distribution-based DNS analyzes the inherent distribution in the dataset, focusing on real negatives but adding the dependency on space complexity. Interpolation DNS balances positive and negative samples by injecting information from positives but faces the over-smoothing issue, neglecting crucial samples.
Mixed DNS has exhibited significant flexibility by combining multiple strategies, but the complex hyperparameter tuning and substantial computational costs bring new challenges to these strategies.
In the subsequent parts, we will introduce the implementation details and the effectiveness mechanisms of the existing dynamic negative sampling strategies for detecting informative negative samples to facilitate model optimization with enhanced accuracy and efficiency.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Universal DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Universal DNS selects the top-ranked item as the negative sample from the randomly selected item candidates <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib195" title="">2020a</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib197" title="">2018</a>; Brin and Page, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib8" title="">1998</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib67" title="">2021b</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib116" title="">2024a</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib96" title="">2024</a>)</cite>, which is proposed to select more informative negative samples with the current model state and has been widely applied in various recommendation scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Top-ranked universal negative selection.</span> DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite> hypothesizes that any unobserved item should not be ranked higher than any observed positive item and subsequently oversamples items top-ranked by the recommender from the randomly selected candidates. Due to the fact that the more popular an item is, the more times it acts as a positive sample in the conventional pairwise training procedure. DNS is expected to alleviate the popularity bias in RS.
Along this line, a series of studies <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib80" title="">2018</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib57" title="">2022</a>)</cite> enhance DNS from different perspectives. For example, MCRec <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib80" title="">2018</a>)</cite> pretrains the nodes’ embedding with a pure matrix factorization method, computes and averages the pairwise similarities between two consecutive nodes along a path instance, and then keeps top-K path instances with the highest average similarities. To efficiently identify hard negative instances, CA-FME <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib101" title="">2020</a>)</cite> samples negative instances from triple sides of the positive triplet, and grants larger sampling probability for harder negative samples according to their compatibility scores.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">Dynamic negative selection with high-ranking scope.</span> PinSage <cite class="ltx_cite ltx_citemacro_citep">(Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib197" title="">2018</a>)</cite> argues that the highest-scored item is likely to be a potentially positive sample, leading to the false negative problem. Meanwhile, it also demonstrates that the top-ranked items are more similar to the positive items than randomly selected ones, which forces the recommender to learn to distinguish items at a finer granularity. Specifically, it utilizes the query items’ PageRank score to rank the uniformly selected candidates, randomly samples hard negative items from the items ranked between 2000 and 5000, and designs a curriculum training scheme to help with convergence. ITSMGCN <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib57" title="">2022</a>)</cite> proposes an improved dynamic negative sampler to detect the best sampling scheme by carefully tuning the number of item candidates and the ranking position of the hard negative sample in the ranking diagram. DNS<math alttext="*" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.1.m1.1"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mo id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><times id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">*</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.1.m1.1d">∗</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite> further reduces the probability of selecting overly hard negative samples by expanding both the number of item candidates and the selection range in DNS.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p4.1.1">Collaborative distillation of dynamic negative samples.</span> To address the sparsity of positive feedback and the ambiguity of missing feedback in recommendation, CD-SG <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib97" title="">2019</a>)</cite> transposes DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite> into knowledge distillation (KD) and proposes collaborative distillation (CD). It demonstrates that the higher-ranked items are more important because they can be potential inclusions in top-N recommendation. Specifically, the student model of CD-SG samples items with the dynamic soft target (the higher the ranking, the more the items are sampled) according to their ranking orders in the teacher model, with the aim to capture both positive and negative correlations and overcome the disadvantage of rank distillation that ignores negative feedback among items. Similarly, DE-RRD <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib86" title="">2020</a>)</cite> reformulates the daunting task of learning all the precise ranking orders in recommendation to a relaxed ranking matching problem within the framework of KD. It first defines both the detailed ranking orders among the interesting items and the relative ranking orders between the interesting items and the uninteresting items from the teacher’s recommendation list and then defines a relaxed permutation probability to locate all the interesting items higher than all the uninteresting items in student model, while maintaining the detailed ranking orders among the interesting items.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p5.1.1">Adjacent relation-based dynamic negative selection in graph structure.</span>
Along the development of GNN-based recommender models, a series of studies incorporated graph structure into the sampling strategy. For example, HIN-Bilinear <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib67" title="">2021b</a>)</cite> leverages relative similar parts of the graph, constructs the sampled mini-batch with the sampled triplets and iteratively selects related items (users) of users (items) in sampled triplets to avoid ruining the topological structure of the large-scale graph. MCNS <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib195" title="">2020a</a>)</cite> argues that the Markov chain for one node is likely to still work well for its neighbors due to the connatural property. It proposes an adapted version of the Metropolis-Hastings algorithm to traverse the graph structure by Depth First Search and generate negative samples from the last node’s Markov chain.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p6.1.1">Dynamic negative selection with representation learning.</span> With the advancement of representation learning in recommendation, a cohort of researchers and industry developers have embarked on an investigation of the distinction between hard negative samples and universal negative samples from the perspective of representation learning.
To illustrate, PDRec <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib116" title="">2024a</a>)</cite> examines the inherent information disparities among observed instances (positive samples) and the latent relevance of soft samples within the unobserved items (negative samples). It believes that following the emphasis on information-rich samples within the corpus as the positive instances, the deployment of hard negative sampling may concurrently exacerbate issues within the recommender, including data imbalance and over-fitting risks.
To address this issue, PDRec designs a noise-free negative sampling strategy to capture items with low confidences as the <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.2">Safe Negative Samples (SNS)</em> which exhibit no relevance to user preferences in model optimization.
Conversely, RealHNS <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite> meticulously investigates the definition of <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.3">Real Hard Negative Samples (RHNS)</em> (samples that are neither too easy nor too hard) and <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.4">False Hard Negative Samples (FHNS)</em> (samples that represents users’ actual preferences in the future) and then defines the equation: <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.5">HNS</em> = <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.6">RHNS</em> + <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.p6.1.7">FHNS</em>.
To effectively leverage these two concepts, RealHNS employs a curriculum learning framework to construct the multi-granular RHNS selector (coarse-grained candidates selector and fine-grained samples selector). This facilitates the dynamic sampling of high-quality RHNS that are suitable for recommender’s current state, thereby alleviating the issue of negative transfer.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>User-similarity DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">User-similarity DNS identifies similar users based on their preferences from their historical behaviors and then dynamically selects items as negative samples according to this similarity association <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Giobergia, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib56" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite>. It can capture the dynamic user interest with historical user behaviors and approximate the user’s conditional preference state with the correlation between these users. Some studies emphasize the explicit associations among users, inferring the information dissemination path and collaborative relationships through the connections in social networks or explicit interaction behaviors. SamWalker <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite> proposes an efficient social-based sampling strategy to both speed up the gradient estimation and reduce the time-consuming of the preference inferring from the massive unobserved feedback. To reduce the extent to which the model relies on other users’ preferences, EvalRS <cite class="ltx_cite ltx_citemacro_citep">(Giobergia, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib56" title="">2022</a>)</cite> proposes a relatively audacious negative sampling strategy to capture neighboring users with their collaborative representations and regards item from the interaction list of the user’s neighbors that the user have not interacted as the negative sample.
Conversely, other studies focus on implicit relationships, which refer to connections that are not directly observable. Such approaches aim to reveal the potential collaborative behaviors, interest similarities, and latent commonalities by mining users’ behavioral patterns. DEEMS <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>)</cite> believes that unobserved positive samples are more likely to exist correlations with observed positive samples than unobserved negative ones. Based on this hypothesis, DEEMS designs a hedging gradient between the user-centered and item-centered correlations to reorder the sampled unobserved items and rank the unobserved positive samples before the negative ones. To handle the absence of explicit social information, SamWalker++ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>)</cite> constructs the pseudo-social network, that is, similar users are connected with specific item nodes or community nodes. It proposes a subtle sampling strategy based on the random walk along the pseudo-social network to precisely model the triplet confidence and determine which data are used to update parameters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Knowledge-aware DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Knowledge-aware DNS dynamically captures the informative samples with their attributes or other relevant features and selects the informative items related to the positive sample as negative samples <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib27" title="">2016</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib159" title="">2021d</a>)</cite>. These strategies are designed to incorporate task-specific information to intelligently select more relevant and informative negative examples for the specific task. In Location Recommendation, users’ negatively-preferred locations are mixed with potentially positive ones in <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">italic_k</annotation></semantics></math> un-visited locations. However, informativeness differs from sample to sample, so treating them equally in optimization is far from optimal. GeoSAN <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>)</cite> designs geography-aware negative sampler to retrieve some nearest locations to the target location as the location candidates and employ negative sampling based on the uniform distribution or a popularity-based distribution. CITING <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib27" title="">2016</a>)</cite> proposes a time-aware negative sampling strategy to select the tweets that were posted in close temporal proximity to his/her positive tweets as the negative samples in Tweet Recommendation. RPRM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib159" title="">2021d</a>)</cite> also designs a novel negative sampling strategy to model the agreement of review properties between the users’ interacted items and the unseen items. To investigate the Debiased Recommendation, FairNeg <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite> dynamically perceives the group-level unfairness based on the performance disparity of different category groups during the training process and adjusts each group’s corresponding negative sampling probability, intending to equalize all groups’ performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4. </span>Distribution-based DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">Distribution-based DNS analyzes the distribution pattern of positive and negative samples, then dynamically selects informative negative samples that do not overly disturb the training phase <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib95" title="">2023</a>)</cite>. Some sampling strategies <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>)</cite> explore the statistical properties of both <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS4.p1.1.1">RHNS</em> and <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S3.SS2.SSS4.p1.1.2">FHNS</em> to distinguish the relevant negative samples that are less informative or misleading, which are more likely to provide valuable information in RS. SRNS <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>)</cite> discovers an empirical conclusion that only a limited number of samples are potentially crucial for the recommender, and false negative samples tend to get stable scores during training and proposes a variance-based sampling function to distinguish negative samples based on the aforementioned findings. GDNS <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>)</cite> then introduces an expectational gain-tuning sampler to leverage the high-gap expectation of users’ preference in the training process to reliably measure the quality of negative samples and dynamically guide the negative sampling procedure. In contrast, DENS <cite class="ltx_cite ltx_citemacro_citep">(Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib95" title="">2023</a>)</cite> argues that real-world interactions are primarily driven by certain relevant factors associated with the items while not encompassing all factors. Therefore, DENS delineates the relevant and irrelevant factors within items that are associated with users or not. Subsequently, it devises a factor-aware sampling strategy to sample the appropriate by comparing the relevant factors while ensuring the similarity of irrelevant factors.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5. </span>Interpolation DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1">Interpolation DNS synthesizes the informative negative samples by injecting information from positive samples with the interpolation technology <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib136" title="">2022</a>)</cite>. These approaches aim to explore the balance between maintaining the informative knowledge while avoiding excessive disturbance to the distribution of the feature space during the optimization process, which allows recommender to adapt to the changing requirements and optimize the balance between positive and negative samples. MixGCF <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib84" title="">2021</a>)</cite> interpolates different proportions of positive embeddings into item candidates, optimizing the representation of the selected samples and translating the potential FNS into negative samples to avoid training bias. SDNS <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib136" title="">2022</a>)</cite> proves that the positive-dominated mixing in MixGCF has the same effect as scaling the sigmoid function in BPR loss function in DNS, and specifically proposes an improvement scheme to define a soft BPR loss function, which not only preserves the integrity of the DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite>, but also yields remarkable improvements in terms of effectiveness and robustness.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6. </span>Mixed DNS</h4>
<div class="ltx_para" id="S3.SS2.SSS6.p1">
<p class="ltx_p" id="S3.SS2.SSS6.p1.3">Mixed DNS combines multiple dynamic negative sampling strategies to generate more diverse and effective negative sample candidates <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib28" title="">2017</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib98" title="">2020</a>; Hidasi and Karatzoglou, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib75" title="">2018</a>; Hidasi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib76" title="">2015</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib83" title="">2020</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib188" title="">2020c</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib182" title="">2021</a>)</cite>. By integrating the uniform negative examples with those above dynamic negative sampling strategies, the existing Mixed DNS strategy provides a flexible framework that allows recommenders to customize the task-specific combination of different dynamic negative sampling strategies and attempt to overcome these limitations and achieve a more comprehensive and robust negative sampling process. As the pioneer Mixed DNS strategy, NNCF <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib28" title="">2017</a>)</cite> proposes three novel sampling strategies to significantly improve training efficiency as well as the recommendation performance, named stratified sampling, negative sharing, and stratified sampling with negative sharing. Stratified sampling selects stratum (set of links on the bipartite graph sharing the same source or destination node) based on either item or user. Negative sharing constructs negative pairs by connecting non-linked users and items in the batch without any increasing computational burdens. And stratified sampling with negative sharing denotes the combination of the above two sampling strategies. For Conversational Recommendation, EAR <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib98" title="">2020</a>)</cite> designs a mixed negative sampling strategy to simultaneously learn the user’s general and specific preference with two types of negative samples (the traditional randomly selected samples and the candidate samples satisfying the partially known preference in the conversation). FB-MNS <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib83" title="">2020</a>)</cite> discovers that models trained with hard negative samples cannot outperform random negative samples. Therefore, It proposes a mixed sampling strategy to blend random and hard negatives and define the ratio of random to hard negative samples as <math alttext="100:1" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.1.m1.1"><semantics id="S3.SS2.SSS6.p1.1.m1.1a"><mrow id="S3.SS2.SSS6.p1.1.m1.1.1" xref="S3.SS2.SSS6.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS6.p1.1.m1.1.1.2" xref="S3.SS2.SSS6.p1.1.m1.1.1.2.cmml">100</mn><mo id="S3.SS2.SSS6.p1.1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS6.p1.1.m1.1.1.1.cmml">:</mo><mn id="S3.SS2.SSS6.p1.1.m1.1.1.3" xref="S3.SS2.SSS6.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.1.m1.1b"><apply id="S3.SS2.SSS6.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS6.p1.1.m1.1.1"><ci id="S3.SS2.SSS6.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS6.p1.1.m1.1.1.1">:</ci><cn id="S3.SS2.SSS6.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS6.p1.1.m1.1.1.2">100</cn><cn id="S3.SS2.SSS6.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS6.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.1.m1.1c">100:1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.1.m1.1d">100 : 1</annotation></semantics></math> in the training process, where the hard negative samples are selected between the rank position 101-500 to achieve the best Recall metric. GRU4Rec<math alttext="*" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.2.m2.1"><semantics id="S3.SS2.SSS6.p1.2.m2.1a"><mo id="S3.SS2.SSS6.p1.2.m2.1.1" xref="S3.SS2.SSS6.p1.2.m2.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.2.m2.1b"><times id="S3.SS2.SSS6.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS6.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.2.m2.1c">*</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.2.m2.1d">∗</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Hidasi and Karatzoglou, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib75" title="">2018</a>)</cite> extends the in-batch negative sampling strategy of GRU4Rec <cite class="ltx_cite ltx_citemacro_citep">(Hidasi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib76" title="">2015</a>)</cite> with the additional shared samples in the mini-batch, where the shared samples are sampled with a popularity-based SNS strategy. MNS <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib188" title="">2020c</a>)</cite> employs the same mixed sampling strategy as GRU4Rec<math alttext="*" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.3.m3.1"><semantics id="S3.SS2.SSS6.p1.3.m3.1a"><mo id="S3.SS2.SSS6.p1.3.m3.1.1" xref="S3.SS2.SSS6.p1.3.m3.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.3.m3.1b"><times id="S3.SS2.SSS6.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS6.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.3.m3.1c">*</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.3.m3.1d">∗</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Hidasi and Karatzoglou, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib75" title="">2018</a>)</cite> that incorporates the randomly selected additional samples as the supplement of the in-batch negative sampling strategy to tackle the selection bias and the inflexibility to adjust sampling distribution. Taking into account user feedback on attributes, FPAN <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib182" title="">2021</a>)</cite> employs DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite> to select informative negative samples in addition to directly sampling the non-interacted items as negative items, accelerating the training process.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Adversarial Negative Generation</h3>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Illustration of two types of commonly-used Adversal Negative Generation algorithms.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.1" style="width:433.6pt;height:305.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.0pt,78.8pt) scale(0.659413464762521,0.659413464762521) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.1.1" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T4.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T4.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T4.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T4.1.1.1.1.1.p1.1.1"></span> <span class="ltx_text" id="S3.T4.1.1.1.1.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T4.1.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.1.1.1.p1.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1.1.p1.1.2.1.1.1.1">Algorithms</span></span></span>
</span></span><span class="ltx_text" id="S3.T4.1.1.1.1.1.p1.1.3"></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.1.2.1">
<tr class="ltx_tr" id="S3.T4.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.2.1.1.1.1">Generation</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.1.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.2.1.2.1.1">strategies</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.3.1">Advantages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.4.1">Challenges</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.2.1" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.2.1.1">
<span class="ltx_p" id="S3.T4.1.1.2.1.1.1">Generative ANG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib51" title="">2019</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib141" title="">2020</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib39" title="">2019</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib180" title="">2022b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib216" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.2.2.1">
<tr class="ltx_tr" id="S3.T4.1.1.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Generating the</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">mendacious but</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.2.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">plausible samples</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.2.3.1">
<tr class="ltx_tr" id="S3.T4.1.1.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Diverse negative generation;</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Improved model generalization</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.2.4.1">
<tr class="ltx_tr" id="S3.T4.1.1.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Complex training process;</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.2.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Pattern breakdown risk</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.3.1" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.3.1.1">
<span class="ltx_p" id="S3.T4.1.1.3.1.1.1">Sampled ANG <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib155" title="">2018b</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib68" title="">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib153" title="">2017b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib150" title="">2018a</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.1.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.3.2.1">
<tr class="ltx_tr" id="S3.T4.1.1.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sampling likely</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">negative instances</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.2.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">related to positives</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.1.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.3.3.1">
<tr class="ltx_tr" id="S3.T4.1.1.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Simplify GAN training;</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Directly leveraging authentic data</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.1.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1.3.4.1">
<tr class="ltx_tr" id="S3.T4.1.1.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Homogenization of negatives;</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.1.1.3.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Fail to cover all user preferences</td>
</tr>
</table>
</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Adversarial Negative Generation (ANG) is a sophisticated technique which devise to enhance the robustness and performance in RS. Same as other negative sampling strategies, the core of ANG seeks to address the challenge of imbalanced training data, where positive interactions tend to dominate while real negative instances are relatively sparse. Existing ANG studies can be fundamentally categorized into two distinct paradigms: Generative ANG and Sampled ANG. The key emphases, advantages, and challenges of each strategy are systematically outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.T4" title="Table 4 ‣ 3.3. Adversarial Negative Generation ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Generative ANG endeavors to leverage a generative adversarial network (GAN) <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib58" title="">2014</a>)</cite>, in which the generator and discriminator play a game-theoretical minimax game. The generator attempts to synthesize the plausible negative instances, while the discriminator seeks to distinguish between the genuine and generated samples. Through the iterative adversarial training, the generator becomes increasingly adept at generating mendacious but plausible negative instances, thereby enabling diverse negative generation, improving model generalization and substantially enriching the robustness of the recommender. However, it faces the risk of pattern breakdown and may lead to the relatively complex training process.
On the other hand, Sampled ANG strategically samples appropriate items from the corpus with the Nash equilibrium state between the generator and discriminator in GAN. These selected items are then incorporated into the recommenders’ training process. Its advantage lies in guaranteeing the simplified GAN training procedure and the utilization of instances originating exclusively from authentic data. This eliminates any potential training bias stemming from varying levels of GAN’s generation complexity. Consequently, it effectively alleviates the imbalance between positive and negative samples in RS, enabling its training on a more balanced dataset. Nevertheless, it encounters challenges such as the homogenization of negatives and the difficulty in covering all potential user preferences.
In the forthcoming sub-sections, we will expound upon these two methods from the existing ANG studies in RS.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Generative ANG</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Generative ANG attempts to capture the underlying data distribution and generate the mendacious but plausible samples with an implicit generative model instead of imposing any assumption on the existing data. It can maintain the Nash equilibrium between generator and discriminator to <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p1.1.1">generate negative samples that are similar to positive samples but have subtle differences</span>, making them hard to distinguish from positive samples.<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib155" title="">2018b</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib51" title="">2019</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib141" title="">2020</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib39" title="">2019</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib68" title="">2021a</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib180" title="">2022b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib216" title="">2021</a>)</cite>. Prevailing research in Generative ANS generally employs three methods for generating negative samples: user-specific generation, distribution-associated generation, and content-aware generation. Subsequently, we will elucidate each of these approaches in accordance with relevant works.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.1">User-specific generation</span>.
Generative ANG leverages the adversarial mechanism to generate the targeted negative samples for different users by generators and maximize the objective function between real and generated item representations by discriminators, thereby pushing the recommender to its performance limit. RNS <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib39" title="">2019</a>)</cite> designs an embedding-based sampler to generate exposure-alike negative instances and train a better recommender as the predictor through the feature matching technique and the adversarial training instead of directly choosing from exposure data. Different from RNS which only models the GAN as the sample generator, ELECRec <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>)</cite> proposes to define the recommendation as a discriminative task rather than a generative task, that is, trains the discriminator as the final recommender to distinguish if a sampled item is a ‘real’ target item or not. It defines the generator as an auxiliary model trained with the next-item prediction task to generative high-quality plausible samples, improving the discrimination ability of the discriminator from the intricate correlations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.5"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.5.1">Distribution-associated generation</span>. Generative ANS also adopts the adversarial training paradigm to model the optimal data distribution from the correlation between users and items. For example, DRCGR <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib51" title="">2019</a>)</cite> generates the relevancy distribution of targeted negative samples for different users with the generator and achieves a better estimate of the relevancy between the generated negative feedback sequence and the relevant positive feedback with the discriminator, thereby automatically learning the optimal recommendation strategies. To learn the true relevance distribution of users and items, PURE <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib216" title="">2021</a>)</cite> continuously samples on both users and items in the embedding space. Specifically, given a positive instance <math alttext="(u,i)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.1.m1.2"><semantics id="S3.SS3.SSS1.p3.1.m1.2a"><mrow id="S3.SS3.SSS1.p3.1.m1.2.3.2" xref="S3.SS3.SSS1.p3.1.m1.2.3.1.cmml"><mo id="S3.SS3.SSS1.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p3.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS3.SSS1.p3.1.m1.1.1" xref="S3.SS3.SSS1.p3.1.m1.1.1.cmml">u</mi><mo id="S3.SS3.SSS1.p3.1.m1.2.3.2.2" xref="S3.SS3.SSS1.p3.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p3.1.m1.2.2" xref="S3.SS3.SSS1.p3.1.m1.2.2.cmml">i</mi><mo id="S3.SS3.SSS1.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p3.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.1.m1.2b"><interval closure="open" id="S3.SS3.SSS1.p3.1.m1.2.3.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.2.3.2"><ci id="S3.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p3.1.m1.1.1">𝑢</ci><ci id="S3.SS3.SSS1.p3.1.m1.2.2.cmml" xref="S3.SS3.SSS1.p3.1.m1.2.2">𝑖</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.1.m1.2c">(u,i)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.1.m1.2d">( italic_u , italic_i )</annotation></semantics></math>, the generator of PURE generates a fake item <math alttext="i^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.2.m2.1"><semantics id="S3.SS3.SSS1.p3.2.m2.1a"><msup id="S3.SS3.SSS1.p3.2.m2.1.1" xref="S3.SS3.SSS1.p3.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p3.2.m2.1.1.2" xref="S3.SS3.SSS1.p3.2.m2.1.1.2.cmml">i</mi><mo id="S3.SS3.SSS1.p3.2.m2.1.1.3" xref="S3.SS3.SSS1.p3.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.2.m2.1b"><apply id="S3.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1.2">𝑖</ci><ci id="S3.SS3.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p3.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.2.m2.1c">i^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.2.m2.1d">italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> that is highly likely to be relevant to <math alttext="u" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.3.m3.1"><semantics id="S3.SS3.SSS1.p3.3.m3.1a"><mi id="S3.SS3.SSS1.p3.3.m3.1.1" xref="S3.SS3.SSS1.p3.3.m3.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.3.m3.1b"><ci id="S3.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p3.3.m3.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.3.m3.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.3.m3.1d">italic_u</annotation></semantics></math> and a fake user <math alttext="u^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.4.m4.1"><semantics id="S3.SS3.SSS1.p3.4.m4.1a"><msup id="S3.SS3.SSS1.p3.4.m4.1.1" xref="S3.SS3.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS3.SSS1.p3.4.m4.1.1.2" xref="S3.SS3.SSS1.p3.4.m4.1.1.2.cmml">u</mi><mo id="S3.SS3.SSS1.p3.4.m4.1.1.3" xref="S3.SS3.SSS1.p3.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.4.m4.1b"><apply id="S3.SS3.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1.2">𝑢</ci><ci id="S3.SS3.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS3.SSS1.p3.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.4.m4.1c">u^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.4.m4.1d">italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> that is likely to be relevant to <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p3.5.m5.1"><semantics id="S3.SS3.SSS1.p3.5.m5.1a"><mi id="S3.SS3.SSS1.p3.5.m5.1.1" xref="S3.SS3.SSS1.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p3.5.m5.1b"><ci id="S3.SS3.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p3.5.m5.1d">italic_i</annotation></semantics></math> from the random Gaussian noise.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p4.1.1">Content-aware generation</span>. With the contextual information, generative ANG can generate negative samples that align more closely with the content of positive instances, drawing from the heterogeneous knowledge encapsulated in positive samples and historical user interactions. For new-item recommendation, LARA <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib141" title="">2020</a>)</cite> designs a generative model with multi-generators to generate the user profile which is not only close to the real profile but also relevant to the given item with the item’s attributes. AFE <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib180" title="">2022b</a>)</cite> defines the generator to take the common features (e.g., user historical behaviors, item features, and contexts) rather than future features for generating “fake” items which may be clicked to confuse the future-aware discriminator, thus preventing the feature inconsistency and overfitting issues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Sampled ANG</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Sampled ANG tries to fit the underlying distribution as much as possible, and <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p1.1.1">samples the most likely items as negative instances to be connected with the positive sample</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib155" title="">2018b</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib68" title="">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib153" title="">2017b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib150" title="">2018a</a>)</cite>. Different from the typical Generative ANG methods, the generator of Sampled ANG works directly select instances from the existing items in the corpus rather than their features. As one of the most famous Sampled ANG works, IRGAN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib153" title="">2017b</a>)</cite> stands as a pioneering endeavor that incorporates GANs <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib58" title="">2014</a>)</cite> into RS with a game-theoretical minimax game which optimizes both generator and discriminator iteratively. The primary objective of its generator lies in strategically selecting negative instances from the candidate pool and intermixing them with positive examples to confound the discriminator. Conversely, its discriminator is designed to distinguish positive samples from the negative instances sampled by the generator, and subsequently provide constructive feedback to refine the generator. Unlike the conventional Generative ANG methods which directly generate negative samples, IRGAN tries to sample negative instances based on the estimated correlation between candidate items and queries vie its generator. Similarly, the generator in GraphGAN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib150" title="">2018a</a>)</cite> attempts to approximate the underlying connectivity distribution in the graph structure and samples the relevant vertices similar to ground truth’s real immediate neighbors to deceive the discriminator. In contrast, its discriminator tries to detect whether the sampled vertex is from ground truth or selected by the generator. NMRN-GAN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib155" title="">2018b</a>)</cite> also develops a GAN-based noise sampler to adaptively sample the hardest negative examples from the randomly selected candidates, which considers both the specific user and the recommender parameters with the policy gradient-based reinforcement learning framework. In Multi-domain recommendation task, AFT <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib68" title="">2021a</a>)</cite> proposes a domain-specific masked encoder and a GAN-based two-step feature translation to aggregate the domain-specific preferences from other domains, thereby retrieving the fake clicked samples in different domains for adversarial training.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Importance Re-weighting</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Importance Re-weighting (IRW) is one of the sophisticated statistical techniques employed in Data Mining and Machine Learning <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib199" title="">2020</a>)</cite>. Its core revolves around the nuanced adjustment of sample weights to prioritize the significance of certain negative samples within a given dataset <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib18" title="">2023d</a>; Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib219" title="">2022</a>)</cite>. It plays an integral role in scenarios where a dataset exhibits long-tailed problems or when specific samples possess varying degrees of importance. For the recommendation task where unobserved samples vastly outnumber positive feedback, IRW can be instrumental in rebalancing the real-world dataset by assigning diverse weights to different samples, thus enabling the algorithm to focus its learning efforts on the rare but crucial negative instances <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. The significance of IRW cannot be overstated. Its essential idea is consistent with the memorization effect: recommenders tend to learn accessible user preferences in the early stage of the training process and eventually memorize all complicated interactions. Properly assigning weights to individual samples attunes recommenders to the intricacies of imbalanced implicit datasets, thus recalibrating the learning process for the disparate data distribution. This adaptive weighting not only addresses the challenge of imbalanced data but also fosters the development of more robust and precise recommenders, ultimately bring in superior prediction accuracy.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The investigation of IRW algorithms classify them into Attention-based IRW, Knowledge-based IRW, and Debiased IRW. These strategies’ critical aspects, advantages, and challenges are systematically delineated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.T5" title="Table 5 ‣ 3.4. Importance Re-weighting ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>. Attention-based IRW emphasizes assigning diverse importance to each item with user interest attention, making it adaptably and capable of flexibly adjusting weights. However, it faces challenges due to its complexity and relatively poor interpretability. Knowledge-based IRW focuses on identifying each item’s importance with external structured knowledge, offering suitability for cold-start issues but introducing the additional dependency on knowledge. Debiased IRW aims to correct ubiquitous biases, assigning higher weights to overlooked items to provide equitable and diverse recommendations. Nevertheless, it involves the trade-off between fairness and precision. In the subsequent sub-sections, we will introduce the technical details of Attention-based IRW, Knowledge-based IRW, and Debiased IRW, respectively.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Illustration of two types of commonly-used Importance Re-weighting algorithms.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T5.1" style="width:433.6pt;height:491pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.0pt,154.0pt) scale(0.614529893514963,0.614529893514963) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.1.1" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T5.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T5.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T5.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T5.1.1.1.1.1.p1.1.1"></span> <span class="ltx_text" id="S3.T5.1.1.1.1.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T5.1.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.1.1.1.p1.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.1.1.p1.1.2.1.1.1.1">Algorithms</span></span></span>
</span></span><span class="ltx_text" id="S3.T5.1.1.1.1.1.p1.1.3"></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.2.1">Reweighting functions</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.3.1">Advantages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.4.1">Challenges</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.2.1" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.1.2.1.1">
<span class="ltx_p" id="S3.T5.1.1.2.1.1.1">Attention-based IRW <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib104" title="">2020a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib156" title="">2021c</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib209" title="">2018</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib53" title="">2022a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib34" title="">2016</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib169" title="">2022a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib18" title="">2023d</a>; Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib219" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.2.2.1">
<tr class="ltx_tr" id="S3.T5.1.1.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Assigning diverse weights to</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">each item with user attention</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.2.3.1">
<tr class="ltx_tr" id="S3.T5.1.1.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Adaptable;</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Flexibly adjusts weights</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.2.4.1">
<tr class="ltx_tr" id="S3.T5.1.1.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Complex;</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.2.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Poor interpretability</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.3.1" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.1.3.1.1">
<span class="ltx_p" id="S3.T5.1.1.3.1.1.1">Knowledge-based IRW <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib90" title="">2021a</a>; Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.3.2.1">
<tr class="ltx_tr" id="S3.T5.1.1.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Identifying item’s importance</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">with the external knowledge</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.3.3.1">
<tr class="ltx_tr" id="S3.T5.1.1.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Interpretable;</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Suitable for cold-start items</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.3.4.1">
<tr class="ltx_tr" id="S3.T5.1.1.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Additional knowledge</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.3.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">dependency</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.4.1" style="width:108.1pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T5.1.1.4.1.1">
<span class="ltx_p" id="S3.T5.1.1.4.1.1.1">Debiased IRW <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib135" title="">2021</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib199" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.1.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.4.2.1">
<tr class="ltx_tr" id="S3.T5.1.1.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Correcting the ubiquitous</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">biases and assigning high</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.4.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.2.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">weights to overlooked items</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.1.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.4.3.1">
<tr class="ltx_tr" id="S3.T5.1.1.4.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Deliver equitable and</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.4.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">diverse recommendations</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.1.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.1.1.4.4.1">
<tr class="ltx_tr" id="S3.T5.1.1.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">trade-off between</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.1.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T5.1.1.4.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">fairness and precision</td>
</tr>
</table>
</td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Attention-based IRW</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Attention-based IRW assigns importance scores and adjusts weights for each items, which could focus on the most relevant items for each user and provide suitable recommendations aligned with the user’s personalized interests with the attention mechanism. <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib53" title="">2022a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib34" title="">2016</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib169" title="">2022a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib18" title="">2023d</a>; Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib219" title="">2022</a>)</cite>.
To eﬃciently train recommenders with millions of items in the real-world application, this strategy attempt to sample negative candidates from the background distribution and then correct them via importance weighting.
Firstly, a series of works attempt to <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p1.1.1">focus on the safe (easy) negative samples</span> while overlooking the hard negative samples. To adaptively prune the large-loss interactions along the training process, ADT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib156" title="">2021c</a>)</cite> devises the truncated loss paradigm to discard the hard samples (large-loss ones) with a dynamic threshold in each iteration and a reweighted loss paradigm to adaptively lower the weights of large-loss samples. UIB <cite class="ltx_cite ltx_citemacro_citep">(Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib219" title="">2022</a>)</cite> innovatively introduces an auxiliary score for each user to define a personalized decision boundary and individually penalize samples that cross the boundary with a hybrid loss of the point-wise paradigm and the pair-wise paradigm.
Moreover, some studies assume that <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p1.1.2">more informative negative samples can contribute more to the gradient of loss function</span>, the magnitude of the gradient becomes much larger so that training can be accelerated. To assist the learning process from the most important historical transitions, DEERS <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib209" title="">2018</a>)</cite> leverage the prioritized sampling strategy to greedily select the most informative negative sample while ensuring that the probability of being sampled is monotonic in a transition’s priority in reinforcement learning.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">Recently, some related studies imitate the learning attention in human perception to <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">assign different weights to diverse negative samples from simple to complex (whose informativeness is from less to more)</span>. PRIS <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib104" title="">2020a</a>)</cite> proposes a personalized negative sampler from simple to complex based on rejection sampling to assign larger weights to more informative negative samples, which can contribute more to the gradient of the ranking loss function and accelerate the convergence rate. With the aim to address the distribution mismatch, REINFORCE <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib25" title="">2019a</a>)</cite> takes the first-order approximation and ignores the state visitation differences as the importance weights of diverse negative samples with the Normalized Importance Sampling (NIS) strategy to construct a slightly biased estimator of the policy gradient with lower variance. SGDL <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib53" title="">2022a</a>)</cite> first investigates the memorization effect within the representative recommenders’ learning process, delineating it into the noise-resistant period and the noise-sensitive period. Then, SGDL collects memorized interactions as denoising signals at the noise-resistant period and imposes a weight on each sample loss to enhance the robustness of the subsequent training during the noise-sensitive period. GeoSAN <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>)</cite> also proposes a weighted BCE loss based on importance sampling to reweight the unvisited locations with the negative probability, such that more informative locations can be more emphasized even with the uniform negative sampler. BIAO <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib18" title="">2023d</a>)</cite> highlights the behavior-level effect in the source domain according to the corresponding item’s global impact and local user-specific impact during the proposed bi-level optimization process, which may have different importance for the cross-domain recommender’s optimization. ContraRec <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib149" title="">2023a</a>)</cite> leverages the temperature hyper-parameter <math alttext="\tau_{1}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.1.m1.1"><semantics id="S3.SS4.SSS1.p2.1.m1.1a"><msub id="S3.SS4.SSS1.p2.1.m1.1.1" xref="S3.SS4.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.p2.1.m1.1.1.2" xref="S3.SS4.SSS1.p2.1.m1.1.1.2.cmml">τ</mi><mn id="S3.SS4.SSS1.p2.1.m1.1.1.3" xref="S3.SS4.SSS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.1.m1.1b"><apply id="S3.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.2">𝜏</ci><cn id="S3.SS4.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.SSS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.1.m1.1c">\tau_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p2.1.m1.1d">italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> in the proposed CTC loss to assign diverse attention to negative samples according to their hardness in prediction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Knowledge-based IRW</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Knowledge-based IRW incorporates the external structured knowledge into the user’s preference modeling process to identify the importance of items from the corpus and assigns the corresponding weights in model optimization <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib90" title="">2021a</a>; Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite>. The solutions employed in the Attention-based IRW studies design a weighting function that maps each sample’s score or loss value to its learning weight, thereby enabling the recommender to adaptively learn the information of each negative sample when model optimizing <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib53" title="">2022a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib34" title="">2016</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib169" title="">2022a</a>)</cite>. However, knowledge-based IRW argues that users’ social contexts and items’ heterogeneous information serve as influential determinants in shaping the reciprocal engagements between users and items <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib189" title="">2023b</a>)</cite>. In tandem, these elements covertly impact the confidence of the data. When an item is more popular among the user’s social neighbors, the user is more likely to know the item, and his feedback is more attributed to his real preference. Correspondingly, in instances where the items in the user’s historical behaviors exhibit a level of coherence in their content, recommender can effortlessly predict the items sharing the similar semantic attributes <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>)</cite>. This, in turn, addresses the user’s preference for particular content within the contextual paradigm, and this knowledge will be more reliable in deriving user preference.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">Within the realm of recommendation, the pre-defined knowledge is typically categorized into user-oriented social knowledge and item-oriented heterogeneous knowledge. Different knowledge-based IRW methodologies are commonly utilized to integrate these distinct forms of knowledge into the process of importance re-weighting.
For the user-oriented social knowledge, SamWalker <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite> simulates item information propagation along the real social network and adaptively specifies individual confidence weights to different interactions based on user’s social contexts. To mitigate SamWalker’s reliance on authentic user affiliations, SamWalker++ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>)</cite> introduces the rationale of “wisdom of the crowds” to define the pseudo-social network, in which similar users are connected with specific item nodes or community nodes to improve the accuracy of learned data confidence. Under this scenario, SamWalker++ can model data confidence and assign adaptive confidence weights to each interaction without any side information.
Furthermore, for the item-oriented heterogeneous knowledge, PREMERE <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib90" title="">2021a</a>)</cite> proposes a meta-learning-based reweighting strategy to produce the most suitable weight at the current optimization state, which can handle the rarity of positive samples and the noisiness of negative samples simultaneously. URL <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib26" title="">2021a</a>)</cite> leverages an auxiliary task to consider the user state and the context knowledge in reinforcement learning and reweights each example with the long-term reward to avoid the high variance in the learning of the policies caused by the inverse-propensity weighting. MMT-Net <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>)</cite> propose an inverse novelty measure, noted as the context-bias, to attenuate the original supervised loss and decorrelate interactions by emulating variance-reduction, thereby inducing a novelty-weighted strategy focused on harder samples as training proceeds.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3. </span>Debiased IRW</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Debiased IRW identifies and corrects the ubiquitous biases that exist in RS (e.g., popularity bias, exposure bias) and assigns higher weights to items that have been overlooked in the past to deliver more equitable and diverse recommendations <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib135" title="">2021</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib199" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. It operates on the premise that traditional recommenders may inadvertently exacerbate disparities by assigning undue weight to certain categories or user preferences.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1">By assigning diverse weights to items with various categories, debiased IRW endeavors to counteract any predispositions that may be inherent in the traditional recommenders. It is able to foster more equitable item representation, ensuring that no certain categories disproportionately influence the prediction results. For example, FairNeg <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite> proposes a negative sampling distribution mixup mechanism, which incorporates both the importance-aware negative sampling distribution and the fairness-aware negative sampling distribution to simultaneously strengthen the feature representations and item-oriented group fairness. Furthermore, some debiased IRW works also meticulously design and incorporate a suite of fairness metrics and concepts to evaluate the fairness and mitigate the inherent bias. SAR-Net <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib135" title="">2021</a>)</cite> designs a concept of Fairness Coefficient (FC) to measure each sample’s importance and use it to reweigh the prediction in the proposed debiased expert networks, thereby mitigating the data fairness issue caused by manual intervention. IF4URec <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib199" title="">2020</a>)</cite> introduces the Influence Function (IF) to measure the effect on the estimator when adding a small perturbation on one training sample, which could reveal the importance of one training sample. It is able to utilize the IF to reweight the training loss of each sample to get less loss in an unbiased validation for debiasing.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Knowledge-enhanced Negative Sampling</h3>
<figure class="ltx_table" id="S3.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>Illustration of two types of commonly-used Knowledge-enhanced Negative Sampling algorithms.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T6.1" style="width:433.6pt;height:286.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-115.4pt,76.2pt) scale(0.652699611761865,0.652699611761865) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1">
<tr class="ltx_tr" id="S3.T6.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.1.1.1" style="width:68.3pt;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_inline-logical-block ltx_align_top" id="S3.T6.1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S3.T6.1.1.1.1.1.p1">
<span class="ltx_p" id="S3.T6.1.1.1.1.1.p1.1"><span class="ltx_text" id="S3.T6.1.1.1.1.1.p1.1.1"></span> <span class="ltx_text" id="S3.T6.1.1.1.1.1.p1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S3.T6.1.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.1.1.1.p1.1.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.1.1.p1.1.2.1.1.1.1">Algorithms</span></span></span>
</span></span><span class="ltx_text" id="S3.T6.1.1.1.1.1.p1.1.3"></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.2.1">Knowledge source</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.3.1">Advantages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.4.1">Challenges</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.1.2.1" style="width:68.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T6.1.1.2.1.1">
<span class="ltx_p" id="S3.T6.1.1.2.1.1.1">General KNS <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib140" title="">2018</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib142" title="">2015</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>; Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.2.2.1">
<tr class="ltx_tr" id="S3.T6.1.1.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sampling negative with</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">user preference patterns</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.2.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">and item semantics</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.2.3.1">
<tr class="ltx_tr" id="S3.T6.1.1.2.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Considering user-item knowledge</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">context; Captures broader</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.3.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.3.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">knowledge in multiple domains</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.2.4.1">
<tr class="ltx_tr" id="S3.T6.1.1.2.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Additional data progressing</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">complexity; Relies on high-</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2.4.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.2.4.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">quality knowledge data</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.1.3.1" style="width:68.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T6.1.1.3.1.1">
<span class="ltx_p" id="S3.T6.1.1.3.1.1.1">KG-based KNS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>; Ali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib2" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib160" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib21" title="">2022a</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.1.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.3.2.1">
<tr class="ltx_tr" id="S3.T6.1.1.3.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Leveraging explicit association</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.2.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">and the high-order correlation</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.2.1.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">among entities in KG</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.1.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.3.3.1">
<tr class="ltx_tr" id="S3.T6.1.1.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.3.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Fully leverages rich</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.3.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">knowledge relationships in KG</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.1.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1.3.4.1">
<tr class="ltx_tr" id="S3.T6.1.1.3.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.4.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Depends on KG construction;</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T6.1.1.3.4.1.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Complexity of maintaining KG</td>
</tr>
</table>
</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Knowledge-enhanced Negative Sampling (KNS) is one of the pivotal negative sampling strategies in recommendation, which leverages the availability of supplementary informative knowledge, such as the auxiliary information (i.e., users’ social contexts and items’ heterogeneous knowledge) and knowledge graphs, to refine the selection of negative samples. It operates within a broad spectrum of recommendation scenarios, ranging from context-aware recommendation to graph-enhanced recommendation, where knowledge plays a crucial role. The core of Knowledge-enhanced Negative Sampling unfolds in two distinctive dimensions: General KNS and KG-based KNS.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">As systematically demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.T6" title="Table 6 ‣ 3.5. Knowledge-enhanced Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">6</span></a>, General KNS operates by exploiting the heterogeneous knowledge items. It outperforms the traditional negative sampling methods by utilizing the broader knowledge in multi-domain scenarios to sample the more informative negative samples with richer semantic meaning. It ensures that the selected negative samples are not only dissimilar to the positive samples but also incorporate meaningful semantic relationships with the positive samples, aligning more closely with the user’s preferences. However, it relies on high-quality knowledge data and additionally introduces data processing complexity. On the other hand, KG-based KNS leverages the structured information from the Knowledge Graph to enhance the quality of the sampled negative items. By incorporating the user-item interactions to the entities in KG, KG-based KNS is able to introduce the high-order structural coherence and the multi-hop node connectivity into negative samples, thereby precisely improving the semantic relevance rather than the feature relevance between positive and negative samples. However, it excessively depends on KG construction and faces challenges related to the computational complexity of modeling and maintaining the KG.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1. </span>General KNS</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.13">The rich auxiliary relationships of users and the informative knowledge of items typically imply user preferences and item semantics, which could help overcome the sparsity issue in RS. General KNS selects negative samples based on the existing latent knowledge patterns to better understand the semantics and relations between users and items <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib140" title="">2018</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib142" title="">2015</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>; Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>)</cite>. The learning process of the knowledge-based recommendation is grounded on contextual information. For instance, MMT-Net <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>)</cite> designs the context-bias <math alttext="s_{\mathbf{c}}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.1.m1.1"><semantics id="S3.SS5.SSS1.p1.1.m1.1a"><msub id="S3.SS5.SSS1.p1.1.m1.1.1" xref="S3.SS5.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS5.SSS1.p1.1.m1.1.1.2" xref="S3.SS5.SSS1.p1.1.m1.1.1.2.cmml">s</mi><mi id="S3.SS5.SSS1.p1.1.m1.1.1.3" xref="S3.SS5.SSS1.p1.1.m1.1.1.3.cmml">𝐜</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.1.m1.1b"><apply id="S3.SS5.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS5.SSS1.p1.1.m1.1.1.2">𝑠</ci><ci id="S3.SS5.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS5.SSS1.p1.1.m1.1.1.3">𝐜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.1.m1.1c">s_{\mathbf{c}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.1.m1.1d">italic_s start_POSTSUBSCRIPT bold_c end_POSTSUBSCRIPT</annotation></semantics></math> to reveal the novel tuples which display uncommon combinations of the interaction and its corresponding context, which can be defined as <math alttext="s_{\mathbf{c}}=\mathbf{w}_{\mathrm{C}}\cdot\mathbf{c}^{n_{\mathrm{C}}}+b_{%
\mathrm{C}}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.2.m2.1"><semantics id="S3.SS5.SSS1.p1.2.m2.1a"><mrow id="S3.SS5.SSS1.p1.2.m2.1.1" xref="S3.SS5.SSS1.p1.2.m2.1.1.cmml"><msub id="S3.SS5.SSS1.p1.2.m2.1.1.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS5.SSS1.p1.2.m2.1.1.2.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.2.2.cmml">s</mi><mi id="S3.SS5.SSS1.p1.2.m2.1.1.2.3" xref="S3.SS5.SSS1.p1.2.m2.1.1.2.3.cmml">𝐜</mi></msub><mo id="S3.SS5.SSS1.p1.2.m2.1.1.1" xref="S3.SS5.SSS1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS5.SSS1.p1.2.m2.1.1.3" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.cmml"><mrow id="S3.SS5.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.cmml"><msub id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.cmml"><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.2.cmml">𝐰</mi><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.3" mathvariant="normal" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.3.cmml">C</mi></msub><mo id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.1.cmml">⋅</mo><msup id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.cmml"><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.2.cmml">𝐜</mi><msub id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.cmml"><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.2.cmml">n</mi><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.3" mathvariant="normal" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.3.cmml">C</mi></msub></msup></mrow><mo id="S3.SS5.SSS1.p1.2.m2.1.1.3.1" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.SS5.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.2" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3.2.cmml">b</mi><mi id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.3" mathvariant="normal" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3.3.cmml">C</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.2.m2.1b"><apply id="S3.SS5.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1"><eq id="S3.SS5.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.1"></eq><apply id="S3.SS5.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS5.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.2.2">𝑠</ci><ci id="S3.SS5.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.2.3">𝐜</ci></apply><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3"><plus id="S3.SS5.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.1"></plus><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2"><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.1">⋅</ci><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.2">𝐰</ci><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.2.3">C</ci></apply><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3">superscript</csymbol><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.2">𝐜</ci><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3">subscript</csymbol><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.2">𝑛</ci><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.2.3.3.3">C</ci></apply></apply></apply><apply id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3.2">𝑏</ci><ci id="S3.SS5.SSS1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS5.SSS1.p1.2.m2.1.1.3.3.3">C</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.2.m2.1c">s_{\mathbf{c}}=\mathbf{w}_{\mathrm{C}}\cdot\mathbf{c}^{n_{\mathrm{C}}}+b_{%
\mathrm{C}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.2.m2.1d">italic_s start_POSTSUBSCRIPT bold_c end_POSTSUBSCRIPT = bold_w start_POSTSUBSCRIPT roman_C end_POSTSUBSCRIPT ⋅ bold_c start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT roman_C end_POSTSUBSCRIPT end_POSTSUPERSCRIPT + italic_b start_POSTSUBSCRIPT roman_C end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\mathbf{c}^{n_{\mathrm{C}}}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.3.m3.1"><semantics id="S3.SS5.SSS1.p1.3.m3.1a"><msup id="S3.SS5.SSS1.p1.3.m3.1.1" xref="S3.SS5.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS5.SSS1.p1.3.m3.1.1.2" xref="S3.SS5.SSS1.p1.3.m3.1.1.2.cmml">𝐜</mi><msub id="S3.SS5.SSS1.p1.3.m3.1.1.3" xref="S3.SS5.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS5.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS5.SSS1.p1.3.m3.1.1.3.2.cmml">n</mi><mi id="S3.SS5.SSS1.p1.3.m3.1.1.3.3" mathvariant="normal" xref="S3.SS5.SSS1.p1.3.m3.1.1.3.3.cmml">C</mi></msub></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.3.m3.1b"><apply id="S3.SS5.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS5.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1.2">𝐜</ci><apply id="S3.SS5.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS5.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1.3.2">𝑛</ci><ci id="S3.SS5.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS5.SSS1.p1.3.m3.1.1.3.3">C</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.3.m3.1c">\mathbf{c}^{n_{\mathrm{C}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.3.m3.1d">bold_c start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT roman_C end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the final context combinations. To enhance the training effectiveness of the proposed context-bias, MMT-Net develops a negative sampling approximation with two learning objectives to randomly identify the likely item given the user and interaction context and identify the likely context given the user and the item, respectively. Then the context-bias <math alttext="s_{\mathbf{c}}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.4.m4.1"><semantics id="S3.SS5.SSS1.p1.4.m4.1a"><msub id="S3.SS5.SSS1.p1.4.m4.1.1" xref="S3.SS5.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS5.SSS1.p1.4.m4.1.1.2" xref="S3.SS5.SSS1.p1.4.m4.1.1.2.cmml">s</mi><mi id="S3.SS5.SSS1.p1.4.m4.1.1.3" xref="S3.SS5.SSS1.p1.4.m4.1.1.3.cmml">𝐜</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.4.m4.1b"><apply id="S3.SS5.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS5.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS5.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS5.SSS1.p1.4.m4.1.1.2">𝑠</ci><ci id="S3.SS5.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS5.SSS1.p1.4.m4.1.1.3">𝐜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.4.m4.1c">s_{\mathbf{c}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.4.m4.1d">italic_s start_POSTSUBSCRIPT bold_c end_POSTSUBSCRIPT</annotation></semantics></math> has been used to attenuate the loss <math alttext="\mathcal{L}_{(u,\mathbf{c},v)}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.5.m5.3"><semantics id="S3.SS5.SSS1.p1.5.m5.3a"><msub id="S3.SS5.SSS1.p1.5.m5.3.4" xref="S3.SS5.SSS1.p1.5.m5.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS1.p1.5.m5.3.4.2" xref="S3.SS5.SSS1.p1.5.m5.3.4.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS1.p1.5.m5.3.3.3.5" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml"><mo id="S3.SS5.SSS1.p1.5.m5.3.3.3.5.1" stretchy="false" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml">(</mo><mi id="S3.SS5.SSS1.p1.5.m5.1.1.1.1" xref="S3.SS5.SSS1.p1.5.m5.1.1.1.1.cmml">u</mi><mo id="S3.SS5.SSS1.p1.5.m5.3.3.3.5.2" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.5.m5.2.2.2.2" xref="S3.SS5.SSS1.p1.5.m5.2.2.2.2.cmml">𝐜</mi><mo id="S3.SS5.SSS1.p1.5.m5.3.3.3.5.3" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.5.m5.3.3.3.3" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.3.cmml">v</mi><mo id="S3.SS5.SSS1.p1.5.m5.3.3.3.5.4" stretchy="false" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.5.m5.3b"><apply id="S3.SS5.SSS1.p1.5.m5.3.4.cmml" xref="S3.SS5.SSS1.p1.5.m5.3.4"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.5.m5.3.4.1.cmml" xref="S3.SS5.SSS1.p1.5.m5.3.4">subscript</csymbol><ci id="S3.SS5.SSS1.p1.5.m5.3.4.2.cmml" xref="S3.SS5.SSS1.p1.5.m5.3.4.2">ℒ</ci><vector id="S3.SS5.SSS1.p1.5.m5.3.3.3.4.cmml" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.5"><ci id="S3.SS5.SSS1.p1.5.m5.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.5.m5.1.1.1.1">𝑢</ci><ci id="S3.SS5.SSS1.p1.5.m5.2.2.2.2.cmml" xref="S3.SS5.SSS1.p1.5.m5.2.2.2.2">𝐜</ci><ci id="S3.SS5.SSS1.p1.5.m5.3.3.3.3.cmml" xref="S3.SS5.SSS1.p1.5.m5.3.3.3.3">𝑣</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.5.m5.3c">\mathcal{L}_{(u,\mathbf{c},v)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.5.m5.3d">caligraphic_L start_POSTSUBSCRIPT ( italic_u , bold_c , italic_v ) end_POSTSUBSCRIPT</annotation></semantics></math> for the tuple <math alttext="(u,\mathbf{c},v)" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.6.m6.3"><semantics id="S3.SS5.SSS1.p1.6.m6.3a"><mrow id="S3.SS5.SSS1.p1.6.m6.3.4.2" xref="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml"><mo id="S3.SS5.SSS1.p1.6.m6.3.4.2.1" stretchy="false" xref="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml">(</mo><mi id="S3.SS5.SSS1.p1.6.m6.1.1" xref="S3.SS5.SSS1.p1.6.m6.1.1.cmml">u</mi><mo id="S3.SS5.SSS1.p1.6.m6.3.4.2.2" xref="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml">,</mo><mi id="S3.SS5.SSS1.p1.6.m6.2.2" xref="S3.SS5.SSS1.p1.6.m6.2.2.cmml">𝐜</mi><mo id="S3.SS5.SSS1.p1.6.m6.3.4.2.3" xref="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml">,</mo><mi id="S3.SS5.SSS1.p1.6.m6.3.3" xref="S3.SS5.SSS1.p1.6.m6.3.3.cmml">v</mi><mo id="S3.SS5.SSS1.p1.6.m6.3.4.2.4" stretchy="false" xref="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.6.m6.3b"><vector id="S3.SS5.SSS1.p1.6.m6.3.4.1.cmml" xref="S3.SS5.SSS1.p1.6.m6.3.4.2"><ci id="S3.SS5.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS5.SSS1.p1.6.m6.1.1">𝑢</ci><ci id="S3.SS5.SSS1.p1.6.m6.2.2.cmml" xref="S3.SS5.SSS1.p1.6.m6.2.2">𝐜</ci><ci id="S3.SS5.SSS1.p1.6.m6.3.3.cmml" xref="S3.SS5.SSS1.p1.6.m6.3.3">𝑣</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.6.m6.3c">(u,\mathbf{c},v)</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.6.m6.3d">( italic_u , bold_c , italic_v )</annotation></semantics></math>. Thus, the attenuated loss <math alttext="\mathcal{L}_{(u,\mathbf{c},v)}^{{}^{\prime}}=\mathcal{L}_{(u,\mathbf{c},v)}-s_%
{\mathbf{c}}" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.7.m7.6"><semantics id="S3.SS5.SSS1.p1.7.m7.6a"><mrow id="S3.SS5.SSS1.p1.7.m7.6.7" xref="S3.SS5.SSS1.p1.7.m7.6.7.cmml"><msubsup id="S3.SS5.SSS1.p1.7.m7.6.7.2" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS1.p1.7.m7.6.7.2.2.2" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.2.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS1.p1.7.m7.3.3.3.5" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml"><mo id="S3.SS5.SSS1.p1.7.m7.3.3.3.5.1" stretchy="false" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml">(</mo><mi id="S3.SS5.SSS1.p1.7.m7.1.1.1.1" xref="S3.SS5.SSS1.p1.7.m7.1.1.1.1.cmml">u</mi><mo id="S3.SS5.SSS1.p1.7.m7.3.3.3.5.2" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.7.m7.2.2.2.2" xref="S3.SS5.SSS1.p1.7.m7.2.2.2.2.cmml">𝐜</mi><mo id="S3.SS5.SSS1.p1.7.m7.3.3.3.5.3" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.7.m7.3.3.3.3" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.3.cmml">v</mi><mo id="S3.SS5.SSS1.p1.7.m7.3.3.3.5.4" stretchy="false" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml">)</mo></mrow><msup id="S3.SS5.SSS1.p1.7.m7.6.7.2.3" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.3.cmml"><mi id="S3.SS5.SSS1.p1.7.m7.6.7.2.3a" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.3.cmml"></mi><mo id="S3.SS5.SSS1.p1.7.m7.6.7.2.3.1" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.3.1.cmml">′</mo></msup></msubsup><mo id="S3.SS5.SSS1.p1.7.m7.6.7.1" xref="S3.SS5.SSS1.p1.7.m7.6.7.1.cmml">=</mo><mrow id="S3.SS5.SSS1.p1.7.m7.6.7.3" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.cmml"><msub id="S3.SS5.SSS1.p1.7.m7.6.7.3.2" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.SSS1.p1.7.m7.6.7.3.2.2" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.2.2.cmml">ℒ</mi><mrow id="S3.SS5.SSS1.p1.7.m7.6.6.3.5" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml"><mo id="S3.SS5.SSS1.p1.7.m7.6.6.3.5.1" stretchy="false" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml">(</mo><mi id="S3.SS5.SSS1.p1.7.m7.4.4.1.1" xref="S3.SS5.SSS1.p1.7.m7.4.4.1.1.cmml">u</mi><mo id="S3.SS5.SSS1.p1.7.m7.6.6.3.5.2" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.7.m7.5.5.2.2" xref="S3.SS5.SSS1.p1.7.m7.5.5.2.2.cmml">𝐜</mi><mo id="S3.SS5.SSS1.p1.7.m7.6.6.3.5.3" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml">,</mo><mi id="S3.SS5.SSS1.p1.7.m7.6.6.3.3" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.3.cmml">v</mi><mo id="S3.SS5.SSS1.p1.7.m7.6.6.3.5.4" stretchy="false" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml">)</mo></mrow></msub><mo id="S3.SS5.SSS1.p1.7.m7.6.7.3.1" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.1.cmml">−</mo><msub id="S3.SS5.SSS1.p1.7.m7.6.7.3.3" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3.cmml"><mi id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.2" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3.2.cmml">s</mi><mi id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.3" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3.3.cmml">𝐜</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.7.m7.6b"><apply id="S3.SS5.SSS1.p1.7.m7.6.7.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7"><eq id="S3.SS5.SSS1.p1.7.m7.6.7.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.1"></eq><apply id="S3.SS5.SSS1.p1.7.m7.6.7.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.7.m7.6.7.2.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2">superscript</csymbol><apply id="S3.SS5.SSS1.p1.7.m7.6.7.2.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.7.m7.6.7.2.2.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2">subscript</csymbol><ci id="S3.SS5.SSS1.p1.7.m7.6.7.2.2.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.2.2">ℒ</ci><vector id="S3.SS5.SSS1.p1.7.m7.3.3.3.4.cmml" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.5"><ci id="S3.SS5.SSS1.p1.7.m7.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.1.1.1.1">𝑢</ci><ci id="S3.SS5.SSS1.p1.7.m7.2.2.2.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.2.2.2.2">𝐜</ci><ci id="S3.SS5.SSS1.p1.7.m7.3.3.3.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.3.3.3.3">𝑣</ci></vector></apply><apply id="S3.SS5.SSS1.p1.7.m7.6.7.2.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.3"><ci id="S3.SS5.SSS1.p1.7.m7.6.7.2.3.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.2.3.1">′</ci></apply></apply><apply id="S3.SS5.SSS1.p1.7.m7.6.7.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3"><minus id="S3.SS5.SSS1.p1.7.m7.6.7.3.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.1"></minus><apply id="S3.SS5.SSS1.p1.7.m7.6.7.3.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.2"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.7.m7.6.7.3.2.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.2">subscript</csymbol><ci id="S3.SS5.SSS1.p1.7.m7.6.7.3.2.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.2.2">ℒ</ci><vector id="S3.SS5.SSS1.p1.7.m7.6.6.3.4.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.5"><ci id="S3.SS5.SSS1.p1.7.m7.4.4.1.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.4.4.1.1">𝑢</ci><ci id="S3.SS5.SSS1.p1.7.m7.5.5.2.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.5.5.2.2">𝐜</ci><ci id="S3.SS5.SSS1.p1.7.m7.6.6.3.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.6.3.3">𝑣</ci></vector></apply><apply id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3"><csymbol cd="ambiguous" id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.1.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3">subscript</csymbol><ci id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.2.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3.2">𝑠</ci><ci id="S3.SS5.SSS1.p1.7.m7.6.7.3.3.3.cmml" xref="S3.SS5.SSS1.p1.7.m7.6.7.3.3.3">𝐜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.7.m7.6c">\mathcal{L}_{(u,\mathbf{c},v)}^{{}^{\prime}}=\mathcal{L}_{(u,\mathbf{c},v)}-s_%
{\mathbf{c}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.7.m7.6d">caligraphic_L start_POSTSUBSCRIPT ( italic_u , bold_c , italic_v ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT = caligraphic_L start_POSTSUBSCRIPT ( italic_u , bold_c , italic_v ) end_POSTSUBSCRIPT - italic_s start_POSTSUBSCRIPT bold_c end_POSTSUBSCRIPT</annotation></semantics></math> can be used to determine the embedding space and induce a novelty-weighted training curriculum focused on harder samples as training proceeds. SA_OCCF <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib142" title="">2015</a>)</cite> leverages sentiment information extracted from user reviews in the dataset to assign the weight for each negative example rather than using a global weighting scheme in wAMAN <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib124" title="">2008</a>)</cite>. Instead of the review information, some works also leverage the informative genre and category knowledge for negative samplings. To list a few, LDA <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib140" title="">2018</a>)</cite> devises a two-tiered sampling approach that first samples a genre <math alttext="g" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.8.m8.1"><semantics id="S3.SS5.SSS1.p1.8.m8.1a"><mi id="S3.SS5.SSS1.p1.8.m8.1.1" xref="S3.SS5.SSS1.p1.8.m8.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.8.m8.1b"><ci id="S3.SS5.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS5.SSS1.p1.8.m8.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.8.m8.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.8.m8.1d">italic_g</annotation></semantics></math> from the learned genre-distribution <math alttext="p(g|u)" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.9.m9.1"><semantics id="S3.SS5.SSS1.p1.9.m9.1a"><mrow id="S3.SS5.SSS1.p1.9.m9.1.1" xref="S3.SS5.SSS1.p1.9.m9.1.1.cmml"><mi id="S3.SS5.SSS1.p1.9.m9.1.1.3" xref="S3.SS5.SSS1.p1.9.m9.1.1.3.cmml">p</mi><mo id="S3.SS5.SSS1.p1.9.m9.1.1.2" xref="S3.SS5.SSS1.p1.9.m9.1.1.2.cmml">⁢</mo><mrow id="S3.SS5.SSS1.p1.9.m9.1.1.1.1" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.cmml"><mo id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.2" stretchy="false" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.2" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.2.cmml">g</mi><mo fence="false" id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.1" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.3" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.3.cmml">u</mi></mrow><mo id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.3" stretchy="false" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.9.m9.1b"><apply id="S3.SS5.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1"><times id="S3.SS5.SSS1.p1.9.m9.1.1.2.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.2"></times><ci id="S3.SS5.SSS1.p1.9.m9.1.1.3.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.3">𝑝</ci><apply id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1"><csymbol cd="latexml" id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.2">𝑔</ci><ci id="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS1.p1.9.m9.1.1.1.1.1.3">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.9.m9.1c">p(g|u)</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.9.m9.1d">italic_p ( italic_g | italic_u )</annotation></semantics></math> for user <math alttext="u" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.10.m10.1"><semantics id="S3.SS5.SSS1.p1.10.m10.1a"><mi id="S3.SS5.SSS1.p1.10.m10.1.1" xref="S3.SS5.SSS1.p1.10.m10.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.10.m10.1b"><ci id="S3.SS5.SSS1.p1.10.m10.1.1.cmml" xref="S3.SS5.SSS1.p1.10.m10.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.10.m10.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.10.m10.1d">italic_u</annotation></semantics></math>, and then samples the movie <math alttext="i" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.11.m11.1"><semantics id="S3.SS5.SSS1.p1.11.m11.1a"><mi id="S3.SS5.SSS1.p1.11.m11.1.1" xref="S3.SS5.SSS1.p1.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.11.m11.1b"><ci id="S3.SS5.SSS1.p1.11.m11.1.1.cmml" xref="S3.SS5.SSS1.p1.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.11.m11.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.11.m11.1d">italic_i</annotation></semantics></math> as the negative sample from the learned distribution <math alttext="p(i|g)" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.12.m12.1"><semantics id="S3.SS5.SSS1.p1.12.m12.1a"><mrow id="S3.SS5.SSS1.p1.12.m12.1.1" xref="S3.SS5.SSS1.p1.12.m12.1.1.cmml"><mi id="S3.SS5.SSS1.p1.12.m12.1.1.3" xref="S3.SS5.SSS1.p1.12.m12.1.1.3.cmml">p</mi><mo id="S3.SS5.SSS1.p1.12.m12.1.1.2" xref="S3.SS5.SSS1.p1.12.m12.1.1.2.cmml">⁢</mo><mrow id="S3.SS5.SSS1.p1.12.m12.1.1.1.1" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.cmml"><mo id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.2" stretchy="false" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.cmml"><mi id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.2" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.2.cmml">i</mi><mo fence="false" id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.1" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.3" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.3.cmml">g</mi></mrow><mo id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.3" stretchy="false" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.12.m12.1b"><apply id="S3.SS5.SSS1.p1.12.m12.1.1.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1"><times id="S3.SS5.SSS1.p1.12.m12.1.1.2.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.2"></times><ci id="S3.SS5.SSS1.p1.12.m12.1.1.3.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.3">𝑝</ci><apply id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1"><csymbol cd="latexml" id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.2">𝑖</ci><ci id="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS1.p1.12.m12.1.1.1.1.1.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.12.m12.1c">p(i|g)</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.12.m12.1d">italic_p ( italic_i | italic_g )</annotation></semantics></math> regarding the genre <math alttext="g" class="ltx_Math" display="inline" id="S3.SS5.SSS1.p1.13.m13.1"><semantics id="S3.SS5.SSS1.p1.13.m13.1a"><mi id="S3.SS5.SSS1.p1.13.m13.1.1" xref="S3.SS5.SSS1.p1.13.m13.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.p1.13.m13.1b"><ci id="S3.SS5.SSS1.p1.13.m13.1.1.cmml" xref="S3.SS5.SSS1.p1.13.m13.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.p1.13.m13.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS1.p1.13.m13.1d">italic_g</annotation></semantics></math>. IntentGC <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>)</cite> samples negative items in the same leaf category as the corresponding positive item to make the model capable of distinguishing hard items. Notably, if the sampled item exists in the interaction sequence, IntentGC discards it and re-samples a new negative item.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2. </span>KG-based KNS</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.3">KG-based KNS leverages the explicit association and high-order correlation among users, items, and other entities in KG to sample negative instances <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>; Ali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib2" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib160" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib21" title="">2022a</a>)</cite>. KG, which introduce extra relations among items and real-world entities (e.g., item attributes) with the comprehensive structural knowledge, are well known for its potential to infer informative negative samples and enhance recommenders’ accuracy and explainability.
To precisely improve the semantic relevance between positive and negative samples and enhance the accessible information of the sampled negative instances, a series of works attempt to introduce the high-order structural coherence and the multi-hop node connectivity into negative samples in RS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>; Ali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib2" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib160" title="">2020</a>)</cite>. For example, AnchorKG <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>)</cite> design a simple but effective strategy to randomly select the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS5.SSS2.p1.1.m1.1"><semantics id="S3.SS5.SSS2.p1.1.m1.1a"><mi id="S3.SS5.SSS2.p1.1.m1.1.1" xref="S3.SS5.SSS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p1.1.m1.1b"><ci id="S3.SS5.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS2.p1.1.m1.1d">italic_k</annotation></semantics></math> in <math alttext="\{1,2,3\}" class="ltx_Math" display="inline" id="S3.SS5.SSS2.p1.2.m2.3"><semantics id="S3.SS5.SSS2.p1.2.m2.3a"><mrow id="S3.SS5.SSS2.p1.2.m2.3.4.2" xref="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml"><mo id="S3.SS5.SSS2.p1.2.m2.3.4.2.1" stretchy="false" xref="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml">{</mo><mn id="S3.SS5.SSS2.p1.2.m2.1.1" xref="S3.SS5.SSS2.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS5.SSS2.p1.2.m2.3.4.2.2" xref="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S3.SS5.SSS2.p1.2.m2.2.2" xref="S3.SS5.SSS2.p1.2.m2.2.2.cmml">2</mn><mo id="S3.SS5.SSS2.p1.2.m2.3.4.2.3" xref="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S3.SS5.SSS2.p1.2.m2.3.3" xref="S3.SS5.SSS2.p1.2.m2.3.3.cmml">3</mn><mo id="S3.SS5.SSS2.p1.2.m2.3.4.2.4" stretchy="false" xref="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p1.2.m2.3b"><set id="S3.SS5.SSS2.p1.2.m2.3.4.1.cmml" xref="S3.SS5.SSS2.p1.2.m2.3.4.2"><cn id="S3.SS5.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS5.SSS2.p1.2.m2.1.1">1</cn><cn id="S3.SS5.SSS2.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS5.SSS2.p1.2.m2.2.2">2</cn><cn id="S3.SS5.SSS2.p1.2.m2.3.3.cmml" type="integer" xref="S3.SS5.SSS2.p1.2.m2.3.3">3</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS2.p1.2.m2.3c">\{1,2,3\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS2.p1.2.m2.3d">{ 1 , 2 , 3 }</annotation></semantics></math> and conduct a random walk along a <math alttext="k" class="ltx_Math" display="inline" id="S3.SS5.SSS2.p1.3.m3.1"><semantics id="S3.SS5.SSS2.p1.3.m3.1a"><mi id="S3.SS5.SSS2.p1.3.m3.1.1" xref="S3.SS5.SSS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p1.3.m3.1b"><ci id="S3.SS5.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS5.SSS2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS2.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS2.p1.3.m3.1d">italic_k</annotation></semantics></math>-hop path starting from the positive sample in KG. To minimize computational complexity in the model optimization, PR-HNE <cite class="ltx_cite ltx_citemacro_citep">(Ali et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib2" title="">2020</a>)</cite> employs unigram distribution to sample items (nodes) in candidates that appear in the KG independently of the presence of other items (nodes) over each interaction (edge). With the reinforcement learning (RL) paradigm, KGPolicy <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib160" title="">2020</a>)</cite> samples knowledge-enhanced negative items from the relationships between items and entities in KG, emphasizing KG entities and items linked within two-hop paths and facilitating the effective learning procedure of the recommender. KGAttack <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib21" title="">2022a</a>)</cite> first proposes knowledge-enhanced candidate selection to employ KG for localizing some relevant item candidates with similar attributes. Subsequently, it leverages RL paradigm to calculate the sampling probabilities for each item in the candidates by calculating the similarity state between the current state representations and graph embeddings.</p>
</div>
<figure class="ltx_table" id="S3.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7. </span>Illustration of five representative recommendation algorithms with their corresponding negative sampling strategies in Collaborative Filtering, Graph-based Recommendation, Sequential Recommendation, Multi-modal Recommendation, Multi-behavior Recommendation and Cross-domain Recommendation.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T7.1" style="width:433.6pt;height:366.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.9pt,95.6pt) scale(0.657524628841298,0.657524628841298) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T7.1.1">
<tr class="ltx_tr" id="S3.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T7.1.1.1.1.1">Recommendation Tasks</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T7.1.1.1.2.1">Algorithms</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T7.1.1.1.3.1">Negative Sampling Strategies</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T7.1.1.1.4.1">Venue</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T7.1.1.1.5.1">Year</span></td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.2.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.2.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.2.1.1.1.1.1">Collaborative</span></span>
<span class="ltx_tr" id="S3.T7.1.1.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.2.1.1.1.2.1">Filtering</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.2.2">NCF <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib71" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.2.3">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.2.4">WWW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.2.5">2017</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.3.1">ENMF <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib14" title="">2020c</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.3.2">Non-sampling SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.3.3">TOIS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.3.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.4.1">AHNS <cite class="ltx_cite ltx_citemacro_citep">(Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib96" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.4.2">Universal DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.4.3">AAAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.4.4">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.5.1">TIL <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib169" title="">2022a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.5.2">Attention-based IRW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.5.3">CIKM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.5.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.6.1">DNS* <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.6.2">Universal DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.6.3">WWW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.6.4">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.7.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.7.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.7.1.1.1.1.1">Graph-based</span></span>
<span class="ltx_tr" id="S3.T7.1.1.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.7.1.1.1.2.1">Recommendation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.7.2">DRCGR <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib51" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.7.3">Generative ANG</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.7.4">ICDM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.7.5">2019</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.8.1">IntentGC <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.8.2">General KNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.8.3">KDD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.8.4">2019</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.9.1">JNSKR <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.9.2">Non-sampling SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.9.3">SIGIR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.9.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.10.1">AnchorKG <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib108" title="">2021b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.10.2">KG-based KNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.10.3">KDD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.10.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.11.1">SGCF <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib70" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.11.2">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.11.3">WSDM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.11.4">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.12.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.12.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.12.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.12.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.12.1.1.1.1.1">Sequential</span></span>
<span class="ltx_tr" id="S3.T7.1.1.12.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.12.1.1.1.2.1">Recommendation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.12.2">DEEMS <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.12.3">User-similarity DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.12.4">KDD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.12.5">2019</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.13.1">GeoSAN <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.13.2">Knowledge-aware DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.13.3">KDD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.13.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.14.1">ELECRec <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.14.2">Generative ANG</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.14.3">SIGIR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.14.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.15.1">PDRec <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib116" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.15.2">Universal DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.15.3">AAAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.15.4">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.16.1">ContraRec <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib149" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.16.2">Attention-based IRW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.16.3">TOIS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.16.4">2024</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.17">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.17.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.17.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.17.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.17.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.17.1.1.1.1.1">Multi-modal</span></span>
<span class="ltx_tr" id="S3.T7.1.1.17.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.17.1.1.1.2.1">Recommendation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.17.2">RPRM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib159" title="">2021d</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.17.3">Knowledge-aware DNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.17.4">WWW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.17.5">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.18.1">KGPL <cite class="ltx_cite ltx_citemacro_citep">(Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.18.2">Popularity-based SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.18.3">WSDM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.18.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.19.1">POWERec <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib42" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.19.2">General KNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.19.3">Information Fusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.19.4">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.20.1">FairNeg <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.20.2">Debiased IRW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.20.3">WWW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.20.4">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.21">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.21.1">FREEDOM <cite class="ltx_cite ltx_citemacro_citep">(Zhou and Shen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib215" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.21.2">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.21.3">MM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.21.4">2023</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.22">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.22.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.22.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.22.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.22.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.22.1.1.1.1.1">Multi-behavior</span></span>
<span class="ltx_tr" id="S3.T7.1.1.22.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.22.1.1.1.2.1">Recommendation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.22.2">DRN <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib210" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.22.3">Predefined SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.22.4">WWW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.22.5">2018</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.23">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.23.1">ATRank <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib213" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.23.2">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.23.3">AAAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.23.4">2018</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.24">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.24.1">KHGT <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib178" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.24.2">KG-based KNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.24.3">AAAI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.24.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.25">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.25.1">MBHT <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib193" title="">2022b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.25.2">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.25.3">KDD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.25.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.26">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.26.1">MBRec <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib177" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.26.2">General KNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.26.3">TNNLS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.26.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.27">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.27.1" rowspan="5"><span class="ltx_text" id="S3.T7.1.1.27.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T7.1.1.27.1.1.1">
<span class="ltx_tr" id="S3.T7.1.1.27.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.27.1.1.1.1.1">Cross-domain</span></span>
<span class="ltx_tr" id="S3.T7.1.1.27.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T7.1.1.27.1.1.1.2.1">Recommendation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.27.2">MMT-Net <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.27.3">Knowledge-based IRW</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.27.4">SIGIR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.27.5">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.28">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.28.1">ML-MGC <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib143" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.28.2">Predefined SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.28.3">IJCNN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.28.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.29">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.29.1">DDGHM <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib211" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.29.2">Scenario-aware NACL</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.29.3">MM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.29.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.30">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.30.1">C2DSR <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib10" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.30.2">Uniform SNS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.30.3">CIKM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.30.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.1.31">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.1.31.1">RealHNS <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.1.31.2">Universal DNS</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.1.31.3">Recsys</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.1.31.4">2023</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Negative Sampling in Multiple Practical Recommendation Scenarios</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The field of recommendation systems is replete with a vast spectrum of diverse recommendation scenarios. This diversity stems from user demands, historical behaviors, and heterogeneous applications, thereby incorporating the field with a blend of challenges and prospects <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib50" title="">2022b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib20" title="">2023a</a>)</cite>. Benefits from its superior performance and easy-to-employ character, negative sampling methods have garnered significant attention from researchers and practitioners in recommendation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib39" title="">2019</a>; Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib104" title="">2020a</a>)</cite>. Some researchers have attempted to design a universal approach to constructing a negative sampling method that is suitable for all recommendation scenarios <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>. Nonetheless, the employment, objective, and data availability significantly vary across diverse recommendation scenarios, leading to the absence of a negative sampling algorithm that can comprehensively encompass all recommendation scenarios. In this section, we undertake an in-depth exploration of the distinctiveness of various recommendation scenarios, delineating subtle disparities in the negative sampling methodologies under the same recommendation scenarios. Ultimately, we conclude with a comprehensive discussion and outlook concerning the paradigm of tailored negative sampling techniques for each specific scenario. Specifically, the subsequent discussions span five pivotal recommendation scenarios in the following subsections: Collaborative-guided Recommendation, Sequential Recommendation, Multi-modal Recommendation, Multi-behavior Recommendation, Cross-domain Recommendation and CL-enhanced Recommendation. These discrete discussions and analyses illustrate the adaptability and customization inherent to negative sampling strategies, a fundamental prerequisite for optimizing recommendation algorithms in diverse recommendation contexts.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Introduction of Negative Feedback in Practical Recommendation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Both positive and negative feedback can reflect the user’s preferences on diverse items in the corpus, which are equally essential for modeling user preferences in RS <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib209" title="">2018</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib168" title="">2022b</a>)</cite>. Apart from explicit feedback and implicit feedback, the presence of real negative feedback is a notable factor in practical RS <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib179" title="">2021</a>)</cite>. Leveraging such data to its maximum capacity can provide the recommender with a significant information gain. Constrained by the exposure bias and popularity bias inherent in real-world RS, the interaction matrix reveals a notable sparsity, which constitutes a computationally insurmountable problem in RS <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib82" title="">2023</a>)</cite>. As mentioned in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S2.SS1" title="2.1. Role of Negative Sampling in Recommendation ‣ 2. Necessity and Challenges of Negative sampling ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">2.1</span></a>, negative sampling emerge as a critical and irreplaceable element in recommendation. How to effectively discover users’ real negative interests in negative sampling stands as a central research objective <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib11" title="">2023c</a>)</cite>, the detailed expressions of the associated concepts and methodologies are described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3" title="3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>. Within this section, our principal endeavor is to introduce the two types of feedback mentioned above  <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Due to the scarcity of real user behaviors, we discuss their definition and application in Sec .<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS5" title="4.5. Multi-behavior Recommendation ‣ 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">4.5</span></a>.</span></span></span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Explicit Feedback:</span> Explicit Feedback materializes in the form of precise ratings to provides unequivocal insights into user preferences (e.g., from 1.0 to 5.0). Actually, this may not necessarily reflect the users’ real preferences. Even for the item explicitly rated as 1.0, they have been chosen by this user from an extensive candidate pool. Therefore, such negative feedback reflects user preferences to a certain extent, that is, they may not be as too ”negative”.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Implicit Feedback:</span> Implicit Feedback encompasses data derived from user behaviors without any ratings and reviews (e.g., residence time and whether to choose dislike). It lacks explicitness and necessitates the model-based inferences to elucidate users’ preferences, which may introduce long-tailed information and exposure bias in RS.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="122" id="S4.F4.g1" src="x4.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>An example of the real user behaviors, where <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.7.g1" src="x5.png" width="10"/> denotes “like”, <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.8.g2" src="x6.png" width="10"/> denotes “dislike”, <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.9.g3" src="x7.png" width="10"/> denotes “add to cart”, <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.10.g4" src="x8.png" width="10"/> denotes “purchase”, <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.11.g5" src="x9.png" width="10"/> denotes “observed real behaviors” and <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="10" id="S4.F4.12.g6" src="x10.png" width="10"/> denotes “unobserved samples”.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">From the above two types of feedback, implicit feedback in the observations is fundamentally different from explicit feedback, which is captured in a structured manner <cite class="ltx_cite ltx_citemacro_citep">(Saito et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib134" title="">2020</a>)</cite>. Precisely, explicit feedback delivers direct insights into user preferences, while implicit feedback unravels concealed user interests. Apart from the implicit and explicit feedback, real-world RS typically involves the concept of real negative feedback, which hides in real feedback and contributes to a more holistic understanding of user preference <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib179" title="">2021</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib82" title="">2023</a>)</cite>. It refers to the authentic user negative behaviors, with its definition intimately linked to various recommendation scenarios. For instance, non-purchases are considered negative samples in the context of purchases, while in the context of clicks, exposures without clicks constitute negative samples. Similarly, non-likes, in comparison to likes, are treated as negative samples. This encapsulates explicit information of users’ disapproval and unclicks, contributing to a more comprehensive understanding of user interest <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib41" title="">2023</a>)</cite>. These authentic feedback wields significant influence in recommendation. With the specific recommendation scenario, negative sampling algorithms can be designed and optimized according to the distinct types of feedback data (See Sec.<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S4.SS5" title="4.5. Multi-behavior Recommendation ‣ 4. Negative Sampling in Multiple Practical Recommendation Scenarios ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S4.F5.g1" src="x11.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>An illustration of collaborative-guided recommendation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Collaborative-guided Recommendation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Collaborative-guided Recommendation (CR) aims to gather and analyze all the collaborative behaviors for predicting the potential preference of a target user <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib133" title="">2012</a>)</cite>. It typically focuses on projecting users and items into the latent embedding space to reflect users’ preferences <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib71" title="">2017</a>)</cite>.
In contrast to multi-modal recommendation and cross-domain recommendation, the available data of CR is exclusively restricted in the user-item interactions, which constrains its practical accuracy. Moreover, it is widely recognized that merely considering positive instances is insufficient to capture the intricacies of user preferences <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite>. In light of this, a series of CF methods have investigated how to maximize the utilization of interaction data for the purpose of sampling more informative negative instances in CF tasks <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib14" title="">2020c</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib169" title="">2022a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Constrained by the data availability inherent to CF tasks, prevailing negative sampling methods typically employ SNS approaches to facilitate the process of negative sample selection. Precisely, randomly sample items from the unobserved corpus with the same probability <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib70" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib71" title="">2017</a>)</cite> are extensively employed in CF. A substantial number of CF approaches also focus on the dynamic selection of negative samples, which are information-rich and contextually suitable in a current context during the training of CF models. DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite> and DNS<math alttext="*" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><times id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">*</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">∗</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite> regard the items with the highest score as the most informative samples and attempt to select them as the negative samples. SRNS <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>)</cite>, GDNS <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>)</cite>, and DENS <cite class="ltx_cite ltx_citemacro_citep">(Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib95" title="">2023</a>)</cite> leverage the inherent distribution (e.g., statistical information and disentangled representation) between positive and negative samples to dynamically select the suitable negative instances under the current context in training. Furthermore, a CL-based CF algorithm EGLN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib194" title="">2021d</a>)</cite> attempts to conduct the structural augmentation on the interaction graph to maximize the local-global consistency in collaborative representation learning.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S4.F6.g1" src="x12.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>An illustration of sequential recommendation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Sequential Recommendation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Sequential Recommendation (SR) is a straightforward and effective approach for capturing users’ dynamic preference patterns from their historical behaviors <cite class="ltx_cite ltx_citemacro_citep">(Dallmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib38" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib149" title="">2023a</a>)</cite>. Its goal is to predict the next items the user is likely to interested with by modeling the temporal correlations in user behavioral sequence.
Some studies have proven the effectiveness of hard negative sampling strategies in SR <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib181" title="">2022a</a>)</cite>. Through the selection of negative examples that exhibit a certain similarity to positive samples, these strategies bolster the recommender’s capability to finely distinguish users’ dynamic preferences in recommendation.
Most of the SR methods randomly select unobserved items from the corpus as the negative samples <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib193" title="">2022b</a>; Dallmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib38" title="">2021</a>; Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib127" title="">2022</a>)</cite>. With the development of contrastive learning, some studies rely on diverse sequence augmentation methods (e.g., crop, mask, reorder, substitute, and insert) to generate negative samples in CL tasks, assisting in modeling users’ temporal preferences. Despite the utilization of the hard negative sampling strategy, DEEMS <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>)</cite> and GeoSAN <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib105" title="">2020b</a>)</cite> have not considered the temporal relationships among behaviors, opting to treat each interaction with equitable importance. Similarly, ELECRec <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib31" title="">2022b</a>)</cite> attempts to leverage the discriminator in GAN to distinguish the target sample, overlooking the sequential knowledge within the behavioral sequence. Evidently, negative sampling holds particular significance for SR, which is a concern that existing research has momentarily disregarded and may serve as a promising avenue of exploration for future scholars.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S4.F7.g1" src="x13.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>An illustration of multi-modal recommendation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Multi-modal Recommendation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Multi-modal Recommendation (MMR) is crucial in real-world recommendation applications with massive modality-specific information, that is, micro-videos, images, audio, and text <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib110" title="">2019</a>; Tao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib144" title="">2022</a>)</cite>.
Since the multi-modal information characteristics may reflect users’ actual preferences within the fine-grained modality level <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>, the idealized multi-modal recommender possesses the capacity to uncover the hidden relationships between different modalities and recover the complementary information that can not be captured by the traditional CF methods <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib42" title="">2024</a>)</cite>. To achieve satisfactory performance, MMR methods typically require a substantial amount of both positive (present in the observed interactions) and negative items (obscured within the unobserved interactions) to train the recommenders in a supervised manner. This motivates researchers to explore how to sample more informative negative instances that better align with users’ actual preferences with the multi-modal knowledge.
Similar to other recommendation scenarios, most of related studies commonly apply SNS (uniform SNS <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib110" title="">2019</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib171" title="">2019a</a>)</cite> and predefined SNS <cite class="ltx_cite ltx_citemacro_citep">(Togashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib146" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib151" title="">2019b</a>)</cite>) to select negative samples in MMR. Some works also investigate the hidden correlations within the items’ multiple modalities to conduct the modalities-aware CL pairs. For the traditional negative sampling strategies, FairNeg <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite> achieves debiased recommendation via capturing the group-level unfairness and balancing the sampling probabilities of each semantic group. RPRM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib159" title="">2021d</a>)</cite> attempts to model the review properties’ consistency among different items and consequently sample the negatives with similar review properties of positive samples. POWERec <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib42" title="">2024</a>)</cite> leverages the strength of the multi-modal representations learned from the identical item to enhance user interest understanding in the unreliable modality, making all the modality-specific user interests to be fully learned. These negative sampling strategies can incorporate the multi-modal information to intelligently refine the selection of the relevant negative instances, which exhibits meaningful semantic relationships with the positive samples and closely aligns with the user’s intricate interests.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S4.F8.g1" src="x14.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>An illustration of multi-behavior recommendation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Multi-behavior Recommendation</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Benefits from the multiple types of interaction between users and items in real-world scenarios, multi-behavior recommendation (MBR) refers to taking full advantage of these heterogeneous user behaviors (e.g., skipped, clicked, and ordered) to alleviate the data sparsity or cold-start issues <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib179" title="">2021</a>; Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib177" title="">2022</a>; Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib60" title="">2021</a>)</cite>. Its core idea is to model the cross-type behavior dependency in real-world RS, which can provide complementary information for user’s interests encoding <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib60" title="">2021</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib213" title="">2018</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib187" title="">2021a</a>)</cite>. In such an application, effectively modeling the multi-type user behaviors can provide auxiliary knowledge and precise user preference to characterize the underlying semantics of the user’s actual interest representation <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib210" title="">2018</a>; Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib59" title="">2022</a>)</cite>. Existing MBR works primarily consider the multi-behavior dependencies with the predefined correlations <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib193" title="">2022b</a>; Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib177" title="">2022</a>)</cite>, while seldom embarking on the selection of diverse negative samples for individual behaviors through negative sampling. To some extent, this limits its capacity to capture the intricate cross-type behavior dependencies within complex real-world recommendation scenarios. For instance, ZEUS <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib60" title="">2021</a>)</cite> and HMG-CR <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib187" title="">2021a</a>)</cite> independently construct negative CL pairs by forming inter-connections between multiple behaviors and generating behavior-aware hyper meta-graphs. It enables the simultaneous modeling of the diverse interaction patterns and underlying cross-type behavior inter-dependencies, verifying the positive effects of incorporating the cross-type of multi-behavioral context in MBR.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S4.F9.g1" src="x15.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>An illustration of cross-domain recommendation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Cross-domain Recommendation</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Cross-domain recommendation (CDR) is one of the representative methods to alleviate the data sparsity problem and the cold-start problems in traditional RS with the auxiliary user behaviors collected from other domains <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib10" title="">2022</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib118" title="">2024b</a>)</cite>. It aims to leverage the correlation of users’ multi-domain behaviors to accurately transfer the informative knowledge from the source domains and model users’ personalized preference in the target domain <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib48" title="">2021a</a>; Bi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib7" title="">2020</a>)</cite>. Traditional CDR methods typically conduct the cross-domain knowledge transfer by modeling the feature-level cross-domain correlations with aligned constraint <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib112" title="">2020</a>)</cite>, adversarial training <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib109" title="">2021a</a>)</cite> and contrastive learning <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib211" title="">2022</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib118" title="">2024b</a>)</cite>. However, these methods merely focus on the randomly-selected negative samples from the target domain <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib93" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib112" title="">2020</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib118" title="">2024b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib109" title="">2021a</a>)</cite>, ignoring the cross-domain differences at the sample level. This may disregard users’ preference discrepancy between multiple domains and leads to the sub-optimal performance. DDGHM <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib211" title="">2022</a>)</cite> utilizes random mask operator for negative pair augmentation in CL optimization. Nevertheless, when it comes to recommendation tasks, the approach of random sampling within the target domain is still employed. RealHNS <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite> is the pioneer to systematically discover the false and refine the real from all HNS in CDR. It conducts the multi-grained HNS selectors to find high-quality general and cross-domain HNS with a curriculum learning framework to alleviate cross-domain negative transfer. Notably, there exists substantial uncharted territories for negative sampling methods in CDR, which awaits the investigation by future researchers. This encompasses various areas, including, but not limited to, KNS, ANG, and Debiased IRW.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7. </span>CL-enhanced Recommendation</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Inspired by the success of Contrastive Learning (CL) in Computer Vision <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib65" title="">2021</a>; Dai and Lin, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib36" title="">2017</a>)</cite>, Natural Language Processing <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib91" title="">2021b</a>; Hassani and Khasahmadi, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib69" title="">2020</a>)</cite> and Graph Representation Learning <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib122" title="">2022</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib52" title="">2021b</a>)</cite>, some works <cite class="ltx_cite ltx_citemacro_citep">(Shuai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib138" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib162" title="">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib214" title="">2020</a>)</cite> apply CL paradigm in RS. CL aims to maximize the agreement between positive pairs (an item’s representation and its corresponding augmented representations) and minimize it between negative pairs (an item’s representation and other items’ augmented representations) to acquire the transferable knowledge between the original view and the augmented view. Intuitively, CL relies heavily on its negative augmentation (NA) strategy, which is due to the fact that the appropriate augmented view can facilitate the recommender to exploit richer underlying semantic correlation. The most widely used augmentation strategy in CL-based recommendation is the in-batch negative sampling. It treats the augmented representations of the remaining items in the mini-batch where the positive example is located as negative examples, rather than selecting from the corpus, thereby aiding in accelerating its training process without the imposition of additional computational complexity.</p>
</div>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">To deeply understand the negative augmentation strategies in CL-based recommendation, we comprehensively dissect its core methodology into four distinct dimensions: pre-defined NA, graph-based NA, sequence-based NA and scenarios-aware NA.
Firstly, pre-defined NA fundamentally aims to refine the CL process by augmenting the negative sample pool based on the pre-defined criteria <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib43" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib203" title="">2021b</a>; Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib59" title="">2022</a>)</cite>, which are typically derived from domain-specific expertise or task-specific benchmarks. For instance, CauseRec <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib203" title="">2021b</a>)</cite> models the counterfactual data distribution by replacing the identified indispensable concepts in user behavioral sequence as the negative pairs.
Secondly, graph-based NA refers to generating the negative pairs based on distinct types of graph structures, including interaction graphs, social graphs, and heterogeneous graphs <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib202" title="">2021a</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib170" title="">2021a</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib194" title="">2021d</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib119" title="">2022</a>)</cite>. It typically perturbs the nodes, edges, and graph structures to provide a nuanced understanding of user-item interactions. SGL <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib170" title="">2021a</a>)</cite>, EGLN <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib194" title="">2021d</a>)</cite>, and RGCL <cite class="ltx_cite ltx_citemacro_citep">(Shuai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib138" title="">2022</a>)</cite> fully consider the correlations between nodes and edges within the graph structure to propose several graph-based NA strategies, including node (edge) dropout, node (edge) discrimination, random walk, and structure perturbation.
Thirdly, sequence-based NA focuses on enhancing the CL optimization by augmenting negative samples (e.g., shuffle, mask, crop, etc.) derived from historical behavior sequences <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib214" title="">2020</a>; Tian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib145" title="">2022</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib181" title="">2022a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib162" title="">2022</a>)</cite>. Its effective mechanism resides in comprehensively capturing the inherent dynamic characteristics of user behavior sequences. With the tailored self-supervised representation augmentation, it effectively handles the sequence structure and builds more robust and accurate recommenders.
Finally, scenarios-aware NA involves tailoring the augmentation of negative samples to accommodate the intricacies and demands of various recommendation scenarios. Here, MMR derives multi-view relationships of the user’s preference and item’s representation across multiple modalities with CL to avoid the problem of limited diversity or bias <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib164" title="">2021</a>; Tao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib144" title="">2022</a>)</cite>, and MBR constructs negative examples based on the auxiliary behavior data to model the different user behavior patterns through CL <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib60" title="">2021</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib187" title="">2021a</a>)</cite>.
Adaptively selecting the appropriate NA strategy based on the scenario requirements and data availability in CL-based recommendation tasks is sensible. Meanwhile, how to construct a universal augmentation method to serve various recommendation scenarios remains an urgent issue.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion and Future Direction</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Sampling the appropriate negative instances that align with the current model optimization state to model users’ unbiased preference is the fundamental yet challenging issue in recommendation. In this survey, we conduct a comprehensive and systematic investigation on negative sampling in recommendation over the past ten years. By holistically analyzing the significance and the challenges of negative sampling research in recommendation, we collect the existing related studies, organize and cluster them into five categories, and outline their diverse sampling schemes to discuss the current research contributions within this area. Subsequently, we systematically detail the diverse insights of the tailored negative sampling approaches in different recommendation scenarios from the perspective of the specific recommendation objective. Finally, we will discuss several important yet not-well-explored research directions in the field of negative sampling to inspire some future studies. We hope this survey is able to offer a comprehensive understanding of this promising but easy-to-ignored area to both newcomers and experts from academia and industry, who are dedicated to negative sampling research in recommendation and to provide some insights for potential future research.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Further Explorations on False Negative Sample Issue</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The primary objective of existing negative sampling algorithms in RS is to strategically sample informative yet not excessively challenging negative instances (<em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.1">RHNS</em>) from item candidates <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>. These negative samples are then incorporated to augment the training process of the recommender. From the former studies, we have ascertained the presence of a rather indistinct boundary between <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.2">SNS</em> and <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p1.1.3">HNS</em> within the prevailing negative sampling strategies <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib195" title="">2020a</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>)</cite>. This implies that a degree of randomness is inherent in these approaches during negative sampling, rendering it challenging to precisely address the issue of false negatives. Furthermore, users occasionally exhibit preferences for some specific items that diverge significantly from their behavioral patterns, making it difficult to precisely model this occasional preference with user’s implicit behaviors <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib62" title="">2020a</a>; Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib196" title="">2019</a>)</cite>. Following the established nomenclature in RealHNS <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>, this type of samples is defined as occasional HNS. Regarding the occasional HNS, the utilization of distinct users’ multiple behaviors and items’ heterogeneous knowledge across different recommendation scenarios may aid in modeling their occasional preference. This could effectively facilitate the utilization or denoising of occasional HNS.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Taking the classical hard negative sampling method (DNS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>)</cite>) as an illustrative example, the size of the candidates effectively determines the “hard” level of the sampled negative samples. In fact, the concept of “hard” possesses varying definitions across diverse recommenders and even distinct datasets. The indiscriminate employment of DNS to varying recommenders and datasets has been proved infeasible, manifesting the absence of interpretability and stability <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib216" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib22" title="">2021d</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib102" title="">2021</a>)</cite>. Consequently, elevating the interpretability, stability, and scalability of existing hard negative sampling strategies while concurrently maintaining the precision remains an open-ended issue and deserves further exploration.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Curriculum Learning on Hard Negative Sampling</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Inspired by the empirical evidence in behavior and cognitive science literature, the curriculum learning scheme has been introduced to improve the generalization capacity and convergence rate of neural networks <cite class="ltx_cite ltx_citemacro_citep">(Peterson, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib126" title="">2004</a>; Bengio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib6" title="">2009</a>)</cite>. The core of curriculum learning is to train the model with easier candidates and then gradually increase the hard level of samples until its convergence <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib157" title="">2021a</a>)</cite>. This process imitates the learning order in human curricula. Its fundamental objective is to enhance the model’s learning efficiency by providing more informative training signals to the model as it gradually captures and refines specific knowledge <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib157" title="">2021a</a>)</cite>. With the assumption that indiscriminately introducing all HNS at the beginning of training may lead to computational wastage, sub-optimal performance, and excessively high gradient magnitudes, some recommendation algorithms attempt to select negative samples with the curriculum learning paradigm <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib191" title="">2021c</a>)</cite>. Curriculum learning is able to gradually incorporate increasingly hard training instances in recommender’s learning process. It aligns seamlessly with the negative sampling algorithms, which are dedicated to the selection of the most informative samples within the item candidates, those that promise the most significant model enhancement <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib17" title="">2021b</a>)</cite>. In this situation, negative sampling algorithms and curriculum learning paradigm exhibit similarity to some extent, as they both focus on directing model learning in dynamic and intricate environments <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib32" title="">2021e</a>)</cite>. Such inherent resemblances present us the opportunity to intricately amalgamate them to guide the training process of recommender more effectively, ultimately culminating in improved performance and enhanced training efficiency.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">In the not-so-distant future, it is imperative to explore the integration of negative sampling algorithms with the curriculum learning paradigm. Confronted with the increasingly intricate scenarios and diverse behaviors, the exploration of refining existing negative sampling methods to bolster the seamless collaboration between them and curriculum learning becomes profoundly pivotal. Additionally, the exploration of diverse training strategies holds the potential to achieve heightened model performance and greater efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Causal Inference for Negative Sample Understanding</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Causality is the science of cause and effect in which the effect partly originates from the cause to some extent. As mentioned in <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib50" title="">2022b</a>)</cite>, causal inference refers to the process of determining and further leveraging the causal relation from the experimental results or observational data. In the recent past, causal inference has been introduced into RS <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib190" title="">2021b</a>)</cite> to extract the causality within the recommenders in modeling intricate user preferences, departing from the traditional correlation-centric paradigm <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib74" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib73" title="">2023b</a>)</cite>. This paradigm aims to determine how user historical behaviors influence the recommendation results, allowing for a comprehensive comprehension of user preferences.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">From the perspective of negative sampling, we posit that the integration of negative sampling with causal inference holds the potential to enable the selection of interpretable and traceable negative samples which can dynamically represent the most suitable instances for recommender. Two kinds of causality widely exist in RS, user-aspect, and interaction-aspect <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib50" title="">2022b</a>)</cite>. Regarding the user-aspect causality, user-similarity DNS can assist in unearthing commonalities within user behaviors by incorporating causal relationships, thus simulating the driving process behind user decision-making <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib173" title="">2019b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib148" title="">2021b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib23" title="">2019b</a>)</cite>. In contrast, concerning interaction-aspect causality, universal DNS and distribution-based DNS are proficient at reinforcing users’ unadulterated preferences by exploring and establishing causal links between user behaviors and recommended results <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib218" title="">2022</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib40" title="">2020</a>)</cite>. Under these circumstances, negative sample selection is not solely based on the item correlation but also involves the discernment of potential causal relationships among them <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib24" title="">2019c</a>)</cite>. Nevertheless, the integration of negative sampling with causal inference remains an underexplored research avenue in RS, with only a limited number of studies delving into this domain. We believe that it holds great potential to offer the opportunity to provide more accurate, interpretable, and user-centric recommendations.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Alleviating Biases in Negative Sampling</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The user behavioral data utilized in the training of recommenders originates from the observed data within the real-world RS rather than experimental data <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib20" title="">2023a</a>)</cite>. Consequently, this inherent source introduces various biases, which are driven by the diverse exposure mechanisms employed within the real-world large-scale RS. These exposure mechanisms serve as determinants of user behaviors, which, in turn, are subsequently utilized as training data for RS <cite class="ltx_cite ltx_citemacro_citep">(Mansoury et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib120" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib152" title="">2023b</a>)</cite>. Disregarding the aforementioned biases and unquestioningly adhering to the original training strategy would not only introduce additional inherent biases but also lead to the emergence of a ”rich get richer” Matthew effect over time <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib47" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">To address this issue, some studies attempt to explore debiased learning by adversarial training <cite class="ltx_cite ltx_citemacro_citep">(Krishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib94" title="">2018</a>; Anelli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib3" title="">2021</a>)</cite>, causal modeling <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib212" title="">2020</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib175" title="">2019e</a>)</cite> and negative sampling <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib39" title="">2019</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>)</cite>. It is able to improve the fairness of RS by rectifying the impact of biases, whether they stem from data imbalances, user historical behaviors, or various other factors <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib20" title="">2023a</a>)</cite>. Existing negative sampling algorithms primarily concentrate on mitigating exposure bias, popularity bias, and unfairness, which is achieved by over-sampling popular items <cite class="ltx_cite ltx_citemacro_citep">(Gantner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib46" title="">2012</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib33" title="">2021</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib72" title="">2016</a>)</cite> or enhancing the sampler with side information to facilitate model debiasing <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib30" title="">2023b</a>; Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib140" title="">2018</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib206" title="">2019</a>)</cite>. Nevertheless, they have not proposed solutions for the broader issues of selection bias, conformity bias, and other biases in RS. The adaptive negative sampling strategies ensure that the learning process accounts for data imbalances and mitigates any potential biases in RS. Investigating how various negative sampling strategies affect the inherent biases in recommenders would be a genuinely interesting and valuable endeavor.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Incorporating Large Language Model into Recommendation</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The rapid developments of Large Language Model (LLM) in Natural Language Processing have recently gained significant attention and revolutionized the learning paradigm of diverse research fields <cite class="ltx_cite ltx_citemacro_citep">(Huang and Chang, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib81" title="">2022</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib208" title="">2023</a>)</cite>. Some studies have indicated that the model size of LLM continues to expand with the immense scale of the corpus size, concomitantly enhancing their capabilities in logical reasoning and generalization <cite class="ltx_cite ltx_citemacro_citep">(Hadi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib64" title="">2023</a>)</cite>. Benefits from the potential to bridge the gap between the down-streaming task and the open-world knowledge, LLM have been incorporated into recommendation to serve as the knowledge extractor <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib167" title="">2021b</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib78" title="">2022</a>)</cite> and the recommender <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib37" title="">2023</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib54" title="">2023b</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib79" title="">2023b</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib87" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib111" title="">2023</a>)</cite>. Most of the existing LLM-based recommendation algorithms typically employ random sampling <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib4" title="">2023</a>; hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib66" title="">2023</a>; Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib123" title="">2023</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib55" title="">2022</a>; Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib44" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib174" title="">2024</a>)</cite> or leverage users’ explicit scores <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib107" title="">2024</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib176" title="">2023</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib37" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib87" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib100" title="">2023</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib107" title="">2024</a>)</cite> in real-world RS (Ratings within the range of <math alttext="[4,5]" class="ltx_Math" display="inline" id="S5.SS5.p1.1.m1.2"><semantics id="S5.SS5.p1.1.m1.2a"><mrow id="S5.SS5.p1.1.m1.2.3.2" xref="S5.SS5.p1.1.m1.2.3.1.cmml"><mo id="S5.SS5.p1.1.m1.2.3.2.1" stretchy="false" xref="S5.SS5.p1.1.m1.2.3.1.cmml">[</mo><mn id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">4</mn><mo id="S5.SS5.p1.1.m1.2.3.2.2" xref="S5.SS5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS5.p1.1.m1.2.2" xref="S5.SS5.p1.1.m1.2.2.cmml">5</mn><mo id="S5.SS5.p1.1.m1.2.3.2.3" stretchy="false" xref="S5.SS5.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.2b"><interval closure="closed" id="S5.SS5.p1.1.m1.2.3.1.cmml" xref="S5.SS5.p1.1.m1.2.3.2"><cn id="S5.SS5.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS5.p1.1.m1.1.1">4</cn><cn id="S5.SS5.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS5.p1.1.m1.2.2">5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.2c">[4,5]</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.1.m1.2d">[ 4 , 5 ]</annotation></semantics></math> are regarded as positive feedback, while defining the rest as negative feedback) for training. The negative samples selected in M6Rec <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib35" title="">2022</a>)</cite> are exclusively from real-world systems, that is, Taobao and Alipay. To prevent the excessive training time and the overwhelming influx of negative information that could impact the model’s training process, M6Rec further down-samples the negative samples by a factor of ten. PALR <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib186" title="">2023a</a>)</cite> incorporates the negative samples which are similar to the positive samples (e.g., movies belonging to the same genres and have been co-watched by many users) into model optimization.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">From the comprehensive review of existing LLM-based recommendation studies, we notice that LLMRank <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib79" title="">2023b</a>)</cite> first investigates the ranking performance of LLM on hard negative candidates, which are retrieved by different classical recommendation algorithms. Moreover, InstructRec <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib201" title="">2023</a>)</cite> has also delved into LLM’s effectiveness and robustness in tasks involving more challenging hard negative samples and larger candidate sets. Nevertheless, these LLM-based recommendation algorithms have not taken into account negative sampling. Furthermore, the prevalent paradigm in these algorithms lies in employing natural language for depicting user interests, context, task formats, and historical behaviors. However, the omission of users’ negative preferences in this paradigm introduces the potential bias during the training and inference of LLM, thereby impacting its downstream recommendation performance. A lot of research has demonstrated that the adoption of well-reasoned negative sampling techniques in RS can further bolster the recommenders’ performance <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib216" title="">2021</a>; Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib104" title="">2020a</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib170" title="">2021a</a>)</cite>. Negative Sampling would serve as a bridge between LLM and recommendation algorithms, facilitating the smooth incorporation of LLM into recommendation tasks. This contributes to being a powerful tool for assisting future researchers in constructing a LLM-based general recommendation framework.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>Understanding Negative Sampling with Theoretical Tools</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.2">To alleviate the inherent sparsity and imbalance in RS, early studies attempt to incorporate suitable negative samples into training from the corpus (auxiliary information or similarity ranking) to provide unbiased negative signals. These works simply attribute their superior performance to the improved convergence of the informative negative samples, without delving further into the profound reasons behind it. Recently, some studies have sought to understand the benefits brought to recommender by negative samples <cite class="ltx_cite ltx_citemacro_citep">(Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib128" title="">2023</a>)</cite> and hard negative samples <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite> through the several theoretical tools. Specifically, gSASRec <cite class="ltx_cite ltx_citemacro_citep">(Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib128" title="">2023</a>)</cite> regards the overconfidence phenomenon of negative sampling as it increases the proportion of positive samples in the training data distribution to force recommenders to overestimate the probabilities of future user-item interactions. It not only results in the recommender’s inability to capture subtle variations within high-scored items but also hinders its effectiveness and training convergence. To address this, gSASRec theoretically defines overconfidence through a probabilistic interpretation of sequential recommendation and proposes gBCE loss to mitigate this issue via negative sampling. Inspired by the existing recognition that uniformly sampled negatives may contribute little to the gradients and model convergence <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>; Rendle and Freudenthaler, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib132" title="">2014</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib205" title="">2013</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>)</cite>, DNS<sup class="ltx_sup" id="S5.SS6.p1.2.1">∗</sup> <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib137" title="">2023</a>)</cite> proves that fast convergence may not be the only advantage of HNS by comparing existing hard negative sampling methods (See Sec.<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS2.SSS1" title="3.2.1. Universal DNS ‣ 3.2. Dynamic Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) with the Non-sampling strategies (See Sec.<a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#S3.SS1.SSS4" title="3.1.4. Non-sampling SNS ‣ 3.1. Static Negative Sampling ‣ 3. Literature review of Negative Sampling in Recommendation ‣ Negative Sampling in Recommendation: A Survey and Future Directions"><span class="ltx_text ltx_ref_tag">3.1.4</span></a>). Then DNS<sup class="ltx_sup" id="S5.SS6.p1.2.2">∗</sup> establish the theoretical foundations and explain the performance gain of HNS by conducting theoretical analyses and simulation studies on One-way Partial AUC. In recent times, these works have demonstrated the superiority of utilizing theoretical tools to understand and guide negative sampling. This proves instrumental in facilitating a deeper exploration of the essence of negative samples in RS and uncovering potential opportunities for performance improvement. It stands out as a promising avenue for future research.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7. </span>Exploring Sampled Metrics in Recommendation Evaluation</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1">Personalized RS aims to leverage users’ historical behaviors to capture their interests and provide appropriate items. It is crucial to evaluate the efficacy of these recommenders during their optimization process. In large-scale RS applications, the candidate corpus often comprises millions of items, while it encompasses tens of thousands of items for academic research. Retrieving suitable samples from such an immense corpus poses a considerable challenge. To simplify the computational complexity of offline evaluations, many researchers typically employ uniform random sampling and popularity sampling in evaluating these recommendation algorithms <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib117" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib118" title="">2024b</a>)</cite>. Some recent LLM-based works even verify their effectiveness and robustness in a more challenging setting, that is, evaluating them on hard negative candidates, which are retrieved by different classical recommenders. However, some recent works have proved that these sampled metrics are inconsistent with the full item coverage in algorithm evaluation <cite class="ltx_cite ltx_citemacro_citep">(Krichene and Rendle, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib92" title="">2020</a>; Dallmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib38" title="">2021</a>; Pellegrini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib125" title="">2022</a>; Bauer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib5" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS7.p2">
<p class="ltx_p" id="S5.SS7.p2.1"><cite class="ltx_cite ltx_citemacro_citep">(Bauer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib5" title="">2023</a>)</cite> is a systematic literature review on the processes surrounding the evaluation of recommendation algorithms, which demonstrates that the predominant evaluation in recommendation algorithms is offline evaluation and that online evaluations are primarily used in combination with offline evaluation. Existing related research has explored the evaluation setting in RS from various perspectives. Precisely, <cite class="ltx_cite ltx_citemacro_citep">(Krichene and Rendle, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib92" title="">2020</a>)</cite> first explores the implications of sampling during recommender evaluation and discovers the bias in evaluation between random sampling and full item coverage. To alleviate this issue and improve the quality of the sampled metrics, a point-wise correction to the sampled metric is proposed by minimizing different criteria (e.g., bias and mean squared error). <cite class="ltx_cite ltx_citemacro_citep">(Dallmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib38" title="">2021</a>)</cite> trains four recommenders on five widely known datasets with three evaluation strategies, including full item coverage, popularity-based sampling, and random sampling. It also verifies that the evaluation results obtained from popularity-based sampling do not equal the full item coverage, which is similar to random sampling. <cite class="ltx_cite ltx_citemacro_citep">(Pellegrini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07237v1#bib.bib125" title="">2022</a>)</cite> investigates the correlation between optimizing popularity-sampled metrics and estimating point-wise mutual information (PMI) and proposes two PMI fitting techniques to improve popularity-sampled metrics for recommenders. How to precisely evaluate the recommendation algorithms with less computational complexity need to be given more attention for the future research.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work is supported in part by the TaiShan Scholars Program (Grant no. tsqn202211289), the Shandong Province Excellent Young Scientists Fund Program (Overseas) (Grant no. 2022HWYQ-048), the Oversea Innovation Team Project of the ”20 Regulations for New Universities” funding program of Jinan (Grant no. 2021GXRC073) and the Young Elite Scientists Sponsorship Program by CAST (2023QNRC001). ChatGPT and Grammarly were utilized to improve grammar and correct spelling.

</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ali et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zafar Ali, Guilin Qi, Khan Muhammad, Bahadar Ali, and Waheed Ahmed Abro. 2020.

</span>
<span class="ltx_bibblock">Paper recommendation based on heterogeneous network embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Knowledge-Based Systems</em> 210 (2020), 106438.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anelli et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Vito Walter Anelli, Tommaso Di Noia, and Felice Antonio Merra. 2021.

</span>
<span class="ltx_bibblock">The idiosyncratic effects of adversarial training on bias in personalized recommendation learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 15th ACM Conference on Recommender Systems</em>. 730–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023.

</span>
<span class="ltx_bibblock">A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.08434" title="">http://arxiv.org/abs/2308.08434</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bauer et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Christine Bauer, Eva Zangerle, and Alan Said. 2023.

</span>
<span class="ltx_bibblock">Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">ACM Transactions on Recommender Systems</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009.

</span>
<span class="ltx_bibblock">Curriculum learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 26th annual international conference on machine learning</em>. 41–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, and Jing Xiao. 2020.

</span>
<span class="ltx_bibblock">DCDIR: A deep cross-domain recommendation system for cold start users in insurance domain. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>. 1661–1664.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brin and Page (1998)</span>
<span class="ltx_bibblock">
Sergey Brin and Lawrence Page. 1998.

</span>
<span class="ltx_bibblock">The anatomy of a large-scale hypertextual web search engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Computer networks and ISDN systems</em> (1998).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Borui Cai, Yong Xiang, Longxiang Gao, He Zhang, Yunfeng Li, and Jianxin Li. 2022.

</span>
<span class="ltx_bibblock">Temporal knowledge graph completion: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:2201.08236</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022.

</span>
<span class="ltx_bibblock">Contrastive Cross-Domain Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 138–147.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Chong Chen, Weizhi Ma, Min Zhang, Chenyang Wang, Yiqun Liu, and Shaoping Ma. 2023c.

</span>
<span class="ltx_bibblock">Revisiting negative sampling vs. non-sampling in implicit recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">ACM Transactions on Information Systems</em> 41, 1 (2023), 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020b.

</span>
<span class="ltx_bibblock">Efficient non-sampling factorization machines for optimal context-aware recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the web conference 2020</em>. 2400–2410.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2019d)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang, Chenyang Wang, Weizhi Ma, Minming Li, Yiqun Liu, and Shaoping Ma. 2019d.

</span>
<span class="ltx_bibblock">An Efficient Adaptive Transfer Neural Network for Social-Aware Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020c)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, and Shaoping Ma. 2020c.

</span>
<span class="ltx_bibblock">Efficient neural matrix factorization without sampling for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ACM Transactions on Information Systems (TOIS)</em> 38, 2 (2020), 1–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020d)</span>
<span class="ltx_bibblock">
Chong Chen, Min Zhang, Yongfeng Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020d.

</span>
<span class="ltx_bibblock">Efficient heterogeneous collaborative filtering without negative sampling for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 34. 19–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. 2020a.

</span>
<span class="ltx_bibblock">Graph representation learning: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">APSIPA Transactions on Signal and Information Processing</em> 9 (2020), e15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Rui Wang, Feng Xia, and Wenwu Zhu. 2021b.

</span>
<span class="ltx_bibblock">Curriculum disentangled recommendation with noisy multi-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Advances in Neural Information Processing Systems</em> 34 (2021), 26924–26936.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Hong Chen, Xin Wang, Ruobing Xie, Yuwei Zhou, and Wenwu Zhu. 2023d.

</span>
<span class="ltx_bibblock">Cross-domain Recommendation with Behavioral Importance Perception. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the World Wide Web Conference (WWW)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. 2021c.

</span>
<span class="ltx_bibblock">AutoDebias: Learning to debias for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 21–30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023a.

</span>
<span class="ltx_bibblock">Bias and debias in recommender system: A survey and future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">ACM Transactions on Information Systems</em> 41, 3 (2023), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing Li, and Yihua Huang. 2022a.

</span>
<span class="ltx_bibblock">Knowledge-enhanced Black-box Attacks for Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 108–117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021d)</span>
<span class="ltx_bibblock">
Junyang Chen, Zhiguo Gong, Wei Wang, and Weiwen Liu. 2021d.

</span>
<span class="ltx_bibblock">HNS: Hierarchical negative sampling for network representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Information Sciences</em> 542 (2021), 343–356.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Jiawei Chen, Can Wang, Sheng Zhou, Qihao Shi, Yan Feng, and Chun Chen. 2019b.

</span>
<span class="ltx_bibblock">SamWalker: Social Recommendation with Informative Sampling Strategy. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the World Wide Web Conference (WWW)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2019c)</span>
<span class="ltx_bibblock">
Li Chen, Yonghua Yang, Ningxia Wang, Keping Yang, and Quan Yuan. 2019c.

</span>
<span class="ltx_bibblock">How serendipity improves user satisfaction with recommendations? a large-scale user evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">The world wide web conference</em>. 240–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019a.

</span>
<span class="ltx_bibblock">Top-k off-policy correction for a REINFORCE recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>. 456–464.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021a.

</span>
<span class="ltx_bibblock">User response models to improve a reinforce recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</em>. 121–129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Tao Chen, Xiangnan He, and Min-Yen Kan. 2016.

</span>
<span class="ltx_bibblock">Context-aware image tweet modelling and recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the 24th ACM international conference on Multimedia</em>. 1018–1027.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong. 2017.

</span>
<span class="ltx_bibblock">On sampling strategies for neural network-based collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. 767–776.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022c)</span>
<span class="ltx_bibblock">
Wei Chen, Yu Liu, Weiping Wang, Erwin M Bakker, Theodoros Georgiou, Paul Fieguth, Li Liu, and Michael S Lew. 2022c.

</span>
<span class="ltx_bibblock">Deep learning for instance retrieval: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang, and Qing Li. 2023b.

</span>
<span class="ltx_bibblock">Fairly Adaptive Negative Sampling for Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2302.08266</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yongjun Chen, Jia Li, and Caiming Xiong. 2022b.

</span>
<span class="ltx_bibblock">ELECRec: Training Sequential Recommenders as Discriminators. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2550–2554.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021e)</span>
<span class="ltx_bibblock">
Yudong Chen, Xin Wang, Miao Fan, Jizhou Huang, Shengwen Yang, and Wenwu Zhu. 2021e.

</span>
<span class="ltx_bibblock">Curriculum meta-learning for next POI recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mingyue Cheng, Fajie Yuan, Qi Liu, Shenyang Ge, Zhi Li, Runlong Yu, Defu Lian, Senchao Yuan, and Enhong Chen. 2021.

</span>
<span class="ltx_bibblock">Learning recommender systems with implicit feedback via soft target enhancement. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 575–584.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay Adams, and Emre Sargin. 2016.

</span>
<span class="ltx_bibblock">Deep neural networks for youtube recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 10th ACM conference on recommender systems</em>. 191–198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</em>.

</span>
<span class="ltx_bibblock">Association for Computing Machinery.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.08084" title="">http://arxiv.org/abs/2205.08084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai and Lin (2017)</span>
<span class="ltx_bibblock">
Bo Dai and Dahua Lin. 2017.

</span>
<span class="ltx_bibblock">Contrastive learning for image captioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Uncovering ChatGPTś Capabilities in Recommender Systems.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3610646" title="">https://doi.org/10.1145/3604915.3610646</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dallmann et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021.

</span>
<span class="ltx_bibblock">A case study on sampling strategies for evaluating neural sequential item recommendation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 15th ACM Conference on Recommender Systems</em>. 505–514.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, and Depeng Jin. 2019.

</span>
<span class="ltx_bibblock">Reinforced Negative Sampling for Recommendation with Exposure Data.. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">IJCAI</em>. Macao, 2230–2236.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, and Depeng Jin. 2020.

</span>
<span class="ltx_bibblock">Simplify and robustify negative sampling for implicit collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Advances in Neural Information Processing Systems</em> 33 (2020), 1094–1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rui Ding, Ruobing Xie, Xiaobo Hao, Xiaochun Yang, Kaikai Ge, Xu Zhang, Jie Zhou, and Leyu Lin. 2023.

</span>
<span class="ltx_bibblock">Interpretable User Retention Modeling in Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 702–708.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xue Dong, Xuemeng Song, Minghui Tian, and Linmei Hu. 2024.

</span>
<span class="ltx_bibblock">Prompt-based and weak-modality enhanced multimodal recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Information Fusion</em> 101 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hanwen Du, Hui Shi, Pengpeng Zhao, Deqing Wang, Victor S Sheng, Yanchi Liu, Guanfeng Liu, and Lei Zhao. 2022.

</span>
<span class="ltx_bibblock">Contrastive Learning with Bidirectional Transformers for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 396–405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yingpeng Du, Di Luo, Rui Yan, Xiaopei Wang, Hongzhi Liu, Hengshu Zhu, Yang Song, and Jie Zhang. 2024.

</span>
<span class="ltx_bibblock">Enhancing job recommendation through llm-based generative adversarial networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 8363–8371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2023.

</span>
<span class="ltx_bibblock">Recommender Systems in the Era of Large Language Models (LLMs).

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.02046" title="">http://arxiv.org/abs/2307.02046</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gantner et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2012.

</span>
<span class="ltx_bibblock">Personalized ranking for non-uniformly sampled items. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of KDD Cup 2011</em>. PMLR, 231–247.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023a.

</span>
<span class="ltx_bibblock">Alleviating matthew effect of offline reinforcement learning in interactive recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:2307.04571</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Chen Gao, Yong Li, Fuli Feng, Xiangning Chen, Kai Zhao, Xiangnan He, and Depeng Jin. 2021a.

</span>
<span class="ltx_bibblock">Cross-domain recommendation with bridge-item embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">ACM Transactions on Knowledge Discovery from Data (TKDD)</em> 16, 1 (2021), 1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al<span class="ltx_text" id="bib.bib49.3.1">.</span> 2023c.

</span>
<span class="ltx_bibblock">A survey of graph neural networks for recommender systems: Challenges, methods, and directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.4.1">ACM Transactions on Recommender Systems</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Chen Gao, Yu Zheng, Wenjie Wang, Fuli Feng, Xiangnan He, and Yong Li. 2022b.

</span>
<span class="ltx_bibblock">Causal inference in recommender systems: A survey and future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2208.12397</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rong Gao, Haifeng Xia, Jing Li, Donghua Liu, Shuai Chen, and Gang Chun. 2019.

</span>
<span class="ltx_bibblock">DRCGR: Deep reinforcement learning framework incorporating CNN and GAN-based for interactive recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">2019 IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 1048–1053.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">arXiv preprint arXiv:2104.08821</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, Ziquan Fang, and Baihua Zheng. 2022a.

</span>
<span class="ltx_bibblock">Self-guided learning to denoise for robust recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1412–1422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023b.

</span>
<span class="ltx_bibblock">Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.14524" title="">http://arxiv.org/abs/2303.14524</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5).

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3523227.3546767" title="">https://doi.org/10.1145/3523227.3546767</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giobergia (2022)</span>
<span class="ltx_bibblock">
Flavio Giobergia. 2022.

</span>
<span class="ltx_bibblock">Triplet Losses-based Matrix Factorization for Robust Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2210.12098</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaiqi Gong, Xiao Song, Senzhang Wang, Songsong Liu, and Yong Li. 2022.

</span>
<span class="ltx_bibblock">ITSM-GCN: Informative Training Sample Mining for Graph Convolutional Network-based Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 614–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shuyun Gu, Xiao Wang, Chuan Shi, and Ding Xiao. 2022.

</span>
<span class="ltx_bibblock">Self-supervised Graph Neural Networks for Multi-behavior Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">International Joint Conference on Artificial Intelligence (IJCAI)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yulong Gu, Wentian Bao, Dan Ou, Xiang Li, Baoliang Cui, Biyu Ma, Haikuan Huang, Qingwen Liu, and Xiaoyi Zeng. 2021.

</span>
<span class="ltx_bibblock">Self-Supervised Learning on Users’ Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>. 3828–3837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2022.

</span>
<span class="ltx_bibblock">Semantic models for the first-stage retrieval: A comprehensive review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">ACM Transactions on Information Systems (TOIS)</em> 40, 4 (2022), 1–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Lei Guo, Hongzhi Yin, Qinyong Wang, Bin Cui, Zi Huang, and Lizhen Cui. 2020a.

</span>
<span class="ltx_bibblock">Group recommendation with latent voting mechanism. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">2020 IEEE 36th International Conference on Data Engineering (ICDE)</em>. IEEE, 121–132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. 2020b.

</span>
<span class="ltx_bibblock">A survey on knowledge graph-based recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 34, 8 (2020), 3549–3568.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hadi et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al<span class="ltx_text" id="bib.bib64.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Junlin Han, Mehrdad Shoeiby, Lars Petersson, and Mohammad Ali Armin. 2021.

</span>
<span class="ltx_bibblock">Dual contrastive learning for unsupervised image-to-image translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 746–755.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">hao et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, and Zhang Kai. 2023.

</span>
<span class="ltx_bibblock">LKPNR: LLM and KG for Personalized News Recommendation Framework.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.12028 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Qianxiu Hao, Qianqian Xu, Zhiyong Yang, and Qingming Huang. 2021b.

</span>
<span class="ltx_bibblock">Learning unified embeddings for recommendation via meta-path semantics. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Proceedings of the 29th ACM International Conference on Multimedia</em>. 3909–3917.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Xiaobo Hao, Yudan Liu, Ruobing Xie, Kaikai Ge, Linyao Tang, Xu Zhang, and Leyu Lin. 2021a.

</span>
<span class="ltx_bibblock">Adversarial feature translation for multi-domain recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 2964–2973.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassani and Khasahmadi (2020)</span>
<span class="ltx_bibblock">
Kaveh Hassani and Amir Hosein Khasahmadi. 2020.

</span>
<span class="ltx_bibblock">Contrastive multi-view representation learning on graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">International conference on machine learning</em>. PMLR, 4116–4126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Li He, Xianzhi Wang, Dingxian Wang, Haoyuan Zou, Hongzhi Yin, and Guandong Xu. 2023a.

</span>
<span class="ltx_bibblock">Simplifying Graph-based Collaborative Filtering for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em>. 60–68.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017.

</span>
<span class="ltx_bibblock">Neural collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Proceedings of the 26th international conference on world wide web</em>. 173–182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016.

</span>
<span class="ltx_bibblock">Fast matrix factorization for online recommendation with implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</em>. 549–558.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, Guohui Ling, and Yongdong Zhang. 2023b.

</span>
<span class="ltx_bibblock">Addressing confounding feature issue for causal recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">ACM Transactions on Information Systems</em> 41, 3 (2023), 1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yue He, Zimu Wang, Peng Cui, Hao Zou, Yafeng Zhang, Qiang Cui, and Yong Jiang. 2022.

</span>
<span class="ltx_bibblock">Causpref: Causal preference learning for out-of-distribution recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the ACM Web Conference 2022</em>. 410–421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidasi and Karatzoglou (2018)</span>
<span class="ltx_bibblock">
Balázs Hidasi and Alexandros Karatzoglou. 2018.

</span>
<span class="ltx_bibblock">Recurrent neural networks with top-k gains for session-based recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 843–852.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidasi et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015.

</span>
<span class="ltx_bibblock">Session-based recommendations with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">arXiv preprint arXiv:1511.06939</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023a.

</span>
<span class="ltx_bibblock">Learning vector-quantized item representation for transferable sequential recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Proceedings of the ACM Web Conference 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">Towards universal sequence representation learning for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023b.

</span>
<span class="ltx_bibblock">Large Language Models are Zero-Shot Rankers for Recommender Systems.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.08845" title="">http://arxiv.org/abs/2305.08845</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018.

</span>
<span class="ltx_bibblock">Leveraging meta-path based context for top-n recommendation with a neural co-attention model. In <em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1531–1540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang (2022)</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang. 2022.

</span>
<span class="ltx_bibblock">Towards reasoning in large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2212.10403</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junjie Huang, Ruobing Xie, Qi Cao, Huawei Shen, Shaoliang Zhang, Feng Xia, and Xueqi Cheng. 2023.

</span>
<span class="ltx_bibblock">Negative can be positive: Signed graph neural networks for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">Information Processing &amp; Management</em> 60, 4 (2023), 103403.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020.

</span>
<span class="ltx_bibblock">Embedding-based retrieval in facebook search. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2553–2561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021.

</span>
<span class="ltx_bibblock">Mixgcf: An improved training method for graph neural network-based recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamigaito and Hayashi (2022)</span>
<span class="ltx_bibblock">
Hidetaka Kamigaito and Katsuhiko Hayashi. 2022.

</span>
<span class="ltx_bibblock">Comprehensive analysis of negative sampling in knowledge graph representation learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">International Conference on Machine Learning</em>. PMLR, 10661–10675.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu. 2020.

</span>
<span class="ltx_bibblock">DE-RRD: A knowledge distillation framework for recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 605–614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023.

</span>
<span class="ltx_bibblock">Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.06474" title="">http://arxiv.org/abs/2305.06474</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaya and Bilge (2019)</span>
<span class="ltx_bibblock">
Mahmut Kaya and Hasan Şakir Bilge. 2019.

</span>
<span class="ltx_bibblock">Deep metric learning: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Symmetry</em> 11, 9 (2019), 1066.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khoshraftar and An (2022)</span>
<span class="ltx_bibblock">
Shima Khoshraftar and Aijun An. 2022.

</span>
<span class="ltx_bibblock">A survey on graph representation learning methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2204.01855</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Minseok Kim, Hwanjun Song, Doyoung Kim, Kijung Shin, and Jae-Gil Lee. 2021a.

</span>
<span class="ltx_bibblock">Premere: Meta-reweighting via self-ensembling for point-of-interest recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">Proceedings of the AAAI Conference on artificial intelligence</em>, Vol. 35. 4164–4171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021b.

</span>
<span class="ltx_bibblock">Self-guided contrastive learning for BERT sentence representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">arXiv preprint arXiv:2106.07345</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krichene and Rendle (2020)</span>
<span class="ltx_bibblock">
Walid Krichene and Steffen Rendle. 2020.

</span>
<span class="ltx_bibblock">On sampled metrics for item recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1748–1757.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnan et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Adit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, and Hari Sundaram. 2020.

</span>
<span class="ltx_bibblock">Transfer learning via contextual invariants for one-to-many cross-domain recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1081–1090.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnan et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Adit Krishnan, Ashish Sharma, Aravind Sankar, and Hari Sundaram. 2018.

</span>
<span class="ltx_bibblock">An adversarial approach to improve long-tail performance in neural collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">Proceedings of the 27th ACM International Conference on information and knowledge management</em>. 1491–1494.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Riwei Lai, Li Chen, Yuhan Zhao, Rui Chen, and Qilong Han. 2023.

</span>
<span class="ltx_bibblock">Disentangled Negative Sampling for Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em>. 96–104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Riwei Lai, Rui Chen, Qilong Han, Chi Zhang, and Li Chen. 2024.

</span>
<span class="ltx_bibblock">Adaptive hardness negative sampling for collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">arXiv preprint arXiv:2401.05191</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jae-woong Lee, Minjin Choi, Jongwuk Lee, and Hyunjung Shim. 2019.

</span>
<span class="ltx_bibblock">Collaborative distillation for top-N recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">2019 IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 369–378.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020.

</span>
<span class="ltx_bibblock">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 304–312.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nian Li, Chen Gao, Jinghua Piao, Xin Huang, Aizhen Yue, Liang Zhou, Qingmin Liao, and Yong Li. 2022.

</span>
<span class="ltx_bibblock">An Exploratory Study of Information Cocoon on Short-form Video Platform. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 4178–4182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023.

</span>
<span class="ltx_bibblock">CTRL: Connect Collaborative and Language Model for CTR Prediction.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.02841" title="">http://arxiv.org/abs/2306.02841</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yang Li, Yadan Luo, and Zi Huang. 2020.

</span>
<span class="ltx_bibblock">Fashion recommendation with multi-relational representation learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I 24</em>. Springer, 3–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zelong Li, Jianchao Ji, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Chong Chen, and Yongfeng Zhang. 2021.

</span>
<span class="ltx_bibblock">Efficient non-sampling knowledge graph embedding. In <em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Proceedings of the Web Conference 2021</em>. 1727–1736.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhi Li, Hongke Zhao, Qi Liu, Zhenya Huang, Tao Mei, and Enhong Chen. 2018.

</span>
<span class="ltx_bibblock">Learning from history and present: Next-item recommendation via discriminatively exploiting user behaviors. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Defu Lian, Qi Liu, and Enhong Chen. 2020a.

</span>
<span class="ltx_bibblock">Personalized ranking with importance sampling. In <em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">Proceedings of The Web Conference 2020</em>. 1093–1103.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020b.

</span>
<span class="ltx_bibblock">Geography-aware sequential location recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 2009–2019.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Defu Lian, Cong Zhao, Xing Xie, Guangzhong Sun, Enhong Chen, and Yong Rui. 2014.

</span>
<span class="ltx_bibblock">GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024.

</span>
<span class="ltx_bibblock">Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">Proceedings of the ACM on Web Conference</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Danyang Liu, Jianxun Lian, Zheng Liu, Xiting Wang, Guangzhong Sun, and Xing Xie. 2021b.

</span>
<span class="ltx_bibblock">Reinforced anchor knowledge graph generation for news recommendation reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 1055–1065.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Huiting Liu, Lingling Guo, Peipei Li, Peng Zhao, and Xindong Wu. 2021a.

</span>
<span class="ltx_bibblock">Collaborative filtering with a deep adversarial and attention network for cross-domain recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">Information Sciences</em> 565 (2021), 370–389.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Hao Liu, Ting Li, Renjun Hu, Yanjie Fu, Jingjing Gu, and Hui Xiong. 2019.

</span>
<span class="ltx_bibblock">Joint representation learning for multi-modal transportation recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 33. 1036–1043.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. 2023.

</span>
<span class="ltx_bibblock">Is ChatGPT a Good Recommender? A Preliminary Study.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.10149" title="">http://arxiv.org/abs/2304.10149</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020.

</span>
<span class="ltx_bibblock">Cross domain recommendation via bi-directional transfer graph collaborative filtering networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">Proceedings of the 29th ACM international conference on information &amp; knowledge management</em>. 885–894.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Tang (2021)</span>
<span class="ltx_bibblock">
Xueyi Liu and Jie Tang. 2021.

</span>
<span class="ltx_bibblock">Network representation learning: A macro and micro view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">AI Open</em> 2 (2021), 43–64.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lou et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Chen Lou, Quan Xie, Yang Feng, and Wonkyung Kim. 2019.

</span>
<span class="ltx_bibblock">Does non-hard-sell content really work? Leveraging the value of branded content marketing in brand building.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">Journal of Product &amp; Brand Management</em> 28, 7 (2019), 773–786.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Chen Ma, Yingxue Zhang, Qinglong Wang, and Xue Liu. 2018.

</span>
<span class="ltx_bibblock">Point-of-interest recommendation: Exploiting self-attentive autoencoders with neighbor-aware influence. In <em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 697–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haokai Ma, Ruobing Xie, Lei Meng, Xin Chen, Xu Zhang, Leyu Lin, and Zhanhui Kang. 2024a.

</span>
<span class="ltx_bibblock">Plug-in Diffusion Model for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haokai Ma, Ruobing Xie, Lei Meng, Xin Chen, Xu Zhang, Leyu Lin, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock">Exploring False Hard Negative Sample in Cross-Domain Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haokai Ma, Ruobing Xie, Lei Meng, Xin Chen, Xu Zhang, Leyu Lin, and Jie Zhou. 2024b.

</span>
<span class="ltx_bibblock">Triple sequence learning for cross-domain recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">ACM Transactions on Information Systems</em> 42, 4 (2024), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yunshan Ma, Yingzhi He, An Zhang, Xiang Wang, and Tat-Seng Chua. 2022.

</span>
<span class="ltx_bibblock">CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">arXiv preprint arXiv:2206.00242</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansoury et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2020.

</span>
<span class="ltx_bibblock">Feedback loop and bias amplification in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">Proceedings of the 29th ACM international conference on information &amp; knowledge management</em>. 2145–2148.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.

</span>
<span class="ltx_bibblock">Distributed Representations of Words and Phrases and their Compositionality.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. 2022.

</span>
<span class="ltx_bibblock">Simple unsupervised graph representation learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 36. 7797–7805.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mysore et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sheshera Mysore, Andrew Mccallum, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Large Language Model Augmented Narrative Driven Recommendations.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608829" title="">https://doi.org/10.1145/3604915.3608829</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Rong Pan, Yunhong Zhou, Bin Cao, Nathan N. Liu, Rajan Lukose, Martin Scholz, and Qiang Yang. 2008.

</span>
<span class="ltx_bibblock">One-Class Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">Proceedings of the IEEE International Conference on Data Mining (ICDM)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pellegrini et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Roberto Pellegrini, Wenjie Zhao, and Iain Murray. 2022.

</span>
<span class="ltx_bibblock">Don’t recommend the obvious: estimate probability ratios. In <em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">Proceedings of the 16th ACM Conference on Recommender Systems</em>. 188–197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peterson (2004)</span>
<span class="ltx_bibblock">
Gail B Peterson. 2004.

</span>
<span class="ltx_bibblock">A day of great illumination: BF Skinner’s discovery of shaping.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Journal of the experimental analysis of behavior</em> 82, 3 (2004), 317–328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov and Macdonald (2022)</span>
<span class="ltx_bibblock">
Aleksandr Petrov and Craig Macdonald. 2022.

</span>
<span class="ltx_bibblock">Effective and Efficient Training for Sequential Recommendation using Recency Sampling. In <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Proceedings of the 16th ACM Conference on Recommender Systems</em>. 81–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov and Macdonald (2023)</span>
<span class="ltx_bibblock">
Aleksandr Vladimirovich Petrov and Craig Macdonald. 2023.

</span>
<span class="ltx_bibblock">gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling. In <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 116–128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jing Qian, Gangmin Li, Katie Atkinson, and Yong Yue. 2020.

</span>
<span class="ltx_bibblock">Negative Sampling in Knowledge Representation Learning: A Mini-Review.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:229449355" title="">https://api.semanticscholar.org/CorpusID:229449355</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiarui Qin, Kan Ren, Yuchen Fang, Weinan Zhang, and Yong Yu. 2020.

</span>
<span class="ltx_bibblock">Sequential recommendation with dual side neighbor-based collaborative relation modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Proceedings of the 13th international conference on web search and data mining</em>. 465–473.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quadrana et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Massimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi. 2017.

</span>
<span class="ltx_bibblock">Personalizing session-based recommendations with hierarchical recurrent neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">proceedings of the Eleventh ACM Conference on Recommender Systems</em>. 130–137.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle and Freudenthaler (2014)</span>
<span class="ltx_bibblock">
Steffen Rendle and Christoph Freudenthaler. 2014.

</span>
<span class="ltx_bibblock">Improving pairwise learning for item recommendation from implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">Proceedings of the 7th ACM international conference on Web search and data mining</em>. 273–282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012.

</span>
<span class="ltx_bibblock">BPR: Bayesian personalized ranking from implicit feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">arXiv preprint arXiv:1205.2618</em> (2012).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saito et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata. 2020.

</span>
<span class="ltx_bibblock">Unbiased recommender learning from missing-not-at-random implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 501–509.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Qijie Shen, Wanjie Tao, Jing Zhang, Hong Wen, Zulong Chen, and Quan Lu. 2021.

</span>
<span class="ltx_bibblock">SAR-Net: A scenario-aware ranking network for personalized fair recommendation in hundreds of travel scenarios. In <em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kexin Shi, Yun Zhang, Bingyi Jing, and Wenjia Wang. 2022.

</span>
<span class="ltx_bibblock">Soft BPR Loss for Dynamic Hard Negative Sampling in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">arXiv preprint arXiv:2211.13912</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wentao Shi, Jiawei Chen, Fuli Feng, Jizhi Zhang, Junkang Wu, Chongming Gao, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">On the Theories Behind Hard Negative Sampling for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">arXiv preprint arXiv:2302.03472</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuai et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jie Shuai, Kun Zhang, Le Wu, Peijie Sun, Richang Hong, Meng Wang, and Yong Li. 2022.

</span>
<span class="ltx_bibblock">A review-aware graph contrastive learning framework for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib138.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1283–1293.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Dongjin Song, David A Meyer, and Dacheng Tao. 2015.

</span>
<span class="ltx_bibblock">Efficient latent link recommendation in signed networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</em>. 1105–1114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steck (2018)</span>
<span class="ltx_bibblock">
Harald Steck. 2018.

</span>
<span class="ltx_bibblock">Calibrated recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Proceedings of the 12th ACM conference on recommender systems</em>. 154–162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Changfeng Sun, Han Liu, Meng Liu, Zhaochun Ren, Tian Gan, and Liqiang Nie. 2020.

</span>
<span class="ltx_bibblock">LARA: Attribute-to-feature adversarial learning for new-item recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">Proceedings of the 13th international conference on web search and data mining</em>. 582–590.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jianshan Sun, Gang Wang, Xusen Cheng, and Yelin Fu. 2015.

</span>
<span class="ltx_bibblock">Mining affective text to improve social media item recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">Information Processing &amp; Management</em> 51, 4 (2015), 444–457.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mingze Sun, Daiyue Xue, Weipeng Wang, Qifu Hu, and Jianping Yu. 2021.

</span>
<span class="ltx_bibblock">Group-based deep transfer learning with mixed gate control for cross-domain recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">2021 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhulin Tao, Xiaohao Liu, Yewei Xia, Xiang Wang, Lifang Yang, Xianglin Huang, and Tat-Seng Chua. 2022.

</span>
<span class="ltx_bibblock">Self-supervised learning for multimedia recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">IEEE Transactions on Multimedia</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Changxin Tian, Zihan Lin, Shuqing Bian, Jinpeng Wang, and Wayne Xin Zhao. 2022.

</span>
<span class="ltx_bibblock">Temporal Contrastive Pre-Training for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 1925–1934.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Togashi et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Riku Togashi, Mayu Otani, and Shin’ichi Satoh. 2021.

</span>
<span class="ltx_bibblock">Alleviating cold-start problems in recommendation through pseudo-labelling over knowledge graph. In <em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">Proceedings of the 14th ACM international conference on web search and data mining</em>. 931–939.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Thanh Tran, Renee Sweeney, and Kyumin Lee. 2019.

</span>
<span class="ltx_bibblock">Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation. In <em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 245–254.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Can Wang, Jiawei Chen, Sheng Zhou, Qihao Shi, Yan Feng, and Chun Chen. 2021b.

</span>
<span class="ltx_bibblock">SamWalker++: recommendation with informative sampling strategy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">IEEE Transactions on Knowledge and Data Engineering</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chenyang Wang, Weizhi Ma, Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023a.

</span>
<span class="ltx_bibblock">Sequential recommendation with multiple contrast signals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">ACM Transactions on Information Systems</em> 41, 1 (2023), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2018a)</span>
<span class="ltx_bibblock">
Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018a.

</span>
<span class="ltx_bibblock">Graphgan: Graph representation learning with generative adversarial nets. In <em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019b.

</span>
<span class="ltx_bibblock">Multi-task feature learning for knowledge graph enhanced recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">The world wide web conference</em>. 2000–2010.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiayin Wang, Weizhi Ma, Chumeng Jiang, Min Zhang, Yuan Zhang, Biao Li, and Peng Jiang. 2023b.

</span>
<span class="ltx_bibblock">Measuring Item Global Residual Value for Fair Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib152.3.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 269–278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2017b)</span>
<span class="ltx_bibblock">
Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng Zhang, and Dell Zhang. 2017b.

</span>
<span class="ltx_bibblock">Irgan: A minimax game for unifying generative and discriminative information retrieval models. In <em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</em>. 515–524.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2017a)</span>
<span class="ltx_bibblock">
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017a.

</span>
<span class="ltx_bibblock">Knowledge graph embedding: A survey of approaches and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 29, 12 (2017), 2724–2743.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2018b)</span>
<span class="ltx_bibblock">
Qinyong Wang, Hongzhi Yin, Zhiting Hu, Defu Lian, Hao Wang, and Zi Huang. 2018b.

</span>
<span class="ltx_bibblock">Neural memory streaming recommender networks with adversarial training. In <em class="ltx_emph ltx_font_italic" id="bib.bib155.3.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2467–2475.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021c.

</span>
<span class="ltx_bibblock">Denoising implicit feedback for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">Proceedings of the 14th ACM international conference on web search and data mining</em>. 373–381.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Xin Wang, Yudong Chen, and Wenwu Zhu. 2021a.

</span>
<span class="ltx_bibblock">A survey on curriculum learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 44, 9 (2021), 4555–4576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019a.

</span>
<span class="ltx_bibblock">Kgat: Knowledge graph attention network for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 950–958.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2021d)</span>
<span class="ltx_bibblock">
Xi Wang, Iadh Ounis, and Craig Macdonald. 2021d.

</span>
<span class="ltx_bibblock">Leveraging review properties for effective recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">Proceedings of the Web Conference 2021</em>. 2209–2219.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang, and Tat-Seng Chua. 2020.

</span>
<span class="ltx_bibblock">Reinforced negative sampling over knowledge graph for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">Proceedings of the web conference 2020</em>. 99–109.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, et al<span class="ltx_text" id="bib.bib161.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Enhanced generative recommendation via content and collaboration integration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.4.1">arXiv preprint arXiv:2403.18480</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yu Wang, Hengrui Zhang, Zhiwei Liu, Liangwei Yang, and Philip S Yu. 2022.

</span>
<span class="ltx_bibblock">ContrastVAE: Contrastive Variational AutoEncoder for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 2056–2066.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang, and Song Wang. 2022.

</span>
<span class="ltx_bibblock">CLEAR: Contrastive Learning for API Recommendation. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock">Contrastive learning for cold-start recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib164.3.1">Proceedings of the 29th ACM International Conference on Multimedia</em>. 5382–5390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2019d)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019d.

</span>
<span class="ltx_bibblock">NPA: neural news recommendation with personalized attention. In <em class="ltx_emph ltx_font_italic" id="bib.bib165.3.1">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 2576–2584.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2019c)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng Huang, and Xing Xie. 2019c.

</span>
<span class="ltx_bibblock">Neural news recommendation with topic-aware news representation. In <em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">Proceedings of the 57th Annual meeting of the association for computational linguistics</em>. 1154–1159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021b.

</span>
<span class="ltx_bibblock">Empowering news recommendation with pre-trained language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib167.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Tao Qi, Qi Liu, Xuan Tian, Jie Li, Wei He, Yongfeng Huang, and Xing Xie. 2022b.

</span>
<span class="ltx_bibblock">Feedrec: News feed recommendation with various user feedbacks. In <em class="ltx_emph ltx_font_italic" id="bib.bib168.3.1">Proceedings of the ACM Web Conference 2022</em>. 2088–2097.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib169.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Haolun Wu, Chen Ma, Yingxue Zhang, Xue Liu, Ruiming Tang, and Mark Coates. 2022a.

</span>
<span class="ltx_bibblock">Adapting Triplet Importance of Implicit Feedback for Personalized Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib169.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 2148–2157.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021a.

</span>
<span class="ltx_bibblock">Self-supervised graph learning for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib170.3.1">Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval</em>. 726–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Le Wu, Lei Chen, Yonghui Yang, Richang Hong, Yong Ge, Xing Xie, and Meng Wang. 2019a.

</span>
<span class="ltx_bibblock">Personalized multimedia item and key frame recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">Proceedings of the 28th International Joint Conference on Artificial Intelligence</em>. 1431–1437.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023.

</span>
<span class="ltx_bibblock">A Survey on Large Language Models for Recommendation.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.19860" title="">http://arxiv.org/abs/2305.19860</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib173.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Qitian Wu, Yirui Gao, Xiaofeng Gao, Paul Weng, and Guihai Chen. 2019b.

</span>
<span class="ltx_bibblock">Dual sequential prediction models linking sequential recommendation and information dissemination. In <em class="ltx_emph ltx_font_italic" id="bib.bib173.3.1">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 447–457.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024.

</span>
<span class="ltx_bibblock">Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">Proceedings of the ACM on Web Conference</em>. 3566–3575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2019e)</span>
<span class="ltx_bibblock">
Yongkai Wu, Lu Zhang, and Xintao Wu. 2019e.

</span>
<span class="ltx_bibblock">Counterfactual fairness: Unidentification, bound and algorithm. In <em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">Proceedings of the twenty-eighth international joint conference on Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023.

</span>
<span class="ltx_bibblock">Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.10933" title="">http://arxiv.org/abs/2306.10933</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, and Liefeng Bo. 2022.

</span>
<span class="ltx_bibblock">Multi-Behavior Graph Neural Networks for Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">IEEE Transactions on Neural Networks and Learning Systems</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Lianghao Xia, Chao Huang, Yong Xu, Peng Dai, Xiyue Zhang, Hongsheng Yang, Jian Pei, and Liefeng Bo. 2021.

</span>
<span class="ltx_bibblock">Knowledge-enhanced hierarchical graph transformer network for multi-behavior recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib178.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 35. 4486–4493.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruobing Xie, Cheng Ling, Yalong Wang, Rui Wang, Feng Xia, and Leyu Lin. 2021.

</span>
<span class="ltx_bibblock">Deep feedback network for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib179.3.1">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</em>. 2519–2525.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2022b.

</span>
<span class="ltx_bibblock">A peep into the future: Adversarial future encoding in recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</em>. 1177–1185.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022a.

</span>
<span class="ltx_bibblock">Contrastive learning for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib181.3.1">2022 IEEE 38th international conference on data engineering (ICDE)</em>. IEEE, 1259–1273.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kerui Xu, Jingxuan Yang, Jun Xu, Sheng Gao, Jun Guo, and Ji-Rong Wen. 2021.

</span>
<span class="ltx_bibblock">Adapting user preference to online feedback in multi-round conversational recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib182.3.1">Proceedings of the 14th ACM international conference on web search and data mining</em>. 364–372.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib183.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Lanling Xu, Jianxun Lian, Wayne Xin Zhao, Ming Gong, Linjun Shou, Daxin Jiang, Xing Xie, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">Negative sampling for contrastive representation learning: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.3.1">arXiv preprint arXiv:2206.00212</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Carl Yang, Lanxiao Bai, Chao Zhang, Quan Yuan, and Jiawei Han. 2017.

</span>
<span class="ltx_bibblock">Bridging collaborative filtering and semi-supervised learning: a neural approach for poi recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib184.3.1">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib185.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Carl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, and Jiawei Han. 2020b.

</span>
<span class="ltx_bibblock">Heterogeneous network representation learning: A unified framework with survey and benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 34, 10 (2020), 4854–4873.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. 2023a.

</span>
<span class="ltx_bibblock">PALR: Personalization Aware LLMs for Recommendation.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.07622" title="">http://arxiv.org/abs/2305.07622</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Haoran Yang, Hongxu Chen, Lin Li, S Yu Philip, and Guandong Xu. 2021a.

</span>
<span class="ltx_bibblock">Hyper meta-path contrastive learning for multi-behavior recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib187.3.1">2021 IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 787–796.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib188.2.2.1">.</span> (2020c)</span>
<span class="ltx_bibblock">
Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H Chi. 2020c.

</span>
<span class="ltx_bibblock">Mixed negative sampling for learning two-tower neural networks in recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib188.3.1">Companion Proceedings of the Web Conference 2020</em>. 441–447.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib189.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Liangwei Yang, Shengjie Wang, Yunzhe Tao, Jiankai Sun, Xiaolong Liu, Philip S Yu, and Taiqing Wang. 2023b.

</span>
<span class="ltx_bibblock">Dgrec: Graph neural network for recommendation with diversified embedding generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib189.3.1">Proceedings of the sixteenth ACM international conference on web search and data mining</em>. 661–669.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Mengyue Yang, Quanyu Dai, Zhenhua Dong, Xu Chen, Xiuqiang He, and Jun Wang. 2021b.

</span>
<span class="ltx_bibblock">Top-n recommendation with counterfactual user preference simulation. In <em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>. 2342–2351.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib191.2.2.1">.</span> (2021c)</span>
<span class="ltx_bibblock">
Sean Bin Yang, Chenjuan Guo, Jilin Hu, Jian Tang, and Bin Yang. 2021c.

</span>
<span class="ltx_bibblock">Unsupervised Path Representation Learning with Curriculum Negative Sampling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.3.1">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib192.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022a.

</span>
<span class="ltx_bibblock">Knowledge graph contrastive learning for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib192.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1434–1443.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib193.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chenliang Li. 2022b.

</span>
<span class="ltx_bibblock">Multi-behavior hypergraph-enhanced transformer for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib193.3.1">Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining</em>. 2263–2274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib194.2.2.1">.</span> (2021d)</span>
<span class="ltx_bibblock">
Yonghui Yang, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2021d.

</span>
<span class="ltx_bibblock">Enhanced graph learning for collaborative filtering via mutual information maximization. In <em class="ltx_emph ltx_font_italic" id="bib.bib194.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 71–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib195.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. 2020a.

</span>
<span class="ltx_bibblock">Understanding negative sampling in graph representation learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib195.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1666–1676.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib196.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Hongzhi Yin, Qinyong Wang, Kai Zheng, Zhixu Li, Jiali Yang, and Xiaofang Zhou. 2019.

</span>
<span class="ltx_bibblock">Social influence-based group representation learning for group recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib196.3.1">2019 IEEE 35th International Conference on Data Engineering (ICDE)</em>. IEEE, 566–577.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib197.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018.

</span>
<span class="ltx_bibblock">Graph convolutional neural networks for web-scale recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib197.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 974–983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib198.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Junliang Yu, Min Gao, Jundong Li, Hongzhi Yin, and Huan Liu. 2018.

</span>
<span class="ltx_bibblock">Adaptive implicit friends identification over heterogeneous network for social recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib198.3.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 357–366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib199.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiangxing Yu, Hong Zhu, Chih-Yao Chang, Xinhua Feng, Bowen Yuan, Xiuqiang He, and Zhenhua Dong. 2020.

</span>
<span class="ltx_bibblock">Influence function for unbiased recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib199.3.1">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1929–1932.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zang et al<span class="ltx_text" id="bib.bib200.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022.

</span>
<span class="ltx_bibblock">A survey on cross-domain recommendation: taxonomies, methods, and future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib200.3.1">ACM Transactions on Information Systems</em> 41, 2 (2022), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib201.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608860" title="">https://doi.org/10.1145/3604915.3608860</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib202.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021a.

</span>
<span class="ltx_bibblock">Double-scale self-supervised hypergraph learning for group recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib202.3.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>. 2557–2567.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib203.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021b.

</span>
<span class="ltx_bibblock">Causerec: Counterfactual user sequence synthesis for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib203.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 367–377.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib204.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, and Wei Ning. 2024.

</span>
<span class="ltx_bibblock">On Practical Diversified Recommendation with Controllable Category Diversity Framework. In <em class="ltx_emph ltx_font_italic" id="bib.bib204.3.1">Companion Proceedings of the ACM on Web Conference 2024</em>. 255–263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib205.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Weinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013.

</span>
<span class="ltx_bibblock">Optimizing top-n collaborative filtering via dynamic negative item sampling. In <em class="ltx_emph ltx_font_italic" id="bib.bib205.3.1">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib206.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jun Zhao, Zhou Zhou, Ziyu Guan, Wei Zhao, Wei Ning, Guang Qiu, and Xiaofei He. 2019.

</span>
<span class="ltx_bibblock">Intentgc: a scalable graph convolution framework fusing heterogeneous information for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib206.3.1">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2347–2357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib207.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Jinpeng Wang, Yulan He, Ji-Rong Wen, Edward Y Chang, and Xiaoming Li. 2016.

</span>
<span class="ltx_bibblock">Mining product adopter information from online reviews for improving product recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib207.3.1">ACM Transactions on Knowledge Discovery from Data (TKDD)</em> 10, 3 (2016), 1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib208.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al<span class="ltx_text" id="bib.bib208.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib208.4.1">arXiv preprint arXiv:2303.18223</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib209.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018.

</span>
<span class="ltx_bibblock">Recommendations with negative feedback via pairwise deep reinforcement learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib209.3.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 1040–1048.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib210.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018.

</span>
<span class="ltx_bibblock">DRN: A deep reinforcement learning framework for news recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib210.3.1">Proceedings of the 2018 world wide web conference</em>. 167–176.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib211.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiaolin Zheng, Jiajie Su, Weiming Liu, and Chaochao Chen. 2022.

</span>
<span class="ltx_bibblock">DDGHM: Dual Dynamic Graph with Hybrid Metric Training for Cross-Domain Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib211.3.1">Proceedings of the 30th ACM International Conference on Multimedia</em>. 471–481.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib212.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2020.

</span>
<span class="ltx_bibblock">Disentangling user interest and popularity bias for recommendation with causal embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib212.3.1">arXiv preprint arXiv:2006.11011</em> (2020), 64.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib213.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, and Jun Gao. 2018.

</span>
<span class="ltx_bibblock">Atrank: An attention-based user behavior modeling framework for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib213.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib214.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020.

</span>
<span class="ltx_bibblock">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In <em class="ltx_emph ltx_font_italic" id="bib.bib214.3.1">Proceedings of the 29th ACM international conference on information &amp; knowledge management</em>. 1893–1902.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Shen (2023)</span>
<span class="ltx_bibblock">
Xin Zhou and Zhiqi Shen. 2023.

</span>
<span class="ltx_bibblock">A tale of two graphs: Freezing and denoising graph structures for multimodal recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">Proceedings of the 31st ACM International Conference on Multimedia</em>. 935–943.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib216.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yao Zhou, Jianpeng Xu, Jun Wu, Zeinab Taghavi, Evren Korpeoglu, Kannan Achan, and Jingrui He. 2021.

</span>
<span class="ltx_bibblock">PURE: Positive-unlabeled recommendation with generative adversarial network. In <em class="ltx_emph ltx_font_italic" id="bib.bib216.3.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 2409–2419.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib217.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Learning tree-based deep model for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib217.3.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 1079–1088.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib218.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qiannan Zhu, Haobo Zhang, Qing He, and Zhicheng Dou. 2022.

</span>
<span class="ltx_bibblock">A Gain-Tuning Dynamic Negative Sampler for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib218.3.1">Proceedings of the ACM Web Conference 2022</em>. 277–285.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo et al<span class="ltx_text" id="bib.bib219.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jianhuan Zhuo, Qiannan Zhu, Yinliang Yue, and Yuhong Zhao. 2022.

</span>
<span class="ltx_bibblock">Learning explicit user interest boundary for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib219.3.1">Proceedings of the ACM Web Conference 2022</em>. 193–202.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib220.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ding Zou, Wei Wei, Feida Zhu, Chuanyu Xu, Tao Zhang, and Chengfu Huo. 2024.

</span>
<span class="ltx_bibblock">Knowledge Enhanced Multi-intent Transformer Network for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib220.3.1">Companion Proceedings of the ACM on Web Conference 2024</em>. 1–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 11 12:47:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
