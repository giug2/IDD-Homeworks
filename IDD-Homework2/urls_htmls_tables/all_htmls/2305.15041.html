<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.15041] Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science</title><meta property="og:description" content="Large Language Models (LLMs) have democratized synthetic data generation, which in turn has the potential to simplify and broaden a wide gamut of NLP tasks.
Here, we tackle a pervasive problem in synthetic data generat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.15041">

<!--Generated on Thu Feb 29 06:02:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generating Faithful Synthetic Data with Large Language Models: 
<br class="ltx_break">A Case Study in Computational Social Science</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Veniamin Veselovsky<sup id="id1.1.1" class="ltx_sup"><math id="id1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\dagger</annotation></semantics></math></sup>,
Manoel Horta Ribeiro<sup id="id2.2.2" class="ltx_sup"><math id="id2.2.2.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id2.2.2.m1.1a"><mo id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\dagger</annotation></semantics></math></sup>,
Akhil Arora<sup id="id3.3.3" class="ltx_sup"><math id="id3.3.3.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id3.3.3.m1.1a"><mo id="id3.3.3.m1.1.1" xref="id3.3.3.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id3.3.3.m1.1b"><ci id="id3.3.3.m1.1.1.cmml" xref="id3.3.3.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m1.1c">\dagger</annotation></semantics></math></sup>,
<br class="ltx_break"><span id="id4.4.4" class="ltx_text ltx_font_bold">Martin Josifoski<sup id="id4.4.4.1" class="ltx_sup"><math id="id4.4.4.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id4.4.4.1.m1.1a"><mo id="id4.4.4.1.m1.1.1" xref="id4.4.4.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id4.4.4.1.m1.1b"><ci id="id4.4.4.1.m1.1.1.cmml" xref="id4.4.4.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.1.m1.1c">\dagger</annotation></semantics></math></sup></span>,
<span id="id5.5.5" class="ltx_text ltx_font_bold">Ashton Anderson<sup id="id5.5.5.1" class="ltx_sup"><math id="id5.5.5.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="id5.5.5.1.m1.1a"><mo id="id5.5.5.1.m1.1.1" xref="id5.5.5.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id5.5.5.1.m1.1b"><times id="id5.5.5.1.m1.1.1.cmml" xref="id5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.1.m1.1c">*</annotation></semantics></math></sup></span>,
<span id="id7.7.7" class="ltx_text ltx_font_bold">Robert West<sup id="id6.6.6.1" class="ltx_sup"><math id="id6.6.6.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id6.6.6.1.m1.1a"><mo id="id6.6.6.1.m1.1.1" xref="id6.6.6.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id6.6.6.1.m1.1b"><ci id="id6.6.6.1.m1.1.1.cmml" xref="id6.6.6.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.1.m1.1c">\dagger</annotation></semantics></math></sup>
<br class="ltx_break"><sup id="id7.7.7.2" class="ltx_sup"><math id="id7.7.7.2.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id7.7.7.2.m1.1a"><mo id="id7.7.7.2.m1.1.1" xref="id7.7.7.2.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id7.7.7.2.m1.1b"><ci id="id7.7.7.2.m1.1.1.cmml" xref="id7.7.7.2.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.2.m1.1c">\dagger</annotation></semantics></math></sup></span> EPFL <sup id="id8.8.8" class="ltx_sup"><math id="id8.8.8.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="id8.8.8.m1.1a"><mo id="id8.8.8.m1.1.1" xref="id8.8.8.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id8.8.8.m1.1b"><times id="id8.8.8.m1.1.1.cmml" xref="id8.8.8.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m1.1c">*</annotation></semantics></math></sup>University of Toronto 
<br class="ltx_break"><span id="id9.9.id1" class="ltx_text ltx_font_typewriter">firstname.lastnames@epfl.ch</span>,
<span id="id10.10.id2" class="ltx_text ltx_font_typewriter">ashton@cs.toronto.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Large Language Models (LLMs) have democratized synthetic data generation, which in turn has the potential to simplify and broaden a wide gamut of NLP tasks.
Here, we tackle a pervasive problem in synthetic data generation: its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is <em id="id11.id1.1" class="ltx_emph ltx_font_italic">unfaithful</em>).
In a case study on sarcasm detection, we study three strategies to increase the faithfulness of synthetic data: grounding, filtering, and taxonomy-based generation.
We evaluate these strategies using the performance of classifiers trained with generated synthetic data on real-world data.
While all three strategies improve the performance of classifiers, we find that grounding works best for the task at hand.
As synthetic data generation plays an ever-increasing role in NLP research, we expect this work to be a stepping stone in improving its utility.
We conclude this paper with some recommendations on how to generate high(er)-fidelity synthetic data for specific tasks.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2305.15041/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Depiction of the proposed strategies to increase the faithfulness of synthetically generated data. On the left-hand side, we depict different prompting strategies: asking an LLM to generate synthetic data with a simple prompt (<span id="S0.F1.5.1" class="ltx_text ltx_font_bold">Simple)</span>;
grounding the synthetic data generation with real-world examples (<span id="S0.F1.6.2" class="ltx_text ltx_font_bold">Grounding</span>-rewrite);
and providing a taxonomy along with your prompt (<span id="S0.F1.7.3" class="ltx_text ltx_font_bold">Taxonomy</span>). We also train a discriminator to distinguish between real and fake prompts and filter the data (as indicated by the dotted orange boxes on the right-hand side; <span id="S0.F1.8.4" class="ltx_text ltx_font_bold">Filtering</span>).
</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">From data annotation <cite class="ltx_cite ltx_citemacro_cite">Gilardi et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> to dataset creation <cite class="ltx_cite ltx_citemacro_cite">Josifoski et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, synthetic data offers previously unseen flexibility in the models we train <cite class="ltx_cite ltx_citemacro_cite">Eldan and Li (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> and in defining what and how we study the world around us <cite class="ltx_cite ltx_citemacro_cite">Ziems et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>.
Further, large language models (hereinafter LLMs) are now easily accessible through APIs, substantially decreasing the expertise and the time necessary to generate synthetic data and labels.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Here, we examine a pervasive problem in synthetic data generation with LLMs: <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">faithfulness</span>.
The generative distribution of synthetic data created by LLMs often differs from the distribution of real-world data that we care about <cite class="ltx_cite ltx_citemacro_cite">Alaa et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>.
For instance, if we ask LLMs to generate tweets, these will likely be much better written than real tweets, and the topics and themes of those are likely to be less diverse.
This is problematic, as classifiers trained on synthetic data would be systematically biased and may not perform well in real-world contexts.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We study three strategies to increase the faithfulness of synthetic data generated by LLMs: grounding, filtering, and taxonomy-based generation.
As illustrated in Fig. <a href="#S0.F1" title="Figure 1 ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, grounding consists of providing real-world examples from a training set in the LLM prompt;
filtering consists of using a discriminator model (trained to distinguish real and synthetic data) to cull unfaithful synthetic data;
and taxonomy-based generation consists of including a taxonomy in the prompt to encourage diversity.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We evaluate the aforementioned proposed strategies with a case study in Computational Social Science (CSS), a multidisciplinary field where easily accessible synthetic data and labels may be transformative in the years to come <cite class="ltx_cite ltx_citemacro_cite">Bail (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>.
Research in CSS often uses simple classifiers to estimate a linguistic characteristic or trait (referred to in the paper as a construct) in large text corpora, often obtained from the Web <cite class="ltx_cite ltx_citemacro_cite">Salganik (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>.
In that context, LLMs have been used to directly annotate the data in zero-shot fashion <cite class="ltx_cite ltx_citemacro_cite">Ziems et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, and, more relevant to the work at hand, to create synthetic data to train models in complex or low-resource tasks <cite class="ltx_cite ltx_citemacro_cite">Møller et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the latter context, we consider the task of sarcasm detection, and using an existing dataset evaluate the performance of each of the proposed strategies in increasing the faithfulness of synthetically generated data.
Using the macro-F1 of the classifiers trained with different prompting strategies as a proxy for the faithfulness of synthetic data, we find that grounding provides the best performance our of all classifiers trained with synthetic data.
However, the model still performs worse in terms of macro-F1 than zero-shot ChatGPT annotation and a model trained on the real data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Data augmentation.</span>
In low-resource and unbalanced settings, augmenting datasets with synthetic data can improve model performance in a variety of NLP tasks,
including relation extraction <cite class="ltx_cite ltx_citemacro_cite">Papanikolaou and Pierleoni (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, sarcasm detection <cite class="ltx_cite ltx_citemacro_cite">Abaskohi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, translation <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite>, and sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">Maqsud (<a href="#bib.bib15" title="" class="ltx_ref">2015</a>)</cite>; see <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> for a comprehensive survey.
Specifically relevant to this paper is the work of <cite class="ltx_cite ltx_citemacro_citet">Møller et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, which uses ChatGPT to generate new samples for sentiment, hate speech, and a social dimension, a low-resource task.
Finally, <cite class="ltx_cite ltx_citemacro_citet">Anaby-Tavor et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> proposed a general methodology for fine-tuning a language model on small datasets. The authors highlight that the synthetic data was unfaithful to the real-world data distribution, thus warranting a filtering scheme to remove unfaithful data points.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Synthetic dataset creation.</span>
Recent work has stretched beyond data augmentation to creating fully synthetic datasets.
<cite class="ltx_cite ltx_citemacro_citet">Eldan and Li (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> used LLMs to create “Tiny Stories,” showcasing how small a language model can learn the language of 2 to 3-year-old children.
This paper relied on a form of “grounding” to encourage diversity in the concepts discussed.
Another work by <cite class="ltx_cite ltx_citemacro_citet">Josifoski et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> sampled knowledge graph triplets and generated texts using GPT-3. They then fine-tuned a model entirely on the synthetic data, and noted that the data was dissimilar from real human data.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Synthetic data as a proxy for humans.</span>
LLMs can also act as good proxies for specific human sub-populations <cite class="ltx_cite ltx_citemacro_cite">Argyle et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>, leading to a series of studies using LLMs as “silicon samples” <cite class="ltx_cite ltx_citemacro_cite">Argyle et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>); Horton (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>); Dillion et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. Typically, these analyses have been done through a variant of controlled text generation (review available here <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>). Further, an ever-increasing body of work illustrated the good performance of using LLMs as a proxy for human labeling <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>); Gilardi et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>); Ziems et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Naïve synthetic data generation with LLMs, e.g., the <span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Simple</span> strategy in Fig. <a href="#S0.F1" title="Figure 1 ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, can lead to data that is unfaithful to the underlying real-world data distribution <cite class="ltx_cite ltx_citemacro_cite">Josifoski et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>.
This paper’s contribution is to propose and evaluate prompting strategies that allow us to address this issue.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">sarcasm detection</em> dataset from the SemEval-2022 Task 6 <cite class="ltx_cite ltx_citemacro_cite">Farha et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>.
The train set includes over two thousand self-disclosed instances of sarcasm being shared on Twitter.
The reason we choose sarcasm is because it is an inherently difficult task to annotate, and construct to capture. Sarcastic texts are highly context-specific and ambiguous by nature. Annotating a sarcastic corpus has been a long standing problem, with sarcastic comments representing &lt; 1% of all text on social media (Reddit, for example).
This renders it infeasible to blindly annotate texts since finding an instance of sarcasm is like searching for a needle in a haystack.
Consequently, papers have traditionally relied on various heuristics to generate these datasets—like using the self-disclosed /s tag or asking users to share their own sarcastic Tweets (our task). These heuristics, however, lead to noisy labels and annotator bias <cite class="ltx_cite ltx_citemacro_cite">Oprea and Magdy (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">When evaluating how well our synthetic data captures a linguistic construct, we make the following assumption: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">if a construct is properly present in a synthetic dataset, then a model fine-tuned on that dataset will successfully generalize to a real human dataset.</span> We thus evaluate our synthetic data in three steps.
First, we split human-annotated data into two groups train and test, throwing away the labels for our train data.
Second, we synthetically generate a new corpus through our various prompting strategies (see below).
Third, we fine-tune a model on the various generated synthetic datasets, and evaluate them on the test portion of the human-annotated data.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Prompting</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To understand where synthetic data fails, we begin our analysis by manually inspecting the generated data.
Three co-authors reviewed hundreds of examples of synthetically generated vs. real sarcastic texts and annotated their differences. We found that synthetic data generated with simple prompts:
1) exhibits a lack of topical diversity, i.e., it centered around a few topics of discussion;
2) lacks diversity in the construct of interest (namely sarcasm<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>There are many ways a linguistic construct like sarcasm can manifest (irony, over- or under-statement, satire, etc.), and typically the language model would retreat to superficial notions of sarcasm like beginning sentences with “Oh” or “Wow”.</span></span></span>); and
3) are not well stylistically aligned with real data; authors could easily discriminate between synthetic and real texts. These three assumptions and corresponding prompt designs are described in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Prompting ‣ 3 Methods ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The prompts in entirety are available at <a target="_blank" href="https://github.com/epfl-dlab/faithful-data-gen" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/epfl-dlab/faithful-data-gen</a>.</span></span></span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We propose three prompting strategies to account for these limitations, each building off the next.
Examples of how the prompts build off each other are illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.4 Models ‣ 3 Methods ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and discussed below.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Goal</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Strategy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">Diversity in construct</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">Taxonomy creation</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">Diversity in topics</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">Grounding</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Stylistic matching</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Rewrite</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Description of objectives in synthetic data generation alongside specific strategies to achieve them.</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Grounding.</span>
We encourage topical diversity by <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">grounding</em> the generations in real textual data. Specifically, in the prompt, we include an example of a real text and ask the model to either
1) generate new semantically similar examples (like in <cite class="ltx_cite ltx_citemacro_citet">Møller et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> or <cite class="ltx_cite ltx_citemacro_citet">Eldan and Li (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>) or 2) rewrite the input text (style transfer).</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.2" class="ltx_p"><span id="S3.SS3.p4.2.1" class="ltx_text ltx_font_bold">Taxonomy-based generation.</span>
We break up generation into two steps, asking the LLM to
1) theorize <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">k</annotation></semantics></math>-ways a text can possess a specific construct and then sample across these <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">k</annotation></semantics></math> approaches, and
2) rewrite the text according to a specific variant of the construct.
The idea here is that generation based on an initial taxonomy can cover a wider segment of <em id="S3.SS3.p4.2.2" class="ltx_emph ltx_font_italic">how</em> a text can actually convey a construct, improving the downstream model.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Filtering.</span>
We fine-tune a model to discriminate between real and synthetic text and run that on the full batch of synthetically generated samples from the <span id="S3.SS3.p5.1.2" class="ltx_text ltx_font_bold">Grounding</span> data.
We then cull the examples that have a high likelihood of being synthetic.
We do this because, at times, the synthetic data has artifacts that are always associated with a construct.
Specifically, we fine-tune a BERT model to distinguish between the first decoding (i.e., if we generate <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mn id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><cn type="integer" id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">10</annotation></semantics></math> sentences, we only take the first sentence) and the real text to include a specific construct.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">For simple prompts, we ask the LLM to generate sarcastic and not-sarcastic text, and for prompts using <span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">grounding</span>, we polarize each point in our dataset into two directions, i.e., making it both sarcastic and not-sarcastic.
In practice, this means that for each prompt in Fig. <a href="#S0.F1" title="Figure 1 ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we have an alternate version where we substitute the word “sarcastic” for “not-sarcastic”, resulting in a synthetic dataset that is balanced across the two classes.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Models</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">Generative model.</span> To generate the synthetic data, we used ChatGPT.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a></span></span></span>
The generation parameters for the model were set to <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">temperature: 1</span>,
<span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">top p: 1</span>,
<span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_typewriter">frequency penalty: 0.5</span>,
<span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_typewriter">presence penalty: 0.4</span>,
<span id="S3.SS4.p1.1.6" class="ltx_text ltx_font_typewriter">max tokens: 700</span>.
We chose these parameters to maximize the diversity in the decoded text.
The frequency penalty reduces the probability of a word depending on the frequency that it occurs, while the presence penalty puts a flat cost on each word when it occurs in the text.
These two forms of penalties help encourage the model to produce higher perplexity texts instead of selecting the next most probable word.
Moreover, temperature scaling produces a more shallow distribution over the next tokens, and a top-<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">p</annotation></semantics></math> set to 1 will cause us to consider all these tokens.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The generative data is then processed by removing artifacts of the generation.
We defined these rules based on manual examination. The two most common problems that occurred were the model responding to the request in the affirmative (“Sure, here you go:”) and outlining which taxonomy it uses prior to generating the sentence (only present in the taxonomy generation prompting).
Both of these issues were addressed by splitting the first colon character “:” and restricting to text after it.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Fine-tuned model.</span>
Similar to previous work, we fine-tune a <span id="S3.SS4.p3.1.2" class="ltx_text ltx_font_typewriter">E5-base</span> model on the synthetic data <cite class="ltx_cite ltx_citemacro_cite">Møller et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>.
This model was originally trained using a contrastive loss and achieves strong performance in a fine-tuned classification setting.
During fine-tuning, we kept the settings from previous work with a learning rate of <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="2e^{-5}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mn id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">​</mo><msup id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.p3.1.m1.1.1.3.2" xref="S3.SS4.p3.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS4.p3.1.m1.1.1.3.3" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml"><mo id="S3.SS4.p3.1.m1.1.1.3.3a" xref="S3.SS4.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS4.p3.1.m1.1.1.3.3.2" xref="S3.SS4.p3.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><times id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">2</cn><apply id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.2">𝑒</ci><apply id="S3.SS4.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3"><minus id="S3.SS4.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.SS4.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.p3.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">2e^{-5}</annotation></semantics></math>, batch size of 32, and trained for 10 epochs.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.15041/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="153" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our prompting approach consists of four modular steps. (1) Initiate the model to generate an initial set of 10 data points. (2) Apply a grounding technique as the model generates these 10 data points. (3) Further augment the grounding process by providing the model with an initial taxonomy. (4) Lastly, the results from the grounding phase are filtered through a real-synthetic classifier to ensure their authenticity.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text">Prompting Strategy</span></th>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><em id="S3.T2.1.1.1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Sarcasm</em></td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r">Accuracy</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Macro-F1</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center">Believability</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<th id="S3.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Simple</th>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.3.3.2.1" class="ltx_text ltx_font_bold">0.71</span></td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.48</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.04</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<th id="S3.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Grounding</th>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.67</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.4.4.3.1" class="ltx_text ltx_font_bold">0.55</span></td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.13</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<th id="S3.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Grounding (rewrite)</th>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.70</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.5.5.3.1" class="ltx_text ltx_font_bold">0.55</span></td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.15</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<th id="S3.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Grounding + Taxonomy</th>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.67</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.51</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">0.20</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<th id="S3.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Grounding + Filtering</th>
<td id="S3.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.27</td>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.26</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.7.7.4.1" class="ltx_text ltx_font_bold">0.56</span></td>
</tr>
<tr id="S3.T2.1.8.8" class="ltx_tr">
<th id="S3.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Groundtruth annotations</th>
<td id="S3.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.72</td>
<td id="S3.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.60</td>
<td id="S3.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_tt">0.95</td>
</tr>
<tr id="S3.T2.1.9.9" class="ltx_tr">
<th id="S3.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">All non-sarcastic</th>
<td id="S3.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.77</td>
<td id="S3.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.43</td>
<td id="S3.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">—</td>
</tr>
<tr id="S3.T2.1.10.10" class="ltx_tr">
<th id="S3.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Zero-shot ChatGPT</th>
<td id="S3.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">0.60</td>
<td id="S3.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">0.59</td>
<td id="S3.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">—</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>For different prompting strategies (rows 2 to 6) and baselines (rows 7 to 10), we show the accuracy, macro-F1 score, and believability in a held-out test set.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Model performance.</span>
We show the accuracy and the macro-F1 score for the different prompting strategies in the second and third columns in Table <a href="#S3.T2" title="Table 2 ‣ 3.4 Models ‣ 3 Methods ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
A baseline predicting all data points in the training set as not-sarcastic (“All non-sarcastic”) yields an accuracy of 0.72 and a macro-F1 score of 0.43.
In practice, we find that models trained in all prompting strategies perform worse accuracy-wise than this baseline, and thus it is more meaningful to compare their macro-F1 score.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">We find that the “simple” prompting strategy generalized the worse (macro-F1 score of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="0.48" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">0.48</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="float" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">0.48</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">0.48</annotation></semantics></math>), perhaps due to the lack of topical and construct diversity in the synthetically generated data.
Note that here we prompted the model to generate 10 random instances of sarcastic and non-sarcastic texts five hundred times.
The two synthetic datasets that performed best (macro-F1 score: <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="0.55" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">0.55</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="float" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">0.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">0.55</annotation></semantics></math>) were derived from the “grounding” prompting strategy, where the prompt asked the LLM to, given an example, generate semantically similar text (“Grounding,” the 2nd row) or re-write it (“Grounding (rewrite),” the 3rd row).
Prompting with grounding and an LLM-generated taxonomy yielded a result between the “simple” and the “grounding” prompting strategies (“Grounding + Taxonomy,” macro-F1 score: <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="0.51" display="inline"><semantics id="S4.p2.3.m3.1a"><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">0.51</mn><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><cn type="float" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">0.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">0.51</annotation></semantics></math>).
Last, grounding the prompt and then filtering responses that were classified as synthetic with our discriminator yielded poor results (“Grounding + Filtering,” macro-F1 score <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="0.26" display="inline"><semantics id="S4.p2.4.m4.1a"><mn id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">0.26</mn><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><cn type="float" id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">0.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">0.26</annotation></semantics></math>).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Finally, we note that zero-shot ChatGPT actually yields a higher macro-F1 score (<math id="S4.p3.1.m1.1" class="ltx_Math" alttext="0.60" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">0.60</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="float" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">0.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">0.60</annotation></semantics></math>) than smaller models trained with synthetically generated data.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Believability.</span>
For each synthetic dataset generated, we further estimate how effective they are at fooling a synthetic vs. real classifier (which we refer to as the dataset’s believability).
The discriminator model was trained on individual generations of sarcastic and non-sarcastic text and then fine-tuned to predict if a text is sarcastic or not. We report the fraction of each dataset predicted to be real by this classifier in the 4th row of Table <a href="#S3.T2" title="Table 2 ‣ 3.4 Models ‣ 3 Methods ‣ Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, “Believability.”
Note that for the groundtruth annotations (which are all real), we obtain a score of 95%, meaning that the model believes that 95% of the text was considered to be real by the classifier.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.5" class="ltx_p">The dataset with the highest “believability” is the one created using the grounding and filtering strategies (“Grounding + Filtering,” believability <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="0.56" display="inline"><semantics id="S4.p5.1.m1.1a"><mn id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">0.56</mn><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><cn type="float" id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">0.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">0.56</annotation></semantics></math>). However, this metric may not capture faithfulness accurately in this case, as the criteria used for filtering are the same as the ones used to calculate the “believability” of a dataset.
Thus, of the remaining strategies, the “Grounding + Taxonomy” strategy presents the highest performance (predicted real: <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="0.20" display="inline"><semantics id="S4.p5.2.m2.1a"><mn id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">0.20</mn><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><cn type="float" id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">0.20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">0.20</annotation></semantics></math>), suggesting that data aided by a taxonomy picks up on fewer artifacts.
Unsurprisingly, the “Simple” strategy performs the worst, (predicted real: <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="0.04" display="inline"><semantics id="S4.p5.3.m3.1a"><mn id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">0.04</mn><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><cn type="float" id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">0.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">0.04</annotation></semantics></math>), which is aligned with our qualitative analysis of the data, where we noted that most data points contain superficial sarcastic indicators like “Oh”, “Wow”, and question marks (“?”).
Last, grounded approaches perform better than the simple strategy (predicted real: <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="0.13" display="inline"><semantics id="S4.p5.4.m4.1a"><mn id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">0.13</mn><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><cn type="float" id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">0.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">0.13</annotation></semantics></math> for “Grounding” and <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="0.15" display="inline"><semantics id="S4.p5.5.m5.1a"><mn id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml">0.15</mn><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><cn type="float" id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1">0.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">0.15</annotation></semantics></math> for “Grounding (rewrite)”).</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Key takeaways.</span>
Through the process of generating synthetic data, we drew takeaways that can be beneficial for future studies using synthetically generated data for either augmentation or as the entire dataset. We list these findings here:</p>
</div>
<div id="S4.p7" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">When producing synthetic data, it is necessary to generate several sentences for each individual real sample. Typically, the later generations capture more interesting forms of sarcasm than the initial generation and cover a broader range of topics.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Grounding data is a key aspect of generating synthetic data. Without grounding, the model tends to generate texts that are specialized in terms of topics discussed and constructed used.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Taxonomy creation can be useful for making the data appear real. However, it performs worse than grounding at staying true to the underlying construct. One potential reason for this is that we assume a uniform distribution over subvariants of sarcasm. This assumption is unlikely to hold in practice—in real life, there are a few types that represent most forms of sarcasm, with the rest representing a long tail. Applying a prior to the types of sarcasm we are likely can lead to more realistic generations.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Filtering works poorly. This result is surprising given its prevalence in other data augmentation studies. This may be improved through a better classifier.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">A small capacity model like E5 may not be capable of capturing complex linguistic features like sarcasm. It may be a worthwhile effort to fine-tune on a larger model like <span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_typewriter">Flan-T5</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Summary of findings</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Investigating the ability of LLMs to generate <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">faithful</em> synthetic data, we find that simple prompting strategies result in data that lacks diversity and differs stylistically from real-world data.
To address these issues, we propose a suite of improved prompting strategies, namely, ‘grounding,’ ‘filtering,’ and ‘taxonomy-based generation,’ which we qualitatively find to generate samples that are more faithful to the real-world data distribution.
Further, comparing the performance of classifiers trained with synthetic data generated using our proposed strategies on a downstream task of sarcasm detection, we find that ‘grounding’ resulted in the highest improvement, thereby indicating the importance of closely capturing topical diversity for the considered tasks.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implications</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We argue that the implications of the aforementioned findings are three-fold.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">First, our results suggest that synthetic data generation can be a resource-friendly alternative to human annotation achieving results five macro-F1 points worse than zero-shot annotation and a model trained on the real data.
With only a few examples of data of the kind researchers wish to study (e.g., sarcastic tweets), they could bootstrap a synthetic dataset that can be used to train relatively simple, yet effective and easily deployable models.
This strategy could also alleviate privacy concerns associated with using real-world data, allowing the study of sensitive topics without relying on collecting data from contexts where personally identifiable information is present (e.g., social media).</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Second, synthetic data generation could be a helpful strategy for training future (potentially smaller) language models.
Previous work has shown that smaller language models fine-tuned using well-curated samples from a constrained domain can outperform larger models on specific tasks <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, and with our prompting strategies, this fine-tuning process could be bootstrapped with another language model, i.e., one could automatically generate this well-curated sample.
More broadly, as language models scale up, and organizations require more and more data to train these models, synthetically generated data may be needed to continue the improvement of these models. Our work could be seen as a stepping stone for more research in this direction.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Finally, we hope that the proposed strategies enable more fine-grained analyses in fields like Computational Social Science that leverage NLP to study human-made constructs. Constructs like sarcasm are not black and white and reflect the subtle complexities of human language; sarcasm can really take many sub-forms like hyperbole, satire, irony, understatements, rhetorical questions, juxtaposition, and sardonic humor.
Building a model to detect these classes of sarcasm can be intractable.
Do we search for distinct datasets for each of these types of sarcasm? Do we annotate a large corpus of sarcastic texts to fit into this taxonomy? It’s not entirely clear. However, this could be done with the taxonomy-based prompting strategy proposed in this work.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Limitations and Future Work</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Owing to its superior efficiency and cost effectiveness, we used ChatGPT for generating synthetic data in this work. However, in the future we aim to repeat all the analyses using data generated via GPT-4, which has shown to achieve substantial improvements over ChatGPT <cite class="ltx_cite ltx_citemacro_cite">Bubeck et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. In the same vein, we would like to fine-tune a larger language model on the order of hundred million parameters for the downstream task of sarcasm detection. This is primarily because sarcasm detection is a difficult task, and therefore could benefit from the abilities that only emerge in LLMs at scale <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Next, we would also like to extend our analyses to diverse NLP tasks. While the present work showcases the ability of our proposed prompting strategies to generate more faithful synthetic data using the task of sarcasm detection, our strategies are general and can be applied for other NLP tasks.
</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">From an evaluation standpoint, we use the downstream performance of classifiers trained on the generated synthetic data to quantitatively assess the quality of generations. However, this is inherently a proxy for evaluating data faithfulness. In the future, we would like to perform a more direct evaluation, such as conducting a turing test, by asking humans to distinguish between real and synthetically generated data.
</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Finally, we intend to perform extensive tuning of different components of our pipeline. For example, while we fix the number of re-writes to 10, it would be fruitful to identify the optimal value of the number of re-writes as well as understand its relationship with the complexity of the underlying task. Similarly, following the success of self-refinement <cite class="ltx_cite ltx_citemacro_cite">Madaan et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, we would like to explore the use of iterative refinement strategies to discriminate between real vs. synthetic data, which is currently performed in a single filtering step.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical considerations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">All the datasets and resources used in this work are publicly available and do not contain any private or sensitive information about individuals. Moreover, all the findings are based on analyses conducted at an aggregate-level, and thus, no individual-level inferences can be drawn. However, human-like synthetic data can be used maliciously. We acknowledge this concern.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abaskohi et al. (2022)</span>
<span class="ltx_bibblock">
Amirhossein Abaskohi, Arash Rasouli, Tanin Zeraati, and Behnam Bahrak. 2022.

</span>
<span class="ltx_bibblock">Utnlp at semeval-2022 task 6: A comparative analysis of sarcasm
detection using generative-based and mutation-based data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.08198</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alaa et al. (2022)</span>
<span class="ltx_bibblock">
Ahmed Alaa, Boris Van Breugel, Evgeny S Saveliev, and Mihaela van der Schaar.
2022.

</span>
<span class="ltx_bibblock">How faithful is your synthetic data? sample-level metrics for
evaluating and auditing generative models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
290–306. PMLR.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anaby-Tavor et al. (2020)</span>
<span class="ltx_bibblock">
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour,
Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020.

</span>
<span class="ltx_bibblock">Do not have enough data? deep learning to the rescue!

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, pages 7383–7390.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Argyle et al. (2022)</span>
<span class="ltx_bibblock">
Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting,
and David Wingate. 2022.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.06899</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bail (2023)</span>
<span class="ltx_bibblock">
Christopher A Bail. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.31235/osf.io/rwtzs" title="" class="ltx_ref ltx_href">Can Generative AI
Improve Social Science?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">SocArXiv</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
et al. 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with
gpt-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12712</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dillion et al. (2023)</span>
<span class="ltx_bibblock">
Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023.

</span>
<span class="ltx_bibblock">Can ai language models replace human participants?

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Trends in Cognitive Sciences</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan and Li (2023)</span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock">Tinystories: How small can language models be and still speak
coherent english?

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07759</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farha et al. (2022)</span>
<span class="ltx_bibblock">
Ibrahim Abu Farha, Silviu Vlad Oprea, Steven Wilson, and Walid Magdy. 2022.

</span>
<span class="ltx_bibblock">Semeval-2022 task 6: isarcasmeval, intended sarcasm detection in
english and arabic.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th International Workshop on Semantic
Evaluation (SemEval-2022)</em>, pages 802–814.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2021)</span>
<span class="ltx_bibblock">
Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,
Teruko Mitamura, and Eduard Hovy. 2021.

</span>
<span class="ltx_bibblock">A survey of data augmentation approaches for nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.03075</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al. (2023)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.

</span>
<span class="ltx_bibblock">Chatgpt outperforms crowd-workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.15056</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horton (2023)</span>
<span class="ltx_bibblock">
John J Horton. 2023.

</span>
<span class="ltx_bibblock">Large language models as simulated economic agents: What can we learn
from homo silicus?

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.07543</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Josifoski et al. (2023)</span>
<span class="ltx_bibblock">
Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023.

</span>
<span class="ltx_bibblock">Exploiting asymmetry for synthetic training data generation: Synthie
and the case of information extraction.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04132</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.
2023.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17651</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maqsud (2015)</span>
<span class="ltx_bibblock">
Umar Maqsud. 2015.

</span>
<span class="ltx_bibblock">Synthetic text generation for sentiment analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Analysis</em>, pages 156–161.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Møller et al. (2023)</span>
<span class="ltx_bibblock">
Anders Giovanni Møller, Jacob Aarup Dalsgaard, Arianna Pera, and Luca Maria
Aiello. 2023.

</span>
<span class="ltx_bibblock">Is a prompt and a few samples all you need? using gpt-4 for data
augmentation in low-resource classification tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.13861</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oprea and Magdy (2019)</span>
<span class="ltx_bibblock">
Silviu Oprea and Walid Magdy. 2019.

</span>
<span class="ltx_bibblock">isarcasm: A dataset of intended sarcasm.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.03123</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papanikolaou and Pierleoni (2020)</span>
<span class="ltx_bibblock">
Yannis Papanikolaou and Andrea Pierleoni. 2020.

</span>
<span class="ltx_bibblock">Dare: Data augmented relation extraction with gpt-2.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.13845</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salganik (2019)</span>
<span class="ltx_bibblock">
Matthew J Salganik. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Bit by bit: Social research in the digital age</em>.

</span>
<span class="ltx_bibblock">Princeton University Press.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2015)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.06709</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu,
Jianfeng Qu, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock">Is chatgpt a good nlg evaluator? a preliminary study.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04048</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.03533</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.07682</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei Song. 2022.

</span>
<span class="ltx_bibblock">A survey of controllable text generation using transformer-based
pre-trained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.05337</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11206</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et al. (2023)</span>
<span class="ltx_bibblock">
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi
Yang. 2023.

</span>
<span class="ltx_bibblock">Can large language models transform computational social science?

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.03514</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.15040" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.15041" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.15041">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.15041" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.15042" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 06:02:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
