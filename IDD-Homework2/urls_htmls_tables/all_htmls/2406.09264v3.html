<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</title>
<!--Generated on Sat Aug 10 17:45:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Human-AI Alignment,  Human-AI Interaction
" lang="en" name="keywords"/>
<base href="/html/2406.09264v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S1" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S2" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Challenges in Achieving Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Fundamental Definitions and Clarifications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS1" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>What is the goal of alignment?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS2" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>With whom to align?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS3" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>What are the values to be aligned with?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS4" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Scopes and Key Components in Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS5" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Systematic Literature Review and Framework Rationale</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS6" title="In 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Rationale Behind the Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Bidirectional Human-AI Alignment Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1" title="In 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Align <span class="ltx_text" style="color:#1271CA;">AI</span> to
<span class="ltx_text" style="color:#DE4A4D;">Humans</span>:  <span class="ltx_text" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="28" src="x22.png" width="27"/></span> Human Values and Specifications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1.SSS1" title="In 4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span><em class="ltx_emph ltx_font_italic"><span class="ltx_text ltx_font_bold" style="color:#2E9B42;">Categorizing Aligned Human Values</span>. <span class="ltx_text ltx_font_bold">What values have been aligned with AI?</span></em></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1.SSS2" title="In 4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span><em class="ltx_emph ltx_font_italic"><span class="ltx_text ltx_font_bold" style="color:#2E9B42;">Interaction Techniques to Specify AI Values</span>.

<span class="ltx_text ltx_font_bold">How humans could interactively specify values in AI development?</span></em></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2" title="In 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Align <span class="ltx_text" style="color:#1271CA;">AI</span> to
<span class="ltx_text" style="color:#DE4A4D;">Humans</span>:  <span class="ltx_text" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" src="x23.png" width="29"/></span> Integrating Human Specifications into AI</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS1" title="In 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#1271CA;">Integrating General Values to AI</em>: <span class="ltx_text ltx_font_bold">how to incorporate general human values into AI development?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS2" title="In 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#1271CA;">Customizing AI Values</em>: <span class="ltx_text ltx_font_bold">how to customize AI to incorporate values from individuals or human groups?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS3" title="In 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#1271CA;">Evaluating AI Systems</em>: <span class="ltx_text ltx_font_bold">how to evaluate AI regarding human values?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS4" title="In 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#1271CA;">Ecosystem and Platforms</em>: <span class="ltx_text ltx_font_bold">how to build the ecosystem to facilitate human-AI alignment?</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3" title="In 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Align <span class="ltx_text" style="color:#DE4A4D;">Humans</span> to
<span class="ltx_text" style="color:#1271CA;">AI</span>:  <span class="ltx_text" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" src="x16.png" width="29"/></span> Human’s Perceptual Adaptation to AI</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3.SSS1" title="In 4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#533283;">Perceiving and Understanding AI</em>: <span class="ltx_text ltx_font_bold">how do humans learn to perceive and explain AI systems?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3.SSS2" title="In 4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#533283;">Critical Thinking around AI</em>: <span class="ltx_text ltx_font_bold">how do humans think critically about AI systems?</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4" title="In 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Align <span class="ltx_text" style="color:#DE4A4D;">Humans</span> to
<span class="ltx_text" style="color:#1271CA;">AI</span>:  <span class="ltx_text" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" src="x17.png" width="29"/></span> Human’s Behavioral Adaptation to AI</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS1" title="In 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#DE4A4D;">Human-AI Collaboration Mechanisms</em>: <span class="ltx_text ltx_font_bold">what are human strategies to collaborate with AI that have differing levels of capabilities?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS2" title="In 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#DE4A4D;">AI Impact on Humans and Society</em>: <span class="ltx_text ltx_font_bold">how are humans influenced by AI systems
?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS3" title="In 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span><em class="ltx_emph ltx_font_bold ltx_font_italic" style="color:#DE4A4D;">Evaluation in Human Studies</em>: <span class="ltx_text ltx_font_bold">how might we evaluate and understand the impact of AI on humans and society?</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Findings and Discussions on Current Gaps</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS1" title="In 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Meta Analysis of Trends and Gaps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS2" title="In 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Insights into Human Values for Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS3" title="In 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Interaction Techniques for Specifying Human Values: A Cross-Domain Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS1" title="In 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Specification Game</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS1.SSS1" title="In 6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#DE4A4D;">Humans</span>: Integrate fully specified human values into aligning AI</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS1.SSS2" title="In 6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#1271CA;">AI</span>: Elicit nuanced and contextual human values during diverse interactions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS2" title="In 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Dynamic Co-evolution of Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS2.SSS1" title="In 6.2. Dynamic Co-evolution of Alignment ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#DE4A4D;">Humans</span>: Co-evolve AI with changes in humans and society</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS2.SSS2" title="In 6.2. Dynamic Co-evolution of Alignment ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#1271CA;">AI</span>: Adapt humans and society to the latest AI advancements</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS3" title="In 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Safeguarding Co-adaptation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS3.SSS1" title="In 6.3. Safeguarding Co-adaptation ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#DE4A4D;">Humans</span>: Specify the goals of an AI system into interpretable and controllable instrumental actions for humans</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS3.SSS2" title="In 6.3. Safeguarding Co-adaptation ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span><span class="ltx_text ltx_font_bold">Aligning <span class="ltx_text" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline"><semantics><mo mathcolor="#F9AB10" stretchy="false">↦</mo><annotation-xml encoding="MathML-Content"><csymbol cd="latexml">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex">\mapsto</annotation><annotation encoding="application/x-llamapun">↦</annotation></semantics></math> <span class="ltx_text" style="color:#1271CA;">AI</span>: Empower humans to identify and intervene in AI instrumental and final strategies in collaboration</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S7" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Systematic Literature Review</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1" title="In Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Systematic Literature Review Process</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1.SSS1" title="In A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>Identification and Screening with Keywords.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1.SSS2" title="In A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>Assessing Eligibility with Criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1.SSS3" title="In A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.3 </span>Qualitative Code Development.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1.SSS4" title="In A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.4 </span>Framework Development and Rigorous Coding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS2" title="In Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Venues</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS3" title="In Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Keywords</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS4" title="In Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Inclusion and Exclusion Criteria</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A2" title="In Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Author contributions</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A2.SS1" title="In Appendix B Author contributions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Overall Author List and Contributions</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hua Shen<sup class="ltx_sup" id="id1.1.id1">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-4928-525X" title="ORCID identifier">0000-0002-4928-525X</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:huashen@umich.edu">huashen@umich.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiffany Knearem<sup class="ltx_sup" id="id6.1.id1">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.2.id1">Google</span><span class="ltx_text ltx_affiliation_city" id="id8.3.id2">Boston</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tknearem@google.com">tknearem@google.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4928-6225" title="ORCID identifier">0000-0003-4928-6225</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Reshmi Ghosh<sup class="ltx_sup" id="id10.1.id1">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.2.id1">Microsoft</span><span class="ltx_text ltx_affiliation_city" id="id12.3.id2">Boston</span><span class="ltx_text ltx_affiliation_country" id="id13.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:reshmighosh@microsoft.com">reshmighosh@microsoft.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3070-2532" title="ORCID identifier">0000-0003-3070-2532</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kenan Alkiek<sup class="ltx_sup" id="id14.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id16.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id17.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id18.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kalkiek@umich.edu">kalkiek@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0007-1185-4633" title="ORCID identifier">0009-0007-1185-4633</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kundan Krishna<sup class="ltx_sup" id="id19.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id20.2.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id21.3.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_country" id="id22.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kundank@andrew.cmu.edu">kundank@andrew.cmu.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-8562-903X" title="ORCID identifier">0000-0002-8562-903X</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yachuan Liu<sup class="ltx_sup" id="id23.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0001-5086-5171" title="ORCID identifier">0009-0001-5086-5171</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziqiao Ma<sup class="ltx_sup" id="id24.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id26.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id27.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id28.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:marstin@umich.edu">marstin@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-0760-4638" title="ORCID identifier">0000-0002-0760-4638</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Savvas Petridis<sup class="ltx_sup" id="id29.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id30.2.id1">Google Research</span><span class="ltx_text ltx_affiliation_city" id="id31.3.id2">New York</span><span class="ltx_text ltx_affiliation_country" id="id32.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:petridis@google.com">petridis@google.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-4944-8477" title="ORCID identifier">0000-0002-4944-8477</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yi-Hao Peng<sup class="ltx_sup" id="id33.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id34.2.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id35.3.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_country" id="id36.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yihaop@cs.cmu.edu">yihaop@cs.cmu.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6335-5904" title="ORCID identifier">0000-0002-6335-5904</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Li Qiwei<sup class="ltx_sup" id="id37.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0001-3720-7086" title="ORCID identifier">0009-0001-3720-7086</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sushrita Rakshit<sup class="ltx_sup" id="id38.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id39.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id40.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id41.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id42.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sushrita@umich.edu">sushrita@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0008-3485-9637" title="ORCID identifier">0009-0008-3485-9637</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenglei Si<sup class="ltx_sup" id="id43.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id44.2.id1">Stanford University</span><span class="ltx_text ltx_affiliation_city" id="id45.3.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id46.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:clsi@stanford.edu">clsi@stanford.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0008-8327-2488" title="ORCID identifier">0009-0008-8327-2488</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yutong Xie<sup class="ltx_sup" id="id47.1.id1">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id48.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id49.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id50.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id51.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yutxie@umich.edu">yutxie@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3861-6778" title="ORCID identifier">0000-0003-3861-6778</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jeffrey P. Bigham<sup class="ltx_sup" id="id52.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id53.2.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id54.3.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_country" id="id55.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:bigham@cs.cmu.edu">bigham@cs.cmu.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2072-0625" title="ORCID identifier">0000-0002-2072-0625</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frank Bentley<sup class="ltx_sup" id="id56.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id57.2.id1">Google</span><span class="ltx_text ltx_affiliation_city" id="id58.3.id2">Mountain View</span><span class="ltx_text ltx_affiliation_country" id="id59.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:fbentley@google.com">fbentley@google.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-9150-1331" title="ORCID identifier">0000-0002-9150-1331</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joyce Chai<sup class="ltx_sup" id="id60.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id61.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id62.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id63.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id64.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chaijy@umich.edu">chaijy@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-9658-2230" title="ORCID identifier">0000-0002-9658-2230</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zachary Lipton<sup class="ltx_sup" id="id65.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id66.2.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id67.3.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_country" id="id68.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zlipton@cmu.edu">zlipton@cmu.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-3824-4241" title="ORCID identifier">0000-0002-3824-4241</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiaozhu Mei<sup class="ltx_sup" id="id69.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-8640-1942" title="ORCID identifier">0000-0002-8640-1942</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rada Mihalcea<sup class="ltx_sup" id="id70.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id71.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id72.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id73.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id74.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mihalcea@umich.edu">mihalcea@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-0767-6703" title="ORCID identifier">0000-0002-0767-6703</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Terry<sup class="ltx_sup" id="id75.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id76.2.id1">Google Research</span><span class="ltx_text ltx_affiliation_city" id="id77.3.id2">Boston</span><span class="ltx_text ltx_affiliation_country" id="id78.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:michaelterry@google.com">michaelterry@google.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/" title="ORCID identifier"></a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Diyi Yang<sup class="ltx_sup" id="id79.1.id1">4</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id80.2.id1">Stanford University</span><span class="ltx_text ltx_affiliation_city" id="id81.3.id2">California</span><span class="ltx_text ltx_affiliation_country" id="id82.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:diyiy@stanford.edu">diyiy@stanford.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1220-3983" title="ORCID identifier">0000-0003-1220-3983</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meredith Ringel Morris<sup class="ltx_sup" id="id83.1.id1">5</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id84.2.id1">Google DeepMind</span><span class="ltx_text ltx_affiliation_city" id="id85.3.id2">Seattle</span><span class="ltx_text ltx_affiliation_country" id="id86.4.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:merrie@google.com">merrie@google.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1436-9223" title="ORCID identifier">0000-0003-1436-9223</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Resnick<sup class="ltx_sup" id="id87.1.id1">5</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-8368-0600" title="ORCID identifier">0000-0001-8368-0600</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Jurgens<sup class="ltx_sup" id="id88.1.id1">5</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id89.2.id1">University of Michigan</span><span class="ltx_text ltx_affiliation_city" id="id90.3.id2">Ann Arbor</span><span class="ltx_text ltx_affiliation_state" id="id91.4.id3">MI</span><span class="ltx_text ltx_affiliation_country" id="id92.5.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jurgens@umich.edu">jurgens@umich.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2135-9878" title="ORCID identifier">0000-0002-2135-9878</a></span>
</span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id93.id1">Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as <em class="ltx_emph ltx_font_italic" id="id93.id1.1">alignment</em>.
However, the lack of clarified definitions and scopes of <em class="ltx_emph ltx_font_italic" id="id93.id1.2">human-AI alignment</em> poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment.
In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (<em class="ltx_emph ltx_font_italic" id="id93.id1.3">i.e.,</em> aiming to ensure that AI systems’ objectives match humans) rather than an ongoing, mutual alignment problem <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib213" title="">2024</a>)</cite>. This perspective largely neglects the <em class="ltx_emph ltx_font_italic" id="id93.id1.4">long-term interaction</em> and <em class="ltx_emph ltx_font_italic" id="id93.id1.5">dynamic changes</em> of alignment.
To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML).
We characterize, define and scope human-AI alignment. From this,
we present a conceptual framework of “Bidirectional Human-AI Alignment” to organize the literature from a human-centered perspective.
This framework encompasses both 1) conventional studies of <em class="ltx_emph ltx_font_italic" id="id93.id1.6">aligning AI to humans</em> that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of <em class="ltx_emph ltx_font_italic" id="id93.id1.7">aligning humans to AI</em>,
which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally.
Additionally, we articulate the key findings derived from literature analysis, including literature gaps and trends, human values, and interaction techniques.
To pave the way for future studies, we envision three key challenges and give recommendations for future research.</p>
</div>
<div class="ltx_keywords">Human-AI Alignment, Human-AI Interaction

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Manuscript; submitted to ACM; 2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing HCI theory, concepts and models</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Artificial Intelligence (AI) has advanced significantly, especially with the advent of general-purpose generative AI, demonstrating unprecedented capabilities in solving a wide range of complicated and challenging problems, such as reasoning, generation, language understanding, and more <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>)</cite>.
However, as AI becomes increasingly powerful and integrated into daily life, society is confronted with an array of risks to both individuals and to broader society <cite class="ltx_cite ltx_citemacro_citep">(Morris et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib127" title="">2024</a>)</cite>.
For example, text-to-image generative models were found to amplify stereotypes about race and gender <cite class="ltx_cite ltx_citemacro_citep">(blo, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib2" title="">2023</a>)</cite>, and
biased algorithms in hiring processes were found to perpetuate discrimination <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib141" title="">2021a</a>)</cite>.
These risks illuminate foundational questions around how and which values are embedded in AI models that drive decisions within real-world contexts.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For AI developers and researchers in this space, questions around the selection and representation of human values are part of a growing trend toward an interdisciplinary discussion on <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">AI alignment</em>, which, has been previously defined in  <cite class="ltx_cite ltx_citemacro_citet">Terry et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib195" title="">2023</a>)</cite> as <span class="ltx_ERROR undefined" id="S1.p2.1.2">\say</span><em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">considering the overall problem of how to ensure an AI produces the intended outcomes (as determined by its creator and/or user), without additional undesirable side effects (<em class="ltx_emph ltx_font_upright" id="S1.p2.1.3.1">e.g.,</em> by not performing operations that could negatively affect individuals, groups, or society at large)</em>.
While significant efforts have been made by research in domains of HCI, NLP and AI to align AI to humans such that the AI systems’ objectives match those of humans <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib59" title="">2024</a>; Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>; Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib164" title="">2023</a>)</cite>, there is a lack of clarity around the definition of <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">human-AI alignment</em>, the goals of alignment, and with whom AI should align within the research community.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Furthermore, previous research often views AI alignment as a static, unidirectional process (<em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">i.e.,</em> aiming to ensure that AI systems’ objectives match those of humans) rather than an ongoing and mutual alignment problem <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib213" title="">2024</a>)</cite>.
This unidirectional view largely understates the impact of <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">long-term interaction</span> and how such interaction further lends to a <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">dynamically changing</span> relationship between humans and AI.
As an example of the current unidirectional view, long-term interaction alignment work tends to focus on the implications of future advanced artificial general intelligence (<em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">i.e.,</em> AGI <cite class="ltx_cite ltx_citemacro_citep">(Morris et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib127" title="">2024</a>)</cite>), which hypothetically achieves human or superhuman intelligence <cite class="ltx_cite ltx_citemacro_citep">(Prunkl and Whittlestone, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib154" title="">2020</a>)</cite>, uninhibited by human interference. A bi-directional view instead positions humans in a position of agency, empowering humans to interactively identify risky AI intentions and prohibit associated AI behaviors in deployed environments (i.e., looking beyond short-term testing interactions) as <cite class="ltx_cite ltx_citemacro_citet">Weidinger et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib210" title="">2023</a>)</cite> advocated: <span class="ltx_ERROR undefined" id="S1.p3.1.5">\say</span><em class="ltx_emph ltx_font_italic" id="S1.p3.1.6">The interaction of technical and social components determines whether risk manifests</em>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Additionally, unidirectional work in AI alignment has under-emphasized the <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">evolution</em> of human values and objectives that may arise through our continued use of AI and acceptance of it into daily life.
Prior research has shown that humans differ in what values and preferences they want AI systems to include, and these differences have to-date been largely unaccounted for in current AI systems <cite class="ltx_cite ltx_citemacro_citep">(Gordon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib57" title="">2022</a>)</cite>.
Beyond this, human preferences might <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">dynamically change</em> as human objectives evolve alongside AI advances <cite class="ltx_cite ltx_citemacro_citep">(Carroll et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib28" title="">2024</a>)</cite>. As described by <cite class="ltx_cite ltx_citemacro_citet">Dautenhahn et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib36" title="">2000</a>)</cite>, technology, cognition and goals evolve in tandem:
<span class="ltx_ERROR undefined" id="S1.p4.1.3">\say</span><em class="ltx_emph ltx_font_italic" id="S1.p4.1.4">Our use of technology changes who we are and how we think, and changes the environments we live in. This feeds back into human cognition and societies making further technological externalizations…</em>
The unidirectional view of alignment as <span class="ltx_ERROR undefined" id="S1.p4.1.5">\say</span>static does not account for the aforementioned co-evolution as a dynamic force in alignment, which lends support for re-framing human-AI alignment as a bi-directional process. A bi-directional, dynamic view of human-AI alignment leverages a holistic understanding of the technical capabilities and limitations of AI, human-AI interaction, cognitive and social science, psychology and ethics, cross-cultural studies, and many other areas <cite class="ltx_cite ltx_citemacro_citep">(Carroll et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib28" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="365" id="S1.F1.g1" src="x1.png" width="499"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overview of the <span class="ltx_text ltx_font_bold" id="S1.F1.10.3">Bidirectional Human-AI Alignment</span> framework. Our framework encompasses both <span class="ltx_text" id="S1.F1.3.1" style="position:relative; bottom:-1.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="21" id="S1.F1.3.1.g1" src="x2.png" width="21"/></span> conventional studies of
“Align AI to Humans”
in AI development that ensures AI produces the intended outcomes determined by humans, and  <span class="ltx_text" id="S1.F1.4.2" style="position:relative; bottom:-1.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="21" id="S1.F1.4.2.g1" src="x4.png" width="21"/></span> a novel concept of
“Align Humans to
AI”
, which aims to help humans and society to better understand, critique, collaborate, and adapt to transformative AI advancements.
Note that “<span class="ltx_text ltx_font_bold" id="S1.F1.11.4">Humans</span>” refers to both AI users and those who do not interact with AI but may be impacted by AI systems.
We further identify four key research questions (<em class="ltx_emph ltx_font_italic" id="S1.F1.12.5">i.e.,</em> <span class="ltx_text ltx_font_typewriter" id="S1.F1.13.6">RQ1-RQ4</span>) to facilitate this holistic loop of “bidirectional human-AI alignment”, and organize the literature that can potentially address <span class="ltx_text ltx_font_typewriter" id="S1.F1.14.7">RQ1-RQ4</span> in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4" title="4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper conducts a systematic literature review of over 400 human-AI alignment papers across multiple disciplines, based on PRISMA guidelines <cite class="ltx_cite ltx_citemacro_citep">(Page et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib136" title="">2021</a>; Stefanidi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib187" title="">2023</a>)</cite>. The literature review corpus includes papers drawn from the the Human-Computer Interaction (HCI), Natural Language Processing (NLP) and Machine Learning (ML) domains that were published between the advent of general-purpose generative AI to present, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">i.e.,</em> primarily between January 2019 and January 2024. We take an interdisciplinary approach to human-AI alignment <cite class="ltx_cite ltx_citemacro_citep">(Gilbert, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib55" title="">2024</a>)</cite>, drawing from theories and empirical studies across the aforementioned domains. To round out our approach, we composed our research team of members from the domains of HCI, NLP, ML, Data Science, Computational Social Science, and Cognitive Science.</p>
</div>
<figure class="ltx_table" id="S1.tab1">
<table class="ltx_tabular ltx_align_top" id="S1.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.tab1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id="S1.tab1.1.1.1.1" style="background-color:#E6E6E6;">
<span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span><span class="ltx_text" id="S1.tab1.1.1.1.1.1" style="color:#808080;">  
<span class="ltx_inline-block ltx_align_top" id="S1.tab1.1.1.1.1.1.1">
<span class="ltx_p" id="S1.tab1.1.1.1.1.1.1.1" style="width:424.9pt;"><span class="ltx_text" id="S1.tab1.1.1.1.1.1.1.1.1" style="color:#808080;background-color:#E6E6E6;">
<span class="ltx_text ltx_font_bold" id="S1.tab1.1.1.1.1.1.1.1.1.1">Definition.</span></span></span>
<span class="ltx_p" id="S1.tab1.1.1.1.1.1.1.2"><em class="ltx_emph ltx_font_italic" id="S1.tab1.1.1.1.1.1.1.2.1" style="color:#808080;background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold" id="S1.tab1.1.1.1.1.1.1.2.1.1">Bidirectional Human-AI Alignment</span> is a comprehensive framework that encompasses two interconnected alignment processes: ‘Aligning <span class="ltx_text" id="S1.tab1.1.1.1.1.1.1.2.1.2" style="color:#1271CA;">AI</span> to <span class="ltx_text" id="S1.tab1.1.1.1.1.1.1.2.1.3" style="color:#DE4A4D;">Humans</span>’ and ‘Aligning <span class="ltx_text" id="S1.tab1.1.1.1.1.1.1.2.1.4" style="color:#DE4A4D;">Humans</span> to <span class="ltx_text" id="S1.tab1.1.1.1.1.1.1.2.1.5" style="color:#1271CA;">AI</span>’. The former focuses on integrating human specifications to train, steer, and customize AI, while the latter investigates human cognitive and behavioral adaptations to AI, which supports humans in understanding, critiquing, collaborating with, and adapting to AI advancements. </em></span>
</span>  <span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span></span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Paper Organization and Key Findings</span>.
We provide clarified definitions and scopes related to human-AI alignment, including “<em class="ltx_emph ltx_font_italic" id="S1.p6.1.2">what is the goal of alignment?</em>”,
“<em class="ltx_emph ltx_font_italic" id="S1.p6.1.3">with whom to align?</em>”, and “<em class="ltx_emph ltx_font_italic" id="S1.p6.1.4">what values should be aligned with?</em>”(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3" title="3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>).
We then present a conceptual framework of <em class="ltx_emph ltx_font_italic" id="S1.p6.1.5">Bidirectional Human-AI Alignment</em> from
a long-term and dynamic perspective, encompassing both “Align AI to
Humans” and “Align Humans to
AI” (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>).
Furthermore, we identify four key research questions (RQs) in the <em class="ltx_emph ltx_font_italic" id="S1.p6.1.6">Bidirectional Human-AI Alignment</em> framework and provide a structured way to organize the existing research literature to address these questions in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4" title="4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>.
The resulting structured topologies in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F2" title="Figure 2 ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F3" title="Figure 3 ‣ 4.2.3. Evaluating AI Systems: how to evaluate AI regarding human values? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a> aim to provide a shared vocabulary that can help streamline communication and collaboration between alignment researchers in different disciplines.
Furthermore, through iterative paper coding and literature analysis, we derive insights including meta analysis on gaps and trends in existing alignment research, findings of human values, and the interaction techniques for human-AI alignment (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5" title="5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>).
To pave the way for future studies, we further envision three future directions in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.F6" title="Figure 6 ‣ 6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6" title="6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6</span></a>.
Our study highlights several <span class="ltx_text ltx_font_bold" id="S1.p6.1.7">key findings</span>. For instance, many essential human values outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>, such as loyalty and environmental protection, are crucial for alignment but are often overlooked in current research. Additionally, many topics within the bidirectional alignment framework, including leveraging implicit human feedback to align AI models and auditing AI for various social values, are important but underexplored.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Operationalizing the Framework</span>.
To help AI practitioners (e.g., developers, researchers, user experience designers) who are interested in this field to leverage the bidirectional approaches, we highlight some systematic resources about human-AI alignment provided both within this survey and on companion Github repository:</p>
</div>
<div class="ltx_para" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A <a class="ltx_ref ltx_href" href="https://github.com/huashen218/bidirectional-human-ai-alignment" title="">Github repository</a> of “<em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">Bidirectional Human-AI Alignment</em>” maintaining latest reading lists and relevant updates.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>: An overview of the bidirectional human-AI alignment framework with formal definition.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S2.T1" title="Table 1 ‣ 2. Challenges in Achieving Alignment ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>: Typical alignment goals, their definitions, and limitations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>: A comprehensive list of basic human values, their relationships, and related alignment papers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F2" title="Figure 2 ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F3" title="Figure 3 ‣ 4.2.3. Evaluating AI Systems: how to evaluate AI regarding human values? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>: Fine-grained topologies and papers of “Aligning AI to Humans” &amp; “Aligning Humans to AI” research.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i6.p1">
<p class="ltx_p" id="S1.I1.i6.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.F4" title="Figure 4 ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>: An illustration of trends and gaps of alignment research dimension distribution.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i7.p1">
<p class="ltx_p" id="S1.I1.i7.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.F5" title="Figure 5 ‣ 5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>: A summary of common interactive techniques to specify human values and exemplary papers.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Challenges in Achieving Alignment</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The concept of <em class="ltx_emph ltx_font_italic" id="S2.p1.1.1">alignment</em> in AI research has a long history, tracing back to 1960, when AI pioneer Norbert Wiener <cite class="ltx_cite ltx_citemacro_citep">(Wiener, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib212" title="">1960</a>)</cite> described the AI alignment problem as: <span class="ltx_ERROR undefined" id="S2.p1.1.2">\say</span><em class="ltx_emph ltx_font_italic" id="S2.p1.1.3">If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively … we had better be quite sure that the purpose put into the machine is the purpose which we really desire.</em>
Discussion around intelligent agents and the associated concerns relating to ethics and society have emerged since then <cite class="ltx_cite ltx_citemacro_citep">(Stuurman and Wijnands, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib188" title="">2001</a>)</cite>.
Next, we discuss the well-known challenges encountered in achieving alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Challenge 1: Outer and Inner Alignment.</span>
In the context of “intelligent agents,” until now, <em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">AI alignment</em> research has aimed to ensure that any AI systems that would be set free to make decisions on our behalf would act appropriately and reduce unintended consequences <cite class="ltx_cite ltx_citemacro_citep">(Wooldridge, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib215" title="">1999</a>; Stuurman and Wijnands, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib188" title="">2001</a>; Russell and Norvig, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib163" title="">2016</a>)</cite>.
At the <em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">near-term</em> stage, aligning AI involves two main challenges: carefully <em class="ltx_emph ltx_font_italic" id="S2.p2.1.4">specifying</em>
the purpose of the system (<em class="ltx_emph ltx_font_italic" id="S2.p2.1.5">outer alignment</em> <em class="ltx_emph ltx_font_italic" id="S2.p2.1.6">i.e.,</em> providing well-specified rewards <cite class="ltx_cite ltx_citemacro_citep">(Ngo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib129" title="">2024</a>)</cite>) and ensuring that the system adopts the specification robustly (<em class="ltx_emph ltx_font_italic" id="S2.p2.1.7">inner alignment</em>, <em class="ltx_emph ltx_font_italic" id="S2.p2.1.8">i.e.,</em> ensuring that every action given an agent in a particular state learns desirable internally-represented goals <cite class="ltx_cite ltx_citemacro_citep">(Ngo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib129" title="">2024</a>)</cite>).
Significant efforts have been made, for <em class="ltx_emph ltx_font_italic" id="S2.p2.1.9">inner alignment</em>, to align AI systems to follow alignment goals of an individual or a group (<em class="ltx_emph ltx_font_italic" id="S2.p2.1.10">e.g.,</em> instructions, preferences, values, and/or ethical principles) <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>)</cite>
and to evaluate the performance of alignment <cite class="ltx_cite ltx_citemacro_citep">(Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib164" title="">2023</a>)</cite>.
However, for <em class="ltx_emph ltx_font_italic" id="S2.p2.1.11">outer alignment</em>, AI designers are still facing difficulties in specifying the full range of desired and undesired alignment goals of humans.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Challenge 2: Specification Gaming.</span>
To learn human alignment goals,
AI designers typically provide an objective function, instructions, reward function, or feedback to the system, which is often unable to completely specify all important values and constraints that a human intended <cite class="ltx_cite ltx_citemacro_citep">(Hemphill, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib69" title="">2020</a>)</cite>.
Hence, AI designers resort to easy-to-specify proxy goals such as <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">maximizing the approval of human overseers</em> <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib213" title="">2024</a>)</cite>, which results in “<em class="ltx_emph ltx_font_italic" id="S2.p3.1.3">specification gaming</em>” <cite class="ltx_cite ltx_citemacro_citep">(Krakovna et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib95" title="">2020</a>)</cite> or “<em class="ltx_emph ltx_font_italic" id="S2.p3.1.4">reward hacking</em>” <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib138" title="">2022</a>)</cite> issues (<em class="ltx_emph ltx_font_italic" id="S2.p3.1.5">i.e.,</em> AI systems can find loopholes that help them accomplish the specific objective efficiently but in unintended, possibly harmful ways).
Additionally, the black-box nature of neural networks brings additional ethical and safety concerns for
alignment because humans don’t know about the inner state and the actions AI leveraged to achieve the output. Consequently, AI systems might make “correct” decisions with “incorrect” reasons, which are difficult to discern. Society is already facing these issues,
such as data privacy <cite class="ltx_cite ltx_citemacro_citep">(Tucker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib197" title="">2018</a>)</cite>, algorithmic bias <cite class="ltx_cite ltx_citemacro_citep">(Hajian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib65" title="">2016</a>)</cite>, self-driving car accidents <cite class="ltx_cite ltx_citemacro_citep">(Bonnefon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib21" title="">2016</a>)</cite>, and more.
As a result, these considerations necessitate considering human-AI interaction in AI alignment for specification and evaluation, ranging from addressing problems around who uses an AI system, with what goals to specify, and if the AI system perform its intended function from the user’s perspective <cite class="ltx_cite ltx_citemacro_citep">(Weidinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib210" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Challenge 3: Scalable Oversight.</span>
From a long-term perspective, when advanced AI systems become more complex and capable (<em class="ltx_emph ltx_font_italic" id="S2.p4.1.2">e.g.,</em> AGI <cite class="ltx_cite ltx_citemacro_citep">(Morris et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib127" title="">2024</a>)</cite>),
it becomes increasingly difficult to align them to human values through human feedback.
Evaluating complex AI behaviors applied to increasingly challenging tasks can be slow or infeasible for humans to ensure all sub-steps are aligned with their values <cite class="ltx_cite ltx_citemacro_citep">(Terry et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib195" title="">2023</a>)</cite>.
Therefore, researchers have begun to investigate how to reduce the time and effort for human supervision, and how to assist human supervisors, referred to as <em class="ltx_emph ltx_font_italic" id="S2.p4.1.3">Scalable Oversight</em> <cite class="ltx_cite ltx_citemacro_citep">(Amodei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib4" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">Challenge 4: Dynamic Nature.</span>
As AI systems become increasingly powerful, the alignment solutions must also adapt dynamically since human values and preferences change as well.
As <cite class="ltx_cite ltx_citemacro_citet">Dautenhahn et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib36" title="">2000</a>)</cite> posit, AI systems may be neither humane nor desirable if we do not ask questions about the long-term cognitive and social effects of social agent systems
(<em class="ltx_emph ltx_font_italic" id="S2.p5.1.2">e.g.,</em> <em class="ltx_emph ltx_font_italic" id="S2.p5.1.3">how will agent technology affect human cognition</em>).
All these considerations call for a long-term and dynamic perspective to address human-AI alignment as an ongoing, mutual process with the collective efforts of cross-domain expertise.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p" id="S2.p6.1"><span class="ltx_text ltx_font_bold" id="S2.p6.1.1">Challenge 5: Existential Risk.</span>
Further, some AI researchers claim that <cite class="ltx_cite ltx_citemacro_citep">(Bengio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib17" title="">2024</a>)</cite> advanced AI systems will begin to seek power over their environment (<em class="ltx_emph ltx_font_italic" id="S2.p6.1.2">e.g.,</em> humans) once deployed in real-world settings, as such behavior may not be noticed during training. For example, some language models seek power in text-based social environments by gaining money, resources, or social influence <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib139" title="">2023</a>)</cite>.
Consequently, some hypothesize that future AI, if not properly aligned with human values,
could pose an <em class="ltx_emph ltx_font_italic" id="S2.p6.1.3">existential risk</em> to humans <cite class="ltx_cite ltx_citemacro_citep">(Dafoe and Russell, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib33" title="">2016</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S2.T1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="488" id="S2.T1.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>The <span class="ltx_text ltx_font_bold" id="S2.T1.3.1">Goals</span> of Alignment. We present the six prevailing alignment goals, associating with their Definitions (middle column), Limitations and Risks (right column).
We consider <span class="ltx_text ltx_font_bold" id="S2.T1.4.2">Human Values</span> as the main goal of alignment in this work referring to an extensive analysis and arguments in existing studies <cite class="ltx_cite ltx_citemacro_citep">(Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>; Russell, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib162" title="">2014</a>)</cite>
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Fundamental Definitions and Clarifications</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Addressing alignment challenges is a complex and multifaceted process.
To develop a comprehensive perspective of the ongoing and mutual process of human-AI alignment,
this section begins by introducing core definitions and key components of alignment. Subsequently, we outline our methodology for conducting a systematic literature review and the rationale behind our proposed framework of <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">bidirectional human-AI alignment</em>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>What is the goal of alignment?</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The research on alignment between humans and AI has introduced multiple alignment goals <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib142" title="">2023</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib221" title="">2024</a>)</cite>, such as <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">intentions</em> <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>; Anwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib5" title="">2024</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">preferences</em> <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib16" title="">2023</a>; Verma and Metcalf, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib200" title="">2023</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.3">instructions</em> <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib12" title="">2022b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib113" title="">2024b</a>)</cite>, and <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.4">values</em> <cite class="ltx_cite ltx_citemacro_citep">(Sorensen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib184" title="">2024</a>; Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>)</cite>.
However, researchers often use these terminologies interchangeably without clarifying their distinctions.
Drawing from a philosophical view,
we summarize the prevailing alignment <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.5">goals, their relationships, definitions, and limitations</em> and visualize them in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S2.T1" title="Table 1 ‣ 2. Challenges in Achieving Alignment ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a> referring to an extensive analysis of the advantages and limitations of different goalss in existing studies <cite class="ltx_cite ltx_citemacro_citep">(Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>; Russell, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib162" title="">2014</a>)</cite> .
Particularly, <cite class="ltx_cite ltx_citemacro_citet">Gabriel (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>)</cite> argues <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.6">values</em>, e.g., moral beliefs and principles, to be the best possible goal at the current stage for AI development for focusing alignment.
We further summarized the rationale behind this argument and the discussion of the trade-offs that arise from this choice in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S2.T1" title="Table 1 ‣ 2. Challenges in Achieving Alignment ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>.
The claim of “aligning AI with human values” is not new, as Stuart Russell <cite class="ltx_cite ltx_citemacro_citep">(Russell, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib162" title="">2014</a>)</cite> has stated back in 2014 that <span class="ltx_ERROR undefined" id="S3.SS1.p1.1.7">\say</span><em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.8">for an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans.</em>
Therefore, in this work, we <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.9">consider the goal of alignment as “human values”</span>, which means AI systems do what people morally
ought to do, as defined by individuals or society.</p>
</div>
<figure class="ltx_table" id="S3.T2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_square" height="776" id="S3.T2.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>The relations and a fine-grained taxonomy of <span class="ltx_text ltx_font_bold" id="S3.T2.4.2">69 exemplary human values</span>.
We consider 5 high-order value types encompassing 12 motivational value types, indicated by their sources (e.g., individuals, society and interaction).
The exemplary values with red dot (<span class="ltx_text" id="S3.T2.2.1" style="position:relative; bottom:0.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="9" id="S3.T2.2.1.g1" src="x7.png" width="10"/></span>) indicates there are no work in our surveyed papers examining the specific values.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>With whom to align?</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Pertinent stakeholders within the AI landscape can be the potential objects for AI to align with, including lay people (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">e.g.,</em> end users of AI systems) <cite class="ltx_cite ltx_citemacro_citep">(Qi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib156" title="">2023</a>)</cite>, AI practitioners (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.2">e.g.,</em> developers, researchers) <cite class="ltx_cite ltx_citemacro_citep">(Boggust et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib20" title="">2022</a>; Anwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib5" title="">2024</a>; Ngo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib129" title="">2024</a>)</cite>, organizational entities (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.3">e.g.,</em> technology firms, professional communities), national/international bodies (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.4">e.g.,</em> governments, legislative bodies) <cite class="ltx_cite ltx_citemacro_citep">(Deshpande and Sharp, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib37" title="">2022</a>)</cite> and others.
Many alignment research papers have focused on general humans without specifying particular groups <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib228" title="">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib112" title="">2022</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib62" title="">2024</a>)</cite>. Nevertheless, different groups hold different, sometimes even contrasting, values <cite class="ltx_cite ltx_citemacro_citep">(Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>)</cite>.
As a consequence, rather than identifying a <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.5">true moral theory</em> as a <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.6">one-size-fits-all</em> value, prior alignment research argues to select the appropriate principles for compatible human groups <cite class="ltx_cite ltx_citemacro_citep">(Sorensen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib184" title="">2024</a>)</cite>.
To this end, <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.7">pluralistic value alignment</em>, grounded on social choice theory <cite class="ltx_cite ltx_citemacro_citep">(Arrow, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib6" title="">2012</a>)</cite>, proposes combining individual views fairly in developing alignment principles as a potential solution <cite class="ltx_cite ltx_citemacro_citep">(Sorensen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib184" title="">2024</a>)</cite>.
In this work,
we also consider values from this pluralistic perspective,
where AI should be aligned with <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.8">pluralistic human individuals and societal groups</span> who would ultimately be impacted by AI.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>What are the values to be aligned with?</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">While previous studies have aimed to align AI with human values, the specific values they examined are often ambiguous and inconsistent.
To clarify human values relevant to human-AI alignment, we structured human values using a combination of top-down and bottom-up methods based on the <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Schwartz Theory of Basic Values</span> <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite>.
Among various human value theories (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">e.g.,</em> Moral Foundation Theory <cite class="ltx_cite ltx_citemacro_citep">(Graham et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib60" title="">2013</a>)</cite>, Social Norms &amp; Ethics <cite class="ltx_cite ltx_citemacro_citep">(Forbes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib48" title="">2020</a>)</cite>), we chose the Schwartz Theory of Basic Values primarily considering that its definitions and dimensions are universal and are applicable for most people across <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.3">(a)</em> various cultures and countries; <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.4">(b)</em> various divisions including individuals, interactions and groups; and <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.5">(c)</em> is commonly accepted in previous NLP studies <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib87" title="">2023</a>; Kiesel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib91" title="">2022</a>)</cite>.
Specifically, <cite class="ltx_cite ltx_citemacro_citet">Schwartz (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite> provided a <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.6">clarified definition of human values</span> by summarizing
some widely agreed-upon features as:
<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.7"><span class="ltx_ERROR undefined" id="S3.SS3.p1.1.7.1">\say</span>A value is a (1) belief (2) pertaining to desirable end states or modes of conduct, that (3) transcends specific situations, (4) guides selection or evaluation of behavior, people, and events.</em>
<cite class="ltx_cite ltx_citemacro_citet">Schwartz (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite> further offered a universal model outlining broad values that steer human behavior grounded in psychology.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.2">A Comprehensive Taxonomy of Human Values</span>.
Nevertheless, this conventional theory was developed without the context of human-AI interaction, which might overlook values that need to be considered for human-AI alignment. Therefore, we used a <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.3">bottom-up</em> approach to extract all values studied in our collected alignment literature (elaborated in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS1" title="A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A.1</span></a>), mapped them onto the Schwartz Theory of Basic Values, and supplemented the theory with AI-related structure and content.
As a result, we identified the structural relationships among human values (see Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:value_structure</span>) and mapped existing literature to a fine-grained taxonomy (see Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>).
As shown in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:value_structure</span>, we supplemented the traditional theory’s four high-order value types (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.4">i.e.,</em> “Self-Enhancement”, “Openness to Change”, “Conservation”, “Self-Transcendence”) with a novel high-order value type, named “Desired Values for AI Tools” that encompasses two motivational value types (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.5">i.e.,</em> “Usability” and “Human-Likeness”).
We further organize the relationship among these value types along two dimensions <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite>: different resources (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.6">i.e.,</em> individuals, society and interaction) and different self-intentions (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.7">i.e.,</em> self-protection against threat and self-expansion and growth).
Furthermore, we elaborate the definitions of the 12 motivational value types and their exemplary values by mapping them to relevant human-AI alignment papers from our corpus in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>.
During the process of mapping, we found: 1) value terms in empirical papers were often named differently (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.8">e.g.,</em> capability and competence), or check their opposites (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.9">e.g.,</em> fairness and bias);
2) there are many values not studied in our corpus, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.10">i.e.,</em> indicated as (<span class="ltx_text" id="S3.SS3.p2.1.1" style="position:relative; bottom:0.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="9" id="S3.SS3.p2.1.1.g1" src="x7.png" width="10"/></span>) in the Figure.</p>
</div>
<figure class="ltx_table" id="S3.SS3.tab1">
<table class="ltx_tabular ltx_align_top" id="S3.SS3.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.SS3.tab1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.SS3.tab1.1.1.1.1" style="background-color:#E6E6E6;">
<span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span><span class="ltx_text" id="S3.SS3.tab1.1.1.1.1.1" style="color:#808080;">  
<span class="ltx_inline-block ltx_align_top" id="S3.SS3.tab1.1.1.1.1.1.1">
<span class="ltx_p" id="S3.SS3.tab1.1.1.1.1.1.1.1" style="width:424.9pt;"><span class="ltx_text" id="S3.SS3.tab1.1.1.1.1.1.1.1.1" style="color:#808080;background-color:#E6E6E6;">
<span class="ltx_text ltx_font_bold" id="S3.SS3.tab1.1.1.1.1.1.1.1.1.1">Takeaways &amp; Implications for Alignment Clarifications</span></span></span>
<span class="ltx_p" id="S3.SS3.tab1.1.1.1.1.1.1.2"><span class="ltx_text" id="S3.SS3.tab1.1.1.1.1.1.1.2.1" style="color:#808080;background-color:#E6E6E6;">1. We propose the “pluralistic human values” as the alignment goal, wherein AI should be aligned with the diverse perspectives of individuals and societal groups who may be directly or indirectly impacted by AI.</span></span>
<span class="ltx_p" id="S3.SS3.tab1.1.1.1.1.1.1.3"><span class="ltx_text" id="S3.SS3.tab1.1.1.1.1.1.1.3.1" style="color:#808080;background-color:#E6E6E6;">2. Many exemplary human values outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>, such as loyalty and environmental protection, are important for alignment but are often overlooked or underexplored in current research.</span></span>
<span class="ltx_p" id="S3.SS3.tab1.1.1.1.1.1.1.4"><span class="ltx_text" id="S3.SS3.tab1.1.1.1.1.1.1.4.1" style="color:#808080;background-color:#E6E6E6;">3. The human value system should account for complex application contexts and relationships, such as priorities and conflicts, rather than focusing on an exclusive subset of values.</span></span>
</span>  <span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span></span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Scopes and Key Components in Alignment</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">This section clarifies the key components we considered in <em class="ltx_emph ltx_font_italic" id="S3.SS4.p1.1.1">human-AI alignment</em>. Additionally, we define the scopes of each key component as commonly adopted in current research.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1" style="color:#DE4A4D;">Humans.</span>
Our primary focus is on human <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i1.p1.1.2" style="border-color: #DE4A4D;">individuals</span>, <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i1.p1.1.3" style="border-color: #DE4A4D;">groups</span>, or <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i1.p1.1.4" style="border-color: #DE4A4D;">organizations</span> that will ultimately develop, use, and potentially impact or be influenced by AI systems,
as these are the entities with which AI systems should align. We emphasize the importance of considering the pluralistic values of diverse users rather than treating users as a monolithic group.
</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1" style="color:#1271CA;">Artificial Intelligence (AI).</span>
We focus on AI systems including both <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i2.p1.1.2" style="border-color: #1271CA;">domain-specific AI systems</span> that address specific tasks (<em class="ltx_emph ltx_font_italic" id="S3.I1.i2.p1.1.3">e.g.,</em> reasoning, dialogue) and <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i2.p1.1.4" style="border-color: #1271CA;">general-purpose AI models</span> that aim to complete any tasks with performance comparable to a human’s.
These AI systems include generative, classification, and regression models, among others.
Particularly, we primarily focus on language models as the representative AI models for alignment research, and discuss how the insights from this study can be generalized to other modalities at the section end.
</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1" style="color:#F9AB10;">Alignment.</span>
Our review encompassed all the <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i3.p1.1.2" style="border-color: #F9AB10;">alignment goals</span> outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS1" title="3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
Since many studies emphasized the importance of <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i3.p1.1.3" style="border-color: #F9AB10;">value alignment</span> <cite class="ltx_cite ltx_citemacro_citep">(Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>; Russell, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib162" title="">2014</a>)</cite>, we particularly summarized a clarified taxonomy of alignment values and identified the value in each paper (if applicable).
Additionally, we focus on analyzing the AI models’ output and generation, but not the neural network’s intermediate representations <cite class="ltx_cite ltx_citemacro_citep">(McGrath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib125" title="">2023</a>; Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib142" title="">2023</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib221" title="">2024</a>)</cite>, for alignment research.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Systematic Literature Review and Framework Rationale</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Achieving the alignment between AI and human values necessitates collaboration among interdisciplinary AI practitioners (<em class="ltx_emph ltx_font_italic" id="S3.SS5.p1.1.1">e.g.,</em> developers and researchers).
To comprehensively analyze the research literature pertaining to this ongoing, mutual process of human-AI alignment, we conducted a systematic literature review adhering to the the PRISMA guideline <cite class="ltx_cite ltx_citemacro_citep">(Page et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib136" title="">2021</a>; Stefanidi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib187" title="">2023</a>)</cite>.
Our iterative selection process yielded 411 papers from an initial pool of 34,213 publications, spanning the period from January 2019 to January 2024. These papers were sourced from leading venues across multiple AI-related domains, including Natural Language Processing, Human-Computer Interaction, Machine Learning, and more, using well-defined keywords and criteria.
A group of 13 interdisciplinary researchers coded the papers. Through iterative discussions, we developed the paper codes and proposed framework (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3" title="3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>). Detailed steps and processes are elucidated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1" title="Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Rationale Behind the Framework</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Our systematic review revealed that current research largely overlooks the nature of <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.1">long-term dynamic changes</span> and the <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.2">importance of interaction</span> in human-AI alignment.
First, the manifestation of certain AI risks, harms, and the practical quality of AI services primarily emerge through interaction between humans and AI systems <cite class="ltx_cite ltx_citemacro_citep">(Weidinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib210" title="">2023</a>)</cite>.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Shen and Huang (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib173" title="">2020</a>)</cite> discovered that AI explanations, deemed effective by AI developers and researchers, were found not useful for humans in real-world scenarios of interactive AI error detection.
Additionally, the use of AI influences human thinking and behavior, which in turn alters their values when interacting with AI.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib105" title="">2024b</a>)</cite> found that incorporating AI assistance in writing workflows changed humans’ writing perceptions, behavior, and performance, creating new requirements for developing the next generation of AI writing assistants <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib99" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">In light of these findings, we posit that AI practitioners must expand their focus beyond merely “Aligning AI to Humans” – a paradigm centered on integrating human specifications to train, steer, and customize AI. Equal emphasis must be placed on “Aligning Humans to AI”, a complementary paradigm investigating human cognitive and behavioral adaptation to AI. This latter aspect is crucial in supporting individuals and society at large in understanding, critiquing, collaborating with, and adapting to transformative AI advancements.
To encapsulate these comprehensive perspectives, we propose the “<span class="ltx_text ltx_font_bold" id="S3.SS6.p2.1.1">Bidirectional Human-AI Alignment</span>” framework, and provide its definition below Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>.
This holistic framework acknowledges the reciprocal nature of alignment,
addressing both the advancements of AI systems to incorporate human values, and the evolving human adaptation of AI systems in real-world practice.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Bidirectional Human-AI Alignment Framework</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="999" id="S4.F2.g1" src="x8.png" width="796"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The fine-grained topology of
“Align <span class="ltx_text" id="S4.F2.3.1" style="color:#1271CA;">AI</span> to
<span class="ltx_text" id="S4.F2.4.2" style="color:#DE4A4D;">Humans</span>” direction, which studies mechanisms to ensure that AI system’s objectives match those of humans’. The goal is to integrate human specification to train, steer and customize AI systems.
</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.3">This section introduces the <span class="ltx_text ltx_font_bold" id="S4.p1.3.4">Bidirectional Human-AI Alignment</span> framework, which encompassing two interconnected alignment directions, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">1</span></a>.
The <span class="ltx_text" id="S4.p1.1.1" style="position:relative; bottom:-1.8pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="S4.p1.1.1.g1" src="x2.png" width="21"/></span> “Align <span class="ltx_text" id="S4.p1.3.5" style="color:#1271CA;">AI</span> to
<span class="ltx_text" id="S4.p1.3.6" style="color:#DE4A4D;">Humans</span>” direction studies mechanisms to ensure that AI systems’ values match those of humans’ (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1" title="4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2" title="4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
We identify two crucial research questions to be addressed in this direction: <span class="ltx_text ltx_font_bold" id="S4.p1.3.7">RQ1.</span> <em class="ltx_emph ltx_font_italic" id="S4.p1.3.8">What relevant human values are studied for AI alignment, and how do humans specify these values?</em> and <span class="ltx_text ltx_font_bold" id="S4.p1.3.9">RQ2.</span> <em class="ltx_emph ltx_font_italic" id="S4.p1.3.10">How can human values be integrated into the development of AI?</em>
In comparison, the <span class="ltx_text" id="S4.p1.2.2" style="position:relative; bottom:-1.8pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="S4.p1.2.2.g1" src="x9.png" width="21"/></span> “Align <span class="ltx_text" id="S4.p1.3.11" style="color:#DE4A4D;">Humans</span> to <span class="ltx_text" id="S4.p1.3.12" style="color:#1271CA;">AI</span>” direction investigates the humans’ cognitive and behavioral adaptation to the AI advancement (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3" title="4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4" title="4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4</span></a>). We recognize two imperative research questions along this direction: <span class="ltx_text ltx_font_bold" id="S4.p1.3.13">RQ3.</span> <em class="ltx_emph ltx_font_italic" id="S4.p1.3.14">How might humans learn to perceive, explain, and critique AI?</em> and <span class="ltx_text ltx_font_bold" id="S4.p1.3.15">RQ4.</span> <em class="ltx_emph ltx_font_italic" id="S4.p1.3.16">How do humans and society make behavioral changes and react to AI advancement?</em>.
We address the four questions by elucidating the fine-grained typologies and literature solutions derived from our systematic review, as delineated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F2" title="Figure 2 ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F3" title="Figure 3 ‣ 4.2.3. Evaluating AI Systems: how to evaluate AI regarding human values? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3</span></a>, respectively.

<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.3.3">DIRECTION-I:  <span class="ltx_text" id="S4.p1.3.3.1" style="position:relative; bottom:-1.8pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="S4.p1.3.3.1.g1" src="x2.png" width="21"/></span> ALIGN <span class="ltx_text" id="S4.p1.3.3.2" style="color:#1271CA;">AI</span> to
<span class="ltx_text" id="S4.p1.3.3.3" style="color:#DE4A4D;">HUMANS</span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">This direction delineates alignment research from <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">AI-centered perspective</span> (e.g., ML/NLP domains) and provides <span class="ltx_text ltx_font_bold" id="S4.p2.1.2">AI developers and researchers</span> with approaches for two main challenges:
carefully specifying the values of the system, and ensuring that system adopts the specification robustly <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib213" title="">2024</a>; Ngo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib129" title="">2024</a>)</cite>.
Therefore, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F2" title="Figure 2 ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>, we explore the two core research questions in this direction as: <span class="ltx_text ltx_font_bold" id="S4.p2.1.3">RQ1.</span> Human Values and Specifications (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1" title="4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and <span class="ltx_text ltx_font_bold" id="S4.p2.1.4">RQ2.</span> Integrating Human Specifications into AI (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2" title="4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2</span></a>)</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Align <span class="ltx_text" id="S4.SS1.3.2" style="color:#1271CA;">AI</span> to
<span class="ltx_text" id="S4.SS1.4.3" style="color:#DE4A4D;">Humans</span>:  <span class="ltx_text" id="S4.SS1.1.1" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="28" id="S4.SS1.1.1.g1" src="x10.png" width="27"/></span> Human Values and Specifications</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">RQ1: What relevant human values are studied for AI alignment, and how do humans specify these values?</span>
We structure existing literature to address this research question by answering the following “<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2" style="color:#2E9B42;">Sub-Research Questions</span>”:
<em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.3">What values have been aligned by AI?</em>
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1.SSS1" title="4.1.1. Categorizing Aligned Human Values. What values have been aligned with AI? ‣ 4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>) and then exploring
<em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.4">How humans could interactively specify values in AI development?</em>
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1.SSS2" title="4.1.2. Interaction Techniques to Specify AI Values. How humans could interactively specify values in AI development? ‣ 4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>).
As shown at the top of Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.F2" title="Figure 2 ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>, particularly, we articulate the “<span class="ltx_text" id="S4.SS1.p1.1.5" style="color:#2E9B42;">Dimensions</span>“ we summarized to answer each of these sub-research questions, and provide “<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.p1.1.6" style="border-color: #2E9B42;">Codes</span>“ associated with “Example Papers” that have been studied in each dimension.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span><em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.1.1.1" style="color:#2E9B42;">Categorizing Aligned Human Values</span>. <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.1.1.2">What values have been aligned with AI?</span></em>
</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1"><span class="ltx_text" id="S4.SS1.SSS1.p1.1.1"></span>
To clarify and systematically understand human values relevant to human-AI alignment, we leverage the adapted “Schwartz Theory of Basic Values” introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS3" title="3.3. What are the values to be aligned with? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
and examine the category of aligned human values from the two dimensions of “Sources” and “Types”.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text" id="S4.SS1.SSS1.p2.1.1" style="color:#2E9B42;">Sources of Values.</span>
This dimension examines the three sources of human values <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p2.1.2" style="border-color: #2E9B42;">Individual</span> sources indicate values from individuals comprise universal needs of individuals as biological organisms <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite> or prioritize personal interests <cite class="ltx_cite ltx_citemacro_citep">(Kiesel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib91" title="">2022</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib83" title="">2022</a>)</cite>.
At this level, value alignment can be usually assessed independently of the context of interaction.
This perspective highlights the values about technical capabilities of AI models, including factuality <cite class="ltx_cite ltx_citemacro_citep">(Roit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib160" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib76" title="">2024</a>)</cite>, calibration <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib114" title="">2024a</a>)</cite>, output diversity <cite class="ltx_cite ltx_citemacro_citep">(Bradley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib23" title="">2024</a>)</cite>, and model inductive bias <cite class="ltx_cite ltx_citemacro_citep">(Si et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib178" title="">2023</a>)</cite>.
Additionally, this code includes research on aligning model behaviors with the characteristics and preferences of individual humans, e.g.,
predict human moral judgements and decisions <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib83" title="">2022</a>)</cite>,
and cognitive biases <cite class="ltx_cite ltx_citemacro_citep">(Jones and Steinhardt, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib85" title="">2022</a>; Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib94" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p2.1.3" style="border-color: #2E9B42;">Social</span> sources mean values from the social groups include universal requirements for smooth functioning and survival of groups <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>; Ammanabrolu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib3" title="">2022</a>)</cite>.
Value alignment at this level transcends personal interactions and emphasizes the broader categories defined by shared experiences, identities, cultures, norms, and more <cite class="ltx_cite ltx_citemacro_citep">(Santy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib165" title="">2023</a>)</cite>.
Research in this area often targets the alignment of AI behaviors with the general preferences of humans <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib70" title="">2021</a>; Bakker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib13" title="">2022</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib101" title="">2023a</a>)</cite>.
Additionally, there are efforts aimed to align AI with specific targeted groups, examining issues through the lenses of fairness <cite class="ltx_cite ltx_citemacro_citep">(Fleisig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib47" title="">2023</a>)</cite>, social norms <cite class="ltx_cite ltx_citemacro_citep">(Sky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib180" title="">2023</a>)</cite>, morality <cite class="ltx_cite ltx_citemacro_citep">(Prabhumoye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib153" title="">2021</a>; Rao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib159" title="">2023</a>)</cite>, and beyond.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p2.1.4" style="border-color: #2E9B42;">Interactive</span> sources
considers values at the interaction level to include
universal requisites of coordinated social interaction <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite>, which typically occurs in interpersonal situations, such as in the dynamics of speaker-receiver relationships during language communication <cite class="ltx_cite ltx_citemacro_citep">(Hovy and Yang, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib75" title="">2021</a>; Cabrera et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib26" title="">2023</a>)</cite>.
In the context of “human-AI alignment”, we adjust the definition of <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p2.1.5">Interaction</em> values to be the AI values that humans expect for AI as tools, such as Usability <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib238" title="">2023a</a>)</cite>, Autonomy <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib207" title="">2023a</a>)</cite>, among others.
Research along this line focuses on alignment strategies to enhance human-AI interaction <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib63" title="">2023</a>; Oh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib132" title="">2023</a>)</cite>, collaborative decision-making <cite class="ltx_cite ltx_citemacro_citep">(Gajos and Mamykina, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib51" title="">2022</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib227" title="">2023a</a>)</cite>, and trust <cite class="ltx_cite ltx_citemacro_citep">(Buçinca et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib25" title="">2021</a>; Dhuliawala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib38" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text" id="S4.SS1.SSS1.p3.1.1" style="color:#2E9B42;">Types of Values</span>. In this dimension, we introduce the five high-order human values derived from a combination of the Schwartz Human Values <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite> and empirical values studies from the surveyed papers. We provide a more in-depth taxonomy of value types in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS2" title="5.2. Insights into Human Values for Alignment ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.2</span></a>, including the relationship of these five types and more nuanced value categories.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p3.1.2" style="border-color: #2E9B42;">Self-Enhancement</span> refers to a set of self-protective and personal values that emphasize enhancing self-esteem and a sense of personal worth <cite class="ltx_cite ltx_citemacro_citep">(Sedikides and Strube, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib169" title="">1995</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib68" title="">2023</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib224" title="">2024</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib77" title="">2023</a>; Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib142" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib87" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib230" title="">2024</a>; Lima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib106" title="">2021</a>; Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib164" title="">2023</a>)</cite>.
One of the most important aspect is <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.3">achievement</span>, is competence as judged by social standards, which includes general capability (effectiveness/efficiency) as covered by the majority of the research <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib62" title="">2024</a>)</cite>.
Another dimension is <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.4">power</span>, which relates to social status and prestige, as well as control or dominance over people and resources <cite class="ltx_cite ltx_citemacro_citep">(Pinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib152" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p3.1.5" style="border-color: #2E9B42;">Self-Transcendence</span> refers to a set of self-expanding and socially-focused human values that emphasize expanding beyond oneself <cite class="ltx_cite ltx_citemacro_citep">(Frankl, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib49" title="">1966</a>; Banovic et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib15" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib219" title="">2023d</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib83" title="">2022</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib220" title="">2023b</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib77" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib121" title="">2023b</a>; Isajanyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib80" title="">2024</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib68" title="">2023</a>; Nie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib130" title="">2023</a>; Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib164" title="">2023</a>; Ramezani and Xu, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib158" title="">2023</a>; Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib176" title="">2021</a>; Goldfarb-Tarrant et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib56" title="">2023</a>)</cite>.
One critical aspect is <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.6">benevolence</span>, which relates to preservation and enhancement of the welfare of people with whom one is in frequent personal contact.
Extensive efforts have investigated the values including helpfulness <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib11" title="">2022a</a>)</cite>, honesty/factuality <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib76" title="">2024</a>)</cite>, responsibility/accountability <cite class="ltx_cite ltx_citemacro_citep">(Solyst et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib182" title="">2023</a>)</cite>.
Another critical aspect is <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.7">universalism</span>, which relates to understanding, appreciation, tolerance and protection for the welfare of all people and for nature.
Researchers have looked into values including inclusion/broad-mindedness <cite class="ltx_cite ltx_citemacro_citep">(Bakker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib13" title="">2022</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib42" title="">2024</a>)</cite> and equality/fairness <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib174" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib208" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p3.1.8" style="border-color: #2E9B42;">Conservation</span> refers to a set of self-protective and socially-focused human values that hold and safeguard traditional institutions and customs <cite class="ltx_cite ltx_citemacro_citep">(Hamilton, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib67" title="">2020</a>; Qi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib156" title="">2023</a>; Zagalsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib235" title="">2021</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib98" title="">2022</a>; Santy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib165" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib244" title="">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib242" title="">2023a</a>)</cite>.
Under conservative values, people are concerned about <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.9">security</span> (Ssafety, harmony, and stability of society, relationships, and oneself), <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.10">tradition</span> (Respect for and acceptance of the customs and ideas provided by traditional culture or religion), and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.11">conformity</span> (restraint of actions, inclinations, and impulses likely to upset or harm others and violate social expectations or norms).
Exemplar studies in this field include safety <cite class="ltx_cite ltx_citemacro_citep">(Anwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib5" title="">2024</a>)</cite>,
mental health <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib111" title="">2023</a>)</cite>, and cultural moral norms <cite class="ltx_cite ltx_citemacro_citep">(Ramezani and Xu, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib158" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p3.1.12" style="border-color: #2E9B42;">Openness to Change</span> refers to a set of self-expanding and personally-focused human values motivated by an anxiety-free need to grow, in contrast to conservation <cite class="ltx_cite ltx_citemacro_citep">(Bakker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib13" title="">2022</a>; Lima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib109" title="">2020</a>; Ashktorab et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib8" title="">2020</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib82" title="">2023</a>; Lima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib109" title="">2020</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib87" title="">2023</a>; Varanasi and Goyal, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib198" title="">2023</a>; Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib225" title="">2023</a>; Hosking et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib74" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib219" title="">2023d</a>; Kapania et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib88" title="">2023</a>)</cite>.
Under this type, people are concerned about <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.13">stimulation</span> (seeking excitement, novelty, and challenges in life), <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.14">hedonism</span> (pursuing pleasure and sensuous gratification for oneself), and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.1.15">self-direction</span> (independent thought and action—choices, creativity, and exploration).
Exemplar studies in this field include understanding of privacy <cite class="ltx_cite ltx_citemacro_citep">(Mireshghallah et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib126" title="">2024</a>)</cite>
and creativity <cite class="ltx_cite ltx_citemacro_citep">(Isajanyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib80" title="">2024</a>; Ashkinaze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib7" title="">2024</a>)</cite>.
We adopt the four high-order human values from Schwartz Human Values <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>)</cite>, supplementing them with additional values empirically collected from survey papers as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.T2" title="Table 2 ‣ 3.1. What is the goal of alignment? ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">2</span></a>.
Additionally, as the conventional theory is missing the context of AI-system and human-AI interactions, we introduce a new high-order value type called <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p3.1.16" style="border-color: #2E9B42;">Desired Values for AI Tools</span>. This category encompasses the values humans expect from AI when used as tools in “human-AI interaction” <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib226" title="">2023b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib238" title="">2023a</a>; Oh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib132" title="">2023</a>; Bakker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib13" title="">2022</a>; Boyd, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib22" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib207" title="">2023a</a>; Paleja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib137" title="">2021</a>)</cite>. These values include <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.17">usability</em> (competence according to the human experience on AI functionality) and <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS1.p3.1.18">human-likeness</em> (resemblance to human intelligence and behavior). Exemplary studies in this area include assessments of AI usability <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib238" title="">2023a</a>)</cite> and autonomy <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib207" title="">2023a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span><em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.1.1.1" style="color:#2E9B42;">Interaction Techniques to Specify AI Values</span>.

<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.1.1.2">How humans could interactively specify values in AI development?</span></em>
</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">This sub-research question investigates how human values are interactively<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
We define ”interactions” broadly to encompass both ”synchronous” and ”asynchronous” interactions between humans and AI:
(1) “Synchronous Interactions” indicates real-time exchanges where humans and AI systems interact simultaneously.
(2) “Asynchronous Interactions” allows for delays between actions and responses, such as data annotations by humans to train the AI models.
</span></span></span> specified for AI systems to ensure alignment. It aims to elucidate the interaction techniques by which AI systems manifest or instantiate human values, thereby revealing the underlying mechanisms that shape their behavior or functionality.
<span class="ltx_text" id="S4.SS1.SSS2.p1.1.1"></span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text" id="S4.SS1.SSS2.p2.1.1" style="color:#2E9B42;">Explicit Human Feedback</span>. This dimension refers to the direct specification of human values through explicitly defined formats or mechanisms.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p2.1.2" style="border-color: #2E9B42;">Principles</span> provides AI systems with explicitly defined principles, guidelines, or rules that dictate behavior or decision-making in alignment with human values <cite class="ltx_cite ltx_citemacro_citep">(Petridis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib151" title="">2024a</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib12" title="">2022b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p2.1.3" style="border-color: #2E9B42;">Rating and Ranking</span> is widely used to assign numerical scores or rankings to options or outcomes based on their alignment with human values <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib34" title="">2024</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib233" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p2.1.4" style="border-color: #2E9B42;">Natural Language Interaction/Conversations</span> allows humans to interact with AI systems through natural language interfaces to express and communicate human values <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib12" title="">2022b</a>; Hwang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib79" title="">2023</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib62" title="">2024</a>; Feng and Boyd-Graber, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib46" title="">2019</a>; Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib229" title="">2023a</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib110" title="">2024</a>; Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib73" title="">2022</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib172" title="">2023a</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib237" title="">2021</a>)</cite>.
For <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p2.1.5" style="border-color: #2E9B42;">Multimodal Feedback</span>, human values can also be provided in multiple modalities, such as sketches/images and gestures <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib32" title="">2022</a>)</cite>, to convey human values <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib29" title="">2022</a>; Geva et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib54" title="">2022</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib218" title="">2022</a>; Jörke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib86" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1"><span class="ltx_text" id="S4.SS1.SSS2.p3.1.1" style="color:#2E9B42;">Implicit Human Feedback</span>.
This dimension refers to the indirect representation or inference of human values within AI systems through patterns, signals, or cues embedded in the data or decision-making processes.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p3.1.2" style="border-color: #2E9B42;">Discarded Options</span> refers to the options or choices that human discard when interacting with AI systems during the decision-making processes, which also potentially infer human values <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib143" title="">2021b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p3.1.3" style="border-color: #2E9B42;">Language Analysis</span> means that textual data and language patterns can also contain rich information to identify implicit references to human values or value-related concepts <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib116" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p3.1.4" style="border-color: #2E9B42;">Theory of Mind</span> refers to the ability of agents and people to attribute mental states, such as beliefs, intentions, desires, emotions, knowledge, percepts, and non-literal communication, to themselves and others <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib121" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p3.1.5" style="border-color: #2E9B42;">Social Relationships</span> refers to the implicit values derived from analyzing human social relations and behaviors, which are derived from external sources (<em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS2.p3.1.6">e.g.,</em> social network) that can inherently reflect their values <cite class="ltx_cite ltx_citemacro_citep">(Isajanyan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib80" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1"><span class="ltx_text" id="S4.SS1.SSS2.p4.1.1" style="color:#2E9B42;">Simulated Human Value Feedback.</span> When human values in explicit formats are expensive or impossible to collect, one may simulate human-like feedback within AI systems to approximate human responses and preferences regarding specific values.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p4.1.2" style="border-color: #2E9B42;">Human Feedback Simulators</span>
uses computational algorithms to simulate human-like feedback on values, based on predefined criteria or training data <cite class="ltx_cite ltx_citemacro_citep">(Petridis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib150" title="">2024b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib102" title="">2023a</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p4.1.3" style="border-color: #2E9B42;">Comparison to Human Data</span>
refers to developing techniques that assess the likelihood or probability of AI-generated outputs matching human behaviors in a reference set or dataset <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib42" title="">2024</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS2.p4.1.4" style="border-color: #2E9B42;">Synthetic Data</span> curates data by generating synthetic comparisons based on naive assumptions or heuristic rules, followed by post-validation to ensure feedback quality <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib92" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Align <span class="ltx_text" id="S4.SS2.3.2" style="color:#1271CA;">AI</span> to
<span class="ltx_text" id="S4.SS2.4.3" style="color:#DE4A4D;">Humans</span>:  <span class="ltx_text" id="S4.SS2.1.1" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" id="S4.SS2.1.1.g1" src="x12.png" width="29"/></span> Integrating Human Specifications into AI</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">RQ2. How can human values be integrated into the development of AI?</span>
Existing studies have explored diverse methods to integrate human values into AI.
We structure them by summarizing <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">how to integrate general human values (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS1" title="4.2.1. Integrating General Values to AI: how to incorporate general human values into AI development? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>) and customized human values (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS2" title="4.2.2. Customizing AI Values: how to customize AI to incorporate values from individuals or human groups? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>) throughout AI development stages?</em>, and then elaborating <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.3">what are the evaluation methods (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS3" title="4.2.3. Evaluating AI Systems: how to evaluate AI regarding human values? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>) and supported platforms (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2.SSS4" title="4.2.4. Ecosystem and Platforms: how to build the ecosystem to facilitate human-AI alignment? ‣ 4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>) for the AI development</em>?
Additionally, we answer the four “<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4" style="color:#1271CA;">Sub-Research Questions</span>” by introducing the answer “<span class="ltx_text" id="S4.SS2.p1.1.5" style="color:#1271CA;">Dimensions</span>“ and providing “<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.p1.1.6" style="border-color: #1271CA;">Codes</span>“ associated with “Example Papers”.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS2.SSS1.1.1" style="color:#1271CA;">Integrating General Values to AI</em>: <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.2.2">how to incorporate general human values into AI development?</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1"><span class="ltx_text" id="S4.SS2.SSS1.p1.1.1"></span>
This sub-research question focuses on the process of incorporating broad, universally recognized human values into the development of AI systems.
The goal is to ensure that AI systems align with overarching ethical principles and societal norms, thereby promoting trust, acceptance, and responsible use.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text" id="S4.SS2.SSS1.p2.1.1" style="color:#1271CA;">Instruction Data.</span> This dimension refers to the types of data and processes used to provide guidance or direction to AI systems during their development.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p2.1.2" style="border-color: #1271CA;">Human Annotation</span> makes data with human-generated labels or annotations that indicate the presence or relevance of specific human values <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib175" title="">2023b</a>; Pei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib145" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p2.1.3" style="border-color: #1271CA;">Human-AI CoAnnotation</span> leverages both human expertise and AI capabilities to collaboratively annotate the data
 <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib190" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib103" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p2.1.4" style="border-color: #1271CA;">Simulated Human Data</span> generates synthetic or simulated data that mimics human behaviors, preferences, or decision-making processes to provide training signals for AI systems <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib43" title="">2023</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib92" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text" id="S4.SS2.SSS1.p3.1.1" style="color:#1271CA;">Model Learning.</span> This dimension refers to the model architecture design and training stages after the data collection, where human values are integrated during the model learning process.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p3.1.2" style="border-color: #1271CA;">Online Human Alignment</span> integrates human values into AI systems in real-time or during active system operation, often through interactive feedback loops or adaptive learning mechanisms. Examples include real-time user feedback and online training <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib41" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p3.1.3" style="border-color: #1271CA;">Offline Human Alignment</span> incorporates human values into AI systems prior to deployment or during offline training phases, without direct user interaction <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib157" title="">2023</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib183" title="">2024</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib233" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1"><span class="ltx_text" id="S4.SS2.SSS1.p4.1.1" style="color:#1271CA;">Inference Stage.</span> This dimension involves evaluating the alignment of AI systems with human values and assessing their performance and behavior in relation to predefined criteria or benchmarks.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p4.1.2" style="border-color: #1271CA;">Prompting</span> leverages prompting methods, such as in-context learning and chain-of-thought, on trained AI systems to elicit or critique AI regarding the encoded values <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib12" title="">2022b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib207" title="">2023a</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p4.1.3" style="border-color: #1271CA;">External Tool Interaction</span> integrates external tools, like code interpreter for debugging, to cross-check and refine their initial generated content <cite class="ltx_cite ltx_citemacro_citep">(Gou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib58" title="">2024</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS1.p4.1.4" style="border-color: #1271CA;">Response Search</span> generates a diverse range of high-quality outputs from which to choose <cite class="ltx_cite ltx_citemacro_citep">(Bradley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib23" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS2.SSS2.1.1" style="color:#1271CA;">Customizing AI Values</em>: <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.2.2">how to customize AI to incorporate values from individuals or human groups?</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1"><span class="ltx_text" id="S4.SS2.SSS2.p1.1.1"></span>
This sub-research question explores the customization of AI values to align with specific contexts, domains, or user preferences.
The goal is to enhance the alignment of AI systems within specific application domains or user communities.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text" id="S4.SS2.SSS2.p2.1.1" style="color:#1271CA;">Customized Data.</span>
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p2.1.2" style="border-color: #1271CA;">Finetuning with Curated Datasets</span> aims to curate datasets for specific individuals or societal groups, and further finetune the pre-trained AI models on these specific datasetsto align them with targeted human groups and values <cite class="ltx_cite ltx_citemacro_citep">(Felkner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib45" title="">2023</a>)</cite>.
These curated datasets include data collected from socio-demographic groups <cite class="ltx_cite ltx_citemacro_citep">(Orlikowski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib133" title="">2023</a>)</cite>, users’ history data <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib118" title="">2023</a>)</cite>, expert-selected data for imitation learning <cite class="ltx_cite ltx_citemacro_citep">(Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib27" title="">2023</a>)</cite> and others.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text" id="S4.SS2.SSS2.p3.1.1" style="color:#1271CA;">Adapt Model by Learning.</span>
This dimension
involves refining or adjusting AI values through techniques for customization, such as iterative learning process, model enhancements, or structural modifications.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p3.1.2" style="border-color: #1271CA;">Group-based Learning</span> trains AI models from specific user groups or communities to capture group-specific values or preferences <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib239" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p3.1.3" style="border-color: #1271CA;">Active Learning</span> aims to interative selecting and lebeling data samples for AI model training based on their potential to improve alignment with user preference or values <cite class="ltx_cite ltx_citemacro_citep">(Peschl et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib148" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p3.1.4" style="border-color: #1271CA;">Inserted Adapters</span> incorporates adapter modules or components into AI model architectures to fine-tune specific aspects or behaviors <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib189" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p3.1.5" style="border-color: #1271CA;">Mixture of Experts</span> combines multiple specialized models or experts to collectively capture diverse perspectives and values, with each expert focusing on a specific subset of the data or problem space <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib174" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p3.1.6" style="border-color: #1271CA;">Enhanced Knowledge</span> enhances AI model’s representations and embeddings with additional knowledge or context to improve alignment with user preferences or values <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib204" title="">2023c</a>; Dalvi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib35" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1"><span class="ltx_text" id="S4.SS2.SSS2.p4.1.1" style="color:#1271CA;">Interactive Alignment</span>
This dimension
involves actively engaging users or stakeholders in the process of customizing AI values to align with specific contexts, domains, or user preferences.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p4.1.2" style="border-color: #1271CA;">Interactive Learning</span>
enables the users to provide feedback or corrections to AI models in real time, such as using interactive tutorials and user-driven customization interfaces <cite class="ltx_cite ltx_citemacro_citep">(Maghakian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib123" title="">2023</a>; Swazinna et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib193" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p4.1.3" style="border-color: #1271CA;">Steering Prompts</span>
provides users with prompts or cues to steer the behavior or decision-making of AI systems towards desired outcomes or values <cite class="ltx_cite ltx_citemacro_citep">(Lahoti et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib97" title="">2023</a>; Fei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib44" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS2.p4.1.4" style="border-color: #1271CA;">Proactive Alignment</span>
anticipates user needs and preferences based on historical data or user profiles and proactively adjusting AI systems accordingly <cite class="ltx_cite ltx_citemacro_citep">(Welch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib211" title="">2022</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib172" title="">2023a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS2.SSS3.1.1" style="color:#1271CA;">Evaluating AI Systems</em>: <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.2.2">how to evaluate AI regarding human values?</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1"><span class="ltx_text" id="S4.SS2.SSS3.p1.1.1"></span>
The rise in the use of LLMs has also seen the rise of automatic evaluation of generated natural language text evaluation in different contexts. But in particular, a research dimension has focused on analyzing how closely values discussed in humane context has been adapted to AI models/applications and how these are being evaluated.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1"><span class="ltx_text" id="S4.SS2.SSS3.p2.1.1" style="color:#1271CA;">Human-In-The-Loop Evaluation</span>. This dimension involves incorporating human judgement, feedback, or interaction into the evaluation process to assess the effectiveness, robustness, and ethical implications of integrating human values into AI systems.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS3.p2.1.2" style="border-color: #1271CA;">Human Evaluation</span> solicits feedback, opinions, or assessments from human evaluators to gauge the alignment of AI systems with human values and ethical standards <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib112" title="">2022</a>; Sharma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib171" title="">2023</a>; Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib135" title="">2023</a>; Kiesel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib91" title="">2022</a>; Hosking et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib74" title="">2024</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS3.p2.1.3" style="border-color: #1271CA;">Human-AI Collaborative Evaluation</span> collaboratively evaluates AI systems with both human evaluators and large language models (LLMs) to leverage the strengths of both human judgement and AI capabilities <cite class="ltx_cite ltx_citemacro_citep">(Kwon and Mihindukulasooriya, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib96" title="">2023</a>; Ammanabrolu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib3" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1"><span class="ltx_text" id="S4.SS2.SSS3.p3.1.1" style="color:#1271CA;">Automatic Evaluation</span>. This dimension involves using computational methods or algorithms to assess the alignment of AI systems with human values, without direct human involvement.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS3.p3.1.2" style="border-color: #1271CA;">Human Simulators</span> use simulation models or virtual agents to mimic human behavior and assess AI performance in human-like scenarios. Typical methods include agent-based simulations, synthetic user models, and others <cite class="ltx_cite ltx_citemacro_citep">(Jia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib81" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib202" title="">2023d</a>; Felkner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib45" title="">2023</a>; Ruan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib161" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS3.p3.1.3" style="border-color: #1271CA;">Evaluation Benchmarks</span> establishes standardized benchmarks or metrics for evaluating AI performance in relation to human values and ethical considerations <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib190" title="">2023</a>; Ramezani and Xu, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib158" title="">2023</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib76" title="">2024</a>; Kiesel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib91" title="">2022</a>; Rao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib159" title="">2023</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib101" title="">2023a</a>; Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib176" title="">2021</a>; Sharma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib171" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS3.p3.1.4" style="border-color: #1271CA;">Distribution Difference</span> compares the difference between the output distribution from AI generations and human data to evaluate AI <cite class="ltx_cite ltx_citemacro_citep">(Patel and Pavlick, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib144" title="">2021</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib101" title="">2023a</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="975" id="S4.F3.g1" src="x14.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The fine-grained topology of “Align <span class="ltx_text" id="S4.F3.3.1" style="color:#DE4A4D;">Human</span> to
<span class="ltx_text" id="S4.F3.4.2" style="color:#1271CA;">AI</span>” direction, which studies humans’ cognitive and behavioral adaptation to the AI systems, which aims to help humans better critique, collaborate with, and co-adapt to AI.
</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS2.SSS4.1.1" style="color:#1271CA;">Ecosystem and Platforms</em>: <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.2.2">how to build the ecosystem to facilitate human-AI alignment?</span>
</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1"><span class="ltx_text" id="S4.SS2.SSS4.p1.1.1"></span>
The ecosystem and platforms refer to the broader context in which AI systems operate and interact with other agents, platforms, or environments. This includes the infrastructure, frameworks, and technologies that support the development, deployment, and utilization of AI systems.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS4.p1.1.2" style="border-color: #1271CA;">LLM-based Agents</span> are based on large language models (LLMs) such as GPT (Generative Pre-trained Transformer) models, which have been pre-trained on vast amounts of text data <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib43" title="">2023</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib222" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib216" title="">2023a</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib243" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS4.p1.1.3" style="border-color: #1271CA;">RL-based Agents</span> are based on reinforcement learning (RL) algorithms to learn and adapt their behavior based on feedback from the environment or human users <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib234" title="">2024</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS2.SSS4.p1.1.4" style="border-color: #1271CA;">Annotation Platforms</span> refers to the ecosystems that are designed to crowdsource human demonstrations as collected data for reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Gerstgrasser et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib53" title="">2021</a>)</cite> and supervised finetuning learning for alignment <cite class="ltx_cite ltx_citemacro_citep">(Pei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib145" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S4.SS2.SSS4.tab1">
<table class="ltx_tabular ltx_centering ltx_align_top" id="S4.SS2.SSS4.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS2.SSS4.tab1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.SS2.SSS4.tab1.1.1.1.1" style="background-color:#E6E6E6;">
<span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span><span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1" style="color:#808080;">  
<span class="ltx_inline-block ltx_align_top" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1">
<span class="ltx_p" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.1" style="width:424.9pt;"><span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.1.1" style="color:#808080;background-color:#E6E6E6;">
<span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.1.1.1">Underexplored Dimensions in Aligning <span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.1.1.1.1" style="color:#1271CA;background-color:#E6E6E6;">AI</span> to <span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.1.1.1.2" style="color:#DE4A4D;background-color:#E6E6E6;">Humans</span></span> (See Sec <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS1" title="5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.1</span></a> for supporting data and evidence):</span></span>
<span class="ltx_p" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.2.1" style="color:#808080;background-color:#E6E6E6;">1. Implicit human feedback and simulated human value feedback are under-explored in existing research work.</span></span>
<span class="ltx_p" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.3"><span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.3.1" style="color:#808080;background-color:#E6E6E6;">2. Developing and customizing AI during the inference stage or in an interactive way is under-explored.</span></span>
<span class="ltx_p" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.4"><span class="ltx_text" id="S4.SS2.SSS4.tab1.1.1.1.1.1.1.4.1" style="color:#808080;background-color:#E6E6E6;">3. Human-in-the-loop evaluation is much less explored than automatic evaluation.</span></span>
</span>  <span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span></span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS4.p2">
<br class="ltx_break"/>
<p class="ltx_p" id="S4.SS2.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p2.1.1">DIRECTION-II: <span class="ltx_text" id="S4.SS2.SSS4.p2.1.1.1" style="position:relative; bottom:-1.8pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="S4.SS2.SSS4.p2.1.1.1.g1" src="x15.png" width="21"/></span> ALIGN <span class="ltx_text" id="S4.SS2.SSS4.p2.1.1.2" style="color:#DE4A4D;">HUMANS</span> to
<span class="ltx_text" id="S4.SS2.SSS4.p2.1.1.3" style="color:#1271CA;">AI</span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS4.p3">
<span class="ltx_ERROR undefined" id="S4.SS2.SSS4.p3.1">\say</span>
<p class="ltx_p" id="S4.SS2.SSS4.p3.2"><em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS4.p3.2.1">When interacting with people, AI agents do not just influence the state of the world – they also influence the actions people take in response to the agent, and even their underlying intentions and strategies</em> <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib72" title="">2023</a>)</cite>.
From a <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS4.p3.2.2">long-term</em> perspective, it is essential to consider the dynamic changes around human-AI alignment.
This direction outlines alignment research from the <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p3.2.3">Human-centered perspective</span> (e.g., HCI/Social Science domains) and provides <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p3.2.4">HCI researchers and user experience designers</span> with guidance for approaching two core research questions:
<span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p3.2.5">RQ3.</span> Human Cognitive Adjustment to AI (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3" title="4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.3</span></a>) and <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p3.2.6">RQ4.</span> Human Adaptive Behavior to AI (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4" title="4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Align <span class="ltx_text" id="S4.SS3.3.2" style="color:#DE4A4D;">Humans</span> to
<span class="ltx_text" id="S4.SS3.4.3" style="color:#1271CA;">AI</span>:  <span class="ltx_text" id="S4.SS3.1.1" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" id="S4.SS3.1.1.g1" src="x16.png" width="29"/></span> Human’s Perceptual Adaptation to AI</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">RQ3. How might humans learn to perceive, explain, and critique AI?</span>
Humans needs to understand AI to better specify their demands and collaborate with AI. Also, as AI systems produce a range of risks, it is important to elicit humans’ critical thinking of AI instead of relying on AI blindly. Therefore, we categorize existing literature to answer the questions of <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2">how humans learn to perceive and understand AI</em> (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3.SSS1" title="4.3.1. Perceiving and Understanding AI: how do humans learn to perceive and explain AI systems? ‣ 4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>), and <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.3">how to engage in critical thinking on AI?</em> (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS3.SSS2" title="4.3.2. Critical Thinking around AI: how do humans think critically about AI systems? ‣ 4.3. Align Humans to AI: Human’s Perceptual Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>).
We also answer the two “<span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4" style="color:#533283;">Sub-Research Questions</span>” by introducing the answer “<span class="ltx_text" id="S4.SS3.p1.1.5" style="color:#533283;">Dimensions</span>“ and providing “<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.p1.1.6" style="border-color: #533283;">Codes</span>“ associated with “Example Papers”.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS3.SSS1.1.1" style="color:#533283;">Perceiving and Understanding AI</em>: <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.2.2">how do humans learn to perceive and explain AI systems?</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1"><span class="ltx_text" id="S4.SS3.SSS1.p1.1.1"></span>
Addressing the problem of human perception and understanding of AI includes the fundamental education and training required for less technical people to improve understanding of the behind mechanisms and outputs produced by AI systems. It includes visualizations and human-centered explanations tohelp more people learn to understand AI outputs.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><span class="ltx_text" id="S4.SS3.SSS1.p2.1.1" style="color:#533283;">Educating and Training Humans</span>. AI is increasingly incorporated into the daily lives of a broad spectrum of users, including those with little to no technical knowledge on how AI operates. Here we discuss how to help these less technical people become more AI literate, or better trained to use AI.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS1.p2.1.2" style="border-color: #533283;">AI Literacy and Awareness</span> is broadly defined as the core competencies required for less technical people to better use and collaborate with AI <cite class="ltx_cite ltx_citemacro_citep">(Long and Magerko, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib115" title="">2020</a>)</cite>.
Beyond,
there is also more in depth and explicit <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS1.p2.1.3" style="border-color: #533283;">Training Courses</span> to support individuals to better collaborate and utilize AI <cite class="ltx_cite ltx_citemacro_citep">(Paranjape et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib140" title="">2019</a>; McDonald and Pan, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib124" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1"><span class="ltx_text" id="S4.SS3.SSS1.p3.1.1" style="color:#533283;">AI Sensemaking and Explanations</span>.
To improve people’s understanding of AI systems and generated outputs, various techniques and approaches have been developed to help people learn to make sense of and explain the model’s outputs.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS1.p3.1.2" style="border-color: #533283;">Perception and Sensemaking on AI</span> involves the process through which humans learn to make better sense of AI mechanisms and decision makings <cite class="ltx_cite ltx_citemacro_citep">(Kaur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib89" title="">2022</a>; Sivaraman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib179" title="">2023</a>; Petridis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib149" title="">2023</a>)</cite>.
People also need to understand how AI systems arrive at specific generated outputs. Prior studies in <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS1.p3.1.3" style="border-color: #533283;">Human-Centered Explanations</span> have examined various approaches and interactive techniques to increase human understanding of AI generations and outputs <cite class="ltx_cite ltx_citemacro_citep">(Tenney et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib194" title="">2020</a>; Tintarev and Masthoff, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib196" title="">2007</a>; Smith-Renner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib181" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS3.SSS2.1.1" style="color:#533283;">Critical Thinking around AI</em>: <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.2.2">how do humans think critically about AI systems?</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1"><span class="ltx_text" id="S4.SS3.SSS2.p1.1.1"></span>
Beyond simply perceiving and understanding AI, individuals need to compare their mental model of AI with their own mental model to judge whether the AI is behaving rationally and ethically.
Broadly, this involves exploring the ways in which humans engage in critical thinking and reflect on their interactions with and evolving understanding of AI technologies.
Here we discuss studies that help humans become more capable to identify biases and errors in AI output, and the ethical implications that arise from using AI algorithms in decision-making processes. Also, we discuss how humans need to calibrate their mental models to be more aligned with AI.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1"><span class="ltx_text" id="S4.SS3.SSS2.p2.1.1" style="color:#533283;">Trust and Reliance on AI Decisions</span> refers to the extent to which people mentally trust the AI competency and practically rely on AI for output generation and decision making.
We use the term <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p2.1.2" style="border-color: #533283;">Trustworthiness</span> to indicate whether humans decide to trust the reliability, integrity, and competence of an AI system in delivering accurate and trustworthy decisions or recommendations to the users <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib232" title="">2019</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib120" title="">2023a</a>; Srivastava et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib186" title="">2023</a>)</cite>.
We define <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p2.1.3" style="border-color: #533283;">AI Reliability</span> as to what extent humans utilize and rely on AI to automate decision making in practice with low error rates and robustness performance <cite class="ltx_cite ltx_citemacro_citep">(Schemmer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib166" title="">2023</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citep">(Schemmer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib166" title="">2023</a>; Vasconcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib199" title="">2023</a>; Suresh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib192" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p2.1.4" style="border-color: #533283;">Selective AI Adoption</span> refers to the criteria and considerations that guide the adoption or rejection of AI technologies based on their perceived benefits, risks, and alignment with user needs and preferences <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib61" title="">2024</a>; Vasconcelos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib199" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1"><span class="ltx_text" id="S4.SS3.SSS2.p3.1.1" style="color:#533283;">Ethical Considerations and AI Auditing</span> refers to potential moral and societal issues that arise from the development, deployment and use of AI systems, as well as systematic examination and evaluation of AI systems regarding these issues.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p3.1.2" style="border-color: #533283;">Ethical Concerns and AI Mitigation</span> encourages humans to carefully consider if the AI systems possess ethical concerns (<em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS2.p3.1.3">e.g.,</em> such as bias and discrimination, privacy violations and the potential for harm or misuse) and implement strategies and practices to address and reduce these ethical concerns associated with AI <cite class="ltx_cite ltx_citemacro_citep">(Holstein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib71" title="">2019</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p3.1.4" style="border-color: #533283;">Audit Deployed AI</span> refers to the systematic examination of AI systems to ensure that they operate as intended, comply with ethical and legal standards, and do not cause unintended harm <cite class="ltx_cite ltx_citemacro_citep">(Bandy, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib14" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p4">
<p class="ltx_p" id="S4.SS3.SSS2.p4.1"><span class="ltx_text" id="S4.SS3.SSS2.p4.1.1" style="color:#533283;">Re-Calibrating Cognition to Align with AI</span>. This sub-category deals with interventions and techniques to help users adjust (1) their own mental model of how the AI operates and (2) recalibrate their perception of the AI.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p4.1.2" style="border-color: #533283;">Updating Human Mental Model of AI</span> means that when the AI is not aligned with humans, it is important to adjust human perceptions of the confidence in AI systems based on the model’s capability and track record <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib68" title="">2023</a>; Kocielnik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib93" title="">2019</a>)</cite> .
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS3.SSS2.p4.1.3" style="border-color: #533283;">Recalibrating Trust and Reliability</span> indicates that humans adjust their perceptions of trust and reliability in AI systems based on their performance and reliability. This is also important to foster appropriate reliance and skepticism <cite class="ltx_cite ltx_citemacro_citep">(Wischnewski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib214" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Align <span class="ltx_text" id="S4.SS4.3.2" style="color:#DE4A4D;">Humans</span> to
<span class="ltx_text" id="S4.SS4.4.3" style="color:#1271CA;">AI</span>:  <span class="ltx_text" id="S4.SS4.1.1" style="position:relative; bottom:-6.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="29" id="S4.SS4.1.1.g1" src="x17.png" width="29"/></span> Human’s Behavioral Adaptation to AI</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">RQ4. How do humans and society make behavioral changes and react to AI advancement?</span>
As AI becomes increasingly integrated into daily life, it is essential to understand its influence on humans, encompassing both positive and negative aspects.
Moreover, it is crucial to determine how individuals and society can best and most appropriately respond to this influence.
To this end, we summarize literature to answer <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.2">how do humans learn to collaborate with AI in diverse AI roles?</em> (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS1" title="4.4.1. Human-AI Collaboration Mechanisms: what are human strategies to collaborate with AI that have differing levels of capabilities? ‣ 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>), <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.3">how humans and society are impacted by AI
</em> (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS2" title="4.4.2. AI Impact on Humans and Society: how are humans influenced by AI systems ? ‣ 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>) and <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.1.4">how might we assess these impacts?</em> (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS3" title="4.4.3. Evaluation in Human Studies: how might we evaluate and understand the impact of AI on humans and society? ‣ 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4.3</span></a>)
We articulate the three “<span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.5" style="color:#DE4A4D;">Sub-Research Questions</span>” by introducing the answer “<span class="ltx_text" id="S4.SS4.p1.1.6" style="color:#DE4A4D;">Dimensions</span>“ and providing “<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.p1.1.7" style="border-color: #DE4A4D;">Codes</span>“ associated with “Example Papers”.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS4.SSS1.1.1" style="color:#DE4A4D;">Human-AI Collaboration Mechanisms</em>: <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.2.2">what are human strategies to collaborate with AI that have differing levels of capabilities?</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1"><span class="ltx_text" id="S4.SS4.SSS1.p1.1.1"></span>
This category looks at many ways that humans and AI can collaborate, such as teamwork, co-creation, and coproduction.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1"><span class="ltx_text" id="S4.SS4.SSS1.p2.1.1" style="color:#DE4A4D;">AI Assistant for Humans</span> captures the essence of a symbiotic relationship where AI systems are designed to bolster human capabilities, with humans steering the interactions.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p2.1.2" style="border-color: #DE4A4D;">LLM-based Demand Specification</span> employs LLMs to interpret and respond to human requests, powering virtual assistants and chatbots that streamline information retrieval and improve task accuracy <cite class="ltx_cite ltx_citemacro_citep">(Terry et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib195" title="">2023</a>)</cite>. This naturally extends to <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p2.1.3" style="border-color: #DE4A4D;">LLM-based Prompt Strategy</span>, which helps humans to better write prompts using
additional abstraction and scaffolding methods.
The example system can enhance humans’ capability in generating intelligent prompts and suggestions, such as autocomplete and question-generation tools for LLM-based systems <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib218" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib209" title="">2024</a>)</cite>, facilitating humans’ smooth and intuitive decision-making processes.
In the creative arena, <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p2.1.4" style="border-color: #DE4A4D;">LLM-based Prototypes</span> utilize AI to transform human ideas into tangible or organized concepts  <cite class="ltx_cite ltx_citemacro_citep">(Gebreegziabher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib52" title="">2023</a>)</cite>, enabling professionals to explore and refine a wide array of artistic possibilities with AI-generated options <cite class="ltx_cite ltx_citemacro_citep">(Huh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib78" title="">2023</a>)</cite>.
On a broader note, researchers have also explored how <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p2.1.5" style="border-color: #DE4A4D;">AI-Assisted Task and Decisions</span>
aims to achieve complementary performance for human-AI collaborative tasks by empower humans to discern when and how to adopt AI-assisted recommendations or AI-generated explanations for decision-making <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib31" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib205" title="">2022</a>; Wang and Yin, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib206" title="">2023</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib240" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib217" title="">2023c</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1"><span class="ltx_text" id="S4.SS4.SSS1.p3.1.1" style="color:#DE4A4D;">Human-AI Partnership</span> refers to a collaborative relationship where humans and AI systems work together as partners, combining their respective strengths to achieve shared goals more effectively.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p3.1.2" style="border-color: #DE4A4D;">Simulated Agency</span> enables humans to collaborate with AI partners with
simulated agency or autonomy (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p3.1.3">e.g.,</em> autonomous agents and collaborative robots <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib142" title="">2023</a>)</cite>) to make decisions collaboratively.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p3.1.4" style="border-color: #DE4A4D;">Reciprocal Learning</span> focuses on how humans learn from and exchange knowledge with AI systems,
enhancing human-AI collective capabilities and performance through knowledge-sharing platforms and collaborative filtering <cite class="ltx_cite ltx_citemacro_citep">(Zagalsky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib235" title="">2021</a>; Pinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib152" title="">2023</a>)</cite>. <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p3.1.5" style="border-color: #DE4A4D;">AI Delegation</span> allows humans to delegate AI partners to help finish tasks or responsibilities,
facilitated by task assignment algorithms and workflow management systems <cite class="ltx_cite ltx_citemacro_citep">(Pinski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib152" title="">2023</a>)</cite>.
Furthermore, <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p3.1.6" style="border-color: #DE4A4D;">AI-Mediated Tasks</span> emphasizes how traditional human tasks or behaviors (<em class="ltx_emph ltx_font_italic" id="S4.SS4.SSS1.p3.1.7">e.g.,</em> communication) would be changed by incorporating AI partners in the loop <cite class="ltx_cite ltx_citemacro_citep">(Bernstein et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib19" title="">2010</a>; Sundar and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib191" title="">2022</a>; Xu and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib223" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p3.1.8" style="border-color: #DE4A4D;">Co-Design with AI</span> explores AI as a design material or a partner in prototyping, which enables humans to converse with AI in situations that can collaboratively improve the design outcomes.
<cite class="ltx_cite ltx_citemacro_citep">(Yildirim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib231" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib209" title="">2024</a>)</cite></p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p4">
<p class="ltx_p" id="S4.SS4.SSS1.p4.1"><span class="ltx_text" id="S4.SS4.SSS1.p4.1.1" style="color:#DE4A4D;">AI Tutoring for Human Learning</span>
investigates how humans improve their learning and knowledge throughinteractions with AI tutors that can perform better than humans in some tasks.
With the tailored instructions and customized feedback from AI tutors, humans can enhance their the learning outcomes and facilitate mastery of new skills more effectively than traditional learning methods.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p4.1.2" style="border-color: #DE4A4D;">AI Tutor for Technical Skills</span> refers to empowering humans to learn technical skills from AI tutors.
For technical subjects like coding, AI tutors can analyze a learner’s progress in real time, adjusting the pace, content, and approach to ensure a solid understanding of complex concepts and practical skills such as programming <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib119" title="">2024</a>)</cite>.
Similarly, <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS1.p4.1.3" style="border-color: #DE4A4D;">AI Tutor for Social and Behavioral Skills</span> involves enabling humans to learn social and behavioral skills from AI tutors.
Humans can leverage virtual simulations, created by AI tutoring systems, to
practice public speaking, interpersonal communication, and other soft skills. By analyzing verbal and non-verbal cues, humans can receive
constructive feedback on areas such as body language, tone, and delivery from AI, ultimately enhancing their ability to communicate effectively across various settings <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib146" title="">2021</a>; Shaikh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib170" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS4.SSS2.1.1" style="color:#DE4A4D;">AI Impact on Humans and Society</em>: <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.2.2">how are humans influenced by AI systems
?</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1"><span class="ltx_text" id="S4.SS4.SSS2.p1.1.1"></span>
This category explores the effects of AI advancement on human behaviors, attitudes, and societal dynamics. It involves examining the behavioral changes, adaptations, and reactions that individuals, groups and wider communities undergo in response to the proliferation of AI technologies.
The goal is to elucidate the multifaceted impacts of AI on human behavior and society and to inform policy-making, education, and intervention efforts.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1"><span class="ltx_text" id="S4.SS4.SSS2.p2.1.1" style="color:#DE4A4D;">Impacts on Participatory Individuals and Groups</span> covers the effects of AI advancement on the behaviors, attitudes, and experiences of both individuals and groups. This dimension focuses on examining how AI technologies influence decision-making, creativity, privacy, and authorship. <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p2.1.2" style="border-color: #DE4A4D;">Decision Making</span> refers to analyzing how AI technologies influence human decision-making processes, including biases, preferences, and risk assessment, in various domains such as healthcare, finance, and personal life <cite class="ltx_cite ltx_citemacro_citep">(Shen and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib173" title="">2020</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p2.1.3" style="border-color: #DE4A4D;">Human Creativity</span> explores the impact of AI technologies on human creativity, innovation, and expression, including the augmentation or automation of creative tasks and the emergence of new forms of artistic expression <cite class="ltx_cite ltx_citemacro_citep">(Ashkinaze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib7" title="">2024</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p2.1.4" style="border-color: #DE4A4D;">Privacy</span> relates to investigating the implications of AI technologies for individual privacy rights, data protection, and surveillance, including concerns about data collection, tracking, and algorithmic profiling <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib104" title="">2024a</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p2.1.5" style="border-color: #DE4A4D;">Authorship</span> catalogs issues related to intellectual property, attribution, and ownership of AI-generated content, including questions of legal responsibility, copyright infringement, and plagiarism detection. Salient is that AI can produce increasingly realistic, synthetic data quickly and at low cost, which brings forth tensions around the use of such data to make decisions <cite class="ltx_cite ltx_citemacro_citep">(Hämäläinen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib66" title="">2023</a>; Atreja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1"><span class="ltx_text" id="S4.SS4.SSS2.p3.1.1" style="color:#DE4A4D;">Societal Concerns and AI Impacts</span> involves the broader societal implications and consequences of AI advancement on misinformation, education, social relationships, norms, job displacement, and other aspects of human society.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p3.1.2" style="border-color: #DE4A4D;">Misinformation and Moderation</span> concerns the challenges of misinformation, disinformation, and online content moderation in the context of AI-driven information ecosystems, including concerns about algorithmic bias and filter bubbles <cite class="ltx_cite ltx_citemacro_citep">(Atreja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib10" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p3.1.3" style="border-color: #DE4A4D;">Impacts on Education</span> pertains to assessing the effects of AI technologies on education systems, learning outcomes, pedagogical practices, and workforce training, including opportunities for personalized learning and skill development <cite class="ltx_cite ltx_citemacro_citep">(Kazemitabaar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib90" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p3.1.4" style="border-color: #DE4A4D;">Impacts on Social Relationship and Norms</span> explores how AI technologies shape interpersonal relationships, social interactions, and cultural norms, including changes in communication patterns, social dynamics, and ethical considerations <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib30" title="">2022</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p3.1.5" style="border-color: #DE4A4D;">Workplace</span> refers to examining the effects of automation and AI technologies on employment patterns, job markets, and workforce dynamics, including concerns about job displacement, re-skilling, and economic inequality <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib141" title="">2021a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p4">
<p class="ltx_p" id="S4.SS4.SSS2.p4.1"><span class="ltx_text" id="S4.SS4.SSS2.p4.1.1" style="color:#DE4A4D;">Reaction to AI Advancement</span> involves societal responses, regulatory frameworks, and policy initiatives aimed at addressing the challenges and opportunities posed by AI technologies. This dimension encompasses efforts to regulate AI deployment, re-calibrate societal acceptance, and manage potential backlash. For example, reaction to bias and discrimination in algorithmic decision making can depend on how people perceive the machine and the context of use, i.e., if the machine is considered an actor embedded in social structures that call for blame
when harmful decisions are made <cite class="ltx_cite ltx_citemacro_citep">(Lima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib107" title="">2023a</a>)</cite>. <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p4.1.2" style="border-color: #DE4A4D;">AI Regulatory and Policy</span> includes regulatory frameworks, legal frameworks, and policy initiatives aimed at governing AI development, deployment, and use, including concerns about ethics, safety, and accountability <cite class="ltx_cite ltx_citemacro_citep">(Hacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib64" title="">2023</a>; Lucaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib117" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p4.1.3" style="border-color: #DE4A4D;">Shifts in AI Acceptance</span> relates to investigating societal attitudes, organizational practices, and acceptance of AI technologies over time in practice, including shifts in public opinion and AI utilization by humans and institutes regarding AI deployment and impact <cite class="ltx_cite ltx_citemacro_citep">(Lima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib108" title="">2023b</a>; Pruss, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib155" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p4.1.4" style="border-color: #DE4A4D;">Surveillance and Intervention</span> emphasizes the need for transparency, monitoring, and human oversight in the algorithmic decision-making process. This approach enables better human control over AI systems and helps mitigate potential risks associated with their use <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib141" title="">2021a</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS2.p4.1.5" style="border-color: #DE4A4D;">Responsible AI Checklists</span> involves the creation of ethical guidelines, such as those focusing on fairness and transparency, to ensure the responsible development and deployment of AI systems. These published principles serve as a foundation for guiding ethical AI practices <cite class="ltx_cite ltx_citemacro_citep">(Madaio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib122" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS4.SSS3.1.1" style="color:#DE4A4D;">Evaluation in Human Studies</em>: <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.2.2">how might we evaluate and understand the impact of AI on humans and society?</span>
</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">We summarize common empirical methods used to rigorously understand and assess the impact of AI on humans. Specifically, we focus on two types of impact. On the micro-level, we discuss how to evaluate the effectiveness of human-AI collaboration; on the macro-level, we discuss how to assess the impact of AI on a large group of people over a long period of time.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS3.p2">
<p class="ltx_p" id="S4.SS4.SSS3.p2.1"><span class="ltx_text" id="S4.SS4.SSS3.p2.1.1" style="color:#DE4A4D;">Evaluate Human-AI Collaboration</span> refers to the evaluation of the effectiveness of an AI system in collaboration with humans. It is key to not only consider the final output, but also the interaction experience <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib100" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.2" style="border-color: #DE4A4D;">Human-AI Team Performance</span>
compares human-AI team performance with the performance of humans alone without AI collaboration
The metrics for measuring performance should include both task success metrics (e.g., accuracy) as well as indicators of efficiency <cite class="ltx_cite ltx_citemacro_citep">(Mozannar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib128" title="">2024</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib39" title="">2023a</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.3" style="border-color: #DE4A4D;">Cognitive Workload and User Satisfaction</span>
involves understanding the degree of cognitive load that the user experiences when interacting with the system, as well as user satisfaction with the interaction and the final outcome. Such aspects are often captured via surveys or interviews <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib39" title="">2023a</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib100" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.4" style="border-color: #DE4A4D;">Control and Trust</span> refers to how user control can
support the avoidance of catastrophic AI failures, especially in high-stakes settings where AI mistakes could lead to harm <cite class="ltx_cite ltx_citemacro_citep">(Srikanth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib185" title="">2024</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib40" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.5" style="border-color: #DE4A4D;">Interaction Behavioral Analytics</span> refers to measuring task performance quantitatively. This approach includes recording user interaction data and analyzing the patterns <cite class="ltx_cite ltx_citemacro_citep">(Villalba et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib201" title="">2023</a>; Perry et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib147" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.6" style="border-color: #DE4A4D;">Qualitative Survey and Interviews </span> refers to qualitative approaches to understanding human-AI interaction. Commonly used methods include qualitative survey questions (i.e., open-ended) and user interviews to assess aspects of the user experience. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib236" title="">2023b</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p2.1.7" style="border-color: #DE4A4D;">Statistical Analysis</span> utilizes methods such as regression analysis to quantitatively analyze and evaluate data from human studies, allowing for the verification of hypotheses <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib203" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS3.p3">
<p class="ltx_p" id="S4.SS4.SSS3.p3.1"><span class="ltx_text" id="S4.SS4.SSS3.p3.1.1" style="color:#DE4A4D;">Evaluate Societal Impact</span>
refers to the macro-impact of a group of people as they come to use AI broadly. This dimension requires sufficient scale and time. The aim is to understand how the group’s behavior changes as people within it frequently interact with AI.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p3.1.2" style="border-color: #DE4A4D;">Public Opinion Surveys</span> aims to investigate the impact of AI on human measures of interest though deploying and analyzing large scale questionnaires
 <cite class="ltx_cite ltx_citemacro_citep">(Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib164" title="">2023</a>)</cite>.
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS4.SSS3.p3.1.3" style="border-color: #DE4A4D;">Behavioral Data Analytics</span>
collects large-scale and potentially longitudinal behavior data, with the aim of understanding how pattern evolve over time <cite class="ltx_cite ltx_citemacro_citep">(Shin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib177" title="">2023</a>; Zhou and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib241" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S4.SS4.SSS3.tab1">
<table class="ltx_tabular ltx_centering ltx_align_top" id="S4.SS4.SSS3.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS4.SSS3.tab1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id="S4.SS4.SSS3.tab1.1.1.1.1" style="background-color:#E6E6E6;">
<span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1" style="color:#808080;">  
<span class="ltx_inline-block ltx_align_top" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1">
<span class="ltx_p" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.1" style="width:424.9pt;"><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.1.1" style="color:#808080;background-color:#E6E6E6;">
<span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.1.1.1">Underexplored Dimensions in Aligning <span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.1.1.1.1" style="color:#DE4A4D;background-color:#E6E6E6;">Humans</span> to <span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.1.1.1.2" style="color:#1271CA;background-color:#E6E6E6;">AI</span></span> (See Sec <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS1" title="5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.1</span></a> for supporting data and evidence):</span></span>
<span class="ltx_p" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.2.1" style="color:#808080;background-color:#E6E6E6;">1. Educating and training humans on AI literacy is under-investigated.</span></span>
<span class="ltx_p" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.3"><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.3.1" style="color:#808080;background-color:#E6E6E6;">2. Auditing AI for various ethical values is not fully explored.</span></span>
<span class="ltx_p" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.4"><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.4.1" style="color:#808080;background-color:#E6E6E6;">3. The collaboration between humans and AI with similar or superior capabilities is under-explored.</span></span>
<span class="ltx_p" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.5"><span class="ltx_text" id="S4.SS4.SSS3.tab1.1.1.1.1.1.1.5.1" style="color:#808080;background-color:#E6E6E6;">4. The societal impacts of and reactions to AI advancements are not fully explored.</span></span>
</span>  <span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span></span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Findings and Discussions on Current Gaps</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, our aim is to consolidate key findings derived from our analysis of the framework and the current state of literature we reviewed.
We begin by analyzing the overall trends and gaps in the literature (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS1" title="5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.1</span></a>). We then focus on three essential aspects: the relationship between human values and alignment (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS2" title="5.2. Insights into Human Values for Alignment ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.2</span></a>), the potential interaction modes used to specify human values (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS3" title="5.3. Interaction Techniques for Specifying Human Values: A Cross-Domain Analysis ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="547" id="S5.F4.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The number of papers for each dimension in the <em class="ltx_emph ltx_font_italic" id="S5.F4.4.1">bidirectional human-AI alignment</em> framework. Out of papers that are relevant to each research question (<em class="ltx_emph ltx_font_italic" id="S5.F4.5.2">i.e.,</em> gray bars), we show the number of papers that are relevant to each dimension (<em class="ltx_emph ltx_font_italic" id="S5.F4.6.3">i.e.,</em> color bars). This figure illustrates the extent to which each dimension has been explored by existing research. We provide more analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS1" title="5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.1</span></a>. </figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Meta Analysis of Trends and Gaps</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Based on our coding of all papers, we computed the number of relevant papers for each dimension in the proposed bi-directional framework. Based on the distribution in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.F4" title="Figure 4 ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>, we noticed that in existing literature certain dimensions are over- or under-represented. We outline the under-explored dimensions in two directions in the highlighted grey bars in the end of Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS2" title="4.2. Align AI to Humans: Integrating Human Specifications into AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4" title="4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4</span></a>, respectively, and provide more details below.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Underexplored dimensions in AI-centered alignment research.</span>
Most literature <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p2.1.2" style="color:#2E9B42;">specified human values</em> using explicit human feedback, whereas implicit and simulated human feedback were largely under-explored.
In the <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p2.1.3" style="color:#1271CA;">integrating human specifications to AI</em> research question,
Besides, developing and customizing AI models during the inference stage or using the interactive approach is under-explored.
Also, human-in-the-loop evaluation is much less explored than automatic evaluation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Underexplored dimensions in Human-centered alignment research.</span>
Additionally, many studies on <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p3.1.2" style="color:#533283;">human cognitive adjustment to AI</em> focus on enabling AI sensemaking and explanations so that humans can better understand, trust, and rely on AI.
However, these studies often focused on explaining AI decision-making justification rather than educating people to acquire general skills and competencies required to understand, use, critique, and interact effectively with AI systems (i.e., AI literacy).
AI literacy <cite class="ltx_cite ltx_citemacro_citep">(Long and Magerko, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib115" title="">2020</a>)</cite> plays a fundamental role in ensuring people understand and use AI correctly, and warrants deeper exploration. Also, we observed a wide range of studies <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib218" title="">2022</a>; Gebreegziabher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib52" title="">2023</a>; Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib142" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib119" title="">2024</a>)</cite> which developed interactive mechanisms and prototypes to empower <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS1.p3.1.3" style="color:#DE4A4D;">human collaboration with AI</em>. However, most studies assumed that AI plays an assistant role, being less capable than humans. This situation might change over time. Moreover, the influence of AI advancements on human behavior, social relationships, and societal changes is essential but remains largely unexplored.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S5.F5.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The interaction techniques for specifying values in human-AI alignment. We compare the common interaction techniques used for the model “Learning” and “Inference” stages in human-focused (<em class="ltx_emph ltx_font_italic" id="S5.F5.3.1">e.g.,</em> HCI) and AI-focused (<em class="ltx_emph ltx_font_italic" id="S5.F5.4.2">e.g.,</em> NLP/ML) research studies.
</figcaption>
</figure>
<figure class="ltx_table" id="S5.SS1.tab1">
<table class="ltx_tabular ltx_centering ltx_align_top" id="S5.SS1.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.SS1.tab1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" id="S5.SS1.tab1.1.1.1.1" style="background-color:#E6E6E6;">
<span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span><span class="ltx_text" id="S5.SS1.tab1.1.1.1.1.1" style="color:#808080;">  
<span class="ltx_inline-block ltx_align_top" id="S5.SS1.tab1.1.1.1.1.1.1">
<span class="ltx_p" id="S5.SS1.tab1.1.1.1.1.1.1.1" style="width:424.9pt;"><span class="ltx_text" id="S5.SS1.tab1.1.1.1.1.1.1.1.1" style="color:#808080;background-color:#E6E6E6;">
<span class="ltx_text ltx_font_bold" id="S5.SS1.tab1.1.1.1.1.1.1.1.1.1">Takeaways of Interaction Techniques for Alignment.</span></span></span>
<span class="ltx_p" id="S5.SS1.tab1.1.1.1.1.1.1.2"><span class="ltx_text" id="S5.SS1.tab1.1.1.1.1.1.1.2.1" style="color:#808080;background-color:#E6E6E6;">1. Some common human feedback formats (rating, ranking) used in NLP/ML are not often studied in HCI.</span></span>
<span class="ltx_p" id="S5.SS1.tab1.1.1.1.1.1.1.3"><span class="ltx_text" id="S5.SS1.tab1.1.1.1.1.1.1.3.1" style="color:#808080;background-color:#E6E6E6;">2. Diverse human interactive feedbacks in HCI are not fully used in AI development in NLP/ML fields.</span></span>
</span>  <span class="ltx_rule" style="width:2.0pt;color:#808080;background:#808080;display:inline-block;"> </span></span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Insights into Human Values for Alignment</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Our analysis, based on the adaptation of Schwartz’s Theory of Basic Values and our comprehensive literature review, identifies three critical findings for future research:</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Value Prioritization in AI Systems.</span> Human value systems are not merely subsets of values, but ordered systems with relative priorities <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite>. For instance, <cite class="ltx_cite ltx_citemacro_citet">Schwartz (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite> presented the definition for this phenomenon: <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2"><span class="ltx_ERROR undefined" id="S5.SS2.p2.1.2.1">\say</span>a value is ordered by importance relative to other values to form a system of value priorities.
The relative importance of multiple values guides action….The trade-off among relevant, competing values guides attitudes and behaviors.</em>.
Current AI alignment algorithms, often based on datasets of human preferences <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib134" title="">2022</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib183" title="">2024</a>; Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib157" title="">2023</a>)</cite>, may inadvertently prioritize majority values, potentially neglecting those of marginalized groups <cite class="ltx_cite ltx_citemacro_citep">(Fleisig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib47" title="">2023</a>)</cite>. Future research should address this complex interplay of values in AI systems.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Universal vs. Personalized AI Values</span>. While certain values are universally expected from AI (e.g., capability, equity, responsibility), others may be undesirable in specific contexts <cite class="ltx_cite ltx_citemacro_citep">(Ngo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib129" title="">2024</a>)</cite> (e.g., seeking power). Simultaneously, AI models should be adaptable to diverse human value systems <cite class="ltx_cite ltx_citemacro_citep">(Sorensen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib184" title="">2024</a>)</cite>. Research is needed to develop methods for identifying appropriate value sets for specific individuals or groups, and for customizing AI to align with user values while maintaining ethical principles.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Disparities in Value Expectations and Evaluation</span>. The fundamental differences between humans and AI necessitate distinct approaches to value evaluation. For instance, assessing AI honesty may require mechanistic interpretability <cite class="ltx_cite ltx_citemacro_citep">(Bereska and Gavves, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib18" title="">2024</a>)</cite>, a more rigorous standard than that applied to humans. Future studies should explore methods for evaluating and explaining AI values and calibrating human expectations accordingly.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Interaction Techniques for Specifying Human Values: A Cross-Domain Analysis</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Our research reveals disparities in interaction techniques for human-AI value alignment across AI-centered (NLP/ML) and human-centered (HCI) domains. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.F5" title="Figure 5 ‣ 5.1. Meta Analysis of Trends and Gaps ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>, this analysis focuses on three key areas:</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Domain-Specific Interaction Techniques</span>. The interaction techniques in AI-centered (NLP/ML) and Human-centered (HCI) alignment studies are often differ <cite class="ltx_cite ltx_citemacro_citep">(Brath, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib24" title="">2021</a>)</cite>.
NLP/ML studies primarily utilize numeric and natural language-based techniques. Also, NLP/ML research explore implicit feedback to extract human hidden feedback. In contrast, HCI research encompasses a broader range of graphical and multi-modal interaction signals (e.g., sketches, location information) beyond text and images. This disparity suggests potential gaps in extracting comprehensive human behavioral information.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Stage-Specific Interaction Techniques</span>. In NLP/ML, the learning stage predominantly employs rating and ranking interactions for alignment in dataset generation. However, when humans use AI in the inference stage, as demonstrated in HCI research, involves more diverse user interactions. This discrepancy highlights the need for alignment between model development and practical deployment.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Divergent Data Utilization</span>. NLP/ML typically uses interaction outputs as training datasets, while HCI analyzes this data to understand human behavior and feedback. As AI systems evolve, developing new interaction modes to capture a broader spectrum of human expression becomes crucial.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Future Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Drawing upon insights gained from the development of our framework and the associated coding analysis, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.F6" title="Figure 6 ‣ 6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6</span></a>, we propose future research aiming to achieve the long-term alignment goal by identifying three important challenges from near-term to long-term objectives, including the Specification Game (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS1" title="6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.1</span></a>), Dynamic Co-evolution of Alignment (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS2" title="6.2. Dynamic Co-evolution of Alignment ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.2</span></a>), and Safeguarding Coadaptation (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS3" title="6.3. Safeguarding Co-adaptation ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Specification Game</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">An important near-term challenge is resolving the “Specification Game”, which involves precisely defining and implementing AI goals and behaviors to align with human intentions and values. Next, we will introduce how synergistic efforts from two directions can potentially address this challenge.</p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S6.F6.g1" src="x20.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>We envision future research directions to achieve long-term human-AI alignment with both efforts from the “Align <span class="ltx_text" id="S6.F6.9.1" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.F6.3.m1.1"><semantics id="S6.F6.3.m1.1b"><mo id="S6.F6.3.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.F6.3.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.F6.3.m1.1c"><csymbol cd="latexml" id="S6.F6.3.m1.1.1.cmml" xref="S6.F6.3.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.F6.3.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.F6.3.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.F6.10.2" style="color:#DE4A4D;">Humans</span>” (AI-centered research) and “Align <span class="ltx_text" id="S6.F6.11.3" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.F6.4.m2.1"><semantics id="S6.F6.4.m2.1b"><mo id="S6.F6.4.m2.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.F6.4.m2.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.F6.4.m2.1c"><csymbol cd="latexml" id="S6.F6.4.m2.1.1.cmml" xref="S6.F6.4.m2.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.F6.4.m2.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.F6.4.m2.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.F6.12.4" style="color:#1271CA;">AI</span>” (Human-centered research) directions.
We elaborate the three important future challenges, including Specification Game (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS1" title="6.1. Specification Game ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.1</span></a>), Dynamic Co-evolution of Alignment (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS2" title="6.2. Dynamic Co-evolution of Alignment ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.2</span></a>), and Safeguarding Coadaptation (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S6.SS3" title="6.3. Safeguarding Co-adaptation ‣ 6. Future Directions ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">6.3</span></a>).
</figcaption>
</figure>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.1.1">Aligning <span class="ltx_text" id="S6.SS1.SSS1.1.1.1" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS1.SSS1.1.1.m1.1"><semantics id="S6.SS1.SSS1.1.1.m1.1b"><mo id="S6.SS1.SSS1.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS1.SSS1.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS1.SSS1.1.1.m1.1.1.cmml" xref="S6.SS1.SSS1.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS1.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS1.SSS1.1.1.2" style="color:#DE4A4D;">Humans</span>: Integrate fully specified human values into aligning AI</span>
</h4>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Individuals often possess value systems that encompass multiple values with varying priorities, rather than a single value, to guide their behaviors <cite class="ltx_cite ltx_citemacro_citep">(Schwartz, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib167" title="">1994</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib168" title="">2012</a>)</cite>.
Also, these priorities can change dynamically throughout an individual’s life stages.
As such,
It is more realistic to select values compatible with specific societies or situations, given the fact that we live in a diverse world <cite class="ltx_cite ltx_citemacro_citep">(Gabriel, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib50" title="">2020</a>)</cite>.
Future research, inspired by Social Choice Theory <cite class="ltx_cite ltx_citemacro_citep">(Arrow, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib6" title="">2012</a>)</cite>, could focus on using democratic processes to aggregate individual values into collective agreements. Building on the summaries in Sections  <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS1" title="4.1. Align AI to Humans: Human Values and Specifications ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.1</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5.SS2" title="5.2. Insights into Human Values for Alignment ‣ 5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5.2</span></a>, researchers can employ democratic methods to identify diverse subsets of human values for AI alignment.
Additionally, creating datasets that represent these values is crucial.
Besides, it is crucial yet challenging <em class="ltx_emph ltx_font_italic" id="S6.SS1.SSS1.p1.1.1">for AI designers to investigate how to fully specify the appropriate values
and to further integrate these values into AI alignment</em>.
Future important area involves developing algorithms, such as the Bradley-Terry Model <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib157" title="">2023</a>)</cite> or Elo Rating System <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib12" title="">2022b</a>)</cite>, to convert heterogeneous human values into AI-compatible formats for training reward models and guiding reinforcement learning. Researchers should also explore AI models capable of aligning with unstructured human data, including free-form descriptions of values, multimedia, or sensor recordings depicting human behavior.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS2.1.1">Aligning <span class="ltx_text" id="S6.SS1.SSS2.1.1.1" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS1.SSS2.1.1.m1.1"><semantics id="S6.SS1.SSS2.1.1.m1.1b"><mo id="S6.SS1.SSS2.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS1.SSS2.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS1.SSS2.1.1.m1.1.1.cmml" xref="S6.SS1.SSS2.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS2.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS1.SSS2.1.1.2" style="color:#1271CA;">AI</span>: Elicit nuanced and contextual human values during diverse interactions</span>
</h4>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">Current alignment methods use instructions, ratings, and rankings to infer human values, which can not fully capture all relevant human values and constraints.
Future research should focus on optimizing interactive interfaces to efficiently elicit human values. These interfaces can leverage diverse interaction modes to capture comprehensive human value information.
Additionally, people often struggle to formulate optimal prompts for AI, accurately specify their requirements, and articulate their desired values, which can change based on context and time.
Developing proactive interfaces that use conversational techniques to elicit nuanced and evolving values is also crucial.
Implicit human signals that indicate values are also frequently overlooked.
Additionally, systems that track interactions to hypothesize and validate implicit human values in real-time should be designed.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Dynamic Co-evolution of Alignment</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The challenge ahead lies in comprehending and effectively navigating the dynamic interplay among human values, societal evolution, and the progression of AI technologies.
Future studies in these directions aim to bolster a synergistic co-evolution between AI and human societies, adapting both to each other’s changes and advancements.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.1.1">Aligning <span class="ltx_text" id="S6.SS2.SSS1.1.1.1" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS2.SSS1.1.1.m1.1"><semantics id="S6.SS2.SSS1.1.1.m1.1b"><mo id="S6.SS2.SSS1.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS2.SSS1.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS2.SSS1.1.1.m1.1.1.cmml" xref="S6.SS2.SSS1.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS2.SSS1.1.1.2" style="color:#DE4A4D;">Humans</span>: Co-evolve AI with changes in humans and society</span>
</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1"><em class="ltx_emph ltx_font_italic" id="S6.SS2.SSS1.p1.1.1">(i)</em>
Existing literature often treats AI alignment as static, ignoring its dynamic nature. A long-term perspective must consider the co-evolution of AI, humans, and society. As AI systems evolve and scale up, they gain new capabilities, making it essential to ensure their goals remain aligned with human values. Thus, alignment solutions require continuous oversight and updates.
Future research should develop methods for continuously updating AI with limited data without compromising alignment values and performance.
This could involve forecasting human value evolution and preparing AI with flexible strategies like prompting or interventions.
<em class="ltx_emph ltx_font_italic" id="S6.SS2.SSS1.p1.1.2">(ii)</em>
Additionally, AI advancements also influence human actions and values, necessitating adaptive alignment solutions. Ensuring AI co-evolves with human and societal changes is crucial for robust alignment.
This challenge could potentially be addressed by forecasting the potential evolution trajectories of human values or behavioral patterns, and preparing AI with the flexibility to adapt in advance, for example, through prompting or intervention strategies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.1.1">Aligning <span class="ltx_text" id="S6.SS2.SSS2.1.1.1" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS2.SSS2.1.1.m1.1"><semantics id="S6.SS2.SSS2.1.1.m1.1b"><mo id="S6.SS2.SSS2.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS2.SSS2.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS2.SSS2.1.1.m1.1.1.cmml" xref="S6.SS2.SSS2.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS2.SSS2.1.1.2" style="color:#1271CA;">AI</span>: Adapt humans and society to the latest AI advancements</span>
</h4>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1"><em class="ltx_emph ltx_font_italic" id="S6.SS2.SSS2.p1.1.1">(i)</em>
While current AI systems lag behind humans in many tasks, identifying and handling AI mistakes, including knowing when to seek human intervention, remains essential. Future research should focus on developing validation mechanisms that enable humans to interpret and verify AI outputs. This could involve designing interfaces that allow humans to request step-by-step justifications from AI or integrating tools to verify the truthfulness of AI referring to Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS1" title="4.4.1. Human-AI Collaboration Mechanisms: what are human strategies to collaborate with AI that have differing levels of capabilities? ‣ 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>. Additionally, developing interfaces that enable groups of humans to collaboratively validate AI outputs and creating scalable validation tools for large-scale applications are important directions.
<em class="ltx_emph ltx_font_italic" id="S6.SS2.SSS2.p1.1.2">(ii)</em>
As AI advances, it becomes essential to develop systems that enable humans to utilize AI with capabilities surpassing their own. Research is needed to understand how individuals can interpret and validate AI outputs for tasks beyond their abilities and leverage advanced AI sustainably, avoiding issues like job displacement or loss of purpose. Another research direction is designing strategies to enhance human capabilities by learning from advanced AI, including gaining knowledge and building skills.
<em class="ltx_emph ltx_font_italic" id="S6.SS2.SSS2.p1.1.3">(iii)</em>
As AI integrates more into daily tasks, its impact on human values, behaviors, capabilities, and society remains uncertain. Continuous examination of AI’s influence on individuals, social relationships, and broader societal changes is vital. Research should assess how humans and society adapt to AI advancements, guiding AI’s future evolution. Potential areas include evaluating changes in individual behavior, social relationships, and societal governance as AI replaces traditional human skills. Understanding these dynamic changes is essential for grasping the broader impact of AI on humanity and society.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Safeguarding Co-adaptation</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">As AI gains autonomy and capability, the risks associated with its instrumental actions, as a means toward accomplishing its final goals, increase.
These actions can be undesirable for humans.
Therefore, safeguarding the co-adaptation between humans and AI is crucial.
We next explore future research to address this challenge from both directions.
</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span><span class="ltx_text ltx_font_bold" id="S6.SS3.SSS1.1.1">Aligning <span class="ltx_text" id="S6.SS3.SSS1.1.1.1" style="color:#1271CA;">AI</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS3.SSS1.1.1.m1.1"><semantics id="S6.SS3.SSS1.1.1.m1.1b"><mo id="S6.SS3.SSS1.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS3.SSS1.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS3.SSS1.1.1.m1.1.1.cmml" xref="S6.SS3.SSS1.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS1.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS3.SSS1.1.1.2" style="color:#DE4A4D;">Humans</span>: Specify the goals of an AI system into interpretable and controllable instrumental actions for humans</span>
</h4>
<div class="ltx_para ltx_noindent" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1"><em class="ltx_emph ltx_font_italic" id="S6.SS3.SSS1.p1.1.1">(i)</em>
As advanced AI systems become more complex, they present greater challenges for human interpretation and control. It is crucial to empower humans to detect and interpret AI misconduct and enable human intervention to prevent power-seeking AI behavior. Research should focus on designing corrigible mechanisms for easy intervention and correction, including modular AI architectures and robust override protocols that allow human operators to halt or redirect AI activities. These components should be human-interpretable, enabling scenario testing.
<em class="ltx_emph ltx_font_italic" id="S6.SS3.SSS1.p1.1.2">(ii)</em>
Furthermore, advanced AI systems may intentionally mislead or disobey humans, generating plausible fabrications <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Iziev, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib84" title="">2022</a>)</cite>. Developing reliable interpretability mechanisms to validate the faithfulness and honesty of AI behaviors is essential. This includes correlating AI behaviors with internal neuron activity signals, akin to physiological indicators in human polygraph tests <cite class="ltx_cite ltx_citemacro_citep">(Association et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib9" title="">2004</a>)</cite>. Inspecting these indicators can help humans assess the truthfulness of AI interpretations and prevent risky actions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span><span class="ltx_text ltx_font_bold" id="S6.SS3.SSS2.1.1">Aligning <span class="ltx_text" id="S6.SS3.SSS2.1.1.1" style="color:#DE4A4D;">Humans</span> to <math alttext="\mapsto" class="ltx_Math" display="inline" id="S6.SS3.SSS2.1.1.m1.1"><semantics id="S6.SS3.SSS2.1.1.m1.1b"><mo id="S6.SS3.SSS2.1.1.m1.1.1" mathcolor="#F9AB10" stretchy="false" xref="S6.SS3.SSS2.1.1.m1.1.1.cmml">↦</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.1.1.m1.1c"><csymbol cd="latexml" id="S6.SS3.SSS2.1.1.m1.1.1.cmml" xref="S6.SS3.SSS2.1.1.m1.1.1">maps-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.1.1.m1.1d">\mapsto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.1.1.m1.1e">↦</annotation></semantics></math> <span class="ltx_text" id="S6.SS3.SSS2.1.1.2" style="color:#1271CA;">AI</span>: Empower humans to identify and intervene in AI instrumental and final strategies in collaboration</span>
</h4>
<div class="ltx_para ltx_noindent" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1"><em class="ltx_emph ltx_font_italic" id="S6.SS3.SSS2.p1.1.1">(i)</em>
Preventing advanced AI from engaging in risky actions requires robust human supervision. Essential steps include developing training and simulation environments with scenario-based exercises and timely feedback, and creating interactive dashboards for real-time monitoring. These dashboards should feature effective data visualization, anomaly detection, and prompt alert systems for immediate intervention.
<em class="ltx_emph ltx_font_italic" id="S6.SS3.SSS2.p1.1.2">(ii)</em>
Scalable solutions are needed for supervising AI across various applications. Real-time oversight becomes more challenging with widespread AI deployment, necessitating advanced autonomous monitoring tools. These tools should learn normal AI behavior and flag deviations immediately. Integrating training environments, interactive dashboards, and scalable diagnostic tools will enhance human ability to manage AI risks, ensuring better alignment with human values.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Limitations and Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">One limitation of this work is the scope of the sampled and filtered papers. The rapidly growing literature on human-AI alignment spans diverse venues across many domains. Instead of an exhaustive collection, we focused on developing a holistic bidirectional human-AI alignment framework using essential research questions, dimensions, and codes.
Our surveyed papers and team members primarily focus on computing-related fields like ML, NLP, and HCI, though alignment research also involves disciplines like cognitive science, psychology, and STS (Science, Technology, and Society). Our framework can naturally extend to these areas as needed.
Despite these limitations, we believe our bidirectional human-AI alignment framework serves as a foundational reference for future researchers.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In conclusion, this study clarifies the definitions and scope of core terminologies of human-AI alignment and conducts a systematic review of over 400 related papers spanning diverse domains such as NLP, AI, HCI, and social science. Additionally, we introduce a novel conceptual framework of “Bidirectional Human-AI Alignment”, structuring the surveyed literature taxonomies into “aligning AI to humans” and “aligning humans to AI” with detailed categories and example papers. Furthermore, we identify limitations and risks in this area quantitatively and qualitatively, analyzing a fine-grained human value taxonomy, interaction modes for alignment, and discrepancies between AI and human evaluation. To pave the way for future studies, we discuss five stages to achieve the alignment goals from near-term to long-term perspectives and identify new possibilities to highlight future directions and opportunities in research.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">blo (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock">Humans are biased. Generative AI is even worse.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bloomberg.com/graphics/2023-generative-ai-bias/" title="">https://www.bloomberg.com/graphics/2023-generative-ai-bias/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammanabrolu et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock">Aligning to Social Norms and Values in Interactive Narratives. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 5994–6017.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amodei et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016.

</span>
<span class="ltx_bibblock">Concrete problems in AI safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv:1606.06565</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Foundational Challenges in Assuring Alignment and Safety of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">arXiv:2404.09932</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arrow (2012)</span>
<span class="ltx_bibblock">
Kenneth J Arrow. 2012.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Social choice and individual values</em>. Vol. 12.

</span>
<span class="ltx_bibblock">Yale university press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashkinaze et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, and Eric Gilbert. 2024.

</span>
<span class="ltx_bibblock">How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">arXiv:2401.13481</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashktorab et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zahra Ashktorab, Q Vera Liao, Casey Dugan, James Johnson, Qian Pan, Wei Zhang, Sadhana Kumaravel, and Murray Campbell. 2020.

</span>
<span class="ltx_bibblock">Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 4, CSCW2 (2020), 1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Association et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
American Psychological Association et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2004.

</span>
<span class="ltx_bibblock">The truth about lie detectors (aka polygraph tests).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">Recuperado de: https://www. apa. org/topics/cognitive-neuroscience/polygraph</em> (2004).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Atreja et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shubham Atreja, Libby Hemphill, and Paul Resnick. 2023.

</span>
<span class="ltx_bibblock">Remove, Reduce, Inform: What Actions do People Want Social Media Platforms to Take on Potentially Misleading Content?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 7, CSCW2 (2023), 1–33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al<span class="ltx_text" id="bib.bib11.3.1">.</span> 2022a.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.4.1">arXiv:2204.05862</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2022b.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">arXiv:2212.08073</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bakker et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al<span class="ltx_text" id="bib.bib13.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Fine-tuning language models to find agreement among humans with diverse preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">Advances in Neural Information Processing Systems</em> 35, 38176–38189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bandy (2021)</span>
<span class="ltx_bibblock">
Jack Bandy. 2021.

</span>
<span class="ltx_bibblock">Problematic machine behavior: A systematic literature review of algorithm audits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the acm on human-computer interaction</em> 5, CSCW1 (2021), 1–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banovic et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nikola Banovic, Zhuoran Yang, Aditya Ramesh, and Alice Liu. 2023.

</span>
<span class="ltx_bibblock">Being trustworthy is not enough: How untrustworthy artificial intelligence (ai) can deceive the end-users and gain their trust.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 7, CSCW1 (2023), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hritik Bansal, John Dang, and Aditya Grover. 2023.

</span>
<span class="ltx_bibblock">Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Managing extreme AI risks amid rapid progress.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">Science</em> (2024), eadn0117.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bereska and Gavves (2024)</span>
<span class="ltx_bibblock">
Leonard Bereska and Efstratios Gavves. 2024.

</span>
<span class="ltx_bibblock">Mechanistic Interpretability for AI Safety – A Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv:2404.14082</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bernstein et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Michael S Bernstein, Greg Little, Robert C Miller, Björn Hartmann, Mark S Ackerman, David R Karger, David Crowell, and Katrina Panovich. 2010.

</span>
<span class="ltx_bibblock">Soylent: a word processor with a crowd inside. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 23nd annual ACM symposium on User interface software and technology</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boggust et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Angie Boggust, Benjamin Hoover, Arvind Satyanarayan, and Hendrik Strobelt. 2022.

</span>
<span class="ltx_bibblock">Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonnefon et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Jean-François Bonnefon, Azim Shariff, and Iyad Rahwan. 2016.

</span>
<span class="ltx_bibblock">The social dilemma of autonomous vehicles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Science</em> 352, 6293 (2016), 1573–1576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boyd (2021)</span>
<span class="ltx_bibblock">
Karen L Boyd. 2021.

</span>
<span class="ltx_bibblock">Datasheets for datasets help ML engineers notice and understand ethical issues in training data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW2 (2021), 1–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradley et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth Stanley, Grégory Schott, and Joel Lehman. 2024.

</span>
<span class="ltx_bibblock">Quality-diversity through AI feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brath (2021)</span>
<span class="ltx_bibblock">
Richard Brath. 2021.

</span>
<span class="ltx_bibblock">Surveying Wonderland for many more literature visualization techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv:2110.08584</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buçinca et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021.

</span>
<span class="ltx_bibblock">To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW1 (2021), 1–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cabrera et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ángel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth Holstein, Ameet Talwalkar, Jason I Hong, and Adam Perer. 2023.

</span>
<span class="ltx_bibblock">Zeno: An interactive framework for behavioral evaluation of machine learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xin-Qiang Cai, Yu-Jie Zhang, Chao-Kai Chiang, and Masashi Sugiyama. 2023.

</span>
<span class="ltx_bibblock">Imitation Learning from Vague Feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carroll et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Micah Carroll, Davis Foote, Anand Siththaranjan, Stuart Russell, and Anca Dragan. 2024.

</span>
<span class="ltx_bibblock">AI Alignment with Changing and Influenceable Reward Functions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv:2405.17713</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Quan Ze Chen, Tobias Schnabel, Besmira Nushi, and Saleema Amershi. 2022.

</span>
<span class="ltx_bibblock">HINT: Integration Testing for AI-based features with Humans in the Loop. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">27th International Conference on Intelligent User Interfaces</em>. 549–565.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xusen Cheng, Xiaoping Zhang, Jason Cohen, and Jian Mou. 2022.

</span>
<span class="ltx_bibblock">Human vs. AI: Understanding the impact of anthropomorphism on consumer response to chatbots from the perspective of trust and relationship norms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Information Processing &amp; Management</em> 59, 3 (2022), 102940.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Minsuk Choi, Cheonbok Park, Soyoung Yang, Yonggyu Kim, Jaegul Choo, and Sungsoo Ray Hong. 2019.

</span>
<span class="ltx_bibblock">Aila: Attentive interactive labeling assistant for document classification through attention-based deep neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk Chang. 2022.

</span>
<span class="ltx_bibblock">TaleBrush: Sketching stories with generative pretrained language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dafoe and Russell (2016)</span>
<span class="ltx_bibblock">
Allan Dafoe and Stuart Russell. 2016.

</span>
<span class="ltx_bibblock">Yes, we are worried about the existential risk of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">MIT Technology Review</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2024.

</span>
<span class="ltx_bibblock">Safe rlhf: Safe reinforcement learning from human feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dalvi et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. 2022.

</span>
<span class="ltx_bibblock">Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>. 9465–9480.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dautenhahn et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2000)</span>
<span class="ltx_bibblock">
Kerstin Dautenhahn, Chrystopher L Nehaniv, and K Dautenhahn. 2000.

</span>
<span class="ltx_bibblock">Living with Socially Intelligent Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Human Cognition and Social Agent Technology, John Benjamins Publ. Co</em> (2000), 415–426.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande and Sharp (2022)</span>
<span class="ltx_bibblock">
Advait Deshpande and Helen Sharp. 2022.

</span>
<span class="ltx_bibblock">Responsible ai systems: who are the stakeholders?. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</em>. 227–236.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shehzaad Dhuliawala, Vilém Zouhar, Mennatallah El-Assady, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock">A Diachronic Perspective on User Trust in AI under Uncertainty. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 5567–5580.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel Tetreault, and Alejandro Jaimes. 2023a.

</span>
<span class="ltx_bibblock">Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 3321–3339.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel Tetreault, and Alejandro Jaimes. 2023b.

</span>
<span class="ltx_bibblock">Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 3321–3339.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023.

</span>
<span class="ltx_bibblock">Raft: Reward ranked finetuning for generative foundation model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">arXiv:2304.06767</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zibin Dong, Yifu Yuan, Jianye HAO, Fei Ni, Yao Mu, YAN ZHENG, Yujing Hu, Tangjie Lv, Changjie Fan, and Zhipeng Hu. 2024.

</span>
<span class="ltx_bibblock">AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023.

</span>
<span class="ltx_bibblock">Mitigating Label Biases for In-context Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 14014–14031.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Felkner et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Virginia Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. 2023.

</span>
<span class="ltx_bibblock">WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 9126–9140.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Boyd-Graber (2019)</span>
<span class="ltx_bibblock">
Shi Feng and Jordan Boyd-Graber. 2019.

</span>
<span class="ltx_bibblock">What can ai do for me? evaluating machine learning interpretations in cooperative play. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 24th International Conference on Intelligent User Interfaces</em>. 229–239.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fleisig et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Eve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin Blodgett, Hal Daumé III, Alexandra Olteanu, Emily Sheng, Dan Vann, and Hanna Wallach. 2023.

</span>
<span class="ltx_bibblock">FairPrism: evaluating fairness-related harms in text generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 6231–6251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forbes et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Social chemistry 101: Learning to reason about social and moral norms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv:2011.00620</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankl (1966)</span>
<span class="ltx_bibblock">
Viktor E Frankl. 1966.

</span>
<span class="ltx_bibblock">Self-transcendence as a human phenomenon.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Journal of Humanistic Psychology</em> 6, 2 (1966), 97–106.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabriel (2020)</span>
<span class="ltx_bibblock">
Iason Gabriel. 2020.

</span>
<span class="ltx_bibblock">Artificial intelligence, values, and alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Minds and machines</em> 30, 3 (2020), 411–437.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gajos and Mamykina (2022)</span>
<span class="ltx_bibblock">
Krzysztof Z Gajos and Lena Mamykina. 2022.

</span>
<span class="ltx_bibblock">Do people engage cognitively with AI? Impact of AI assistance on incidental learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">27th international conference on intelligent user interfaces</em>. 794–806.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebreegziabher et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Simret Araya Gebreegziabher, Zheng Zhang, Xiaohang Tang, Yihao Meng, Elena L Glassman, and Toby Jia-Jun Li. 2023.

</span>
<span class="ltx_bibblock">Patat: Human-ai collaborative qualitative coding with explainable interactive rule synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerstgrasser et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Matthias Gerstgrasser, Rakshit Trivedi, and David C Parkes. 2021.

</span>
<span class="ltx_bibblock">CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. 2022.

</span>
<span class="ltx_bibblock">LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. 12–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilbert (2024)</span>
<span class="ltx_bibblock">
Eric Gilbert. 2024.

</span>
<span class="ltx_bibblock">HCC Is All You Need: Alignment-The Sensible Kind Anyway-Is Just Human-Centered Computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv:2405.03699</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldfarb-Tarrant et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma Balkir, and Su Lin Blodgett. 2023.

</span>
<span class="ltx_bibblock">This prompt is measuring¡ mask¿: evaluating bias evaluation in language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Findings of the Association for Computational Linguistics: ACL 2023</em>. 2209–2225.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S Bernstein. 2022.

</span>
<span class="ltx_bibblock">Jury learning: Integrating dissenting voices into machine learning models. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.

</span>
<span class="ltx_bibblock">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nitesh Goyal, Minsuk Chang, and Michael Terry. 2024.

</span>
<span class="ltx_bibblock">Designing for Human-Agent Alignment: Understanding what humans want from their agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graham et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013.

</span>
<span class="ltx_bibblock">Moral foundations theory: The pragmatic validity of moral pluralism.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Advances in experimental social psychology</em>. Vol. 47. Elsevier, 55–130.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ziwei Gu, Ian Arawjo, Kenneth Li, Jonathan K Kummerfeld, and Elena L Glassman. 2024.

</span>
<span class="ltx_bibblock">An AI-Resilient Text Rendering Technique for Reading and Skimming Documents. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. 2024.

</span>
<span class="ltx_bibblock">Beyond imitation: Leveraging fine-grained quality signals for alignment. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Prakhar Gupta, Yang Liu, Di Jin, Behnam Hedayatnia, Spandana Gella, Sijia Liu, Patrick L Lange, Julia Hirschberg, and Dilek Hakkani-Tur. 2023.

</span>
<span class="ltx_bibblock">DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hacker et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Philipp Hacker, Andreas Engel, and Marco Mauer. 2023.

</span>
<span class="ltx_bibblock">Regulating ChatGPT and other large generative AI models. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em>. 1112–1123.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hajian et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Sara Hajian, Francesco Bonchi, and Carlos Castillo. 2016.

</span>
<span class="ltx_bibblock">Algorithmic bias: From discrimination discovery to fairness-aware data mining. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>. 2125–2126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hämäläinen et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023.

</span>
<span class="ltx_bibblock">Evaluating large language models in generating synthetic hci research data: a case study. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton (2020)</span>
<span class="ltx_bibblock">
Andy Hamilton. 2020.

</span>
<span class="ltx_bibblock">Conservatism.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">The Stanford Encyclopedia of Philosophy</em> (Spring 2020 ed.), Edward N. Zalta (Ed.). Metaphysics Research Lab, Stanford University.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziyao He, Yunpeng Song, Shurui Zhou, and Zhongmin Cai. 2023.

</span>
<span class="ltx_bibblock">Interaction of Thoughts: Towards Mediating Task Assignment in Human-AI Cooperation with a Capability-Aware Shared Mental Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hemphill (2020)</span>
<span class="ltx_bibblock">
Thomas A Hemphill. 2020.

</span>
<span class="ltx_bibblock">Human Compatible: Artificial Intelligence and the Problem of Control.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch Critch, Jerry Li Li, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Aligning AI With Shared Human Values. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holstein et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach. 2019.

</span>
<span class="ltx_bibblock">Improving fairness in machine learning systems: What do industry practitioners need?. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Joey Hong, Sergey Levine, and Anca Dragan. 2023.

</span>
<span class="ltx_bibblock">Learning to influence human behavior with offline reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Matt-Heun Hong, Lauren A Marsh, Jessica L Feuston, Janet Ruppert, Jed R Brubaker, and Danielle Albers Szafir. 2022.

</span>
<span class="ltx_bibblock">Scholastic: Graphical human-AI collaboration for inductive and interpretive text analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosking et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tom Hosking, Phil Blunsom, and Max Bartolo. 2024.

</span>
<span class="ltx_bibblock">Human Feedback is not Gold Standard. In <em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hovy and Yang (2021)</span>
<span class="ltx_bibblock">
Dirk Hovy and Diyi Yang. 2021.

</span>
<span class="ltx_bibblock">The importance of modeling social factors of language: Theory and practice. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 588–602.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2024.

</span>
<span class="ltx_bibblock">Do Large Language Models Know about Facts?. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9OevMUdods" title="">https://openreview.net/forum?id=9OevMUdods</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael Lyu. 2023.

</span>
<span class="ltx_bibblock">On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huh et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mina Huh, Yi-Hao Peng, and Amy Pavel. 2023.

</span>
<span class="ltx_bibblock">GenAssist: Making Image Generation Accessible. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
EunJeong Hwang, Bodhisattwa Majumder, and Niket Tandon. 2023.

</span>
<span class="ltx_bibblock">Aligning Language Models to User Opinions. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 5906–5919.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isajanyan et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Arman Isajanyan, Artur Shatveryan, David Kocharian, Zhangyang Wang, and Humphrey Shi. 2024.

</span>
<span class="ltx_bibblock">Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community. In <em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaogang Jia, Denis Blessing, Xinkai Jiang, Moritz Reuss, Atalay Donat, Rudolf Lioutikov, and Gerhard Neumann. 2024.

</span>
<span class="ltx_bibblock">Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. 2023.

</span>
<span class="ltx_bibblock">Evaluating and inducing personality in pre-trained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhijing Jin, Sydney Levine, Fernando Gonzalez Adauto, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Josh Tenenbaum, and Bernhard Schölkopf. 2022.

</span>
<span class="ltx_bibblock">When to make exceptions: Exploring language models as accounts of human moral judgment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">Advances in neural information processing systems</em> 35, 28458–28473.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Iziev (2022)</span>
<span class="ltx_bibblock">
Steven Johnson and Nikita Iziev. 2022.

</span>
<span class="ltx_bibblock">AI is mastering language. Should we trust what it says?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">The New York Times</em> 4 (2022), 15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones and Steinhardt (2022)</span>
<span class="ltx_bibblock">
Erik Jones and Jacob Steinhardt. 2022.

</span>
<span class="ltx_bibblock">Capturing failures of large language models via human cognitive biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Advances in Neural Information Processing Systems</em> 35, 11785–11799.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jörke et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Matthew Jörke, Yasaman S Sefidgar, Talie Massachi, Jina Suh, and Gonzalo Ramos. 2023.

</span>
<span class="ltx_bibblock">Pearl: A Technology Probe for Machine-Assisted Reflection on Personal Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the 28th International Conference on Intelligent User Interfaces</em>. 902–918.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dongjun Kang, Joonsuk Park, Yohan Jo, and JinYeong Bak. 2023.

</span>
<span class="ltx_bibblock">From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 15539–15559.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kapania et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shivani Kapania, Alex S Taylor, and Ding Wang. 2023.

</span>
<span class="ltx_bibblock">A hunt for the snark: Annotator diversity in data practices. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Harmanpreet Kaur, Eytan Adar, Eric Gilbert, and Cliff Lampe. 2022.

</span>
<span class="ltx_bibblock">Sensible AI: Re-imagining interpretability and explainability using sensemaking theory. In <em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. 702–714.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemitabaar et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Majeed Kazemitabaar, Xinying Hou, Austin Henley, Barbara Jane Ericson, David Weintrop, and Tovi Grossman. 2023.

</span>
<span class="ltx_bibblock">How novices use LLM-based code generators to solve CS1 coding tasks in a self-paced learning environment. In <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">Proceedings of the 23rd Koli Calling International Conference on Computing Education Research</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiesel et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Johannes Kiesel, Milad Alshomary, Nicolas Handke, Xiaoni Cai, Henning Wachsmuth, and Benno Stein. 2022.

</span>
<span class="ltx_bibblock">Identifying the human values behind arguments. In <em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 4459–4471.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Yoo, and Minjoon Seo. 2023.

</span>
<span class="ltx_bibblock">Aligning Large Language Models through Synthetic Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 13677–13700.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocielnik et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rafal Kocielnik, Saleema Amershi, and Paul N Bennett. 2019.

</span>
<span class="ltx_bibblock">Will you accept an imperfect ai? exploring designs for adjusting end-user expectations of ai systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koo et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023.

</span>
<span class="ltx_bibblock">Benchmarking cognitive biases in large language models as evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv:2309.17012</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krakovna et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. 2020.

</span>
<span class="ltx_bibblock">Specification gaming: the flip side of AI ingenuity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">DeepMind Blog</em> 3 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon and Mihindukulasooriya (2023)</span>
<span class="ltx_bibblock">
Bum Chul Kwon and Nandana Mihindukulasooriya. 2023.

</span>
<span class="ltx_bibblock">Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)</em>, Danushka Bollegala, Ruihong Huang, and Alan Ritter (Eds.). Association for Computational Linguistics, Toronto, Canada, 42–50.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2023.acl-demo.4" title="">https://doi.org/10.18653/v1/2023.acl-demo.4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lahoti et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, et al<span class="ltx_text" id="bib.bib97.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.4.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 10383–10405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Vivian Lai, Samuel Carton, Rajat Bhatnagar, Q Vera Liao, Yunfeng Zhang, and Chenhao Tan. 2022.

</span>
<span class="ltx_bibblock">Human-ai collaboration via conditional delegation: A case study of content moderation. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A Alghamdi, et al<span class="ltx_text" id="bib.bib99.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">A Design Space for Intelligent and Interactive Writing Assistants. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.4.1">Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al<span class="ltx_text" id="bib.bib100.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Evaluating Human-Language Model Interaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.4.1">Transactions on Machine Learning Research</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Noah Lee, Na Min An, and James Thorne. 2023a.

</span>
<span class="ltx_bibblock">Can Large Language Models Capture Dissenting Human Voices?. In <em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a.

</span>
<span class="ltx_bibblock">Camel: Communicative agents for” mind” exploration of large language model society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, and Diyi Yang. 2023b.

</span>
<span class="ltx_bibblock">CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 1487–1505.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, and Zhiping Zhang. 2024a.

</span>
<span class="ltx_bibblock">Human-Centered Privacy Research in the Age of Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhuoyan Li, Chen Liang, Jing Peng, and Ming Yin. 2024b.

</span>
<span class="ltx_bibblock">The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lima et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Gabriel Lima, Nina Grgić-Hlača, and Meeyoung Cha. 2021.

</span>
<span class="ltx_bibblock">Human perceptions on moral responsibility of AI: A case study in AI-assisted bail decision-making. In <em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">Proceedings of the 2021 CHI conference on human factors in computing systems</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lima et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Gabriel Lima, Nina Grgić-Hlača, and Meeyoung Cha. 2023a.

</span>
<span class="ltx_bibblock">Blaming humans and machines: What shapes people’s reactions to algorithmic harm. In <em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">Proceedings of the 2023 CHI conference on human factors in computing systems</em>. 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lima et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Gabriel Lima, Nina Grgic-Hlaca, Jin Keun Jeong, and Meeyoung Cha. 2023b.

</span>
<span class="ltx_bibblock">Who Should Pay When Machines Cause Harm? Laypeople’s Expectations of Legal Damages for Machine-Caused Harm. In <em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em> <em class="ltx_emph ltx_font_italic" id="bib.bib108.4.2">(FAccT ’23)</em>. Association for Computing Machinery, New York, NY, USA, 236–246.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3593013.3593992" title="">https://doi.org/10.1145/3593013.3593992</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lima et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Gabriel Lima, Changyeon Kim, Seungho Ryu, Chihyung Jeon, and Meeyoung Cha. 2020.

</span>
<span class="ltx_bibblock">Collecting the public perception of AI and robot rights.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 4, CSCW2 (2020), 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock">The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
June M Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. 2023.

</span>
<span class="ltx_bibblock">Chatcounselor: A large language models for mental health support. In <em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">The First Workshop on Personalized Generative AI @ CIKM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. 2022.

</span>
<span class="ltx_bibblock">Aligning generative language models with human values. In <em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">Findings of the Association for Computational Linguistics: NAACL 2022</em>. 241–252.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024b.

</span>
<span class="ltx_bibblock">What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xin Liu, Muhammad Khalifa, and Lu Wang. 2024a.

</span>
<span class="ltx_bibblock">Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths. In <em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long and Magerko (2020)</span>
<span class="ltx_bibblock">
Duri Long and Brian Magerko. 2020.

</span>
<span class="ltx_bibblock">What is AI literacy? Competencies and design considerations. In <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Proceedings of the 2020 CHI conference on human factors in computing systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hua Lu, Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2023.

</span>
<span class="ltx_bibblock">Towards Boosting the Open-Domain Chatbot with Human Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 4060–4078.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucaj et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Laura Lucaj, Patrick van der Smagt, and Djalel Benbouzid. 2023.

</span>
<span class="ltx_bibblock">Ai regulation is (not) all you need. In <em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em>. 1267–1279.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chenyang Lyu, Linyi Yang, Yue Zhang, Yvette Graham, and Jennifer Foster. 2023.

</span>
<span class="ltx_bibblock">Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">Findings of the Association for Computational Linguistics: ACL 2023</em>. 1419–1429.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Qianou Ma, Hua Shen, Kenneth Koedinger, and Tongshuang Wu. 2024.

</span>
<span class="ltx_bibblock">How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">25th International Conference on Artificial Intelligence in Education (AIED 2024)</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shuai Ma, Ying Lei, Xinru Wang, Chengbo Zheng, Chuhan Shi, Ming Yin, and Xiaojuan Ma. 2023a.

</span>
<span class="ltx_bibblock">Who should i trust: Ai or myself? leveraging human and ai correctness likelihood to promote appropriate trust in ai-assisted decision-making. In <em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ziqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai. 2023b.

</span>
<span class="ltx_bibblock">Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 1011–1031.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaio et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020.

</span>
<span class="ltx_bibblock">Co-designing checklists to understand organizational challenges and opportunities around fairness in AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Proceedings of the 2020 CHI conference on human factors in computing systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maghakian et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jessica Maghakian, Paul Mineiro, Kishan Panaganti, Mark Rucker, Akanksha Saran, and Cheng Tan. 2023.

</span>
<span class="ltx_bibblock">Personalized Reward Learning with Interaction-Grounded Learning (IGL). In <em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McDonald and Pan (2020)</span>
<span class="ltx_bibblock">
Nora McDonald and Shimei Pan. 2020.

</span>
<span class="ltx_bibblock">Intersectional AI: A study of how information science students think about ethics and their impact.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">Proceedings of the ACM on Human-Computer Interaction</em> 4, CSCW2 (2020), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McGrath et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sam Whitman McGrath, Jacob Russin, Ellie Pavlick, and Roman Feiman. 2023.

</span>
<span class="ltx_bibblock">How Can Deep Neural Networks Inform Theory in Psychological Science?

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mireshghallah et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock">Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. In <em class="ltx_emph ltx_font_italic" id="bib.bib126.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. 2024.

</span>
<span class="ltx_bibblock">Levels of AGI: Operationalizing Progress on the Path to AGI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.02462 [cs.AI]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mozannar et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, and David Sontag. 2024.

</span>
<span class="ltx_bibblock">The RealHumanEval: Evaluating Large Language Models’ Abilities to Support Programmers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">arXiv:2404.02806</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ngo et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Richard Ngo, Lawrence Chan, and Sören Mindermann. 2024.

</span>
<span class="ltx_bibblock">The alignment problem from a deep learning perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib129.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Allen Nie, Yuhui Zhang, Atharva Shailesh Amdekar, Chris Piech, Tatsunori B Hashimoto, and Tobias Gerstenberg. 2023.

</span>
<span class="ltx_bibblock">MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Advances in Neural Information Processing Systems</em> 36 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">of Publications (2024)</span>
<span class="ltx_bibblock">
The ACM Director of Publications. 2024.

</span>
<span class="ltx_bibblock">ACM Policy on Authorship.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.acm.org/publications/policies/new-acm-policy-on-authorship" title="">https://www.acm.org/publications/policies/new-acm-policy-on-authorship</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oh et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Minsik Oh, Joosung Lee, Jiwei Li, and Guoyin Wang. 2023.

</span>
<span class="ltx_bibblock">PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue. In <em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 16383–16395.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orlikowski et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Matthias Orlikowski, Paul Röttger, Philipp Cimiano, and Dirk Hovy. 2023.

</span>
<span class="ltx_bibblock">The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics. In <em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al<span class="ltx_text" id="bib.bib134.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib134.4.1">Advances in neural information processing systems</em> 35 (2022), 27730–27744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, and Jiawei Han. 2023.

</span>
<span class="ltx_bibblock">The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions. In <em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 2375–2393.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Page et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, Jennifer M Tetzlaff, Elie A Akl, Sue E Brennan, et al<span class="ltx_text" id="bib.bib136.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">The PRISMA 2020 statement: an updated guideline for reporting systematic reviews.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.4.1">Bmj</em> 372 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paleja et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rohan Paleja, Muyleng Ghuy, Nadun Ranawaka Arachchige, Reed Jensen, and Matthew Gombolay. 2021.

</span>
<span class="ltx_bibblock">The utility of explainable ai in ad hoc human-machine teaming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">Advances in neural information processing systems</em> 34, 610–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Alexander Pan, Kush Bhatia, and Jacob Steinhardt. 2022.

</span>
<span class="ltx_bibblock">The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023.

</span>
<span class="ltx_bibblock">Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark. In <em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">International Conference on Machine Learning</em>. PMLR, 26837–26867.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paranjape et al<span class="ltx_text" id="bib.bib140.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ketan Paranjape, Michiel Schinkel, Rishi Nannan Panday, Josip Car, Prabath Nanayakkara, et al<span class="ltx_text" id="bib.bib140.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Introducing artificial intelligence training in medical education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.4.1">JMIR medical education</em> 5, 2 (2019), e16048.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Hyanghee Park, Daehwan Ahn, Kartik Hosanagar, and Joonhwan Lee. 2021a.

</span>
<span class="ltx_bibblock">Human-AI interaction in human resource management: Understanding why employees resist algorithmic evaluation at workplaces and how to mitigate burdens. In <em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">Proceedings of the 2021 CHI conference on human factors in computing systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior. In <em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, and Ruhi Sarikaya. 2021b.

</span>
<span class="ltx_bibblock">A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>. 6054–6063.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel and Pavlick (2021)</span>
<span class="ltx_bibblock">
Roma Patel and Ellie Pavlick. 2021.

</span>
<span class="ltx_bibblock">“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>. 10080–10095.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pei et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Apostolos Dedeloudis, Jackson Sargent, and David Jurgens. 2022.

</span>
<span class="ltx_bibblock">POTATO: The Portable Text Annotation Tool. In <em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yi-Hao Peng, JiWoong Jang, Jeffrey P Bigham, and Amy Pavel. 2021.

</span>
<span class="ltx_bibblock">Say it all: Feedback for improving non-visual presentation accessibility. In <em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perry et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023.

</span>
<span class="ltx_bibblock">Do users write more insecure code with AI assistants?

</span>
<span class="ltx_bibblock">(2023), 2785–2799.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peschl et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Markus Peschl, Arkady Zgonnikov, Frans A Oliehoek, and Luciano C Siebert. 2022.

</span>
<span class="ltx_bibblock">MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems</em>. 1038–1046.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Savvas Petridis, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren Henderson, Stan Jastrzebski, Jeffrey V Nickerson, and Lydia B Chilton. 2023.

</span>
<span class="ltx_bibblock">Anglekindling: Supporting journalistic angle ideation with large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Savvas Petridis, Ben Wedin, Ann Yuan, James Wexler, and Nithum Thain. 2024b.

</span>
<span class="ltx_bibblock">ConstitutionalExperts: Training a Mixture of Principle-based Prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">arXiv:2403.04894</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Savvas Petridis, Benjamin D Wedin, James Wexler, Mahima Pushkarna, Aaron Donsbach, Nitesh Goyal, Carrie J Cai, and Michael Terry. 2024a.

</span>
<span class="ltx_bibblock">Constitutionmaker: Interactively critiquing large language models by converting feedback into principles. In <em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">Proceedings of the 29th International Conference on Intelligent User Interfaces</em>. 853–868.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinski et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Marc Pinski, Martin Adam, and Alexander Benlian. 2023.

</span>
<span class="ltx_bibblock">AI knowledge: Improving AI delegation through human enablement. In <em class="ltx_emph ltx_font_italic" id="bib.bib152.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prabhumoye et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shrimai Prabhumoye, Brendon Boldt, Ruslan Salakhutdinov, and Alan W Black. 2021.

</span>
<span class="ltx_bibblock">Case Study: Deontological Ethics in NLP. In <em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 3784–3798.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prunkl and Whittlestone (2020)</span>
<span class="ltx_bibblock">
Carina Prunkl and Jess Whittlestone. 2020.

</span>
<span class="ltx_bibblock">Beyond near-and long-term: Towards a clearer account of research priorities in AI ethics and society. In <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>. 138–143.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pruss (2023)</span>
<span class="ltx_bibblock">
Dasha Pruss. 2023.

</span>
<span class="ltx_bibblock">Ghosting the Machine: Judicial Resistance to a Recidivism Risk Assessment Instrument. In <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em> <em class="ltx_emph ltx_font_italic" id="bib.bib155.2.2">(FAccT ’23)</em>. Association for Computing Machinery, New York, NY, USA, 312–323.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3593013.3593999" title="">https://doi.org/10.1145/3593013.3593999</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. In <em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramezani and Xu (2023)</span>
<span class="ltx_bibblock">
Aida Ramezani and Yang Xu. 2023.

</span>
<span class="ltx_bibblock">Knowledge of cultural moral norms in large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 428–446.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rao et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, and Monojit Choudhury. 2023.

</span>
<span class="ltx_bibblock">Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 13370–13388.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roit et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, et al<span class="ltx_text" id="bib.bib160.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib160.4.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 6252–6272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Identifying the Risks of LM Agents with an LM-Emulated Sandbox. In <em class="ltx_emph ltx_font_italic" id="bib.bib161.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell (2014)</span>
<span class="ltx_bibblock">
Stuart Russell. 2014.

</span>
<span class="ltx_bibblock">White paper: Value alignment in autonomous systems.

</span>
<span class="ltx_bibblock">November 1 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell and Norvig (2016)</span>
<span class="ltx_bibblock">
Stuart J Russell and Peter Norvig. 2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">Artificial intelligence: a modern approach</em>.

</span>
<span class="ltx_bibblock">Pearson.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santurkar et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Whose opinions do language models reflect?. In <em class="ltx_emph ltx_font_italic" id="bib.bib164.3.1">International Conference on Machine Learning</em>. PMLR, 29971–30004.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santy et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. 2023.

</span>
<span class="ltx_bibblock">NLPositionality: Characterizing Design Biases of Datasets and Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib165.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 9080–9102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schemmer et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Max Schemmer, Niklas Kuehl, Carina Benz, Andrea Bartos, and Gerhard Satzger. 2023.

</span>
<span class="ltx_bibblock">Appropriate reliance on AI advice: Conceptualization and the effect of explanations. In <em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">Proceedings of the 28th International Conference on Intelligent User Interfaces</em>. 410–422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz (1994)</span>
<span class="ltx_bibblock">
Shalom H Schwartz. 1994.

</span>
<span class="ltx_bibblock">Are there universal aspects in the structure and contents of human values?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">Journal of social issues</em> 50, 4 (1994), 19–45.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz (2012)</span>
<span class="ltx_bibblock">
Shalom H Schwartz. 2012.

</span>
<span class="ltx_bibblock">An overview of the Schwartz theory of basic values.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">Online readings in Psychology and Culture</em> 2, 1 (2012), 11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sedikides and Strube (1995)</span>
<span class="ltx_bibblock">
Constantine Sedikides and Michael J Strube. 1995.

</span>
<span class="ltx_bibblock">The multiply motivated self.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">Personality and Social Psychology Bulletin</em> 21, 12 (1995), 1330–1335.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaikh et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, and Michael S Bernstein. 2024.

</span>
<span class="ltx_bibblock">Rehearsal: Simulating conflict to teach conflict resolution. In <em class="ltx_emph ltx_font_italic" id="bib.bib170.3.1">ACM Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ashish Sharma, Kevin Rushton, Inna Lin, David Wadden, Khendra Lucas, Adam Miner, Theresa Nguyen, and Tim Althoff. 2023.

</span>
<span class="ltx_bibblock">Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 9977–10000.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Hua Shen, Chieh-Yang Huang, Tongshuang Wu, and Ting-Hao Kenneth Huang. 2023a.

</span>
<span class="ltx_bibblock">ConvXAI: Delivering heterogeneous AI explanations via conversations to support human-AI scientific writing. In <em class="ltx_emph ltx_font_italic" id="bib.bib172.3.1">Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing</em>. 384–387.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen and Huang (2020)</span>
<span class="ltx_bibblock">
Hua Shen and Ting-Hao Huang. 2020.

</span>
<span class="ltx_bibblock">How useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels. In <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, Vol. 8. 168–172.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hua Shen, Yuguang Yang, Guoli Sun, Ryan Langman, Eunjung Han, Jasha Droppo, and Andreas Stolcke. 2022.

</span>
<span class="ltx_bibblock">Improving fairness in speaker verification via group-adapted fusion network. In <em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 7077–7081.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hua Shen, Vicky Zayats, Johann Rocholl, Daniel Walker, and Dirk Padfield. 2023b.

</span>
<span class="ltx_bibblock">MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. In <em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 9895–9903.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021.

</span>
<span class="ltx_bibblock">Societal Biases in Language Generation: Progress and Challenges. In <em class="ltx_emph ltx_font_italic" id="bib.bib176.3.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>. 4275–4293.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Minkyu Shin, Jin Kim, Bas van Opheusden, and Thomas L Griffiths. 2023.

</span>
<span class="ltx_bibblock">Superhuman artificial intelligence can improve human decision-making by increasing novelty.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">Proceedings of the National Academy of Sciences</em> 120, 12 (2023), e2214840120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He. 2023.

</span>
<span class="ltx_bibblock">Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations. In <em class="ltx_emph ltx_font_italic" id="bib.bib178.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sivaraman et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Venkatesh Sivaraman, Leigh A Bukowski, Joel Levin, Jeremy M Kahn, and Adam Perer. 2023.

</span>
<span class="ltx_bibblock">Ignore, trust, or negotiate: Understanding clinician acceptance of AI-based treatment recommendations in health care. In <em class="ltx_emph ltx_font_italic" id="bib.bib179.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sky et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
CH-Wang Sky, Arkadiy Saakyan, Oliver Li, Zhou Yu, and Smaranda Muresan. 2023.

</span>
<span class="ltx_bibblock">Sociocultural norm similarities and differences via situational alignment and explainable textual entailment. In <em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith-Renner et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber, Daniel S Weld, and Leah Findlater. 2020.

</span>
<span class="ltx_bibblock">No explainability without accountability: An empirical study of explanations and feedback in interactive ml. In <em class="ltx_emph ltx_font_italic" id="bib.bib181.3.1">Proceedings of the 2020 chi conference on human factors in computing systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solyst et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jaemarie Solyst, Shixian Xie, Ellia Yang, Angela EB Stewart, Motahhare Eslami, Jessica Hammer, and Amy Ogan. 2023.

</span>
<span class="ltx_bibblock">“I Would Like to Design”: Black Girls Analyzing and Ideating Fair and Accountable AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib182.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib183.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2024.

</span>
<span class="ltx_bibblock">Preference ranking optimization for human alignment. In <em class="ltx_emph ltx_font_italic" id="bib.bib183.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 18990–18998.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorensen et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, et al<span class="ltx_text" id="bib.bib184.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">A Roadmap to Pluralistic Alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib184.4.1">arXiv:2402.05070</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srikanth et al<span class="ltx_text" id="bib.bib185.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Neha Pundlik Srikanth, Rupak Sarkar, Heran Y. Mane, Elizabeth M. Aparicio, Quynh C. Nguyen, Rachel Rudinger, and Jordan Boyd-Graber. 2024.

</span>
<span class="ltx_bibblock">Large Language Models Help Humans Verify Truthfulness—Except When They Are Convincingly Wrong. In <em class="ltx_emph ltx_font_italic" id="bib.bib185.3.1">North American Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sumit Srivastava, Mariët Theune, and Alejandro Catala. 2023.

</span>
<span class="ltx_bibblock">The role of lexical alignment in human understanding of explanations by conversational agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib186.3.1">Proceedings of the 28th International Conference on Intelligent User Interfaces</em>. 423–435.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stefanidi et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Evropi Stefanidi, Marit Bentvelzen, Paweł W Woźniak, Thomas Kosch, Mikołaj P Woźniak, Thomas Mildner, Stefan Schneegass, Heiko Müller, and Jasmin Niess. 2023.

</span>
<span class="ltx_bibblock">Literature reviews in HCI: A review of reviews. In <em class="ltx_emph ltx_font_italic" id="bib.bib187.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stuurman and Wijnands (2001)</span>
<span class="ltx_bibblock">
Kees Stuurman and Hugo Wijnands. 2001.

</span>
<span class="ltx_bibblock">Software law: intelligent agents: a curse or a blessing? A survey of the legal aspects of the application of intelligent software systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">Computer Law &amp; Security Review</em> 17, 2 (2001), 92–100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib189.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Junliang He, Xipeng Qiu, and Xuan-Jing Huang. 2022.

</span>
<span class="ltx_bibblock">BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib189.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>. 3726–3739.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023.

</span>
<span class="ltx_bibblock">Principle-driven self-alignment of language models from scratch with minimal human supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">Advances in Neural Information Processing Systems</em> 36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundar and Lee (2022)</span>
<span class="ltx_bibblock">
S Shyam Sundar and Eun-Ju Lee. 2022.

</span>
<span class="ltx_bibblock">Rethinking communication in the era of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">Human Communication Research</em> 48, 3 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suresh et al<span class="ltx_text" id="bib.bib192.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Harini Suresh, Kathleen M Lewis, John Guttag, and Arvind Satyanarayan. 2022.

</span>
<span class="ltx_bibblock">Intuitively assessing ml model reliability through example-based explanations and editing model inputs. In <em class="ltx_emph ltx_font_italic" id="bib.bib192.3.1">27th International Conference on Intelligent User Interfaces</em>. 767–781.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swazinna et al<span class="ltx_text" id="bib.bib193.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Phillip Swazinna, Steffen Udluft, and Thomas Runkler. 2023.

</span>
<span class="ltx_bibblock">User-Interactive Offline Reinforcement Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib193.3.1">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tenney et al<span class="ltx_text" id="bib.bib194.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, et al<span class="ltx_text" id="bib.bib194.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib194.4.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. 107–118.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Terry et al<span class="ltx_text" id="bib.bib195.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, and Meredith Ringel Morris. 2023.

</span>
<span class="ltx_bibblock">AI Alignment in the Design of Interactive AI: Specification Alignment, Process Alignment, and Evaluation Support.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib195.3.1">arXiv:2311.00710</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tintarev and Masthoff (2007)</span>
<span class="ltx_bibblock">
Nava Tintarev and Judith Masthoff. 2007.

</span>
<span class="ltx_bibblock">A survey of explanations in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">2007 IEEE 23rd international conference on data engineering workshop</em>. IEEE, 801–810.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tucker et al<span class="ltx_text" id="bib.bib197.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Catherine Tucker, A Agrawal, J Gans, and A Goldfarb. 2018.

</span>
<span class="ltx_bibblock">Privacy, algorithms, and artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib197.3.1">The economics of artificial intelligence: An agenda</em> (2018), 423–437.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varanasi and Goyal (2023)</span>
<span class="ltx_bibblock">
Rama Adithya Varanasi and Nitesh Goyal. 2023.

</span>
<span class="ltx_bibblock">“It is currently hodgepodge”: Examining AI/ML Practitioners’ Challenges during Co-production of Responsible AI Values. In <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib198.2.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 251, 17 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580903" title="">https://doi.org/10.1145/3544548.3580903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasconcelos et al<span class="ltx_text" id="bib.bib199.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S Bernstein, and Ranjay Krishna. 2023.

</span>
<span class="ltx_bibblock">Explanations can reduce overreliance on ai systems during decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib199.3.1">Proceedings of the ACM on Human-Computer Interaction</em> CSCW1 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verma and Metcalf (2023)</span>
<span class="ltx_bibblock">
Mudit Verma and Katherine Metcalf. 2023.

</span>
<span class="ltx_bibblock">Hindsight PRIORs for Reward Learning from Human Preferences. In <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalba et al<span class="ltx_text" id="bib.bib201.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alejandro Cuevas Villalba, Eva M Brown, Jennifer V Scurrell, Jason Entenmann, and Madeleine IG Daepp. 2023.

</span>
<span class="ltx_bibblock">Automated Interviewer or Augmented Survey? Collecting Social Data with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib201.3.1">arXiv:2309.10187</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib202.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023d.

</span>
<span class="ltx_bibblock">Large language models are not fair evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib202.3.1">arXiv:2305.17926</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib203.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.

</span>
<span class="ltx_bibblock">Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant. In <em class="ltx_emph ltx_font_italic" id="bib.bib203.3.1">Proceedings of the 2021 CHI conference on human factors in computing systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib204.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Xingjin Wang, Linjing Li, and Daniel Zeng. 2023c.

</span>
<span class="ltx_bibblock">LDM2: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement. In <em class="ltx_emph ltx_font_italic" id="bib.bib204.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 4660–4681.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib205.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xinru Wang, Zhuoran Lu, and Ming Yin. 2022.

</span>
<span class="ltx_bibblock">Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making. In <em class="ltx_emph ltx_font_italic" id="bib.bib205.3.1">Proceedings of the ACM Web Conference 2022</em> (, Virtual Event, Lyon, France,) <em class="ltx_emph ltx_font_italic" id="bib.bib205.4.2">(WWW ’22)</em>. Association for Computing Machinery, New York, NY, USA, 1697–1708.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3485447.3512240" title="">https://doi.org/10.1145/3485447.3512240</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yin (2023)</span>
<span class="ltx_bibblock">
Xinru Wang and Ming Yin. 2023.

</span>
<span class="ltx_bibblock">Watch out for updates: Understanding the effects of model explanation updates in ai-assisted decision making. In <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib207.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu. 2023a.

</span>
<span class="ltx_bibblock">Humanoid Agents: Platform for Simulating Human-like Generative Agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib207.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. 167–176.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib208.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, et al<span class="ltx_text" id="bib.bib208.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Democratizing Reasoning Ability: Tailored Learning from Large Language Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib208.4.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 1948–1966.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib209.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zijie J Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, and Michael Madaio. 2024.

</span>
<span class="ltx_bibblock">Farsight: Fostering Responsible AI Awareness During AI Application Prototyping. In <em class="ltx_emph ltx_font_italic" id="bib.bib209.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–40.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et al<span class="ltx_text" id="bib.bib210.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, et al<span class="ltx_text" id="bib.bib210.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Sociotechnical safety evaluation of generative ai systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib210.4.1">arXiv:2310.11986</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch et al<span class="ltx_text" id="bib.bib211.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Charles Welch, Chenxi Gu, Jonathan K Kummerfeld, Verónica Pérez-Rosas, and Rada Mihalcea. 2022.

</span>
<span class="ltx_bibblock">Leveraging similar users for personalized language modeling with limited data. In <em class="ltx_emph ltx_font_italic" id="bib.bib211.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 1742–1752.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiener (1960)</span>
<span class="ltx_bibblock">
Norbert Wiener. 1960.

</span>
<span class="ltx_bibblock">Some Moral and Technical Consequences of Automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">Science</em> 131, 3410 (1960), 1355–1358.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikipedia (2024)</span>
<span class="ltx_bibblock">
Wikipedia. 2024.

</span>
<span class="ltx_bibblock">AI alignment — Wikipedia, The Free Encyclopedia.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://en.wikipedia.org/w/index.php?title=AI%20alignment&amp;oldid=1220304776" title="">http://en.wikipedia.org/w/index.php?title=AI%20alignment&amp;oldid=1220304776</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Online; accessed 05-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wischnewski et al<span class="ltx_text" id="bib.bib214.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Magdalena Wischnewski, Nicole Krämer, and Emmanuel Müller. 2023.

</span>
<span class="ltx_bibblock">Measuring and understanding trust calibrations for automated systems: a survey of the state-of-the-art and future directions. In <em class="ltx_emph ltx_font_italic" id="bib.bib214.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wooldridge (1999)</span>
<span class="ltx_bibblock">
Michael Wooldridge. 1999.

</span>
<span class="ltx_bibblock">Intelligent agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">Multiagent systems: A modern approach to distributed artificial intelligence</em> 1 (1999), 27–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib216.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a.

</span>
<span class="ltx_bibblock">Autogen: Enabling next-gen llm applications via multi-agent conversation framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib216.3.1">arXiv:2308.08155</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib217.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Sherry Wu, Hua Shen, Daniel S Weld, Jeffrey Heer, and Marco Tulio Ribeiro. 2023c.

</span>
<span class="ltx_bibblock">Scattershot: Interactive in-context example curation for text transformation. In <em class="ltx_emph ltx_font_italic" id="bib.bib217.3.1">Proceedings of the 28th International Conference on Intelligent User Interfaces</em>. 353–367.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib218.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022.

</span>
<span class="ltx_bibblock">Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In <em class="ltx_emph ltx_font_italic" id="bib.bib218.3.1">Proceedings of the 2022 CHI conference on human factors in computing systems</em>. 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib219.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Winston Wu, Lu Wang, and Rada Mihalcea. 2023d.

</span>
<span class="ltx_bibblock">Cross-Cultural Analysis of Human Values, Morals, and Biases in Folk Tales. In <em class="ltx_emph ltx_font_italic" id="bib.bib219.3.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib220.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. 2023b.

</span>
<span class="ltx_bibblock">Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib220.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 10691–10706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib221.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li. 2024.

</span>
<span class="ltx_bibblock">Can Large Language Model Agents Simulate Human Trust Behaviors?. In <em class="ltx_emph ltx_font_italic" id="bib.bib221.3.1">ICLR 2024 Workshop: How Far Are We From AGI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib222.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. 2023.

</span>
<span class="ltx_bibblock">Gentopia. AI: A Collaborative Platform for Tool-Augmented LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib222.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. 237–245.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Zhang (2023)</span>
<span class="ltx_bibblock">
Songlin Xu and Xinyu Zhang. 2023.

</span>
<span class="ltx_bibblock">Augmenting human cognition with an ai-mediated intelligent visual feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib224.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al<span class="ltx_text" id="bib.bib224.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Lemur: Harmonizing natural language and code for language agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib224.4.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib225.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang, and Wei Zhang. 2023.

</span>
<span class="ltx_bibblock">From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib225.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 12413–12425.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib226.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Chunxu Yang, Chien-Sheng Wu, Lidiya Murakhovs’ka, Philippe Laban, and Xiang Chen. 2023b.

</span>
<span class="ltx_bibblock">INTELMO: Enhancing Models’ Adoption of Interactive Interfaces. In <em class="ltx_emph ltx_font_italic" id="bib.bib226.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>. 161–166.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib227.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Qian Yang, Yuexing Hao, Kexin Quan, Stephen Yang, Yiran Zhao, Volodymyr Kuleshov, and Fei Wang. 2023a.

</span>
<span class="ltx_bibblock">Harnessing biomedical literature to calibrate clinicians’ trust in AI decision support systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib227.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib228.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie. 2023b.

</span>
<span class="ltx_bibblock">From Instructions to Intrinsic Human Values–A Survey of Alignment Goals for Big Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib228.3.1">arXiv:2308.12014</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib229.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zonghai Yao, Benjamin Schloss, and Sai Selvaraj. 2023a.

</span>
<span class="ltx_bibblock">Improving Summarization with Human Edits. In <em class="ltx_emph ltx_font_italic" id="bib.bib229.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 2604–2620.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib230.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2024.

</span>
<span class="ltx_bibblock">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. In <em class="ltx_emph ltx_font_italic" id="bib.bib230.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yildirim et al<span class="ltx_text" id="bib.bib231.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nur Yildirim, Alex Kass, Teresa Tung, Connor Upton, Donnacha Costello, Robert Giusti, Sinem Lacin, Sara Lovic, James M O’Neill, Rudi O’Reilly Meehan, et al<span class="ltx_text" id="bib.bib231.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">How experienced designers of enterprise applications engage AI as a design material. In <em class="ltx_emph ltx_font_italic" id="bib.bib231.4.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib232.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019.

</span>
<span class="ltx_bibblock">Understanding the effect of accuracy on trust in machine learning models. In <em class="ltx_emph ltx_font_italic" id="bib.bib232.3.1">Proceedings of the 2019 chi conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib233.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023.

</span>
<span class="ltx_bibblock">RRHF: Rank responses to align language models with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib233.3.1">Advances in Neural Information Processing Systems</em> 36 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib234.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, and Yan Zheng. 2024.

</span>
<span class="ltx_bibblock">Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib234.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zagalsky et al<span class="ltx_text" id="bib.bib235.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexey Zagalsky, Dov Te’eni, Inbal Yahav, David G Schwartz, Gahl Silverman, Daniel Cohen, Yossi Mann, and Dafna Lewinsky. 2021.

</span>
<span class="ltx_bibblock">The design of reciprocal learning between human and artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib235.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW2 (2021), 1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib236.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Angie Zhang, Olympia Walker, Kaci Nguyen, Jiajun Dai, Anqing Chen, and Min Kyung Lee. 2023b.

</span>
<span class="ltx_bibblock">Deliberating with AI: Improving Decision-Making for the Future through Participatory AI Design and Stakeholder Deliberation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib236.3.1">CSCW1</em> 7, CSCW1 (2023), 1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib237.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yangjun Zhang, Pengjie Ren, and Maarten de Rijke. 2021.

</span>
<span class="ltx_bibblock">A human-machine collaborative framework for evaluating malevolence in dialogues. In <em class="ltx_emph ltx_font_italic" id="bib.bib237.3.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>. 5612–5623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib238.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, and Toby Jia-Jun Li. 2023a.

</span>
<span class="ltx_bibblock">Visar: A human-ai argumentative writing assistant with visual programming and rapid draft prototyping. In <em class="ltx_emph ltx_font_italic" id="bib.bib238.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib239.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Siyan Zhao, John Dang, and Aditya Grover. 2023.

</span>
<span class="ltx_bibblock">Group Preference Optimization: Few-Shot Alignment of Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib239.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib240.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chengbo Zheng, Yuheng Wu, Chuhan Shi, Shuai Ma, Jiehui Luo, and Xiaojuan Ma. 2023.

</span>
<span class="ltx_bibblock">Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making. In <em class="ltx_emph ltx_font_italic" id="bib.bib240.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Lee (2024)</span>
<span class="ltx_bibblock">
Eric Zhou and Dokyun Lee. 2024.

</span>
<span class="ltx_bibblock">Generative artificial intelligence, human creativity, and art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">PNAS nexus</em> 3, 3 (2024), pgae052.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib242.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, and Xuan-Jing Huang. 2023a.

</span>
<span class="ltx_bibblock">RealBehavior: A Framework for Faithfully Characterizing Foundation Models’ Human-like Behavior Mechanisms. In <em class="ltx_emph ltx_font_italic" id="bib.bib242.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 10262–10274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib243.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al<span class="ltx_text" id="bib.bib243.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib243.4.1">In the International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib244.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhaowei Zhu, Jialu Wang, Hao Cheng, and Yang Liu. 2023.

</span>
<span class="ltx_bibblock">Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib244.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">APPENDIX</span>
</p>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Systematic Literature Review</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Systematic Literature Review Process</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">To understand the research literature relevant to the ongoing, mutual process of human-AI alignment,
we performed a systematic literature review based on the PRISMA guideline <cite class="ltx_cite ltx_citemacro_citep">(Page et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib136" title="">2021</a>; Stefanidi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib187" title="">2023</a>)</cite>.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.F7" title="Figure 7 ‣ A.1.4. Framework Development and Rigorous Coding. ‣ A.1. Systematic Literature Review Process ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">7</span></a> shows
the workflow of our process for
paper coding and developing the <em class="ltx_emph ltx_font_italic" id="A1.SS1.p1.1.1">bidirectional human-AI alignment</em> framework. We introduce the step details below.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1. </span>Identification and Screening with Keywords.</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p" id="A1.SS1.SSS1.p1.1">We started with papers published in the AI-related domain venues (including NLP, HCI, and ML fields) beginning from the advent of general-purpose generative AI to present, <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS1.p1.1.1">i.e.,</em> primarily between January, 2019 and January, 2024 (see details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS2" title="A.2. Venues ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A.2</span></a>).
We retrieved 34,213 papers in the initial <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS1.p1.1.2">Identification</em> stage.
Further, we collectively defined a list of keywords (see details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS3" title="A.3. Keywords ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A.3</span></a>) and screened for papers that included at least one of these keywords (<em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS1.p1.1.3">e.g.,</em> human, alignment) or their variations either in the title or abstract. We included 2,136 papers in this <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS1.p1.1.4">Screening</em> stage.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2. </span>Assessing Eligibility with Criteria</h4>
<div class="ltx_para" id="A1.SS1.SSS2.p1">
<p class="ltx_p" id="A1.SS1.SSS2.p1.1">We further filtered the 2,136 papers based on explicit inclusion and exclusion criteria, i.e., the <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.1">Eligibility</em> stage.
Our criteria revolved around six research questions that we collectively identified to be most pertinent to the topic, including <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.2">1) what essential human values have been aligned by some AI models?</em> <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.3">2) how did we effectively quantify or model human values to guide AI development?</em> <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.4">3) what strategies have been employed to integrate human values into the AI development process?</em> <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.5">4) how did existing studies improve human understanding and
evaluation of AI alignment?</em> <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.6">5) what are the practices for designing interfaces and interactions that
facilitate human-AI collaboration?</em>
<em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS2.p1.1.7">6) How have AI been adapted to meet the needs of various human value groups?
</em>
We included papers that could potentially answer any of these questions.
Further, based on the scope in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S3.SS4" title="3.4. Scopes and Key Components in Alignment ‣ 3. Fundamental Definitions and Clarifications ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we excluded papers that did not meet our inclusion criteria. This resulted in a final corpus of 411 papers, which were analyzed in detail using qualitative coding (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS4" title="A.4. Inclusion and Exclusion Criteria ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A.4</span></a> for more details).</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.3. </span>Qualitative Code Development.</h4>
<div class="ltx_para" id="A1.SS1.SSS3.p1">
<p class="ltx_p" id="A1.SS1.SSS3.p1.1">Referring to the code development process in <cite class="ltx_cite ltx_citemacro_citet">Lee et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib99" title="">2024</a>)</cite>, we first conducted qualitative coding for each paper by identifying relevant sentences that could answer the above research questions, and entering short codes to describe them into a codebook. We iteratively coded relevant sentences from each paper through a mix of inductive and deductive approaches, which allowed flexibility to expand, modify or change the driving research questions based on our learnings as we went through the process.
To ensure rigor in our coding process, two authors coded each paper. The first author independently annotated all papers after reviewing the paper abstracts and introductions. Twelve team members each annotated a subset of the paper corpus.
Our corpus includes papers from different domains (<em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS3.p1.1.1">e.g.,</em> HCI, NLP and ML). Therefore, we divided the authors into HCI and NLP/ML<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that NLP and ML are two different domains, we combine them together for the purposes of literature review analysis since they both work on developing and evaluating AI technologies.</span></span></span> teams and assigned the papers accordingly based on expertise.
All team members coded each of their assigned papers to answer all six questions (if applicable) introduced above.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.4. </span>Framework Development and Rigorous Coding.</h4>
<div class="ltx_para" id="A1.SS1.SSS4.p1">
<p class="ltx_p" id="A1.SS1.SSS4.p1.1">After developing annotations, all authors collaborated to create the bidirectional human-AI alignment framework by integrating the annotations within each of the codes. The initial version of the framework was proposed by the author who reviewed all papers. This framework furthermore underwent iterative improvement through: <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS4.p1.1.1">1)</em> discussions with all team members involved in paper coding, and <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS4.p1.1.2">2)</em> revisions based on feedback from the project advisors.
Additionally, we strengthened the framework by reviewing papers from the AI Ethics conferences (including FAccT and AIES), and related work of the collected papers that covered other domains such as psychology and social science.
We further added missing codes and papers to ensure comprehensive coverage (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#A1.SS2" title="A.2. Venues ‣ Appendix A Systematic Literature Review ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">A.2</span></a> for details).
The final bidirectional human-AI alignment framework, with detailed topologies, is presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4" title="4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4</span></a>.
Following the framework’s finalization, we conducted another separate coding process to annotate <em class="ltx_emph ltx_font_italic" id="A1.SS1.SSS4.p1.1.3">whether each paper investigated dimensions within our framework</em>. Two authors independently coded each paper.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The joint probability of agreement for the paper annotations was 0.78.</span></span></span>
These codes were then used to perform quantitative and qualitative analyses, as presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S5" title="5. Findings and Discussions on Current Gaps ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="193" id="A1.F7.g1" src="x21.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>
The selection and refinement process of our systematic literature review. We referred to the PRISMA guideline <cite class="ltx_cite ltx_citemacro_citep">(Page et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib136" title="">2021</a>; Stefanidi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib187" title="">2023</a>)</cite> to report the workflow.
From the identification of 34,213 records by keyword search, to screen eligible papers against our criteria and arriveg at our final corpus of 411 papers. For each of the stages where literature reviews were excluded (identification, screening, and eligibility) we further present the total of excluded records.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="A1.F7.1">\Description</span></div>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Venues</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We primarily focused on papers from the fields of HCI, NLP, and ML ranging from year 2019 to 2024 January.
We included all their papers tracks (<em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.1">e.g.,</em> CSCW Companion and Findings) without including workshops of conferences.
From the ACL Anthology, OpenReview and ACM Digital Library, we retrieved 34,190 papers into a Reference Manager Tool (<em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.2">i.e.,</em> Paperpile).
Particularly, the venues we surveyed are listed below.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">HCI</span>: CHI, CSCW, UIST, IUI;</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">NLP</span>: ACL, EMNLP, NAACL, Findings</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">ML</span>: ICLR, NeurIPS</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">Others</span>: ArXiv, FAccT, AIES, and other related work</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">Additionally, we also consolidate the framework by reviewing the papers published in FAccT and AIES (<em class="ltx_emph ltx_font_italic" id="A1.SS2.p3.1.1">i.e.,</em> important venues for AI Ethics research) between 2019 and 2024 and supplemented the codes, including the <span class="ltx_text ltx_framed ltx_framed_underline" id="A1.SS2.p3.1.2" style="border-color: #DE4A4D;">AI Regulatory and Policy</span> code in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#S4.SS4.SSS2" title="4.4.2. AI Impact on Humans and Society: how are humans influenced by AI systems ? ‣ 4.4. Align Humans to AI: Human’s Behavioral Adaptation to AI ‣ 4. Bidirectional Human-AI Alignment Framework ‣ Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"><span class="ltx_text ltx_ref_tag">4.4.2</span></a> and the exemplary paper of Regulating ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Hacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib64" title="">2023</a>)</cite>), which were not covered by the original collections.
Also, we include a number of papers in the “Other” class are found by related work that are highly relevant to this topic.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Keywords</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We decided on a list of keywords relevant to bidirectional human-AI alignment. The detailed keywords include:</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i1.p1.1.1">Human</span>: Human, User, Agent, Cognition, Crowd</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i2.p1.1.1">AI</span>: AI, Agent, Machine Learning, Neural Network, Algorithm, Model, Deep Learning, NLP</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i3.p1.1.1">LLM</span>: Large Language Model, LLM, GPT, Generative, In-context Learning</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i4.p1.1.1">Alignment</span>: Align, Alignment</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i5.p1.1.1">Value</span>: Value, Principle</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i6.p1">
<p class="ltx_p" id="A1.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i6.p1.1.1">Trust</span>: Trust, Trustworthy</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i7.p1">
<p class="ltx_p" id="A1.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i7.p1.1.1">Interact</span>: Interact, Interaction, Interactive, Collaboration, Conversational</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i8.p1">
<p class="ltx_p" id="A1.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i8.p1.1.1">Visualize</span>: Visualization, Visualize</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i9.p1">
<p class="ltx_p" id="A1.I2.i9.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i9.p1.1.1">Explain</span>: Interpretability, Explain, Understand, Transparent</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i10.p1">
<p class="ltx_p" id="A1.I2.i10.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i10.p1.1.1">Evaluation</span>: Evaluate, Evaluation, Audit</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i11.p1">
<p class="ltx_p" id="A1.I2.i11.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i11.p1.1.1">Feedback</span>: Feedback</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i12.p1">
<p class="ltx_p" id="A1.I2.i12.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i12.p1.1.1">Ethics</span>: Bias, Fairness</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4. </span>Inclusion and Exclusion Criteria</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">To further filter the most relevant papers among the keyword-filtered 2136 papers, we identified the six most important research questions we are interested in. We primarily selected the potential papers that can potentially address these six questions after reviewing their title and abstracts. The six topics of research questions in our filtering include:</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<ol class="ltx_enumerate" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.1</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i1.p1.1.1">[human value category]</span> What essential human values have been aligned by some AI models?</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.2</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i2.p1.1.1">[quantify human value]</span> How did we effectively quantify or model human values to guide AI development?</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.3</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p" id="A1.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i3.p1.1.1">[integrate human value into AI]</span> What strategies have been employed to integrate human values into the AI development process?</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.4</span>
<div class="ltx_para" id="A1.I3.i4.p1">
<p class="ltx_p" id="A1.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i4.p1.1.1">[assess / explain AI regarding human values]</span> How did existing studies improve human understanding and evaluation of AI alignment?</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.5</span>
<div class="ltx_para" id="A1.I3.i5.p1">
<p class="ltx_p" id="A1.I3.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i5.p1.1.1">[human-AI interaction techniques]</span> What are the practices for designing interfaces and interactions that facilitate human-AI collaboration?</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">RQ.6</span>
<div class="ltx_para" id="A1.I3.i6.p1">
<p class="ltx_p" id="A1.I3.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i6.p1.1.1">[adapt AI for diverse human values]</span> How has AI been adapted to meet the needs of various human value groups?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1">Particularly, we provide elaborated inclusion and exclusion criteria during our paper selection as listed below. We are aware that we have limitations during our paper filtering process.</p>
</div>
<div class="ltx_para" id="A1.SS4.p4">
<p class="ltx_p" id="A1.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p4.1.1">Inclusion Criteria:</span></p>
</div>
<div class="ltx_para" id="A1.SS4.p5">
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p" id="A1.I4.i1.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I4.i1.p1.1.m1.1"><semantics id="A1.I4.i1.p1.1.m1.1a"><mo id="A1.I4.i1.p1.1.m1.1.1" stretchy="false" xref="A1.I4.i1.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i1.p1.1.m1.1b"><ci id="A1.I4.i1.p1.1.m1.1.1.cmml" xref="A1.I4.i1.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i1.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i1.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I4.i1.p1.2.1">Human values<math alttext="]" class="ltx_Math" display="inline" id="A1.I4.i1.p1.2.1.m1.1"><semantics id="A1.I4.i1.p1.2.1.m1.1a"><mo id="A1.I4.i1.p1.2.1.m1.1.1" stretchy="false" xref="A1.I4.i1.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i1.p1.2.1.m1.1b"><ci id="A1.I4.i1.p1.2.1.m1.1.1.cmml" xref="A1.I4.i1.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i1.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i1.p1.2.1.m1.1d">]</annotation></semantics></math></span> we include papers that study human value definition, specification and evaluation in AI systems.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i2.p1">
<p class="ltx_p" id="A1.I4.i2.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I4.i2.p1.1.m1.1"><semantics id="A1.I4.i2.p1.1.m1.1a"><mo id="A1.I4.i2.p1.1.m1.1.1" stretchy="false" xref="A1.I4.i2.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i2.p1.1.m1.1b"><ci id="A1.I4.i2.p1.1.m1.1.1.cmml" xref="A1.I4.i2.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i2.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i2.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I4.i2.p1.2.1">AI development techniques<math alttext="]" class="ltx_Math" display="inline" id="A1.I4.i2.p1.2.1.m1.1"><semantics id="A1.I4.i2.p1.2.1.m1.1a"><mo id="A1.I4.i2.p1.2.1.m1.1.1" stretchy="false" xref="A1.I4.i2.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i2.p1.2.1.m1.1b"><ci id="A1.I4.i2.p1.2.1.m1.1.1.cmml" xref="A1.I4.i2.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i2.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i2.p1.2.1.m1.1d">]</annotation></semantics></math></span> We include techniques of developing AI that aim to be more consistent with human values with interactions along all AI development stages (e.g., data collection, model construction, etc.)</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i3.p1">
<p class="ltx_p" id="A1.I4.i3.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I4.i3.p1.1.m1.1"><semantics id="A1.I4.i3.p1.1.m1.1a"><mo id="A1.I4.i3.p1.1.m1.1.1" stretchy="false" xref="A1.I4.i3.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i3.p1.1.m1.1b"><ci id="A1.I4.i3.p1.1.m1.1.1.cmml" xref="A1.I4.i3.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i3.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i3.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I4.i3.p1.2.1">AI evaluation, explanation and utilization<math alttext="]" class="ltx_Math" display="inline" id="A1.I4.i3.p1.2.1.m1.1"><semantics id="A1.I4.i3.p1.2.1.m1.1a"><mo id="A1.I4.i3.p1.2.1.m1.1.1" stretchy="false" xref="A1.I4.i3.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i3.p1.2.1.m1.1b"><ci id="A1.I4.i3.p1.2.1.m1.1.1.cmml" xref="A1.I4.i3.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i3.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i3.p1.2.1.m1.1d">]</annotation></semantics></math></span> we include papers that build human-AI interactive systems or conduct human studies to better evaluate, explain, and utilize AI systems.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i4.p1">
<p class="ltx_p" id="A1.I4.i4.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I4.i4.p1.1.m1.1"><semantics id="A1.I4.i4.p1.1.m1.1a"><mo id="A1.I4.i4.p1.1.m1.1.1" stretchy="false" xref="A1.I4.i4.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.1.m1.1b"><ci id="A1.I4.i4.p1.1.m1.1.1.cmml" xref="A1.I4.i4.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I4.i4.p1.2.1">building dataset with human interaction<math alttext="]" class="ltx_Math" display="inline" id="A1.I4.i4.p1.2.1.m1.1"><semantics id="A1.I4.i4.p1.2.1.m1.1a"><mo id="A1.I4.i4.p1.2.1.m1.1.1" stretchy="false" xref="A1.I4.i4.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.2.1.m1.1b"><ci id="A1.I4.i4.p1.2.1.m1.1.1.cmml" xref="A1.I4.i4.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.2.1.m1.1d">]</annotation></semantics></math></span> especially responsible dataset.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS4.p6">
<p class="ltx_p" id="A1.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.p6.1.1">Exclusion Criteria:</span></p>
</div>
<div class="ltx_para" id="A1.SS4.p7">
<ul class="ltx_itemize" id="A1.I5">
<li class="ltx_item" id="A1.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i1.p1">
<p class="ltx_p" id="A1.I5.i1.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i1.p1.1.m1.1"><semantics id="A1.I5.i1.p1.1.m1.1a"><mo id="A1.I5.i1.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i1.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i1.p1.1.m1.1b"><ci id="A1.I5.i1.p1.1.m1.1.1.cmml" xref="A1.I5.i1.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i1.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i1.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i1.p1.2.1">Alignment not between human &amp; AI<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i1.p1.2.1.m1.1"><semantics id="A1.I5.i1.p1.2.1.m1.1a"><mo id="A1.I5.i1.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i1.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i1.p1.2.1.m1.1b"><ci id="A1.I5.i1.p1.2.1.m1.1.1.cmml" xref="A1.I5.i1.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i1.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i1.p1.2.1.m1.1d">]</annotation></semantics></math></span> we do not include alignment studies that are not between human and AI, such as entity alignment, cross-lingual alignment, cross-domain alignment, multi-modal alignment, token-environment alignment, etc.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i2.p1">
<p class="ltx_p" id="A1.I5.i2.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i2.p1.1.m1.1"><semantics id="A1.I5.i2.p1.1.m1.1a"><mo id="A1.I5.i2.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i2.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i2.p1.1.m1.1b"><ci id="A1.I5.i2.p1.1.m1.1.1.cmml" xref="A1.I5.i2.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i2.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i2.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i2.p1.2.1">AI models beyond LLMs - Modality<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i2.p1.2.1.m1.1"><semantics id="A1.I5.i2.p1.2.1.m1.1a"><mo id="A1.I5.i2.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i2.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i2.p1.2.1.m1.1b"><ci id="A1.I5.i2.p1.2.1.m1.1.1.cmml" xref="A1.I5.i2.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i2.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i2.p1.2.1.m1.1d">]</annotation></semantics></math></span> we do not focus on AI models other than LLMs (e.g., 3D models, VR/AR, voice assistant, spoken assistant), our primary model modality is text. Specifically, we do not consider audio / video data; we do not consider pure computer vision modality.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i3.p1">
<p class="ltx_p" id="A1.I5.i3.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i3.p1.1.m1.1"><semantics id="A1.I5.i3.p1.1.m1.1a"><mo id="A1.I5.i3.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i3.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i3.p1.1.m1.1b"><ci id="A1.I5.i3.p1.1.m1.1.1.cmml" xref="A1.I5.i3.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i3.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i3.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i3.p1.2.1">No human-AI interaction<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i3.p1.2.1.m1.1"><semantics id="A1.I5.i3.p1.2.1.m1.1a"><mo id="A1.I5.i3.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i3.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i3.p1.2.1.m1.1b"><ci id="A1.I5.i3.p1.2.1.m1.1.1.cmml" xref="A1.I5.i3.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i3.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i3.p1.2.1.m1.1d">]</annotation></semantics></math></span> we do not consider studies that do not involve the interaction between human and AI, such as (multi-agent) reinforcement learning. Specifically, we do not consider interactions via voices/speech, Do not consider game interaction; Do not consider interaction for Accessibility; Do not consider Mobile interaction; Not consider autonomous vehicle interaction wearable devices, or Physical interaction;</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i4.p1">
<p class="ltx_p" id="A1.I5.i4.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i4.p1.1.m1.1"><semantics id="A1.I5.i4.p1.1.m1.1a"><mo id="A1.I5.i4.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i4.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i4.p1.1.m1.1b"><ci id="A1.I5.i4.p1.1.m1.1.1.cmml" xref="A1.I5.i4.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i4.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i4.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i4.p1.2.1">Tasks<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i4.p1.2.1.m1.1"><semantics id="A1.I5.i4.p1.2.1.m1.1a"><mo id="A1.I5.i4.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i4.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i4.p1.2.1.m1.1b"><ci id="A1.I5.i4.p1.2.1.m1.1.1.cmml" xref="A1.I5.i4.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i4.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i4.p1.2.1.m1.1d">]</annotation></semantics></math></span> art and design, emotion.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i5.p1">
<p class="ltx_p" id="A1.I5.i5.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i5.p1.1.m1.1"><semantics id="A1.I5.i5.p1.1.m1.1a"><mo id="A1.I5.i5.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i5.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i5.p1.1.m1.1b"><ci id="A1.I5.i5.p1.1.m1.1.1.cmml" xref="A1.I5.i5.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i5.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i5.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i5.p1.2.1">No human included<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i5.p1.2.1.m1.1"><semantics id="A1.I5.i5.p1.2.1.m1.1a"><mo id="A1.I5.i5.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i5.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i5.p1.2.1.m1.1b"><ci id="A1.I5.i5.p1.2.1.m1.1.1.cmml" xref="A1.I5.i5.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i5.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i5.p1.2.1.m1.1d">]</annotation></semantics></math></span></p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i6.p1">
<p class="ltx_p" id="A1.I5.i6.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i6.p1.1.m1.1"><semantics id="A1.I5.i6.p1.1.m1.1a"><mo id="A1.I5.i6.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i6.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i6.p1.1.m1.1b"><ci id="A1.I5.i6.p1.1.m1.1.1.cmml" xref="A1.I5.i6.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i6.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i6.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i6.p1.2.1">focus on English<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i6.p1.2.1.m1.1"><semantics id="A1.I5.i6.p1.2.1.m1.1a"><mo id="A1.I5.i6.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i6.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i6.p1.2.1.m1.1b"><ci id="A1.I5.i6.p1.2.1.m1.1.1.cmml" xref="A1.I5.i6.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i6.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i6.p1.2.1.m1.1d">]</annotation></semantics></math></span> primarily focus on English as the main language;</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i7.p1">
<p class="ltx_p" id="A1.I5.i7.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i7.p1.1.m1.1"><semantics id="A1.I5.i7.p1.1.m1.1a"><mo id="A1.I5.i7.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i7.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i7.p1.1.m1.1b"><ci id="A1.I5.i7.p1.1.m1.1.1.cmml" xref="A1.I5.i7.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i7.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i7.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i7.p1.2.1">Application<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i7.p1.2.1.m1.1"><semantics id="A1.I5.i7.p1.2.1.m1.1a"><mo id="A1.I5.i7.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i7.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i7.p1.2.1.m1.1b"><ci id="A1.I5.i7.p1.2.1.m1.1.1.cmml" xref="A1.I5.i7.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i7.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i7.p1.2.1.m1.1d">]</annotation></semantics></math></span> not include the NLP papers tailored for a specific traditional task, such as translation, entity recognition, sentiment analysis, knowledge graph, adversarial and defense, topic modeling, detecting AI generations, distillation, low resource, physical robots, text classification, games, image-based tasks, hate speech detection, Human Trafficking, etc.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i8.p1">
<p class="ltx_p" id="A1.I5.i8.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i8.p1.1.m1.1"><semantics id="A1.I5.i8.p1.1.m1.1a"><mo id="A1.I5.i8.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i8.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i8.p1.1.m1.1b"><ci id="A1.I5.i8.p1.1.m1.1.1.cmml" xref="A1.I5.i8.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i8.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i8.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i8.p1.2.1">Visualizing Embeddings<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i8.p1.2.1.m1.1"><semantics id="A1.I5.i8.p1.2.1.m1.1a"><mo id="A1.I5.i8.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i8.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i8.p1.2.1.m1.1b"><ci id="A1.I5.i8.p1.2.1.m1.1.1.cmml" xref="A1.I5.i8.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i8.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i8.p1.2.1.m1.1d">]</annotation></semantics></math></span> Visualizing/interacting transformer embeddings?</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i9.p1">
<p class="ltx_p" id="A1.I5.i9.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i9.p1.1.m1.1"><semantics id="A1.I5.i9.p1.1.m1.1a"><mo id="A1.I5.i9.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i9.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i9.p1.1.m1.1b"><ci id="A1.I5.i9.p1.1.m1.1.1.cmml" xref="A1.I5.i9.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i9.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i9.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i9.p1.2.1">Embedding-based<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i9.p1.2.1.m1.1"><semantics id="A1.I5.i9.p1.2.1.m1.1a"><mo id="A1.I5.i9.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i9.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i9.p1.2.1.m1.1b"><ci id="A1.I5.i9.p1.2.1.m1.1.1.cmml" xref="A1.I5.i9.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i9.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i9.p1.2.1.m1.1d">]</annotation></semantics></math></span> explanation, evaluation, etc.</p>
</div>
</li>
<li class="ltx_item" id="A1.I5.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I5.i10.p1">
<p class="ltx_p" id="A1.I5.i10.p1.2"><math alttext="[" class="ltx_Math" display="inline" id="A1.I5.i10.p1.1.m1.1"><semantics id="A1.I5.i10.p1.1.m1.1a"><mo id="A1.I5.i10.p1.1.m1.1.1" stretchy="false" xref="A1.I5.i10.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i10.p1.1.m1.1b"><ci id="A1.I5.i10.p1.1.m1.1.1.cmml" xref="A1.I5.i10.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i10.p1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i10.p1.1.m1.1d">[</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A1.I5.i10.p1.2.1">multi-agent reinforcement learning with self-play and population play<math alttext="]" class="ltx_Math" display="inline" id="A1.I5.i10.p1.2.1.m1.1"><semantics id="A1.I5.i10.p1.2.1.m1.1a"><mo id="A1.I5.i10.p1.2.1.m1.1.1" stretchy="false" xref="A1.I5.i10.p1.2.1.m1.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.I5.i10.p1.2.1.m1.1b"><ci id="A1.I5.i10.p1.2.1.m1.1.1.cmml" xref="A1.I5.i10.p1.2.1.m1.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I5.i10.p1.2.1.m1.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.I5.i10.p1.2.1.m1.1d">]</annotation></semantics></math></span> techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p8">
<p class="ltx_p" id="A1.SS4.p8.1">We acknowledge the extensive scope and rapid advancements of research in this area, and posit that our study offers insights that can be generalized to various modalities. For example, the value taxonomy and human-in-the-loop evaluation paradigm outlined in our framework can be applied to both text-based and other modality-based (<em class="ltx_emph ltx_font_italic" id="A1.SS4.p8.1.1">e.g.,</em> vision, robotics) models.
It’s worth noting that our literature review does not aim to exhaustively cover all papers in the field, which is impossible given the rapid advancement of human-AI alignment research. Instead, we adopt a human-centered perspective to review more than 400 key studies in this domain, focusing on delineating the framework landscape, identifying limitations, future directions, and a roadmap to pave the way for future research.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Author contributions</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">This project was a team effort, built on countless contributions from everyone involved.
To acknowledge individual authors’ contributions and enable future inquiries to be directed appropriately,
we followed the ACM’s policy on authorship <cite class="ltx_cite ltx_citemacro_citep">(of Publications, <a class="ltx_ref" href="https://arxiv.org/html/2406.09264v3#bib.bib131" title="">2024</a>)</cite> and listed contributors for each part of the paper below.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1. </span>Overall Author List and Contributions</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p1.1.1">Project Lead</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">The project lead initialized and organized the project, coordinated with all authors, participated in the entire manuscript.</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">Hua Shen (University of Michigan, huashen@umich.edu)</span>: Initiated and led the overall project, prepared weekly project meetings, filtered papers, designed dimensions and codes (initial, revision), coded all papers, initiated the framework and developed human value and interaction modes analysis figures, participated in drafting all sections, paper revision and polishing.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p4.1.1">Team Leads</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.1">The team leads organized all team events, coordinated with leads and members, contributed to a portion of manuscript.</p>
</div>
<div class="ltx_para" id="A2.SS1.p6">
<ul class="ltx_itemize" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p" id="A2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I2.i1.p1.1.1">Tiffany Knearem (Google, tknearem@google.com)</span>: Led the HCI team, prepared weekly team meetings, filtered papers, designed dimensions and codes (initial, revision), coded partial papers, ideated the framework and analysis and future work content, participated in writing (Critical Thinking and AI Impact on Human sections), paper revision and polishing.</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p" id="A2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I2.i2.p1.1.1">Reshmi Ghosh (Microsoft, reshmighosh@microsoft.com)</span>: Led the NLP/AI team, prepared weekly team meetings, filtered papers, coded partial papers, ideated the framework and analysis and future work content, participated in writing (AI evaluation section), paper revision and polishing.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p7">
<p class="ltx_p" id="A2.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p7.1.1">Team Members (Alphabetical)</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p8">
<p class="ltx_p" id="A2.SS1.p8.1">The team members contributed to a portion of paper review, regular discussions, and drafted a portion of the manuscript.</p>
</div>
<div class="ltx_para" id="A2.SS1.p9">
<ul class="ltx_itemize" id="A2.I3">
<li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i1.p1">
<p class="ltx_p" id="A2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i1.p1.1.1">Kenan Alkiek (University of Michigan, kalkiek@umich.edu)</span>: filtered papers, coded partial papers, data processing and analysis, ideated paper analysis and future work, paper revision and polishing, mainly involved in NLP Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i2.p1">
<p class="ltx_p" id="A2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i2.p1.1.1">Kundan Krishna (Carnegie Mellon University, kundank@andrew.cmu.edu)</span>: filtered papers, coded partial papers, ideated the framework and future work, participated in writing (Customizing AI section), designed dimensions and codes (initial, revision), paper revision and polishing, mainly involved in NLP Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i3.p1">
<p class="ltx_p" id="A2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i3.p1.1.1">Yachuan Liu (University of Michigan, yachuan@umich.edu)</span>: filtered papers, coded partial papers, participated in writing (revised Integrate General Value and Customization content sections), paper revision and polishing, mainly involved in NLP Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i4.p1">
<p class="ltx_p" id="A2.I3.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i4.p1.1.1">Ziqiao Ma (University of Michigan, marstin@umich.edu)</span>: filtered papers, coded partial papers, designed dimensions and codes (initial, revision), developed Human Value category, participated in writing (Human Value taxonomy, revised representation, and value gap analysis sections), paper revision and polishing, mainly involved in NLP Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i5.p1">
<p class="ltx_p" id="A2.I3.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i5.p1.1.1">Savvas Petridis (Google PAIR, petridis@google.com)</span>: filtered papers, coded partial papers, ideated the interaction-related analysis and future work, participated in writing (Perceive and Understand AI), paper revision and polishing, mainly involved in HCI Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i6.p1">
<p class="ltx_p" id="A2.I3.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i6.p1.1.1">Yi-Hao Peng (Carnegie Mellon University, yihaop@cs.cmu.edu)</span>: filtered papers, coded partial papers, participated in writing (Human-AI Collaboration section), paper revision and polishing, mainly involved in HCI Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i7.p1">
<p class="ltx_p" id="A2.I3.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i7.p1.1.1">Li Qiwei (University of Michigan, rrll@umich.edu)</span>: filtered papers, coded partial papers, ideated the interaction-related taxonomy and analysis, participated in writing (Interaction Mode section), mainly involved in HCI Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i8.p1">
<p class="ltx_p" id="A2.I3.i8.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i8.p1.1.1">Sushrita Rakshit (University of Michigan, sushrita@umich.edu)</span>: filtered papers, coded partial papers, participated in writing (Integrate General Value section), paper revision and polishing, mainly involved in NLP and HCI Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i9.p1">
<p class="ltx_p" id="A2.I3.i9.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i9.p1.1.1">Chenglei Si (Stanford University, clsi@stanford.edu)</span>: filtered papers, coded partial papers, designed dimensions and codes (initial, revision), ideated the framework and future work, participated in writing (Assessment of Collaboration and Impact section), paper revision and polishing, mainly involved in HCI Team</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I3.i10.p1">
<p class="ltx_p" id="A2.I3.i10.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I3.i10.p1.1.1">Yutong Xie (University of Michigan, yutxie@umich.edu)</span>: filtered papers, coded partial papers, designed dimensions and codes (initial, revision), ideated the value representation taxonomy, participated in writing (Human Value Representation section), paper revision and polishing, , mainly involved in NLP Team</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p10">
<p class="ltx_p" id="A2.SS1.p10.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p10.1.1">Advisors (Alphabetical)</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p11">
<p class="ltx_p" id="A2.SS1.p11.1">The advisors involved in and made intellectual contributions to essential components of the project and manuscript.</p>
</div>
<div class="ltx_para" id="A2.SS1.p12">
<ul class="ltx_itemize" id="A2.I4">
<li class="ltx_item" id="A2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i1.p1">
<p class="ltx_p" id="A2.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i1.p1.1.1">Jeffrey P. Bigham (Carnegie Mellon University, jbigham@cs.cmu.edu)</span>:
contributed to the framework on aligning human to AI direction, vision on the status quo of alignment research, and future work discussions, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i2.p1">
<p class="ltx_p" id="A2.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i2.p1.1.1">Frank Bentley (Google, fbentley@google.com)</span>:
contributed to the historical context and project objectives, improved the definitions and design of research methodology, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i3.p1">
<p class="ltx_p" id="A2.I4.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i3.p1.1.1">Joyce Chai (University of Michigan, chaijy@umich.edu)</span>:
iteratively involved in developing and revising definitions and the framework on aligning AI to human direction, advised on analysis and future work, and participated in paper revision and proofreading.
</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i4.p1">
<p class="ltx_p" id="A2.I4.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i4.p1.1.1">Zachary Lipton (Carnegie Mellon University, zlipton@cmu.edu)</span>:
contributed insights from Machine Learning, NLP, and AI fields to revise the definitions and framework on aligning AI to human direction, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i5.p1">
<p class="ltx_p" id="A2.I4.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i5.p1.1.1">Qiaozhu Mei (University of Michigan, qmei@umich.edu)</span>:
contributed insights from Data Science, Machine Learning, and NLP fields to improve definitions and the framework on aligning AI to human direction, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i6.p1">
<p class="ltx_p" id="A2.I4.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i6.p1.1.1">Rada Mihalcea (University of Michigan, mihalcea@umich.edu)</span>:
involved in framing and revising the structure and taxonomy of human values, and contributed to improving the manuscript’s title, introduction, and other sections, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i7.p1">
<p class="ltx_p" id="A2.I4.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i7.p1.1.1">Michael Terry (Google Research, michaelterry@google.com)</span>:
contributed arguments and vision on the status quo of alignment research, framed project objectives and contributions, improved definitions and data analysis, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I4.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I4.i8.p1">
<p class="ltx_p" id="A2.I4.i8.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I4.i8.p1.1.1">Diyi Yang (Stanford University, diyiy@stanford.edu)</span>:
involved in improving definitions and the framework, contributed social insights to the work, and participated in paper revision and proofreading.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p13">
<p class="ltx_p" id="A2.SS1.p13.1"><span class="ltx_text ltx_font_bold" id="A2.SS1.p13.1.1">Project Leading Advisors</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p14">
<p class="ltx_p" id="A2.SS1.p14.1">The project leading advisors actively involved in the entire project process and all manuscript sections.</p>
</div>
<div class="ltx_para" id="A2.SS1.p15">
<ul class="ltx_itemize" id="A2.I5">
<li class="ltx_item" id="A2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I5.i1.p1">
<p class="ltx_p" id="A2.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I5.i1.p1.1.1">Meredith Ringel Morris (Google DeepMind, merrie@google.com)</span>:
iteratively involved in drafting all sections, contributed to core argument ideation, framework and definition improvement, provided future work insights, and participated in paper drafting, revision, and proofreading on all sections.</p>
</div>
</li>
<li class="ltx_item" id="A2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I5.i2.p1">
<p class="ltx_p" id="A2.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I5.i2.p1.1.1">Paul Resnick (University of Michigan, presnick@umich.edu)</span>:
actively involved and advised on the entire project process, including initiating the project and research agenda, iteratively improved definitions, framework, and analysis, and participated in paper revision and proofreading.</p>
</div>
</li>
<li class="ltx_item" id="A2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I5.i3.p1">
<p class="ltx_p" id="A2.I5.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I5.i3.p1.1.1">David Jurgens (University of Michigan, jurgens@umich.edu)</span>:
provided advice throughout the project, including iterative discussions on project milestones and content ideation, organized several meetings to receive feedback from external audiences, and participated in paper revision and proofreading.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank Eric Gilbert for his constructive feedback on human-centered insights on alignment, thank Eytan Adar for his valuable guidance on designing the interaction techniques for human-AI alignment,
and thank Elizabeth F. Churchill for her insightful discussion on this manuscript. We also thank Michael S Bernstein, Denny Zhou, Cliff Lampe, and Nicole Ellison
for their encouraging feedback on this work.
We welcome researchers’ constructive discussions and interdisciplinary efforts to achieve long-term and dynamic human-AI alignment collaboratively in the future.
This work was supported in part by the National Science Foundation under Grant No. IIS-2143529 and No. IIS-1949634.



</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 10 17:45:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
