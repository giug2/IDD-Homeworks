<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Disaggregating Embedding Recommendation Systems with FlexEMR</title>
<!--Generated on Sat Sep 28 01:53:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.12794v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S1" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.SS1" title="In 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background: EMR models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.SS2" title="In 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Motivation: Disaggregated EMR serving</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.SS3" title="In 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Key research challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.SS4" title="In 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Our solution: FlexEMR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>FlexEMR design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1" title="In 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Locality-enhanced disaggregation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1.SSS1" title="In 3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Adaptive EMB caching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1.SSS2" title="In 3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Hierarchical EMB pooling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS2" title="In 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>EMB lookup with Multi-threaded RDMA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S4" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Preliminary results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S5" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S6" title="In Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion &amp; Future work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\mdfdefinestyle</span>
<p class="ltx_p" id="p1.2">backgroundbackgroundcolor=lightorange,innerrightmargin=0cm,innertopmargin=-0.1cm,innerbottommargin=-0.10cm,leftmargin=+0cm, roundcorner=2pt



 












</p>
</div>
<h1 class="ltx_title ltx_title_document">Disaggregating Embedding Recommendation Systems with FlexEMR</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yibo Huang, Zhenning Yang, Jiarong Xing<sup class="ltx_sup" id="id9.9.id1"><span class="ltx_text ltx_font_italic" id="id9.9.id1.1">†</span></sup>, Yi Dai<sup class="ltx_sup" id="id10.10.id2">⋆</sup>, Yiming Qiu 
<br class="ltx_break"/>Dingming Wu<sup class="ltx_sup" id="id11.11.id3">∗</sup>, Fan Lai<sup class="ltx_sup" id="id12.12.id4">⋄</sup>, Ang Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.8.4" style="font-size:120%;">University of Michigan    <sup class="ltx_sup" id="id8.8.4.5"><span class="ltx_text ltx_font_italic" id="id8.8.4.5.1">†</span></sup>Rice University    <sup class="ltx_sup" id="id8.8.4.6">⋆</sup>Fudan University 
<br class="ltx_break"/>   <sup class="ltx_sup" id="id8.8.4.7">∗</sup>Unaffiliated    <sup class="ltx_sup" id="id8.8.4.8">⋄</sup>University of Illinois Urbana-Champaign</span><span class="ltx_text" id="id13.13.id1" style="font-size:120%;">
<span class="ltx_text ltx_affiliation_country" id="id13.13.id1.1"></span>
</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id14.id1">Efficiently serving embedding-based recommendation (EMR) models remains a significant challenge due to their increasingly large memory requirements.
Today’s practice splits the model across many monolithic servers, where a mix of GPUs, CPUs, and DRAM is provisioned in fixed proportions. This approach leads to suboptimal resource utilization and increased costs. Disaggregating embedding operations from neural network inference is a promising solution but raises novel networking challenges.
In this paper, we discuss the design of FlexEMR for optimized EMR disaggregation. FlexEMR proposes two sets of techniques to tackle the networking challenges: Leveraging the temporal and spatial locality of embedding lookups to reduce data movement over the network and designing an optimized multi-threaded RDMA engine for concurrent lookup subrequests. We outline the design space for each technique and present initial results from our early prototype.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>;  2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>;</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Embedding-based Recommendation (EMR) models, widely used in e-commerce, search engines, and short video services, dominate AI inference cycles in production datacenters, such as those at Meta <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib13" title="">2020c</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib29" title="">2023</a>)</cite>.
They process user queries using both continuous and categorical features, transforming categorical features into dense vectors via embedding table lookups, and finally, combining them with the continuous features for neural network (NN) scoring.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Serving EMR models at scale leads to pressing memory requirements <cite class="ltx_cite ltx_citemacro_citep">(Mudigere et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib33" title="">2022</a>)</cite> as embedding tables can grow to terabytes in size, accounting for over 99% of model parameters <cite class="ltx_cite ltx_citemacro_citep">(Desai and Shrivastava, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib5" title="">2022</a>)</cite>.
In practice, we need to partition an EMR model and distribute it across multiple monolithic servers with a mix of GPUs, CPUs, and DRAM <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib55" title="">2022a</a>; Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib34" title="">2019</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib29" title="">2023</a>)</cite>.
On a specific server,
existing advances propose to decouple the embedding lookup from the NN computation and use DRAM for embedding store <cite class="ltx_cite ltx_citemacro_citep">(Lui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib31" title="">2021</a>)</cite>.
Recent work <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>)</cite> further enhances this approach by employing an embedding cache on GPUs to optimize lookup performance.
However, this monolithic approach has limitations in scalability and total
cost of ownership (TCO) in practice.
Recommendation workloads need a mix of resources—memory for embedding store and GPUs for NN computation, and this mixture varies across models and evolves over time. Monolithic servers that provision resources in a fixed portion is hard to achieve both performance and cost efficiency. Recent studies show that it can lead to idle resources and wasted costs of up to 23.1% <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib25" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">A promising approach to achieve performant and cost-efficient large EMR model serving is to disaggregate embedding storage and NN computation into independent servers.
Specifically, using CPU-based servers to store embedding tables in memory while utilizing GPU nodes for NN computations. These components are interconnected via high-speed networks, such as remote direct memory access (RDMA) <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib2" title="">2023</a>; Shan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib41" title="">2018</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib10" title="">2016</a>)</cite>.
This decouples the memory and GPU resources and allows them to scale independently, improving the total resource efficiency and reducing the TCO. Disaggregation also increases system robustness, as failures are isolated to individual components.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, disaggregating EMR model serving raises novel networking challenges.
First, remote embedding lookup involves extensive data transmission over the network.
For example, an 8-byte categorical feature index could generate a returned embedding vector with hundreds or even thousands of float values <cite class="ltx_cite ltx_citemacro_citep">(Research, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib38" title="">2023</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib56" title="">2022b</a>)</cite>.
Worse still, each lookup needs to query multiple such indices, and each batch contains up to thousands of lookups <cite class="ltx_cite ltx_citemacro_citep">(Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib34" title="">2019</a>; Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib19" title="">2023b</a>)</cite>.
This can be efficiently handled by local GPU memory with high memory bandwidth in a monolithic design. However, decoupling embedding storage and computation shifts this pressure to the network, with a much smaller bandwidth, potentially causing network bottlenecks.
On the other hand, intensive data transmission imposes stringent performance requirements on the network layer. Unfortunately, today’s RDMA systems <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib8" title="">2021</a>; Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib49" title="">2019</a>)</cite> are not designed for EMR disaggregation. For instance, the single-thread RDMA I/O models that are commonly used in regular applications <cite class="ltx_cite ltx_citemacro_citep">(Kalia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib23" title="">2019</a>; Dragojević et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib6" title="">2014</a>)</cite> will suffer from high software queuing latency for EMR serving.
The recent design on disaggregated EMR systems <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib25" title="">2022</a>)</cite> mainly focuses on resource provisioning but overlooks the above networking challenges.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we design an optimized disaggregated EMR system called FlexEMR. FlexEMR optimizes the disaggregation by proposing two classes of techniques to tackle the challenges discussed above.
The first set of techniques explores the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">temporal</span> and <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">spatial</span> locality of embedding lookup.
While existing works <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>)</cite> implement embedding caches on GPUs to leverage temporal locality,
we observe that such caches could compete with NN computation for limited GPU memory, and propose mechanisms to dynamically adjust caching strategy to avoid contention.
Given that multiple lookup subrequests could point to the same embedding server, we further investigate the benefit of spatial locality.
we design a hierarchical embedding pooling strategy that partially offloads pooling operations into the embedding servers, utilizing their available CPU cycles. This reduces embedding movement in the network and mitigate pressure on the rankers, while putting CPU resources to good use.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The second set of techniques aims at optimizing the RDMA I/O engines used for remote embedding lookups. First, we explore the design of a contention-free multi-threaded RDMA service, allowing concurrent RDMA threads to post lookup requests to different embedding servers in parallel. This approach significantly reduces queuing latency compared to commonly used single-threaded RDMA solutions. Additionally, we handle skewed access patterns by periodically migrating connections across embedding servers, and deploy credit-based flow control to mitigate response congestions. These optimizations collectively enhance the performance of remote embedding lookups in FlexEMR.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Working together, these two sets of techniques enable an efficient, flexible, and cost effective EMR model serving architecture. We validate the key ideas of FlexEMR using micro-benchmarks and present preliminary results in §<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S4" title="4. Preliminary results ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Overview</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide background on EMR model serving, describe the motivation and challenges for disaggregated EMR serving, and present an overview of our solutions.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Background: EMR models</h3>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S2.F1.g1" src="x1.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">A representative EMR model—Deep Learning Recommendation Model (DLRM).</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">An EMR model handles two types of input features: <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">categorical</span> features (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">sparse</span>) representing discrete categories or groups and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">continuous</span> features (<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.4">dense</span>) representing measurements or quantities that are continuous in nature <cite class="ltx_cite ltx_citemacro_citep">(Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib34" title="">2019</a>)</cite>. For example, in a video recommendation system, categorical features could include video IDs, genres, or user IDs, while continuous features could include user age or watch time.
The categorical features often have very high cardinality, as each feature can consist of millions of instances (e.g., numerous specific IDs for users in feature “user IDs”).
EMR models convert these high-dimensional categorical features into dense vector representations via <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.5">embedding tables</span>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F1" title="Figure 1 ‣ 2.1. Background: EMR models ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the key components and workflow of a representative EMR model. It takes candidate items as input, including both categorical and continuous features from upstream.
For those accessed instances (e.g., IDs) in a batch, EMR retrieves their associated embeddings (dense vectors), which will be aggregated into a single fixed-size embedding vector through pooling operations such as sum or average.
Meanwhile, the continuous features are processed by a bottom neural network (<span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">bottom NN</span>) which is typically a multilayer perceptron (MLP) to generate high-dimensional dense vectors.
The feature interaction process combines the dense vectors from categorical and continuous input features through operations such as element-wise multiplication or concatenation.
The combined result is fed into a top neural network (<span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">top NN</span>) to compute user-item scores for top-k ranking.
The items with the highest scores are presented to the user.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Motivation: Disaggregated EMR serving</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">State-of-the-art EMR models consist of hundreds of sparse features, each associated with an embedding table with potentially millions of embedding rows <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib11" title="">2020a</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib35" title="">2024</a>)</cite>.
Indeed, production-level EMR models could have TB-level embedding tables (e.g., Meta uses 50TB DLRM model <cite class="ltx_cite ltx_citemacro_citep">(Mudigere et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib33" title="">2022</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The large-size embeddings have presented significant challenges for EMR serving because they cannot be stored on a single GPU.
Therefore, EMR embeddings are often partitioned and scattered across multiple servers, each server has a combination of GPUs, CPUs, and DRAM.
Considering that EMR workloads require two distinct types of resources—large memory for embedding storage and GPUs for NN computation—researchers propose decoupling them for better flexibility. Specifically, this approach leverages DRAM and CPUs for embedding storage and lookup, while utilizing GPUs on the same servers for NN computation.
As a further improvement, an embedding cache is employed in GPU memory to cache the “hot” entries to optimize the lookup performance <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The embedding-NN decoupling enables more flexible EMR serving, but doing that on monolithic servers has several limitations.
Monolithic servers provision GPU, CPU, and DRAM resources in fixed proportions, but the demands for these resources by EMR workloads can evolve across models and change over time due to varied recommendation workloads.
Scaling up the whole server for the most demanding resource or the peak workload will lead to low resource utilization and waste of costs.
A recent study <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib25" title="">2022</a>)</cite> has found that fixed resource provision on monolithic servers can result in wasted costs of up to 23.1%.
Therefore, more resource- and cost-efficient EMR serving are urgently needed.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Building upon the trend of disaggregation in datacenters <cite class="ltx_cite ltx_citemacro_citep">(Ewais and Chow, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib7" title="">2023</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib14" title="">2023</a>)</cite>, a promising solution is to fully disaggregate the embedding layer and dense NN compute into network-interconnected CPU embedding servers and GPUs (rankers), respectively.
The rankers access embeddings stored on embedding servers over high-speed networks, such as RDMA.
This new EMR serving paradigm offers multifold benefits including: (1) Flexible scalability. It allows each component to scale independently, e.g., allocating additional memory to accommodate larger embedding tables.
(2) Cost efficiency. It allows rankers to multiplex many embeddings streamed from CPU embedding servers, greatly improving resource utilization and reducing the number of rankers needed for deploying EMR models.
(3) Improved robustness. It enhances system robustness because the embedding operations and NN computation failures can be perfectly isolated.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">EMR disaggregation is an emerging direction that remains underexplored. The most closely related work, DisaggRec <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib25" title="">2022</a>)</cite>, shares similar disaggregation concepts with us but primarily focuses on resource provisioning and scheduling post-disaggregation. However, EMR disaggregation introduces several networking challenges, as we will discuss next, that have not yet been thoroughly studied.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S2.F2.g1" src="x2.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Embedding layer dominates EMR serving.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Key research challenges </h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Disaggregated EMR serving involves a large volume of data movements over the network.
Typically, given some categorical feature indices as input, the ranker first fetches all corresponding embedding vectors from remote embedding servers and then performs feature pooling operations.
As a result, the network bandwidth between embedding servers and rankers becomes a major bottleneck (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F2" title="Figure 2 ‣ 2.2. Motivation: Disaggregated EMR serving ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">2</span></a>), presenting several domain-specific challenges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">(1) Contention in GPU memory.</span>
To reduce data movement over the network, existing works <cite class="ltx_cite ltx_citemacro_citep">(Kal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib22" title="">2021</a>; Ibrahim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib18" title="">2021</a>; Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib24" title="">2020</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib30" title="">2021</a>; Wilkening et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib47" title="">2021</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib48" title="">2022</a>; Kurniawan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib26" title="">2023</a>; Kwon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib28" title="">2019</a>)</cite> attempt to cache frequently accessed embedding entries in GPU memory.
However, we observe that embedding cache is far from a perfect solution.
Using precious GPU memory for caching could significantly reduce serving throughput, especially when the NN model size and request batch size are large. Essentially, NN inference also requires a large amount of GPU memory, and the existing caching strategy could cause serious resource contention between the two tasks.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">(2) Large-scale fan-out pattern.</span>
Remote embedding lookup generates large-scale fan-out subrequests.
For example, an 8-byte categorical feature index could generate a returned embedding vector with hundreds or even thousands of bytes in dimension size. Moreover, each lookup needs to query multiple such indices, and each batch contains up to thousands of lookups.
Unlike local memory, the network bandwidth is significantly lower.
Hence, issuing hundreds or thousands of concurrent batched embedding lookups can lead to severe network contention and degraded performance.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">(3) RDMA engine efficiency.</span>
RDMA is commonly used for remote data access. Most existing RDMA applications employ single-threaded RDMA I/O models, which send out RDMA read requests to different target machines using one thread.
This leads to extended queuing latency in our scenario. We need to design a more efficient RDMA I/O engine capable of handling concurrent embedding lookup requests and results, while effectively re-balancing skewed workload patterns across distributed embedding servers.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="505" id="S2.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">FlexEMR architecture overview.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Our solution: FlexEMR</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In this paper, we propose FlexEMR—an EMR serving system that aims at addressing the aforementioned challenges.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F3" title="Figure 3 ‣ 2.3. Key research challenges ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the envisioned system architecture. At a high level, the rankers initiate embedding lookups. Each lookup contains multiple subrequests, which firstly go through an adaptive embedding cache on rankers serving as a lookup fast path for reduced latency.
The requests are then sent to embedding servers via a set of optimized RDMA engines. Once the corresponding embedding vectors are found, FlexEMR initiates a hierarchical pooling process to retrieve results without causing network contentions.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Our design is primarily based on two sets of techniques.
First, we reduce embedding movement over the network by exploiting <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.1">temporal</span> and <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.2">spatial</span> locality across embedding lookups and subrequests.
Temporal locality means that (i) a non-negligible portion of embeddings (e.g., 10%<math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mo id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><csymbol cd="latexml" id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">∼</annotation></semantics></math>15%) are the most frequently accessed in a period (i.e. <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.3">hot embeddings</span> <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib11" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib12" title="">b</a>; Wilkening et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib47" title="">2021</a>)</cite>), and (ii) some subrequests often appear together in the same lookup (i.e. <span class="ltx_text ltx_font_italic" id="S2.SS4.p2.1.4">embedding co-occurrence</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib50" title="">2023</a>)</cite>).
Existing work has leveraged temporal locality to design embedding caches on GPUs, but we argue that the caching design should be dynamically adjusted to avoid GPU memory contention.
Spatial locality means that multiple embedding tables/shards often co-locate in the same embedding server, so many subrequests in a lookup will be sent to the same destinations.
As such, we propose to push-down lightweight pooling operations onto embedding servers.
This leverages the fact that
embedding servers also contain CPU resources, but they are under-utilized at runtime <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib20" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">Second, we improve the networking layer of FlexEMR serving with a fleet of RDMA optimizations. Specifically, we propose to use multi-thread RDMA for embedding lookups. It allows multiple RDMA IO threads to concurrently handle lookups into different embedding servers, thus reducing the queuing latency. We address the RNIC resource contention problem across concurrent RDMA requests using a mapping-aware RDMA IO engine, and envision a load-balanced live migration mechanism to overcome the problem of skewed requests across embedding servers. A new credit-based flow control mechanism is further implemented to avoid head-of-line blocking caused by traffic bursts.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S2.F4.g1" src="x4.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Hierarchical EMB pooling. Pooling computation handled solely by the ranker can cause network contention (<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F4" title="Figure 4 ‣ 2.4. Our solution: FlexEMR ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">4</span></a>a). Performing pooling hierarchically, sending only the intermediate results to the ranker can reduce network traffic (<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F4" title="Figure 4 ‣ 2.4. Our solution: FlexEMR ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">4</span></a>b). </span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>FlexEMR design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we outline a potential system design and optimizations.
We first highlight the design of an adaptive caching mechanism and a hierarchical EMB pooling architecture in section <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1" title="3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
Next, in section <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS2" title="3.2. EMB lookup with Multi-threaded RDMA ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we discuss how multi-threaded RDMA further optimizes the embedding lookup.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Locality-enhanced disaggregation</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Adaptive EMB caching</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">A common practice to reduce embedding lookup latency is to leverage the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">temporal locality</span> across requests and cache “hot” lookup or pooling results in GPU memory <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>)</cite>.
However, because the embedding caches share GPU memory with NN computation, an enlarged cache inevitably leads to a smaller batch size for NN computing due to GPU memory contention, thereby degrading overall throughput. In this work, we explore an approach to adaptively adjust the size of cache: when the system is overloaded, FlexEMR reduces cache size automatically to preserve overall throughput; otherwise, it expands the cache to improve latency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p2.1.1">Tracing temporal dynamics.</span>
The first step towards an adaptive caching strategy is to capture the workload temporal dynamics (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.F5" title="Figure 5 ‣ 3.1.2. Hierarchical EMB pooling ‣ 3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">5</span></a>).
In reality, the ranker often uses a task queue to receive batches of requests from upstream, then feeds them into downstream EMR models.
FlexEMR could monitor the size of these batches, then apply a sliding window algorithm to determine whether the system is under high load.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p3.1.1">Adjusting cache size.</span>
Once a decision is made to enlarge or shrink cache size, we need to consider how to
enforce these actions accordingly. This involves two sub-tasks: Firstly, we need to determine the updated cache size. Our observation here is that, given the incoming batch size and the EMR model architecture, it is possible to build a model to estimate the memory size required by NN computation. The ideal cache size is the difference between GPU memory capacity and the parts reserved for NN. The second task is to
swap embeddings into or out of GPU memory
For the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p3.1.2">swap in</span> action, FlexEMR could initiate RDMA reads from the ranker to asynchronously fetch the hot embeddings from embedding servers in a transparent manner.
For the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p3.1.3">swap out</span> action, FlexEMR should remove part of embedding cache lines based on a LRU algorithm, and free up the corresponding GPU memory.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Hierarchical EMB pooling</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Apart from temporal locality, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p1.1.1">spatial locality</span> is also prevalent in EMR serving systems.
In a disaggregated architecture, embedding tables are placed onto a set of remote embedding servers.
Given an embedding lookup request from the ranker, a typical workflow is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F4" title="Figure 4 ‣ 2.4. Our solution: FlexEMR ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">4</span></a>(a):
First, the ranker sends sub-requests to remote embedding servers and asks them to return corresponding embedding vectors.
The ranker then aggregates these results through <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p1.1.2">pooling</span> operations.
This communication leads to extensive embedding movement over the network and increased latency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.1.1">Hierarchical pooling leveraging spatial locality.</span>
We seek to reduce the embedding movements between the ranker and embedding servers for higher throughput under bounded latency.
Our finding here is that the CPUs in embedding servers could be utilized to perform <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p2.1.2">partial pooling</span> operations. If an embedding server contains multiple required vectors (i.e. spatial locality), then it could aggregate them first before sending to the ranker.
Motivated by this finding, we envision a hierarchical pooling architecture, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S2.F4" title="Figure 4 ‣ 2.4. Our solution: FlexEMR ‣ 2. Overview ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">4</span></a>(b).
For each embedding lookup, FlexEMR first invokes embedding server CPUs to perform partial pooling whenever possible, then asks the ranker to retrieve their outputs and perform <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p2.1.3">global pooling</span> to obtain the final results.
Unlike existing works <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib58" title="">2022</a>)</cite>,
FlexEMR is the first to explore parallel operator push-downs (i.e., pushing pooling operations down to embedding servers). This design could potentially generalize to other workloads with large-scale fan-out patterns.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.1.1">Routing table for identifying co-located embeddings.</span>
An important question here is how to identify the embedding <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p3.1.2">spatial locality</span> among embedding servers—i.e., given as input a set of sparse feature indices, we need to identify which indices are co-located at where.
A naïve solution is to maintain a routing table storing the
<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p3.1.3">¡feature indice, dest embedding server¿</span> mapping pairs.
It then queries all corresponding embedding servers and aggregates the sparse feature indices into multiple groups depending on their embedding servers.
However, this leads to huge memory footprints due to numerous sparse feature spaces.
We observe a large embedding table is often partitioned into multiple shards in a row-wise manner, and each shard corresponds to an embedding range containing <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p3.1.4">start</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p3.1.5">end</span> indices.
Based on this, we envision a range-based routing table in ranker where we store <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p3.1.6">¡(start index, end index), dest embedding server¿</span> pairs for each embedding shard.
For a list of sparse indices, FlexEMR only needs to use the range to which each indice belongs to get the target embedding server efficiently.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S3.F5.g1" src="x5.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Distribution of inference workloads in Alibaba PAI platform over one week.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>EMB lookup with Multi-threaded RDMA</h3>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="262" id="S3.F6.g1" src="x6.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">FlexEMR multi-threaded embedding lookup. Mapping awareness eliminates inter-thread contentions.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We next discuss how to optimize the RDMA I/O engine for remote embedding lookup.
Given a batch of embedding lookup requests, each containing a large amount of fan-out subrequests, rankers in FlexEMR require an efficient RDMA I/O engine to forward subrequests to remote embedding servers. Since the completion time of an embedding lookup is dominated by the slowest subrequest, the overall pipeline is very sensitive to tail latency. The single-threaded RDMA IO model used for most existing RDMA applications <cite class="ltx_cite ltx_citemacro_citep">(Dragojević et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib6" title="">2014</a>)</cite> becomes a major bottleneck, since it incurs high queuing latency overhead between the embedding and transport layers.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">A promising solution is to use <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">multi-threaded RDMA</span>, where RDMA connections to embedding servers are assigned to multiple I/O threads, and embedding subrequests are distributed to these connections according to corresponding destinations.
However, naïvely using multi-threaded RDMA introduces non-negligible contentions due to limited RNIC parallelism resources (e.g., user access regions <cite class="ltx_cite ltx_citemacro_citep">(Technologies, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib44" title="">2016</a>)</cite>):
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S4.F8" title="Figure 8 ‣ 4. Preliminary results ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">8</span></a> (left) shows that it can lead to up to 62% throughput drop in our microbenchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Contention-free multi-threaded embedding lookup.</span>
To understand the root cause of contentions under concurrent lookup subrequests, we delve deep into the architecture of multi-threaded RDMA. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.F6" title="Figure 6 ‣ 3.2. EMB lookup with Multi-threaded RDMA ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">6</span></a>, we find that each RDMA engine contains a dedicated I/O thread, and each thread encompasses multiple RDMA connections.
The RNIC parallelism units are allocated to each newly created connection in a round-robin manner, resulting in a one-to-many mapping between RDMA parallelism units and connections.
However, the I/O threads for remote embedding lookup are not aware of such mappings,
thereby enforcing multiple RDMA connections belonging to different I/O threads to access the same parallelism unit simultaneously.
To coordinate different I/O threads, each parallelism unit must implement a complex locking mechanism, which could introduce significant performance overhead.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">To solve this problem, we envision a mapping-aware multi-threaded RDMA engine, capable of transparently generating one-to-one mapping between I/O threads and RNIC parallelism units, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.F6" title="Figure 6 ‣ 3.2. EMB lookup with Multi-threaded RDMA ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">6</span></a> (right).
The key-enabling technique is the <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.1">resource domain</span> feature provided by RDMA <cite class="ltx_cite ltx_citemacro_citep">(Corporation, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib3" title="">2024</a>; Mellanox, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib32" title="">2017</a>)</cite>, which exposes the mapping between connections and RNIC parallelism units to the application layer.
As such, FlexEMR could ensure that all connections assigned to the same parallelism unit are allocated to the same RDMA engine, thus preventing contention from concurrent threads.
Essentially, in the cluster initialization stage, FlexEMR firstly creates RDMA connections between embedding servers and rankers, then identifies their resource domains. Since there is a static mapping between the resource domain and parallelism unit, FlexEMR could subsequently aggregate connections into different RDMA engines according to the resource domain, so that each RDMA engine points to a dedicated parallelism unit.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Live connection migratation among RDMA engines.</span>
Another common problem in practice is skewed subrequest patterns.
Connections to different embedding servers might experience vastly different utilization rate, which leads to imbalanced loads among RDMA engines.
Since an RDMA engine can manage multiple connections used for different embedding servers,
a strawman solution is to live-migrate connections in overloaded engines to
under-utilized engines: Periodically, FlexEMR traces the number of queued subrequests in each connection. When a connection becomes overloaded, FlexEMR selects the least loaded RDMA engine and initiate the migration process.
However, this workflow brings back the RDMA multi-thread contention problem, because the migrated connection still points to the old parallelism unit.
FlexEMR aims at a live migration strategy without RDMA contention concerns. The key idea is to <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.2">re-associate</span> the migrated connection with the resource domain used by the new RDMA engine.
Notably, FlexEMR detaches the connection from the old domain, and then attaches it to the resource domain of the new one.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.1.1">Fast credit-based flow control with RDMA QoS.</span>
FlexEMR pipelines pooling computation and remote embedding lookup to further reduce serving latency.
To do that, FlexEMR introduces a per-connection task queue between remote embedding servers and RDMA engines,
so that the responses (i.e., embedding vectors) from embedding servers can be pushed into their respective queues asynchronously.
However, without careful flow control, concurrent subrequests could result in response bursts, which might overflow corresponding task queues and drastically increase tail latency.
A strawman solution is to leverage existing credit-based flow control <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib45" title="">2022</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib16" title="">2020</a>)</cite>—specifically, the ranker controls the size of each task queues via <span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.2">credits</span> and proactively piggybacks the credits in lookup requests.
However, since these credit messages share channels with regular lookup messages, the latter could easily introduce head-of-line blocking and delay reception of the former. As a result, embedding servers won’t be able to adjust sending rate of responses in time.
To mitigate such head-of-line blocking, we envision a fast credit control channel with higher priority.
By leveraging the hardware feature of connection-level quality of service (QoS) offered by RDMA, FlexEMR can create a dedicated RDMA connection with a higher service level for each <span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.3">¡ranker, embedding server¿</span> pair. At runtime, FlexEMR can use such connections as fast path to transfer credits timely even under high load pressure.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Preliminary results</h2>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S4.F7.g1" src="x7.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">GPU caching significantly limits the maximum batch size for inferences due to EMB-NN contentions.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F8.1" style="width:238.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="714" id="S4.F8.1.g1" src="x8.png" width="968"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F8.2" style="width:169.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="998" id="S4.F8.2.g1" src="x9.png" width="968"/>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.4.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S4.F8.5.2" style="font-size:90%;">Performance comparison between baseline and FlexEMR: Multi-threaded EMB lookup (left) and credit flow control for lookup (right). </span></figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Even though FlexEMR is still work-in-progress, we showcase initial evidence around its major components, including adaptive embedding caching (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1.SSS1" title="3.1.1. Adaptive EMB caching ‣ 3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) and multi-threaded RDMA embedding lookup (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS2" title="3.2. EMB lookup with Multi-threaded RDMA ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
We use the popular MLPerf framework <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib37" title="">2020</a>)</cite> and a set of production-scale embedding lookup traces released by Meta <cite class="ltx_cite ltx_citemacro_citep">(Research, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib38" title="">2023</a>)</cite> to synthesize inference workloads.
Our testbed includes two interconnected Intel Xeon servers each equipped with 32 CPU cores, 128GB memory, and a 100Gbps Mellanox RDMA NIC. One of them is equipped with a Nvidia A100 GPU with 80GB of memory.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Naïve caching leads to GPU contentions.</span>
To understand the benefit of adaptive EMB caching,
we analyze a pure GPU caching-based solution on a representative RMC2 model <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib20" title="">2023a</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib11" title="">2020a</a>)</cite>.
For the GPU caching baseline, we vary the size of EMB caches and observe the changes on supported batch sizes.
As Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S4.F7" title="Figure 7 ‣ 4. Preliminary results ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">7</span></a> shows, as we increase the GPU cache size, the caching-based solution has to settle with smaller batch size due to contention on GPU memory capacity, resulting in decreased inference throughput and wasted GPU compute cycles.
FlexEMR on the other hand aims to achieve the highest batch size through an
adaptive embedding caches,
as proposed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS1.SSS1" title="3.1.1. Adaptive EMB caching ‣ 3.1. Locality-enhanced disaggregation ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>, mitigating memory contention in most scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">FlexEMR outperforms naïve RDMA-based embedding lookup in efficiency.</span>
Next, we compare the lookup performance of a naïve multi-threaded RDMA baseline against our FlexEMR prototype.
As Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S4.F8" title="Figure 8 ‣ 4. Preliminary results ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates,
with mapping aware multi-threading, FlexEMR achieves higher throughput than baseline by up to 2.3x.
Moreover, FlexEMR achieves 35% lower latency on credits transmission, which further reduces possible congestion between rankers and embedding servers.
This demonstrates the importance of an efficient multi-threaded RDMA engine (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#S3.SS2" title="3.2. EMB lookup with Multi-threaded RDMA ‣ 3. FlexEMR design ‣ Disaggregating Embedding Recommendation Systems with FlexEMR"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Many existing works treat EMR as generic deep learning models and adopt GPU-centric approaches for their deployment <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib46" title="">2022</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib55" title="">2022a</a>; Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib34" title="">2019</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib29" title="">2023</a>)</cite>, leading to under-utilized GPU resources.
Recent projects apply a variety of caching mechanism<cite class="ltx_cite ltx_citemacro_citep">(Kal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib22" title="">2021</a>; Ibrahim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib18" title="">2021</a>; Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib24" title="">2020</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib30" title="">2021</a>; Wilkening et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib47" title="">2021</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib48" title="">2022</a>; Kurniawan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib26" title="">2023</a>; Kwon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib28" title="">2019</a>)</cite> to speed up embedding lookups.
However, these solutions suffer from low cache hit rate in production environments
 <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib20" title="">2023a</a>)</cite>.
Specialized hardware such as FPGAs has also been explored to enhance recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib54" title="">2022</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib21" title="">2021</a>; Hsia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib15" title="">2023</a>)</cite>,
but we strive for a generic solution with commodity hardware.
Compression <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib51" title="">2021</a>; Desai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib4" title="">2022</a>; Ginart et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib9" title="">2021</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib42" title="">2020a</a>)</cite> and sharding <cite class="ltx_cite ltx_citemacro_citep">(Sethi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib39" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib40" title="">2023</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib55" title="">2022a</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib43" title="">2020b</a>; Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib57" title="">2022c</a>)</cite> are common optimizations to embedding table lookup.
These works are complementary to ours, as the proposed techniques can be integrated seamlessly into FlexEMR for further improved performance.
DisaggRec <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib25" title="">2022</a>)</cite> proposed a similar disaggregated memory system. However, the resource distribution is fixed and determined through an exhaustive search. This approach introduces overhead and fails to capture serving dynamics.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion &amp; Future work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Embedding-based recommendation (EMR) model serving consumes the majority of AI inference cycles in production datacenters due to its unique embedding-dominated characteristics and stringent service-level objectives.
However, prior serving systems for EMR models struggle to achieve high performance at low cost.
We propose FlexEMR, a fast and efficient system that disaggregates embedding
table lookups from NN computation.
FlexEMR uses a set of locality-enhanced optimizations atop a multi-threaded RDMA engine to ensure performance and resource efficiency.
We envision FlexEMR to
improve user experience of Internet-scale recommendation services
while driving down costs for their providers.
Furthermore, we believe this paradigm can benefit other ML workloads, including large language models (LLM) <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib27" title="">2023</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib53" title="">2022</a>)</cite>, multimodal models <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib17" title="">2024</a>)</cite>, and mixture-of-expert (MoE) <cite class="ltx_cite ltx_citemacro_citep">(Rajbhandari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib36" title="">2022</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12794v1#bib.bib52" title="">2024</a>)</cite>, which we will also investigate in future works.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib2.4.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wei Bai, Shanim Sainul Abdeen, Ankit Agrawal, Krishan Kumar Attre, Paramvir Bahl, Ameya Bhagat, Gowri Bhaskara, Tanya Brokhman, Lei Cao, Ahmad Cheema, et al<span class="ltx_text" id="bib.bib2.5.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Empowering azure storage with <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib2.1.m1.1"><semantics id="bib.bib2.1.m1.1a"><mo id="bib.bib2.1.m1.1.1" stretchy="false" xref="bib.bib2.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.1.m1.1b"><ci id="bib.bib2.1.m1.1.1.cmml" xref="bib.bib2.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib2.1.m1.1d">{</annotation></semantics></math>RDMA<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib2.2.m2.1"><semantics id="bib.bib2.2.m2.1a"><mo id="bib.bib2.2.m2.1.1" stretchy="false" xref="bib.bib2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.2.m2.1b"><ci id="bib.bib2.2.m2.1.1.cmml" xref="bib.bib2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib2.2.m2.1d">}</annotation></semantics></math>. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.6.1">NSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corporation (2024)</span>
<span class="ltx_bibblock">
NVIDIA Corporation. 2024.

</span>
<span class="ltx_bibblock">Resource Domain.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.nvidia.com/networking/display/rdmacore50/resource+domain" title="">https://docs.nvidia.com/networking/display/rdmacore50/resource+domain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aditya Desai, Li Chou, and Anshumali Shrivastava. 2022.

</span>
<span class="ltx_bibblock">Random Offset Block Embedding (ROBE) for compressed embedding tables in deep learning recommendation systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">MLSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai and Shrivastava (2022)</span>
<span class="ltx_bibblock">
Aditya Desai and Anshumali Shrivastava. 2022.

</span>
<span class="ltx_bibblock">The trade-offs of model size in large recommendation models: 100GB to 10MB Criteo-tb DLRM model. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dragojević et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Aleksandar Dragojević, Dushyanth Narayanan, Miguel Castro, and Orion Hodson. 2014.

</span>
<span class="ltx_bibblock">FaRM: Fast Remote Memory. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">NSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ewais and Chow (2023)</span>
<span class="ltx_bibblock">
Mohammad Ewais and Paul Chow. 2023.

</span>
<span class="ltx_bibblock">Disaggregated Memory in the Datacenter: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE Access</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib8.4.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yixiao Gao, Qiang Li, Lingbo Tang, Yongqing Xi, Pengcheng Zhang, Wenwen Peng, Bo Li, Yaohui Wu, Shaozong Liu, Lei Yan, et al<span class="ltx_text" id="bib.bib8.5.1">.</span> 2021.

</span>
<span class="ltx_bibblock">When cloud storage meets <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib8.1.m1.1"><semantics id="bib.bib8.1.m1.1a"><mo id="bib.bib8.1.m1.1.1" stretchy="false" xref="bib.bib8.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib8.1.m1.1b"><ci id="bib.bib8.1.m1.1.1.cmml" xref="bib.bib8.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib8.1.m1.1d">{</annotation></semantics></math>RDMA<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib8.2.m2.1"><semantics id="bib.bib8.2.m2.1a"><mo id="bib.bib8.2.m2.1.1" stretchy="false" xref="bib.bib8.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib8.2.m2.1b"><ci id="bib.bib8.2.m2.1.1.cmml" xref="bib.bib8.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib8.2.m2.1d">}</annotation></semantics></math>. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.6.1">NSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginart et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
A.A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021.

</span>
<span class="ltx_bibblock">Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">ISIT</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye, and Marina Lipshteyn. 2016.

</span>
<span class="ltx_bibblock">RDMA over commodity ethernet at scale. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">SIGCOMM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole-Jean Wu. 2020a.

</span>
<span class="ltx_bibblock">DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole-Jean Wu. 2020b.

</span>
<span class="ltx_bibblock">DeepRecSys: A System for Optimizing End-To-End At-Scale Neural Recommendation Inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020c)</span>
<span class="ltx_bibblock">
Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al<span class="ltx_text" id="bib.bib13.3.1">.</span> 2020c.

</span>
<span class="ltx_bibblock">The architectural implications of facebook’s dnn-based personalized recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">HPCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bowen He, Xiao Zheng, Yuan Chen, Weinan Li, Yajin Zhou, Xin Long, Pengcheng Zhang, Xiaowei Lu, Linquan Jiang, Qiang Liu, Dennis Cai, and Xiantao Zhang. 2023.

</span>
<span class="ltx_bibblock">DxPU: Large-scale Disaggregated GPU Pools in the Datacenter.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ACM Trans. Archit. Code Optim.</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsia et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Samuel Hsia, Udit Gupta, Bilge Acun, Newsha Ardalani, Pan Zhong, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2023.

</span>
<span class="ltx_bibblock">MP-Rec: Hardware-Software Co-design to Enable Multi-path Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Shuihai Hu, Wei Bai, Gaoxiong Zeng, Zilong Wang, Baochen Qiao, Kai Chen, Kun Tan, and Yi Wang. 2020.

</span>
<span class="ltx_bibblock">Aeolus: A Building Block for Proactive Transport in Datacenters. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">SIGCOMM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jun Huang, Zhen Zhang, Shuai Zheng, Feng Qin, and Yida Wang. 2024.

</span>
<span class="ltx_bibblock">DISTMM: Accelerating Distributed Multimodal Model Training. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">NSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ibrahim et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mohamed Assem Ibrahim, Onur Kayiran, and Shaizeen Aga. 2021.

</span>
<span class="ltx_bibblock">Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">MEMSYS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Rishabh Jain, Scott Cheng, Vishwas Kalagi, Vrushabh Sanghavi, Samvit Kaul, Meena Arunachalam, Kiwan Maeng, Adwait Jog, Anand Sivasubramaniam, Mahmut Taylan Kandemir, et al<span class="ltx_text" id="bib.bib19.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Optimizing cpu performance for recommendation systems at-scale. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.4.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Rishabh Jain, Scott Cheng, Vishwas Kalagi, Vrushabh Sanghavi, Samvit Kaul, Meena Arunachalam, Kiwan Maeng, Adwait Jog, Anand Sivasubramaniam, Mahmut Taylan Kandemir, and Chita R. Das. 2023a.

</span>
<span class="ltx_bibblock">Optimizing CPU Performance for Recommendation Systems At-Scale. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wenqi Jiang, Zhenhao He, Shuai Zhang, Kai Zeng, Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, Ce Zhang, and Gustavo Alonso. 2021.

</span>
<span class="ltx_bibblock">FleetRec: Large-Scale Recommendation Inference on Hybrid GPU-FPGA Clusters. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kal et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongju Kal, Seokmin Lee, Gun Ko, and Won Woo Ro. 2021.

</span>
<span class="ltx_bibblock">SPACE: Locality-Aware Processing in Heterogeneous Memory for Personalized Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalia et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Anuj Kalia, Michael Kaminsky, and David Andersen. 2019.

</span>
<span class="ltx_bibblock">Datacenter RPCs can be General and Fast. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">NSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S Lee, et al<span class="ltx_text" id="bib.bib24.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Recnmp: Accelerating personalized recommendation with near-memory processing. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Liu Ke, Xuan Zhang, Benjamin Lee, G. Edward Suh, and Hsien-Hsin S. Lee. 2022.

</span>
<span class="ltx_bibblock">DisaggRec: Architecting Disaggregated Systems for Large-Scale Personalized Recommendation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurniawan et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniar H. Kurniawan, Ruipu Wang, Kahfi S. Zulkifli, Fandi A. Wiranata, John Bent, Ymir Vigfusson, and Haryadi S. Gunawi. 2023.

</span>
<span class="ltx_bibblock">EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient Memory Management for Large Language Model Serving with PagedAttention. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">SOSP</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Youngeun Kwon, Yunjae Lee, and Minsoo Rhu. 2019.

</span>
<span class="ltx_bibblock">Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">MICRO</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fan Lai, Wei Zhang, Rui Liu, William Tsai, Xiaohan Wei, Yuxi Hu, Sabin Devkota, Jianyu Huang, Jongsoo Park, Xing Liu, Zeliang Chen, Ellie Wen, Paul Rivera, Jie You, Chun cheng Jason Chen, and Mosharaf Chowdhury. 2023.

</span>
<span class="ltx_bibblock">AdaEmbed: Adaptive Embedding for Large-Scale Recommendation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">OSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yejin Lee, Seong Hoon Seo, Hyunji Choi, Hyoung Uk Sul, Soosung Kim, Jae W. Lee, and Tae Jun Ham. 2021.

</span>
<span class="ltx_bibblock">MERCI: efficient embedding reduction on commodity hardware via sub-query memoization. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lui et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Michael Lui, Yavuz Yetim, Özgür Özkan, Zhuoran Zhao, Shin-Yeh Tsai, Carole-Jean Wu, and Mark Hempstead. 2021.

</span>
<span class="ltx_bibblock">Understanding capacity-driven scale-out neural recommendation inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">ISPASS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mellanox (2017)</span>
<span class="ltx_bibblock">
Mellanox. 2017.

</span>
<span class="ltx_bibblock">Verbs: Introduce resource domain.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://patchwork.kernel.org/project/linux-rdma/patch/1505648922-21346-1-git-send-email-yishaih@mellanox.com/" title="">https://patchwork.kernel.org/project/linux-rdma/patch/1505648922-21346-1-git-send-email-yishaih@mellanox.com/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mudigere et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, et al<span class="ltx_text" id="bib.bib33.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Software-hardware co-design for fast and scalable training of deep learning recommendation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">ISCA</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naumov et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al<span class="ltx_text" id="bib.bib34.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for personalization and recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.4.1">arXiv preprint arXiv:1906.00091</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zaifeng Pan, Zhen Zheng, Feng Zhang, Ruofan Wu, Hao Liang, Dalin Wang, Xiafei Qiu, Junjie Bai, Wei Lin, and Xiaoyong Du. 2024.

</span>
<span class="ltx_bibblock">RECom: A Compiler Approach to Accelerating Recommendation Model Inference with Massive Embedding Columns. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022.

</span>
<span class="ltx_bibblock">DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">ICML</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois, William Chou, Ramesh Chukka, Cody Coleman, Sam Davis, Pan Deng, Greg Diamos, Jared Duke, Dave Fick, J. Scott Gardner, Itay Hubara, Sachin Idgunji, Thomas B. Jablin, Jeff Jiao, Tom St. John, Pankaj Kanwar, David Lee, Jeffery Liao, Anton Lokhmotov, Francisco Massa, Peng Meng, Paulius Micikevicius,
Colin Osborne, Gennady Pekhimenko, Arun Tejusve Raghunath Rajan, Dilip Sequeira, Ashish Sirasao, Fei Sun, Hanlin Tang, Michael Thomson, Frank Wei, Ephrem Wu, Lingjie Xu, Koichi Yamada, Bing Yu, George Yuan, Aaron Zhong, Peizhao Zhang, and Yuchen Zhou. 2020.

</span>
<span class="ltx_bibblock">MLPerf Inference Benchmark.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Research (2023)</span>
<span class="ltx_bibblock">
Meta Research. 2023.

</span>
<span class="ltx_bibblock">GitHub - Set of datasets for the deep learning recommendation model (DLRM).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/dlrm_datasets" title="">https://github.com/facebookresearch/dlrm_datasets</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sethi et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. 2022.

</span>
<span class="ltx_bibblock">RecShard: statistical feature-based memory optimization for industry-scale neural recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sethi et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Geet Sethi, Pallab Bhattacharya, Dhruv Choudhary, Carole-Jean Wu, and Christos Kozyrakis. 2023.

</span>
<span class="ltx_bibblock">FlexShard: Flexible Sharding for Industry-Scale Sequence Recommendation Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2301.02959</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan et al<span class="ltx_text" id="bib.bib41.4.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. 2018.

</span>
<span class="ltx_bibblock">Legoos: A disseminated, distributed <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib41.1.m1.1"><semantics id="bib.bib41.1.m1.1a"><mo id="bib.bib41.1.m1.1.1" stretchy="false" xref="bib.bib41.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.1.m1.1b"><ci id="bib.bib41.1.m1.1.1.cmml" xref="bib.bib41.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.1.m1.1d">{</annotation></semantics></math>OS<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib41.2.m2.1"><semantics id="bib.bib41.2.m2.1a"><mo id="bib.bib41.2.m2.1.1" stretchy="false" xref="bib.bib41.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.2.m2.1b"><ci id="bib.bib41.2.m2.1.1.cmml" xref="bib.bib41.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.2.m2.1d">}</annotation></semantics></math> for hardware resource disaggregation. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.5.1">OSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020a.

</span>
<span class="ltx_bibblock">Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang. 2020b.

</span>
<span class="ltx_bibblock">Compositional embeddings using complementary partitions for memory-efficient recommendation systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Technologies (2016)</span>
<span class="ltx_bibblock">
Mellanox Technologies. 2016.

</span>
<span class="ltx_bibblock">Mellanox Adapters Programmer’s Reference Manual (PRM).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf" title="">https://network.nvidia.com/files/doc-2020/ethernet-adapters-programming-manual.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhe Wang, Teng Ma, Linghe Kong, Zhenzao Wen, Jingxuan Li, Zhuo Song, Yang Lu, Guihai Chen, and Wei Cao. 2022.

</span>
<span class="ltx_bibblock">Zero Overhead Monitoring for Cloud-native Infrastructure using RDMA. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">USENIX ATC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yingcan Wei, Matthias Langer, Fan Yu, Minseok Lee, Jie Liu, Ji Shi, and Zehuan Wang. 2022.

</span>
<span class="ltx_bibblock">A GPU-specialized inference parameter server for large-scale deep recommendation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">RecSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkening et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Wilkening, Udit Gupta, Samuel Hsia, Caroline Trippel, Carole-Jean Wu, David Brooks, and Gu-Yeon Wei. 2021.

</span>
<span class="ltx_bibblock">RecSSD: near data processing for solid state drive based recommendation inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Minhui Xie, Youyou Lu, Jiazhen Lin, Qing Wang, Jian Gao, Kai Ren, and Jiwu Shu. 2022.

</span>
<span class="ltx_bibblock">Fleche: an efficient GPU embedding cache for personalized recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">EuroSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jilong Xue, Youshan Miao, Cheng Chen, Ming Wu, Lintao Zhang, and Lidong Zhou. 2019.

</span>
<span class="ltx_bibblock">Fast distributed deep learning over rdma. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">EuroSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haojie Ye, Sanketh Vedula, Yuhan Chen, Yichen Yang, Alex Bronstein, Ronald Dreslinski, Trevor Mudge, and Nishil Talati. 2023.

</span>
<span class="ltx_bibblock">GRACE: A Scalable Graph-Based Approach to Accelerating Recommendation Model Inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">ASPLOS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chunxing Yin, Bilge Acun, Carole-Jean Wu, and Xing Liu. 2021.

</span>
<span class="ltx_bibblock">TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">MLSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dianhai Yu, Liang Shen, Hongxiang Hao, Weibao Gong, Huachao Wu, Jiang Bian, Lirong Dai, and Haoyi Xiong. 2024.

</span>
<span class="ltx_bibblock">MoESys: A Distributed and Efficient Mixture-of-Experts Training and Inference System for Internet Services.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">IEEE Transactions on Services Computing</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022.

</span>
<span class="ltx_bibblock">Orca: A Distributed Serving System for Transformer-Based Generative Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">OSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib54.6.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chaoliang Zeng, Layong Luo, Qingsong Ning, Yaodong Han, Yuhang Jiang, Ding Tang, Zilong Wang, Kai Chen, and Chuanxiong Guo. 2022.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib54.1.m1.1"><semantics id="bib.bib54.1.m1.1a"><mo id="bib.bib54.1.m1.1.1" stretchy="false" xref="bib.bib54.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib54.1.m1.1b"><ci id="bib.bib54.1.m1.1.1.cmml" xref="bib.bib54.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib54.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib54.1.m1.1d">{</annotation></semantics></math>FAERY<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib54.2.m2.1"><semantics id="bib.bib54.2.m2.1a"><mo id="bib.bib54.2.m2.1.1" stretchy="false" xref="bib.bib54.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib54.2.m2.1b"><ci id="bib.bib54.2.m2.1.1.cmml" xref="bib.bib54.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib54.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib54.2.m2.1d">}</annotation></semantics></math>: An <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib54.3.m3.1"><semantics id="bib.bib54.3.m3.1a"><mo id="bib.bib54.3.m3.1.1" stretchy="false" xref="bib.bib54.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib54.3.m3.1b"><ci id="bib.bib54.3.m3.1.1.cmml" xref="bib.bib54.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib54.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib54.3.m3.1d">{</annotation></semantics></math>FPGA-accelerated<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib54.4.m4.1"><semantics id="bib.bib54.4.m4.1a"><mo id="bib.bib54.4.m4.1.1" stretchy="false" xref="bib.bib54.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib54.4.m4.1b"><ci id="bib.bib54.4.m4.1.1.cmml" xref="bib.bib54.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib54.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib54.4.m4.1d">}</annotation></semantics></math> Embedding-based Retrieval System. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.7.1">OSDI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Daochen Zha, Louis Feng, Bhargav Bhushanam, Dhruv Choudhary, Jade Nie, Yuandong Tian, Jay Chae, Yinbin Ma, Arun Kejariwal, and Xia Hu. 2022a.

</span>
<span class="ltx_bibblock">Autoshard: Automated embedding table sharding for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Daochen Zha, Louis Feng, Qiaoyu Tan, Zirui Liu, Kwei-Herng Lai, Bhargav Bhushanam, Yuandong Tian, Arun Kejariwal, and Xia Hu. 2022b.

</span>
<span class="ltx_bibblock">Dreamshard: Generalizable embedding table placement for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">NeurIPS</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022c)</span>
<span class="ltx_bibblock">
Daochen Zha, Louis Feng, Qiaoyu Tan, Zirui Liu, Kwei-Herng Lai, Bhargav Bhushanam, Yuandong Tian, Arun Kejariwal, and Xia Hu. 2022c.

</span>
<span class="ltx_bibblock">DreamShard: Generalizable Embedding Table Placement for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qizhen Zhang, Xinyi Chen, Sidharth Sankhe, Zhilei Zheng, Ke Zhong, Sebastian Angel, Ang Chen, Vincent Liu, and Boon Thau Loo. 2022.

</span>
<span class="ltx_bibblock">Optimizing data-intensive systems in disaggregated data centers with teleport. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">SIGMOD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 28 01:53:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
