<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Medical Image Segmentation with SAM-generated Annotations</title>
<!--Generated on Mon Sep 30 12:26:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Foundation Model Segment Anything Model Medical Image Segmentation Data Annotation" lang="en" name="keywords"/>
<base href="/html/2409.20253v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S1" title="In Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S2" title="In Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3" title="In Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS1" title="In 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Simulated Semantic Annotations Using SAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS2" title="In 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Weakly-supervised Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS3" title="In 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4" title="In Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.SS1" title="In 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.SS2" title="In 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S5" title="In Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Medical Image Segmentation with SAM-generated Annotations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Iira Häkkinen<span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0009-0009-2225-6959
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Iaroslav Melekhov<span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0003-3819-5280
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Erik Englesson<span class="ltx_ERROR undefined" id="id7.1.id1">\orcidlink</span>0000-0003-4535-2520
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Hossein Azizpour<span class="ltx_ERROR undefined" id="id8.1.id1">\orcidlink</span>0000-0001-5211-6388
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Juho Kannala<span class="ltx_ERROR undefined" id="id9.5.id1">\orcidlink</span>0000-0001-5088-4041
<br class="ltx_break"/><sup class="ltx_sup" id="id10.6.id2">1</sup>University of Helsinki   <sup class="ltx_sup" id="id11.7.id3">2</sup>Aalto University 
<br class="ltx_break"/><sup class="ltx_sup" id="id12.8.id4">3</sup>KTH Royal Institute of Technology <sup class="ltx_sup" id="id13.9.id5">4</sup>University of Oulu
</span><span class="ltx_author_notes">2244</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">The field of medical image segmentation is hindered by the scarcity of large, publicly available annotated datasets. Not all datasets are made public for privacy reasons, and creating annotations for a large dataset is time-consuming and expensive, as it requires specialized expertise to accurately identify regions of interest (ROIs) within the images. To address these challenges, we evaluate the performance of the Segment Anything Model (SAM) as an annotation tool for medical data by using it to produce so-called “pseudo labels” on the Medical Segmentation Decathlon (MSD) computed tomography (CT) tasks. The pseudo labels are then used in place of ground truth labels to train a UNet model in a weakly-supervised manner.
We experiment with different prompt types on SAM and find that the bounding box prompt is a simple yet effective method for generating pseudo labels. This method allows us to develop a weakly-supervised model that performs comparably to a fully supervised model.

</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Foundation Model Segment Anything Model Medical Image Segmentation Data Annotation
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The field of medical image segmentation (MIS) faces challenges due to the scarcity of large, publicly available annotated datasets. The process of annotating segmentation masks is both time-consuming and expensive, typically requiring the expertise of medical professionals to accurately identify regions of interest (ROIs) within images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib25" title="">25</a>]</cite>. This challenge is one reason for the limited amount of publicly available medical datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Foundational models have been proposed as an interesting way to deal with this challenge. Foundation models are large general models trained on massive, diverse datasets, and have been proven to be effective across multiple domains  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib7" title="">7</a>]</cite>. As the field of MIS still relies on task-specific supervised models  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib25" title="">25</a>]</cite>, foundation models could be used as data annotation tools to accelerate the labelling process of ground truth (GT) data  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite>. Depending on the quality of the zero-shot predictions of the computer vision foundation model, the segmentations could either be accepted as is, or fine-tuned manually by experts.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In particular, one recent foundation model for semantic segmentation is the Segment Anything Model (SAM) developed by Meta AI  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite>. SAM is a promptable encoder-decoder model trained on 11 million images along with over 1 billion masks, on mostly natural images. Along with the input image, a prompt can be given to SAM to indicate the region of interest (ROI). The prompt can be <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">e.g</em>.<span class="ltx_text" id="S1.p3.1.2"></span>, a point, or a bounding box around the ROI. SAM also provides a mode called everything mode, where the model is prompted to segment everything from the image with an even grid of point prompts over the input image. With the image and the prompt, the SAM model predicts one or several segmentation masks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using SAM as a data annotation tool has been studied in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite>. These studies have compared the predictions with different prompting methods for SAM against ground truth labels. However, to the best of our knowledge, the final performance of models trained on SAM-generated labels has not been studied. This is crucial, as the distribution of the labels produced by SAM could be different from real labels, which can impact the model’s training process and final performance. As the SAM-generated labels inevitably incorporate new noise into the training data, it is essential to evaluate the final model performance to assess the complete impact of the noisy labels.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Without taking doctors away from patients, we propose to get as close to a real setting as possible by i) simulating using SAM as an annotation tool for medical images, and ii) evaluating the generalization ability of neural networks trained on these SAM-generated labels. We experiment with different prompt methods and train two sets of UNet models: fully supervised UNet trained with ground truth labels, and UNet trained with the SAM-generated pseudo labels using the simple and effective box prompt. The experiments are conducted on six abdominal medical segmentation tasks. The results show that the SAM model has great potential as a data annotation tool for medical images, and encourages further experimentation.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Several studies have evaluated the zero-shot performance of SAM in the medical domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>. Researchers agree that SAM’s performance in medical image segmentation tasks varies significantly depending on the specific task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>. SAM performs well when ROI is large and distinct from the background but struggles with smaller, more complex structures. These challenges become more pronounced with ambiguous prompts, as the chosen prompting strategy affects SAM’s final segmentation performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>. Additionally, SAM’s performance is influenced by image qualities such as imaging modality, image dimensions, and contrast <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Huang <em class="ltx_emph ltx_font_italic" id="S2.p2.1.1">et al</em>.<span class="ltx_text" id="S2.p2.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>]</cite>, Ma <em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">et al</em>.<span class="ltx_text" id="S2.p2.1.4"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>]</cite>, Wu <em class="ltx_emph ltx_font_italic" id="S2.p2.1.5">et al</em>.<span class="ltx_text" id="S2.p2.1.6"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib22" title="">22</a>]</cite>, Zhang <em class="ltx_emph ltx_font_italic" id="S2.p2.1.7">et al</em>.<span class="ltx_text" id="S2.p2.1.8"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>, and Zhang and Liu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib24" title="">24</a>]</cite> all report that for SAM to achieve state-of-the-art performance, the model needs to be fine-tuned for medical data. One significant factor contributing to the performance gap between SAM and other state-of-the-art medical segmentation approaches is the nature of the images in the SA-1B dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite> used to train SAM. The SA-1B dataset consists mostly of natural images, which typically have distinct edges between objects and backgrounds, and are quite different from medical images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>. Moreover, since SAM rarely meets state-of-the-art standards in its default mode (the Everything Mode) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>, it is clear that educated human input in the form of prompts is necessary for SAM to perform well in the medical domain.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Recent methods have suggested using SAM as a data annotation tool for medical data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite>. Both <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> propose that SAM could accelerate the process of creating ground truth labels if data annotators used SAM as a tool. Additionally, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>]</cite> report that SAM could help human annotators by reducing the time needed for a single annotation while maintaining high quality. The authors conducted a study with three doctors who had to annotate 620 masks consisting of 55 different objects covering 9 medical imaging modalities. The doctors performed annotations in two ways: a) from scratch and b) by adjusting labels predicted by SAM. The study found that using SAM improved annotation speed by approximately 25% compared to fully manual annotation. Furthermore, the SAM approach achieved higher annotation quality, as measured by the Human Correction Efforts (HCE) index. The HCE index estimates the human effort required to fix predictions to reach a required accuracy level. While these studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> have thoroughly evaluated different prompting strategies for SAM to produce medical labels, to the best of our knowledge, the final performance of models trained on SAM-generated labels has not been experimented on. We believe that assessing such models is crucial, especially in the medical domain, as it would provide a deep understanding of their generalization performance and it could also facilitate further research on self-supervised methods for semantic segmentation.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center ltx_align_center" id="S2.F1.1.1"><span class="ltx_text" id="S2.F1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="375" id="S2.F1.1.1.1.g1" src="x1.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.8.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F1.9.2" style="font-size:90%;">Pipeline<span class="ltx_text ltx_font_medium" id="S2.F1.9.2.1">. A set of 2D CT scans is propagated through the pre-trained SAM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite> to obtain the corresponding pseudo segmentation masks (<em class="ltx_emph ltx_font_italic" id="S2.F1.9.2.1.1">cf</em>.<span class="ltx_text" id="S2.F1.9.2.1.2"></span> Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS1" title="3.1 Simulated Semantic Annotations Using SAM ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Two independent UNet models are then trained (<em class="ltx_emph ltx_font_italic" id="S2.F1.9.2.1.3">cf</em>.<span class="ltx_text" id="S2.F1.9.2.1.4"></span> Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS2" title="3.2 Weakly-supervised Semantic Segmentation ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.2</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS3" title="3.3 Optimization ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.3</span></a>) using ground truth and pseudo labels to perform semantic segmentation.</span></span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our objective is to assess the performance of the SAM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite> as an annotation tool for medical images and perform semantic segmentation in a weakly-supervised manner using masks produced by SAM. The full procedure is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">1</span></a>. Next, we go into details of the pseudo labels generation process (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS1" title="3.1 Simulated Semantic Annotations Using SAM ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.1</span></a>), the weakly-supervised semantic segmentation (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS2" title="3.2 Weakly-supervised Semantic Segmentation ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and the objective function (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.SS3" title="3.3 Optimization ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Simulated Semantic Annotations Using SAM</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">In this work, the SAM model is employed to generate semantic annotations by simulating expert input. Specifically, we consider the following prompting methods based on available ground-truth masks: a) <math alttext="\text{Point}_{\text{CM}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mtext id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2a.cmml">Point</mtext><mtext id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3a.cmml">CM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2a.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><mtext id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">Point</mtext></ci><ci id="S3.SS1.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><mtext id="S3.SS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.1.m1.1.1.3">CM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\text{Point}_{\text{CM}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">Point start_POSTSUBSCRIPT CM end_POSTSUBSCRIPT</annotation></semantics></math>: a single positive point that represents the approximate center (center of mass, CM) of the region of interest (ROI); b) <math alttext="\text{Point}_{\text{interior}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mtext id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2a.cmml">Point</mtext><mtext id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3a.cmml">interior</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2a.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><mtext id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">Point</mtext></ci><ci id="S3.SS1.p1.2.m2.1.1.3a.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><mtext id="S3.SS1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.2.m2.1.1.3">interior</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\text{Point}_{\text{interior}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">Point start_POSTSUBSCRIPT interior end_POSTSUBSCRIPT</annotation></semantics></math>: a single positive point that is furthest from the ground-truth edges of ROI; c) Box: a bounding box prompt is placed to <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.6.1">tightly</span> enclose ROI; d) <math alttext="\text{Box}_{\text{noise}}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mtext id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2a.cmml">Box</mtext><mtext id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3a.cmml">noise</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2a.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><mtext id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">Box</mtext></ci><ci id="S3.SS1.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><mtext id="S3.SS1.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.3.m3.1.1.3">noise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\text{Box}_{\text{noise}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">Box start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT</annotation></semantics></math>: a bounding box prompt is placed to <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.6.2">loosely</span> enclose ROI; e) <math alttext="\text{Box}_{\text{+PP/NP}}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mtext id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2a.cmml">Box</mtext><mtext id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3a.cmml">+PP/NP</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2a.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><mtext id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">Box</mtext></ci><ci id="S3.SS1.p1.4.m4.1.1.3a.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><mtext id="S3.SS1.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.4.m4.1.1.3">+PP/NP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\text{Box}_{\text{+PP/NP}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">Box start_POSTSUBSCRIPT +PP/NP end_POSTSUBSCRIPT</annotation></semantics></math>: a Box prompt followed by an interactive positive or negative point using <math alttext="\text{Point}_{\text{interior}}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mtext id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2a.cmml">Point</mtext><mtext id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3a.cmml">interior</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2a.cmml" xref="S3.SS1.p1.5.m5.1.1.2"><mtext id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">Point</mtext></ci><ci id="S3.SS1.p1.5.m5.1.1.3a.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><mtext id="S3.SS1.p1.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.5.m5.1.1.3">interior</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\text{Point}_{\text{interior}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">Point start_POSTSUBSCRIPT interior end_POSTSUBSCRIPT</annotation></semantics></math>. This last prompting method simulates using SAM as an iterative annotation tool by using the resulting segmentation mask from the Box prompt to choose where to place and what type of point (positive/negative) to use. Suppose the largest component of false positives (in SAM prediction but not in GT) is larger than the largest component of false negatives (in GT but not in SAM prediction). In that case, a positive point is used, otherwise a negative, and the point is placed in the largest component using <math alttext="\text{Point}_{\text{interior}}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mtext id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2a.cmml">Point</mtext><mtext id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3a.cmml">interior</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2a.cmml" xref="S3.SS1.p1.6.m6.1.1.2"><mtext id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">Point</mtext></ci><ci id="S3.SS1.p1.6.m6.1.1.3a.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><mtext id="S3.SS1.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.6.m6.1.1.3">interior</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\text{Point}_{\text{interior}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">Point start_POSTSUBSCRIPT interior end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We evaluate SAM with the aforementioned input prompts on the training split of the target dataset and report results in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4.2</span></a> (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">cf</em>.<span class="ltx_text" id="S3.SS1.p2.1.2"></span> Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T3" title="Table 3 ‣ 4.1 Results ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3</span></a>). Although the <math alttext="\text{Box}_{\text{+PP/NP}}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mtext id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2a.cmml">Box</mtext><mtext id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3a.cmml">+PP/NP</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2a.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><mtext id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">Box</mtext></ci><ci id="S3.SS1.p2.1.m1.1.1.3a.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><mtext id="S3.SS1.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.1.m1.1.1.3">+PP/NP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\text{Box}_{\text{+PP/NP}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">Box start_POSTSUBSCRIPT +PP/NP end_POSTSUBSCRIPT</annotation></semantics></math> prompt leads to superior semantic segmentation performance, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> and adopt the simple and effective Box prompt to generate pseudo semantic masks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Weakly-supervised Semantic Segmentation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In our experiments, we first create pseudo labels on the training images using SAM with box prompt. Then, for each experimented segmentation task, two separate neural networks are trained on the training split of the dataset: a) a fully supervised network, and b) a network that uses the pseudo labels in place of ground truth labels during training. The training pipelines of the networks are exactly identical, except for the ground truth mask data used. We evaluate the networks on the testing split of the target dataset, and report results in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.SS1" title="4.1 Results ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4.1</span></a></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Models.</span>  We use the UNet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib18" title="">18</a>]</cite> for our neural network in all our experiments due to its simplicity and widespread use in the field of medical segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib25" title="">25</a>]</cite>.
The number of feature channels in our approach are 64, 128, 256, and 512, following the original UNet structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib18" title="">18</a>]</cite>. Each convolution operation has a kernel size of 3, a stride of 1, and padding of 1. We incorporate batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib8" title="">8</a>]</cite> after each convolution to normalize data before the next step. The max-pooling layers and transposed convolutions are used for feature map downsampling and upsampling. The number of output layers of the network is <math alttext="1" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">1</annotation></semantics></math> to perform binary segmentation. The pseudo semantic masks are generated using the pre-trained SAM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite> with ViT-H backbone. We also experiment with MedSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>]</cite>, a foundation model fine-tuned on the medical image domain, and report results in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4" title="4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Optimization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">One of the criteria to optimize the network is to address class imbalance, a common issue in medical image segmentation where the region of interest may occupy a small portion of the image compared to the background. Specifically, we utilize the Dice loss which directly maximizes the overlap between the predicted segmentation and the ground truth and can be defined as follows:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{dice}}=1-\frac{2\sum_{i}\hat{p}_{i}p_{i}}{\sum_{i}\hat{p}_{%
i}^{2}+\sum_{i}p_{i}^{2}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3a.cmml">dice</mtext></msub><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">1</mn><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">−</mo><mfrac id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mrow id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mn id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">2</mn><mo id="S3.E1.m1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.3.3.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml"><msub id="S3.E1.m1.1.1.3.3.2.3.1" xref="S3.E1.m1.1.1.3.3.2.3.1.cmml"><mo id="S3.E1.m1.1.1.3.3.2.3.1.2" xref="S3.E1.m1.1.1.3.3.2.3.1.2.cmml">∑</mo><mi id="S3.E1.m1.1.1.3.3.2.3.1.3" xref="S3.E1.m1.1.1.3.3.2.3.1.3.cmml">i</mi></msub><mrow id="S3.E1.m1.1.1.3.3.2.3.2" xref="S3.E1.m1.1.1.3.3.2.3.2.cmml"><msub id="S3.E1.m1.1.1.3.3.2.3.2.2" xref="S3.E1.m1.1.1.3.3.2.3.2.2.cmml"><mover accent="true" id="S3.E1.m1.1.1.3.3.2.3.2.2.2" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.3.2.2.2.2" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2.2.cmml">p</mi><mo id="S3.E1.m1.1.1.3.3.2.3.2.2.2.1" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.1.1.3.3.2.3.2.2.3" xref="S3.E1.m1.1.1.3.3.2.3.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.3.3.2.3.2.1" xref="S3.E1.m1.1.1.3.3.2.3.2.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.3.3.2.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2.3.2.3.2" xref="S3.E1.m1.1.1.3.3.2.3.2.3.2.cmml">p</mi><mi id="S3.E1.m1.1.1.3.3.2.3.2.3.3" xref="S3.E1.m1.1.1.3.3.2.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mrow id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mrow id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml"><msub id="S3.E1.m1.1.1.3.3.3.2.1" xref="S3.E1.m1.1.1.3.3.3.2.1.cmml"><mo id="S3.E1.m1.1.1.3.3.3.2.1.2" xref="S3.E1.m1.1.1.3.3.3.2.1.2.cmml">∑</mo><mi id="S3.E1.m1.1.1.3.3.3.2.1.3" xref="S3.E1.m1.1.1.3.3.3.2.1.3.cmml">i</mi></msub><msubsup id="S3.E1.m1.1.1.3.3.3.2.2" xref="S3.E1.m1.1.1.3.3.3.2.2.cmml"><mover accent="true" id="S3.E1.m1.1.1.3.3.3.2.2.2.2" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2.2.2.2.2" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2.2.cmml">p</mi><mo id="S3.E1.m1.1.1.3.3.3.2.2.2.2.1" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.1.1.3.3.3.2.2.2.3" xref="S3.E1.m1.1.1.3.3.3.2.2.2.3.cmml">i</mi><mn id="S3.E1.m1.1.1.3.3.3.2.2.3" xref="S3.E1.m1.1.1.3.3.3.2.2.3.cmml">2</mn></msubsup></mrow><mo id="S3.E1.m1.1.1.3.3.3.1" rspace="0.055em" xref="S3.E1.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml"><msub id="S3.E1.m1.1.1.3.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.3.1.cmml"><mo id="S3.E1.m1.1.1.3.3.3.3.1.2" xref="S3.E1.m1.1.1.3.3.3.3.1.2.cmml">∑</mo><mi id="S3.E1.m1.1.1.3.3.3.3.1.3" xref="S3.E1.m1.1.1.3.3.3.3.1.3.cmml">i</mi></msub><msubsup id="S3.E1.m1.1.1.3.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.3.3.2.2.2" xref="S3.E1.m1.1.1.3.3.3.3.2.2.2.cmml">p</mi><mi id="S3.E1.m1.1.1.3.3.3.3.2.2.3" xref="S3.E1.m1.1.1.3.3.3.3.2.2.3.cmml">i</mi><mn id="S3.E1.m1.1.1.3.3.3.3.2.3" xref="S3.E1.m1.1.1.3.3.3.3.2.3.cmml">2</mn></msubsup></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">ℒ</ci><ci id="S3.E1.m1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.2.3"><mtext id="S3.E1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.2.3">dice</mtext></ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><cn id="S3.E1.m1.1.1.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.2">1</cn><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><divide id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3"></divide><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><times id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.1"></times><cn id="S3.E1.m1.1.1.3.3.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.2.2">2</cn><apply id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3"><apply id="S3.E1.m1.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.3.1.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.3.2.3.1.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1.2"></sum><ci id="S3.E1.m1.1.1.3.3.2.3.1.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2"><times id="S3.E1.m1.1.1.3.3.2.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.1"></times><apply id="S3.E1.m1.1.1.3.3.2.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2">subscript</csymbol><apply id="S3.E1.m1.1.1.3.3.2.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2"><ci id="S3.E1.m1.1.1.3.3.2.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2.1">^</ci><ci id="S3.E1.m1.1.1.3.3.2.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2.2.2">𝑝</ci></apply><ci id="S3.E1.m1.1.1.3.3.2.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.3.3.2.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.3.2">𝑝</ci><ci id="S3.E1.m1.1.1.3.3.2.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3.2.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><plus id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.1"></plus><apply id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2"><apply id="S3.E1.m1.1.1.3.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.2.1.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.3.3.2.1.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.1.2"></sum><ci id="S3.E1.m1.1.1.3.3.3.2.1.3.cmml" xref="S3.E1.m1.1.1.3.3.3.2.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.3.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2">superscript</csymbol><apply id="S3.E1.m1.1.1.3.3.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2">subscript</csymbol><apply id="S3.E1.m1.1.1.3.3.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2"><ci id="S3.E1.m1.1.1.3.3.3.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2.1">^</ci><ci id="S3.E1.m1.1.1.3.3.3.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2.2.2.2">𝑝</ci></apply><ci id="S3.E1.m1.1.1.3.3.3.2.2.2.3.cmml" xref="S3.E1.m1.1.1.3.3.3.2.2.2.3">𝑖</ci></apply><cn id="S3.E1.m1.1.1.3.3.3.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.3.2.2.3">2</cn></apply></apply><apply id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3"><apply id="S3.E1.m1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.3.1.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.3.3.3.1.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.1.2"></sum><ci id="S3.E1.m1.1.1.3.3.3.3.1.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2">superscript</csymbol><apply id="S3.E1.m1.1.1.3.3.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.3.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2.2.2">𝑝</ci><ci id="S3.E1.m1.1.1.3.3.3.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3.2.2.3">𝑖</ci></apply><cn id="S3.E1.m1.1.1.3.3.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.3.3.2.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}_{\text{dice}}=1-\frac{2\sum_{i}\hat{p}_{i}p_{i}}{\sum_{i}\hat{p}_{%
i}^{2}+\sum_{i}p_{i}^{2}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT dice end_POSTSUBSCRIPT = 1 - divide start_ARG 2 ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2">where <math alttext="\hat{p}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2.2" xref="S3.SS3.p3.1.m1.1.1.2.2.cmml">p</mi><mo id="S3.SS3.p3.1.m1.1.1.2.1" xref="S3.SS3.p3.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><apply id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2"><ci id="S3.SS3.p3.1.m1.1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.1.2.1">^</ci><ci id="S3.SS3.p3.1.m1.1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2.2">𝑝</ci></apply><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\hat{p}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">p</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑝</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represent the predicted and the ground truth values, respectively. To further improve the overall pixel-level accuracy, we adapt the Cross-Entropy loss which measures the pixel-wise classification error and penalizes the model heavily for misclassifying individual pixels:</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{ce}}=-\sum_{i}p_{i}\log(\hat{p}_{i})" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.2.2.3.3" xref="S3.E2.m1.2.2.3.3a.cmml">ce</mtext></msub><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mo id="S3.E2.m1.2.2.1a" xref="S3.E2.m1.2.2.1.cmml">−</mo><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><munder id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.2.2" movablelimits="false" xref="S3.E2.m1.2.2.1.1.2.2.cmml">∑</mo><mi id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml">i</mi></munder><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.3.2.cmml">p</mi><mi id="S3.E2.m1.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.2" lspace="0.167em" xref="S3.E2.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">log</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1a" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">p</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2">ℒ</ci><ci id="S3.E2.m1.2.2.3.3a.cmml" xref="S3.E2.m1.2.2.3.3"><mtext id="S3.E2.m1.2.2.3.3.cmml" mathsize="70%" xref="S3.E2.m1.2.2.3.3">ce</mtext></ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><minus id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1"></minus><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2"></sum><ci id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2">𝑝</ci><ci id="S3.E2.m1.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><log id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></log><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.2">𝑝</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}_{\text{ce}}=-\sum_{i}p_{i}\log(\hat{p}_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">caligraphic_L start_POSTSUBSCRIPT ce end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">The final loss is a weighted sum of the Dice and Cross-Entropy loss defined as:</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{dice}}+\alpha\mathcal{L}_{\text{%
ce}}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3a.cmml">total</mtext></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3a.cmml">dice</mtext></msub><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">α</mi><mo id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">⁢</mo><msub id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3a.cmml">ce</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">ℒ</ci><ci id="S3.E3.m1.1.1.2.3a.cmml" xref="S3.E3.m1.1.1.2.3"><mtext id="S3.E3.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.E3.m1.1.1.2.3">total</mtext></ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">ℒ</ci><ci id="S3.E3.m1.1.1.3.2.3a.cmml" xref="S3.E3.m1.1.1.3.2.3"><mtext id="S3.E3.m1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E3.m1.1.1.3.2.3">dice</mtext></ci></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><times id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝛼</ci><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E3.m1.1.1.3.3.3.3a.cmml" xref="S3.E3.m1.1.1.3.3.3.3"><mtext id="S3.E3.m1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S3.E3.m1.1.1.3.3.3.3">ce</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{dice}}+\alpha\mathcal{L}_{\text{%
ce}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">caligraphic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT dice end_POSTSUBSCRIPT + italic_α caligraphic_L start_POSTSUBSCRIPT ce end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">Combining these two losses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a>]</cite> provides a balance between the pixel-level accuracy and the region-level overlap. The Cross-Entropy loss helps in the initial stages of training by providing strong gradients, while the Dice loss becomes more important later, refining the segmentation boundaries and efficiently handling class imbalance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.9"><span class="ltx_text ltx_font_bold" id="S3.SS3.p8.9.1">Implementation details.</span>  Before any evaluation or training, the pixel values of the 2D CT image slices are scaled to the range <math alttext="[0,1]" class="ltx_Math" display="inline" id="S3.SS3.p8.1.m1.2"><semantics id="S3.SS3.p8.1.m1.2a"><mrow id="S3.SS3.p8.1.m1.2.3.2" xref="S3.SS3.p8.1.m1.2.3.1.cmml"><mo id="S3.SS3.p8.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.p8.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS3.p8.1.m1.1.1" xref="S3.SS3.p8.1.m1.1.1.cmml">0</mn><mo id="S3.SS3.p8.1.m1.2.3.2.2" xref="S3.SS3.p8.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p8.1.m1.2.2" xref="S3.SS3.p8.1.m1.2.2.cmml">1</mn><mo id="S3.SS3.p8.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.p8.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.1.m1.2b"><interval closure="closed" id="S3.SS3.p8.1.m1.2.3.1.cmml" xref="S3.SS3.p8.1.m1.2.3.2"><cn id="S3.SS3.p8.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p8.1.m1.1.1">0</cn><cn id="S3.SS3.p8.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p8.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>. For UNet training, additional data transformations are applied: from each training image, four random crops of size <math alttext="1\times 256\times 256" class="ltx_Math" display="inline" id="S3.SS3.p8.2.m2.1"><semantics id="S3.SS3.p8.2.m2.1a"><mrow id="S3.SS3.p8.2.m2.1.1" xref="S3.SS3.p8.2.m2.1.1.cmml"><mn id="S3.SS3.p8.2.m2.1.1.2" xref="S3.SS3.p8.2.m2.1.1.2.cmml">1</mn><mo id="S3.SS3.p8.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p8.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p8.2.m2.1.1.3" xref="S3.SS3.p8.2.m2.1.1.3.cmml">256</mn><mo id="S3.SS3.p8.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p8.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p8.2.m2.1.1.4" xref="S3.SS3.p8.2.m2.1.1.4.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.2.m2.1b"><apply id="S3.SS3.p8.2.m2.1.1.cmml" xref="S3.SS3.p8.2.m2.1.1"><times id="S3.SS3.p8.2.m2.1.1.1.cmml" xref="S3.SS3.p8.2.m2.1.1.1"></times><cn id="S3.SS3.p8.2.m2.1.1.2.cmml" type="integer" xref="S3.SS3.p8.2.m2.1.1.2">1</cn><cn id="S3.SS3.p8.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p8.2.m2.1.1.3">256</cn><cn id="S3.SS3.p8.2.m2.1.1.4.cmml" type="integer" xref="S3.SS3.p8.2.m2.1.1.4">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.2.m2.1c">1\times 256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.2.m2.1d">1 × 256 × 256</annotation></semantics></math> pixels are generated. These random crops are produced with a <math alttext="2/3" class="ltx_Math" display="inline" id="S3.SS3.p8.3.m3.1"><semantics id="S3.SS3.p8.3.m3.1a"><mrow id="S3.SS3.p8.3.m3.1.1" xref="S3.SS3.p8.3.m3.1.1.cmml"><mn id="S3.SS3.p8.3.m3.1.1.2" xref="S3.SS3.p8.3.m3.1.1.2.cmml">2</mn><mo id="S3.SS3.p8.3.m3.1.1.1" xref="S3.SS3.p8.3.m3.1.1.1.cmml">/</mo><mn id="S3.SS3.p8.3.m3.1.1.3" xref="S3.SS3.p8.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.3.m3.1b"><apply id="S3.SS3.p8.3.m3.1.1.cmml" xref="S3.SS3.p8.3.m3.1.1"><divide id="S3.SS3.p8.3.m3.1.1.1.cmml" xref="S3.SS3.p8.3.m3.1.1.1"></divide><cn id="S3.SS3.p8.3.m3.1.1.2.cmml" type="integer" xref="S3.SS3.p8.3.m3.1.1.2">2</cn><cn id="S3.SS3.p8.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p8.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.3.m3.1c">2/3</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.3.m3.1d">2 / 3</annotation></semantics></math> probability that the center pixel is classified as foreground in the ground truth data, and a <math alttext="1/3" class="ltx_Math" display="inline" id="S3.SS3.p8.4.m4.1"><semantics id="S3.SS3.p8.4.m4.1a"><mrow id="S3.SS3.p8.4.m4.1.1" xref="S3.SS3.p8.4.m4.1.1.cmml"><mn id="S3.SS3.p8.4.m4.1.1.2" xref="S3.SS3.p8.4.m4.1.1.2.cmml">1</mn><mo id="S3.SS3.p8.4.m4.1.1.1" xref="S3.SS3.p8.4.m4.1.1.1.cmml">/</mo><mn id="S3.SS3.p8.4.m4.1.1.3" xref="S3.SS3.p8.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.4.m4.1b"><apply id="S3.SS3.p8.4.m4.1.1.cmml" xref="S3.SS3.p8.4.m4.1.1"><divide id="S3.SS3.p8.4.m4.1.1.1.cmml" xref="S3.SS3.p8.4.m4.1.1.1"></divide><cn id="S3.SS3.p8.4.m4.1.1.2.cmml" type="integer" xref="S3.SS3.p8.4.m4.1.1.2">1</cn><cn id="S3.SS3.p8.4.m4.1.1.3.cmml" type="integer" xref="S3.SS3.p8.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.4.m4.1c">1/3</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.4.m4.1d">1 / 3</annotation></semantics></math> probability that it is classified as background. Images smaller than <math alttext="1\times 256\times 256" class="ltx_Math" display="inline" id="S3.SS3.p8.5.m5.1"><semantics id="S3.SS3.p8.5.m5.1a"><mrow id="S3.SS3.p8.5.m5.1.1" xref="S3.SS3.p8.5.m5.1.1.cmml"><mn id="S3.SS3.p8.5.m5.1.1.2" xref="S3.SS3.p8.5.m5.1.1.2.cmml">1</mn><mo id="S3.SS3.p8.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p8.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p8.5.m5.1.1.3" xref="S3.SS3.p8.5.m5.1.1.3.cmml">256</mn><mo id="S3.SS3.p8.5.m5.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p8.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p8.5.m5.1.1.4" xref="S3.SS3.p8.5.m5.1.1.4.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.5.m5.1b"><apply id="S3.SS3.p8.5.m5.1.1.cmml" xref="S3.SS3.p8.5.m5.1.1"><times id="S3.SS3.p8.5.m5.1.1.1.cmml" xref="S3.SS3.p8.5.m5.1.1.1"></times><cn id="S3.SS3.p8.5.m5.1.1.2.cmml" type="integer" xref="S3.SS3.p8.5.m5.1.1.2">1</cn><cn id="S3.SS3.p8.5.m5.1.1.3.cmml" type="integer" xref="S3.SS3.p8.5.m5.1.1.3">256</cn><cn id="S3.SS3.p8.5.m5.1.1.4.cmml" type="integer" xref="S3.SS3.p8.5.m5.1.1.4">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.5.m5.1c">1\times 256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.5.m5.1d">1 × 256 × 256</annotation></semantics></math> pixels are padded to this size. The resulting images are then rotated by 90 degrees with a probability of <math alttext="0.5" class="ltx_Math" display="inline" id="S3.SS3.p8.6.m6.1"><semantics id="S3.SS3.p8.6.m6.1a"><mn id="S3.SS3.p8.6.m6.1.1" xref="S3.SS3.p8.6.m6.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.6.m6.1b"><cn id="S3.SS3.p8.6.m6.1.1.cmml" type="float" xref="S3.SS3.p8.6.m6.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.6.m6.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.6.m6.1d">0.5</annotation></semantics></math>. All UNet models are trained for 50 epochs. After each epoch, the model is validated with the validation dataset and saved only if the average DSC metric on the validation set improves. We use a batch size of 2 and a learning rate of <math alttext="5\cdot 10^{-4}" class="ltx_Math" display="inline" id="S3.SS3.p8.7.m7.1"><semantics id="S3.SS3.p8.7.m7.1a"><mrow id="S3.SS3.p8.7.m7.1.1" xref="S3.SS3.p8.7.m7.1.1.cmml"><mn id="S3.SS3.p8.7.m7.1.1.2" xref="S3.SS3.p8.7.m7.1.1.2.cmml">5</mn><mo id="S3.SS3.p8.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p8.7.m7.1.1.1.cmml">⋅</mo><msup id="S3.SS3.p8.7.m7.1.1.3" xref="S3.SS3.p8.7.m7.1.1.3.cmml"><mn id="S3.SS3.p8.7.m7.1.1.3.2" xref="S3.SS3.p8.7.m7.1.1.3.2.cmml">10</mn><mrow id="S3.SS3.p8.7.m7.1.1.3.3" xref="S3.SS3.p8.7.m7.1.1.3.3.cmml"><mo id="S3.SS3.p8.7.m7.1.1.3.3a" xref="S3.SS3.p8.7.m7.1.1.3.3.cmml">−</mo><mn id="S3.SS3.p8.7.m7.1.1.3.3.2" xref="S3.SS3.p8.7.m7.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.7.m7.1b"><apply id="S3.SS3.p8.7.m7.1.1.cmml" xref="S3.SS3.p8.7.m7.1.1"><ci id="S3.SS3.p8.7.m7.1.1.1.cmml" xref="S3.SS3.p8.7.m7.1.1.1">⋅</ci><cn id="S3.SS3.p8.7.m7.1.1.2.cmml" type="integer" xref="S3.SS3.p8.7.m7.1.1.2">5</cn><apply id="S3.SS3.p8.7.m7.1.1.3.cmml" xref="S3.SS3.p8.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p8.7.m7.1.1.3.1.cmml" xref="S3.SS3.p8.7.m7.1.1.3">superscript</csymbol><cn id="S3.SS3.p8.7.m7.1.1.3.2.cmml" type="integer" xref="S3.SS3.p8.7.m7.1.1.3.2">10</cn><apply id="S3.SS3.p8.7.m7.1.1.3.3.cmml" xref="S3.SS3.p8.7.m7.1.1.3.3"><minus id="S3.SS3.p8.7.m7.1.1.3.3.1.cmml" xref="S3.SS3.p8.7.m7.1.1.3.3"></minus><cn id="S3.SS3.p8.7.m7.1.1.3.3.2.cmml" type="integer" xref="S3.SS3.p8.7.m7.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.7.m7.1c">5\cdot 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.7.m7.1d">5 ⋅ 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> for all trained models. The AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib13" title="">13</a>]</cite> with the weight decay of <math alttext="10^{-5}" class="ltx_Math" display="inline" id="S3.SS3.p8.8.m8.1"><semantics id="S3.SS3.p8.8.m8.1a"><msup id="S3.SS3.p8.8.m8.1.1" xref="S3.SS3.p8.8.m8.1.1.cmml"><mn id="S3.SS3.p8.8.m8.1.1.2" xref="S3.SS3.p8.8.m8.1.1.2.cmml">10</mn><mrow id="S3.SS3.p8.8.m8.1.1.3" xref="S3.SS3.p8.8.m8.1.1.3.cmml"><mo id="S3.SS3.p8.8.m8.1.1.3a" xref="S3.SS3.p8.8.m8.1.1.3.cmml">−</mo><mn id="S3.SS3.p8.8.m8.1.1.3.2" xref="S3.SS3.p8.8.m8.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.8.m8.1b"><apply id="S3.SS3.p8.8.m8.1.1.cmml" xref="S3.SS3.p8.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p8.8.m8.1.1.1.cmml" xref="S3.SS3.p8.8.m8.1.1">superscript</csymbol><cn id="S3.SS3.p8.8.m8.1.1.2.cmml" type="integer" xref="S3.SS3.p8.8.m8.1.1.2">10</cn><apply id="S3.SS3.p8.8.m8.1.1.3.cmml" xref="S3.SS3.p8.8.m8.1.1.3"><minus id="S3.SS3.p8.8.m8.1.1.3.1.cmml" xref="S3.SS3.p8.8.m8.1.1.3"></minus><cn id="S3.SS3.p8.8.m8.1.1.3.2.cmml" type="integer" xref="S3.SS3.p8.8.m8.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.8.m8.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.8.m8.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> is employed for model optimization. During training, the cross-entropy term in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.E3" title="Equation 3 ‣ 3.3 Optimization ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3</span></a>) is scaled by <math alttext="\alpha=1" class="ltx_Math" display="inline" id="S3.SS3.p8.9.m9.1"><semantics id="S3.SS3.p8.9.m9.1a"><mrow id="S3.SS3.p8.9.m9.1.1" xref="S3.SS3.p8.9.m9.1.1.cmml"><mi id="S3.SS3.p8.9.m9.1.1.2" xref="S3.SS3.p8.9.m9.1.1.2.cmml">α</mi><mo id="S3.SS3.p8.9.m9.1.1.1" xref="S3.SS3.p8.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS3.p8.9.m9.1.1.3" xref="S3.SS3.p8.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.9.m9.1b"><apply id="S3.SS3.p8.9.m9.1.1.cmml" xref="S3.SS3.p8.9.m9.1.1"><eq id="S3.SS3.p8.9.m9.1.1.1.cmml" xref="S3.SS3.p8.9.m9.1.1.1"></eq><ci id="S3.SS3.p8.9.m9.1.1.2.cmml" xref="S3.SS3.p8.9.m9.1.1.2">𝛼</ci><cn id="S3.SS3.p8.9.m9.1.1.3.cmml" type="integer" xref="S3.SS3.p8.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.9.m9.1c">\alpha=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p8.9.m9.1d">italic_α = 1</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.1.1"><span class="ltx_text" id="S3.F2.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="553" id="S3.F2.1.1.1.g1" src="x2.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.5.2" style="font-size:90%;">Qualitative semantic segmentation results<span class="ltx_text ltx_font_medium" id="S3.F2.5.2.1">. Each column displays one example case of each of the 6 segmentation tasks (from left to right: Liver, Lung, Pancreas, Hepatic Vessel, Spleen, Colon), and the rows from top to bottom display the ground truth label, SAM’s predictions (with bounding box prompt), UNet’s predictions, and predictions obtained by the UNet with pseudo labels.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate the proposed prompting methods and assess the performance of the SAM model on medical semantic segmentation and compare against state-of-the-art semantic segmentation models. We also provide an ablation of different prompt techniques of the proposed pipeline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Dataset.</span>  We use the Medical Segmentation Decathlon (MSD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib1" title="">1</a>]</cite> dataset covering 10 segmentation tasks from the imaging modalities of computed tomography (CT), magnetic resonance imaging (MRI), and multiparametric magnetic resonance imaging (mp-MRI). In this work, we focus on the semantic segmentation of CT scans. The dataset consists of 6 different organs (tasks): Liver, Lung (tumor), Pancreas, Hepatic Vessels, Spleen, and Colon (tumor). Each organ is represented by corresponding training/test splits of 2D slices, as follows: Liver: 15429/3734; Lung: 1225/432; Pancreas: 6884/1908; Hepatic Vessels: 11053/1993; Spleen: 870/181; Colon: 1045/240.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.4"><span class="ltx_text ltx_font_bold" id="S4.p3.4.1">Metric.</span>  All models have been evaluated with the dice coefficient on the test split of each task, <em class="ltx_emph ltx_font_italic" id="S4.p3.4.2">i.e</em>.<span class="ltx_text" id="S4.p3.4.3"></span> <math alttext="\text{DSC}=2TP/(2TP+FP+FN)" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mtext id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3a.cmml">DSC</mtext><mo id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">=</mo><mrow id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml"><mrow id="S4.p3.1.m1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.3.cmml"><mn id="S4.p3.1.m1.1.1.1.3.2" xref="S4.p3.1.m1.1.1.1.3.2.cmml">2</mn><mo id="S4.p3.1.m1.1.1.1.3.1" xref="S4.p3.1.m1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.3.3" xref="S4.p3.1.m1.1.1.1.3.3.cmml">T</mi><mo id="S4.p3.1.m1.1.1.1.3.1a" xref="S4.p3.1.m1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.3.4" xref="S4.p3.1.m1.1.1.1.3.4.cmml">P</mi></mrow><mo id="S4.p3.1.m1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.2.cmml">/</mo><mrow id="S4.p3.1.m1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml"><mo id="S4.p3.1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.1.m1.1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml"><mrow id="S4.p3.1.m1.1.1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.1.1.2.cmml"><mn id="S4.p3.1.m1.1.1.1.1.1.1.2.2" xref="S4.p3.1.m1.1.1.1.1.1.1.2.2.cmml">2</mn><mo id="S4.p3.1.m1.1.1.1.1.1.1.2.1" xref="S4.p3.1.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.1.1.1.2.3" xref="S4.p3.1.m1.1.1.1.1.1.1.2.3.cmml">T</mi><mo id="S4.p3.1.m1.1.1.1.1.1.1.2.1a" xref="S4.p3.1.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.1.1.1.2.4" xref="S4.p3.1.m1.1.1.1.1.1.1.2.4.cmml">P</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p3.1.m1.1.1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.1.1.1.1.3.2" xref="S4.p3.1.m1.1.1.1.1.1.1.3.2.cmml">F</mi><mo id="S4.p3.1.m1.1.1.1.1.1.1.3.1" xref="S4.p3.1.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.1.1.1.3.3" xref="S4.p3.1.m1.1.1.1.1.1.1.3.3.cmml">P</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.1.1a" xref="S4.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p3.1.m1.1.1.1.1.1.1.4" xref="S4.p3.1.m1.1.1.1.1.1.1.4.cmml"><mi id="S4.p3.1.m1.1.1.1.1.1.1.4.2" xref="S4.p3.1.m1.1.1.1.1.1.1.4.2.cmml">F</mi><mo id="S4.p3.1.m1.1.1.1.1.1.1.4.1" xref="S4.p3.1.m1.1.1.1.1.1.1.4.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.1.1.1.1.4.3" xref="S4.p3.1.m1.1.1.1.1.1.1.4.3.cmml">N</mi></mrow></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2"></eq><ci id="S4.p3.1.m1.1.1.3a.cmml" xref="S4.p3.1.m1.1.1.3"><mtext id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">DSC</mtext></ci><apply id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"><divide id="S4.p3.1.m1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.2"></divide><apply id="S4.p3.1.m1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.3"><times id="S4.p3.1.m1.1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.1.3.1"></times><cn id="S4.p3.1.m1.1.1.1.3.2.cmml" type="integer" xref="S4.p3.1.m1.1.1.1.3.2">2</cn><ci id="S4.p3.1.m1.1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.1.3.3">𝑇</ci><ci id="S4.p3.1.m1.1.1.1.3.4.cmml" xref="S4.p3.1.m1.1.1.1.3.4">𝑃</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1"><plus id="S4.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.1"></plus><apply id="S4.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2"><times id="S4.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2.1"></times><cn id="S4.p3.1.m1.1.1.1.1.1.1.2.2.cmml" type="integer" xref="S4.p3.1.m1.1.1.1.1.1.1.2.2">2</cn><ci id="S4.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2.3">𝑇</ci><ci id="S4.p3.1.m1.1.1.1.1.1.1.2.4.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2.4">𝑃</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.3"><times id="S4.p3.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.p3.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.3.2">𝐹</ci><ci id="S4.p3.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.3.3">𝑃</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.1.1.4.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.4"><times id="S4.p3.1.m1.1.1.1.1.1.1.4.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.4.1"></times><ci id="S4.p3.1.m1.1.1.1.1.1.1.4.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.4.2">𝐹</ci><ci id="S4.p3.1.m1.1.1.1.1.1.1.4.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.4.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\text{DSC}=2TP/(2TP+FP+FN)</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">DSC = 2 italic_T italic_P / ( 2 italic_T italic_P + italic_F italic_P + italic_F italic_N )</annotation></semantics></math>, where <math alttext="TP" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">T</mi><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝑇</ci><ci id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">TP</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_T italic_P</annotation></semantics></math>, <math alttext="FP" class="ltx_Math" display="inline" id="S4.p3.3.m3.1"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mi id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">F</mi><mo id="S4.p3.3.m3.1.1.1" xref="S4.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><times id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1"></times><ci id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">𝐹</ci><ci id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">FP</annotation><annotation encoding="application/x-llamapun" id="S4.p3.3.m3.1d">italic_F italic_P</annotation></semantics></math>, and <math alttext="FN" class="ltx_Math" display="inline" id="S4.p3.4.m4.1"><semantics id="S4.p3.4.m4.1a"><mrow id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml"><mi id="S4.p3.4.m4.1.1.2" xref="S4.p3.4.m4.1.1.2.cmml">F</mi><mo id="S4.p3.4.m4.1.1.1" xref="S4.p3.4.m4.1.1.1.cmml">⁢</mo><mi id="S4.p3.4.m4.1.1.3" xref="S4.p3.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><apply id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1"><times id="S4.p3.4.m4.1.1.1.cmml" xref="S4.p3.4.m4.1.1.1"></times><ci id="S4.p3.4.m4.1.1.2.cmml" xref="S4.p3.4.m4.1.1.2">𝐹</ci><ci id="S4.p3.4.m4.1.1.3.cmml" xref="S4.p3.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">FN</annotation><annotation encoding="application/x-llamapun" id="S4.p3.4.m4.1d">italic_F italic_N</annotation></semantics></math> refer to the number of true positive, false positive, and false negative cases respectively.
The calculations are done pixel-wise by comparing the model prediction with the corresponding ground truth mask from the test set.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.5.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T1.6.2" style="font-size:90%;">Semantic Segmentation: Foundation Models.<span class="ltx_text ltx_font_medium" id="S4.T1.6.2.1"> We report the semantic segmentation results (the DSC metric, <em class="ltx_emph ltx_font_italic" id="S4.T1.6.2.1.1">cf</em>.<span class="ltx_text" id="S4.T1.6.2.1.2"></span> Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4" title="4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4</span></a>) on each task/organ of the MSD dataset obtained by two foundation models, MedSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> and SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite>. The bounding box prompt is used for both models.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.7.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.7.1.1.1" style="padding:0.2pt 8.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.2" style="padding:0.2pt 8.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.3" style="padding:0.2pt 8.5pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.4" style="padding:0.2pt 8.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.5" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.1.1.5.1" style="font-size:80%;">Hepatic</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.7.1.1.6" style="padding:0.2pt 8.5pt;"></th>
<td class="ltx_td ltx_border_tt" id="S4.T1.7.1.1.7" style="padding:0.2pt 8.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.7.2.2.1" style="padding:0.2pt 8.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.2" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.2.1" style="font-size:80%;">Liver</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.3" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.3.1" style="font-size:80%;">Lung</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.4" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.4.1" style="font-size:80%;">Pancreas</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.5" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.5.1" style="font-size:80%;">Vessels</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.6" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.6.1" style="font-size:80%;">Spleen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.2.2.7" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.2.2.7.1" style="font-size:80%;">Colon</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.7.3.3.1" style="padding:0.2pt 8.5pt;">
<span class="ltx_text" id="S4.T1.7.3.3.1.1" style="font-size:80%;">MedSAM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.7.3.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib15" title="">15</a><span class="ltx_text" id="S4.T1.7.3.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.2" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.3.3.2.1" style="font-size:80%;">0.927</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.3" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.3.3.3.1" style="font-size:80%;">0.654</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.4" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.3.3.4.1" style="font-size:80%;">0.592</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.5" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.3.3.5.1" style="font-size:80%;">0.581</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.6" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.3.3.6.1" style="font-size:80%;">0.926</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.3.3.7" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.3.3.7.1" style="font-size:80%;">0.644</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.7.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.7.4.4.1" style="padding:0.2pt 8.5pt;">
<span class="ltx_text" id="S4.T1.7.4.4.1.1" style="font-size:80%;">SAM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.7.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a><span class="ltx_text" id="S4.T1.7.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.2" style="padding:0.2pt 8.5pt;"><span class="ltx_text" id="S4.T1.7.4.4.2.1" style="font-size:80%;">0.926</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.3" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.4.4.3.1" style="font-size:80%;">0.734</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.4" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.4.4.4.1" style="font-size:80%;">0.836</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.5" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.4.4.5.1" style="font-size:80%;">0.752</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.6" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.4.4.6.1" style="font-size:80%;">0.932</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.4.4.7" style="padding:0.2pt 8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.4.4.7.1" style="font-size:80%;">0.754</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.4.2" style="font-size:90%;">Semantic Segmentation on the Medical Segmentation Decathlon.<span class="ltx_text ltx_font_medium" id="S4.T2.4.2.1"> The DSC score is reported for each organ/task of the MSD dataset. The SAM* model is SAM evaluated in the Everything mode. Along with our models (UNet-based), we also test the recent fully-supervised medical semantic segmentation approaches.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.5.1.1.1" style="padding:0.2pt 2.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.1.1.2.1" style="font-size:80%;">Sup.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.1.1.3.1" style="font-size:80%;">Unsup.</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1.4" style="padding:0.2pt 2.5pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1.5" style="padding:0.2pt 2.5pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1.6" style="padding:0.2pt 2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.5.1.1.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.1.1.7.1" style="font-size:80%;">Hepatic</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1.8" style="padding:0.2pt 2.5pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1.9" style="padding:0.2pt 2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.5.2.2.1" style="padding:0.2pt 2.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.2.1" style="font-size:80%;">Labels</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.3.1" style="font-size:80%;">Labels</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.4.1" style="font-size:80%;">Liver</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.5.1" style="font-size:80%;">Lung</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.6.1" style="font-size:80%;">Pancreas</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.7.1" style="font-size:80%;">Vessels</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.8.1" style="font-size:80%;">Spleen</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.2.2.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.2.2.9.1" style="font-size:80%;">Colon</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.3.3.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.3.3.1.1" style="font-size:80%;">Trans VW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.3.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib3" title="">3</a><span class="ltx_text" id="S4.T2.5.3.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.4.1" style="font-size:80%;">0.952</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.5.1" style="font-size:80%;">0.745</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.6.1" style="font-size:80%;">0.814</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.7.1" style="font-size:80%;">0.658</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.8.1" style="font-size:80%;">0.974</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.3.3.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.3.3.9.1" style="font-size:80%;">0.515</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.4.4.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.4.4.1.1" style="font-size:80%;">C2FNAS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.T2.5.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.4.1" style="font-size:80%;">0.950</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.5.1" style="font-size:80%;">0.704</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.6.1" style="font-size:80%;">0.808</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.7.1" style="font-size:80%;">0.643</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.8.1" style="font-size:80%;">0.963</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.4.4.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.4.4.9.1" style="font-size:80%;">0.589</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.5.5.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.5.5.1.1" style="font-size:80%;">Models Gen. </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.5.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib27" title="">27</a><span class="ltx_text" id="S4.T2.5.5.5.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.4.1" style="font-size:80%;">0.957</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.5.1" style="font-size:80%;">0.745</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.6.1" style="font-size:80%;">0.814</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.7.1" style="font-size:80%;">0.658</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.8.1" style="font-size:80%;">0.974</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.5.5.9.1" style="font-size:80%;">0.515</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.6.6.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.6.6.1.1" style="font-size:80%;">nnUNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.6.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib9" title="">9</a><span class="ltx_text" id="S4.T2.5.6.6.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.4.1" style="font-size:80%;">0.958</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.5.1" style="font-size:80%;">0.740</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.6.1" style="font-size:80%;">0.816</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.7.1" style="font-size:80%;">0.665</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.8.1" style="font-size:80%;">0.974</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.6.6.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.6.6.9.1" style="font-size:80%;">0.583</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.7.7.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.7.7.1.1" style="font-size:80%;">DiNTS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.7.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib4" title="">4</a><span class="ltx_text" id="S4.T2.5.7.7.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.4.1" style="font-size:80%;">0.954</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.5.1" style="font-size:80%;">0.748</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.6.1" style="font-size:80%;">0.810</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.7.1" style="font-size:80%;">0.645</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.8.1" style="font-size:80%;">0.970</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.7.7.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.7.7.9.1" style="font-size:80%;">0.592</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.8.8.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.8.8.1.1" style="font-size:80%;">Swin UNETR </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.8.8.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib20" title="">20</a><span class="ltx_text" id="S4.T2.5.8.8.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.4.1" style="font-size:80%;">0.954</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.5.1" style="font-size:80%;">0.766</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.6.1" style="font-size:80%;">0.819</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.7.1" style="font-size:80%;">0.657</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.8.1" style="font-size:80%;">0.970</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.8.8.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.8.8.9.1" style="font-size:80%;">0.595</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.9.9.1" style="padding:0.2pt 2.5pt;">
<span class="ltx_text" id="S4.T2.5.9.9.1.1" style="font-size:80%;">Universal model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.5.9.9.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib12" title="">12</a><span class="ltx_text" id="S4.T2.5.9.9.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.4.1" style="font-size:80%;">0.954</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.5.1" style="font-size:80%;">0.800</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.6.1" style="font-size:80%;">0.828</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.7.1" style="font-size:80%;">0.672</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.8.1" style="font-size:80%;">0.973</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.9.9.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.9.9.9.1" style="font-size:80%;">0.631</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.10.10.1" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.1.1" style="font-size:80%;">SAM*</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.2.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.4.1" style="font-size:80%;">0.687</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.5.1" style="font-size:80%;">0.409</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.6.1" style="font-size:80%;">0.378</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.7.1" style="font-size:80%;">0.234</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.8.1" style="font-size:80%;">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.10.10.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.10.10.9.1" style="font-size:80%;">0.306</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.11.11.1" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.1.1" style="font-size:80%;">UNet</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.3.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.4.1" style="font-size:80%;">0.925</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.5.1" style="font-size:80%;">0.543</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.6.1" style="font-size:80%;">0.721</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.7.1" style="font-size:80%;">0.562</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.8.1" style="font-size:80%;">0.852</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.11.11.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.11.11.9.1" style="font-size:80%;">0.553</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.5.12.12.1" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.1.1" style="font-size:80%;">UNet</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.2" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.2.1" style="font-size:80%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.3" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.3.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.4" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.4.1" style="font-size:80%;">0.903</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.5" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.5.1" style="font-size:80%;">0.508</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.6" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.6.1" style="font-size:80%;">0.681</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.7" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.7.1" style="font-size:80%;">0.542</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.8" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.8.1" style="font-size:80%;">0.834</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.12.12.9" style="padding:0.2pt 2.5pt;"><span class="ltx_text" id="S4.T2.5.12.12.9.1" style="font-size:80%;">0.522</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">SAM vs. MedSAM.</span>  We first evaluate two foundation models, SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib11" title="">11</a>]</cite> and MedSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> on the test splits of the MSD dataset in the zero-shot setting. The MedSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib17" title="">17</a>]</cite> follows the network architecture of SAM but is fine-tuned on a very large and diverse medical dataset consisting of CT, magnetic resonance imaging (MRI), and optical coherence tomography (OCT) data. We use the bounding box prompt for both models and report results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">1</span></a>. Interestingly, the SAM model outperforms its more advanced domain-specific counterpart, except for the Liver. We assume that while using more diverse and specialized medical training data can improve performance for MedSAM, it can also limit the generalization ability compared to a more broadly trained model like SAM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.3.1">Supervised- vs. Weakly-supervised Methods.</span>  Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">2</span></a> presents the averaged test dice scores for UNet and UNet with pseudo labels, along with the zero-shot SAM results and state-of-the-art semantic segmentation models. To compute the pseudo labels, the box prompt was chosen as the prompting method for SAM through ablation studies on different prompt types (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.3.2">cf</em>.<span class="ltx_text" id="S4.SS1.p2.3.3"></span> Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.SS2" title="4.2 Ablation Studies ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4.2</span></a>). The box prompt provides consistent results with test dice scores (<math alttext="\text{DSC}\geq 0.734" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mtext id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2a.cmml">DSC</mtext><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">≥</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">0.734</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><geq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></geq><ci id="S4.SS1.p2.1.m1.1.1.2a.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><mtext id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">DSC</mtext></ci><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p2.1.m1.1.1.3">0.734</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\text{DSC}\geq 0.734</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">DSC ≥ 0.734</annotation></semantics></math>) across the six tasks of various sizes and complexity, while being a relatively simple prompting strategy. The experimental results indicate that UNet with pseudo labels performs comparably to the fully-supervised baseline, UNet, with the absolute difference in test Dice score ranging from <math alttext="0.018" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">0.018</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn id="S4.SS1.p2.2.m2.1.1.cmml" type="float" xref="S4.SS1.p2.2.m2.1.1">0.018</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">0.018</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">0.018</annotation></semantics></math> (Spleen) to <math alttext="0.04" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">0.04</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn id="S4.SS1.p2.3.m3.1.1.cmml" type="float" xref="S4.SS1.p2.3.m3.1.1">0.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">0.04</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">0.04</annotation></semantics></math> (Pancreas). Interestingly, the weakly-supervised UNet even performs on par with state-of-the-art models on several tasks/organs such as Liver, Colon, and Spleen. Notably, the UNet-based models demonstrate modest performance on some tasks. Specifically, the Lung, Spleen, and Colon have the smallest training dataset sizes (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.3.4">cf</em>.<span class="ltx_text" id="S4.SS1.p2.3.5"></span> Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4" title="4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">4</span></a>, Datasets) that could partially explain UNet’s weak Dice score results for the Lung and Colon tasks. However, UNet achieves good accuracy for the Spleen, despite having the smallest training dataset among the six tasks. The Spleen is a large organ with a homogeneous form and clear boundaries, whereas the Lung and Colon tasks involve tumors that often have outlines difficult for even experts to agree on which is in line with previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib1" title="">1</a>]</cite>. Therefore, in addition to dataset sizes, the characteristics of the tasks seem to affect the final test Dice scores of UNet. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S3.F2" title="Figure 2 ‣ 3.3 Optimization ‣ 3 Method ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">2</span></a>, we qualitatively examine the SAM-generated labels and the predictions of the UNet models on examples from the six tasks.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.8.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T3.9.2" style="font-size:90%;">Ablation on different prompt types.<span class="ltx_text ltx_font_medium" id="S4.T3.9.2.1"> SAM’s zero-shot dice score performance on the training set with different prompt types. CM refers to the centre of mass, and +PP/ NP refers to an additional positive or negative point.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.6.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T3.5.6.1.1" style="padding:0.2pt 8.0pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.6.1.2" style="padding:0.2pt 8.0pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.6.1.3" style="padding:0.2pt 8.0pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.6.1.4" style="padding:0.2pt 8.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.6.1.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.6.1.5.1" style="font-size:80%;">Hepatic</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.6.1.6" style="padding:0.2pt 8.0pt;"></th>
<td class="ltx_td ltx_border_tt" id="S4.T3.5.6.1.7" style="padding:0.2pt 8.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T3.5.7.2.1" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.1.1" style="font-size:80%;">Prompt</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.2.1" style="font-size:80%;">Liver</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.3.1" style="font-size:80%;">Lung</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.4.1" style="font-size:80%;">Pancreas</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.5.1" style="font-size:80%;">Vessel</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.6.1" style="font-size:80%;">Spleen</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.5.7.2.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.5.7.2.7.1" style="font-size:80%;">Colon</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1" style="padding:0.2pt 8.0pt;"><math alttext="\text{Point}_{\text{CM}}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mtext id="S4.T3.1.1.1.m1.1.1.2" mathsize="80%" xref="S4.T3.1.1.1.m1.1.1.2a.cmml">Point</mtext><mtext id="S4.T3.1.1.1.m1.1.1.3" mathsize="80%" xref="S4.T3.1.1.1.m1.1.1.3a.cmml">CM</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.m1.1.1.2a.cmml" xref="S4.T3.1.1.1.m1.1.1.2"><mtext id="S4.T3.1.1.1.m1.1.1.2.cmml" mathsize="80%" xref="S4.T3.1.1.1.m1.1.1.2">Point</mtext></ci><ci id="S4.T3.1.1.1.m1.1.1.3a.cmml" xref="S4.T3.1.1.1.m1.1.1.3"><mtext id="S4.T3.1.1.1.m1.1.1.3.cmml" mathsize="56%" xref="S4.T3.1.1.1.m1.1.1.3">CM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\text{Point}_{\text{CM}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">Point start_POSTSUBSCRIPT CM end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.2.1" style="font-size:80%;">0.818</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.3.1" style="font-size:80%;">0.500</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.4.1" style="font-size:80%;">0.580</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.5.1" style="font-size:80%;">0.130</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.6.1" style="font-size:80%;">0.863</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.1.1.7.1" style="font-size:80%;">0.480</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.1" style="padding:0.2pt 8.0pt;"><math alttext="\text{Point}_{\text{interior}}" class="ltx_Math" display="inline" id="S4.T3.2.2.1.m1.1"><semantics id="S4.T3.2.2.1.m1.1a"><msub id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml"><mtext id="S4.T3.2.2.1.m1.1.1.2" mathsize="80%" xref="S4.T3.2.2.1.m1.1.1.2a.cmml">Point</mtext><mtext id="S4.T3.2.2.1.m1.1.1.3" mathsize="80%" xref="S4.T3.2.2.1.m1.1.1.3a.cmml">interior</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><apply id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T3.2.2.1.m1.1.1.2a.cmml" xref="S4.T3.2.2.1.m1.1.1.2"><mtext id="S4.T3.2.2.1.m1.1.1.2.cmml" mathsize="80%" xref="S4.T3.2.2.1.m1.1.1.2">Point</mtext></ci><ci id="S4.T3.2.2.1.m1.1.1.3a.cmml" xref="S4.T3.2.2.1.m1.1.1.3"><mtext id="S4.T3.2.2.1.m1.1.1.3.cmml" mathsize="56%" xref="S4.T3.2.2.1.m1.1.1.3">interior</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\text{Point}_{\text{interior}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.1.m1.1d">Point start_POSTSUBSCRIPT interior end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:80%;">0.830</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.3.1" style="font-size:80%;">0.500</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.4.1" style="font-size:80%;">0.593</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.5.1" style="font-size:80%;">0.130</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.6.1" style="font-size:80%;">0.865</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.2.2.7.1" style="font-size:80%;">0.525</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.3.1" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_markedasmath" id="S4.T3.3.3.1.1" style="font-size:80%;">Box</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.2.1" style="font-size:80%;">0.927</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.3.1" style="font-size:80%;">0.713</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.4.1" style="font-size:80%;">0.822</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.5.1" style="font-size:80%;">0.735</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.6.1" style="font-size:80%;">0.923</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.3.3.7.1" style="font-size:80%;">0.782</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.4.4.1" style="padding:0.2pt 8.0pt;"><math alttext="\text{Box}_{\text{noise}}" class="ltx_Math" display="inline" id="S4.T3.4.4.1.m1.1"><semantics id="S4.T3.4.4.1.m1.1a"><msub id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml"><mtext id="S4.T3.4.4.1.m1.1.1.2" mathsize="80%" xref="S4.T3.4.4.1.m1.1.1.2a.cmml">Box</mtext><mtext id="S4.T3.4.4.1.m1.1.1.3" mathsize="80%" xref="S4.T3.4.4.1.m1.1.1.3a.cmml">noise</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><apply id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1">subscript</csymbol><ci id="S4.T3.4.4.1.m1.1.1.2a.cmml" xref="S4.T3.4.4.1.m1.1.1.2"><mtext id="S4.T3.4.4.1.m1.1.1.2.cmml" mathsize="80%" xref="S4.T3.4.4.1.m1.1.1.2">Box</mtext></ci><ci id="S4.T3.4.4.1.m1.1.1.3a.cmml" xref="S4.T3.4.4.1.m1.1.1.3"><mtext id="S4.T3.4.4.1.m1.1.1.3.cmml" mathsize="56%" xref="S4.T3.4.4.1.m1.1.1.3">noise</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">\text{Box}_{\text{noise}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.1.m1.1d">Box start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.2.1" style="font-size:80%;">0.913</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.3.1" style="font-size:80%;">0.677</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.4.1" style="font-size:80%;">0.774</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.5.1" style="font-size:80%;">0.499</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.6.1" style="font-size:80%;">0.907</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text" id="S4.T3.4.4.7.1" style="font-size:80%;">0.704</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.5.1" style="padding:0.2pt 8.0pt;"><math alttext="\text{Box}_{\text{+PP/NP}}" class="ltx_Math" display="inline" id="S4.T3.5.5.1.m1.1"><semantics id="S4.T3.5.5.1.m1.1a"><msub id="S4.T3.5.5.1.m1.1.1" xref="S4.T3.5.5.1.m1.1.1.cmml"><mtext id="S4.T3.5.5.1.m1.1.1.2" mathsize="80%" xref="S4.T3.5.5.1.m1.1.1.2a.cmml">Box</mtext><mtext id="S4.T3.5.5.1.m1.1.1.3" mathsize="80%" xref="S4.T3.5.5.1.m1.1.1.3a.cmml">+PP/NP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.1.m1.1b"><apply id="S4.T3.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1">subscript</csymbol><ci id="S4.T3.5.5.1.m1.1.1.2a.cmml" xref="S4.T3.5.5.1.m1.1.1.2"><mtext id="S4.T3.5.5.1.m1.1.1.2.cmml" mathsize="80%" xref="S4.T3.5.5.1.m1.1.1.2">Box</mtext></ci><ci id="S4.T3.5.5.1.m1.1.1.3a.cmml" xref="S4.T3.5.5.1.m1.1.1.3"><mtext id="S4.T3.5.5.1.m1.1.1.3.cmml" mathsize="56%" xref="S4.T3.5.5.1.m1.1.1.3">+PP/NP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.1.m1.1c">\text{Box}_{\text{+PP/NP}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.1.m1.1d">Box start_POSTSUBSCRIPT +PP/NP end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.2" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.2.1" style="font-size:80%;">0.935</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.3" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.3.1" style="font-size:80%;">0.774</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.4" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.4.1" style="font-size:80%;">0.844</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.5.1" style="font-size:80%;">0.743</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.6" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1" style="font-size:80%;">0.931</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.7" style="padding:0.2pt 8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.7.1" style="font-size:80%;">0.808</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Studies</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">The need for prompts was validated by running SAM with Everything Mode on the test splits of the tasks (<em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.2.1">cf</em>.<span class="ltx_text" id="S4.SS2.p1.2.2"></span> Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">2</span></a>). As the Everything Mode outputs multiple masks for one input image, the final mask was chosen as the best dice score match with the ground truth mask of the test image in question, which adds supervision to the approach. The averaged test dice scores on everything mode are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">2</span></a>, and range from <math alttext="0.234" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0.234</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1">0.234</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">0.234</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">0.234</annotation></semantics></math> (Hepatic Vessels) to <math alttext="0.803" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">0.803</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn id="S4.SS2.p1.2.m2.1.1.cmml" type="float" xref="S4.SS2.p1.2.m2.1.1">0.803</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">0.803</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">0.803</annotation></semantics></math> (Spleen). The SAM model with Everything Mode struggled to segment small structures and ROIs with unclear edges. While this approach could achieve promising dice scores for some of the large and clearly outlined organs, the good performance on such tasks was not consistent.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F3.1.1"><span class="ltx_text" id="S4.F3.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="S4.F3.1.1.1.g1" src="x3.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.5.2" style="font-size:90%;">Ablation on different prompt types<span class="ltx_text ltx_font_medium" id="S4.F3.5.2.1">. Segmentation masks from different prompts. Green and red stars are positive and negative points, respectively.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.T3" title="Table 3 ‣ 4.1 Results ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3</span></a> show the importance of different prompts when using SAM as an annotation tool. We find that point prompts generally perform worse than box prompts. This is highlighted especially on the more ambiguous tasks, <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.1">e.g</em>.<span class="ltx_text" id="S4.SS2.p2.1.2"></span>, hepatic vessels, where the mask foreground consists of multiple small, irregular clusters. These experiments confirm observations done in previous literature  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#bib.bib26" title="">26</a>]</cite>, where it was suggested that SAM can achieve great performance on larger, homogeneous structures, such as liver and spleen, but struggles more on heterogeneous, complex structures. However, combining the box prompt with an additional positive or negative point achieves the highest test dice scores on all datasets. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.20253v1#S4.F3" title="Figure 3 ‣ 4.2 Ablation Studies ‣ 4 Experiments ‣ Medical Image Segmentation with SAM-generated Annotations"><span class="ltx_text ltx_ref_tag">3</span></a>, we present qualitative results for the different prompt methods on example CT scans. As the simple box prompt offers strong performance with less work for annotators compared to <math alttext="\text{Box}_{\text{+PP/NP}}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mtext id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2a.cmml">Box</mtext><mtext id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3a.cmml">+PP/NP</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2a.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><mtext id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">Box</mtext></ci><ci id="S4.SS2.p2.1.m1.1.1.3a.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><mtext id="S4.SS2.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p2.1.m1.1.1.3">+PP/NP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\text{Box}_{\text{+PP/NP}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">Box start_POSTSUBSCRIPT +PP/NP end_POSTSUBSCRIPT</annotation></semantics></math>, it was chosen as the prompting method for the pseudo label generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To conclude, our experiments show positive results on the possibility of using SAM as a data annotation tool for medical images, and encourages further experiments on the subject. Most significantly, our findings indicate that a model trained on SAM-generated pseudo labels can achieve performance comparable to that of a fully supervised model. We note that the prompting strategy has a significant effect on pseudo label accuracy. While SAM’s bounding box pseudo label accuracy can be further improved with additional iterative points, the experiments show that a simpler prompting method, <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">i.e</em>.<span class="ltx_text" id="S5.p1.1.2"></span> box prompt, is still able to generate consistently reliable pseudo labels across tasks of various sizes and complexities. This observation is noteworthy, as a simple and efficient prompting method allows for faster data annotation, while maintaining final model performance comparable to fully supervised training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Acknowledgments </span>
This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Some of the experiments were performed using the supercomputing resource Berzelius provided by the National Supercomputer Centre at Linköping University and the Knut and Alice Wallenberg foundation. JK also acknowledges funding from the Academy of Finland (grant No. 327911, 353138). We acknowledge CSC – IT Center for Science, Finland, and the Aalto Science-IT project for computational resources.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Kopp-Schneider, A., Landman, B.A., Litjens, G., Menze, B., Ronneberger, O., Summers, R.M., van Ginneken, B., Bilello, M., Bilic, P., Christ, P.F., Do, R.K.G., Gollub, M.J., Heckers, S.H., Huisman, H., Jarnagin, W.R., McHugo, M.K., Napel, S., Pernicka, J.S.G., Rhode, K., Tobon-Gomez, C., Vorontsov, E., Meakin, J.A., Ourselin, S., Wiesenfarth, M., Arbeláez, P., Bae, B., Chen, S., Daza, L., Feng, J., He, B., Isensee, F., Ji, Y., Jia, F., Kim, I., Maier-Hein, K., Merhof, D., Pai, A., Park, B., Perslev, M., Rezaiifar, R., Rippel, O., Sarasua, I., Shen, W., Son, J., Wachinger, C., Wang, L., Wang, Y., Xia, Y., Xu, D., Xu, Z., Zheng, Y., Simpson, A.L., Maier-Hein, L., Cardoso, M.J.: The medical segmentation decathlon. Nature Communications <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">13</span>(1) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Gutiérrez, J.D., Rodriguez-Echeverria, R., Delgado, E., Rodrigo, M.Á.S., Sánchez-Figueroa, F.: No more training: SAM’s zero-shot transfer capabilities for cost-efficient medical image segmentation. IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">12</span>, 24205–24216 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Haghighi, F., Taher, M., Zhou, Z., Gotway, M.B., Liang, J.: Transferable visual words: Exploiting the semantics of anatomical patterns for self-supervised learning. In: IEEE Transactions on Medical Imaging (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
He, Y., Yang, D., Roth, H., Zhao, C., Xu, D.: DiNTS: Differentiable neural network topology search for 3d medical image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5841–5850 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hesamian, M.H., Jia, W., He, X., Kennedy, P.: Deep learning techniques for medical image segmentation: Achievements and challenges. Journal of Digital Imaging <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">32</span>(4), 582–596 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Huang, Y., Yang, X., Liu, L., Zhou, H., Chang, A., Zhou, X., Chen, R., Yu, J., Chen, J., Chen, C., Liu, S., Chi, H., Hu, X., Yue, K., Li, L., Grau, V., Fan, D.P., Dong, F., Ni, D.: Segment anything model for medical images? Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">92</span>, 103061 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Huix, J.P., Ganeshan, A.R., Haslum, J.F., Söderberg, M., Matsoukas, C., Smith, K.: Are natural domain foundation models useful for medical image classification? arXiv:2310.19522 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: International Conference on Machine Learning. pp. 448–456. PMLR (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnU-net: A self-configuring method for deep learning-based biomedical. Nature Methods <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">18</span>, 203–211 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ji, G.P., Fan, D.P., Xu, P., Zhou, B., Cheng, M.M., Van Gool, L.: SAM struggles in concealed scenes — empirical study on “segment anything”. Science China Information Sciences <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">66</span>(12), 226101 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Liu, J., Zhang, Y., Chen, J.N., Xiao, J., Lu, Y., A Landman, B., Yuan, Y., Yuille, A., Tang, Y., Zhou, Z.: Clip-driven universal model for organ segmentation and tumor detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 21152–21164 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv:1711.05101 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang, X., Martel, A.L.: Loss odyssey in medical image segmentation. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">71</span> (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical images. Nature Communications <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">15</span>(1) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Mattjie, C., De Moura, L.V., Ravazio, R., Kupssinskü, L., Parraga, O., Delucis, M.M., Barros, R.C.: Zero-shot performance of the segment anything model (SAM) in 2d medical imaging: A comprehensive evaluation and practical guidelines. In: International Conference on Bioinformatics and Bioengineering (BIBE). pp. 108–112 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mazurowski, M.A., Dong, H., Gu, H., Yang, J., Konz, N., Zhang, Y.: Segment anything model for medical image analysis: An experimental study. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib17.1.1">89</span> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In: Proceedings of the Medical Image Computing and Computer Assisted Intervention – MICCAI. pp. 234–241. Springer International Publishing (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Roy, S., Wald, T., Koehler, G., Rokuss, M.R., Disch, N., Holzschuh, J., Zimmerer, D., Maier-Hein, K.H.: SAM.MD: Zero-shot medical image segmentation capabilities of the segment anything model. arXiv:2304.05396 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V., Hatamizadeh, A.: Self-supervised pre-training of swin transformers for 3d medical image analysis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 20730–20740 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wang, R., Lei, T., Cui, R., Zhang, B., Meng, H., Nandi, A.K.: Medical image segmentation using deep learning: A survey. IET Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">16</span>(5), 1243–1267 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wu, J., Ji, W., Liu, Y., Fu, H., Xu, M., Xu, Y., Jin, Y.: Medical SAM adapter: Adapting segment anything model for medical image segmentation. arXiv:2304.12620 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Yu, Q., Yang, D., Roth, H., Bai, Y., Zhang, Y., Yuille, A.L., Xu, D.: C2FNAS: Coarse-to-fine neural architecture search for 3d medical image segmentation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4126–4135 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Zhang, K., Liu, D.: Customized segment anything model for medical image segmentation. arXiv:2304.13785 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhang, S., Metaxas, D.: On the challenges and perspectives of foundation models for medical image analysis. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">91</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhang, Y., Shen, Z., Jiao, R.: Segment anything model for medical image segmentation: Current applications and future directions. Computers in Biology and Medicine <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">171</span> (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhou, Z., Sodha, V., Pang, J., Gotway, M.B., Liang, J.: Models genesis. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">67</span> (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">(glossaries)                Package glossaries Warning: No \printglossary or \printglossaries found. ˙(Remove \makeglossaries if you
don’t want any glossaries.) ˙This document will not
have a glossary</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 12:26:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
