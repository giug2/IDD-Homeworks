<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.04851] AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction</title><meta property="og:description" content="Recent advancements in sensor technology and deep learning have led to significant progress in 3D human body reconstruction. However, most existing approaches rely on data from a specific sensor, which can be unreliabl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.04851">

<!--Generated on Sat Oct  5 21:06:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
3D human body reconstruction,  adaptive multi-modal multi-view fusion.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anjun Chen, Xiangyu Wang, Zhi Xu, Kun Shi, Yan Qin, Yuchi Huo, Jiming Chen,  and Qi Ye
</span><span class="ltx_author_notes">Anjun Chen, Xiangyu Wang, Zhi Xu, Kun Shi, Jiming Chen, and Qi Ye are with the State Key Laboratory of Industrial Control Technology, Zhejiang University. Email: {anjunchen, xy_wong, xuzhi, kuns, cjm, qi.ye}@zju.edu.cn
Yan Qin is with the School of Automation, Chongqing University. Email: yan.qin@cqu.edu.cn
Yuchi Huo is with the State Key Lab of CAD&amp;CG, Zhejiang University and Zhejiang Lab. Email: eehyc0@zju.edu.cn
This work was supported in part by NSFC under Grants 62088101, 62233013, 61790571, 62103372, and the Fundamental Research Funds for the Central Universities.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent advancements in sensor technology and deep learning have led to significant progress in 3D human body reconstruction. However, most existing approaches rely on data from a specific sensor, which can be unreliable due to the inherent limitations of individual sensing modalities. On the other hand, existing multi-modal fusion methods generally require customized designs based on the specific sensor combinations or setups, which limits the flexibility and generality of these methods. Furthermore, conventional point-image projection-based and Transformer-based fusion networks are susceptible to the influence of noisy modalities and sensor poses. To address these limitations and achieve robust 3D human body reconstruction in various conditions, we propose AdaptiveFusion, a generic adaptive multi-modal multi-view fusion framework that can effectively incorporate arbitrary combinations of uncalibrated sensor inputs. By treating different modalities from various viewpoints as equal tokens, and our handcrafted modality sampling module by leveraging the inherent flexibility of Transformer models, AdaptiveFusion is able to cope with arbitrary numbers of inputs and accommodate noisy modalities with only a single training network. Extensive experiments on large-scale human datasets demonstrate the effectiveness of AdaptiveFusion in achieving high-quality 3D human body reconstruction in various environments. In addition, our method achieves superior accuracy compared to state-of-the-art fusion methods.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
3D human body reconstruction, adaptive multi-modal multi-view fusion.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D human body reconstruction is widely studied in many practical vision applications, including human-robot interaction, XR technologies, and motion capture. Most of the prevailing reconstruction approaches leverage single-modal data, such as RGB image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, depth image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and radar point cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Nonetheless, all kinds of sensing modalities have their own defects, and therefore single-modal information sources inevitably suffer from unreliability. For example, the perception capability of the RGB camera deteriorates rapidly in poor illumination and inclement weather <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Radar signals are restricted by sparsity and multi-path effect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, while depth camera performs poorly in smoke and occlusion conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Therefore, fusing sensory data from disparate modalities to combine their strengths is essential to realize robust 3D human body reconstruction in both normal and adverse conditions.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance of different fusion methods for RGB images and depth (and noisy) point clouds on the mmBody dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Noisy Kinect depth is downsampled Kinect depth point clouds with missing parts.</figcaption>
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:208.1pt;height:45.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-144.3pt,31.4pt) scale(0.419041019673821,0.419041019673821) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S1.T1.1.1.1.1.1.1" class="ltx_text">Methods</span></th>
<td id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Image w/ Kinect Depth</td>
<td id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Image w/ Noisy Kinect Depth</td>
</tr>
<tr id="S1.T1.1.1.2.2" class="ltx_tr">
<td id="S1.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">MPJPE ↓</td>
<td id="S1.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPVE ↓</td>
<td id="S1.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">MPJPE ↓</td>
<td id="S1.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">MPVE ↓</td>
</tr>
<tr id="S1.T1.1.1.3.3" class="ltx_tr">
<th id="S1.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Point w/ Image Feature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S1.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">4.4</td>
<td id="S1.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.8</td>
<td id="S1.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T1.1.1.3.3.4.1" class="ltx_text ltx_framed ltx_framed_underline">5.0</span> (<span id="S1.T1.1.1.3.3.4.2" class="ltx_text ltx_framed ltx_framed_underline">13%</span> ↑)</td>
<td id="S1.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T1.1.1.3.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">6.5</span> (<span id="S1.T1.1.1.3.3.5.2" class="ltx_text ltx_framed ltx_framed_underline">12%</span> ↑)</td>
</tr>
<tr id="S1.T1.1.1.4.4" class="ltx_tr">
<th id="S1.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S1.T1.1.1.4.4.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.4.4.2.1" class="ltx_text ltx_framed ltx_framed_underline">4.3</span></td>
<td id="S1.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.4.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">5.5</span></td>
<td id="S1.T1.1.1.4.4.4" class="ltx_td ltx_align_center">5.3 (23% ↑)</td>
<td id="S1.T1.1.1.4.4.5" class="ltx_td ltx_align_center">6.6 (20% ↑)</td>
</tr>
<tr id="S1.T1.1.1.5.5" class="ltx_tr">
<th id="S1.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S1.T1.1.1.5.5.2" class="ltx_td ltx_align_center">4.7</td>
<td id="S1.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">6.2</td>
<td id="S1.T1.1.1.5.5.4" class="ltx_td ltx_align_center">7.8 (65% ↑)</td>
<td id="S1.T1.1.1.5.5.5" class="ltx_td ltx_align_center">10.4 (67% ↑)</td>
</tr>
<tr id="S1.T1.1.1.6.6" class="ltx_tr">
<th id="S1.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">AdaptiveFusion (Ours)</th>
<td id="S1.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.1.1.6.6.2.1" class="ltx_text ltx_font_bold">3.5</span></td>
<td id="S1.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S1.T1.1.1.6.6.3.1" class="ltx_text ltx_font_bold">4.7</span></td>
<td id="S1.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T1.1.1.6.6.4.1" class="ltx_text ltx_font_bold">3.7</span> (<span id="S1.T1.1.1.6.6.4.2" class="ltx_text ltx_font_bold">5%</span> ↑)</td>
<td id="S1.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T1.1.1.6.6.5.1" class="ltx_text ltx_font_bold">4.9</span> (<span id="S1.T1.1.1.6.6.5.2" class="ltx_text ltx_font_bold">4%</span> ↑)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, combining multi-modal information is not trivial. Early LiDAR-camera fusion approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> adopt point-to-image projection to combine point clouds and image pixel values/features through element-wise addition or channel-wise concatenation. These approaches rely on the local projection relationship between the point clouds and images, which can break down if one of the modalities is compromised or fails. Undesirable issues like random incompleteness and temporal fluctuations of point clouds can result in the retrieval of inadequate or incorrect features from corresponding images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Furthermore, the degradation of image features in challenging environments, such as low lighting conditions, can extremely impair the performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. More recently, several customized Transformer-based structures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have been proposed for multi-modal fusion. These networks still suffer from the issues of noisy point clouds. As demonstrated in <a href="#S1.T1" title="In I Introduction ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>, both point-image projection-based and transformer-based fusion networks deteriorate rapidly with noisy depth<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text" style="color:#000000;">We process the original human depth point clouds to generate noisy depth point clouds with missing parts and sparsity. Specifically, we utilize ground-truth joint locations to remove most of the depth points in the lower body region of the human body, and then randomly remove points near 1-5 limb joints to simulate the random missing characteristics of the radar point cloud. Subsequently, the remaining depth point cloud is downsampled to 256 points and fed into the fusion models with RGB images.</span></span></span></span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Besides, most of these fusion frameworks work with the fusion of two modalities from the same viewpoint. Only several recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> explore flexible sensor fusion for object detection. These works are constrained by the requirements of different training weights for various modal combinations or accurate poses for different sensors and views.
Despite various fusion methods, to the best of our knowledge, how to integrate information from diverse uncalibrated sensor setups and combinations in an adaptive way remains unexplored. With the development of mobile robots and the Internet of Things, information fusion from dynamically combined multi-view sensors with unknown/noisy poses is an important problem to be solved to enable collaboration between agents.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Therefore, in this paper, we present AdaptiveFusion, the first generic multi-view and multi-modal fusion framework for 3D human body reconstruction that can adaptively fuse arbitrary numbers of uncalibrated sensor inputs from different viewpoints to achieve flexible and robust reconstruction. In our framework, different from fusion via projection, we resort to a Fusion Transformer Module to adaptively select local features from different inputs based on their feature strengths instead of the spatial affinity of features and fuse the more informative features with quantity irrelevant operators. Additionally, in contrast to previous fusion methods that regard point clouds as the main modality, our framework does not assume a main modality and treats features from different modalities as equal tokens. The corrupted tokens from one modality could possibly be remedied by others or disregarded to accommodate the sparsity and missing parts.
Further, we propose an innovative modality sampling module, ensuring the model encounters all modal combinations during training. Consequently, we only need to train a single network to handle arbitrary input combinations during inference.
With all designs, i.e. the notion of equal tokens for different modalities rather than a predefined main modality and the ingenious training strategy, our framework not only enables to accommodate arbitrary numbers of modalities but also enhances fusion capability to handle the fusion with noisy point clouds much more effectively as verified in <a href="#S1.T1" title="In I Introduction ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We conduct extensive experiments on the mmBody <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> dataset, including extreme weather conditions like smoke, rain, and night. We evaluate the performance of AdaptiveFusion under different scenes, and it outperforms traditional fusion methods in all weather environments. Additionally, we evaluate AdaptiveFusion on the other large-scale human datasets Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, HuMMan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> BEHAVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and it outperforms existing works by a large margin.
The contributions of our work can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose AdaptiveFusion, the first generic adaptive fusion framework that can adapt to arbitrary combinations of multi-modal multi-view inputs without calibration for 3D human body reconstruction.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our novel notion of treating different modalities as equal tokens and the incorporation of coupled attention modules effectively handles fusion with noisy point clouds while also enabling a generic framework.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">AdaptiveFusion realizes excellent performance across different sensor configurations, is insusceptible to sensor defects in various weather conditions, and outperforms stat-of-the-art fusion approaches significantly.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper extends our previous conference version <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> of fusing two fixed modalities from the same viewpoint to fusing arbitrary combinations of multi-modal multi-view inputs. As the current submission addresses a different problem, it only keeps the idea of fusion by attention operations on tokens from different modalities while the introduction, the related work, the detailed methodology, and the experiments all differ from the conference version.
The rest of this paper is organized as follows: <a href="#S2" title="II Related Works ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">II</span></a> gives a brief overview of related works on 3D human reconstruction and multi-sensor fusion. <a href="#S3" title="III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">III</span></a> presents our proposed adaptive multi-modal multi-view fusion method, AdaptiveFusion. <a href="#S4" title="IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">IV</span></a> elaborates experimental results. <a href="#S5" title="V Conclusions ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">V</span></a> finally concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Human Body Reconstruction</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">3D human body reconstruction can be broadly categorized into parametric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and non-parametric approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. In the former, a body model, such as SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, is used to represent the human body. Despite greatly reducing regressing parameters, it is still challenging to estimate precise coefficients from a single image<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. To improve the reconstruction, researchers make efforts by utilizing multi-view information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> or dense depth maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. On the other hand, non-parametric approaches directly regress the vertices of the 3D mesh from the input image. Most pioneers choose Graph Convolutional Neural Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to model the local interactions between neighboring vertices with an adjacency matrix. More recent approaches, such as METRO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, utilize transformer encoders to jointly model the relationships between vertices and joints.
Recently, millimeter wave (mmWave) sensors have gained popularity for their ability to work in challenging conditions such as rain, smoke, and occlusion. Several wireless systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> have been developed to reconstruct the human body and the mmWave-based system is one of them. Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> propose an accessible real-time human mesh reconstruction solution utilizing commercial portable mmWave devices.
Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> present a large-scale mmWave human body dataset with paired RGBD images in various environments. With this dataset, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> introduce an mmWave-RGB fusion method for 3D human body reconstruction. These methods have shown decent results in 3D human body reconstruction from various modalities. However, the capability of the reconstruction from the uncalibrated multi-sensor combinations is not studied. In this work, we adopt the non-parametric approach for body reconstruction from unfixed sensor inputs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Multi-Sensor Fusion</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Conventional fusion methods can be broadly classified into three categories: decision-level, data-level, and feature-level fusion.
Decision-level fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> usually utilizes information from one modality to generate regions of interest containing valid objects.
However, such coarse-grained fusion strategies may not fully release the potential of multiple modalities.
Data-level fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> commonly entails the coordinate projection technique, which is easily affected by the sensor misalignment and defective image features.
Feature-level fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> typically involves the fusion of proposal-wise features in multi-modal feature maps, while determining the optimal weighting for features of each modality is challenging.
Recently, promising performance has been achieved by Transformer-based fusion, which sheds light on the possibility of leveraging the Transformer structure as a substitute for manually designed alignment operations.
Specifically, DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> uses a learnable alignment mechanism to dynamically correlate LiDAR information with the most relevant camera features.
TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> prunes feature tokens among single-modal Transformer layers to preserve better information and then re-utilizes the pruned tokens for multi-modal fusion.
TransFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> employs a soft-association approach to process inferior image situations.
Chen <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> propose a radar-camera fusion method ImmFusion to combine mmWave point clouds and RGB images to reconstruct 3D human body.
However, these methods have not achieved the capability of handling unfixed numbers of input modalities.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Currently, only <span id="S2.SS2.p2.1.1" class="ltx_text" style="color:#000000;">limited</span> literature reveals insights on fusion methods with more than three sensors. SeeingThroughFog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> advances an entropy-driven fusion architecture utilizing RGB cameras, LiDARs, radars, and infrared cameras for object detection. Some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> propose to formulate unified end-to-end multi-sensor fusion frameworks for 3D detection.
These multi-modal fusion schemes, however, can not handle arbitrary modalities without calibration and are tailored for the object detection task, which is inapplicable for the fusion-based human body reconstruction task. To this end, we make efforts in overcoming potential sensor defects to robustly reconstruct 3D human body from multi-view multi-modal inputs in various weather conditions.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present our proposed AdaptiveFusion, which takes arbitrary numbers of uncalibrated sensor inputs for 3D human body reconstruction.
<a href="#S3.F1" title="In III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> illustrates the framework of AdaptiveFusion. The structure aims to effectively utilize the sensor information at global and local levels to predict the human body mesh. Given any sensor inputs, the global/local features for each modality are firstly extracted by the corresponding backbone, respectively. Next, global features of different modalities are incorporated as one global feature vector by the Global Integrated Module (GIM) and embedded with SMPL-X template positions. Then, all global/local features are tokenized as input of a multi-layer Fusion Transformer Module (FTM) to dynamically fuse the information of all modalities and directly regress the coordinates of 3D human joints and coarse mesh vertices. <span id="S3.p1.1.1" class="ltx_text" style="color:#000000;">With</span> the adaptability to the arbitrary number of token inputs of Transformer, GIM and FTM can adaptively handle any input feature combinations, including single modality input. At last, we employ Multi-Layer Perceptrons (MLPs) to upsample the coarse mesh vertices to the full mesh vertices.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.04851/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of different fusion methods. (a) Framework of our proposed AdaptiveFusion.
We first extract global and local features from each of the sampled modalities using corresponding backbones. Next, we utilize Global Integrated Module to incorporate global features. Then, we employ Fusion Transformer Module to fuse global and local features and to regress locations of joints and vertices.
D.R. MLP stands for a dimension reduction MLP.
(b) DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. (c) TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. (d) FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem Formulation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">AdaptiveFusion aims to predict the 3D positions of the joints and vertices of human meshes from any input modality combinations:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="f:s\rightarrow Y\quad s\in S(X)," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml">f</mi><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">:</mo><mrow id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.3.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">s</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">→</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml">Y</mi></mrow><mspace width="1em" id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.3a.cmml"></mspace><mrow id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml">s</mi><mo id="S3.E1.m1.2.2.1.1.2.2.2.1" xref="S3.E1.m1.2.2.1.1.2.2.2.1.cmml">∈</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.3.2" xref="S3.E1.m1.2.2.1.1.2.2.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.2.2.3.1" xref="S3.E1.m1.2.2.1.1.2.2.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2.3.3.2" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.3.3.2.1" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">X</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.3.3.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">:</ci><ci id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4">𝑓</ci><apply id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.3a.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">→</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">𝑠</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">𝑌</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><in id="S3.E1.m1.2.2.1.1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.1"></in><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2">𝑠</ci><apply id="S3.E1.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.3"><times id="S3.E1.m1.2.2.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.3.1"></times><ci id="S3.E1.m1.2.2.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.3.2">𝑆</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑋</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">f:s\rightarrow Y\quad s\in S(X),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.5" class="ltx_p">where <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">X</annotation></semantics></math> is a set of different input modalities, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="S(X)" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.2" xref="S3.SS1.p1.2.m2.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.2.2" xref="S3.SS1.p1.2.m2.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.2.1" xref="S3.SS1.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.p1.2.m2.1.2.3.2" xref="S3.SS1.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.3.2.1" xref="S3.SS1.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">X</mi><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.3.2.2" xref="S3.SS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.2.cmml" xref="S3.SS1.p1.2.m2.1.2"><times id="S3.SS1.p1.2.m2.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.2.1"></times><ci id="S3.SS1.p1.2.m2.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.2.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">S(X)</annotation></semantics></math> contains all permutations of the nonempty power set of <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">X</annotation></semantics></math>, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">s</annotation></semantics></math> denotes one combination of input sensors, and <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">Y</annotation></semantics></math> is the output joints and vertices.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p">Specifically, the available inputs of the mmBody dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are RGB images and depth point clouds from 2 viewpoints, and radar point clouds from one viewpoint. Hence, <math id="S3.SS1.p2.1.m1.5" class="ltx_Math" alttext="X=\{I_{m},D_{m},R\},m=1,2" display="inline"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.5.2" xref="S3.SS1.p2.1.m1.5.5.3.cmml"><mrow id="S3.SS1.p2.1.m1.4.4.1.1" xref="S3.SS1.p2.1.m1.4.4.1.1.cmml"><mi id="S3.SS1.p2.1.m1.4.4.1.1.4" xref="S3.SS1.p2.1.m1.4.4.1.1.4.cmml">X</mi><mo id="S3.SS1.p2.1.m1.4.4.1.1.3" xref="S3.SS1.p2.1.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.4.4.1.1.2.2" xref="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.4.4.1.1.2.2.3" xref="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml">{</mo><msub id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.3.cmml">m</mi></msub><mo id="S3.SS1.p2.1.m1.4.4.1.1.2.2.4" xref="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml">,</mo><msub id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.cmml"><mi id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.2" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.2.cmml">D</mi><mi id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.3" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.3.cmml">m</mi></msub><mo id="S3.SS1.p2.1.m1.4.4.1.1.2.2.5" xref="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml">,</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">R</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.4.4.1.1.2.2.6" xref="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml">}</mo></mrow></mrow><mo id="S3.SS1.p2.1.m1.5.5.2.3" xref="S3.SS1.p2.1.m1.5.5.3a.cmml">,</mo><mrow id="S3.SS1.p2.1.m1.5.5.2.2" xref="S3.SS1.p2.1.m1.5.5.2.2.cmml"><mi id="S3.SS1.p2.1.m1.5.5.2.2.2" xref="S3.SS1.p2.1.m1.5.5.2.2.2.cmml">m</mi><mo id="S3.SS1.p2.1.m1.5.5.2.2.1" xref="S3.SS1.p2.1.m1.5.5.2.2.1.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.5.5.2.2.3.2" xref="S3.SS1.p2.1.m1.5.5.2.2.3.1.cmml"><mn id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">1</mn><mo id="S3.SS1.p2.1.m1.5.5.2.2.3.2.1" xref="S3.SS1.p2.1.m1.5.5.2.2.3.1.cmml">,</mo><mn id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">2</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><apply id="S3.SS1.p2.1.m1.5.5.3.cmml" xref="S3.SS1.p2.1.m1.5.5.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.5.5.3a.cmml" xref="S3.SS1.p2.1.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.1.m1.4.4.1.1.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1"><eq id="S3.SS1.p2.1.m1.4.4.1.1.3.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.3"></eq><ci id="S3.SS1.p2.1.m1.4.4.1.1.4.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.4">𝑋</ci><set id="S3.SS1.p2.1.m1.4.4.1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2"><apply id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.2">𝐼</ci><ci id="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.1.1.1.3">𝑚</ci></apply><apply id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.1.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.2">𝐷</ci><ci id="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.4.4.1.1.2.2.2.3">𝑚</ci></apply><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑅</ci></set></apply><apply id="S3.SS1.p2.1.m1.5.5.2.2.cmml" xref="S3.SS1.p2.1.m1.5.5.2.2"><eq id="S3.SS1.p2.1.m1.5.5.2.2.1.cmml" xref="S3.SS1.p2.1.m1.5.5.2.2.1"></eq><ci id="S3.SS1.p2.1.m1.5.5.2.2.2.cmml" xref="S3.SS1.p2.1.m1.5.5.2.2.2">𝑚</ci><list id="S3.SS1.p2.1.m1.5.5.2.2.3.1.cmml" xref="S3.SS1.p2.1.m1.5.5.2.2.3.2"><cn type="integer" id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">1</cn><cn type="integer" id="S3.SS1.p2.1.m1.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3">2</cn></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">X=\{I_{m},D_{m},R\},m=1,2</annotation></semantics></math>. <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="I_{m}\in\mathbb{R}^{224\times 224\times 3}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><msub id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">I</mi><mi id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml">m</mi></msub><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml"><mn id="S3.SS1.p2.2.m2.1.1.3.3.2" xref="S3.SS1.p2.2.m2.1.1.3.3.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.3.3.1" xref="S3.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.2.m2.1.1.3.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.3.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.3.3.1a" xref="S3.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.2.m2.1.1.3.3.4" xref="S3.SS1.p2.2.m2.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><in id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></in><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.2">𝐼</ci><ci id="S3.SS1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3">𝑚</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3"><times id="S3.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.2">224</cn><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.3">224</cn><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.4.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">I_{m}\in\mathbb{R}^{224\times 224\times 3}</annotation></semantics></math> is the cropped body region of RGB images with a size of <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mn id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">224</cn><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">224\times 224</annotation></semantics></math>, <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="D_{m}\in\mathbb{R}^{4096\times 3}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><msub id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml">D</mi><mi id="S3.SS1.p2.4.m4.1.1.2.3" xref="S3.SS1.p2.4.m4.1.1.2.3.cmml">m</mi></msub><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mn id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml">4096</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.3.1" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><in id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></in><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2">𝐷</ci><ci id="S3.SS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.1.1.2.3">𝑚</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><times id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2">4096</cn><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">D_{m}\in\mathbb{R}^{4096\times 3}</annotation></semantics></math> depth point clouds with 4096 points, and <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="R\in\mathbb{R}^{1024\times 3}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">R</mi><mo id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml"><mn id="S3.SS1.p2.5.m5.1.1.3.3.2" xref="S3.SS1.p2.5.m5.1.1.3.3.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.5.m5.1.1.3.3.1" xref="S3.SS1.p2.5.m5.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.5.m5.1.1.3.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><in id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"></in><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑅</ci><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3"><times id="S3.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.2">1024</cn><cn type="integer" id="S3.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">R\in\mathbb{R}^{1024\times 3}</annotation></semantics></math> radar point clouds with 1024 points. <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">m</annotation></semantics></math> indicates the viewpoints. With these inputs, a total of 31 input combinations are supported. The output <math id="S3.SS1.p2.7.m7.2" class="ltx_Math" alttext="Y=[J,V]" display="inline"><semantics id="S3.SS1.p2.7.m7.2a"><mrow id="S3.SS1.p2.7.m7.2.3" xref="S3.SS1.p2.7.m7.2.3.cmml"><mi id="S3.SS1.p2.7.m7.2.3.2" xref="S3.SS1.p2.7.m7.2.3.2.cmml">Y</mi><mo id="S3.SS1.p2.7.m7.2.3.1" xref="S3.SS1.p2.7.m7.2.3.1.cmml">=</mo><mrow id="S3.SS1.p2.7.m7.2.3.3.2" xref="S3.SS1.p2.7.m7.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.7.m7.2.3.3.2.1" xref="S3.SS1.p2.7.m7.2.3.3.1.cmml">[</mo><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">J</mi><mo id="S3.SS1.p2.7.m7.2.3.3.2.2" xref="S3.SS1.p2.7.m7.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.7.m7.2.2" xref="S3.SS1.p2.7.m7.2.2.cmml">V</mi><mo stretchy="false" id="S3.SS1.p2.7.m7.2.3.3.2.3" xref="S3.SS1.p2.7.m7.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.2b"><apply id="S3.SS1.p2.7.m7.2.3.cmml" xref="S3.SS1.p2.7.m7.2.3"><eq id="S3.SS1.p2.7.m7.2.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.1"></eq><ci id="S3.SS1.p2.7.m7.2.3.2.cmml" xref="S3.SS1.p2.7.m7.2.3.2">𝑌</ci><interval closure="closed" id="S3.SS1.p2.7.m7.2.3.3.1.cmml" xref="S3.SS1.p2.7.m7.2.3.3.2"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝐽</ci><ci id="S3.SS1.p2.7.m7.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2">𝑉</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.2c">Y=[J,V]</annotation></semantics></math>, where <math id="S3.SS1.p2.8.m8.2" class="ltx_Math" alttext="J\in\mathbb{R}^{22\times 3},V\in\mathbb{R}^{10475\times 3}" display="inline"><semantics id="S3.SS1.p2.8.m8.2a"><mrow id="S3.SS1.p2.8.m8.2.2.2" xref="S3.SS1.p2.8.m8.2.2.3.cmml"><mrow id="S3.SS1.p2.8.m8.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.1.1.2" xref="S3.SS1.p2.8.m8.1.1.1.1.2.cmml">J</mi><mo id="S3.SS1.p2.8.m8.1.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.8.m8.1.1.1.1.3" xref="S3.SS1.p2.8.m8.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.8.m8.1.1.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.cmml"><mn id="S3.SS1.p2.8.m8.1.1.1.1.3.3.2" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.2.cmml">22</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.8.m8.1.1.1.1.3.3.1" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.8.m8.1.1.1.1.3.3.3" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><mo id="S3.SS1.p2.8.m8.2.2.2.3" xref="S3.SS1.p2.8.m8.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p2.8.m8.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.cmml"><mi id="S3.SS1.p2.8.m8.2.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.2.cmml">V</mi><mo id="S3.SS1.p2.8.m8.2.2.2.2.1" xref="S3.SS1.p2.8.m8.2.2.2.2.1.cmml">∈</mo><msup id="S3.SS1.p2.8.m8.2.2.2.2.3" xref="S3.SS1.p2.8.m8.2.2.2.2.3.cmml"><mi id="S3.SS1.p2.8.m8.2.2.2.2.3.2" xref="S3.SS1.p2.8.m8.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.8.m8.2.2.2.2.3.3" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.cmml"><mn id="S3.SS1.p2.8.m8.2.2.2.2.3.3.2" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.2.cmml">10475</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.8.m8.2.2.2.2.3.3.1" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.8.m8.2.2.2.2.3.3.3" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.3.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.2b"><apply id="S3.SS1.p2.8.m8.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.2.3a.cmml" xref="S3.SS1.p2.8.m8.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p2.8.m8.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1"><in id="S3.SS1.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1"></in><ci id="S3.SS1.p2.8.m8.1.1.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.2">𝐽</ci><apply id="S3.SS1.p2.8.m8.1.1.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.8.m8.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3"><times id="S3.SS1.p2.8.m8.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p2.8.m8.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.2">22</cn><cn type="integer" id="S3.SS1.p2.8.m8.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.3.3.3">3</cn></apply></apply></apply><apply id="S3.SS1.p2.8.m8.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2"><in id="S3.SS1.p2.8.m8.2.2.2.2.1.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.1"></in><ci id="S3.SS1.p2.8.m8.2.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2">𝑉</ci><apply id="S3.SS1.p2.8.m8.2.2.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3">superscript</csymbol><ci id="S3.SS1.p2.8.m8.2.2.2.2.3.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3.2">ℝ</ci><apply id="S3.SS1.p2.8.m8.2.2.2.2.3.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3"><times id="S3.SS1.p2.8.m8.2.2.2.2.3.3.1.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.1"></times><cn type="integer" id="S3.SS1.p2.8.m8.2.2.2.2.3.3.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.2">10475</cn><cn type="integer" id="S3.SS1.p2.8.m8.2.2.2.2.3.3.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.3.3.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.2c">J\in\mathbb{R}^{22\times 3},V\in\mathbb{R}^{10475\times 3}</annotation></semantics></math> are the XYZ coordinates of 22 joints and 10475 vertices. The number of joints and vertices varies depending on the body template used, and the template in the mmBody dataset is SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> template.
As we focus on reconstruction, we utilize the bounding boxes automatically annotated from the ground-truth mesh joints to crop the region of interest containing only the body part.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Muilti-Modal Multi-View Feature Extraction</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In order to address the issue of feature alignment, we independently extract global and local features for each modality following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This strategy allows for the better extraction of global contextual dependencies and modeling of local interactions. Specifically, we directly feed the human body region of multi-modal inputs to the commonly used backbones to extract features. Backbones can be replaced with alternative options for new modalities input.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.13" class="ltx_p">We use a shared HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="E^{i}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">E</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐸</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">E^{i}</annotation></semantics></math> to acquire <span id="S3.SS2.p2.13.1" class="ltx_text" style="color:#000000;">local</span> image grid features <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="L_{m}^{i}\in\mathbb{R}^{49\times 2051}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><msubsup id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.2.cmml">L</mi><mi id="S3.SS2.p2.2.m2.1.1.2.2.3" xref="S3.SS2.p2.2.m2.1.1.2.2.3.cmml">m</mi><mi id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mn id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">49</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m2.1.1.3.3.1" xref="S3.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.cmml">2051</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2.2">𝐿</ci><ci id="S3.SS2.p2.2.m2.1.1.2.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><times id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">49</cn><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3">2051</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">L_{m}^{i}\in\mathbb{R}^{49\times 2051}</annotation></semantics></math> and the global image feature <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="G_{m}^{i}\in\mathbb{R}^{2048}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msubsup id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.2.cmml">G</mi><mi id="S3.SS2.p2.3.m3.1.1.2.2.3" xref="S3.SS2.p2.3.m3.1.1.2.2.3.cmml">m</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><in id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></in><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2.2">𝐺</ci><ci id="S3.SS2.p2.3.m3.1.1.2.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">G_{m}^{i}\in\mathbb{R}^{2048}</annotation></semantics></math> from multi-view images <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="I_{m}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">I</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝐼</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">I_{m}</annotation></semantics></math>. Then we use an MLP to reduce the dimension of grid features
to the same size as the cluster features of point clouds.
For multi-view depth point clouds <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="D_{m}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐷</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">D_{m}</annotation></semantics></math>, we obtain <span id="S3.SS2.p2.13.2" class="ltx_text" style="color:#000000;">local</span> depth cluster features <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="L_{m}^{d}\in\mathbb{R}^{49\times(3+2048)}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.2" xref="S3.SS2.p2.6.m6.1.2.cmml"><msubsup id="S3.SS2.p2.6.m6.1.2.2" xref="S3.SS2.p2.6.m6.1.2.2.cmml"><mi id="S3.SS2.p2.6.m6.1.2.2.2.2" xref="S3.SS2.p2.6.m6.1.2.2.2.2.cmml">L</mi><mi id="S3.SS2.p2.6.m6.1.2.2.2.3" xref="S3.SS2.p2.6.m6.1.2.2.2.3.cmml">m</mi><mi id="S3.SS2.p2.6.m6.1.2.2.3" xref="S3.SS2.p2.6.m6.1.2.2.3.cmml">d</mi></msubsup><mo id="S3.SS2.p2.6.m6.1.2.1" xref="S3.SS2.p2.6.m6.1.2.1.cmml">∈</mo><msup id="S3.SS2.p2.6.m6.1.2.3" xref="S3.SS2.p2.6.m6.1.2.3.cmml"><mi id="S3.SS2.p2.6.m6.1.2.3.2" xref="S3.SS2.p2.6.m6.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml"><mn id="S3.SS2.p2.6.m6.1.1.1.3" xref="S3.SS2.p2.6.m6.1.1.1.3.cmml">49</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.6.m6.1.1.1.2" xref="S3.SS2.p2.6.m6.1.1.1.2.cmml">×</mo><mrow id="S3.SS2.p2.6.m6.1.1.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m6.1.1.1.1.1.2" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.6.m6.1.1.1.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.cmml"><mn id="S3.SS2.p2.6.m6.1.1.1.1.1.1.2" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml">3</mn><mo id="S3.SS2.p2.6.m6.1.1.1.1.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p2.6.m6.1.1.1.1.1.1.3" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml">2048</mn></mrow><mo stretchy="false" id="S3.SS2.p2.6.m6.1.1.1.1.1.3" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.2.cmml" xref="S3.SS2.p2.6.m6.1.2"><in id="S3.SS2.p2.6.m6.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.2.1"></in><apply id="S3.SS2.p2.6.m6.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.2.2.1.cmml" xref="S3.SS2.p2.6.m6.1.2.2">superscript</csymbol><apply id="S3.SS2.p2.6.m6.1.2.2.2.cmml" xref="S3.SS2.p2.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.2.2.2.1.cmml" xref="S3.SS2.p2.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.2.2.2.2.cmml" xref="S3.SS2.p2.6.m6.1.2.2.2.2">𝐿</ci><ci id="S3.SS2.p2.6.m6.1.2.2.2.3.cmml" xref="S3.SS2.p2.6.m6.1.2.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p2.6.m6.1.2.2.3.cmml" xref="S3.SS2.p2.6.m6.1.2.2.3">𝑑</ci></apply><apply id="S3.SS2.p2.6.m6.1.2.3.cmml" xref="S3.SS2.p2.6.m6.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.2.3.1.cmml" xref="S3.SS2.p2.6.m6.1.2.3">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.2.3.2.cmml" xref="S3.SS2.p2.6.m6.1.2.3.2">ℝ</ci><apply id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.1.2"></times><cn type="integer" id="S3.SS2.p2.6.m6.1.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.1.3">49</cn><apply id="S3.SS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1.1.1"><plus id="S3.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.2">3</cn><cn type="integer" id="S3.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.1.1.1.1.3">2048</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">L_{m}^{d}\in\mathbb{R}^{49\times(3+2048)}</annotation></semantics></math> using shared PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="E^{d}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><msup id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">E</mi><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">𝐸</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">E^{d}</annotation></semantics></math>, where 49 denotes the number of seed points sampled by Farthest Point Sample (FPS), 3 denotes the spatial coordinate, and 2048 denotes the dimension of features extracted from the grouping local points. A global feature vector <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="G_{m}^{d}\in\mathbb{R}^{2048}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><msubsup id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2.2.2" xref="S3.SS2.p2.8.m8.1.1.2.2.2.cmml">G</mi><mi id="S3.SS2.p2.8.m8.1.1.2.2.3" xref="S3.SS2.p2.8.m8.1.1.2.2.3.cmml">m</mi><mi id="S3.SS2.p2.8.m8.1.1.2.3" xref="S3.SS2.p2.8.m8.1.1.2.3.cmml">d</mi></msubsup><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml"><mi id="S3.SS2.p2.8.m8.1.1.3.2" xref="S3.SS2.p2.8.m8.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p2.8.m8.1.1.3.3" xref="S3.SS2.p2.8.m8.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><in id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1"></in><apply id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.2.1.cmml" xref="S3.SS2.p2.8.m8.1.1.2">superscript</csymbol><apply id="S3.SS2.p2.8.m8.1.1.2.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.2.2.1.cmml" xref="S3.SS2.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.2.2.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2.2.2">𝐺</ci><ci id="S3.SS2.p2.8.m8.1.1.2.2.3.cmml" xref="S3.SS2.p2.8.m8.1.1.2.2.3">𝑚</ci></apply><ci id="S3.SS2.p2.8.m8.1.1.2.3.cmml" xref="S3.SS2.p2.8.m8.1.1.2.3">𝑑</ci></apply><apply id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.p2.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.p2.8.m8.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">G_{m}^{d}\in\mathbb{R}^{2048}</annotation></semantics></math> is further extracted from cluster features <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="L_{m}^{d}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><msubsup id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2.2" xref="S3.SS2.p2.9.m9.1.1.2.2.cmml">L</mi><mi id="S3.SS2.p2.9.m9.1.1.2.3" xref="S3.SS2.p2.9.m9.1.1.2.3.cmml">m</mi><mi id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">d</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">superscript</csymbol><apply id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.2.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2.2">𝐿</ci><ci id="S3.SS2.p2.9.m9.1.1.2.3.cmml" xref="S3.SS2.p2.9.m9.1.1.2.3">𝑚</ci></apply><ci id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">L_{m}^{d}</annotation></semantics></math> using an MLP.
We employ another PointNet++ <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="E^{r}" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><msup id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml"><mi id="S3.SS2.p2.10.m10.1.1.2" xref="S3.SS2.p2.10.m10.1.1.2.cmml">E</mi><mi id="S3.SS2.p2.10.m10.1.1.3" xref="S3.SS2.p2.10.m10.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><apply id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m10.1.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">superscript</csymbol><ci id="S3.SS2.p2.10.m10.1.1.2.cmml" xref="S3.SS2.p2.10.m10.1.1.2">𝐸</ci><ci id="S3.SS2.p2.10.m10.1.1.3.cmml" xref="S3.SS2.p2.10.m10.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">E^{r}</annotation></semantics></math> to extract local radar cluster features <math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="L^{r}" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><msup id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml"><mi id="S3.SS2.p2.11.m11.1.1.2" xref="S3.SS2.p2.11.m11.1.1.2.cmml">L</mi><mi id="S3.SS2.p2.11.m11.1.1.3" xref="S3.SS2.p2.11.m11.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><apply id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m11.1.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">superscript</csymbol><ci id="S3.SS2.p2.11.m11.1.1.2.cmml" xref="S3.SS2.p2.11.m11.1.1.2">𝐿</ci><ci id="S3.SS2.p2.11.m11.1.1.3.cmml" xref="S3.SS2.p2.11.m11.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">L^{r}</annotation></semantics></math> and global feature <math id="S3.SS2.p2.12.m12.1" class="ltx_Math" alttext="G^{r}" display="inline"><semantics id="S3.SS2.p2.12.m12.1a"><msup id="S3.SS2.p2.12.m12.1.1" xref="S3.SS2.p2.12.m12.1.1.cmml"><mi id="S3.SS2.p2.12.m12.1.1.2" xref="S3.SS2.p2.12.m12.1.1.2.cmml">G</mi><mi id="S3.SS2.p2.12.m12.1.1.3" xref="S3.SS2.p2.12.m12.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m12.1b"><apply id="S3.SS2.p2.12.m12.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.12.m12.1.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1">superscript</csymbol><ci id="S3.SS2.p2.12.m12.1.1.2.cmml" xref="S3.SS2.p2.12.m12.1.1.2">𝐺</ci><ci id="S3.SS2.p2.12.m12.1.1.3.cmml" xref="S3.SS2.p2.12.m12.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m12.1c">G^{r}</annotation></semantics></math> for the radar point cloud <math id="S3.SS2.p2.13.m13.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.p2.13.m13.1a"><mi id="S3.SS2.p2.13.m13.1.1" xref="S3.SS2.p2.13.m13.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m13.1b"><ci id="S3.SS2.p2.13.m13.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m13.1c">R</annotation></semantics></math> due to its sparse and noisy nature.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Modality Fusion with Transformers</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.7" class="ltx_p">Multi-head attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> has been effective for modeling the relationship between information tokens and processing unordered, heterogeneous, and length-undefined data structures such as words and sentences. To allow our fusion framework to effectively select informative token features from arbitrary input modalities, and to dynamically fuse these features from different viewpoints and different modalities, we formulate our fusion problem into the attention framework by exacting <span id="S3.SS3.p1.7.1" class="ltx_text ltx_font_italic">words</span> (local features) and <span id="S3.SS3.p1.7.2" class="ltx_text ltx_font_italic">sentences</span> (global features) from different inputs and designing the interaction modules between these <span id="S3.SS3.p1.7.3" class="ltx_text ltx_font_italic">words</span> and <span id="S3.SS3.p1.7.4" class="ltx_text ltx_font_italic">sentences</span>.
Given <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">n</annotation></semantics></math> modality tokens <math id="S3.SS3.p1.2.m2.4" class="ltx_Math" alttext="m_{1},m_{2},...,m_{n}" display="inline"><semantics id="S3.SS3.p1.2.m2.4a"><mrow id="S3.SS3.p1.2.m2.4.4.3" xref="S3.SS3.p1.2.m2.4.4.4.cmml"><msub id="S3.SS3.p1.2.m2.2.2.1.1" xref="S3.SS3.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.2.2.1.1.2" xref="S3.SS3.p1.2.m2.2.2.1.1.2.cmml">m</mi><mn id="S3.SS3.p1.2.m2.2.2.1.1.3" xref="S3.SS3.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p1.2.m2.4.4.3.4" xref="S3.SS3.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.2.m2.3.3.2.2" xref="S3.SS3.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS3.p1.2.m2.3.3.2.2.2" xref="S3.SS3.p1.2.m2.3.3.2.2.2.cmml">m</mi><mn id="S3.SS3.p1.2.m2.3.3.2.2.3" xref="S3.SS3.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.2.m2.4.4.3.5" xref="S3.SS3.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS3.p1.2.m2.4.4.3.6" xref="S3.SS3.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.2.m2.4.4.3.3" xref="S3.SS3.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS3.p1.2.m2.4.4.3.3.2" xref="S3.SS3.p1.2.m2.4.4.3.3.2.cmml">m</mi><mi id="S3.SS3.p1.2.m2.4.4.3.3.3" xref="S3.SS3.p1.2.m2.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.4b"><list id="S3.SS3.p1.2.m2.4.4.4.cmml" xref="S3.SS3.p1.2.m2.4.4.3"><apply id="S3.SS3.p1.2.m2.2.2.1.1.cmml" xref="S3.SS3.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.2.2.1.1.2">𝑚</ci><cn type="integer" id="S3.SS3.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.p1.2.m2.3.3.2.2.cmml" xref="S3.SS3.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS3.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS3.p1.2.m2.3.3.2.2.2">𝑚</ci><cn type="integer" id="S3.SS3.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS3.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">…</ci><apply id="S3.SS3.p1.2.m2.4.4.3.3.cmml" xref="S3.SS3.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS3.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS3.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS3.p1.2.m2.4.4.3.3.2">𝑚</ci><ci id="S3.SS3.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS3.p1.2.m2.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.4c">m_{1},m_{2},...,m_{n}</annotation></semantics></math>, our fusion module updates vertices <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">V</annotation></semantics></math> (also tokens) by aggregating these features as <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="V=w_{1}m_{1}+w_{2}m_{2}+...+w_{n}m_{n}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">V</mi><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml"><mrow id="S3.SS3.p1.4.m4.1.1.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.cmml"><msub id="S3.SS3.p1.4.m4.1.1.3.2.2" xref="S3.SS3.p1.4.m4.1.1.3.2.2.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2.2.2" xref="S3.SS3.p1.4.m4.1.1.3.2.2.2.cmml">w</mi><mn id="S3.SS3.p1.4.m4.1.1.3.2.2.3" xref="S3.SS3.p1.4.m4.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.3.2.1" xref="S3.SS3.p1.4.m4.1.1.3.2.1.cmml">​</mo><msub id="S3.SS3.p1.4.m4.1.1.3.2.3" xref="S3.SS3.p1.4.m4.1.1.3.2.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.3.2.cmml">m</mi><mn id="S3.SS3.p1.4.m4.1.1.3.2.3.3" xref="S3.SS3.p1.4.m4.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S3.SS3.p1.4.m4.1.1.3.1" xref="S3.SS3.p1.4.m4.1.1.3.1.cmml">+</mo><mrow id="S3.SS3.p1.4.m4.1.1.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.cmml"><msub id="S3.SS3.p1.4.m4.1.1.3.3.2" xref="S3.SS3.p1.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.3.2.2" xref="S3.SS3.p1.4.m4.1.1.3.3.2.2.cmml">w</mi><mn id="S3.SS3.p1.4.m4.1.1.3.3.2.3" xref="S3.SS3.p1.4.m4.1.1.3.3.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.3.3.1" xref="S3.SS3.p1.4.m4.1.1.3.3.1.cmml">​</mo><msub id="S3.SS3.p1.4.m4.1.1.3.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.3.3.2" xref="S3.SS3.p1.4.m4.1.1.3.3.3.2.cmml">m</mi><mn id="S3.SS3.p1.4.m4.1.1.3.3.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.cmml">2</mn></msub></mrow><mo id="S3.SS3.p1.4.m4.1.1.3.1a" xref="S3.SS3.p1.4.m4.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S3.SS3.p1.4.m4.1.1.3.4" xref="S3.SS3.p1.4.m4.1.1.3.4.cmml">…</mi><mo id="S3.SS3.p1.4.m4.1.1.3.1b" xref="S3.SS3.p1.4.m4.1.1.3.1.cmml">+</mo><mrow id="S3.SS3.p1.4.m4.1.1.3.5" xref="S3.SS3.p1.4.m4.1.1.3.5.cmml"><msub id="S3.SS3.p1.4.m4.1.1.3.5.2" xref="S3.SS3.p1.4.m4.1.1.3.5.2.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.5.2.2" xref="S3.SS3.p1.4.m4.1.1.3.5.2.2.cmml">w</mi><mi id="S3.SS3.p1.4.m4.1.1.3.5.2.3" xref="S3.SS3.p1.4.m4.1.1.3.5.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.3.5.1" xref="S3.SS3.p1.4.m4.1.1.3.5.1.cmml">​</mo><msub id="S3.SS3.p1.4.m4.1.1.3.5.3" xref="S3.SS3.p1.4.m4.1.1.3.5.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.5.3.2" xref="S3.SS3.p1.4.m4.1.1.3.5.3.2.cmml">m</mi><mi id="S3.SS3.p1.4.m4.1.1.3.5.3.3" xref="S3.SS3.p1.4.m4.1.1.3.5.3.3.cmml">n</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><eq id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></eq><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑉</ci><apply id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3"><plus id="S3.SS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.1"></plus><apply id="S3.SS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2"><times id="S3.SS3.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.1"></times><apply id="S3.SS3.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.2.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.2.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.2.2">𝑤</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.2.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.2.3">1</cn></apply><apply id="S3.SS3.p1.4.m4.1.1.3.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.2.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.3">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.2.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.3.2">𝑚</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.2.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2.3.3">1</cn></apply></apply><apply id="S3.SS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3"><times id="S3.SS3.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.1"></times><apply id="S3.SS3.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.2.2">𝑤</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.2.3">2</cn></apply><apply id="S3.SS3.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.2">𝑚</ci><cn type="integer" id="S3.SS3.p1.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3">2</cn></apply></apply><ci id="S3.SS3.p1.4.m4.1.1.3.4.cmml" xref="S3.SS3.p1.4.m4.1.1.3.4">…</ci><apply id="S3.SS3.p1.4.m4.1.1.3.5.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5"><times id="S3.SS3.p1.4.m4.1.1.3.5.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.1"></times><apply id="S3.SS3.p1.4.m4.1.1.3.5.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.5.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.2">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.5.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.2.2">𝑤</ci><ci id="S3.SS3.p1.4.m4.1.1.3.5.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.2.3">𝑛</ci></apply><apply id="S3.SS3.p1.4.m4.1.1.3.5.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.5.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.3">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.5.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.3.2">𝑚</ci><ci id="S3.SS3.p1.4.m4.1.1.3.5.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.5.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">V=w_{1}m_{1}+w_{2}m_{2}+...+w_{n}m_{n}</annotation></semantics></math>, where the weight is determined by the similarity of tokens. With the invariance of <math id="S3.SS3.p1.5.m5.4" class="ltx_Math" alttext="w_{1},w_{2},...,w_{n}" display="inline"><semantics id="S3.SS3.p1.5.m5.4a"><mrow id="S3.SS3.p1.5.m5.4.4.3" xref="S3.SS3.p1.5.m5.4.4.4.cmml"><msub id="S3.SS3.p1.5.m5.2.2.1.1" xref="S3.SS3.p1.5.m5.2.2.1.1.cmml"><mi id="S3.SS3.p1.5.m5.2.2.1.1.2" xref="S3.SS3.p1.5.m5.2.2.1.1.2.cmml">w</mi><mn id="S3.SS3.p1.5.m5.2.2.1.1.3" xref="S3.SS3.p1.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p1.5.m5.4.4.3.4" xref="S3.SS3.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.5.m5.3.3.2.2" xref="S3.SS3.p1.5.m5.3.3.2.2.cmml"><mi id="S3.SS3.p1.5.m5.3.3.2.2.2" xref="S3.SS3.p1.5.m5.3.3.2.2.2.cmml">w</mi><mn id="S3.SS3.p1.5.m5.3.3.2.2.3" xref="S3.SS3.p1.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.5.m5.4.4.3.5" xref="S3.SS3.p1.5.m5.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS3.p1.5.m5.4.4.3.6" xref="S3.SS3.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.5.m5.4.4.3.3" xref="S3.SS3.p1.5.m5.4.4.3.3.cmml"><mi id="S3.SS3.p1.5.m5.4.4.3.3.2" xref="S3.SS3.p1.5.m5.4.4.3.3.2.cmml">w</mi><mi id="S3.SS3.p1.5.m5.4.4.3.3.3" xref="S3.SS3.p1.5.m5.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.4b"><list id="S3.SS3.p1.5.m5.4.4.4.cmml" xref="S3.SS3.p1.5.m5.4.4.3"><apply id="S3.SS3.p1.5.m5.2.2.1.1.cmml" xref="S3.SS3.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS3.p1.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS3.p1.5.m5.2.2.1.1.2">𝑤</ci><cn type="integer" id="S3.SS3.p1.5.m5.2.2.1.1.3.cmml" xref="S3.SS3.p1.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.SS3.p1.5.m5.3.3.2.2.cmml" xref="S3.SS3.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.3.3.2.2.1.cmml" xref="S3.SS3.p1.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS3.p1.5.m5.3.3.2.2.2">𝑤</ci><cn type="integer" id="S3.SS3.p1.5.m5.3.3.2.2.3.cmml" xref="S3.SS3.p1.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">…</ci><apply id="S3.SS3.p1.5.m5.4.4.3.3.cmml" xref="S3.SS3.p1.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.4.4.3.3.1.cmml" xref="S3.SS3.p1.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.SS3.p1.5.m5.4.4.3.3.2.cmml" xref="S3.SS3.p1.5.m5.4.4.3.3.2">𝑤</ci><ci id="S3.SS3.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS3.p1.5.m5.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.4c">w_{1},w_{2},...,w_{n}</annotation></semantics></math> to the order and quantity, our framework can incorporate arbitrary combinations of sensor inputs. For the noisy modalities <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="m_{k}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msub id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">m</mi><mi id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">𝑚</ci><ci id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">m_{k}</annotation></semantics></math>, the model can learn to decrease their weights <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="w_{k}" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><msub id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml">w</mi><mi id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2">𝑤</ci><ci id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">w_{k}</annotation></semantics></math> because noisy tokens are far from normal distribution and their similarities with other tokens are lower.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">Specifically, all global features are firstly fused into a global feature <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="G\in\mathbb{R}^{2048}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">G</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">2048</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><in id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></in><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝐺</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">2048</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">G\in\mathbb{R}^{2048}</annotation></semantics></math> by Global Integrated Module (GIM) <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\Phi_{g}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">Φ</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\Phi_{g}</annotation></semantics></math> implemented using a tiny Transformer module similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>,</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="G=\Phi_{g}(G_{m}^{i},G_{m}^{d},G^{r})," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">G</mi><mo id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.1.3.5" xref="S3.E2.m1.1.1.1.1.3.5.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.5.2" xref="S3.E2.m1.1.1.1.1.3.5.2.cmml">Φ</mi><mi id="S3.E2.m1.1.1.1.1.3.5.3" xref="S3.E2.m1.1.1.1.1.3.5.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.3.4.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.3.3.4" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">m</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.3.3.3.5" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">,</mo><msubsup id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml">G</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml">m</mi><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml">d</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.3.3.3.6" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">,</mo><msup id="S3.E2.m1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.3.3.2.cmml">G</mi><mi id="S3.E2.m1.1.1.1.1.3.3.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.3.3.cmml">r</mi></msup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.3.3.7" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"></eq><ci id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5">𝐺</ci><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.4"></times><apply id="S3.E2.m1.1.1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.5.1.cmml" xref="S3.E2.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.5.2.cmml" xref="S3.E2.m1.1.1.1.1.3.5.2">Φ</ci><ci id="S3.E2.m1.1.1.1.1.3.5.3.cmml" xref="S3.E2.m1.1.1.1.1.3.5.3">𝑔</ci></apply><vector id="S3.E2.m1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2">𝐺</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3">𝑚</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.2">𝐺</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.3">𝑚</ci></apply><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3">𝑑</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3.2">𝐺</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3.3">𝑟</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">G=\Phi_{g}(G_{m}^{i},G_{m}^{d},G^{r}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.3" class="ltx_p">where <math id="S3.SS3.p2.3.m1.1" class="ltx_Math" alttext="\Phi_{g}" display="inline"><semantics id="S3.SS3.p2.3.m1.1a"><msub id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p2.3.m1.1.1.2" xref="S3.SS3.p2.3.m1.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p2.3.m1.1.1.3" xref="S3.SS3.p2.3.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.1b"><apply id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m1.1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m1.1.1.2.cmml" xref="S3.SS3.p2.3.m1.1.1.2">Φ</ci><ci id="S3.SS3.p2.3.m1.1.1.3.cmml" xref="S3.SS3.p2.3.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.1c">\Phi_{g}</annotation></semantics></math> is a three-layer attention module ending with a sum operation to integrate the global features. The fusion of global features provides overall context information to mitigate the feature degradation caused by sparsity, missing parts, and adverse weather conditions.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">After <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\Phi_{g}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">Φ</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\Phi_{g}</annotation></semantics></math>, we perform positional embedding by attaching the 3D coordinates of 22 joints and 655 vertices in a coarse mesh downsampled from a SMPL-X template mesh to the global vector, <math id="S3.SS3.p3.2.m2.3" class="ltx_Math" alttext="G^{T}=cat(J^{template},V^{template},G)" display="inline"><semantics id="S3.SS3.p3.2.m2.3a"><mrow id="S3.SS3.p3.2.m2.3.3" xref="S3.SS3.p3.2.m2.3.3.cmml"><msup id="S3.SS3.p3.2.m2.3.3.4" xref="S3.SS3.p3.2.m2.3.3.4.cmml"><mi id="S3.SS3.p3.2.m2.3.3.4.2" xref="S3.SS3.p3.2.m2.3.3.4.2.cmml">G</mi><mi id="S3.SS3.p3.2.m2.3.3.4.3" xref="S3.SS3.p3.2.m2.3.3.4.3.cmml">T</mi></msup><mo id="S3.SS3.p3.2.m2.3.3.3" xref="S3.SS3.p3.2.m2.3.3.3.cmml">=</mo><mrow id="S3.SS3.p3.2.m2.3.3.2" xref="S3.SS3.p3.2.m2.3.3.2.cmml"><mi id="S3.SS3.p3.2.m2.3.3.2.4" xref="S3.SS3.p3.2.m2.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.3" xref="S3.SS3.p3.2.m2.3.3.2.3.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.5" xref="S3.SS3.p3.2.m2.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.3a" xref="S3.SS3.p3.2.m2.3.3.2.3.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.6" xref="S3.SS3.p3.2.m2.3.3.2.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.3b" xref="S3.SS3.p3.2.m2.3.3.2.3.cmml">​</mo><mrow id="S3.SS3.p3.2.m2.3.3.2.2.2" xref="S3.SS3.p3.2.m2.3.3.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.2.m2.3.3.2.2.2.3" xref="S3.SS3.p3.2.m2.3.3.2.2.3.cmml">(</mo><msup id="S3.SS3.p3.2.m2.2.2.1.1.1.1" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.cmml"><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.2" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.2.cmml">J</mi><mrow id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.cmml"><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.2" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.3" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1a" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.4" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1b" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.5" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1c" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.6" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1d" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.7" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1e" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.8" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1f" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.9" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.9.cmml">e</mi></mrow></msup><mo id="S3.SS3.p3.2.m2.3.3.2.2.2.4" xref="S3.SS3.p3.2.m2.3.3.2.2.3.cmml">,</mo><msup id="S3.SS3.p3.2.m2.3.3.2.2.2.2" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.cmml"><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.2" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.2.cmml">V</mi><mrow id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.cmml"><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.2" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.3" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1a" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.4" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1b" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.5" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1c" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.6" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1d" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.7" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1e" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.8" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1f" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.9" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.9.cmml">e</mi></mrow></msup><mo id="S3.SS3.p3.2.m2.3.3.2.2.2.5" xref="S3.SS3.p3.2.m2.3.3.2.2.3.cmml">,</mo><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">G</mi><mo stretchy="false" id="S3.SS3.p3.2.m2.3.3.2.2.2.6" xref="S3.SS3.p3.2.m2.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.3b"><apply id="S3.SS3.p3.2.m2.3.3.cmml" xref="S3.SS3.p3.2.m2.3.3"><eq id="S3.SS3.p3.2.m2.3.3.3.cmml" xref="S3.SS3.p3.2.m2.3.3.3"></eq><apply id="S3.SS3.p3.2.m2.3.3.4.cmml" xref="S3.SS3.p3.2.m2.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.3.3.4.1.cmml" xref="S3.SS3.p3.2.m2.3.3.4">superscript</csymbol><ci id="S3.SS3.p3.2.m2.3.3.4.2.cmml" xref="S3.SS3.p3.2.m2.3.3.4.2">𝐺</ci><ci id="S3.SS3.p3.2.m2.3.3.4.3.cmml" xref="S3.SS3.p3.2.m2.3.3.4.3">𝑇</ci></apply><apply id="S3.SS3.p3.2.m2.3.3.2.cmml" xref="S3.SS3.p3.2.m2.3.3.2"><times id="S3.SS3.p3.2.m2.3.3.2.3.cmml" xref="S3.SS3.p3.2.m2.3.3.2.3"></times><ci id="S3.SS3.p3.2.m2.3.3.2.4.cmml" xref="S3.SS3.p3.2.m2.3.3.2.4">𝑐</ci><ci id="S3.SS3.p3.2.m2.3.3.2.5.cmml" xref="S3.SS3.p3.2.m2.3.3.2.5">𝑎</ci><ci id="S3.SS3.p3.2.m2.3.3.2.6.cmml" xref="S3.SS3.p3.2.m2.3.3.2.6">𝑡</ci><vector id="S3.SS3.p3.2.m2.3.3.2.2.3.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2"><apply id="S3.SS3.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.2">𝐽</ci><apply id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3"><times id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.1"></times><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.2.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.2">𝑡</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.3.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.3">𝑒</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.4.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.4">𝑚</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.5.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.5">𝑝</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.6.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.6">𝑙</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.7.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.7">𝑎</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.8.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.8">𝑡</ci><ci id="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.9.cmml" xref="S3.SS3.p3.2.m2.2.2.1.1.1.1.3.9">𝑒</ci></apply></apply><apply id="S3.SS3.p3.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.3.3.2.2.2.2.1.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2">superscript</csymbol><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.2.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.2">𝑉</ci><apply id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3"><times id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.1"></times><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.2.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.2">𝑡</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.3.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.3">𝑒</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.4.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.4">𝑚</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.5.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.5">𝑝</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.6.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.6">𝑙</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.7.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.7">𝑎</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.8.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.8">𝑡</ci><ci id="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.9.cmml" xref="S3.SS3.p3.2.m2.3.3.2.2.2.2.3.9">𝑒</ci></apply></apply><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝐺</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.3c">G^{T}=cat(J^{template},V^{template},G)</annotation></semantics></math>, where <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="G^{T}\in\mathbb{R}^{677\times 2051}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><msup id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2.2" xref="S3.SS3.p3.3.m3.1.1.2.2.cmml">G</mi><mi id="S3.SS3.p3.3.m3.1.1.2.3" xref="S3.SS3.p3.3.m3.1.1.2.3.cmml">T</mi></msup><mo id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml"><mn id="S3.SS3.p3.3.m3.1.1.3.3.2" xref="S3.SS3.p3.3.m3.1.1.3.3.2.cmml">677</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.3.m3.1.1.3.3.1" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p3.3.m3.1.1.3.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.3.cmml">2051</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><in id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"></in><apply id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2.2">𝐺</ci><ci id="S3.SS3.p3.3.m3.1.1.2.3.cmml" xref="S3.SS3.p3.3.m3.1.1.2.3">𝑇</ci></apply><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3"><times id="S3.SS3.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.1"></times><cn type="integer" id="S3.SS3.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.2">677</cn><cn type="integer" id="S3.SS3.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.3">2051</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">G^{T}\in\mathbb{R}^{677\times 2051}</annotation></semantics></math>. These template features serve like initial object queries in AnchorDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> for 3D object detection.
Simultaneously, the ordinal numbers of each modality are embedded in local features, i.e. cluster and grid features. The incorporation of these local features enables remedy or discard for a local corrupted area only, rather than the entire modality in order to retain as much effective information as possible.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Second, we utilize the Fusion Transformer Module (FTM) <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\Phi_{f}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">Φ</ci><ci id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\Phi_{f}</annotation></semantics></math> to adaptively fuse the information from arbitrary inputs. FTM transforms all modality features into Queries and conducts self-attention between them:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.34" class="ltx_Math" alttext="\begin{split}G^{T\prime},L_{m}^{i\prime},L_{m}^{d\prime},L^{r\prime}=\Phi_{f}\left(G^{T},L_{m}^{i},L_{m}^{d},L^{r}\right),\end{split}" display="block"><semantics id="S3.E3.m1.34a"><mtable displaystyle="true" id="S3.E3.m1.34.34.2"><mtr id="S3.E3.m1.34.34.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.34.34.2b"><mrow id="S3.E3.m1.34.34.2.33.33.33.33"><mrow id="S3.E3.m1.34.34.2.33.33.33.33.1"><mrow id="S3.E3.m1.34.34.2.33.33.33.33.1.4.4"><msup id="S3.E3.m1.34.34.2.33.33.33.33.1.1.1.1"><mi id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">G</mi><mrow id="S3.E3.m1.2.2.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.2.2.1.3.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.2.2.1.1.cmml">T</mi><mo lspace="0em" id="S3.E3.m1.2.2.2.2.2.2.1.2.2" xref="S3.E3.m1.2.2.2.2.2.2.1.3.cmml">⁣</mo><mo mathsize="142%" id="S3.E3.m1.2.2.2.2.2.2.1.2.1" xref="S3.E3.m1.2.2.2.2.2.2.1.2.1.cmml">′</mo></mrow></msup><mo id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msubsup id="S3.E3.m1.34.34.2.33.33.33.33.1.2.2.2"><mi id="S3.E3.m1.4.4.4.4.4.4" xref="S3.E3.m1.4.4.4.4.4.4.cmml">L</mi><mi id="S3.E3.m1.5.5.5.5.5.5.1" xref="S3.E3.m1.5.5.5.5.5.5.1.cmml">m</mi><mrow id="S3.E3.m1.6.6.6.6.6.6.1.2" xref="S3.E3.m1.6.6.6.6.6.6.1.3.cmml"><mi id="S3.E3.m1.6.6.6.6.6.6.1.1" xref="S3.E3.m1.6.6.6.6.6.6.1.1.cmml">i</mi><mo lspace="0em" id="S3.E3.m1.6.6.6.6.6.6.1.2.2" xref="S3.E3.m1.6.6.6.6.6.6.1.3.cmml">⁣</mo><mo mathsize="142%" id="S3.E3.m1.6.6.6.6.6.6.1.2.1" xref="S3.E3.m1.6.6.6.6.6.6.1.2.1.cmml">′</mo></mrow></msubsup><mo id="S3.E3.m1.7.7.7.7.7.7" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msubsup id="S3.E3.m1.34.34.2.33.33.33.33.1.3.3.3"><mi id="S3.E3.m1.8.8.8.8.8.8" xref="S3.E3.m1.8.8.8.8.8.8.cmml">L</mi><mi id="S3.E3.m1.9.9.9.9.9.9.1" xref="S3.E3.m1.9.9.9.9.9.9.1.cmml">m</mi><mrow id="S3.E3.m1.10.10.10.10.10.10.1.2" xref="S3.E3.m1.10.10.10.10.10.10.1.3.cmml"><mi id="S3.E3.m1.10.10.10.10.10.10.1.1" xref="S3.E3.m1.10.10.10.10.10.10.1.1.cmml">d</mi><mo lspace="0em" id="S3.E3.m1.10.10.10.10.10.10.1.2.2" xref="S3.E3.m1.10.10.10.10.10.10.1.3.cmml">⁣</mo><mo mathsize="142%" id="S3.E3.m1.10.10.10.10.10.10.1.2.1" xref="S3.E3.m1.10.10.10.10.10.10.1.2.1.cmml">′</mo></mrow></msubsup><mo id="S3.E3.m1.11.11.11.11.11.11" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msup id="S3.E3.m1.34.34.2.33.33.33.33.1.4.4.4"><mi id="S3.E3.m1.12.12.12.12.12.12" xref="S3.E3.m1.12.12.12.12.12.12.cmml">L</mi><mrow id="S3.E3.m1.13.13.13.13.13.13.1.2" xref="S3.E3.m1.13.13.13.13.13.13.1.3.cmml"><mi id="S3.E3.m1.13.13.13.13.13.13.1.1" xref="S3.E3.m1.13.13.13.13.13.13.1.1.cmml">r</mi><mo lspace="0em" id="S3.E3.m1.13.13.13.13.13.13.1.2.2" xref="S3.E3.m1.13.13.13.13.13.13.1.3.cmml">⁣</mo><mo mathsize="142%" id="S3.E3.m1.13.13.13.13.13.13.1.2.1" xref="S3.E3.m1.13.13.13.13.13.13.1.2.1.cmml">′</mo></mrow></msup></mrow><mo id="S3.E3.m1.14.14.14.14.14.14" xref="S3.E3.m1.14.14.14.14.14.14.cmml">=</mo><mrow id="S3.E3.m1.34.34.2.33.33.33.33.1.8"><msub id="S3.E3.m1.34.34.2.33.33.33.33.1.8.6"><mi mathvariant="normal" id="S3.E3.m1.15.15.15.15.15.15" xref="S3.E3.m1.15.15.15.15.15.15.cmml">Φ</mi><mi id="S3.E3.m1.16.16.16.16.16.16.1" xref="S3.E3.m1.16.16.16.16.16.16.1.cmml">f</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.34.34.2.33.33.33.33.1.8.5" xref="S3.E3.m1.33.33.1.1.1.cmml">​</mo><mrow id="S3.E3.m1.34.34.2.33.33.33.33.1.8.4.4"><mo id="S3.E3.m1.17.17.17.17.17.17" xref="S3.E3.m1.33.33.1.1.1.cmml">(</mo><msup id="S3.E3.m1.34.34.2.33.33.33.33.1.5.1.1.1"><mi id="S3.E3.m1.18.18.18.18.18.18" xref="S3.E3.m1.18.18.18.18.18.18.cmml">G</mi><mi id="S3.E3.m1.19.19.19.19.19.19.1" xref="S3.E3.m1.19.19.19.19.19.19.1.cmml">T</mi></msup><mo id="S3.E3.m1.20.20.20.20.20.20" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msubsup id="S3.E3.m1.34.34.2.33.33.33.33.1.6.2.2.2"><mi id="S3.E3.m1.21.21.21.21.21.21" xref="S3.E3.m1.21.21.21.21.21.21.cmml">L</mi><mi id="S3.E3.m1.22.22.22.22.22.22.1" xref="S3.E3.m1.22.22.22.22.22.22.1.cmml">m</mi><mi id="S3.E3.m1.23.23.23.23.23.23.1" xref="S3.E3.m1.23.23.23.23.23.23.1.cmml">i</mi></msubsup><mo id="S3.E3.m1.24.24.24.24.24.24" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msubsup id="S3.E3.m1.34.34.2.33.33.33.33.1.7.3.3.3"><mi id="S3.E3.m1.25.25.25.25.25.25" xref="S3.E3.m1.25.25.25.25.25.25.cmml">L</mi><mi id="S3.E3.m1.26.26.26.26.26.26.1" xref="S3.E3.m1.26.26.26.26.26.26.1.cmml">m</mi><mi id="S3.E3.m1.27.27.27.27.27.27.1" xref="S3.E3.m1.27.27.27.27.27.27.1.cmml">d</mi></msubsup><mo id="S3.E3.m1.28.28.28.28.28.28" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo><msup id="S3.E3.m1.34.34.2.33.33.33.33.1.8.4.4.4"><mi id="S3.E3.m1.29.29.29.29.29.29" xref="S3.E3.m1.29.29.29.29.29.29.cmml">L</mi><mi id="S3.E3.m1.30.30.30.30.30.30.1" xref="S3.E3.m1.30.30.30.30.30.30.1.cmml">r</mi></msup><mo id="S3.E3.m1.31.31.31.31.31.31" xref="S3.E3.m1.33.33.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.32.32.32.32.32.32" xref="S3.E3.m1.33.33.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E3.m1.34b"><apply id="S3.E3.m1.33.33.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><eq id="S3.E3.m1.14.14.14.14.14.14.cmml" xref="S3.E3.m1.14.14.14.14.14.14"></eq><list id="S3.E3.m1.33.33.1.1.1.4.5.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><apply id="S3.E3.m1.33.33.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">𝐺</ci><list id="S3.E3.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.2"><ci id="S3.E3.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.1">𝑇</ci><ci id="S3.E3.m1.2.2.2.2.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.2.1">′</ci></list></apply><apply id="S3.E3.m1.33.33.1.1.1.2.2.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E3.m1.33.33.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.4.4.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4.4.4">𝐿</ci><ci id="S3.E3.m1.5.5.5.5.5.5.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5.1">𝑚</ci></apply><list id="S3.E3.m1.6.6.6.6.6.6.1.3.cmml" xref="S3.E3.m1.6.6.6.6.6.6.1.2"><ci id="S3.E3.m1.6.6.6.6.6.6.1.1.cmml" xref="S3.E3.m1.6.6.6.6.6.6.1.1">𝑖</ci><ci id="S3.E3.m1.6.6.6.6.6.6.1.2.1.cmml" xref="S3.E3.m1.6.6.6.6.6.6.1.2.1">′</ci></list></apply><apply id="S3.E3.m1.33.33.1.1.1.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E3.m1.33.33.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.3.3.3.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.8.8.8.8.8.8.cmml" xref="S3.E3.m1.8.8.8.8.8.8">𝐿</ci><ci id="S3.E3.m1.9.9.9.9.9.9.1.cmml" xref="S3.E3.m1.9.9.9.9.9.9.1">𝑚</ci></apply><list id="S3.E3.m1.10.10.10.10.10.10.1.3.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.2"><ci id="S3.E3.m1.10.10.10.10.10.10.1.1.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.1">𝑑</ci><ci id="S3.E3.m1.10.10.10.10.10.10.1.2.1.cmml" xref="S3.E3.m1.10.10.10.10.10.10.1.2.1">′</ci></list></apply><apply id="S3.E3.m1.33.33.1.1.1.4.4.4.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.4.4.4.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E3.m1.12.12.12.12.12.12.cmml" xref="S3.E3.m1.12.12.12.12.12.12">𝐿</ci><list id="S3.E3.m1.13.13.13.13.13.13.1.3.cmml" xref="S3.E3.m1.13.13.13.13.13.13.1.2"><ci id="S3.E3.m1.13.13.13.13.13.13.1.1.cmml" xref="S3.E3.m1.13.13.13.13.13.13.1.1">𝑟</ci><ci id="S3.E3.m1.13.13.13.13.13.13.1.2.1.cmml" xref="S3.E3.m1.13.13.13.13.13.13.1.2.1">′</ci></list></apply></list><apply id="S3.E3.m1.33.33.1.1.1.8.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><times id="S3.E3.m1.33.33.1.1.1.8.5.cmml" xref="S3.E3.m1.3.3.3.3.3.3"></times><apply id="S3.E3.m1.33.33.1.1.1.8.6.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.8.6.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.15.15.15.15.15.15.cmml" xref="S3.E3.m1.15.15.15.15.15.15">Φ</ci><ci id="S3.E3.m1.16.16.16.16.16.16.1.cmml" xref="S3.E3.m1.16.16.16.16.16.16.1">𝑓</ci></apply><vector id="S3.E3.m1.33.33.1.1.1.8.4.5.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><apply id="S3.E3.m1.33.33.1.1.1.5.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.5.1.1.1.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E3.m1.18.18.18.18.18.18.cmml" xref="S3.E3.m1.18.18.18.18.18.18">𝐺</ci><ci id="S3.E3.m1.19.19.19.19.19.19.1.cmml" xref="S3.E3.m1.19.19.19.19.19.19.1">𝑇</ci></apply><apply id="S3.E3.m1.33.33.1.1.1.6.2.2.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.6.2.2.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E3.m1.33.33.1.1.1.6.2.2.2.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.6.2.2.2.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.21.21.21.21.21.21.cmml" xref="S3.E3.m1.21.21.21.21.21.21">𝐿</ci><ci id="S3.E3.m1.22.22.22.22.22.22.1.cmml" xref="S3.E3.m1.22.22.22.22.22.22.1">𝑚</ci></apply><ci id="S3.E3.m1.23.23.23.23.23.23.1.cmml" xref="S3.E3.m1.23.23.23.23.23.23.1">𝑖</ci></apply><apply id="S3.E3.m1.33.33.1.1.1.7.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.7.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><apply id="S3.E3.m1.33.33.1.1.1.7.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.7.3.3.3.2.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.25.25.25.25.25.25.cmml" xref="S3.E3.m1.25.25.25.25.25.25">𝐿</ci><ci id="S3.E3.m1.26.26.26.26.26.26.1.cmml" xref="S3.E3.m1.26.26.26.26.26.26.1">𝑚</ci></apply><ci id="S3.E3.m1.27.27.27.27.27.27.1.cmml" xref="S3.E3.m1.27.27.27.27.27.27.1">𝑑</ci></apply><apply id="S3.E3.m1.33.33.1.1.1.8.4.4.4.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.33.33.1.1.1.8.4.4.4.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">superscript</csymbol><ci id="S3.E3.m1.29.29.29.29.29.29.cmml" xref="S3.E3.m1.29.29.29.29.29.29">𝐿</ci><ci id="S3.E3.m1.30.30.30.30.30.30.1.cmml" xref="S3.E3.m1.30.30.30.30.30.30.1">𝑟</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.34c">\begin{split}G^{T\prime},L_{m}^{i\prime},L_{m}^{d\prime},L^{r\prime}=\Phi_{f}\left(G^{T},L_{m}^{i},L_{m}^{d},L^{r}\right),\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.11" class="ltx_p">where coarse mesh <math id="S3.SS3.p4.2.m1.2" class="ltx_Math" alttext="G^{T\prime}\in\mathbb{R}^{677\times 3}" display="inline"><semantics id="S3.SS3.p4.2.m1.2a"><mrow id="S3.SS3.p4.2.m1.2.3" xref="S3.SS3.p4.2.m1.2.3.cmml"><msup id="S3.SS3.p4.2.m1.2.3.2" xref="S3.SS3.p4.2.m1.2.3.2.cmml"><mi id="S3.SS3.p4.2.m1.2.3.2.2" xref="S3.SS3.p4.2.m1.2.3.2.2.cmml">G</mi><mrow id="S3.SS3.p4.2.m1.2.2.2.2" xref="S3.SS3.p4.2.m1.2.2.2.3.cmml"><mi id="S3.SS3.p4.2.m1.1.1.1.1" xref="S3.SS3.p4.2.m1.1.1.1.1.cmml">T</mi><mo lspace="0em" id="S3.SS3.p4.2.m1.2.2.2.2.2" xref="S3.SS3.p4.2.m1.2.2.2.3.cmml">⁣</mo><mo mathsize="142%" id="S3.SS3.p4.2.m1.2.2.2.2.1" xref="S3.SS3.p4.2.m1.2.2.2.2.1.cmml">′</mo></mrow></msup><mo id="S3.SS3.p4.2.m1.2.3.1" xref="S3.SS3.p4.2.m1.2.3.1.cmml">∈</mo><msup id="S3.SS3.p4.2.m1.2.3.3" xref="S3.SS3.p4.2.m1.2.3.3.cmml"><mi id="S3.SS3.p4.2.m1.2.3.3.2" xref="S3.SS3.p4.2.m1.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p4.2.m1.2.3.3.3" xref="S3.SS3.p4.2.m1.2.3.3.3.cmml"><mn id="S3.SS3.p4.2.m1.2.3.3.3.2" xref="S3.SS3.p4.2.m1.2.3.3.3.2.cmml">677</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.2.m1.2.3.3.3.1" xref="S3.SS3.p4.2.m1.2.3.3.3.1.cmml">×</mo><mn id="S3.SS3.p4.2.m1.2.3.3.3.3" xref="S3.SS3.p4.2.m1.2.3.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m1.2b"><apply id="S3.SS3.p4.2.m1.2.3.cmml" xref="S3.SS3.p4.2.m1.2.3"><in id="S3.SS3.p4.2.m1.2.3.1.cmml" xref="S3.SS3.p4.2.m1.2.3.1"></in><apply id="S3.SS3.p4.2.m1.2.3.2.cmml" xref="S3.SS3.p4.2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m1.2.3.2.1.cmml" xref="S3.SS3.p4.2.m1.2.3.2">superscript</csymbol><ci id="S3.SS3.p4.2.m1.2.3.2.2.cmml" xref="S3.SS3.p4.2.m1.2.3.2.2">𝐺</ci><list id="S3.SS3.p4.2.m1.2.2.2.3.cmml" xref="S3.SS3.p4.2.m1.2.2.2.2"><ci id="S3.SS3.p4.2.m1.1.1.1.1.cmml" xref="S3.SS3.p4.2.m1.1.1.1.1">𝑇</ci><ci id="S3.SS3.p4.2.m1.2.2.2.2.1.cmml" xref="S3.SS3.p4.2.m1.2.2.2.2.1">′</ci></list></apply><apply id="S3.SS3.p4.2.m1.2.3.3.cmml" xref="S3.SS3.p4.2.m1.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m1.2.3.3.1.cmml" xref="S3.SS3.p4.2.m1.2.3.3">superscript</csymbol><ci id="S3.SS3.p4.2.m1.2.3.3.2.cmml" xref="S3.SS3.p4.2.m1.2.3.3.2">ℝ</ci><apply id="S3.SS3.p4.2.m1.2.3.3.3.cmml" xref="S3.SS3.p4.2.m1.2.3.3.3"><times id="S3.SS3.p4.2.m1.2.3.3.3.1.cmml" xref="S3.SS3.p4.2.m1.2.3.3.3.1"></times><cn type="integer" id="S3.SS3.p4.2.m1.2.3.3.3.2.cmml" xref="S3.SS3.p4.2.m1.2.3.3.3.2">677</cn><cn type="integer" id="S3.SS3.p4.2.m1.2.3.3.3.3.cmml" xref="S3.SS3.p4.2.m1.2.3.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m1.2c">G^{T\prime}\in\mathbb{R}^{677\times 3}</annotation></semantics></math>, <math id="S3.SS3.p4.3.m2.2" class="ltx_Math" alttext="L_{m}^{i\prime}" display="inline"><semantics id="S3.SS3.p4.3.m2.2a"><msubsup id="S3.SS3.p4.3.m2.2.3" xref="S3.SS3.p4.3.m2.2.3.cmml"><mi id="S3.SS3.p4.3.m2.2.3.2.2" xref="S3.SS3.p4.3.m2.2.3.2.2.cmml">L</mi><mi id="S3.SS3.p4.3.m2.2.3.2.3" xref="S3.SS3.p4.3.m2.2.3.2.3.cmml">m</mi><mrow id="S3.SS3.p4.3.m2.2.2.2.2" xref="S3.SS3.p4.3.m2.2.2.2.3.cmml"><mi id="S3.SS3.p4.3.m2.1.1.1.1" xref="S3.SS3.p4.3.m2.1.1.1.1.cmml">i</mi><mo lspace="0em" id="S3.SS3.p4.3.m2.2.2.2.2.2" xref="S3.SS3.p4.3.m2.2.2.2.3.cmml">⁣</mo><mo mathsize="142%" id="S3.SS3.p4.3.m2.2.2.2.2.1" xref="S3.SS3.p4.3.m2.2.2.2.2.1.cmml">′</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m2.2b"><apply id="S3.SS3.p4.3.m2.2.3.cmml" xref="S3.SS3.p4.3.m2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m2.2.3.1.cmml" xref="S3.SS3.p4.3.m2.2.3">superscript</csymbol><apply id="S3.SS3.p4.3.m2.2.3.2.cmml" xref="S3.SS3.p4.3.m2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m2.2.3.2.1.cmml" xref="S3.SS3.p4.3.m2.2.3">subscript</csymbol><ci id="S3.SS3.p4.3.m2.2.3.2.2.cmml" xref="S3.SS3.p4.3.m2.2.3.2.2">𝐿</ci><ci id="S3.SS3.p4.3.m2.2.3.2.3.cmml" xref="S3.SS3.p4.3.m2.2.3.2.3">𝑚</ci></apply><list id="S3.SS3.p4.3.m2.2.2.2.3.cmml" xref="S3.SS3.p4.3.m2.2.2.2.2"><ci id="S3.SS3.p4.3.m2.1.1.1.1.cmml" xref="S3.SS3.p4.3.m2.1.1.1.1">𝑖</ci><ci id="S3.SS3.p4.3.m2.2.2.2.2.1.cmml" xref="S3.SS3.p4.3.m2.2.2.2.2.1">′</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m2.2c">L_{m}^{i\prime}</annotation></semantics></math>, <math id="S3.SS3.p4.4.m3.2" class="ltx_Math" alttext="L_{m}^{d\prime}" display="inline"><semantics id="S3.SS3.p4.4.m3.2a"><msubsup id="S3.SS3.p4.4.m3.2.3" xref="S3.SS3.p4.4.m3.2.3.cmml"><mi id="S3.SS3.p4.4.m3.2.3.2.2" xref="S3.SS3.p4.4.m3.2.3.2.2.cmml">L</mi><mi id="S3.SS3.p4.4.m3.2.3.2.3" xref="S3.SS3.p4.4.m3.2.3.2.3.cmml">m</mi><mrow id="S3.SS3.p4.4.m3.2.2.2.2" xref="S3.SS3.p4.4.m3.2.2.2.3.cmml"><mi id="S3.SS3.p4.4.m3.1.1.1.1" xref="S3.SS3.p4.4.m3.1.1.1.1.cmml">d</mi><mo lspace="0em" id="S3.SS3.p4.4.m3.2.2.2.2.2" xref="S3.SS3.p4.4.m3.2.2.2.3.cmml">⁣</mo><mo mathsize="142%" id="S3.SS3.p4.4.m3.2.2.2.2.1" xref="S3.SS3.p4.4.m3.2.2.2.2.1.cmml">′</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m3.2b"><apply id="S3.SS3.p4.4.m3.2.3.cmml" xref="S3.SS3.p4.4.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m3.2.3.1.cmml" xref="S3.SS3.p4.4.m3.2.3">superscript</csymbol><apply id="S3.SS3.p4.4.m3.2.3.2.cmml" xref="S3.SS3.p4.4.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m3.2.3.2.1.cmml" xref="S3.SS3.p4.4.m3.2.3">subscript</csymbol><ci id="S3.SS3.p4.4.m3.2.3.2.2.cmml" xref="S3.SS3.p4.4.m3.2.3.2.2">𝐿</ci><ci id="S3.SS3.p4.4.m3.2.3.2.3.cmml" xref="S3.SS3.p4.4.m3.2.3.2.3">𝑚</ci></apply><list id="S3.SS3.p4.4.m3.2.2.2.3.cmml" xref="S3.SS3.p4.4.m3.2.2.2.2"><ci id="S3.SS3.p4.4.m3.1.1.1.1.cmml" xref="S3.SS3.p4.4.m3.1.1.1.1">𝑑</ci><ci id="S3.SS3.p4.4.m3.2.2.2.2.1.cmml" xref="S3.SS3.p4.4.m3.2.2.2.2.1">′</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m3.2c">L_{m}^{d\prime}</annotation></semantics></math>, and <math id="S3.SS3.p4.5.m4.2" class="ltx_Math" alttext="L^{r\prime}\in\mathbb{R}^{49\times 3}" display="inline"><semantics id="S3.SS3.p4.5.m4.2a"><mrow id="S3.SS3.p4.5.m4.2.3" xref="S3.SS3.p4.5.m4.2.3.cmml"><msup id="S3.SS3.p4.5.m4.2.3.2" xref="S3.SS3.p4.5.m4.2.3.2.cmml"><mi id="S3.SS3.p4.5.m4.2.3.2.2" xref="S3.SS3.p4.5.m4.2.3.2.2.cmml">L</mi><mrow id="S3.SS3.p4.5.m4.2.2.2.2" xref="S3.SS3.p4.5.m4.2.2.2.3.cmml"><mi id="S3.SS3.p4.5.m4.1.1.1.1" xref="S3.SS3.p4.5.m4.1.1.1.1.cmml">r</mi><mo lspace="0em" id="S3.SS3.p4.5.m4.2.2.2.2.2" xref="S3.SS3.p4.5.m4.2.2.2.3.cmml">⁣</mo><mo mathsize="142%" id="S3.SS3.p4.5.m4.2.2.2.2.1" xref="S3.SS3.p4.5.m4.2.2.2.2.1.cmml">′</mo></mrow></msup><mo id="S3.SS3.p4.5.m4.2.3.1" xref="S3.SS3.p4.5.m4.2.3.1.cmml">∈</mo><msup id="S3.SS3.p4.5.m4.2.3.3" xref="S3.SS3.p4.5.m4.2.3.3.cmml"><mi id="S3.SS3.p4.5.m4.2.3.3.2" xref="S3.SS3.p4.5.m4.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p4.5.m4.2.3.3.3" xref="S3.SS3.p4.5.m4.2.3.3.3.cmml"><mn id="S3.SS3.p4.5.m4.2.3.3.3.2" xref="S3.SS3.p4.5.m4.2.3.3.3.2.cmml">49</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.5.m4.2.3.3.3.1" xref="S3.SS3.p4.5.m4.2.3.3.3.1.cmml">×</mo><mn id="S3.SS3.p4.5.m4.2.3.3.3.3" xref="S3.SS3.p4.5.m4.2.3.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m4.2b"><apply id="S3.SS3.p4.5.m4.2.3.cmml" xref="S3.SS3.p4.5.m4.2.3"><in id="S3.SS3.p4.5.m4.2.3.1.cmml" xref="S3.SS3.p4.5.m4.2.3.1"></in><apply id="S3.SS3.p4.5.m4.2.3.2.cmml" xref="S3.SS3.p4.5.m4.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m4.2.3.2.1.cmml" xref="S3.SS3.p4.5.m4.2.3.2">superscript</csymbol><ci id="S3.SS3.p4.5.m4.2.3.2.2.cmml" xref="S3.SS3.p4.5.m4.2.3.2.2">𝐿</ci><list id="S3.SS3.p4.5.m4.2.2.2.3.cmml" xref="S3.SS3.p4.5.m4.2.2.2.2"><ci id="S3.SS3.p4.5.m4.1.1.1.1.cmml" xref="S3.SS3.p4.5.m4.1.1.1.1">𝑟</ci><ci id="S3.SS3.p4.5.m4.2.2.2.2.1.cmml" xref="S3.SS3.p4.5.m4.2.2.2.2.1">′</ci></list></apply><apply id="S3.SS3.p4.5.m4.2.3.3.cmml" xref="S3.SS3.p4.5.m4.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m4.2.3.3.1.cmml" xref="S3.SS3.p4.5.m4.2.3.3">superscript</csymbol><ci id="S3.SS3.p4.5.m4.2.3.3.2.cmml" xref="S3.SS3.p4.5.m4.2.3.3.2">ℝ</ci><apply id="S3.SS3.p4.5.m4.2.3.3.3.cmml" xref="S3.SS3.p4.5.m4.2.3.3.3"><times id="S3.SS3.p4.5.m4.2.3.3.3.1.cmml" xref="S3.SS3.p4.5.m4.2.3.3.3.1"></times><cn type="integer" id="S3.SS3.p4.5.m4.2.3.3.3.2.cmml" xref="S3.SS3.p4.5.m4.2.3.3.3.2">49</cn><cn type="integer" id="S3.SS3.p4.5.m4.2.3.3.3.3.cmml" xref="S3.SS3.p4.5.m4.2.3.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m4.2c">L^{r\prime}\in\mathbb{R}^{49\times 3}</annotation></semantics></math>. <math id="S3.SS3.p4.6.m5.1" class="ltx_Math" alttext="\Phi_{f}" display="inline"><semantics id="S3.SS3.p4.6.m5.1a"><msub id="S3.SS3.p4.6.m5.1.1" xref="S3.SS3.p4.6.m5.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p4.6.m5.1.1.2" xref="S3.SS3.p4.6.m5.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p4.6.m5.1.1.3" xref="S3.SS3.p4.6.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m5.1b"><apply id="S3.SS3.p4.6.m5.1.1.cmml" xref="S3.SS3.p4.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.6.m5.1.1.1.cmml" xref="S3.SS3.p4.6.m5.1.1">subscript</csymbol><ci id="S3.SS3.p4.6.m5.1.1.2.cmml" xref="S3.SS3.p4.6.m5.1.1.2">Φ</ci><ci id="S3.SS3.p4.6.m5.1.1.3.cmml" xref="S3.SS3.p4.6.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m5.1c">\Phi_{f}</annotation></semantics></math> is implemented with a three-layer Transformer module that uses several attention heads in parallel to fuse global and local features.
While attending to valid features and restricting undesirable features, FTM <math id="S3.SS3.p4.7.m6.1" class="ltx_Math" alttext="\Phi_{f}" display="inline"><semantics id="S3.SS3.p4.7.m6.1a"><msub id="S3.SS3.p4.7.m6.1.1" xref="S3.SS3.p4.7.m6.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p4.7.m6.1.1.2" xref="S3.SS3.p4.7.m6.1.1.2.cmml">Φ</mi><mi id="S3.SS3.p4.7.m6.1.1.3" xref="S3.SS3.p4.7.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m6.1b"><apply id="S3.SS3.p4.7.m6.1.1.cmml" xref="S3.SS3.p4.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m6.1.1.1.cmml" xref="S3.SS3.p4.7.m6.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m6.1.1.2.cmml" xref="S3.SS3.p4.7.m6.1.1.2">Φ</ci><ci id="S3.SS3.p4.7.m6.1.1.3.cmml" xref="S3.SS3.p4.7.m6.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m6.1c">\Phi_{f}</annotation></semantics></math> adopts attention between joint/vertex queries <math id="S3.SS3.p4.8.m7.1" class="ltx_Math" alttext="G^{T}" display="inline"><semantics id="S3.SS3.p4.8.m7.1a"><msup id="S3.SS3.p4.8.m7.1.1" xref="S3.SS3.p4.8.m7.1.1.cmml"><mi id="S3.SS3.p4.8.m7.1.1.2" xref="S3.SS3.p4.8.m7.1.1.2.cmml">G</mi><mi id="S3.SS3.p4.8.m7.1.1.3" xref="S3.SS3.p4.8.m7.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m7.1b"><apply id="S3.SS3.p4.8.m7.1.1.cmml" xref="S3.SS3.p4.8.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m7.1.1.1.cmml" xref="S3.SS3.p4.8.m7.1.1">superscript</csymbol><ci id="S3.SS3.p4.8.m7.1.1.2.cmml" xref="S3.SS3.p4.8.m7.1.1.2">𝐺</ci><ci id="S3.SS3.p4.8.m7.1.1.3.cmml" xref="S3.SS3.p4.8.m7.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m7.1c">G^{T}</annotation></semantics></math> generated from global features <math id="S3.SS3.p4.9.m8.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p4.9.m8.1a"><mi id="S3.SS3.p4.9.m8.1.1" xref="S3.SS3.p4.9.m8.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m8.1b"><ci id="S3.SS3.p4.9.m8.1.1.cmml" xref="S3.SS3.p4.9.m8.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m8.1c">G</annotation></semantics></math> and modality tokens from local features <math id="S3.SS3.p4.10.m9.3" class="ltx_Math" alttext="L_{m}^{i},L_{m}^{d},L^{r}" display="inline"><semantics id="S3.SS3.p4.10.m9.3a"><mrow id="S3.SS3.p4.10.m9.3.3.3" xref="S3.SS3.p4.10.m9.3.3.4.cmml"><msubsup id="S3.SS3.p4.10.m9.1.1.1.1" xref="S3.SS3.p4.10.m9.1.1.1.1.cmml"><mi id="S3.SS3.p4.10.m9.1.1.1.1.2.2" xref="S3.SS3.p4.10.m9.1.1.1.1.2.2.cmml">L</mi><mi id="S3.SS3.p4.10.m9.1.1.1.1.2.3" xref="S3.SS3.p4.10.m9.1.1.1.1.2.3.cmml">m</mi><mi id="S3.SS3.p4.10.m9.1.1.1.1.3" xref="S3.SS3.p4.10.m9.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.SS3.p4.10.m9.3.3.3.4" xref="S3.SS3.p4.10.m9.3.3.4.cmml">,</mo><msubsup id="S3.SS3.p4.10.m9.2.2.2.2" xref="S3.SS3.p4.10.m9.2.2.2.2.cmml"><mi id="S3.SS3.p4.10.m9.2.2.2.2.2.2" xref="S3.SS3.p4.10.m9.2.2.2.2.2.2.cmml">L</mi><mi id="S3.SS3.p4.10.m9.2.2.2.2.2.3" xref="S3.SS3.p4.10.m9.2.2.2.2.2.3.cmml">m</mi><mi id="S3.SS3.p4.10.m9.2.2.2.2.3" xref="S3.SS3.p4.10.m9.2.2.2.2.3.cmml">d</mi></msubsup><mo id="S3.SS3.p4.10.m9.3.3.3.5" xref="S3.SS3.p4.10.m9.3.3.4.cmml">,</mo><msup id="S3.SS3.p4.10.m9.3.3.3.3" xref="S3.SS3.p4.10.m9.3.3.3.3.cmml"><mi id="S3.SS3.p4.10.m9.3.3.3.3.2" xref="S3.SS3.p4.10.m9.3.3.3.3.2.cmml">L</mi><mi id="S3.SS3.p4.10.m9.3.3.3.3.3" xref="S3.SS3.p4.10.m9.3.3.3.3.3.cmml">r</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.10.m9.3b"><list id="S3.SS3.p4.10.m9.3.3.4.cmml" xref="S3.SS3.p4.10.m9.3.3.3"><apply id="S3.SS3.p4.10.m9.1.1.1.1.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m9.1.1.1.1.1.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p4.10.m9.1.1.1.1.2.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m9.1.1.1.1.2.1.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.10.m9.1.1.1.1.2.2.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1.2.2">𝐿</ci><ci id="S3.SS3.p4.10.m9.1.1.1.1.2.3.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1.2.3">𝑚</ci></apply><ci id="S3.SS3.p4.10.m9.1.1.1.1.3.cmml" xref="S3.SS3.p4.10.m9.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.p4.10.m9.2.2.2.2.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m9.2.2.2.2.1.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p4.10.m9.2.2.2.2.2.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m9.2.2.2.2.2.1.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.10.m9.2.2.2.2.2.2.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2.2.2">𝐿</ci><ci id="S3.SS3.p4.10.m9.2.2.2.2.2.3.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2.2.3">𝑚</ci></apply><ci id="S3.SS3.p4.10.m9.2.2.2.2.3.cmml" xref="S3.SS3.p4.10.m9.2.2.2.2.3">𝑑</ci></apply><apply id="S3.SS3.p4.10.m9.3.3.3.3.cmml" xref="S3.SS3.p4.10.m9.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m9.3.3.3.3.1.cmml" xref="S3.SS3.p4.10.m9.3.3.3.3">superscript</csymbol><ci id="S3.SS3.p4.10.m9.3.3.3.3.2.cmml" xref="S3.SS3.p4.10.m9.3.3.3.3.2">𝐿</ci><ci id="S3.SS3.p4.10.m9.3.3.3.3.3.cmml" xref="S3.SS3.p4.10.m9.3.3.3.3.3">𝑟</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.10.m9.3c">L_{m}^{i},L_{m}^{d},L^{r}</annotation></semantics></math> to aggregate relevant contextual information for multi-modal input. Additionally, the self-attention mechanism helps reason interrelations between each pair of candidate queries for single-modal input. Then, we adopt a dimension-reduction architecture, Graph Convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, to decode the queries <math id="S3.SS3.p4.11.m10.2" class="ltx_Math" alttext="G^{T\prime}" display="inline"><semantics id="S3.SS3.p4.11.m10.2a"><msup id="S3.SS3.p4.11.m10.2.3" xref="S3.SS3.p4.11.m10.2.3.cmml"><mi id="S3.SS3.p4.11.m10.2.3.2" xref="S3.SS3.p4.11.m10.2.3.2.cmml">G</mi><mrow id="S3.SS3.p4.11.m10.2.2.2.2" xref="S3.SS3.p4.11.m10.2.2.2.3.cmml"><mi id="S3.SS3.p4.11.m10.1.1.1.1" xref="S3.SS3.p4.11.m10.1.1.1.1.cmml">T</mi><mo lspace="0em" id="S3.SS3.p4.11.m10.2.2.2.2.2" xref="S3.SS3.p4.11.m10.2.2.2.3.cmml">⁣</mo><mo mathsize="142%" id="S3.SS3.p4.11.m10.2.2.2.2.1" xref="S3.SS3.p4.11.m10.2.2.2.2.1.cmml">′</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.11.m10.2b"><apply id="S3.SS3.p4.11.m10.2.3.cmml" xref="S3.SS3.p4.11.m10.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.11.m10.2.3.1.cmml" xref="S3.SS3.p4.11.m10.2.3">superscript</csymbol><ci id="S3.SS3.p4.11.m10.2.3.2.cmml" xref="S3.SS3.p4.11.m10.2.3.2">𝐺</ci><list id="S3.SS3.p4.11.m10.2.2.2.3.cmml" xref="S3.SS3.p4.11.m10.2.2.2.2"><ci id="S3.SS3.p4.11.m10.1.1.1.1.cmml" xref="S3.SS3.p4.11.m10.1.1.1.1">𝑇</ci><ci id="S3.SS3.p4.11.m10.2.2.2.2.1.cmml" xref="S3.SS3.p4.11.m10.2.2.2.2.1">′</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.11.m10.2c">G^{T\prime}</annotation></semantics></math> containing rich cross-modalities information into 3D coordinates of joints and vertices following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Last, a linear projection network implemented using MLPs upsamples the coarse output mesh to the original 10475 vertices.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Modality Sampling and Masking</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To make our model more versatile, we design a modality sampling module to randomly sample one combination of input modalities during a forward training process. This simple yet effective strategy ensures the model encounters all possible combinations of modalities within a single training session, enabling the model to adapt to different modality combinations without retraining. Besides, we empirically observe that our strategy surprisingly enables one combination to benefit from other combinations: even with a small portion of training data for each modality combination, our model achieves better results than the performance of models training with fixed modality as demonstrated in <a href="#S4.F2" title="In IV-B1 Effectiveness Analysis ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. For the current era of large models where training often involves datasets on the scale of millions, this discovery is highly advantageous.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Despite the superiority of the multi-head attention mechanism, the model is prone to struggle with data imbalance of training data (without data under adverse conditions) for multi-modal input according to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which makes the Transformer focus all attention on the single modality that performs better under normal circumstances like image or depth data.
Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, to effectively activate the potential of the model across all scenarios,
we utilize a modality masking module to randomly mask some of the modalities for multi-modal input and thus enforce the model to learn from all modalities in various situations.
As a result, this module enables our model to overcome the training data bias problem and consider all modalities, which further facilitates the model to perform better across all scenarios in our experiments.
For the mask proportion, we set it to 30% in our experiments as it achieves the best accuracy.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Mean errors (cm) of different methods for 3D body reconstruction in different scenes of mmBody dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. For the two columns of each scene, the first column is for MPJPE and the second is for MPVE.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:142.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.0pt,27.9pt) scale(0.718342068975977,0.718342068975977) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text">Methods</span></th>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="6">Basic Scenes</td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="8">Adverse Environments</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2" rowspan="2"><span id="S3.T2.1.1.1.1.4.1" class="ltx_text">Average</span></td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Lab1</span></td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T2.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Lab2</span></td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S3.T2.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Furnished</span></td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T2.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Rain</span></td>
<td id="S3.T2.1.1.2.2.5" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T2.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Smoke</span></td>
<td id="S3.T2.1.1.2.2.6" class="ltx_td ltx_align_center" colspan="2"><span id="S3.T2.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Poor Lighting</span></td>
<td id="S3.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S3.T2.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Occlusion</span></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<th id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">4.3</td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.4</td>
<td id="S3.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">4.1</td>
<td id="S3.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.3</td>
<td id="S3.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">5.6</td>
<td id="S3.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.2</td>
<td id="S3.T2.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">5.0</td>
<td id="S3.T2.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.3</td>
<td id="S3.T2.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
<td id="S3.T2.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.3</td>
<td id="S3.T2.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">5.2</td>
<td id="S3.T2.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.2</td>
<td id="S3.T2.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_t">7.8</td>
<td id="S3.T2.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.5</td>
<td id="S3.T2.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_t">5.7</td>
<td id="S3.T2.1.1.3.3.17" class="ltx_td ltx_align_center ltx_border_t">7.2</td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<th id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_center">4.4</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">5.9</td>
<td id="S3.T2.1.1.4.4.4" class="ltx_td ltx_align_center">4.5</td>
<td id="S3.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">6.0</td>
<td id="S3.T2.1.1.4.4.6" class="ltx_td ltx_align_center">5.3</td>
<td id="S3.T2.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r">7.0</td>
<td id="S3.T2.1.1.4.4.8" class="ltx_td ltx_align_center">5.1</td>
<td id="S3.T2.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r">6.6</td>
<td id="S3.T2.1.1.4.4.10" class="ltx_td ltx_align_center">8.2</td>
<td id="S3.T2.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r">10.6</td>
<td id="S3.T2.1.1.4.4.12" class="ltx_td ltx_align_center">9.4</td>
<td id="S3.T2.1.1.4.4.13" class="ltx_td ltx_align_center ltx_border_r">13.6</td>
<td id="S3.T2.1.1.4.4.14" class="ltx_td ltx_align_center">9.2</td>
<td id="S3.T2.1.1.4.4.15" class="ltx_td ltx_align_center ltx_border_r">12.9</td>
<td id="S3.T2.1.1.4.4.16" class="ltx_td ltx_align_center">6.6</td>
<td id="S3.T2.1.1.4.4.17" class="ltx_td ltx_align_center">8.9</td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<th id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_center">4.0</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">5.3</td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_center">4.1</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">5.3</td>
<td id="S3.T2.1.1.5.5.6" class="ltx_td ltx_align_center">4.7</td>
<td id="S3.T2.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">6.0</td>
<td id="S3.T2.1.1.5.5.8" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.5.5.8.1" class="ltx_text ltx_framed ltx_framed_underline">4.3</span></td>
<td id="S3.T2.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.5.5.9.1" class="ltx_text ltx_framed ltx_framed_underline">5.4</span></td>
<td id="S3.T2.1.1.5.5.10" class="ltx_td ltx_align_center">7.8</td>
<td id="S3.T2.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r">10.2</td>
<td id="S3.T2.1.1.5.5.12" class="ltx_td ltx_align_center">4.4</td>
<td id="S3.T2.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_r">5.7</td>
<td id="S3.T2.1.1.5.5.14" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.5.5.14.1" class="ltx_text ltx_framed ltx_framed_underline">7.2</span></td>
<td id="S3.T2.1.1.5.5.15" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.5.5.15.1" class="ltx_text ltx_framed ltx_framed_underline">9.5</span></td>
<td id="S3.T2.1.1.5.5.16" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.5.5.16.1" class="ltx_text ltx_framed ltx_framed_underline">5.3</span></td>
<td id="S3.T2.1.1.5.5.17" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.5.5.17.1" class="ltx_text ltx_framed ltx_framed_underline">6.8</span></td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<th id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ImmFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_center">4.1</td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">5.4</td>
<td id="S3.T2.1.1.6.6.4" class="ltx_td ltx_align_center">3.7</td>
<td id="S3.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r">4.7</td>
<td id="S3.T2.1.1.6.6.6" class="ltx_td ltx_align_center">5.2</td>
<td id="S3.T2.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r">6.4</td>
<td id="S3.T2.1.1.6.6.8" class="ltx_td ltx_align_center">5.6</td>
<td id="S3.T2.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r">6.8</td>
<td id="S3.T2.1.1.6.6.10" class="ltx_td ltx_align_center">7.6</td>
<td id="S3.T2.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r">9.8</td>
<td id="S3.T2.1.1.6.6.12" class="ltx_td ltx_align_center">6.8</td>
<td id="S3.T2.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_r">9.0</td>
<td id="S3.T2.1.1.6.6.14" class="ltx_td ltx_align_center">7.8</td>
<td id="S3.T2.1.1.6.6.15" class="ltx_td ltx_align_center ltx_border_r">11.0</td>
<td id="S3.T2.1.1.6.6.16" class="ltx_td ltx_align_center">5.9</td>
<td id="S3.T2.1.1.6.6.17" class="ltx_td ltx_align_center">7.4</td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<th id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">AdaptiveFusion (img1)</th>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">4.3</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.0</td>
<td id="S3.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">4.5</td>
<td id="S3.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.9</td>
<td id="S3.T2.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">5.9</td>
<td id="S3.T2.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.3</td>
<td id="S3.T2.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t">5.5</td>
<td id="S3.T2.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.5</td>
<td id="S3.T2.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t">8.6</td>
<td id="S3.T2.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.6</td>
<td id="S3.T2.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_t">9.8</td>
<td id="S3.T2.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.8</td>
<td id="S3.T2.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_t">9.9</td>
<td id="S3.T2.1.1.7.7.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.7</td>
<td id="S3.T2.1.1.7.7.16" class="ltx_td ltx_align_center ltx_border_t">6.9</td>
<td id="S3.T2.1.1.7.7.17" class="ltx_td ltx_align_center ltx_border_t">9.2</td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<th id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaptiveFusion (dep1)</th>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.8.2.1" class="ltx_text ltx_framed ltx_framed_underline">3.3</span></td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.8.8.3.1" class="ltx_text ltx_framed ltx_framed_underline">4.3</span></td>
<td id="S3.T2.1.1.8.8.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.8.4.1" class="ltx_text ltx_framed ltx_framed_underline">3.4</span></td>
<td id="S3.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.8.8.5.1" class="ltx_text ltx_framed ltx_framed_underline">4.3</span></td>
<td id="S3.T2.1.1.8.8.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.8.6.1" class="ltx_text ltx_framed ltx_framed_underline">4.0</span></td>
<td id="S3.T2.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.8.8.7.1" class="ltx_text ltx_framed ltx_framed_underline">4.9</span></td>
<td id="S3.T2.1.1.8.8.8" class="ltx_td ltx_align_center">5.3</td>
<td id="S3.T2.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r">6.5</td>
<td id="S3.T2.1.1.8.8.10" class="ltx_td ltx_align_center">10.6</td>
<td id="S3.T2.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r">15.0</td>
<td id="S3.T2.1.1.8.8.12" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.8.8.12.1" class="ltx_text ltx_framed ltx_framed_underline">3.5</span></td>
<td id="S3.T2.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.8.8.13.1" class="ltx_text ltx_framed ltx_framed_underline">4.4</span></td>
<td id="S3.T2.1.1.8.8.14" class="ltx_td ltx_align_center">9.4</td>
<td id="S3.T2.1.1.8.8.15" class="ltx_td ltx_align_center ltx_border_r">14.0</td>
<td id="S3.T2.1.1.8.8.16" class="ltx_td ltx_align_center">5.6</td>
<td id="S3.T2.1.1.8.8.17" class="ltx_td ltx_align_center">7.6</td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<th id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaptiveFusion (radar)</th>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_center">6.1</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">8.1</td>
<td id="S3.T2.1.1.9.9.4" class="ltx_td ltx_align_center">5.5</td>
<td id="S3.T2.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r">7.4</td>
<td id="S3.T2.1.1.9.9.6" class="ltx_td ltx_align_center">5.9</td>
<td id="S3.T2.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r">8.1</td>
<td id="S3.T2.1.1.9.9.8" class="ltx_td ltx_align_center">7.0</td>
<td id="S3.T2.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r">8.8</td>
<td id="S3.T2.1.1.9.9.10" class="ltx_td ltx_align_center">8.1</td>
<td id="S3.T2.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r">10.1</td>
<td id="S3.T2.1.1.9.9.12" class="ltx_td ltx_align_center">5.8</td>
<td id="S3.T2.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_r">7.9</td>
<td id="S3.T2.1.1.9.9.14" class="ltx_td ltx_align_center">8.0</td>
<td id="S3.T2.1.1.9.9.15" class="ltx_td ltx_align_center ltx_border_r">10.7</td>
<td id="S3.T2.1.1.9.9.16" class="ltx_td ltx_align_center">6.7</td>
<td id="S3.T2.1.1.9.9.17" class="ltx_td ltx_align_center">8.7</td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<th id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaptiveFusion (img1-radar)</th>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_center">4.1</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">5.4</td>
<td id="S3.T2.1.1.10.10.4" class="ltx_td ltx_align_center">4.2</td>
<td id="S3.T2.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r">5.4</td>
<td id="S3.T2.1.1.10.10.6" class="ltx_td ltx_align_center">5.2</td>
<td id="S3.T2.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r">6.3</td>
<td id="S3.T2.1.1.10.10.8" class="ltx_td ltx_align_center">5.2</td>
<td id="S3.T2.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r">6.0</td>
<td id="S3.T2.1.1.10.10.10" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.10.10.10.1" class="ltx_text ltx_framed ltx_framed_underline">7.0</span></td>
<td id="S3.T2.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.1.10.10.11.1" class="ltx_text ltx_framed ltx_framed_underline">9.0</span></td>
<td id="S3.T2.1.1.10.10.12" class="ltx_td ltx_align_center">6.4</td>
<td id="S3.T2.1.1.10.10.13" class="ltx_td ltx_align_center ltx_border_r">8.6</td>
<td id="S3.T2.1.1.10.10.14" class="ltx_td ltx_align_center">7.7</td>
<td id="S3.T2.1.1.10.10.15" class="ltx_td ltx_align_center ltx_border_r">10.1</td>
<td id="S3.T2.1.1.10.10.16" class="ltx_td ltx_align_center">5.8</td>
<td id="S3.T2.1.1.10.10.17" class="ltx_td ltx_align_center">7.3</td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<th id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">AdaptiveFusion (full-modalities)</th>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.2.1" class="ltx_text ltx_font_bold">3.1</span></td>
<td id="S3.T2.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.3.1" class="ltx_text ltx_font_bold">4.1</span></td>
<td id="S3.T2.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.4.1" class="ltx_text ltx_font_bold">3.3</span></td>
<td id="S3.T2.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.5.1" class="ltx_text ltx_font_bold">4.2</span></td>
<td id="S3.T2.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.6.1" class="ltx_text ltx_font_bold">3.7</span></td>
<td id="S3.T2.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.7.1" class="ltx_text ltx_font_bold">4.4</span></td>
<td id="S3.T2.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.8.1" class="ltx_text ltx_font_bold">3.4</span></td>
<td id="S3.T2.1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.9.1" class="ltx_text ltx_font_bold">4.4</span></td>
<td id="S3.T2.1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.10.1" class="ltx_text ltx_font_bold">5.2</span></td>
<td id="S3.T2.1.1.11.11.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.11.1" class="ltx_text ltx_font_bold">6.4</span></td>
<td id="S3.T2.1.1.11.11.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.12.1" class="ltx_text ltx_font_bold">3.5</span></td>
<td id="S3.T2.1.1.11.11.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.13.1" class="ltx_text ltx_font_bold">4.3</span></td>
<td id="S3.T2.1.1.11.11.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.14.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S3.T2.1.1.11.11.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.1.1.11.11.15.1" class="ltx_text ltx_font_bold">6.6</span></td>
<td id="S3.T2.1.1.11.11.16" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.16.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S3.T2.1.1.11.11.17" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.1.11.11.17.1" class="ltx_text ltx_font_bold">4.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Training Loss</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.7" class="ltx_p">Our AdaptiveFusion applies <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">L_{1}</annotation></semantics></math> loss to the reconstructed mesh to constrain the 3D vertices <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">V</annotation></semantics></math> and joints <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="J" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><mi id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><ci id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">J</annotation></semantics></math>. We also incorporate <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><msub id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml"><mi id="S3.SS5.p1.4.m4.1.1.2" xref="S3.SS5.p1.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS5.p1.4.m4.1.1.3" xref="S3.SS5.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><apply id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.4.m4.1.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p1.4.m4.1.1.2.cmml" xref="S3.SS5.p1.4.m4.1.1.2">𝐿</ci><cn type="integer" id="S3.SS5.p1.4.m4.1.1.3.cmml" xref="S3.SS5.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">L_{1}</annotation></semantics></math> loss to the 2D joints <math id="S3.SS5.p1.5.m5.1" class="ltx_Math" alttext="J_{2D}" display="inline"><semantics id="S3.SS5.p1.5.m5.1a"><msub id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml"><mi id="S3.SS5.p1.5.m5.1.1.2" xref="S3.SS5.p1.5.m5.1.1.2.cmml">J</mi><mrow id="S3.SS5.p1.5.m5.1.1.3" xref="S3.SS5.p1.5.m5.1.1.3.cmml"><mn id="S3.SS5.p1.5.m5.1.1.3.2" xref="S3.SS5.p1.5.m5.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS5.p1.5.m5.1.1.3.1" xref="S3.SS5.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p1.5.m5.1.1.3.3" xref="S3.SS5.p1.5.m5.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><apply id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.5.m5.1.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS5.p1.5.m5.1.1.2.cmml" xref="S3.SS5.p1.5.m5.1.1.2">𝐽</ci><apply id="S3.SS5.p1.5.m5.1.1.3.cmml" xref="S3.SS5.p1.5.m5.1.1.3"><times id="S3.SS5.p1.5.m5.1.1.3.1.cmml" xref="S3.SS5.p1.5.m5.1.1.3.1"></times><cn type="integer" id="S3.SS5.p1.5.m5.1.1.3.2.cmml" xref="S3.SS5.p1.5.m5.1.1.3.2">2</cn><ci id="S3.SS5.p1.5.m5.1.1.3.3.cmml" xref="S3.SS5.p1.5.m5.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">J_{2D}</annotation></semantics></math> obtained by projecting predicted 3D joints to the image space using estimated camera parameters. In addition, the coarse meshes <math id="S3.SS5.p1.6.m6.2" class="ltx_Math" alttext="V_{d1},V_{d2}" display="inline"><semantics id="S3.SS5.p1.6.m6.2a"><mrow id="S3.SS5.p1.6.m6.2.2.2" xref="S3.SS5.p1.6.m6.2.2.3.cmml"><msub id="S3.SS5.p1.6.m6.1.1.1.1" xref="S3.SS5.p1.6.m6.1.1.1.1.cmml"><mi id="S3.SS5.p1.6.m6.1.1.1.1.2" xref="S3.SS5.p1.6.m6.1.1.1.1.2.cmml">V</mi><mrow id="S3.SS5.p1.6.m6.1.1.1.1.3" xref="S3.SS5.p1.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS5.p1.6.m6.1.1.1.1.3.2" xref="S3.SS5.p1.6.m6.1.1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.6.m6.1.1.1.1.3.1" xref="S3.SS5.p1.6.m6.1.1.1.1.3.1.cmml">​</mo><mn id="S3.SS5.p1.6.m6.1.1.1.1.3.3" xref="S3.SS5.p1.6.m6.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS5.p1.6.m6.2.2.2.3" xref="S3.SS5.p1.6.m6.2.2.3.cmml">,</mo><msub id="S3.SS5.p1.6.m6.2.2.2.2" xref="S3.SS5.p1.6.m6.2.2.2.2.cmml"><mi id="S3.SS5.p1.6.m6.2.2.2.2.2" xref="S3.SS5.p1.6.m6.2.2.2.2.2.cmml">V</mi><mrow id="S3.SS5.p1.6.m6.2.2.2.2.3" xref="S3.SS5.p1.6.m6.2.2.2.2.3.cmml"><mi id="S3.SS5.p1.6.m6.2.2.2.2.3.2" xref="S3.SS5.p1.6.m6.2.2.2.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.6.m6.2.2.2.2.3.1" xref="S3.SS5.p1.6.m6.2.2.2.2.3.1.cmml">​</mo><mn id="S3.SS5.p1.6.m6.2.2.2.2.3.3" xref="S3.SS5.p1.6.m6.2.2.2.2.3.3.cmml">2</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.2b"><list id="S3.SS5.p1.6.m6.2.2.3.cmml" xref="S3.SS5.p1.6.m6.2.2.2"><apply id="S3.SS5.p1.6.m6.1.1.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS5.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1.2">𝑉</ci><apply id="S3.SS5.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1.3"><times id="S3.SS5.p1.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1.3.1"></times><ci id="S3.SS5.p1.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1.3.2">𝑑</ci><cn type="integer" id="S3.SS5.p1.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS5.p1.6.m6.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS5.p1.6.m6.2.2.2.2.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p1.6.m6.2.2.2.2.1.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2">subscript</csymbol><ci id="S3.SS5.p1.6.m6.2.2.2.2.2.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2.2">𝑉</ci><apply id="S3.SS5.p1.6.m6.2.2.2.2.3.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2.3"><times id="S3.SS5.p1.6.m6.2.2.2.2.3.1.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2.3.1"></times><ci id="S3.SS5.p1.6.m6.2.2.2.2.3.2.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2.3.2">𝑑</ci><cn type="integer" id="S3.SS5.p1.6.m6.2.2.2.2.3.3.cmml" xref="S3.SS5.p1.6.m6.2.2.2.2.3.3">2</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.2c">V_{d1},V_{d2}</annotation></semantics></math> are also supervised by downsampled ground truth meshes using <math id="S3.SS5.p1.7.m7.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS5.p1.7.m7.1a"><msub id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml"><mi id="S3.SS5.p1.7.m7.1.1.2" xref="S3.SS5.p1.7.m7.1.1.2.cmml">L</mi><mn id="S3.SS5.p1.7.m7.1.1.3" xref="S3.SS5.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><apply id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.7.m7.1.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS5.p1.7.m7.1.1.2.cmml" xref="S3.SS5.p1.7.m7.1.1.2">𝐿</ci><cn type="integer" id="S3.SS5.p1.7.m7.1.1.3.cmml" xref="S3.SS5.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">L_{1}</annotation></semantics></math> loss to accelerate convergence.
The total loss of AdaptiveFusion is calculated by:</p>
<table id="S3.E4" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E4X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}=\alpha\|J-\bar{J}\|_{1}+\beta\|J_{2D}" display="inline"><semantics id="S3.E4X.2.1.1.m1.1a"><mrow id="S3.E4X.2.1.1.m1.1.1" xref="S3.E4X.2.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4X.2.1.1.m1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.3.cmml">ℒ</mi><mo id="S3.E4X.2.1.1.m1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E4X.2.1.1.m1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.1.1.2.cmml">​</mo><msub id="S3.E4X.2.1.1.m1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">J</mi><mo id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.2.cmml">J</mi><mo id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo stretchy="false" id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E4X.2.1.1.m1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.3.cmml">1</mn></msub></mrow><mo id="S3.E4X.2.1.1.m1.1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E4X.2.1.1.m1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.3.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.3.2" xref="S3.E4X.2.1.1.m1.1.1.1.3.2.cmml">β</mi><mo id="S3.E4X.2.1.1.m1.1.1.1.3.1" xref="S3.E4X.2.1.1.m1.1.1.1.3.1.cmml">∥</mo><msub id="S3.E4X.2.1.1.m1.1.1.1.3.3" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.3.3.2" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.2.cmml">J</mi><mrow id="S3.E4X.2.1.1.m1.1.1.1.3.3.3" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.cmml"><mn id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.2" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.1" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.3" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.3.cmml">D</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4X.2.1.1.m1.1b"><apply id="S3.E4X.2.1.1.m1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1"><eq id="S3.E4X.2.1.1.m1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.2"></eq><ci id="S3.E4X.2.1.1.m1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.3">ℒ</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1"><plus id="S3.E4X.2.1.1.m1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.2"></plus><apply id="S3.E4X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1"><times id="S3.E4X.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.2"></times><ci id="S3.E4X.2.1.1.m1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.3">𝛼</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1"><minus id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2">𝐽</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3"><ci id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.1">¯</ci><ci id="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.2">𝐽</ci></apply></apply></apply><cn type="integer" id="S3.E4X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E4X.2.1.1.m1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.1.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.1">conditional</csymbol><ci id="S3.E4X.2.1.1.m1.1.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.2">𝛽</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.3.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.1.1.1.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.1.1.1.3.3.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.2">𝐽</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3"><times id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.1"></times><cn type="integer" id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.2">2</cn><ci id="S3.E4X.2.1.1.m1.1.1.1.3.3.3.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3.3.3.3">𝐷</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4X.2.1.1.m1.1c">\displaystyle\mathcal{L}=\alpha\|J-\bar{J}\|_{1}+\beta\|J_{2D}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4X.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle-\bar{J}_{2D}\|_{1}+\gamma(\|V-\bar{V}\|_{1}" display="inline"><semantics id="S3.E4X.3.2.2.m1.1a"><mrow id="S3.E4X.3.2.2.m1.1b"><mo id="S3.E4X.3.2.2.m1.1.2">−</mo><msub id="S3.E4X.3.2.2.m1.1.3"><mover accent="true" id="S3.E4X.3.2.2.m1.1.3.2"><mi id="S3.E4X.3.2.2.m1.1.3.2.2">J</mi><mo id="S3.E4X.3.2.2.m1.1.3.2.1">¯</mo></mover><mrow id="S3.E4X.3.2.2.m1.1.3.3"><mn id="S3.E4X.3.2.2.m1.1.3.3.2">2</mn><mo lspace="0em" rspace="0em" id="S3.E4X.3.2.2.m1.1.3.3.1">​</mo><mi id="S3.E4X.3.2.2.m1.1.3.3.3">D</mi></mrow></msub><msub id="S3.E4X.3.2.2.m1.1.4"><mo lspace="0em" rspace="0em" id="S3.E4X.3.2.2.m1.1.4.2">∥</mo><mn id="S3.E4X.3.2.2.m1.1.1.1">1</mn></msub><mo lspace="0em" id="S3.E4X.3.2.2.m1.1.5">+</mo><mi id="S3.E4X.3.2.2.m1.1.6">γ</mi><mrow id="S3.E4X.3.2.2.m1.1.7"><mo stretchy="false" id="S3.E4X.3.2.2.m1.1.7.1">(</mo><mo lspace="0em" rspace="0.167em" id="S3.E4X.3.2.2.m1.1.7.2">∥</mo><mi id="S3.E4X.3.2.2.m1.1.7.3">V</mi><mo id="S3.E4X.3.2.2.m1.1.7.4">−</mo><mover accent="true" id="S3.E4X.3.2.2.m1.1.7.5"><mi id="S3.E4X.3.2.2.m1.1.7.5.2">V</mi><mo id="S3.E4X.3.2.2.m1.1.7.5.1">¯</mo></mover><msub id="S3.E4X.3.2.2.m1.1.7.6"><mo lspace="0em" id="S3.E4X.3.2.2.m1.1.7.6.2">∥</mo><mn id="S3.E4X.3.2.2.m1.1.7.6.3">1</mn></msub></mrow></mrow><annotation encoding="application/x-tex" id="S3.E4X.3.2.2.m1.1c">\displaystyle-\bar{J}_{2D}\|_{1}+\gamma(\|V-\bar{V}\|_{1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
<tr id="S3.E4Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle+\|V_{d1}-\bar{V}_{d1}\|_{1}" display="inline"><semantics id="S3.E4Xa.2.1.1.m1.1a"><mrow id="S3.E4Xa.2.1.1.m1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.cmml"><mo id="S3.E4Xa.2.1.1.m1.1.1a" xref="S3.E4Xa.2.1.1.m1.1.1.cmml">+</mo><msub id="S3.E4Xa.2.1.1.m1.1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.cmml"><mrow id="S3.E4Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">V</mi><mrow id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mn id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.2.cmml">V</mi><mo id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mrow id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.2" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.1" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mn id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E4Xa.2.1.1.m1.1.1.1.3" xref="S3.E4Xa.2.1.1.m1.1.1.1.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E4Xa.2.1.1.m1.1b"><apply id="S3.E4Xa.2.1.1.m1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1"><plus id="S3.E4Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1"></plus><apply id="S3.E4Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E4Xa.2.1.1.m1.1.1.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1">subscript</csymbol><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4Xa.2.1.1.m1.1.1.1.1.2.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1"><minus id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.1"></minus><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.2">𝑉</ci><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3"><times id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.2">𝑑</ci><cn type="integer" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2"><ci id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.2.2">𝑉</ci></apply><apply id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3"><times id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.2">𝑑</ci><cn type="integer" id="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply><cn type="integer" id="S3.E4Xa.2.1.1.m1.1.1.1.3.cmml" xref="S3.E4Xa.2.1.1.m1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4Xa.2.1.1.m1.1c">\displaystyle+\|V_{d1}-\bar{V}_{d1}\|_{1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4Xa.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle+\|V_{d2}-\bar{V}_{d2}\|_{1})," display="inline"><semantics id="S3.E4Xa.3.2.2.m1.1a"><mrow id="S3.E4Xa.3.2.2.m1.1b"><mo rspace="0em" id="S3.E4Xa.3.2.2.m1.1.1">+</mo><mo lspace="0em" rspace="0.167em" id="S3.E4Xa.3.2.2.m1.1.2">∥</mo><msub id="S3.E4Xa.3.2.2.m1.1.3"><mi id="S3.E4Xa.3.2.2.m1.1.3.2">V</mi><mrow id="S3.E4Xa.3.2.2.m1.1.3.3"><mi id="S3.E4Xa.3.2.2.m1.1.3.3.2">d</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.3.2.2.m1.1.3.3.1">​</mo><mn id="S3.E4Xa.3.2.2.m1.1.3.3.3">2</mn></mrow></msub><mo id="S3.E4Xa.3.2.2.m1.1.4">−</mo><msub id="S3.E4Xa.3.2.2.m1.1.5"><mover accent="true" id="S3.E4Xa.3.2.2.m1.1.5.2"><mi id="S3.E4Xa.3.2.2.m1.1.5.2.2">V</mi><mo id="S3.E4Xa.3.2.2.m1.1.5.2.1">¯</mo></mover><mrow id="S3.E4Xa.3.2.2.m1.1.5.3"><mi id="S3.E4Xa.3.2.2.m1.1.5.3.2">d</mi><mo lspace="0em" rspace="0em" id="S3.E4Xa.3.2.2.m1.1.5.3.1">​</mo><mn id="S3.E4Xa.3.2.2.m1.1.5.3.3">2</mn></mrow></msub><msub id="S3.E4Xa.3.2.2.m1.1.6"><mo lspace="0em" rspace="0.167em" id="S3.E4Xa.3.2.2.m1.1.6.2">∥</mo><mn id="S3.E4Xa.3.2.2.m1.1.6.3">1</mn></msub><mo stretchy="false" id="S3.E4Xa.3.2.2.m1.1.7">)</mo><mo id="S3.E4Xa.3.2.2.m1.1.8">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E4Xa.3.2.2.m1.1c">\displaystyle+\|V_{d2}-\bar{V}_{d2}\|_{1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS5.p1.10" class="ltx_p">where <math id="S3.SS5.p1.8.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS5.p1.8.m1.1a"><mi id="S3.SS5.p1.8.m1.1.1" xref="S3.SS5.p1.8.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.8.m1.1b"><ci id="S3.SS5.p1.8.m1.1.1.cmml" xref="S3.SS5.p1.8.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.8.m1.1c">\alpha</annotation></semantics></math>, <math id="S3.SS5.p1.9.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS5.p1.9.m2.1a"><mi id="S3.SS5.p1.9.m2.1.1" xref="S3.SS5.p1.9.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.9.m2.1b"><ci id="S3.SS5.p1.9.m2.1.1.cmml" xref="S3.SS5.p1.9.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.9.m2.1c">\beta</annotation></semantics></math>, and <math id="S3.SS5.p1.10.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS5.p1.10.m3.1a"><mi id="S3.SS5.p1.10.m3.1.1" xref="S3.SS5.p1.10.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.10.m3.1b"><ci id="S3.SS5.p1.10.m3.1.1.cmml" xref="S3.SS5.p1.10.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.10.m3.1c">\gamma</annotation></semantics></math> denote the weight of each component, and variables with overline represent the ground truth.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Comparison with Relevant Methods</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">We compare our proposed AdaptiveFusion with other relevant fusion methods, i.e. DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which show the state-of-the-art accuracy in the 3D object detection task.
We extend these methods to be applicable to 3D human body reconstruction, and the frameworks are illustrated in <a href="#S3.F1" title="In III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. For DeepFusion and TokenFusion, we employ the parametric reconstruction pipeline by replacing the detection head of the two methods with linear projection to regress SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> parameters. For FUTR3D, we follow its original design and implement the non-parametric pipeline by adding positional embedding of the human body template into the fused features. Subsequently, we employ a Transformer decoder to directly regress joints and vertices.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">DeepFusion employs Transformer to integrate local features extracted from multiple modalities.
This strategy shows the limited capability of leveraging information from other vertices for global interaction due to the absence of global features. Besides, it requires the LiDAR point clouds as the main modality and cannot work with different combinations of modalities, e.g.only RGB images. The framework of DeepFusion for 3D body reconstruction is shown in <a href="#S3.F1" title="In III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> (b).
AdaptiveFusion makes a further step to capture the global context by incorporating global features extracted from GIM and it supports arbitrary combinations of input modalities.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">For TokenFusion in <a href="#S3.F1" title="In III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> (c), it aims to substitute unimportant modality tokens detected by Score Nets with features from other modality streams, and the scores are learned with inter-modal projections among Transformer layers. The design still cannot support arbitrary numbers and orders of modalities. Also, it is ineffective to incorporate the modality streams in adverse environments as undesirable issues like severe sparsity and temporally flicking of point clouds would lead to fetching wrong image features.
In addition, the adoption of the non-parametric mechanism enables interactions between vertices, joints, local features, and global features, which can further enhance the reconstruction performance of AdaptiveFusion.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">To enable a unified framework for 3D object detection, FUTR3D first encodes features for each modality individually and then employs a query-based Modality-Agnostic Feature Sampler (MAFS) that works in a unified domain to extract features from different modalities as <a href="#S3.F1" title="In III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> (d) suggests. Finally, a transformer decoder operates on a set of 3D queries and performs set predictions of objects. Despite its flexibility for input modalities, FUTR3D still needs to train different networks for different modality combinations. Furthermore, it is unable to handle uncalibrated input modalities, limiting its capability to integrate uncalibrated information from other vehicle sources.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Experimental Settings</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Datasets</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">We conduct the experiments on the large-scale mmWave 3D human body dataset mmBody <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which consists of synchronized and calibrated mmWave radar point clouds and RGBD images in various conditions and mesh annotations for humans in the scenes. Following the general setting, we choose 20 sequences in the lab scenes (Lab1 and Lab2) as the training set while 2 sequences for each scene including labs, furnished, rain, smoke, poor lighting, and occlusion<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Only radar and camera1 are occluded and camera2 can still function.</span></span></span> as the test set. As mentioned above, we randomly split the training set into 31 parts to train a single network, and each part consists of a combination of inputs. For testing, we test the network with all combinations on the entire test set. In addition, we evaluate AdaptiveFusion on the other multi-modal and/or multi-view human datasets, Human3.6M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, HuMMan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and BEHAVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, to demonstrate its adaptability. For the Human3.6M dataset, we conduct mix-training using 3D and 2D training data following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We follow the common setting where subjects S1, S5, S6, S7, and S8 are used in training, and subjects S9 and S11 for testing. We further evaluate our method on the single-view image dataset 3DPW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to demonstrate its adaptability in the wild.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Metrics</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">To evaluate the performance of the reconstruction on the mmBody dataset, we employ commonly used metrics, Mean Per Joint Position Error (MPJPE) and Mean Per Vertex Error (MPVE), which quantify the average Euclidean distance between the prediction and the ground truth for joints/vertices in each frame. For the HuMMan and BEHAVE datasets, we additionally employ Procrustes Analysis MPJPE (PA-MPJPE) to evaluate the alignment accuracy. For the Human3.6M dataset, we report MPJPE and PA-MPJPE following the P2 protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.5.1.1" class="ltx_text">IV-A</span>3 </span>Implementation Details</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.3" class="ltx_p">We implement all the models using Pytorch. For the mmBody, HuMMan, and BEHAVE datasets, we train all the networks on Nvidia GeForce RTX 3090 GPUs for 50 epochs from scratch with an Adam optimizer and an initial learning rate of 0.001. For the Human3.6M dataset, we train our model on 8 NVIDIA V100 GPUs for 200 epochs. For each epoch, our training takes about 2 hours. The loss weights of <math id="S4.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mi id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><ci id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">\alpha</annotation></semantics></math>, <math id="S4.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.SSS3.p1.2.m2.1a"><mi id="S4.SS1.SSS3.p1.2.m2.1.1" xref="S4.SS1.SSS3.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.2.m2.1b"><ci id="S4.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.2.m2.1c">\beta</annotation></semantics></math>, and <math id="S4.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S4.SS1.SSS3.p1.3.m3.1a"><mi id="S4.SS1.SSS3.p1.3.m3.1.1" xref="S4.SS1.SSS3.p1.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.3.m3.1b"><ci id="S4.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p1.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.3.m3.1c">\gamma</annotation></semantics></math> in our experiments are 1000, 100, and 100, respectively.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Experimental Results</span>
</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.5.1.1" class="ltx_text">IV-B</span>1 </span>Effectiveness Analysis</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p"><span id="S4.SS2.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">We comprehensively analyze the effectiveness of our method. Experimental results demonstrate that AdaptiveFusion can not only adapt to arbitrary modal inputs but also achieve effective fusion by selecting informative features in various environments.
</span></p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p"><span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Effectiveness in View and Modality Fusion.</span>
We evaluate AdaptiveFusion using different input combinations from the mmBody dataset as reported in <a href="#S3.T2" title="In III-D Modality Sampling and Masking ‣ III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a> and average errors are shown in <a href="#S4.F2" title="In IV-B1 Effectiveness Analysis ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. We can see that the error decreases with more modalities and views added, which demonstrates our model can integrate the strengths of each modality effectively.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p"><span id="S4.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Effectiveness in Arbitrary Input Combinations.</span>
Our experiments show that AdaptiveFusion can effectively adapt to different input combinations and even improves performance from single-modal approaches.
We compare its testing results using a single-modal input with models trained only using the fixed single modality.
As shown in <a href="#S4.F2" title="In IV-B1 Effectiveness Analysis ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, AdaptiveFusion trained with different input combinations performs better than models trained with the fixed modality. Notice that AdaptiveFusion only accesses 1/31 of the training data for one combination. This demonstrates the significant advantage of our combination training strategy, which enhances the performance across various modal combinations.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.04851/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of AdaptiveFusion using different input combinations with other methods on the mmBody dataset. Img1 and dep1 denote the RGB images and depth point clouds from the first viewpoint. Img2 and dep2 are from the second viewpoint (no adverse conditions for this viewpoint).</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.04851/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative results. Each row represents an adverse weather scene (rain, smoke, poor lighting, and occlusion) and each column shows the reconstructed mesh and attention weights, respectively. From top to bottom, weights are for the estimation of the left ankle, right elbow, right ankle, and right shoulder. The darker color in the Vertices column indicates larger attention weights. The reddish color indicates larger attention weights and the bluish color smaller from the Image1 to Radar columns.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p"><span id="S4.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Attention Interactions.</span>
<a href="#S4.F3" title="In IV-B1 Effectiveness Analysis ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows some reconstructed meshes from AdaptiveFusion for different poses and subjects in different scenarios. Overall, the reconstructed meshes for most samples are close to the ground truth.
We further analyze the attention weights extracted from the last layer of FTM to better understand the effect of AdaptiveFusion in learning interactions between multi-modal multi-view features. <a href="#S4.F3" title="In IV-B1 Effectiveness Analysis ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the visualization of the attention map between a specific joint and other token features including vertices and local features of each modality. For example, at the first row, the depth point clouds are contaminated with rain, leading to the estimation for the left ankle attending more to the image region and the radar point cloud of the left leg.
In the smoke scene at the second row, both image and depth modalities are severely degraded, which forces the right elbow prediction more reliant on information from the radar point cloud and other vertices. In the poor lighting scene at the third row, the images are highly noisy and the radar points for the right ankle are missing. In this case, the estimation relies more heavily on the depth points of the right leg to accurately regress the location. At the fourth row, camera1 is significantly impaired by the occlusion, and AdaptiveFusion utilizes more information from camera2 and radar to predict the right shoulder position.
Overall, AdaptiveFusion is able to adapt to various sensor degradation scenarios and effectively integrate relevant information from different modalities to make accurate predictions.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.5.1.1" class="ltx_text">IV-B</span>2 </span>Comparison Results</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p"><a href="#S3.T2" title="In III-D Modality Sampling and Masking ‣ III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a><span id="S4.SS2.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">, <a href="#S4.T3" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>, <a href="#S4.T4" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a>, <a href="#S4.T5" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a>, and <a href="#S4.T6" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VI</span></a> summarize the main results of models tested on different datasets. Compared with existing fusion solutions and baselines, our approach performs better in every aspect: in addition to the superior fusion effect of multi-modal inputs by utilizing the complementary feature of each modality, it also achieves significant performance from multi-view inputs. We provide more quantitative comparisons in the supplementary material.
</span></p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Comparison with Multi-Modal Fusion Methods.</span>
We compare AdaptiveFusion with the state-of-the-art fusion methods, DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Since the original works are designed for object detection, we extend these approaches to be applicable to 3D human body reconstruction with multi-modal inputs. <a href="#S3.T2" title="In III-D Modality Sampling and Masking ‣ III Method ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a> includes reconstruction errors with full-modalities input in all scenes of these methods. As we can see, AdaptiveFusion achieves more desirable results in all scenarios.
DeepFusion exhibits a lower performance compared to AdaptiveFusion in all scenes. This can be mainly attributed to the lack of global features, which reduces global interactions during the fusion stage.
Furthermore, it requires the point clouds as the main modality and cannot work with different combinations of modalities, e.g. only RGB images.
TokenFusion aims to substitute unimportant modality tokens detected by Score Nets with features from other modality streams, and the scores are learned with inter-modal projections among Transformer layers. The design still can not support arbitrary numbers and orders of modalities. Besides, it is ineffective to incorporate the modality streams in adverse environments as undesirable issues like severe sparsity and temporally flicking of point clouds would lead to fetching wrong image features.
For FUTR3D, despite strengthening interactions between vertices and joints to improve performance, the overall performance of the non-parametric FUTR3D is still inferior to that of AdaptiveFusion.
We also compare AdaptiveFusion with the radar-camera fusion method ImmFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. AdaptiveFusion using an image-radar combination achieves comparative or better performance in most scenes.
Furthermore, AdaptiveFusion with full-modalities input significantly outperforms ImmFusion in all aspects.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results (mm) on the Human3.6M dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. *  denotes pre-trained methods using extra datasets.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:203.8pt;height:130.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.4pt,69.6pt) scale(0.484536799798326,0.484536799798326) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Input</th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Methods</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPJPE ↓</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PA-MPJPE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="7"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text">Monocular Image</span></th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">62.5</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">41.1</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Pose2Mesh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center">64.9</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center">46.3</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center">51.2</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center">34.5</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">PyMAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">57.7</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center">38.7</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<th id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">POTTER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center">56.5</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center">35.1</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<th id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center">48.3</td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center">32.9</td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<th id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">AdaptiveFusion (Ours)</th>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center">53.3</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center">35.6</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<th id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="7"><span id="S4.T3.1.1.9.8.1.1" class="ltx_text">Multi-View Images</span></th>
<th id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">79.9</td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_t">45.1</td>
</tr>
<tr id="S4.T3.1.1.10.9" class="ltx_tr">
<th id="S4.T3.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Shin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</th>
<td id="S4.T3.1.1.10.9.2" class="ltx_td ltx_align_center">46.9</td>
<td id="S4.T3.1.1.10.9.3" class="ltx_td ltx_align_center">32.5</td>
</tr>
<tr id="S4.T3.1.1.11.10" class="ltx_tr">
<th id="S4.T3.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</th>
<td id="S4.T3.1.1.11.10.2" class="ltx_td ltx_align_center">64.8</td>
<td id="S4.T3.1.1.11.10.3" class="ltx_td ltx_align_center">43.8</td>
</tr>
<tr id="S4.T3.1.1.12.11" class="ltx_tr">
<th id="S4.T3.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">ProHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</th>
<td id="S4.T3.1.1.12.11.2" class="ltx_td ltx_align_center">62.2</td>
<td id="S4.T3.1.1.12.11.3" class="ltx_td ltx_align_center">34.5</td>
</tr>
<tr id="S4.T3.1.1.13.12" class="ltx_tr">
<th id="S4.T3.1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</th>
<td id="S4.T3.1.1.13.12.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.13.12.3" class="ltx_td ltx_align_center">33.0</td>
</tr>
<tr id="S4.T3.1.1.14.13" class="ltx_tr">
<th id="S4.T3.1.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Calib-free PaFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> *</th>
<td id="S4.T3.1.1.14.13.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.14.13.2.1" class="ltx_text ltx_framed ltx_framed_underline">44.8</span></td>
<td id="S4.T3.1.1.14.13.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.14.13.3.1" class="ltx_text ltx_font_bold">28.2</span></td>
</tr>
<tr id="S4.T3.1.1.15.14" class="ltx_tr">
<th id="S4.T3.1.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">AdaptiveFusion (Ours)</th>
<td id="S4.T3.1.1.15.14.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.15.14.2.1" class="ltx_text ltx_font_bold">43.8</span></td>
<td id="S4.T3.1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.15.14.3.1" class="ltx_text ltx_framed ltx_framed_underline">31.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p"><span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Comparison with Multi-View Fusion Methods.</span>
We validate AdaptiveFusion with multi-view images from four views on the Human3.6M dataset as summarized in <a href="#S4.T3" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>. For single-view methods, AdaptiveFusion performs close to the state-of-the-art methods in MPJPE. However, our method with multi-view images significantly enhances performance by fusing information from multiple perspectives. Compared with other multi-view methods, AdaptiveFusion achieves better or on-par performance with a simple framework and without the need for calibration and pre-training.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Results (mm) on the HuMMan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> datasets.</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:203.8pt;height:43.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-109.9pt,23.4pt) scale(0.481010765006374,0.481010765006374) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Methods</th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Inputs</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPJPE ↓</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPVE ↓</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PA-MPJPE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<td id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">HMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Monocular Image</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">54.78</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">36.14</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<td id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_center">VoteHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center">Monocular Depth</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center">144.99</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_center">106.32</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<td id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_center">AdaptiveFusion (Ours)</td>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center">Monocular Image</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.3.1" class="ltx_text ltx_framed ltx_framed_underline">37.47</span></td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.4.1" class="ltx_text ltx_framed ltx_framed_underline">42.60</span></td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.4.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">21.56</span></td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<td id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">AdaptiveFusion (Ours)</td>
<td id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">Multi-View Images</td>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.5.4.3.1" class="ltx_text ltx_font_bold">34.43</span></td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.5.4.4.1" class="ltx_text ltx_font_bold">42.08</span></td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.5.4.5.1" class="ltx_text ltx_font_bold">20.82</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results (mm) on the BEHAVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> datasets.</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:203.8pt;height:65.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.4pt,39.3pt) scale(0.454266386187358,0.454266386187358) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Methods</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Inputs</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPJPE ↓</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPVE ↓</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PA-MPJPE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<td id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Monocular Image</td>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">65.35</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">83.81</td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">39.23</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<td id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_center">VoteHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center">Monocular Depth</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center">63.34</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center">72.28</td>
<td id="S4.T5.1.1.3.2.5" class="ltx_td ltx_align_center">40.33</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<td id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_center">MV-SMPLify <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center">Multi-View Images</td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center">47.32</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center">56.27</td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center">38.93</td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<td id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_center">AdaptiveFusion (Ours)</td>
<td id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_center">Monocular Image</td>
<td id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center">64.64</td>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center">82.14</td>
<td id="S4.T5.1.1.5.4.5" class="ltx_td ltx_align_center">40.98</td>
</tr>
<tr id="S4.T5.1.1.6.5" class="ltx_tr">
<td id="S4.T5.1.1.6.5.1" class="ltx_td ltx_align_center">AdaptiveFusion (Ours)</td>
<td id="S4.T5.1.1.6.5.2" class="ltx_td ltx_align_center">Monocular Depth</td>
<td id="S4.T5.1.1.6.5.3" class="ltx_td ltx_align_center">45.12</td>
<td id="S4.T5.1.1.6.5.4" class="ltx_td ltx_align_center">60.15</td>
<td id="S4.T5.1.1.6.5.5" class="ltx_td ltx_align_center">35.35</td>
</tr>
<tr id="S4.T5.1.1.7.6" class="ltx_tr">
<td id="S4.T5.1.1.7.6.1" class="ltx_td ltx_align_center">AdaptiveFusion (Ours)</td>
<td id="S4.T5.1.1.7.6.2" class="ltx_td ltx_align_center">Multi-View Images</td>
<td id="S4.T5.1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">44.16</span></td>
<td id="S4.T5.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.6.4.1" class="ltx_text ltx_framed ltx_framed_underline">51.36</span></td>
<td id="S4.T5.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">27.33</span></td>
</tr>
<tr id="S4.T5.1.1.8.7" class="ltx_tr">
<td id="S4.T5.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_bb">AdaptiveFusion (Ours)</td>
<td id="S4.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">Multi-View Multi-Modal</td>
<td id="S4.T5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.8.7.3.1" class="ltx_text ltx_font_bold">31.08</span></td>
<td id="S4.T5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.8.7.4.1" class="ltx_text ltx_font_bold">37.89</span></td>
<td id="S4.T5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.8.7.5.1" class="ltx_text ltx_font_bold">20.82</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">We further validate AdaptiveFusion on the HuMMan and BEHAVE datasets, and the results are summarized in <a href="#S4.T4" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S4.T5" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a>. As the HuMMan dataset does not provide point data currently, we only utilize the image modalities from four viewpoints.
We compare AdaptiveFusion with other single-modality methods HMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and VoteHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> on this dataset<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Results of existing works on the HuMMan dataset are taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</span></span></span>. HMR is a classic image-based 3D human body reconstruction method and VoteHMR is a recent work that takes point clouds as the input for human mesh recovery. AdaptiveFusion with image modality inputs achieves lower MPJPE and PA-MPJPE compared to the two methods.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p">The BEHAVE dataset is a large-scale human-object interactions dataset with challenges of object occlusions and variations in background environments. We evaluate the state-of-the-art monocular-image method Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, depth-based reconstruction method VoteHMR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and the multi-view image-based method MV-SMPLify <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> on the BEHAVE dataset.
AdaptiveFusion with single-modality and multi-view inputs exhibits relatively high performance. Furthermore, by combining both multi-view and multi-modal information, AdaptiveFusion with full-modalities input achieves the best results as expected.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Results (mm) on the 3DPW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Methods</th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPJPE ↓</th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PA-MPJPE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SPIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<td id="S4.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">96.9</td>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">59.2</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<th id="S4.T6.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Pose2Mesh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T6.1.3.2.2" class="ltx_td ltx_align_center">89.5</td>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_center">56.3</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<th id="S4.T6.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Graphormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T6.1.4.3.2" class="ltx_td ltx_align_center">74.7</td>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_center">45.6</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<th id="S4.T6.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PyMAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<td id="S4.T6.1.5.4.2" class="ltx_td ltx_align_center">92.8</td>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_center">58.9</td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<th id="S4.T6.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SMPLer-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</th>
<td id="S4.T6.1.6.5.2" class="ltx_td ltx_align_center">75.2</td>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_center">50.5</td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<th id="S4.T6.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">OSX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</th>
<td id="S4.T6.1.7.6.2" class="ltx_td ltx_align_center">74.7</td>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_center">45.1</td>
</tr>
<tr id="S4.T6.1.8.7" class="ltx_tr">
<th id="S4.T6.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">POTTER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S4.T6.1.8.7.2" class="ltx_td ltx_align_center">75.0</td>
<td id="S4.T6.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.7.3.1" class="ltx_text ltx_font_bold">44.8</span></td>
</tr>
<tr id="S4.T6.1.9.8" class="ltx_tr">
<th id="S4.T6.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T6.1.9.8.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.9.8.2.1" class="ltx_text ltx_font_bold">73.9</span></td>
<td id="S4.T6.1.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.9.8.3.1" class="ltx_text ltx_framed ltx_framed_underline">44.9</span></td>
</tr>
<tr id="S4.T6.1.10.9" class="ltx_tr">
<th id="S4.T6.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">AdaptiveFusion (Ours)</th>
<td id="S4.T6.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.10.9.2.1" class="ltx_text ltx_framed ltx_framed_underline">74.6</span></td>
<td id="S4.T6.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb">45.6</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p6.1" class="ltx_p"><span id="S4.SS2.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Results in the Wild.</span>
We further validate AdaptiveFusion with in-the-wild moving monocular images on the 3DPW dataset as shown in <a href="#S4.T6" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VI</span></a>. Our method achieves comparative results, demonstrating its capability to work under outdoor mobile perspectives.
It should be noted that our method is designed for multi-view and multi-modal inputs. The result tested with monocular image input is a degraded case for our multi-view model. However, it is comparable with state-of-the-art methods elaborated for single-view images.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Results of time consumption for different fusion methods.</figcaption>
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Methods</th>
<th id="S4.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Params</th>
<th id="S4.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Time (ms)</th>
<th id="S4.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FPS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.2.1" class="ltx_tr">
<th id="S4.T7.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">DeepFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">212.5M</td>
<td id="S4.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">64.1</td>
<td id="S4.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">15.6</td>
</tr>
<tr id="S4.T7.1.3.2" class="ltx_tr">
<th id="S4.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TokenFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S4.T7.1.3.2.2" class="ltx_td ltx_align_center">280.4M</td>
<td id="S4.T7.1.3.2.3" class="ltx_td ltx_align_center">80.6</td>
<td id="S4.T7.1.3.2.4" class="ltx_td ltx_align_center">12.4</td>
</tr>
<tr id="S4.T7.1.4.3" class="ltx_tr">
<th id="S4.T7.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FUTR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T7.1.4.3.2" class="ltx_td ltx_align_center">250.4M</td>
<td id="S4.T7.1.4.3.3" class="ltx_td ltx_align_center">74.1</td>
<td id="S4.T7.1.4.3.4" class="ltx_td ltx_align_center">13.5</td>
</tr>
<tr id="S4.T7.1.5.4" class="ltx_tr">
<th id="S4.T7.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">AdaptiveFusion (Ours)</th>
<td id="S4.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">228.3M</td>
<td id="S4.T7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">68.5</td>
<td id="S4.T7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">14.6</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.SSS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p7.1" class="ltx_p"><span id="S4.SS2.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Time Consumption.</span>
We provide results of time consumption in <a href="#S4.T7" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VII</span></a>. Our model consumes affordable computational resources and can achieve real-time performance.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Ablation study on the mmBody dataset.</figcaption>
<div id="S4.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:85.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.4pt,11.1pt) scale(0.79356888433163,0.79356888433163) ;">
<table id="S4.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.1.1.1.1" class="ltx_tr">
<th id="S4.T8.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T8.1.1.1.1.1.1" class="ltx_text">Methods</span></th>
<th id="S4.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="6">Basic Scenes</th>
<th id="S4.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="8">Adverse Environments</th>
<th id="S4.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" rowspan="2"><span id="S4.T8.1.1.1.1.4.1" class="ltx_text">Average</span></th>
</tr>
<tr id="S4.T8.1.1.2.2" class="ltx_tr">
<th id="S4.T8.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S4.T8.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Lab1</span></th>
<th id="S4.T8.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S4.T8.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Lab2</span></th>
<th id="S4.T8.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2"><span id="S4.T8.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Furnished</span></th>
<th id="S4.T8.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S4.T8.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Rain</span></th>
<th id="S4.T8.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S4.T8.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Smoke</span></th>
<th id="S4.T8.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2"><span id="S4.T8.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Poor Lighting</span></th>
<th id="S4.T8.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2"><span id="S4.T8.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Occlusion</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1.3.1" class="ltx_tr">
<th id="S4.T8.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">AdaptiveFusion-w/o-LF</th>
<td id="S4.T8.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">4.6</td>
<td id="S4.T8.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.9</td>
<td id="S4.T8.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">4.3</td>
<td id="S4.T8.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8</td>
<td id="S4.T8.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">5.1</td>
<td id="S4.T8.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.3</td>
<td id="S4.T8.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">5.3</td>
<td id="S4.T8.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.6</td>
<td id="S4.T8.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">7.9</td>
<td id="S4.T8.1.1.3.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.4</td>
<td id="S4.T8.1.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t">6.6</td>
<td id="S4.T8.1.1.3.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.2</td>
<td id="S4.T8.1.1.3.1.14" class="ltx_td ltx_align_center ltx_border_t">9.8</td>
<td id="S4.T8.1.1.3.1.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.0</td>
<td id="S4.T8.1.1.3.1.16" class="ltx_td ltx_align_center ltx_border_t">6.2</td>
<td id="S4.T8.1.1.3.1.17" class="ltx_td ltx_align_center ltx_border_t">8.0</td>
</tr>
<tr id="S4.T8.1.1.4.2" class="ltx_tr">
<th id="S4.T8.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaptiveFusion-w/o-GIM</th>
<td id="S4.T8.1.1.4.2.2" class="ltx_td ltx_align_center">3.4</td>
<td id="S4.T8.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">4.6</td>
<td id="S4.T8.1.1.4.2.4" class="ltx_td ltx_align_center">3.7</td>
<td id="S4.T8.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">4.8</td>
<td id="S4.T8.1.1.4.2.6" class="ltx_td ltx_align_center">4.1</td>
<td id="S4.T8.1.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r">5.2</td>
<td id="S4.T8.1.1.4.2.8" class="ltx_td ltx_align_center">4.1</td>
<td id="S4.T8.1.1.4.2.9" class="ltx_td ltx_align_center ltx_border_r">5.3</td>
<td id="S4.T8.1.1.4.2.10" class="ltx_td ltx_align_center">6.2</td>
<td id="S4.T8.1.1.4.2.11" class="ltx_td ltx_align_center ltx_border_r">7.9</td>
<td id="S4.T8.1.1.4.2.12" class="ltx_td ltx_align_center">3.9</td>
<td id="S4.T8.1.1.4.2.13" class="ltx_td ltx_align_center ltx_border_r">4.7</td>
<td id="S4.T8.1.1.4.2.14" class="ltx_td ltx_align_center">7.1</td>
<td id="S4.T8.1.1.4.2.15" class="ltx_td ltx_align_center ltx_border_r">10.1</td>
<td id="S4.T8.1.1.4.2.16" class="ltx_td ltx_align_center">4.7</td>
<td id="S4.T8.1.1.4.2.17" class="ltx_td ltx_align_center">6.1</td>
</tr>
<tr id="S4.T8.1.1.5.3" class="ltx_tr">
<th id="S4.T8.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaptiveFusion-w/o-MMM</th>
<td id="S4.T8.1.1.5.3.2" class="ltx_td ltx_align_center">3.5</td>
<td id="S4.T8.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">4.7</td>
<td id="S4.T8.1.1.5.3.4" class="ltx_td ltx_align_center">3.6</td>
<td id="S4.T8.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r">4.7</td>
<td id="S4.T8.1.1.5.3.6" class="ltx_td ltx_align_center">4.2</td>
<td id="S4.T8.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r">5.4</td>
<td id="S4.T8.1.1.5.3.8" class="ltx_td ltx_align_center">3.9</td>
<td id="S4.T8.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_r">5.1</td>
<td id="S4.T8.1.1.5.3.10" class="ltx_td ltx_align_center">6.6</td>
<td id="S4.T8.1.1.5.3.11" class="ltx_td ltx_align_center ltx_border_r">8.8</td>
<td id="S4.T8.1.1.5.3.12" class="ltx_td ltx_align_center">5.2</td>
<td id="S4.T8.1.1.5.3.13" class="ltx_td ltx_align_center ltx_border_r">6.6</td>
<td id="S4.T8.1.1.5.3.14" class="ltx_td ltx_align_center">10.3</td>
<td id="S4.T8.1.1.5.3.15" class="ltx_td ltx_align_center ltx_border_r">14.7</td>
<td id="S4.T8.1.1.5.3.16" class="ltx_td ltx_align_center">5.3</td>
<td id="S4.T8.1.1.5.3.17" class="ltx_td ltx_align_center">7.1</td>
</tr>
<tr id="S4.T8.1.1.6.4" class="ltx_tr">
<th id="S4.T8.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">AdaptiveFusion</th>
<td id="S4.T8.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.2.1" class="ltx_text ltx_font_bold">3.1</span></td>
<td id="S4.T8.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.3.1" class="ltx_text ltx_font_bold">4.1</span></td>
<td id="S4.T8.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.4.1" class="ltx_text ltx_font_bold">3.3</span></td>
<td id="S4.T8.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.5.1" class="ltx_text ltx_font_bold">4.2</span></td>
<td id="S4.T8.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.6.1" class="ltx_text ltx_font_bold">3.7</span></td>
<td id="S4.T8.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.7.1" class="ltx_text ltx_font_bold">4.4</span></td>
<td id="S4.T8.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.8.1" class="ltx_text ltx_font_bold">3.4</span></td>
<td id="S4.T8.1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.9.1" class="ltx_text ltx_font_bold">4.4</span></td>
<td id="S4.T8.1.1.6.4.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.10.1" class="ltx_text ltx_font_bold">5.2</span></td>
<td id="S4.T8.1.1.6.4.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.11.1" class="ltx_text ltx_font_bold">6.4</span></td>
<td id="S4.T8.1.1.6.4.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.12.1" class="ltx_text ltx_font_bold">3.5</span></td>
<td id="S4.T8.1.1.6.4.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.13.1" class="ltx_text ltx_font_bold">4.3</span></td>
<td id="S4.T8.1.1.6.4.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.14.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S4.T8.1.1.6.4.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T8.1.1.6.4.15.1" class="ltx_text ltx_font_bold">6.6</span></td>
<td id="S4.T8.1.1.6.4.16" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.16.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S4.T8.1.1.6.4.17" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.1.6.4.17.1" class="ltx_text ltx_font_bold">4.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we conduct a comprehensive study as reported in <a href="#S4.T8" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VIII</span></a> to validate the effectiveness of the local features, Global Integrated Module (GIM), and modality masking module of AdaptiveFusion.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Effectiveness of Local Features</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To analyze the effectiveness of the local features, we compared the results of the original AdaptiveFusion with its variation AdaptiveFusion-w/o-LF, in which the cluster features and grid features are removed from the backward computation graph. As indicated in <a href="#S4.T8" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VIII</span></a>, the mean errors of AdaptiveFusion-w/o-LF are obviously greater than AdaptiveFusion.
Despite the assistance of MMM, the errors in extreme conditions like occlusion are even worse than those of the Radar-Only method. These results strongly support our motivation of utilizing local features to benefit the quality of reconstruction.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Effectiveness of Global Integrated Module</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In our framework, GIM serves as a mixer to integrate global features of multi-modal input. Instead of element-wise addition or channel-wise concatenation, GIM uses learnable parameters to control the weights of global features from different modalities.
In extreme scenarios in which some sensors may not function at all, such as the smoke and occlusion scenes where camera1 is completely occluded,
GIM allows the model to select the most informative features from the global features of other modalities, resulting in improved accuracy and robustness of reconstruction.
AdaptiveFusion-w/o-GIM is on par with AdaptiveFusion in the basic scenes where the proportion of valid information from RGB, depth, and mmWave sensors is balanced. Simultaneously, with MMM, AdaptiveFusion-w/o-GIM performs well in smoke and poor lighting scenes as well. However, in the extreme scenarios in which some sensors may not function at all, such as the occlusion scene where camera1 is completely occluded, GIM allows the model to select the most informative features from the global features of other modalities, resulting in improved accuracy and robustness of 3D human body reconstruction.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.5.1.1" class="ltx_text">IV-C</span>3 </span>Effectiveness of Modality Masking Module</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">As stated above, the training set only consisting of basic scene data would force the Transformer module to pay more attention to a single modality, which leads to a rapid decline in performance when the sensor fails in adverse environments.
As shown in <a href="#S4.T8" title="In IV-B2 Comparison Results ‣ IV-B Experimental Results ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">VIII</span></a>, AdaptiveFusion-w/o-MMM can achieve similar errors with AdaptiveFusion in the basic scenes. However, in adverse environments, AdaptiveFusion outperforms AdaptiveFusion-w/o-MMM significantly, demonstrating the importance of MMM in improving the performance of reconstruction. In particular, MMM gains over more than 50% reconstruction error reduction, and the mean joint/vertex errors for AdaptiveFusion can reach as low as 4.5cm/7.1cm. This is mainly attributed to the fact that MMM forces the Transformer module to lean more attention on the other camera and radar to select helpful features.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS4.5.1.1" class="ltx_text">IV-C</span>4 </span>Ablation on Mask Proportion</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">We provide more ablation results for the mask proportion (in Lab1 of mmBody dataset) as shown in <a href="#S4.T9" title="In IV-C4 Ablation on Mask Proportion ‣ IV-C Ablation Study ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IX</span></a>. We set it to 30% in the experiments of our main paper as it achieves the best accuracy.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.04851/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="531" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Failure cases in the furnished, smoke, and occlusion scene (Radar points in green and depth points in orange).</figcaption>
</figure>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>Ablation study on the mask proportion.</figcaption>
<table id="S4.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.1.1.1" class="ltx_tr">
<th id="S4.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Mask Proportion</th>
<th id="S4.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPJPE ↓</th>
<th id="S4.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPVE ↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.1.2.1" class="ltx_tr">
<td id="S4.T9.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">10%</td>
<td id="S4.T9.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">4.1</td>
<td id="S4.T9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">5.3</td>
</tr>
<tr id="S4.T9.1.3.2" class="ltx_tr">
<td id="S4.T9.1.3.2.1" class="ltx_td ltx_align_center">30%</td>
<td id="S4.T9.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T9.1.3.2.2.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S4.T9.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T9.1.3.2.3.1" class="ltx_text ltx_font_bold">4.9</span></td>
</tr>
<tr id="S4.T9.1.4.3" class="ltx_tr">
<td id="S4.T9.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">50%</td>
<td id="S4.T9.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">4.0</td>
<td id="S4.T9.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">5.2</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Limitations</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Though with the masking module, our model has gained a certain level of generalization ability, it does not achieve zero-shot capability: it exhibits relatively high error in furnished, smoke, and occlusion conditions on the mmBody dataset due to the sensor defects and data imbalance as exampled in <a href="#S4.F4" title="In IV-C4 Ablation on Mask Proportion ‣ IV-C Ablation Study ‣ IV Experiments ‣ AdaptiveFusion: Adaptive Multi-Modal Multi-View Fusion for 3D Human Body Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. In the furnished and smoke scenario, the poor fusion performance can be attributed to the background noise of images and defects of sensors like missing point clouds and multi-path effects, respectively. In the occlusion scenario, the subject is much taller than other subjects in the training set and the reconstructed shape appears shorter than the ground truth.
Contrastive and predictive multi-modal pre-training are promising solutions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduce AdaptiveFusion, a generic adaptive 3D reconstruction framework that is capable of processing an unlimited number and combination of sensor inputs to achieve robust and precise reconstruction performance. Our approach uses a Transformer network to fuse both global and local features, enabling AdaptiveFusion to adaptively handle arbitrary sensor inputs and accommodate noisy modalities. Furthermore, AdaptiveFusion demonstrates strong adaptability, achieving competitive results even with limited training data for each input combination. We investigate various fusion approaches and demonstrate that AdaptiveFusion outperforms multi-view, LiDAR-camera, and other state-of-the-art multi-modal fusion methods in various environments.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, “End-to-end recovery of
human shape and pose,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2018, pp. 7122–7131.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh reconstruction
with transformers,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2021, pp. 1954–1963.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Hu, E. S.-L. Ho, and A. Munteanu, “3dbodynet: fast reconstruction of 3d
animatable human body shape from a single commodity depth camera,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, vol. 24, pp. 2139–2149, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Liu, Y. Rong, and L. Sheng, “Votehmr: Occlusion-aware voting network for
robust 3d human mesh recovery from partial point clouds,” in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</em>,
2021, pp. 955–964.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Xue, Y. Ju, C. Miao, Y. Wang, S. Wang, A. Zhang, and L. Su, “mmMesh:
Towards 3d real-time dynamic human mesh construction using millimeter-wave,”
in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th Annual International Conference on Mobile
Systems, Applications, and Services</em>, 2021, pp. 269–282.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Chen, X. Wang, S. Zhu, Y. Li, J. Chen, and Q. Ye, “mmbody benchmark: 3d
body reconstruction dataset and analysis for millimeter wave radar,” in
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>,
2022, pp. 3501–3510.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and
F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor
fusion in unseen adverse weather,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2020, pp.
11 682–11 692.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Shi, Z. Shi, C. Yang, S. He, J. Chen, and A. Chen, “Road-map aided gm-phd
filter for multivehicle tracking with automotive radar,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Industrial Informatics</em>, vol. 18, no. 1, pp. 97–108, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Zhang, L. Hu, X. Deng, and S. Xia, “Sequential 3d human pose estimation
using adaptive point cloud sampling strategy.” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
International Joint Conference on Artificial Intelligence</em>, 2021, pp.
1330–1337.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Wang, C. Ma, M. Zhu, and X. Yang, “Pointaugmenting: Cross-modal
augmentation for 3d object detection,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2021, pp.
11 794–11 803.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Li, A. W. Yu, T. Meng, B. Caine, J. Ngiam, D. Peng, J. Shen, Y. Lu, D. Zhou,
Q. V. Le <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deepfusion: Lidar-camera deep fusion for
multi-modal 3d object detection,” in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2022, pp.
17 182–17 191.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Wang, X. Chen, L. Cao, W. Huang, F. Sun, and Y. Wang, “Multimodal token
fusion for vision transformers,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2022, pp.
12 186–12 195.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
fusion for 3d object detection,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2020, pp. 4604–4612.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Chen, X. Wang, K. Shi, S. Zhu, B. Fang, Y. Chen, J. Chen, Y. Huo, and Q. Ye,
“Immfusion: Robust mmwave-rgb fusion for 3d human body reconstruction in all
weather conditions,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics
and Automation (ICRA)</em>.   IEEE, 2023,
pp. 2752–2758.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Wu, F. Hao, W. Liang, and J. Xu, “Transformer fusion and pixel-level
contrastive learning for rgb-d salient object detection,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Multimedia</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X. Chen, T. Zhang, Y. Wang, Y. Wang, and H. Zhao, “Futr3d: A unified sensor
fusion framework for 3d detection,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 172–181.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Wang, H. Tang, S. Shi, A. Li, Z. Li, B. Schiele, and L. Wang, “Unitr: A
unified and efficient multi-modal transformer for bird’s-eye-view
representation,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>, 2023, pp. 6792–6802.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3. 6m: Large scale
datasets and predictive methods for 3d human sensing in natural
environments,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, vol. 36, no. 7, pp. 1325–1339, 2013.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Z. Cai, D. Ren, A. Zeng, Z. Lin, T. Yu, W. Wang, X. Fan, Y. Gao, Y. Yu, L. Pan,
F. Hong, M. Zhang, C. C. Loy, L. Yang, and Z. Liu, “Humman: Multi-modal 4d
human dataset for versatile sensing and modeling,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the European Conference on Computer Vision (ECCV)</em>, 2022, pp. 557–577.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B. L. Bhatnagar, X. Xie, I. Petrov, C. Sminchisescu, C. Theobalt, and
G. Pons-Moll, “Behave: Dataset and method for tracking human object
interactions,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>.   IEEE, jun 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll,
“Recovering accurate 3d human pose in the wild using imus and a moving
camera,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</em>, 2018, pp. 601–617.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis, “Learning to
reconstruct 3d human pose and shape via model-fitting in the loop,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2019, pp. 2252–2261.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Li, J. Liu, Z. Zhang, S. Xu, and Y. Yan, “Cliff: Carrying location
information in full frames into human pose and shape estimation,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2022, pp. 590–606.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N. Kolotouros, G. Pavlakos, and K. Daniilidis, “Convolutional mesh regression
for single-image human shape reconstruction,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
4501–4510.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Kim, M.-G. Gwon, H. Park, H. Kwon, G.-M. Um, and W. Kim, “Sampling is
matter: Point-guided 3d human mesh reconstruction,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023,
pp. 12 880–12 889.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, “SMPL: A
skinned multi-person linear model,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ACM transactions on graphics</em>,
vol. 34, no. 6, pp. 1–16, 2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas, and
M. J. Black, “Expressive body capture: 3d hands, face, and body from a
single image,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer
Vision and Pattern Recognition</em>, 2019, pp. 10 975–10 985.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
G. Pavlakos, J. Malik, and A. Kanazawa, “Human mesh recovery from multiple
shots,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 2022, pp. 1485–1495.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Liang and M. C. Lin, “Shape-aware human pose and shape reconstruction using
multi-view images,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>, 2019, pp. 4352–4362.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
N. Kolotouros, G. Pavlakos, D. Jayaraman, and K. Daniilidis, “Probabilistic
modeling for human mesh recovery,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2021, pp. 11 605–11 614.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. Jia, H. Zhang, L. An, and Y. Liu, “Delving deep into pixel alignment
feature for accurate multi-view human mesh recovery,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the AAAI Conference on Artificial Intelligence</em>, Febuary 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Zhao, S. Li, K. N. Ngan, and F. Wu, “3-d reconstruction of human body shape
from a single commodity depth camera,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, vol. 21, no. 1, pp. 114–123, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Xu, W. Chang, Y. Zhu, L. Dong, H. Zhou, and Q. Zhang, “Building
high-fidelity human body models from user-generated data,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Multimedia</em>, vol. 23, pp. 1542–1556, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Lin, L. Wang, and Z. Liu, “Mesh graphormer,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF International Conference on Computer Vision</em>, 2021, pp.
12 939–12 948.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Zhao, T. Li, M. Abu Alsheikh, Y. Tian, H. Zhao, A. Torralba, and D. Katabi,
“Through-wall human pose estimation using radio signals,” in
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2018, pp. 7356–7365.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
F. Wang, S. Zhou, S. Panev, J. Han, and D. Huang, “Person-in-wifi:
Fine-grained person perception using wifi,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 5452–5461.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
C. Yu, D. Zhang, Z. Wu, C. Xie, Z. Lu, Y. Hu, and Y. Chen, “Mobirfpose:
Portable rf-based 3d human pose camera,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
network for autonomous driving,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition</em>, 2017, pp. 1907–1915.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for 3d
object detection from rgb-d data,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2018, pp. 918–927.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
R. Nabati and H. Qi, “Centerfusion: Center-based radar and camera fusion for
3d object detection,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference
on Applications of Computer Vision</em>, 2021, pp. 1527–1536.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Cheng, H. Xu, and Y. Liu, “Robust small object detection on the water
surface through fusion of camera and millimeter wave radar,” in
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</em>, 2021, pp. 15 263–15 272.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Long, D. Morris, X. Liu, M. Castro, and P. Narayanan, “Radar-camera pixel
depth association for depth completion,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp.
12 507–12 516.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
T. Huang, Z. Liu, X. Chen, and X. Bai, “Epnet: Enhancing point features with
image semantics for 3d object detection,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XV 16</em>.   Springer,
2020, pp. 35–52.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
G. Zhou, Y. Yan, D. Wang, and Q. Chen, “A novel depth and color feature fusion
framework for 6d object pose estimation,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, vol. 23, pp. 1630–1639, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Piergiovanni, V. Casser, M. S. Ryoo, and A. Angelova, “4d-net for learned
multi-modal alignment,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision</em>, 2021, pp. 15 435–15 445.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B. Wan, X. Zhou, Y. Sun, T. Wang, C. Lv, S. Wang, H. Yin, and C. Yan, “Mffnet:
Multi-modal feature fusion network for vdt salient object detection,”
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, “Transfusion:
Robust lidar-camera fusion for 3d object detection with transformers,” in
<em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2022, pp. 1090–1099.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Yan, Y. Liu, J. Sun, F. Jia, S. Li, T. Wang, and X. Zhang, “Cross modal
transformer: Towards fast and robust 3d object detection,” in
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer
Vision</em>, 2023, pp. 18 268–18 278.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan,
X. Wang <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deep high-resolution representation learning for
visual recognition,” <em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, vol. 43, no. 10, pp. 3349–3364, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
<em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y. Wang, X. Zhang, T. Yang, and J. Sun, “Anchor detr: Query design for
transformer-based detector,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on
artificial intelligence</em>, 2022, pp. 2567–2575.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
H. Choi, G. Moon, and K. M. Lee, “Pose2mesh: Graph convolutional network for
3d human pose and mesh recovery from a 2d human pose,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Computer
Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part VII 16</em>.   Springer, 2020, pp. 769–787.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, and Z. Sun, “Pymaf: 3d
human pose and shape regression with pyramidal mesh alignment feedback
loop,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, 2021, pp. 11 446–11 456.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
C. Zheng, X. Liu, G.-J. Qi, and C. Chen, “Potter: Pooling attention
transformer for efficient human mesh recovery,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp.
1611–1620.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
S. Shin and E. Halilaj, “Multi-view human pose and shape estimation using
learnable volumetric aggregation,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.13427</em>,
2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Z. Li, M. Oskarsson, and A. Heyden, “3d human pose and shape estimation
through collaborative learning and multi-view model-fitting,” in
<em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF winter conference on applications of
computer vision</em>, 2021, pp. 1888–1897.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Z. Yu, L. Zhang, Y. Xu, C. Tang, L. Tran, C. Keskin, and H. S. Park,
“Multiview human body reconstruction from uncalibrated cameras,”
<em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp.
7879–7891, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Z. Cai, W. Yin, A. Zeng, C. Wei, Q. Sun, W. Yanjun, H. E. Pang, H. Mei,
M. Zhang, L. Zhang <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Smpler-x: Scaling up expressive human
pose and shape estimation,” <em id="bib.bib59.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
J. Lin, A. Zeng, H. Wang, L. Zhang, and Y. Li, “One-stage 3d whole-body mesh
recovery with component aware transformer,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp.
21 159–21 168.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.04850" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.04851" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.04851">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.04851" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.04852" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:06:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
