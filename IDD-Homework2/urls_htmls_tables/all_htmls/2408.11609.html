<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Xinyu: An Efficient LLM-based System for Commentary Generation</title>
<!--Generated on Fri Aug 23 03:40:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="LLM-based System,  Commentary Generation,  Supervised Fine-tuning,  Retrieval Augmented Generation" lang="en" name="keywords"/>
<base href="/html/2408.11609v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S1" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S2" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S2.SS1" title="In 2. Related Work ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S2.SS2" title="In 2. Related Work ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Domain-specific LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S3" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Technical route of Xinyu</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1" title="In 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Main Components</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1.SSS1" title="In 4.1. Main Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Peg Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1.SSS2" title="In 4.1. Main Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Main Argument Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1.SSS3" title="In 4.1. Main Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Supporting Argument Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1.SSS4" title="In 4.1. Main Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Evidence Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS1.SSS5" title="In 4.1. Main Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.5 </span>Article Combination</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS2" title="In 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Auxiliary Components</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS2.SSS1" title="In 4.2. Auxiliary Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Argument Ranking Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S4.SS2.SSS2" title="In 4.2. Auxiliary Components ‣ 4. Technical route of Xinyu ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Evidence Database Construction</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS1" title="In 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Evaluation Metrics of Commentary</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS1.SSS0.Px1" title="In 5.1. Evaluation Metrics of Commentary ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Automatic evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS1.SSS0.Px2" title="In 5.1. Evaluation Metrics of Commentary ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Human evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS2" title="In 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Experiment Settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS2.SSS0.Px1" title="In 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS2.SSS0.Px2" title="In 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS2.SSS0.Px3" title="In 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Test Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS2.SSS0.Px4" title="In 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Ablation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS3" title="In 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Experimental Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS3.SSS0.Px1" title="In 5.3. Experimental Results and Analysis ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Results of commentary generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS3.SSS0.Px2" title="In 5.3. Experimental Results and Analysis ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Result of ablation study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.SS3.SSS0.Px3" title="In 5.3. Experimental Results and Analysis ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title">Case study</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S6" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#A1" title="In Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#A1.SS1" title="In Appendix A appendix ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Overall Generation Process</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\useunder</span>
<p class="ltx_p" id="p1.2"><span class="ltx_text ltx_ulem_uline" id="p1.2.1"></span><span class="ltx_ERROR undefined" id="p1.2.2">\ul</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Xinyu: An Efficient LLM-based System for Commentary Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiquan Wu<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>* These authors contributed equally to this work.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wuyiquan@zju.edu.cn">wuyiquan@zju.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Zhejiang University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Tang<span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>* These authors contributed equally to this work.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tangb@iaar.ac.cn">tangb@iaar.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Science and Technology of China, Hefei, China</span><span class="ltx_text ltx_affiliation_institution" id="id5.2.id2">Institute for Advanced Algorithms Research, Shanghai, China</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenyang Xi<span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>† These authors contributed equally to this work.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xicy@iaar.ac.cn">xicy@iaar.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Institute for Advanced Algorithms Research</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yu Yu<span class="ltx_note ltx_role_footnote" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>† These authors contributed equally to this work.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yuy@iaar.ac.cn">yuy@iaar.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Institute for Advanced Algorithms Research</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pengyu Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:2371407@stu.neu.edu.cn">2371407@stu.neu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Northeastern University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Shenyang</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifei Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liuyifei@zju.edu.cn">liuyifei@zju.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Zhejiang University</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kun Kuang<span class="ltx_note ltx_role_footnote" id="footnotex5"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>‡ Corresponding Author.</span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kunkuang@zju.edu.cn">kunkuang@zju.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">Zhejiang University</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haiying Deng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:denghaiying@xinhuaskl.com">denghaiying@xinhuaskl.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">State Key Laboratory of Media Convergence Production Technology and Systems</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiyu Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lizy@iaar.ac.cn">lizy@iaar.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">Institute for Advanced Algorithms Research</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id27.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Feiyu Xiong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xiongfy@iaar.ac.cn">xiongfy@iaar.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id28.1.id1">Institute for Advanced Algorithms Research</span><span class="ltx_text ltx_affiliation_city" id="id29.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id30.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:hujie1@chinatelecom.cn">hujie1@chinatelecom.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id31.1.id1">Research Institute of China Telecom</span><span class="ltx_text ltx_affiliation_city" id="id32.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id33.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chengpeng@xinhua.org">chengpeng@xinhua.org</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id34.1.id1">State Key Laboratory of Media Convergence Production Technology and Systems</span><span class="ltx_text ltx_affiliation_city" id="id35.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id36.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhonghao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wangzhonghao@xinhua.org">wangzhonghao@xinhua.org</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id37.1.id1">State Key Laboratory of Media Convergence Production Technology and Systems</span><span class="ltx_text ltx_affiliation_city" id="id38.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id39.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yi Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wangyi08@xinhua.org">wangyi08@xinhua.org</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id40.1.id1">State Key Laboratory of Media Convergence Production Technology and Systems</span><span class="ltx_text ltx_affiliation_city" id="id41.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id42.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yi Luo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:luoyi@xinhua.org">luoyi@xinhua.org</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id43.1.id1">State Key Laboratory of Media Convergence Production Technology and Systems</span><span class="ltx_text ltx_affiliation_city" id="id44.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id45.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingchuan Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yangmch@chinatelecom.cn">yangmch@chinatelecom.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id46.1.id1">Research Institute of China Telecom</span><span class="ltx_text ltx_affiliation_city" id="id47.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id48.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id49.id1">Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators.
Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence.
In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology.
To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation.
Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries.</p>
</div>
<div class="ltx_keywords">LLM-based System, Commentary Generation, Supervised Fine-tuning, Retrieval Augmented Generation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; August 25–29, 2024; Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3637528.3671537</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0490-1/24/08</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language generation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
The illustration of the commentary generation task. To generate a commentary, it usually requires argument mining, evidence searching, and article embellishing. With the Xinyu, the intermediate steps can be sped up. The right part demonstrates the structure of a commentary, which consists of a title, a main argument, several supporting arguments and evidence, and an ending. This example is translated from Chinese.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advancement of natural language processing (NLP), particularly large language models (LLMs), numerous text-generation systems have been proposed to enhance the effectiveness and efficiency of individuals across various fields, such as education, medicine and law <cite class="ltx_cite ltx_citemacro_citep">(Adeshola and Adepoju, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib2" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib33" title="">2023b</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib5" title="">2023</a>)</cite>. Commentary is a type of article that contains diverse arguments and compelling evidence, which aims to provide readers with a deep understanding of certain events. As Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">1</span></a> shows, a commentator usually spends several hours writing a commentary, which includes mining arguments, searching for evidence, and embellishing the article. Given the continuous nature of news, their workload is substantial. Therefore, exploring the application of LLMs in commentary generation is worthwhile.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although LLMs have benefited many generative tasks, they face challenges when directly applied to commentary generation due to unique task requirements. Broadly, the requirements for a commentary can be divided into two levels:</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">1) Fundamental requirements:</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">∙</annotation></semantics></math> The structure should be regular and complete. As Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">1</span></a> shows, the commentary should follow a total division structure.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">∙</annotation></semantics></math> The content should be self-consistent. For example, the arguments in the commentary should not be contradictory, and the evidence must support the arguments.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">2) Advanced requirements:</span></p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">∙</annotation></semantics></math> Arguments should be specific and original. The argument is key to the commentary, representing the author’s stance.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">∙</annotation></semantics></math> Evidence should be convincing, which means the LLMs can’t generate fake evidence, and the evidence is preferably new.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">In this paper, we propose Xinyu, an efficient LLM-based system to assist commentators in Chinese commentary generation. Specifically, for the fundamental requirements, we decompose the generation into several sequential steps, ensuring the generated text is well-structured. We also design targeted supervised fine-tuning (SFT) and strategies for each step to maintain content consistency. For the advanced requirements, we propose an argument ranking model for ranking candidate arguments to ensure quality. Moreover, we construct a comprehensive evidence database, which maintains up-to-date events and books, and then use the technology of retrieval augmented generation (RAG) to generate convincing evidence.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Given the dynamic nature of commentary, traditional metrics for text generation tasks, such as ROUGE or BLEU, fall short in evaluating the overall quality of the commentary. Thus, we propose a comprehensive of evaluation metric for commentary generation that considers five distinct perspectives. In our pilot study, GPT-4 demonstrated performance on par with human annotators, so we employ GPT-4 as the evaluator. The experimental results underscore the quality of the content generated by our system, Xinyu.
In practical terms, we also examined how Xinyu could enhance the work efficiency of human commentators and the result shows that with Xinyu, the speed of commentary generation increased dramatically, reducing the average creation time from 4 hours to a mere 20 minutes. Importantly, this increase in efficiency does not sacrifice the quality of the commentaries.</p>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">To sum up, our main contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We leverage LLMs for the task of commentary generation and propose a system named Xinyu that can assist commentators in generating Chinese commentary 10 times faster with even quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We decompose the commentary and generate it in steps, applying targeted supervised fine-tuning (SFT) for each. This approach ensures the commentary meets its fundamental requirements: it is well-structured and self-consistent.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose an argument ranking module to improve the quality of the arguments and construct a comprehensive knowledge database (e.g., up-to-date events and books) for the generation of evidence with the help of retrieval augmented generation (RAG). This approach ensures the commentary meets the advanced requirements: it is specific and convincing.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We design a comprehensive evaluation method for the commentary generation task with 5 distinct perspectives. The experimental results demonstrate the effectiveness of our proposed Xinyu.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Large Language Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The domain of Natural Language Processing (NLP) has witnessed substantial progress <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib31" title="">2020</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib22" title="">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib43" title="">2022</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib32" title="">2023a</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib15" title="">2023b</a>)</cite>, especially through the advent of Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib19" title="">2022</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib18" title="">2023</a>; Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib28" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite>. These models show exceptional text generation proficiency, yielding high fluency and readability outputs <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib33" title="">2023b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib40" title="">2024</a>)</cite>. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Supervised Fine-Tuning</span> (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model’s parameters to align with the data distribution and task requirements, ensuring the model’s behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research.
<cite class="ltx_cite ltx_citemacro_citet">Ouyang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib19" title="">2022</a>)</cite> pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent.
<cite class="ltx_cite ltx_citemacro_citet">Zhou et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib42" title="">2023</a>)</cite> compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Retrieval Augmented Generation</span> (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib16" title="">2023</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib10" title="">2023</a>)</cite>.
RAG has been further enhanced by a range of innovative techniques:
fine-tuning retrieval models to obtain precise semantic representations <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib12" title="">2023</a>; VoyageAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib29" title="">2023</a>; Xiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib34" title="">2023</a>)</cite>,
reformulating queries to align with the semantic space of queries and documents <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib9" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib30" title="">2023</a>; Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib21" title="">2023</a>)</cite>,
fine-tuning LLMs to harmonize the output of the retriever with the LLM’s preference <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib35" title="">2023</a>; Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib11" title="">2022</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib23" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">In our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Domain-specific LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Large Language Models (LLMs) have advanced the field of natural language processing, providing a task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, and the diversity of the constraints <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib41" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Numerous researchers have devoted their efforts to domain-specific Language Models (LLMs) tailored for various fields.
These specialized LLMs have been designed to cater to the unique requirements of domains such as
medicine <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib13" title="">2023</a>; Singhal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib24" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib25" title="">2023</a>)</cite> for medical diagnosis,
law <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib7" title="">2023</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib37" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib38" title="">a</a>)</cite> for handling legal documents,
counselling <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib14" title="">2023a</a>)</cite> for mental health support,
education <cite class="ltx_cite ltx_citemacro_citep">(Malinka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib17" title="">2023</a>; Gan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib8" title="">2023</a>)</cite> for teaching assistance,
science<cite class="ltx_cite ltx_citemacro_citep">(Cardenas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib4" title="">2023</a>)</cite> for crafting scientific journalism, and so on.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The former domain-specific LLMs mainly focus on injecting domain knowledge (e.g., medical or legal knowledge) into LLMs. In this paper, our focus is on commentary generation, to support commentators in their writing process and produce well-structured, logically consistent commentaries that present novel arguments and convincing evidence.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Preliminaries</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section is dedicated to
defining key concepts that will be consistently referenced throughout this paper.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Peg</span>, within the scope of commentary generation, denotes the specific aspect of the event that the commentary is responding to or building upon. It acts as an anchor for the commentary. For instance, in Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, the peg is ‘Age of smokers decrease’.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Main Argument</span>, in the context of commentary generation, signifies the central point that the generated commentary seeks to communicate. It forms the core message around which the commentary is structured.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Supporting Argument</span>, is an additional point that helps to substantiate the main argument. Typically, a commentary will contain several supporting arguments that collectively contribute to the strength and depth of the main argument.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Evidence</span> refers to the data, facts, or information employed to support the argument. In the process of commentary generation, evidence can be derived from the content itself or external sources.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Given a peg, the corresponding commentary will include one main argument and several supporting arguments, all of which are supported by evidence.

</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="S3.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The overall framework of Xinyu. The generation process is divided into 5 steps.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Technical route of Xinyu</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we delve into Xinyu’s comprehensive technical route. Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S3.F2" title="Figure 2 ‣ 3. Preliminaries ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall framework of Xinyu. In Section 4.1, we introduce the five main generative components used in detail during the commentary generation process. In Section 4.2, we shift our focus to the two auxiliary components essential for meeting the advanced requirements of the commentary, including the argument ranking model and the construction of an evidence database. Note that without these two auxiliary components, the system can still fulfill the fundamental requirements.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Main Components</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Based on the structure, we decompose the commentary generation into five steps: peg generation, main argument generation, supporting argument generation, evidence generation, and finally, article combination. This sequential approach is implemented with the help of SFT and RAG<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Note the user can interact with Xinyu by providing additional input or editing the output at each step and here we mainly describe the automatic process.</span></span></span>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Peg Generation</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The peg generation serves as a preliminary step in the commentary generation process, designed to swiftly summarize event details for the user. Utilizing a search engine, this component retrieves event details based on given keywords to generate a peg. Alternatively, users have the option to manually compose the peg, bypassing this automated step.
Specifically, the content from the top three most relevant search results is processed as input, and the LLM condenses this information into a concise peg. The procedure is exemplified as follows:
</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p2.1.1">[You are a commentary writing expert, and here are the details of an event. Event detail: </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS1.p2.1.2">event detail</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p2.1.3">. Please refine it into a concise and well-articulated peg:]</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">To enhance the model’s proficiency in condensing event details during peg generation, we develop SFT data specifically for this step. By inputting event details and using the peg as a label, this method trains the model to more effectively summarize and pinpoint essential information, resulting in pegs that are informationally dense.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Main Argument Generation</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">This step aims to provide the main argument. Due to the variety of pegs, main arguments can be driven in different directions. Here the strategy involves directing the LLM to generate across ten distinct directions: technology, finance, society, politics, literature and arts, lifestyle, environment, sports, education, and science. Each direction emphasizes its specific thematic elements, such as highlighting technological advancements or economic trends.
To operationalize this strategy, we combine the peg, event details, and a chosen direction as input for the LLM, which then generates candidate main arguments one at a time. An example of the usage is below:
</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.1">[You are a commentary writing expert. Please complete the main argument in the direction of </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS2.p2.1.2">direction</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.3"> based on the peg: </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS2.p2.1.4">peg</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.5"> and event detail: </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS2.p2.1.6">event detail</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.7">. The main argument should be profound, concise, and strongly related to the peg. Please provide the main argument:]
</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">To enhance the model’s ability to generate the main arguments, we design the corresponding SFT data. The input for this SFT data includes retrieved event details based on the peg and the direction of the article.
The label for this SFT data is the main argument derived from the input. This data construction approach not only facilitates the generation of a helpful main argument for diverse article directions but also ensures its consistency with the initial peg, thus guaranteeing both relevance and alignment in the narrative.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">To meet the advanced requirements of argument, these generated main arguments will then get a score from the argument ranking model, based on their novelty, and objectivity. Then these candidate main arguments will be ranked based on the scores.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Supporting Argument Generation</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">This step aims to generate supporting arguments that seamlessly align with both the main argument and the event’s details. To achieve this, the system synthesizes the main argument, event details, and a predefined argument structure. Available argument structures include parallel, progressive, and contrasting formats, each facilitating a unique commentary structure.
This integration process enables the LLM to produce relevant supporting arguments. The LLM will decide the number of supporting arguments <math alttext="m" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.1.m1.1"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mi id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><ci id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.1.m1.1d">italic_m</annotation></semantics></math> itself. An example of usage could be:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.1">[You are a commentary writing expert. Based on the given main argument </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p2.1.2">main argument</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.3"> of the commentary and event detail </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p2.1.4">event detail</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.5">, generate multiple supporting arguments for the commentary.
The supporting arguments form </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p2.1.6">structure</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.7"> structure, refining around the </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p2.1.8">main argument</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.9"> with multi-level, multi-faceted, and multi-angle perspectives. Please provide the supporting arguments:]
</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1">The SFT data for this step is constructed to facilitate this process. We utilize inputs comprising event details, main arguments, and argument structures. The labels are the marked corresponding supporting arguments. This data construction approach ensures that the model is fine-tuned to produce supporting arguments that enrich and substantiate the main argument effectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Evidence Generation</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">In the Evidence Generation step, the system aims to generate accurate and contextually relevant evidence.
The process begins with accessing reference information of a supporting argument from the evidence database to ensure veracity, effectively mitigating the hallucinations. This reference information, along with the provided main and supporting arguments, serves as the input. Then, the LLM will generate evidence that is tailored to align precisely with the given supporting argument. An example of the usage is below:
</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.1">[You are a commentary writing expert. Surrounding the main argument </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS4.p2.1.2">main argument</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.3">, please use the evidence provided in the reference information, including dates, data, viewpoints, core content, etc., to continue writing evidence in the commentary to support the supporting argument </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS4.p2.1.4">supporting argument</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.5">. Please annotate the corresponding reference information numbers in the continuation.
Reference information: </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS4.p2.1.6">reference</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.7">.
Please provide the evidence:
]</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p3">
<p class="ltx_p" id="S4.SS1.SSS4.p3.1">The SFT data for evidence generation is structured with inputs including reference information, main and supporting arguments. The output label is the evidence associated with the corresponding supporting argument. These elements guide the model in generating evidence that is precise and contextually relevant to the provided supporting arguments.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5. </span>Article Combination</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">This step is aimed at generating the title and ending, then forming the overall commentary.
To achieve this, the system integrates the preceding event details, main arguments, supporting arguments, and evidence as inputs. Following this integration, the LLM then outputs the title and ending. An example of the usage for ending generation is below:
</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS5.p2.1.1">[You are a commentary writing expert. Please write a conclusion for the article, maintaining smooth language, consistent style, and logical coherence with the preceding text. The preceding text is as follows: </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS5.p2.1.2">preceding text</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS5.p2.1.3">. Please provide the ending:]
</span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p3">
<p class="ltx_p" id="S4.SS1.SSS5.p3.1">The usage for title generation is similar to the ending generation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p4">
<p class="ltx_p" id="S4.SS1.SSS5.p4.1">To facilitate the model in generating context-appropriate and coherent title and ending, the SFT data is constructed with inputs including the event details, main argument, combined supporting arguments and evidence. The label for this data is the corresponding title and ending. This structured approach ensures that the model is adept at crafting titles and endings that effectively encapsulate the various dimensions of the commentary, providing a fitting start and end to the narrative.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p5">
<p class="ltx_p" id="S4.SS1.SSS5.p5.1">After generating the title and ending, the system will combine all the output to form a complete commentary.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Auxiliary Components</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this section, we introduce two auxiliary components that assist Xinyu in meeting advanced requirements: the argument ranking model and the evidence database.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Argument Ranking Model</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The argument ranking model plays a crucial role in the main argument generation process by assessing and ranking candidate main arguments.
This aids users in selecting the most compelling argument.
</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Developing the ranking model presents a central challenge due to the subjective nature of assessing arguments, which lack universally accepted standards, unlike quantifiable metrics.
For example, evaluating the generated arguments based on factors like novelty is cumbersome.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.6">To address this challenge, we train a BERT-based scoring model with a pairwise loss function.
This approach converts the ranking challenge into a series of binary comparisons, simplifying the task to discerning relative superiority between pairs of arguments.
The loss function is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(x)=\sum\Phi(f(x_{a})-f(x_{b}))" class="ltx_Math" display="block" id="S4.E1.m1.2"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mrow id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.3.2" xref="S4.E1.m1.2.2.3.2.cmml">ℒ</mi><mo id="S4.E1.m1.2.2.3.1" xref="S4.E1.m1.2.2.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.3.3.2" xref="S4.E1.m1.2.2.3.cmml"><mo id="S4.E1.m1.2.2.3.3.2.1" stretchy="false" xref="S4.E1.m1.2.2.3.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">x</mi><mo id="S4.E1.m1.2.2.3.3.2.2" stretchy="false" xref="S4.E1.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.2" rspace="0.111em" xref="S4.E1.m1.2.2.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.cmml"><mo id="S4.E1.m1.2.2.1.2" movablelimits="false" xref="S4.E1.m1.2.2.1.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.3" mathvariant="normal" xref="S4.E1.m1.2.2.1.1.3.cmml">Φ</mi><mo id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.3.cmml">f</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">a</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml">−</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.3.cmml">f</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml">(</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.2.cmml">x</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.3.cmml">b</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.3" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><eq id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"></eq><apply id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3"><times id="S4.E1.m1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.3.1"></times><ci id="S4.E1.m1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.3.2">ℒ</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝑥</ci></apply><apply id="S4.E1.m1.2.2.1.cmml" xref="S4.E1.m1.2.2.1"><sum id="S4.E1.m1.2.2.1.2.cmml" xref="S4.E1.m1.2.2.1.2"></sum><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1"><times id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2"></times><ci id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3">Φ</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3"></minus><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.3">𝑓</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝑎</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"><times id="S4.E1.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.2"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.3">𝑓</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.2">𝑥</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2.1.1.1.3">𝑏</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\mathcal{L}(x)=\sum\Phi(f(x_{a})-f(x_{b}))</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.2d">caligraphic_L ( italic_x ) = ∑ roman_Φ ( italic_f ( italic_x start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ) - italic_f ( italic_x start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.SSS1.p3.5">where <math alttext="f(x)" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.1.m1.1"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mrow id="S4.SS2.SSS1.p3.1.m1.1.2" xref="S4.SS2.SSS1.p3.1.m1.1.2.cmml"><mi id="S4.SS2.SSS1.p3.1.m1.1.2.2" xref="S4.SS2.SSS1.p3.1.m1.1.2.2.cmml">f</mi><mo id="S4.SS2.SSS1.p3.1.m1.1.2.1" xref="S4.SS2.SSS1.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.SSS1.p3.1.m1.1.2.3.2" xref="S4.SS2.SSS1.p3.1.m1.1.2.cmml"><mo id="S4.SS2.SSS1.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S4.SS2.SSS1.p3.1.m1.1.2.cmml">(</mo><mi id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.SSS1.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S4.SS2.SSS1.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><apply id="S4.SS2.SSS1.p3.1.m1.1.2.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.2"><times id="S4.SS2.SSS1.p3.1.m1.1.2.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.2.1"></times><ci id="S4.SS2.SSS1.p3.1.m1.1.2.2.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.2.2">𝑓</ci><ci id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">f(x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.1.m1.1d">italic_f ( italic_x )</annotation></semantics></math> represents the scoring function for a given argument <math alttext="x" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.2.m2.1"><semantics id="S4.SS2.SSS1.p3.2.m2.1a"><mi id="S4.SS2.SSS1.p3.2.m2.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.2.m2.1b"><ci id="S4.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.2.m2.1d">italic_x</annotation></semantics></math>, and <math alttext="\Phi" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.3.m3.1"><semantics id="S4.SS2.SSS1.p3.3.m3.1a"><mi id="S4.SS2.SSS1.p3.3.m3.1.1" mathvariant="normal" xref="S4.SS2.SSS1.p3.3.m3.1.1.cmml">Φ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.3.m3.1b"><ci id="S4.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p3.3.m3.1.1">Φ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.3.m3.1c">\Phi</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.3.m3.1d">roman_Φ</annotation></semantics></math> is a non-linear transformation applied to the calculated difference between the two arguments <math alttext="x_{a}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.4.m4.1"><semantics id="S4.SS2.SSS1.p3.4.m4.1a"><msub id="S4.SS2.SSS1.p3.4.m4.1.1" xref="S4.SS2.SSS1.p3.4.m4.1.1.cmml"><mi id="S4.SS2.SSS1.p3.4.m4.1.1.2" xref="S4.SS2.SSS1.p3.4.m4.1.1.2.cmml">x</mi><mi id="S4.SS2.SSS1.p3.4.m4.1.1.3" xref="S4.SS2.SSS1.p3.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.4.m4.1b"><apply id="S4.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p3.4.m4.1.1.1.cmml" xref="S4.SS2.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p3.4.m4.1.1.2.cmml" xref="S4.SS2.SSS1.p3.4.m4.1.1.2">𝑥</ci><ci id="S4.SS2.SSS1.p3.4.m4.1.1.3.cmml" xref="S4.SS2.SSS1.p3.4.m4.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.4.m4.1c">x_{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="x_{b}" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.5.m5.1"><semantics id="S4.SS2.SSS1.p3.5.m5.1a"><msub id="S4.SS2.SSS1.p3.5.m5.1.1" xref="S4.SS2.SSS1.p3.5.m5.1.1.cmml"><mi id="S4.SS2.SSS1.p3.5.m5.1.1.2" xref="S4.SS2.SSS1.p3.5.m5.1.1.2.cmml">x</mi><mi id="S4.SS2.SSS1.p3.5.m5.1.1.3" xref="S4.SS2.SSS1.p3.5.m5.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.5.m5.1b"><apply id="S4.SS2.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS2.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p3.5.m5.1.1.1.cmml" xref="S4.SS2.SSS1.p3.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p3.5.m5.1.1.2.cmml" xref="S4.SS2.SSS1.p3.5.m5.1.1.2">𝑥</ci><ci id="S4.SS2.SSS1.p3.5.m5.1.1.3.cmml" xref="S4.SS2.SSS1.p3.5.m5.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.5.m5.1c">x_{b}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">The quantity of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p4.1.1">likes</span> on articles from opinion-sharing platforms, such as Zhihu, is often indicative of the novelty and objectivity of the arguments they present. Consequently, this metric is leveraged to assess the quality of the arguments within these articles. In the process of constructing the training dataset, we collect articles from such platforms, utilizing the number of likes as a criterion to establish a partial order among pairs of texts, and the dataset consists of 240,000 text pairs. This order serves to reflect their relative quality. In the inference phase, we translate the order to numerical score, and the scores of the candidate main arguments are utilized to rank these arguments.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Evidence Database Construction</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In pursuit of generating convincing evidence, we construct an Evidence Database to store Chinese knowledge sourced from events and books for retrieval.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">For the events knowledge, we first legally collect the daily updated article titles on the website’s hot list, and then prompt the LLM to complete the following four tasks given the article title:
(1) summarize the event related to the article title;
(2) determine which direction (e.g., technology, finance) the event belongs to;
(3) extract the six elements of the event, including time, location, person, cause, process and result;
(4) describe the event in a paragraph based on the six elements.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">For knowledge from books,
we gather classic works in law, finance, and various other subjects legally, segmenting the contents of these books into chunks and storing them within the database.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.2">The evidence database is built upon 200,000 event knowledge data and 110,000 book knowledge data.
Following the construction phase, we implement the ElasticSearch engine, anchored to the evidence database, to enhance retrieval capabilities. During retrieval, the supporting argument is inputted, prompting the fetching of the
<math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p4.1.m1.1"><semantics id="S4.SS2.SSS2.p4.1.m1.1a"><mi id="S4.SS2.SSS2.p4.1.m1.1.1" xref="S4.SS2.SSS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p4.1.m1.1b"><ci id="S4.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p4.1.m1.1d">italic_k</annotation></semantics></math> most pertinent references from the database. These references are then fed into the Large Language Model (LLM) to generate evidence in support of the arguments. In practical application, the value of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p4.2.m2.1"><semantics id="S4.SS2.SSS2.p4.2.m2.1a"><mi id="S4.SS2.SSS2.p4.2.m2.1.1" xref="S4.SS2.SSS2.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p4.2.m2.1b"><ci id="S4.SS2.SSS2.p4.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p4.2.m2.1d">italic_k</annotation></semantics></math> is determined by the similarity score between the input argument and the existing knowledge, and we set a predefined threshold at 0.6 in the experiment.
In addition, to maintain access to the most current event knowledge, we continuously collect data from online platforms and update our database daily.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiment</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Evaluation Metrics of Commentary</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Automatic evaluation</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.5">Existing automatic generation evaluation metrics, including but not limited to ROUGE and BLEU<cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib20" title="">2002</a>)</cite>, mainly focus on the degree of similarity to a reference text. However, in the context of commentary generation, the inherent diversity of the commentary content poses a significant challenge to these similarity-based metrics, often leading to an incomplete evaluation. To address this limitation, we propose a novel evaluation metric that assesses commentaries across five distinct dimensions:
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.1">Structure Soundness</span>: clarity of the hierarchy, compactness of the writing, and rationality of the layout;
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><mo id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.2.m2.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.2">Logic Consistency</span>: consistency of the content, rationality of the argument, and thoughtfulness;
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS1.SSS0.Px1.p1.3.m3.1a"><mo id="S5.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.3.m3.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.3.m3.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.3">Argument Quality</span>: freshness and directionality of the topic conception;
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S5.SS1.SSS0.Px1.p1.4.m4.1a"><mo id="S5.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S5.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.4.m4.1b"><ci id="S5.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.4.m4.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.4.m4.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.4">Evidence Support</span>: specificity and appropriateness of the evidence used; and
<math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S5.SS1.SSS0.Px1.p1.5.m5.1a"><mo id="S5.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S5.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.5.m5.1b"><ci id="S5.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.5.m5.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.5.m5.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.5.m5.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.5">Language Finesse</span>: fluency, depth, and vividness of the expression style. Besides, we calculate the average of the five scores as <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.5.6">Overall</span>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">The prompt templates are as follows:</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p3.1"><span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p3.1.1">[You are an expert in scoring generated commentaries. Please rate your answers from the </span>{<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.SSS0.Px1.p3.1.2">perspective</span>}<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p3.1.3"> perspective based on the provided commentary. The scoring criteria are:</span></p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p4">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p4.1"><span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p4.1.1">(1) 10 points represent…
(2) 8 points represent…
(3) 6 points …
</span></p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p5">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p5.1"><span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p5.1.1">Please output a line that contains only one value representing the score. Please avoid any potential biases, and ensure that there are no factors other than the text that affect your judgment.]</span></p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p6">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p6.1">These dimensions are chosen to encompass both the fundamental and advanced requirements of commentary. Structural soundness and logical consistency constitute the fundamental requirements, ensuring a well-organized and logically coherent commentary. Conversely, the quality of argumentation and the adequacy of evidentiary support represent the advanced requirements, reflecting the depth and persuasiveness of the commentary. The dimensions are scored on a scale of 1-10, with 1 being the lowest and 10 the highest.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p7">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p7.1">In our experiments, we utilize GPT-4 for automatic evaluation by crafting specific prompts for each dimension. To validate GPT-4’s accuracy, we compare its scores against those from human annotators for 30 randomly selected commentaries, calculating the Pearson correlation coefficient <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib6" title="">2009</a>)</cite> for each dimension. As Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T1" title="Table 1 ‣ Human evaluation ‣ 5.1. Evaluation Metrics of Commentary ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates, the Pearson correlation coefficient of each dimension surpasses 0.6, which proves GPT-4 is competent for this task.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Human evaluation</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">In our ablation study, we assess the <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.1">Timeliness</span> of evidence. Due to the training limitations of GPT-4, which is based on data available only up to a specific date, it is not equipped to accurately ascertain the recency of evidence. Therefore, this aspect is evaluated through human judgment. The scoring for this metric ranges from 1 to 10, where 1 represents the lowest and 10 the highest possible score.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.1">Dimension</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.2">Structure</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.3">Logic</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.4">Argu.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.5">Evidence</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.6">Language</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.1">Pearson’s r</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.2">0.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.3">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.4">0.73</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.5">0.66</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.1.2.2.6">0.64</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Consistency analysis of Human and GPT-4 on five dimensions. Arug. refers to Argument, and r means the Pearson correlation coefficient.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Experiment Settings</h3>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Implementation</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">The base model of Xinyu is LLaMA2-13B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib28" title="">2023</a>)</cite>, and we specifically adapted it to better accommodate the nuances of the Chinese language. This adaptation involved expanding the LLaMA-13B tokenizer with an additional 28,000 Chinese tokens. To further optimize the model, we continued pre-training on LLaMA-13B using a corpus comprising 500B tokens, which contains both English and Chinese corpus. For supervised fine-tuning, we not only utilized the dataset introduced in Section 4 but also incorporated the general SFT dataset<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/BelleGroup/train_2M_CN" title="">https://huggingface.co/datasets/BelleGroup/train_2M_CN</a></span></span></span> to maintain consistency with the data distribution of previous training phases. The amount of SFT data is 400,000 and the distribution of it is shown in Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.F3" title="Figure 3 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.
Our training process leveraged the Megatron-DeepSpeed framework. The continued pre-training phase lasted 20 days on 128 Nvidia A800 80G GPUs, while the supervised fine-tuning (SFT) process took 2 days on 8 Nvidia A800 80G GPUs.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S5.F3.g1" src="x3.png" width="290"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>
The distribution of SFT datast. Argu. refers to Argument, Sup. means Supporting.
</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Methods</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Overall</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Structure</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1">Logic</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.5.1">Argument</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.6.1">Evidence</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.7.1">Language</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.2.2.1">Baichuan2-Turbo <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.2">7.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.3">8.11*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.4">8.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.5">8.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.6">5.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.7">8.17</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.3.3.1">Qwen-72B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.2">7.37</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.3">7.90</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.4">7.88</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.5">7.87</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.6">5.28</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.3.7">7.90</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.4.1">GLM-4 <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib39" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.2">7.72</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.3">8.11*</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.4">8.35</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.5">8.08*</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.6">5.82</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.7">8.24</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.5.1">ERNIE-4 <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib26" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.2">7.71</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.3">8.05</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.4">8.35</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.5">8.03</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.6">5.73</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.7">8.38*</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.6.1">GPT-3.5-Turbo <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib19" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.2">7.70</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.3">8.00</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.4">8.08</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.5">8.00</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.6">6.39</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.7">8.05</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.7.7.1">GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib18" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.2">7.78</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.3">8.05</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.4">8.40*</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.5">8.05</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.6">6.02</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.7.7">8.38*</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.8.8.1">Baichuan2-13B-Chat <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.2">7.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.3">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.8.8.3.1">8.11</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.4">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.5">7.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.6">4.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.8.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.8.8.7.1">8.05</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.9.9.1">Qwen-14B-Chat <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.2">7.25</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.3">8.05</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.4">7.88</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.5">7.85</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.6">4.55</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.7">7.95</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.10.10.1">InternLM-20B-Chat <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib27" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.2">7.26</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.3">7.80</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.4">7.83</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.5">7.88</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.6">4.83</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.10.10.7">8.00</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T2.1.11.11.1">Xinyu (based on fine-tuned LLaMA2-13B)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.2">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.11.11.2.1">7.93</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.3">8.00</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.11.4.1">8.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.11.5.1">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.6">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.11.11.6.1">7.41</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.11.11.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.11.7.1">8.05</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Evaluation of commentary generated by baseline LLMs using GPT-4. Reference is the published commentaries. ‘Bold’ indicates the highest score within the 20B scale baselines, and an asterisk (*) denotes the highest score among all baselines.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Structure</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">Logic</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.5.1">Argument</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.6.1">Evidence</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.7.1">Language</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.1">Qwen-72B-Chat <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">7.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.3">7.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.4">7.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.5">7.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.6">7.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.7">7.90</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.2.1">Baichuan2-Turbo <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.2">7.91</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.3">8.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.4">7.93</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.5">7.70</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.6">8.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.7">7.80</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.3.1">GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib18" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.2">8.30*</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.3">8.10*</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.4">8.58*</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.5">8.15*</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.6">8.17*</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.7">8.50*</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.5.4.1">Baichuan2-13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.2">6.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.3">5.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.4">6.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.5">6.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.6">6.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.7">6.23</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.6.5.1">Qwen-14B-Chat <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.2">6.22</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.3">5.60</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.4">6.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.5">6.25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.6">6.80</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.7">6.20</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.7.6.1">Xinyu</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.2.1">7.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.3.1">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.4.1">8.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.5.1">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.6.1">7.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.7.6.7.1">8.05</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>GPT-4’s evaluation of commentary generated by our framework with different base models. ‘Bold’ indicates the highest score within the 20B scale baselines, and an asterisk (*) denotes the highest score among all baselines.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">Overall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Structure</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">Logic</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.5.1">Argument</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.6.1">Evidence</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.7.1">Language</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.2.1.1">w/o Ranking</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.2">7.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.3">7.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.4">8.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.5">7.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.6">7.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.2.1.7">8.02</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T4.1.3.2.1">Xinyu</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.2.1">7.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.3.1">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.4.1">8.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.5.1">8.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.6.1">7.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S5.T4.1.3.2.7.1">8.05</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Results of ablation study on Argument Ranking Model.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">We employ the following methods as our baselines:</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p2.1">Baichuan2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib36" title="">2023</a>)</cite> represents a series of large-scale, multilingual language models trained from scratch on 2.6 trillion tokens. We select <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.1">Baichuan2-13B-Chat</span> and <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.2">Baichuan2-Turbo</span> as our baseline models. Qwen <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib3" title="">2023</a>)</cite> is a comprehensive series of language models featuring a range of models with varying parameter counts. In this context, we choose <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.3">Qwen-72B</span> and <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.4">Qwen-72B-Chat</span> as our baseline models. InternLM <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib27" title="">2023</a>)</cite> consists of a series of multilingual foundation models and chat models, with <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.5">InternLM-20B-Chat</span> selected as the baseline model. GLM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib39" title="">2023</a>)</cite> is a series of bilingual (English and Chinese) pre-trained language models, for which we use <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.6">GLM-4</span> as the baseline model. ERNIE <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib26" title="">2021</a>)</cite> serves as a unified framework for pre-training large-scale knowledge-enhanced models, with <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.7">ERNIE-4</span> chosen as our baseline model. Finally, GPT is a series of large language models released by OpenAI, with <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.8">GPT-3.5-Turbo</span> <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib19" title="">2022</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p2.1.9">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#bib.bib18" title="">2023</a>)</cite> used as baseline models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Test Cases</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">For our test cases, we have carefully chosen 41 commentaries from the “Three Commentaries” section of People’s Daily Online<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://opinion.people.com.cn/GB/8213/420650/index.html" title="">http://opinion.people.com.cn/GB/8213/420650/index.html</a></span></span></span>. This selection encompasses a diverse range of topics including economics, livelihood, technology, culture, social issues, sports, and art, reflecting current news and societal trends. The prominence of the site and the authoritative nature of the series ensure that these articles represent high-quality journalistic commentary. In our baselines, we employ a one-step generation process using Event Detail, and extra Title, Argument, and Evidence, which are from the real commentary of the event. This setup is designed to emulate real-world scenarios where commentators use LLMs. The translated prompt in English is:</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p2.1"><span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p2.1.1">[I will provide you with a news background: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p2.1.2">Event detail</span>}</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p3.1"><span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.1">Based on this news, with the title ’Title’, please create a commentary article. The article should have clear and profound arguments, true and abundant evidence, smooth logical reasoning, reasonable structure, and appropriate commentary language. Your article should reference the following argument and evidence:</span>
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.2">
Argument 1: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.3">Argument 1</span>}
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.4">
Evidence 1: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.5">Evidence 1</span>}
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.6">
Argument 2: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.7">Argument 2</span>}
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.8">
Evidence 2: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.9">Evidence 2</span>}
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.10">
Argument 3: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.11">Argument 3</span>}
<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.12">
Evidence 3: </span>{<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.13">Evidence 3</span>}<span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p3.1.14">]</span></p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p4">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p4.1">The number of <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p4.1.1">Arguments</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p4.1.2">Evidence</span> will be 0 to 3.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p5">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p5.1">However, for Xinyu, we only supply the event details to evaluate the effectiveness of our approach in generating commentary.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Ablation</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">In our ablation experiments, we adopt the following configurations to assess specific components:</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p2.1">1) Evaluating the impact of the framework: While maintaining the overall framework intact, we replace the Xinyu-13B model with alternative large language models (LLMs) to determine the effectiveness of the framework.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p3.1">2) Assessing ranking efficiency: We eliminate the argument ranking component, allowing the LLM to directly generate a main argument without a predefined direction, to evaluate the ranking model’s contribution.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p4">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p4.1">3) Investigating the role of RAG: By omitting the retrieval process from the evidence database, we let the LLM independently produce the evidence, aiming to understand the significance of the RAG component in enhancing model performance and the effectiveness of our construed evidence database. Moreover, we assess the timeliness of the generated evidence especially.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p5">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p5.1">4) Measuring the practicality of Xinyu: We compared the quality of commentary articles generated by human experts with a journalism background with the aid of Xinyu, against those written purely manually, while also comparing the time spent. We randomly select 10 test cases.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S5.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Case study. The content is translated from Chinese.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.2.1">Evidence</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.3.1">Timeliness</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.1.1">w/o RAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.2">5.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.3">8.85</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.3.2.1">w/ Event</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.2.2">7.14</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.2.3">9.10</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T5.1.4.3.1">w/ Event + Book</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.3.2.1">7.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.3.3.1">9.30</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Results of ablation study on Evidence Database. The Kappa value of Timeliness exceeds 0.78.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S5.F5.g1" src="x5.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>
Human vs. Xinyu-Assisted.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Experimental Results and Analysis</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Based on the model’s size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T2" title="Table 2 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of commentary generation with GPT-4’s evaluation. We report the results of the ablation study in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T3" title="Table 3 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T4" title="Table 4 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T5" title="Table 5 ‣ Ablation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, and Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.F5" title="Figure 5 ‣ Ablation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Results of commentary generation</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">From Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T2" title="Table 2 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">2</span></a>, we can conclude that:
(1) Generally, the bigger the language model’s size, the better it does. However, GPT-4’s leading advantage in this task is not as pronounced as in other generative tasks.
(2) When looking at models within the 20 billion parameter size, our method achieved the best results in most of the metrics.
(3) Compared to large-scale LLMs such as GPT-4, our method attained the best overall score, primarily due to our superior performance in the advanced requirements of argument and evidence.
(4) There’s not a huge gap between the scores of the different methods. This is largely because GPT-4 is generally not harsh in its scoring, rarely giving out very low scores.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Result of ablation study</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">From Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T3" title="Table 3 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, we have the following observations:
(1) Our framework significantly enhances the performance of large-sized base models. For instance, the overall score of Qwen-72B increased from 7.37 to 7.82.
(2) GPT-4 achieved the best performance with an overall score of 8.3, and our Xinyu ranks just behind GPT-4. Considering the size of the model, our method has greater potential in practice.
(3) For the 20B scale base models such as Qwen-14B-Chat, using our framework actually decreased their performance. This might be due to these base models’ inherent limitations in generating text step-by-step. This also demonstrates the effectiveness of our SFT.
</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p2.1">From Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T4" title="Table 4 ‣ Implementation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, we have the following observations:
(1) The implementation of the argument ranking model has significantly improved the effectiveness of argumentation, underscoring its impact.
(2) Enhancements are observed across all metrics, illustrating the interrelationship among these aspects of commentary.
</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p3">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p3.1">From Tab. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.T5" title="Table 5 ‣ Ablation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, we can find that:
(1) The implementation of retrieval augmented generation (RAG) significantly enhances the generation of evidence, with scores improving from 5.60 to 7.41. Additionally, the timeliness of the generated evidence also saw an increase, rising from 8.85 to 9.30.
(2) After incorporating the book dataset into RAG, its performance experienced further improvements.
(3) The improvement proves the effectiveness of our evidence dataset.
</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p4">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p4.1">From Figure. <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.F5" title="Figure 5 ‣ Ablation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, we have the following observations:
(1) Utilizing Xinyu’s assistance can significantly increase writing speed, and the average time for a commentary speeds up from more than 4 hours to 20 mins.
(2) Moreover, commentaries generated with LLMs have achieved the same scores as manual writing, demonstrating the practicality of our system.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Case study</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11609v2#S5.F4" title="Figure 4 ‣ Ablation ‣ 5.2. Experiment Settings ‣ 5. Experiment ‣ Xinyu: An Efficient LLM-based System for Commentary Generation"><span class="ltx_text ltx_ref_tag">4</span></a> presents three commentaries on a certain peg generated by Xinyu, GPT-4, and Baichuan2-13B-Chat, respectively. All three commentaries exhibit good language fluency and structural coherence, highlighting the capabilities of these large language models (LLMs). However, the commentary from Baichuan2-13B-Chat focuses solely on facts without offering specific arguments. In contrast, both GPT-4 and Xinyu provide detailed arguments. Notably, Xinyu’s commentary stands out by presenting more convincing evidence and demonstrating a logical correlation in its supporting arguments.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce Xinyu, an innovative commentary generation system based on large language models (LLMs) designed to enhance the efficiency of commentators. Our approach involves breaking down the generation process into five steps, with supervised fine-tuning (SFT) applied to each step to ensure the output is well-structured and coherent, addressing the basic requirements of commentary. To fulfill the higher demands for novelty and persuasiveness, we develop an argument ranking model and employ retrieval-augmented generation (RAG) techniques for evidence generation. For RAG, we have compiled an evidence database comprising both current events and classical books. To better measure the generated commentaries, we design a comprehensive evaluation method with 5 distinct perspectives. Our comprehensive experiments demonstrate the system’s effectiveness. Remarkably, in practical applications, Xinyu has reduced the average commentary creation time from 4 hours to just 20 minutes and maintained the quality.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In the future, we will consider the following directions to enhance our system: 1) Improve evidence recall accuracy, ensuring relevance to the arguments; 2) Utilize Reinforcement Learning with Human Feedback (RLHF) to better align commentaries with human preferences and specific writing styles.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the National Natural Science Foundation of China (62441605, 62376243, 62037001, U20A20387), and the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010).</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Finally, we would like to thank the anonymous reviewers for their helpful feedback and suggestions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adeshola and Adepoju (2023)</span>
<span class="ltx_bibblock">
Ibrahim Adeshola and Adeola Praise Adepoju. 2023.

</span>
<span class="ltx_bibblock">The opportunities and challenges of ChatGPT in education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Interactive Learning Environments</em> (2023), 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">CoRR</em> abs/2309.16609 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.16609" title="">https://doi.org/10.48550/ARXIV.2309.16609</a>
arXiv:2309.16609

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cardenas et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ronald Cardenas, Bingsheng Yao, Dakuo Wang, and Yufang Hou. 2023.

</span>
<span class="ltx_bibblock">’Don’t Get Too Technical with Me’: A Discourse Structure-Based Framework for Automatic Science Journalism. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 1186–1202.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.76" title="">https://aclanthology.org/2023.emnlp-main.76</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Szu-Wei Cheng, Chung-Wen Chang, Wan-Jung Chang, Hao-Wei Wang, Chih-Sung Liang, Taishiro Kishimoto, Jane Pei-Chen Chang, John S Kuo, and Kuan-Pin Su. 2023.

</span>
<span class="ltx_bibblock">The now and future of ChatGPT and GPT in psychiatry.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Psychiatry and clinical neurosciences</em> 77, 11 (2023), 592–596.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009.

</span>
<span class="ltx_bibblock">Pearson correlation coefficient.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Noise reduction in speech processing</em> (2009), 1–4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023.

</span>
<span class="ltx_bibblock">ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">CoRR</em> abs/2306.16092 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2306.16092" title="">https://doi.org/10.48550/ARXIV.2306.16092</a>
arXiv:2306.16092

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wensheng Gan, Zhenlian Qi, Jiayang Wu, and Jerry Chun-Wei Lin. 2023.

</span>
<span class="ltx_bibblock">Large Language Models in Education: Vision and Opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">CoRR</em> abs/2311.13160 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2311.13160" title="">https://doi.org/10.48550/ARXIV.2311.13160</a>
arXiv:2311.13160

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022.

</span>
<span class="ltx_bibblock">Precise Zero-Shot Dense Retrieval without Relevance Labels.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.10496 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">CoRR</em> abs/2312.10997 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2312.10997" title="">https://doi.org/10.48550/ARXIV.2312.10997</a>
arXiv:2312.10997

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2208.03299 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Li (2023)</span>
<span class="ltx_bibblock">
Xianming Li and Jing Li. 2023.

</span>
<span class="ltx_bibblock">AnglE-optimized Text Embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em> abs/2309.12871 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.12871" title="">https://doi.org/10.48550/ARXIV.2309.12871</a>
arXiv:2309.12871

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. 2023.

</span>
<span class="ltx_bibblock">ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CoRR</em> abs/2303.14070 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2303.14070" title="">https://doi.org/10.48550/ARXIV.2303.14070</a>
arXiv:2303.14070

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
June M. Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. 2023a.

</span>
<span class="ltx_bibblock">ChatCounselor: A Large Language Models for Mental Health Support.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">CoRR</em> abs/2309.15461 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.15461" title="">https://doi.org/10.48550/ARXIV.2309.15461</a>
arXiv:2309.15461

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yifei Liu, Yiquan Wu, Yating Zhang, Changlong Sun, Weiming Lu, Fei Wu, and Kun Kuang. 2023b.

</span>
<span class="ltx_bibblock">Ml-ljp: Multi-law aware legal judgment prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1023–1034.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting for Retrieval-Augmented Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">CoRR</em> abs/2305.14283 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2305.14283" title="">https://doi.org/10.48550/ARXIV.2305.14283</a>
arXiv:2305.14283

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinka et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kamil Malinka, Martin Peresíni, Anton Firc, Ondrej Hujnak, and Filip Janus. 2023.

</span>
<span class="ltx_bibblock">On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1, ITiCSE 2023, Turku, Finland, July 7-12, 2023</em>, Mikko-Jussi Laakso, Mattia Monga, Simon, and Judithe Sheard (Eds.). ACM, 47–53.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3587102.3588827" title="">https://doi.org/10.1145/3587102.3588827</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em> abs/2303.08774 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2303.08774" title="">https://doi.org/10.48550/ARXIV.2303.08774</a>
arXiv:2303.08774

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="">http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a Method for Automatic Evaluation of Machine Translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA</em>. ACL, 311–318.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3115/1073083.1073135" title="">https://doi.org/10.3115/1073083.1073135</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.15294 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kai Shen, Yichong Leng, Xu Tan, Siliang Tang, Yuan Zhang, Wenjie Liu, and Edward Lin. 2022.

</span>
<span class="ltx_bibblock">Mask the correct tokens: An embarrassingly simple approach for error correction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2211.13252</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023.

</span>
<span class="ltx_bibblock">REPLUG: Retrieval-Augmented Black-Box Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.12652 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schärli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Agüera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan
Karthikesalingam, and Vivek Natarajan. 2022.

</span>
<span class="ltx_bibblock">Large Language Models Encode Clinical Knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">CoRR</em> abs/2212.13138 (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2212.13138" title="">https://doi.org/10.48550/ARXIV.2212.13138</a>
arXiv:2212.13138

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan
Karthikesalingam, and Vivek Natarajan. 2023.

</span>
<span class="ltx_bibblock">Towards Expert-Level Medical Question Answering with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">CoRR</em> abs/2305.09617 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2305.09617" title="">https://doi.org/10.48550/ARXIV.2305.09617</a>
arXiv:2305.09617

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021.

</span>
<span class="ltx_bibblock">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">CoRR</em> abs/2107.02137 (2021).

</span>
<span class="ltx_bibblock">arXiv:2107.02137

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.02137" title="">https://arxiv.org/abs/2107.02137</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
InternLM Team. 2023.

</span>
<span class="ltx_bibblock">InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/InternLM/InternLM" title="">https://github.com/InternLM/InternLM</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">CoRR</em> abs/2302.13971 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2302.13971" title="">https://doi.org/10.48550/ARXIV.2302.13971</a>
arXiv:2302.13971

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VoyageAI (2023)</span>
<span class="ltx_bibblock">
VoyageAI. 2023.

</span>
<span class="ltx_bibblock">VoyageAI. Voyage’s embedding models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.voyageai.com/embeddings/" title="">https://docs.voyageai.com/embeddings/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Query2doc: Query Expansion with Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.07678 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. 2020.

</span>
<span class="ltx_bibblock">De-biased court’s view generation with causality. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 763–780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yiquan Wu, Weiming Lu, Yating Zhang, Adam Jatowt, Jun Feng, Changlong Sun, Fei Wu, and Kun Kuang. 2023a.

</span>
<span class="ltx_bibblock">Focus-aware response generation in inquiry conversation. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Findings of the Association for Computational Linguistics: ACL 2023</em>. 12585–12599.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, and Kun Kuang. 2023b.

</span>
<span class="ltx_bibblock">Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2310.09241</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.

</span>
<span class="ltx_bibblock">C-Pack: Packaged Resources To Advance General Chinese Embedding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.07597 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock">RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.04408 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">CoRR</em> abs/2309.10305 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.10305" title="">https://doi.org/10.48550/ARXIV.2309.10305</a>
arXiv:2309.10305

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, and Fangzhou Yao. 2023b.

</span>
<span class="ltx_bibblock">FedJudge: Federated Legal Large Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">CoRR</em> abs/2309.08173 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.08173" title="">https://doi.org/10.48550/ARXIV.2309.08173</a>
arXiv:2309.08173

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei. 2023a.

</span>
<span class="ltx_bibblock">DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">CoRR</em> abs/2309.11325 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2309.11325" title="">https://doi.org/10.48550/ARXIV.2309.11325</a>
arXiv:2309.11325

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock">GLM-130B: An Open Bilingual Pre-trained Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=-Aw0rrrPUF" title="">https://openreview.net/pdf?id=-Aw0rrrPUF</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, and Chao Zhang. 2024.

</span>
<span class="ltx_bibblock">PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.02886 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Li Yun, Hejie Cui, Zhang Xuchao, Tianjiao Zhao, et al<span class="ltx_text" id="bib.bib41.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Domain specialization as the key to make large language models disruptive: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.4.1">arXiv preprint arXiv:2305.18703</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock">LIMA: Less Is More for Alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">CoRR</em> abs/2305.11206 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2305.11206" title="">https://doi.org/10.48550/ARXIV.2305.11206</a>
arXiv:2305.11206

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Siying Zhou, Yifei Liu, Yiquan Wu, Kun Kuang, Chunyan Zheng, and Fei Wu. 2022.

</span>
<span class="ltx_bibblock">Similar case based prison term prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">CAAI International Conference on Artificial Intelligence</em>. Springer, 284–297.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Overall Generation Process</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">This section presents a complete example corresponding to each step in section 4.1.
The overall process consists of Peg Generation -¿ Main Argument Generation -¿ Supporting Argument Generation -¿ Evidence Generation -¿ Ending Generation &amp; Title Generation.
All the content is translated from Chinese.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="115" id="A1.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Peg Generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="A1.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Main Argument Generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="A1.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Supporting Argument Generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="A1.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Evidence Generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="A1.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Ending Generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="A1.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Title Generation.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 23 03:40:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
