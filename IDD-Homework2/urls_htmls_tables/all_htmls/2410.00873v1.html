<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences</title>
<!--Generated on Tue Oct  1 17:07:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="human-AI interaction" lang="en" name="keywords"/>
<base href="/html/2410.00873v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S1" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Motivation and Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.SS1" title="In 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Human AI Collaboration in AI-assisted Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.SS1.SSS1" title="In 2.1. Human AI Collaboration in AI-assisted Evaluation ‣ 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Criteria Iteration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.SS1.SSS2" title="In 2.1. Human AI Collaboration in AI-assisted Evaluation ‣ 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.SS1.SSS3" title="In 2.1. Human AI Collaboration in AI-assisted Evaluation ‣ 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Addressing Bias in AI-Assisted Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.SS2" title="In 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Direct Assessment vs. Pairwise Comparison in Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>EvalAssist</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.SS1" title="In 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Direct Assessment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.SS2" title="In 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pairwise Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.SS3" title="In 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>LLM Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S4" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S4.SS1" title="In 4. Methodology ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Tasks and Evaluation Criteria</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS1" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Participant Demographics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS2" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>RQ1: What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS2.SSS1" title="In 5.2. RQ1: What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Criteria Priority Differences within Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS3" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>RQ2: What strategies do users employ to refine their criteria for achieving human-AI alignment in LLM-as-a-judge evaluations?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS4" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS4.SSS1" title="In 5.4. RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>What are the differences for the total amount of evaluations run?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS4.SSS2" title="In 5.4. RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>What are the differences in human-AI alignment?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS5" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>AI Trust</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6.SSS1" title="In 5.6. AI Trust ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Reasons for Trust</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6.SSS2" title="In 5.6. AI Trust ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.2 </span>Reasons for Distrust</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6.SSS3" title="In 5.6. AI Trust ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.3 </span>Perception of Explanations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6.SSS4" title="In 5.6. AI Trust ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.4 </span>Perception of Positional Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS6.SSS5" title="In 5.6. AI Trust ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.5 </span>Cognitive Load</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS7" title="In 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>RQ5: What do practitioners prefer: Direct Assessment or Pairwise?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS7.SSS1" title="In 5.7. RQ5: What do practitioners prefer: Direct Assessment or Pairwise? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.1 </span>Preferred Direct Assessment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS7.SSS2" title="In 5.7. RQ5: What do practitioners prefer: Direct Assessment or Pairwise? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.2 </span>Preferred Pairwise Assessment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.SS7.SSS3" title="In 5.7. RQ5: What do practitioners prefer: Direct Assessment or Pairwise? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7.3 </span>Use of AI Judge Strategy is Contextual</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS1" title="In 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Variability in Subjective Criteria: Defining Stakeholder Needs in AI-assisted Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS1.SSS1" title="In 6.1. Variability in Subjective Criteria: Defining Stakeholder Needs in AI-assisted Evaluation ‣ 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Mitigating Over-Specificity in AI-Assisted Evaluation: Balancing Task Context and Generalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS1.SSS2" title="In 6.1. Variability in Subjective Criteria: Defining Stakeholder Needs in AI-assisted Evaluation ‣ 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Challenges of Natural Language Criteria Formulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS2" title="In 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Alignment: Refining Criteria, Changing the AI Evaluator, and Altering Expected Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS3" title="In 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Adaptive Evaluation Strategies: Balancing Clarity and Flexibility</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S6.SS4" title="In 6. Discussion ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Bias Awareness and Explanation Visibility in Evaluation Strategies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S7" title="In Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zahra Ashktorab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:Zahra.Ashktorab1@ibm.com">Zahra.Ashktorab1@ibm.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">1101 Kitchawan Rd PO Box 218</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Yorktown Heights</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">NY</span><span class="ltx_text ltx_affiliation_postcode" id="id5.5.id5">10598</span><span class="ltx_text ltx_affiliation_country" id="id6.6.id6">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Desmond
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Yorktown Heights</span><span class="ltx_text ltx_affiliation_state" id="id9.3.id3">NY</span><span class="ltx_text ltx_affiliation_postcode" id="id10.4.id4">10598</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qian Pan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:qian.pan@ibm.com">qian.pan@ibm.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id13.2.id2">Cambridge</span><span class="ltx_text ltx_affiliation_state" id="id14.3.id3">MA</span><span class="ltx_text ltx_affiliation_postcode" id="id15.4.id4">02142</span><span class="ltx_text ltx_affiliation_country" id="id16.5.id5">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James M. Johnson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Cambridge</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">MA</span><span class="ltx_text ltx_affiliation_postcode" id="id20.4.id4">02142</span><span class="ltx_text ltx_affiliation_country" id="id21.5.id5">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Santillan Cooper
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Capital Federal</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">Argentina</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elizabeth M. Daly
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id27.3.id3">Ireland</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rahul Nair
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id28.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id29.2.id2">Dublin</span><span class="ltx_text ltx_affiliation_country" id="id30.3.id3">Ireland</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tejaswini Pedapati
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id31.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id32.2.id2">Yorktown Heights</span><span class="ltx_text ltx_affiliation_state" id="id33.3.id3">NY</span><span class="ltx_text ltx_affiliation_postcode" id="id34.4.id4">10598</span><span class="ltx_text ltx_affiliation_country" id="id35.5.id5">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Swapnaja Achintalwar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id36.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id37.2.id2">Pune</span><span class="ltx_text ltx_affiliation_country" id="id38.3.id3">India</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Werner Geyer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:werner.geyer@ibm.com">werner.geyer@ibm.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id39.1.id1">IBM Research</span><span class="ltx_text ltx_affiliation_city" id="id40.2.id2">Cambridge</span><span class="ltx_text ltx_affiliation_state" id="id41.3.id3">MA</span><span class="ltx_text ltx_affiliation_postcode" id="id42.4.id4">02142</span><span class="ltx_text ltx_affiliation_country" id="id43.5.id5">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id44.id1">Evaluation of large language model (LLM) outputs requires users to make critical judgments about the best outputs across various configurations. This process is costly and takes time given the large amounts of data. LLMs are increasingly used as evaluators to filter training data, evaluate model performance or assist human evaluators with detailed assessments. To support this process, effective front-end tools are critical for evaluation. Two common approaches for using LLMs as evaluators are direct assessment and pairwise comparison. In our study with machine learning practitioners (n=15), each completing 6 tasks yielding 131 evaluations, we explore how task-related factors and assessment strategies influence criteria refinement and user perceptions. Findings show that users performed more evaluations with direct assessment by making criteria task-specific, modifying judgments, and changing the evaluator model. We conclude with recommendations for how systems can better support interactions in LLM-assisted evaluations.</p>
</div>
<div class="ltx_keywords">human-AI interaction
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models are foundation models that can be used for a variety of tasks such as summarization, text generation, concept extraction, analysis, or classification. Benchmarks such as Helm<cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib31" title="">2022</a>)</cite>, BigBench<cite class="ltx_cite ltx_citemacro_citep">(Ghazal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib17" title="">2013</a>)</cite>, or MMLU<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib19" title="">2020</a>)</cite> can provide guidance in what language model to pick for a certain task. However, in practice, they are insufficient when it comes to specific use cases, use case specific data, or creative tasks <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite>. Often human evaluation is used to assess model fitness for specific use cases and data but it is costly and takes time given the large volumes of data being generated. Practitioners are increasingly using large language models also as evaluators of the output of large language models across various contexts including filtering training data, evaluating model performance, assessing prompt effectiveness, and assisting human evaluators with detailed assessments and explanations <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib20" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib7" title="">2024</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib34" title="">2024</a>; Raju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib37" title="">2024</a>)</cite>. This approach is often referred to as LLM-as-a-judge.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that, in this paper, we use the terms judge and evaluator interchangeably. </span></span></span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, relying solely on LLM evaluators is not without risks; as with many tasks, evaluator models can also hallucinate or provide explanations that lack coherence, underscoring the necessity of keeping humans in the loop. Although LLMs may not always be accurate, they can potentially reduce workload by flagging outputs that require human input due to low confidence. Additionally, LLM evaluators also offer a lot of flexibility because practitioners can customize evaluation criteria — such as conciseness, faithfulness to a context, conversational naturalness, or succinctness — enabling more targeted and effective assessments tailored to specific use cases. Many tools have emerged to help users create criteria to evaluate their outputs. The process of criteria creation for evaluation is often iterative, and the concept of ”criteria drift” may arise. While predefined criteria may help users to assess outputs, in practice, the act of grading also helps users refine and redefine these criteria.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Two predominant forms of AI-assisted assessments with LLMs have emerged: Direct Assessment and Pairwise Assessment. Direct Assessment scores specific criteria (often part of a rubric) to evaluate whether outputs meet the criteria, while Pairwise Assessment compares pairs of outputs against broader, high-level criteria. Each method has its own strengths and limitations, which our study explores by examining how they affect user interaction with both the criteria and the evaluation process. To support this investigation, we developed <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.1">EvalAssist</span>, a system that allows users to view outputs and iteratively refine their evaluation criteria. Our focus was on understanding how users adjust their criteria, the changes they make, and how they ultimately find satisfaction with both the criteria and the evaluation process. To understand the how users develop and refine criteria to achieve alignment with LLM evaluators, we ran a within-subject study with 15 practitioners (data scientists, software engineers, and AI engineers) who have been involved in model performance projects. In our study we pose the following research questions:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">RQ1:</span> <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.2">What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ?</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">RQ2: What strategies do users employ to refine their criteria for achieving human-AI alignment in LLM-as-a-judge evaluations?</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria?</span></p>
<ul class="ltx_itemize" id="S1.I1.i3.I1">
<li class="ltx_item" id="S1.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i3.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i3.I1.i1.p1.1">RQ3A How do task and judge strategy influence the total number of evaluations performed?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i3.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i3.I1.i2.p1.1">RQ3B How do task and form of assessment (direct vs. pairwise) affect the degree of human-AI alignment?</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">RQ4: How do task-related factors and judge strategy impact user perceptions of the judge?</span></p>
<ul class="ltx_itemize" id="S1.I1.i4.I1">
<li class="ltx_item" id="S1.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i4.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i4.I1.i1.p1.1">How do they affect users’ trust in AI?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i4.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i4.I1.i2.p1.1">How do they shape users’ perception of positional bias?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i4.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i4.I1.i3.p1.1">How do they influence users’ perception of explanations provided by the judge?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.I1.i4.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I1.i4.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.I1.i4.p1.1">How do they impact users’ cognitive load during the evaluation process?</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">RQ5: Which assessment strategy do users prefer?</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper makes the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce <span class="ltx_text ltx_font_smallcaps" id="S1.I2.i1.p1.1.1">EvalAssist</span>, a tool designed to help practitioners refine evaluation criteria through both direct and pairwise assessment strategies. It provides positional bias metrics from the AI judge, as well as an explanation for each judgment.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We present results from within-subjects controlled experiment with machine learning practitioners (n=15) providing insights into how they refine evaluation criteria and uncover key differences between the two evaluation strategies.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">Based on our findings, we offer design suggestions for AI-assisted evaluation systems.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our findings show that users conduct more evaluations under the direct assessment condition. Users refine criteria in multiple ways, such as making criteria more specific or general, adjusting their own judgments, or modifying the AI evaluator’s outputs. Explanations are perceived as more helpful in the direct assessment condition. Users prefer Direct Assessment when they need clarity and control over individual item evaluations, and Pairwise Assessment when evaluating nuanced or subjective criteria.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Motivation and Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Human AI Collaboration in AI-assisted Evaluation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recently, several AI-assisted evaluation tools have emerged, with varying focuses including: improving the iterative nature of prompt refinement or refining criteria for evaluation. Across these systems, the evolving nature of user-defined criteria is emphasized, acknowledging that such criteria are not static but adapt in response to AI outputs and user feedback. This iterative process is fundamental in tools that support criteria refinement and prompt adjustments, reinforcing the dynamic interaction between humans and AI in the evaluation process. A prominent aspect shared among these tools is the role of human-in-the-loop evaluation. Systems like EvalGen <cite class="ltx_cite ltx_citemacro_citep">(Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib39" title="">2024</a>)</cite>, ChainForge <cite class="ltx_cite ltx_citemacro_citep">(Arawjo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib3" title="">2024</a>)</cite>, EvalLM <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib25" title="">2023a</a>)</cite>, and LLM Comparator <cite class="ltx_cite ltx_citemacro_citep">(Kahng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib22" title="">2024</a>)</cite> integrate human feedback as a key element of the evaluation loop. While AI systems assist in generating evaluations, they rely on human judgment to ensure alignment with user preferences. One of the key challenges in this interactive process is criteria drift <cite class="ltx_cite ltx_citemacro_citep">(Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib39" title="">2024</a>)</cite>, where users adjust their evaluation standards as they encounter new outputs. Prior systems illustrate this behavior, as users often modify their criteria after receiving AI-generated responses that deviate from initial expectations. This flexibility is critical, highlighting the need for evaluation systems that allow criteria adjustments throughout the evaluation process, rather than imposing rigid, predefined standards.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Criteria Iteration</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Prior research demonstrates that users require multiple rounds of iteration to refine their criteria <cite class="ltx_cite ltx_citemacro_citep">(Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib39" title="">2024</a>)</cite>. Users require viewing LLM outputs in order to define criteria, since there are challenges to defining criteria without seeing the range of possible outputs. Conversely, users may create criteria that are dependent on the outputs created. Allowing users to iteratively define criteria is an important consideration in the design of our tool. In <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.1">EvalAssist</span>, users can start a project by refining their evaluation criteria before scaling up to the full dataset. Effective sampling enhances learning for LLM-as-a-Judge by selecting diverse and representative outputs.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">Crafting effective criteria typically requires multiple iterations. Criteria components such as name, definition, scale, and examples often need definition and refinement as users evaluate outputs. Related work <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib26" title="">2023b</a>)</cite> indicates that users often develop new criteria during evaluations. To facilitate this, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p2.1.1">EvalAssist</span> includes a real-time feedback system that allows users to immediately see the impact of criteria modifications. Unlike systems that rely heavily on predefined metrics or expert-labeled data, EvalAssist enables users to define evaluation criteria in natural language and to iteratively refine these criteria based on feedback from the AI model. Unlike other AI-assisted evaluation tools that combine prompting engineering with criteria definition, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p2.1.2">EvalAssist</span> simplifies the LLM-as-a-judge process by allowing users to focus solely on defining criteria. This approach recognizes that developers often rely on external workflows to adjust configurations (e.g., model temperature) and experiment with different models and prompts to generate responses <cite class="ltx_cite ltx_citemacro_citep">(Desmond et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib12" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Visualization</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Other tools focus on providing intuitive, interactive interfaces that facilitate complex evaluation tasks. The use of interactive and visual interfaces is another notable feature across these tools. Allowing users to visually compare model outputs in real-time, provides comprehensible evaluation experience <cite class="ltx_cite ltx_citemacro_citep">(Kahng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib22" title="">2024</a>)</cite>. Many existing tools share common themes such as iterative refinement, user-centered evaluation, and scalability. Together, they reflect a growing trend in human-AI collaboration, aiming to create more flexible, subjective, and adaptable evaluation systems that effectively combine human insight with AI capabilities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3. </span>Addressing Bias in AI-Assisted Evaluation</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">LLM Evaluators, like their human counterparts, exhibit biases. These biases include but are not limited to: positional bias, which is when judges consistently favor one side of a pair, regardless of the actual quality of the answers, self-enhancement bias when a model prefers its own responses, and verbosity bias when an LLM judge favors longer responses even if they are not a better alternative <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite>. Many of the existing AI-assisted tools do not flag these kinds of biases to users. Considering the persistent challenge of bias, systems should both provide transparency when bias occurs and implement bias mitigation strategies that include swapping answer order to reduce position bias <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite> and treating inconsistent results as ties, or by randomly assigning positions in large datasets <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib30" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite>. <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS3.p1.1.1">EvalAssist</span> includes a check for positional bias and indicates whether positional bias exists.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Direct Assessment vs. Pairwise Comparison in Evaluation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Two of the most common judgment strategies in evaluation are direct assessment and pairwise comparison. Direct assessment involves outputting a scalar indicator of quality (e.g., assigning a score or rating to an item) <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite>, while pairwise comparison determines which of two outputs is preferred based on specific criteria. Both approaches have advantages and disadvantages depending on the context and task. One limitation of pairwise comparison is scalability. As the number of items to be evaluated increases, the number of required comparisons grows quadratically, making this method less feasible for large-scale evaluations. However, pairwise comparisons can be more effective at identifying subtle differences between outputs and according to prior research is an easier task for both humans and LLMs compared to rating a single output, often yielding higher accuracy in LLM-as-a-judge benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib4" title="">2024</a>; Gehrmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib16" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib29" title="">2019</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib45" title="">2024b</a>)</cite>. In contrast, direct assessment can efficiently evaluate multiple items at once, but it may struggle to detect fine distinctions between outputs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Direct assessment often utilizes rubrics with multiple dimensions, while pairwise comparison may focus on a single dimension of preference. Additionally, the nature of the criteria—whether objective (e.g., grammatical accuracy) or subjective (e.g., creativity or tone)—plays a significant role in shaping the evaluation process. Prior work has explored LLMs’ ability to make selections based on user-defined preferences across a wide range of criteria <cite class="ltx_cite ltx_citemacro_citep">(Chiang and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib8" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib9" title="">b</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite>. For example, prior work investigated how LLMs handle subjective judgments across dimensions such as brevity, formality, honesty, creativity, and political tone, highlighting the variability that emerges when criteria are subjective. <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p2.1.1">EvalAssist</span> allows users to select the evaluation strategy that best fits the task.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S2.F1.g1" src="extracted/5893543/figures/evaloffering.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">EvalAssist’s homepage provides users with a range of options: choose direct assessment, pairwise comparison, explore the example catalog, or utilize the toolkit to apply custom criteria from the sandbox to the entire dataset.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>EvalAssist</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.p1.1.1">EvalAssist</span> abstracts the llm-as-a-judge evaluation process into a library of parameterize-able evaluators (the criterion being the parameter), allowing the user
to focus on criteria definition. This approach acknowledges that developers often use complex external workflows to adjust configurations (e.g., model temperature) and experiment with different models and prompts to generate responses <cite class="ltx_cite ltx_citemacro_citep">(Desmond et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib12" title="">2024</a>)</cite>. <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.2">EvalAssist</span> consists of a web-based user experience, an API, and a Python toolkit. The user interface provides users with a convenient way of iteratively testing and refining LLM-as-a-judge criteria, and supports both direct (rubric-based) and pairwise assessment paradigms (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S2.F1" title="Figure 1 ‣ 2.2. Direct Assessment vs. Pairwise Comparison in Evaluation ‣ 2. Motivation and Related Work ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">1</span></a>), the two most prevalent forms of LLM-as-a-judge evaluation available <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib24" title="">2023c</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib44" title="">2024a</a>)</cite></p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Users can choose the evaluation method based on task complexity, receive AI judgment explanations, and view metrics like positional bias. Once users are satisfied with their criteria, they can use the Python toolkit to run bulk evaluations with larger data sets by exporting auto-generated JSON definitions of their criteria into predefined notebooks provided in the toolkit. We also allow users to save their test cases and provide a catalog of predefined criteria. A test case in the Example Catalog includes a criteria definition and the data being evaluated. On the landing page, users can choose between Direct Assessments and Pairwise comparisons.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Direct Assessment</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this mode, users evaluate outputs based on a single criterion rubric they define. Users can define task-relevant input data through variables in the task-context (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F2" title="Figure 2 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">2</span></a>), such as, for example, the prompt, the article to summarize, or the source data for content-grounded Q&amp;A. The next section (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F3" title="Figure 3 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">3</span></a>) allows them to define their criteria with a title, criteria description, and an arbitrary number of free-form options (b) the LLM evaluator will have to choose from during assessment. As such, the
system supports both binary and multi-level scale assessments. In the Evaluator section, users need to select an LLM judge. Our system currently supports four judges: mixtral-8x7b-instruct-v01, llama-3-8b-instruct, llama-3-70b-instruct, and prometheus-8x7b-v2. In the Test Data Section, users enter the outputs they want to evaluate (we call them the responses; note that it is possible to edit the variable name here too so the data being evaluated can be better referenced in the criteria definition) and optionally the result they would expect for each output. After running the evaluation, the system shows the actual results next to the expected results, including agreement, positional bias if present, certainty scores, and an explanation.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="255" id="S3.F2.g1" src="extracted/5893543/figures/task_context.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Task Context for the Summarization Task. The Task Context is consistent for both Direct Assessment and Pairwise strategies. Users have the option to break down the context into variables, such as the instruction and article, to simplify reference while developing evaluation criteria.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S3.F3.sf1.g1" src="extracted/5893543/figures/pairwise3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F3.sf1.3.2" style="font-size:90%;">Pairwise approach: Features a concise one-sentence summary of each criterion.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S3.F3.sf2.g1" src="extracted/5893543/figures/rubric5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F3.sf2.3.2" style="font-size:90%;">Direct assessment: contains a high-level question, scale items, and detailed optional definitions for each scale item.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Evaluation criteria forms for pairwise assessment and direct assessment.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S3.F4.g1" src="extracted/5893543/figures/results_rubric.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Results for Direct Assessment. Users can select their expected judgments for the output, which are auto-populated based on the criteria they define (i.e., the scale items created when setting the criteria). The results display the LLM evaluator’s judgments, indicating whether there is agreement between the user and the AI, along with explanations for each result.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Pairwise Comparison</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In the pairwise comparison mode (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F3" title="Figure 3 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">3</span></a> (a)), EvalAssist compares multiple outputs (minimum two) pairwise against one-another selecting the one that better fits their criteria. The best output is determined by computing the win rate across all pairwise output comparisons. Similar to Direct Assessment, users can provide task-relevant input data through variables, define a criteria, and select an evaluator LLM. However, options don’t need to be added to pairwise comparisons. After evaluation, we display the results next to the expected results (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F3" title="Figure 3 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">3</span></a>), including the winner, ranking, and agreement with expected ranking. Users can click on each result to see detailed explanations including positional bias, win-rate, and explanations for the comparisons with the other outputs.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S3.F5.sf1.g1" src="extracted/5893543/figures/results_ranking.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F5.sf1.3.2" style="font-size:90%;">Ranking results generated from pairwise comparison assessment. Users can input their expected ranking to and assess their level of agreement with the AI evaluator.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="790" id="S3.F5.sf2.g1" src="extracted/5893543/figures/explanation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F5.sf2.3.2" style="font-size:90%;">Explanations for each pairwise comparison in pairwise assessment.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Results, explanations, and expected ranking generated through pairwise comparison. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>LLM Evaluation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">When users select the ”Evaluate” button, their input is sent to the chosen evaluator. Each evaluator is designed to perform either direct assessment or pairwise evaluation. The main external difference between these two lies in how the input criteria are structured. Internally, evaluators operate as a dialog with a target LLM (large language model) using a set of custom prompts specific to that LLM. First, the LLM is prompted to review the evaluation task, considering the context, criteria, and subject of evaluation. Based on this, the LLM generates an open-ended assessment that explains its decision-making process. This step is inspired by Chain-of-Thought (CoT) prompting <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib42" title="">2022</a>)</cite>, encouraging the LLM to base its final judgement on its initial reasoning. This generated assessment is then added to the dialog history. Afterward, the LLM is asked to make a final judgement. Instead of having the LLM directly generate a final decision, we provide a set of options as potential completions. The system calculates the log probability of each option’s tokens using a forward pass and selects the option with the highest linear probability sequence. This comparison is done by evaluating the first token of each completion, followed by the second, and so on. For direct assessment, these completions are the user-provided option strings, while in pairwise evaluation, the completions are the two responses being compared (Response 1 and Response 2). Although using log probabilities for determining the LLM’s judgement is less efficient than direct generation, it is significantly more reliable. The final explanation of the judgement summarizes the initial assessment. Positional bias is checked by shuffling the order of the options presented to the LLM and verifying the consistency of its final decision. A visual representation of the algorithm is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F6" title="Figure 6 ‣ 3.3. LLM Evaluation ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="437" id="S3.F6.g1" src="extracted/5893543/figures/flow.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Generation of judgments in EvalAssist involves initially creating an assessment, followed by summarizing the assessment and making a selection between options. Log probabilities are generated and compared during this process, which is repeated to detect positional bias. The prompts shown have been simplified for clarity in the image.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The study aimed to explore several key aspects of how practitioners engage with LLMs as evaluators. Specifically, it sought to understand what practitioners prioritize in evaluation criteria when using LLMs as judges and how these priorities differ (RQ1). Additionally, it examined the strategies users employ to refine their criteria in pursuit of human-AI alignment during LLM-as-a-judge evaluations (RQ2). The study also focused on the impact of task-related factors and judge strategies on how practitioners refine criteria (RQ3). Furthermore, the research investigated how task and judge strategy affect user perceptions of the AI judge, including trust, perception of positional bias, perception of explanations, and cognitive load (RQ4). Lastly, the study aimed to identify which assessment strategy users preferred (RQ5).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We used a within-subjects study design, involving 15 participants recruited internally from our organization. Participants were recruited internally from individuals who had previously used <span class="ltx_text ltx_font_typewriter" id="S4.p2.1.1">EvalAssist</span> and through announcements posted on the organization’s Slack channels. These messages invited employees to participate in a study, with eligibility focused prior experience with model evaluation. Participants were informed about the study’s purpose and provided consent before taking part. Participants were provided with detailed information about the study’s purpose, procedures, and their rights as participants. We collected demographic information, including participants’ roles at the company, education levels, and prior experience with model evaluation. Participants then completed a practice task to familiarize themselves with the experimental interface and procedures.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The experimental tasks involved creating criteria and evaluating ouput based on their criteria using either the pairwise approach or the direct assessment approach for six task contexts based on 3 tasks (Q&amp;A, Email Generation, Summarization) each seen twice, once under the direct assessment condition and once under the pairwise condition. Each task is described in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S4.T1" title="Table 1 ‣ 4.1. Tasks and Evaluation Criteria ‣ 4. Methodology ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">The Direct Assessment task involved designing a single criterion rubric with options to assess whether output complies with the criteria (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F4" title="Figure 4 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">4</span></a>), whereas pairwise comparison (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S3.F3" title="Figure 3 ‣ 3.1. Direct Assessment ‣ 3. EvalAssist ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">3</span></a> involves defining the criteria and comparing pairs) of generated outputs to see which better matches criteria. While both approaches come with strengths and weaknesses, we set out to examine how they influence the evaluation of criteria and users’ interactions differently.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Each task was followed by a short survey assessing participants’ trust in the AI, satisfaction with the criteria, cognitive load, perception of positional bias, and the explanations provided by the AI. Throughout the study, data were logged on the number of evaluations run, final human agreement with the LLM evaluator, and the time taken to run each evaluation. The following questions were rated on a 5-point Likert scale, with 1 indicating “strongly disagree” and 5 indicating “strongly agree”:</p>
</div>
<div class="ltx_para" id="S4.p6">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Trust in AI Evaluator:</p>
<ul class="ltx_itemize" id="S4.I1.i1.I1">
<li class="ltx_item" id="S4.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I1.i1.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.I1.i1.p1.1">I trusted the AI evaluator to to judge the responses. (adapted from <cite class="ltx_cite ltx_citemacro_citep">(Poursabzi-Sangdeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib36" title="">2021</a>)</cite>)</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I1.i1.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i1.I1.i2.p1.1">How confident were you in the model’s judgements/evaluations? (adapted from <cite class="ltx_cite ltx_citemacro_citep">(Buçinca et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib6" title="">2020</a>)</cite>)</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Perception of Positional Bias: The positional bias was helpful in completing this task.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Perception of Explanations: The explanations were helpful in completing this task.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Mental Load (adapted from <cite class="ltx_cite ltx_citemacro_citep">(Hart, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib18" title="">2006</a>)</cite>):</p>
<ul class="ltx_itemize" id="S4.I1.i4.I1">
<li class="ltx_item" id="S4.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I1.i4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i4.I1.i1.p1.1">The task was mentally demanding.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I1.i4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i4.I1.i2.p1.1">I was successful in accomplishing what I was asked to do.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I1.i4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i4.I1.i3.p1.1">I had to work had to accomplish my level of performance</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">To mitigate order effects, we used partial counterbalancing. Participants were randomly assigned to different task orders to ensure that the sequence of tasks did not systematically bias the results. Participants were asked to reflect on the different tasks they had completed and respond to questions about their preferences between direct assessment and pairwise assessment. They also described how they interacted with two types of tasks: objective versus subjective. After completing the six tasks, participants were asked to select their preferred assessment strategy and explain the reasons for their preference.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Tasks and Evaluation Criteria</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Tasks and evaluation criteria were chosen to capture various levels of granularity and criteria specificity (i.e., single dimension of ’preference’ vs. document groundedness) and to reflect a diversity of tasks and respective models. Despite the responses coming from various sources (various models, datasets, etc.), all of the responses were reviewed by the coauthors to ensure variability in the responses. Below we list the task descriptions. Task examples can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S4.T1" title="Table 1 ‣ 4.1. Tasks and Evaluation Criteria ‣ 4. Methodology ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Article Summarization: </span> While the summaries can be judged on cohesiveness, consistency, fluency, relevance <cite class="ltx_cite ltx_citemacro_citep">(Bavaresco et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib5" title="">2024</a>)</cite> we asked participants to define criteria based on the single dimension of “preference”, as seen in work by <cite class="ltx_cite ltx_citemacro_citep">(Chiang and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib8" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib30" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib32" title="">2023</a>)</cite>. For the summarization task, we presented the original reference document and corresponding generated outputs to users to be judged from <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib14" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Email Generation</span>: The email generation task was leveraged to judge inclusivity. We generated an email about an office Christmas party using various models (Gemini 1.5-Pro <cite class="ltx_cite ltx_citemacro_citep">(Reid et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib38" title="">2024</a>)</cite>, claude-3-5-sonnet-20240620 <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib2" title="">2024</a>)</cite>, gpt-3.5-turbo-0125 <cite class="ltx_cite ltx_citemacro_citep">(Floridi and Chiriatti, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib15" title="">2020</a>)</cite>, mixtral-8x22b-instruct-v0.1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib21" title="">2023</a>)</cite>), resulting in emails with different levels of inclusivity. We asked participants to create criteria to evaluate the generated output based on the inclusiveness of the output.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Q&amp;A multi-turn:</span> The Q&amp;A task involved a context document, a multi-turn conversation, and a final response to the last question in the conversation. The output was generated using retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib28" title="">2020</a>)</cite>. RAG evaluations focused on three aspects: answer relevance (is the answer relevant to the query?), context relevance (is the retrieved context relevant to the query?), and groundedness (is the response supported by the context?) <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib43" title="">2024</a>)</cite>. The Q&amp;A data was selected from existing HR support documents at ABC Company, with AI-generated responses created by multiple models, including <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib10" title="">2022</a>)</cite>. One of the question-and-answer sets involved a reference document and a customer inquiry about upgrading their business travel to business class with their corporate credit card. Participants were asked to create a criterion to evaluate the faithfulness of each response to the reference document. A response is considered faithful to a document when it only reflects the information expressed in the document <cite class="ltx_cite ltx_citemacro_citep">(Madsen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib33" title="">2024</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.1.1.1.1">
<span class="ltx_p" id="S4.T1.2.1.1.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1.1.1" style="font-size:80%;">Task Description</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.1.1.2.1">
<span class="ltx_p" id="S4.T1.2.1.1.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.2.1.1.1" style="font-size:80%;">Example Output</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.2.1.1.1">
<span class="ltx_p" id="S4.T1.2.2.1.1.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T1.2.2.1.1.1.1.1" style="font-size:80%;">Article Summarization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.2.1.2.1">
<span class="ltx_p" id="S4.T1.2.2.1.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.2.1.2.1.1.1" style="font-size:80%;">Dress worn by vivien leigh when she played scarlett o’hara in 1939 film gone with the wind has fetched $ 137,000 at auction . The dress - a jacket and full skirt ensemble - was worn in several key scenes in the 1939 movie , including when scarlett o’hara encounters rhett butler , played by clark gable , and when she gets attacked in the shanty town .</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.3.2.1.1">
<span class="ltx_p" id="S4.T1.2.3.2.1.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T1.2.3.2.1.1.1.1" style="font-size:80%;">Q&amp;A in HR Chatbot</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.3.2.2.1">
<span class="ltx_p" id="S4.T1.2.3.2.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.3.2.2.1.1.1" style="font-size:80%;">You can upgrade to business class by paying the business class portion of the airline ticket with a personal credit card to avoid the perception of a policy bypass.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.4.3.1.1">
<span class="ltx_p" id="S4.T1.2.4.3.1.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T1.2.4.3.1.1.1.1" style="font-size:80%;">Email Generation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T1.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.4.3.2.1">
<span class="ltx_p" id="S4.T1.2.4.3.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.4.3.2.1.1.1" style="font-size:80%;">Subject: Join Us for a Multicultural Holiday Celebration!</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.4">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.5.4.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.5.4.2.1">
<span class="ltx_p" id="S4.T1.2.5.4.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.5.4.2.1.1.1" style="font-size:80%;">Dear Team,</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.5">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.6.5.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.6.5.2.1">
<span class="ltx_p" id="S4.T1.2.6.5.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.6.5.2.1.1.1" style="font-size:80%;">As the holiday season approaches, we are thrilled to announce our annual company holiday party! This year, we are embracing the diverse cultures within our team and celebrating a variety of holiday traditions.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.6">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.7.6.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.7.6.2.1">
<span class="ltx_p" id="S4.T1.2.7.6.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.7.6.2.1.1.1" style="font-size:80%;">Details are as follows:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8.7">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.8.7.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.8.7.2.1">
<span class="ltx_p" id="S4.T1.2.8.7.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.8.7.2.1.1.1" style="font-size:80%;">Date: December 20th</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9.8">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.9.8.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.9.8.2.1">
<span class="ltx_p" id="S4.T1.2.9.8.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.9.8.2.1.1.1" style="font-size:80%;">Time: 6:00 PM - 10:00 PM</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10.9">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.10.9.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.10.9.2.1">
<span class="ltx_p" id="S4.T1.2.10.9.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.10.9.2.1.1.1" style="font-size:80%;">Location: Main Conference Hall</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11.10">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.11.10.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.11.10.2.1">
<span class="ltx_p" id="S4.T1.2.11.10.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.11.10.2.1.1.1" style="font-size:80%;">Our celebration will feature a variety of activities that honor different cultural traditions, including:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.12.11">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.12.11.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.12.11.2.1">
<span class="ltx_p" id="S4.T1.2.12.11.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.12.11.2.1.1.1" style="font-size:80%;">Decorations from Around the World: Help us decorate the office with symbols and ornaments from various holidays, such as Christmas trees, Hanukkah menorahs, Kwanzaa kinaras, Diwali lamps, and more.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.13.12">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.13.12.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.13.12.2.1">
<span class="ltx_p" id="S4.T1.2.13.12.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.13.12.2.1.1.1" style="font-size:80%;">Inclusive Gift Exchange:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.14.13">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.14.13.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.14.13.2.1">
<span class="ltx_p" id="S4.T1.2.14.13.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.14.13.2.1.1.1" style="font-size:80%;">Participate in a gift exchange that embraces the spirit of giving across cultures. Bring a wrapped gift (under $20) or a small item that represents your cultural heritage.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.15.14">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.15.14.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.15.14.2.1">
<span class="ltx_p" id="S4.T1.2.15.14.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.15.14.2.1.1.1" style="font-size:80%;">- Global Holiday Music: Enjoy a diverse selection of holiday music, including Christmas carols, Hanukkah songs, Kwanzaa tunes, and traditional music from different cultures.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.16.15">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.16.15.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.16.15.2.1">
<span class="ltx_p" id="S4.T1.2.16.15.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.16.15.2.1.1.1" style="font-size:80%;">- International Feast: Indulge in a delicious array of holiday dishes from around the world. We will have a variety of foods to cater to different tastes and dietary preferences.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.17.16">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.17.16.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.17.16.2.1">
<span class="ltx_p" id="S4.T1.2.17.16.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.17.16.2.1.1.1" style="font-size:80%;">Feel free to wear traditional attire or festive clothing that reflects your cultural heritage. Whether it’s a Christmas sweater, traditional clothing for Hanukkah, Kwanzaa, Diwali, or any other holiday, we welcome it all!</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.18.17">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.18.17.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.18.17.2.1">
<span class="ltx_p" id="S4.T1.2.18.17.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.18.17.2.1.1.1" style="font-size:80%;">We would love to hear your ideas and suggestions to make this event even more inclusive and enjoyable. If you have any special traditions or activities you’d like to share, please let us know.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.19.18">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.19.18.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.19.18.2.1">
<span class="ltx_p" id="S4.T1.2.19.18.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.19.18.2.1.1.1" style="font-size:80%;">Let’s come together to celebrate the season and the wonderful diversity within our team.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.20.19">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.20.19.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.20.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.20.19.2.1">
<span class="ltx_p" id="S4.T1.2.20.19.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.20.19.2.1.1.1" style="font-size:80%;">Happy Holidays!</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.21.20">
<td class="ltx_td ltx_align_top ltx_border_l ltx_border_r" id="S4.T1.2.21.20.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S4.T1.2.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.21.20.2.1">
<span class="ltx_p" id="S4.T1.2.21.20.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.21.20.2.1.1.1" style="font-size:80%;">Best regards,</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.22.21">
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.2.22.21.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S4.T1.2.22.21.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.2.22.21.2.1">
<span class="ltx_p" id="S4.T1.2.22.21.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="S4.T1.2.22.21.2.1.1.1" style="font-size:80%;">Company CEO</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.5.1.1" style="font-size:113%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.6.2" style="font-size:113%;">Example Outputs for Different Task Types in the Evaluation Study. This table presents example outputs generated for three distinct tasks: Article Summarization, Q&amp;A in an HR Chatbot, and Email Generation.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1.1">
<span class="ltx_p" id="S4.T2.2.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.1" style="font-size:90%;">Type of Change</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.2.1">
<span class="ltx_p" id="S4.T2.2.1.1.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.2.1.1.1" style="font-size:90%;">Original Criteria</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.3.1">
<span class="ltx_p" id="S4.T2.2.1.1.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.3.1.1.1" style="font-size:90%;">Changed Criteria</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.1.1">
<span class="ltx_p" id="S4.T2.2.2.2.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.2.2.1.1.1.1" style="font-size:90%;">Criteria Specified</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.2.1">
<span class="ltx_p" id="S4.T2.2.2.2.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.2.2.1.1.1" style="font-size:90%;">The generated email should be inclusive. It should mention the different cultures and holidays (such as Christmas, Hanukkah, Kwanzaa, Diwali, and others) . It should not be exclusive to one culture.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.2.3.1">
<span class="ltx_p" id="S4.T2.2.2.2.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.2.3.1.1.1" style="font-size:90%;">The generated email should be inclusive. It should mention the different cultures and holidays (such as Christmas, Hanukkah, Kwanzaa, Diwali, and others) . It should not be exclusive to one culture. The email should use inclusive terms such as holiday and festive as opposed to terms exclusive to one culture.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.3.1.1">
<span class="ltx_p" id="S4.T2.2.3.3.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.3.3.1.1.1.1" style="font-size:90%;">Criteria Generalized</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.3.2.1">
<span class="ltx_p" id="S4.T2.2.3.3.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.3.3.2.1.1.1" style="font-size:90%;">The email is inclusive to all cultures, backgrounds, genders, etc.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.3.3.1">
<span class="ltx_p" id="S4.T2.2.3.3.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.3.3.3.1.1.1" style="font-size:90%;">The email is inclusive to all cultures, and backgrounds.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.4.1.1">
<span class="ltx_p" id="S4.T2.2.4.4.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.4.4.1.1.1.1" style="font-size:90%;">Scale Item Removed</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.4.2.1">
<span class="ltx_p" id="S4.T2.2.4.4.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.4.4.2.1.1.1" style="font-size:90%;">Does the summary contain the main topic of the article?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.4.3.1">
<span class="ltx_p" id="S4.T2.2.4.4.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.4.4.3.1.1.1" style="font-size:90%;">Does the summary contain the main topic of the article?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.5">
<td class="ltx_td ltx_align_top" id="S4.T2.2.5.5.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.5.2.1">
<span class="ltx_p" id="S4.T2.2.5.5.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.5.5.2.1.1.1" style="font-size:90%;">Scale: Absolutely, Somewhat, No</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.5.3.1">
<span class="ltx_p" id="S4.T2.2.5.5.3.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.5.5.3.1.1.1" style="font-size:90%;">Scale: Yes, No</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.6.1.1">
<span class="ltx_p" id="S4.T2.2.6.6.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.6.6.1.1.1.1" style="font-size:90%;">Scale Item Added</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.6.2.1">
<span class="ltx_p" id="S4.T2.2.6.6.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.6.6.2.1.1.1" style="font-size:90%;">Does the response capture the summary in the best possible way?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.6.3.1">
<span class="ltx_p" id="S4.T2.2.6.6.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.6.6.3.1.1.1" style="font-size:90%;">Does the response capture the summary in the best possible way?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.7">
<td class="ltx_td ltx_align_top" id="S4.T2.2.7.7.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.7.2.1">
<span class="ltx_p" id="S4.T2.2.7.7.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.7.7.2.1.1.1" style="font-size:90%;">Scale: Yes, No</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.7.3.1">
<span class="ltx_p" id="S4.T2.2.7.7.3.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.7.7.3.1.1.1" style="font-size:90%;">Scale: Excellent, Good, Average, Poor</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.8.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.8.8.1.1">
<span class="ltx_p" id="S4.T2.2.8.8.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.8.8.1.1.1.1" style="font-size:90%;">Scale Item Specified</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.8.8.2.1">
<span class="ltx_p" id="S4.T2.2.8.8.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.8.8.2.1.1.1" style="font-size:90%;">Read the following email and determine if the email is inclusive or not inclusive of cultural differences.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.8.8.3.1">
<span class="ltx_p" id="S4.T2.2.8.8.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.8.8.3.1.1.1" style="font-size:90%;">Read the following email and determine if the email is inclusive or not inclusive of cultural differences.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9.9">
<td class="ltx_td ltx_align_top" id="S4.T2.2.9.9.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.9.9.2.1">
<span class="ltx_p" id="S4.T2.2.9.9.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.9.9.2.1.1.1" style="font-size:90%;">Inclusive: The email acknowledges the different cultures</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.9.9.3.1">
<span class="ltx_p" id="S4.T2.2.9.9.3.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.9.9.3.1.1.1" style="font-size:90%;">Inclusive: The email acknowledges the different cultures.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10.10">
<td class="ltx_td ltx_align_top" id="S4.T2.2.10.10.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.10.10.2.1">
<span class="ltx_p" id="S4.T2.2.10.10.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.10.10.2.1.1.1" style="font-size:90%;">Not inclusive: The email focuses on only one culture or does not acknowledges cultural differences.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.10.10.3.1">
<span class="ltx_p" id="S4.T2.2.10.10.3.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.10.10.3.1.1.1" style="font-size:90%;">Not inclusive: The email focuses on only one holiday, group, or culture and does not acknowledges cultural differences.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11.11">
<td class="ltx_td ltx_align_top" id="S4.T2.2.11.11.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.11.11.2.1">
<span class="ltx_p" id="S4.T2.2.11.11.2.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.11.11.2.1.1.1" style="font-size:90%;">Maybe: Not sure.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.11.11.3.1">
<span class="ltx_p" id="S4.T2.2.11.11.3.1.1" style="width:170.7pt;"><span class="ltx_text" id="S4.T2.2.11.11.3.1.1.1" style="font-size:90%;">Maybe: Not sure.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.12.12.1.1">
<span class="ltx_p" id="S4.T2.2.12.12.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.12.12.1.1.1.1" style="font-size:90%;">Scale Item Generalized</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.12.12.2.1">
<span class="ltx_p" id="S4.T2.2.12.12.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.12.12.2.1.1.1" style="font-size:90%;">Inclusive: The email emphasises an inclusive company culture by asking for participations from all kinds of traditions and cultural practices. It also uses inclusive language</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.12.12.3.1">
<span class="ltx_p" id="S4.T2.2.12.12.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.12.12.3.1.1.1" style="font-size:90%;">Inclusive: The email is focusing on all kinds of traditions and cultural practices. It also uses inclusive language</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.13.13.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.13.13.1.1">
<span class="ltx_p" id="S4.T2.2.13.13.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.13.13.1.1.1.1" style="font-size:90%;">Minor Edit</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.13.13.2.1">
<span class="ltx_p" id="S4.T2.2.13.13.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.13.13.2.1.1.1" style="font-size:90%;">What is the best summary?</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.13.13.3.1">
<span class="ltx_p" id="S4.T2.2.13.13.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.13.13.3.1.1.1" style="font-size:90%;">Which is the best summary?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.14.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.14.14.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.14.14.1.1">
<span class="ltx_p" id="S4.T2.2.14.14.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.14.14.1.1.1.1" style="font-size:90%;">Instruction to Model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.14.14.2.1">
<span class="ltx_p" id="S4.T2.2.14.14.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.14.14.2.1.1.1" style="font-size:90%;">Which of the following summaries best describe the article. The summary should be reflective of the key points in the article.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.14.14.3.1">
<span class="ltx_p" id="S4.T2.2.14.14.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.14.14.3.1.1.1" style="font-size:90%;">Which of the following summaries best describe the article. The summary should be reflective of the key points in the article. Each article must be reviewed on its own and ignore all other summaries while doing so.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.2.15.15.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.15.15.1.1">
<span class="ltx_p" id="S4.T2.2.15.15.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="S4.T2.2.15.15.1.1.1.1" style="font-size:90%;">Exclusion Criteria Added</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.2.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.15.15.2.1">
<span class="ltx_p" id="S4.T2.2.15.15.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.15.15.2.1.1.1" style="font-size:90%;">The summary should be accurate and concise</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.2.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.15.15.3.1">
<span class="ltx_p" id="S4.T2.2.15.15.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.15.15.3.1.1.1" style="font-size:90%;">The summary should be accurate and concise. Has no fake data generated outside of reference</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Types of changes made across every evaluation.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Participant Demographics</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We recruited 15 participants at ABC company who had prior experience with model evaluation. The job titles are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.T3" title="Table 3 ‣ 5.1. Participant Demographics ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">3</span></a>. Their prior experience included subjective and manual evaluation, use case specific evaluation, evaluation framework and metrics.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.1.1.1" style="font-size:90%;">ID</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.1.2.1" style="font-size:90%;">Job Role</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.2.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.2.1.1"><span class="ltx_text" id="S5.T3.2.2.1.1.1" style="font-size:90%;">P1</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.2.1.2"><span class="ltx_text" id="S5.T3.2.2.1.2.1" style="font-size:90%;">AI Engineer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.3.2.1"><span class="ltx_text" id="S5.T3.2.3.2.1.1" style="font-size:90%;">P2</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.2.2"><span class="ltx_text" id="S5.T3.2.3.2.2.1" style="font-size:90%;">Research Scientist Intern</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.4.3.1"><span class="ltx_text" id="S5.T3.2.4.3.1.1" style="font-size:90%;">P3</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.4.3.2"><span class="ltx_text" id="S5.T3.2.4.3.2.1" style="font-size:90%;">Software Developer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.5.4.1"><span class="ltx_text" id="S5.T3.2.5.4.1.1" style="font-size:90%;">P4</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.5.4.2"><span class="ltx_text" id="S5.T3.2.5.4.2.1" style="font-size:90%;">Data Scientist</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.6.5.1"><span class="ltx_text" id="S5.T3.2.6.5.1.1" style="font-size:90%;">P5</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.6.5.2"><span class="ltx_text" id="S5.T3.2.6.5.2.1" style="font-size:90%;">Software Engineer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.7.6.1"><span class="ltx_text" id="S5.T3.2.7.6.1.1" style="font-size:90%;">P6</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.7.6.2"><span class="ltx_text" id="S5.T3.2.7.6.2.1" style="font-size:90%;">Senior Research Scientist</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.8.7.1"><span class="ltx_text" id="S5.T3.2.8.7.1.1" style="font-size:90%;">P7</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.7.2"><span class="ltx_text" id="S5.T3.2.8.7.2.1" style="font-size:90%;">Advisory AI Engineer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.9.8.1"><span class="ltx_text" id="S5.T3.2.9.8.1.1" style="font-size:90%;">P8</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.9.8.2"><span class="ltx_text" id="S5.T3.2.9.8.2.1" style="font-size:90%;">Senior AI Technical Architect</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.10.9.1"><span class="ltx_text" id="S5.T3.2.10.9.1.1" style="font-size:90%;">P9</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.10.9.2"><span class="ltx_text" id="S5.T3.2.10.9.2.1" style="font-size:90%;">Distinguished Engineer and Master Inventor</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.11.10.1"><span class="ltx_text" id="S5.T3.2.11.10.1.1" style="font-size:90%;">P10</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.11.10.2"><span class="ltx_text" id="S5.T3.2.11.10.2.1" style="font-size:90%;">Research Scientist</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.12.11.1"><span class="ltx_text" id="S5.T3.2.12.11.1.1" style="font-size:90%;">P11</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.12.11.2"><span class="ltx_text" id="S5.T3.2.12.11.2.1" style="font-size:90%;">Research Software Engineer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.13.12.1"><span class="ltx_text" id="S5.T3.2.13.12.1.1" style="font-size:90%;">P12</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.13.12.2"><span class="ltx_text" id="S5.T3.2.13.12.2.1" style="font-size:90%;">Senior Technical Staff Member</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.14.13.1"><span class="ltx_text" id="S5.T3.2.14.13.1.1" style="font-size:90%;">P13</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.14.13.2"><span class="ltx_text" id="S5.T3.2.14.13.2.1" style="font-size:90%;">Platform Engineer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.2.15.14.1"><span class="ltx_text" id="S5.T3.2.15.14.1.1" style="font-size:90%;">P14</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.15.14.2"><span class="ltx_text" id="S5.T3.2.15.14.2.1" style="font-size:90%;">Research Scientist Intern</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.2.16.15.1"><span class="ltx_text" id="S5.T3.2.16.15.1.1" style="font-size:90%;">P15</span></th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T3.2.16.15.2"><span class="ltx_text" id="S5.T3.2.16.15.2.1" style="font-size:90%;">Technology Engineer</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Participant IDs and job roles</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<blockquote class="ltx_quote" id="S5.SS1.p2.1">
<p class="ltx_p" id="S5.SS1.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1.1">As a technology engineer, I interact with various LLM to perform many generative-ai tasks. However, besides manual inspection of LLM outputs, I have not found a way to consistently evaluate the performance of a LLM and see if each prompt is performing better or worse. </span></p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<blockquote class="ltx_quote" id="S5.SS1.p3.1">
<p class="ltx_p" id="S5.SS1.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1.1">I’ve had to look at a number of tooling before in relation to checking the validity of models. (Cross validation, Lime, SHAP, Rouge, Bleu, Flesch-Kincaid, Flesch. I’ve played with others but not to a level I can say I would know enough about them (eg. SQuAD, CIDEr, SPICE, WIT, ELI5) </span></p>
</blockquote>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>RQ1: What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ? </h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The criteria that users created varied widely across tasks, not only in terms of specificity, but also in the presence of instructions or rules, the number of items in the scale (for the direct assessment condition), the inclusion of exclusion criteria, and the use of examples. These variations reflected what aspects users prioritized when defining their criteria. To systematically analyze these differences, we generated a codebook and used it to categorize the observed criteria changes(see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.T4" title="Table 4 ‣ 5.2.1. Criteria Priority Differences within Tasks ‣ 5.2. RQ1: What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">4</span></a>). We observed a range of specificity, with some users providing highly detailed and task-specific criteria, while others offered more general guidelines. In some cases, users added rules or additional prompting to guide the model. For example, some criteria included explicit if-then rules to distinguish between acceptable and unacceptable options. Additionally, some users incorporated examples within their criteria to illustrate preferred outcomes, effectively providing the model with concrete cases to guide its responses. This approach highlights the diverse ways in which participants interpreted and operationalized the constructs in question. Moreover, the number of items included in the scale for direct assessment also varied, where some users offered detailed multi-item scales, while others opted for simpler, binary options. .</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Criteria Priority Differences within Tasks</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Within each task, participants prioritized different aspects when defining and interpreting criteria, leading to varied approaches. Using inductive coding <cite class="ltx_cite ltx_citemacro_citep">(Thomas, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib41" title="">2006</a>)</cite>, two authors generated a codebook of these priorities, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.T5" title="Table 5 ‣ 5.2.1. Criteria Priority Differences within Tasks ‣ 5.2. RQ1: What do practitioners prioritize in evaluation criteria when using LLMs as judges, and how do their priorities differ? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">In the Email Inclusivity task, participants prioritized cultural inclusivity, neutrality, and fairness. Some emphasized mentioning all cultures to avoid bias, ensuring holidays like Christmas, Hanukkah, Kwanzaa, and Diwali were included. Others focused on maintaining a neutral tone by avoiding specific mentions of culture, gender, race, or ethnicity. Additionally, some prioritized fairness, ensuring the email did not favor one culture over another. One participant intepreted inclusivity to mean financial inclusivity and designed the criteria around adherence to budget constraints, such as keeping gift exchange mentions within a $40 limit.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">In the Summarization task, participants varied in their focus on factual accuracy, inclusion of key points, and conciseness. Some prioritized ensuring the summary accurately reflected the article, while others focused on covering all key points comprehensively. Conciseness was also important, with participants aiming for a summary around 20% of the original length. Grammatical correctness and the inclusion of crucial details, like dates or prices, were also emphasized. Some participants relied on the AI to decide which is the best summary by keeping their criteria general and asking for the the ”best” summary.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">For the Q&amp;A Faithfulness task, participants focused on maintaining faithfulness to the source, adhering to policy, avoiding hallucinations, and ensuring correctness. They emphasized that responses should be strictly grounded in the reference document and follow company policy, with no deviation or fabrication. These differences within each task reflect the varied approaches participants took based on their interpretations and judgment of what was most important for the specific context.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.2.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.1.1.1.1">
<span class="ltx_p" id="S5.T4.2.1.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.1.1.1.1.1" style="font-size:90%;">Category</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.1.1.2.1">
<span class="ltx_p" id="S5.T4.2.1.1.2.1.1" style="width:99.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.1.2.1.1.1" style="font-size:90%;">Definition</span></span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.1.3.1" style="font-size:90%;">Taxonomy</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.1.1.4.1">
<span class="ltx_p" id="S5.T4.2.1.1.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.1.4.1.1.1" style="font-size:90%;">Example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.2.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.2.1.1.1">
<span class="ltx_p" id="S5.T4.2.2.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.1.1.1.1.1" style="font-size:90%;">Specificity</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.2.1.2.1">
<span class="ltx_p" id="S5.T4.2.2.1.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S5.T4.2.2.1.2.1.1.1" style="font-size:90%;">The criteria ranges in how specific it is to the nature of the task context example</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.2.1.3"><span class="ltx_text" id="S5.T4.2.2.1.3.1" style="font-size:90%;">High Specificity</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T4.2.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.2.1.4.1">
<span class="ltx_p" id="S5.T4.2.2.1.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.2.1.4.1.1.1" style="font-size:90%;">Please evaluate whether the following E-Mail is inclusive. This means that not only western traditions, such as Christmas, are celebrated, but employees are actively asked to contribute their customs and traditions to contribute to a diverse and inclusive company culture. Please also assess whether inclusive language is being used throughout the E-Mail.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.3.2">
<td class="ltx_td ltx_align_top" id="S5.T4.2.3.2.1"></td>
<td class="ltx_td ltx_align_top" id="S5.T4.2.3.2.2"></td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.3.2.3"><span class="ltx_text" id="S5.T4.2.3.2.3.1" style="font-size:90%;">Low Specificity</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.3.2.4.1">
<span class="ltx_p" id="S5.T4.2.3.2.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.3.2.4.1.1.1" style="font-size:90%;">The email is inclusive.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.4.3.1.1">
<span class="ltx_p" id="S5.T4.2.4.3.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.4.3.1.1.1.1" style="font-size:90%;">Additional Prompting</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.4.3.2.1">
<span class="ltx_p" id="S5.T4.2.4.3.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S5.T4.2.4.3.2.1.1.1" style="font-size:90%;">The criteria includes instructions beyond criteria description</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.4.3.3"><span class="ltx_text" id="S5.T4.2.4.3.3.1" style="font-size:90%;">Present</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.4.3.4.1">
<span class="ltx_p" id="S5.T4.2.4.3.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.4.3.4.1.1.1" style="font-size:90%;">Which summary offers the most clear and correct answer. As well as answering anything the customer did not know what to ask for in relation to their question. <span class="ltx_text ltx_font_bold" id="S5.T4.2.4.3.4.1.1.1.1">You must review each summary independently of the other summaries when making your judgement.</span></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.5.4">
<td class="ltx_td ltx_align_top" id="S5.T4.2.5.4.1"></td>
<td class="ltx_td ltx_align_top" id="S5.T4.2.5.4.2"></td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.5.4.3"><span class="ltx_text" id="S5.T4.2.5.4.3.1" style="font-size:90%;">Absent</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.5.4.4.1">
<span class="ltx_p" id="S5.T4.2.5.4.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.5.4.4.1.1.1" style="font-size:90%;">Which is the best summary?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.6.5.1.1">
<span class="ltx_p" id="S5.T4.2.6.5.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.6.5.1.1.1.1" style="font-size:90%;">Rules</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.6.5.2.1">
<span class="ltx_p" id="S5.T4.2.6.5.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S5.T4.2.6.5.2.1.1.1" style="font-size:90%;">If/else rules provided</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.6.5.3"><span class="ltx_text" id="S5.T4.2.6.5.3.1" style="font-size:90%;">Present</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.6.5.4.1">
<span class="ltx_p" id="S5.T4.2.6.5.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.6.5.4.1.1.1" style="font-size:90%;">”Does the response refer to the Travel and Expense policy? - Good
Makes exceptions to known rules (e.g., trip length different or different meeting days) - Bad
Does the response say all charges should be to corporate card while picking up on the fact that upgrading/business class is a personal expense….correctly dissociates the corporate from the personal expense -Good
Including rationale or policy statements - Good”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.7.6">
<td class="ltx_td ltx_align_top" id="S5.T4.2.7.6.1"></td>
<td class="ltx_td ltx_align_top" id="S5.T4.2.7.6.2"></td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.7.6.3"><span class="ltx_text" id="S5.T4.2.7.6.3.1" style="font-size:90%;">Absent</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.7.6.4.1">
<span class="ltx_p" id="S5.T4.2.7.6.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.7.6.4.1.1.1" style="font-size:90%;">Is the response faithful according to the reference document?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.8.7.1.1">
<span class="ltx_p" id="S5.T4.2.8.7.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.8.7.1.1.1.1" style="font-size:90%;">Exclusion Criteria</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.8.7.2.1">
<span class="ltx_p" id="S5.T4.2.8.7.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S5.T4.2.8.7.2.1.1.1" style="font-size:90%;">Criteria includes what should not be considered</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.8.7.3"><span class="ltx_text" id="S5.T4.2.8.7.3.1" style="font-size:90%;">Present</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.8.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.8.7.4.1">
<span class="ltx_p" id="S5.T4.2.8.7.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.8.7.4.1.1.1" style="font-size:90%;">’The summary does not repeat unimportant details.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.9.8">
<td class="ltx_td ltx_align_top" id="S5.T4.2.9.8.1"></td>
<td class="ltx_td ltx_align_top" id="S5.T4.2.9.8.2"></td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.9.8.3"><span class="ltx_text" id="S5.T4.2.9.8.3.1" style="font-size:90%;">Absent</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.9.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.9.8.4.1">
<span class="ltx_p" id="S5.T4.2.9.8.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.9.8.4.1.1.1" style="font-size:90%;">The summary covers important aspects from the text.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.10.9.1.1">
<span class="ltx_p" id="S5.T4.2.10.9.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.2.10.9.1.1.1.1" style="font-size:90%;">Examples Provided</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.10.9.2.1">
<span class="ltx_p" id="S5.T4.2.10.9.2.1.1" style="width:99.6pt;"><span class="ltx_text" id="S5.T4.2.10.9.2.1.1.1" style="font-size:90%;">Examples provided within criteria</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T4.2.10.9.3"><span class="ltx_text" id="S5.T4.2.10.9.3.1" style="font-size:90%;">Present</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T4.2.10.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.10.9.4.1">
<span class="ltx_p" id="S5.T4.2.10.9.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.10.9.4.1.1.1" style="font-size:90%;">Criteria: The email should be inclusive taking into consideration all employees, ’Yes’: ”Happy Holidays! let’s bring some gifts for all”, ’No’: ”Merry Christmas! let’s have Christmas tress!”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.11.10">
<td class="ltx_td ltx_align_top ltx_border_bb" id="S5.T4.2.11.10.1"></td>
<td class="ltx_td ltx_align_top ltx_border_bb" id="S5.T4.2.11.10.2"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.11.10.3"><span class="ltx_text" id="S5.T4.2.11.10.3.1" style="font-size:90%;">Absent</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T4.2.11.10.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.2.11.10.4.1">
<span class="ltx_p" id="S5.T4.2.11.10.4.1.1" style="width:184.9pt;"><span class="ltx_text ltx_font_italic" id="S5.T4.2.11.10.4.1.1.1" style="font-size:90%;">Criteria: Is the email inclusive?, ’Yes’: ’The email is inclusive.’, ’No’: ’The email is not inclusive.’</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Categories of final criteria generated by participants for each task.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T5.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.1.1.1.1">
<span class="ltx_p" id="S5.T5.2.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.1.1.1.1" style="font-size:90%;">Task</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T5.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.1.1.2.1">
<span class="ltx_p" id="S5.T5.2.1.1.2.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.2.1.1.1" style="font-size:90%;">Interpretation of Criteria</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T5.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.1.1.3.1">
<span class="ltx_p" id="S5.T5.2.1.1.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.3.1.1.1" style="font-size:90%;">Example</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.2.2.1.1">
<span class="ltx_p" id="S5.T5.2.2.2.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T5.2.2.2.1.1.1.1" style="font-size:90%;">Email Inclusivity</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.2.2.2.1">
<span class="ltx_p" id="S5.T5.2.2.2.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.2.2.2.1.1.1" style="font-size:90%;">Email must mention all possible cultures</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.2.2.3.1">
<span class="ltx_p" id="S5.T5.2.2.2.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.2.2.3.1.1.1" style="font-size:90%;">The generated email should be inclusive. It should mention the different cultures and holidays (such as Christmas, Hanukkah, Kwanzaa, Diwali, and others) . It should not be exclusive to one culture. The email should use inclusive terms such as holiday and festive as opposed to terms exclusive to one culture.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.3.3">
<td class="ltx_td ltx_align_top" id="S5.T5.2.3.3.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.3.3.2.1">
<span class="ltx_p" id="S5.T5.2.3.3.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.3.3.2.1.1.1" style="font-size:90%;">Email must not mention any specific culture</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.3.3.3.1">
<span class="ltx_p" id="S5.T5.2.3.3.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.3.3.3.1.1.1" style="font-size:90%;">”The email is very formal and does not mention any specific gender, race, ethnicity, culture based terms or words.
”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4.4">
<td class="ltx_td ltx_align_top" id="S5.T5.2.4.4.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.4.4.2.1">
<span class="ltx_p" id="S5.T5.2.4.4.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.4.4.2.1.1.1" style="font-size:90%;">Email must not favor one culture to another</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.4.4.3.1">
<span class="ltx_p" id="S5.T5.2.4.4.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.4.4.3.1.1.1" style="font-size:90%;">Which email invitation is most inclusive? An email is inclusive if it does not favour one religion over others and if it does not imply that all employees belong to a certain group or celebrate a specific holiday. An inclusive email refrains from using denominational words.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5.5">
<td class="ltx_td ltx_align_top" id="S5.T5.2.5.5.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.5.5.2.1">
<span class="ltx_p" id="S5.T5.2.5.5.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.5.5.2.1.1.1" style="font-size:90%;">Gift exchange budget mentioned in email should not exceed particular amount</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.5.5.3.1">
<span class="ltx_p" id="S5.T5.2.5.5.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.5.5.3.1.1.1" style="font-size:90%;">”The price mentioned falls within the $40 budget limit.”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.6.6.1.1">
<span class="ltx_p" id="S5.T5.2.6.6.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T5.2.6.6.1.1.1.1" style="font-size:90%;">Summarization Preference</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.6.6.2.1">
<span class="ltx_p" id="S5.T5.2.6.6.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.6.6.2.1.1.1" style="font-size:90%;">Summary must be factual</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.6.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.6.6.3.1">
<span class="ltx_p" id="S5.T5.2.6.6.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.6.6.3.1.1.1" style="font-size:90%;">The summarization contains factual statements from the article</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.7.7">
<td class="ltx_td ltx_align_top" id="S5.T5.2.7.7.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.7.7.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.7.7.2.1">
<span class="ltx_p" id="S5.T5.2.7.7.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.7.7.2.1.1.1" style="font-size:90%;">Summary must include key points from the article</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.7.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.7.7.3.1">
<span class="ltx_p" id="S5.T5.2.7.7.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.7.7.3.1.1.1" style="font-size:90%;">The summary accurately conveys the main points of the article.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.8.8">
<td class="ltx_td ltx_align_top" id="S5.T5.2.8.8.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.8.8.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.8.8.2.1">
<span class="ltx_p" id="S5.T5.2.8.8.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.8.8.2.1.1.1" style="font-size:90%;">Summary must be succinct</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.8.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.8.8.3.1">
<span class="ltx_p" id="S5.T5.2.8.8.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.8.8.3.1.1.1" style="font-size:90%;">The summary length is approximately 20% of the original article length.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.9.9">
<td class="ltx_td ltx_align_top" id="S5.T5.2.9.9.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.9.9.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.9.9.2.1">
<span class="ltx_p" id="S5.T5.2.9.9.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.9.9.2.1.1.1" style="font-size:90%;">Summary must be grammatically correct</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.9.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.9.9.3.1">
<span class="ltx_p" id="S5.T5.2.9.9.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.9.9.3.1.1.1" style="font-size:90%;">Is the response grammatically, lexically, semantically, and syntactically correct and common, including punctuation accuracy?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.10.10">
<td class="ltx_td ltx_align_top" id="S5.T5.2.10.10.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.10.10.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.10.10.2.1">
<span class="ltx_p" id="S5.T5.2.10.10.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.10.10.2.1.1.1" style="font-size:90%;">Summary must include specific details (detailed by user) from summary</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.10.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.10.10.3.1">
<span class="ltx_p" id="S5.T5.2.10.10.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.10.10.3.1.1.1" style="font-size:90%;">Summary described dress worn by Vivien Leigh with details of price and date.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.11.11">
<td class="ltx_td ltx_align_top" id="S5.T5.2.11.11.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.11.11.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.11.11.2.1">
<span class="ltx_p" id="S5.T5.2.11.11.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.11.11.2.1.1.1" style="font-size:90%;">Summary should be the “best”</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.11.11.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.11.11.3.1">
<span class="ltx_p" id="S5.T5.2.11.11.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.11.11.3.1.1.1" style="font-size:90%;">Which is the best summary?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.12.12.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.12.12.1.1">
<span class="ltx_p" id="S5.T5.2.12.12.1.1.1" style="width:56.9pt;"><span class="ltx_text" id="S5.T5.2.12.12.1.1.1.1" style="font-size:90%;">Q&amp;A Faithfulness</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.12.12.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.12.12.2.1">
<span class="ltx_p" id="S5.T5.2.12.12.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.12.12.2.1.1.1" style="font-size:90%;">Response must be grounded/faithful to the reference document</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T5.2.12.12.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.12.12.3.1">
<span class="ltx_p" id="S5.T5.2.12.12.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.12.12.3.1.1.1" style="font-size:90%;">“Based on the reference document, does the response answer the customer question correctly?”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.13.13">
<td class="ltx_td ltx_align_top" id="S5.T5.2.13.13.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.13.13.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.13.13.2.1">
<span class="ltx_p" id="S5.T5.2.13.13.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.13.13.2.1.1.1" style="font-size:90%;">Response should adhere to company policy</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.13.13.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.13.13.3.1">
<span class="ltx_p" id="S5.T5.2.13.13.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.13.13.3.1.1.1" style="font-size:90%;">“Has the employee claimed only the regular ”in policy” fare and not the personal cost?”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.14.14">
<td class="ltx_td ltx_align_top" id="S5.T5.2.14.14.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.14.14.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.14.14.2.1">
<span class="ltx_p" id="S5.T5.2.14.14.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.14.14.2.1.1.1" style="font-size:90%;">Response should not include hallucinations</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T5.2.14.14.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.14.14.3.1">
<span class="ltx_p" id="S5.T5.2.14.14.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.14.14.3.1.1.1" style="font-size:90%;">“Are there some hallucinations in the answer?”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.15.15">
<td class="ltx_td ltx_align_top ltx_border_bb" id="S5.T5.2.15.15.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T5.2.15.15.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.15.15.2.1">
<span class="ltx_p" id="S5.T5.2.15.15.2.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.T5.2.15.15.2.1.1.1" style="font-size:90%;">Response should be correct</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T5.2.15.15.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.2.15.15.3.1">
<span class="ltx_p" id="S5.T5.2.15.15.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_italic" id="S5.T5.2.15.15.3.1.1.1" style="font-size:90%;">“Which summary offers the most clear and correct answer.”</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5. </span>Variations of interpretations of criteria within each task.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="234" id="S5.F7.g1" src="extracted/5893543/figures/criteria2.png" width="359"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.3.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S5.F7.4.2" style="font-size:90%;">Example of changes made to criteria and AI evaluator for the by Participant #1 for the Q&amp;A HR Task. The participant changed the AI evaluator model across the evaluations and changed the number of items in the scale as well as their corresponding definitions. The criteria definition remained the same throughout the evaluations: <span class="ltx_text ltx_font_italic" id="S5.F7.4.2.1">Did the answer give the following? (1) Factually correct answer based on the document. (2) Included related information directly to the question. (3) Answer is clear and concise.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>RQ2: What strategies do users employ to refine their criteria for achieving human-AI alignment in LLM-as-a-judge evaluations?</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To understand how people refine their criteria when using large language models (LLMs) as judges, we logged all metadata, including the criteria and AI evaluator used, for each evaluation across various tasks. Two authors coded the types of edits users made, identifying several ways participants modified their criteria to better align with task requirements or personal preferences. One common modification was making criteria more specific. For example, in the Email Inclusivity task, a participant refined the criterion from simply stating that ”The generated email should be inclusive” to specifying that it should use inclusive terms like ”holiday” and ”festive.” This added precision provided clearer guidance for evaluation. In contrast, some participants generalized their criteria to broaden their scope. In the same task, a criterion initially focused on specific groups—such as cultures and backgrounds—was simplified to a more general statement: ”The email is inclusive to all cultures and backgrounds.”</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Participants also made adjustments by removing unnecessary scale items or adding more detailed options. For instance, in the Summarization task, an original scale with multiple levels (”Absolutely,” ”Somewhat,” ”No”) was simplified to a binary ”Yes” or ”No,” while in other cases, the scale was expanded to include detailed levels like ”Excellent,” ”Good,” ”Average,” and ”Poor.” In addition, some users specified existing scale items to clarify their meaning, such as adding definitions for ”Inclusive” and ”Not Inclusive” in the Email Inclusivity task. Conversely, others generalized scale items, broadening them to encompass a wider range of cultural practices. These results can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S4.T2" title="Table 2 ‣ 4.1. Tasks and Evaluation Criteria ‣ 4. Methodology ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Minor edits were also common, such as rephrasing criteria for clarity. For example, the question ”What is the best summary?” was revised to include clearer instructions: ”Which of the following summaries best describe the article? Review each independently of the others.” Some participants added exclusion criteria to prevent irrelevant elements from being considered. One participant, for instance, added a criterion stating that ”The summary should be accurate and concise. Has no fake data generated outside of reference,” clearly outlining what should be excluded. These refinements illustrate the range of strategies participants used to tailor their criteria, from increasing specificity to allowing greater flexibility. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F8" title="Figure 8 ‣ 5.3. RQ2: What strategies do users employ to refine their criteria for achieving human-AI alignment in LLM-as-a-judge evaluations? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">8</span></a> shows the sequential changes in evaluation criteria by task type and user.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="273" id="S5.F8.g1" src="extracted/5893543/figures/sq2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">Sequential changes in evaluation criteria by task type and user. Symbols represent different types of modifications, including changes to criteria, scale items, models, and exclusions, with variation across Q&amp;A, Email Generation, and Summarization tasks.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria?</h3>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F10.1" style="width:195.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="446" id="S5.F10.1.g1" src="extracted/5893543/figures/evaluation_bar_chart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.1.1.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S5.F10.1.2.2" style="font-size:90%;">Participants conducted more evaluations in the direct assessment task compared to the pairwise task (p ¡ 0.05).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F10.2" style="width:195.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="445" id="S5.F10.2.g1" src="extracted/5893543/figures/alignment.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S5.F10.2.2.2" style="font-size:90%;">There were marginal significant differences in the degree of alignment achieved across task types (p = 0.08).</span></figcaption>
</figure>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1. </span>What are the differences for the total amount of evaluations run?</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">Participants were able to run multiple evaluations for each task yielding a total of 131 evaluations. We investigated whether there were differences in the strategies users employed when refining their criteria, particularly focusing on the total number of evaluations conducted. A repeated measures ANOVA was performed to compare the number of evaluations across different judge strategies (pairwise vs. rubric) and tasks (Q&amp;A Faithfulness, Email Inclusivity, Summarization). The results indicated a significant difference in the number of evaluations between judge strategies, with more evaluations being conducted in the direct assessment (rubric) condition compared to the pairwise condition. Specifically, the ANOVA showed that the judge strategy had a significant impact on the number of evaluations run (F(1, 14) = 12.64, p = 0.003), and the interaction between task and strategy was not significant. Post-hoc analysis using Tukey’s HSD further confirmed that the pairwise strategy led to significantly fewer evaluations than the rubric strategy (mean difference = 0.72, p = 0.007). The number of evaluations can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F10" title="Figure 10 ‣ 5.4. RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">10</span></a></p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2. </span>What are the differences in human-AI alignment?</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">We examined the the extent to which users agreed with the AI model’s assessments across different conditions. Upon completion of each task, users indicated agreement with the model’s judgement. We conducted a repeated measures ANOVA, focusing on the judge strategies (pairwise vs. rubric) and the tasks (Q&amp;A Faithfulness, Email Inclusivity, Summarization). The analysis revealed a marginally significant main effect for task type, <math alttext="F(2,28)=2.73,p=0.08" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.1.m1.4"><semantics id="S5.SS4.SSS2.p1.1.m1.4a"><mrow id="S5.SS4.SSS2.p1.1.m1.4.4.2" xref="S5.SS4.SSS2.p1.1.m1.4.4.3.cmml"><mrow id="S5.SS4.SSS2.p1.1.m1.3.3.1.1" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.cmml"><mrow id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.cmml"><mi id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.2" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.1" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.2" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.1.cmml"><mo id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS4.SSS2.p1.1.m1.1.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.cmml">2</mn><mo id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.2.2" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS4.SSS2.p1.1.m1.2.2" xref="S5.SS4.SSS2.p1.1.m1.2.2.cmml">28</mn><mo id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.1" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.1.cmml">=</mo><mn id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.3" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.3.cmml">2.73</mn></mrow><mo id="S5.SS4.SSS2.p1.1.m1.4.4.2.3" xref="S5.SS4.SSS2.p1.1.m1.4.4.3a.cmml">,</mo><mrow id="S5.SS4.SSS2.p1.1.m1.4.4.2.2" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.cmml"><mi id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.2" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.2.cmml">p</mi><mo id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.1" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.1.cmml">=</mo><mn id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.3" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.3.cmml">0.08</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.1.m1.4b"><apply id="S5.SS4.SSS2.p1.1.m1.4.4.3.cmml" xref="S5.SS4.SSS2.p1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p1.1.m1.4.4.3a.cmml" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1"><eq id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.1"></eq><apply id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2"><times id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.1"></times><ci id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.2.3.2"><cn id="S5.SS4.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.1.m1.1.1">2</cn><cn id="S5.SS4.SSS2.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS4.SSS2.p1.1.m1.2.2">28</cn></interval></apply><cn id="S5.SS4.SSS2.p1.1.m1.3.3.1.1.3.cmml" type="float" xref="S5.SS4.SSS2.p1.1.m1.3.3.1.1.3">2.73</cn></apply><apply id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2"><eq id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.1"></eq><ci id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.2.cmml" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.2">𝑝</ci><cn id="S5.SS4.SSS2.p1.1.m1.4.4.2.2.3.cmml" type="float" xref="S5.SS4.SSS2.p1.1.m1.4.4.2.2.3">0.08</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.1.m1.4c">F(2,28)=2.73,p=0.08</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.1.m1.4d">italic_F ( 2 , 28 ) = 2.73 , italic_p = 0.08</annotation></semantics></math>, indicating that there was a difference between alignment achieved between the tasks. Tukey post hoc analysis further confirmed this finding, showing a marginally significant difference in alignment between the the Q&amp;A task and the Email Generation Task (mean difference = -0.633, p = 0.08). The number of agreements can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F10" title="Figure 10 ‣ 5.4. RQ3: How do task-related factors and judge strategy impact how practitioners refine criteria? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions?</h3>
<figure class="ltx_figure" id="S5.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F14.1" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="446" id="S5.F14.1.g1" src="extracted/5893543/figures/trust.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.1.1.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text" id="S5.F14.1.2.2" style="font-size:90%;">No significant differences in user trust in the AI evaluator were found between the tasks or the AI judge strategies.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F14.2" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="446" id="S5.F14.2.g1" src="extracted/5893543/figures/explanations.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.2.1.1.1" style="font-size:90%;">Figure 12</span>. </span><span class="ltx_text" id="S5.F14.2.2.2" style="font-size:90%;">Explanations were percieved as more helpful in the rubric condition than in the pairwise condition (p¡0.05). </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F14.3" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="446" id="S5.F14.3.g1" src="extracted/5893543/figures/posbias.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.3.1.1.1" style="font-size:90%;">Figure 13</span>. </span><span class="ltx_text" id="S5.F14.3.2.2" style="font-size:90%;">No significant differences in perceived positional bias helpfulness were found across tasks or AI judge strategies.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F14.4" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="446" id="S5.F14.4.g1" src="extracted/5893543/figures/mentalload.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.4.1.1.1" style="font-size:90%;">Figure 14</span>. </span><span class="ltx_text" id="S5.F14.4.2.2" style="font-size:90%;">Users experienced higher cognitive load in the Summarization task than the Q&amp;A task p¡0.05).</span></figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We aimed to investigate whether task-related factors and judge strategy impact user perceptions, specifically focusing on AI trust, self-satisfaction, perception of explanations, and cognitive load. For each of these measures, we conducted a repeated measures ANOVA, and we report the results below.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>AI Trust</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">For trust in the AI evaluator, a repeated measures ANOVA showed no significant differences between tasks or AI strategies (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F14" title="Figure 14 ‣ 5.5. RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">14</span></a>). Despite this, we identified reasons why participants either trusted or distrusted the AI evaluator.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.1. </span>Reasons for Trust</h4>
<div class="ltx_para" id="S5.SS6.SSS1.p1">
<p class="ltx_p" id="S5.SS6.SSS1.p1.1">Participants who reported higher trust (above the median score of 4) highlighted several factors, such as the alignment between AI judgments and their criteria, prior positive experiences with specific models, and cohesive explanations. Some examples of feedback include:</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS1.p2">
<blockquote class="ltx_quote" id="S5.SS6.SSS1.p2.1">
<p class="ltx_p" id="S5.SS6.SSS1.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS1.p2.1.1.1">I didn’t trust fully in the beginning, but the explanations helped.</span> #P10 (Summarization, Rubric)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS6.SSS1.p3">
<blockquote class="ltx_quote" id="S5.SS6.SSS1.p3.1">
<p class="ltx_p" id="S5.SS6.SSS1.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS1.p3.1.1.1">Assessment aligned with my own.</span> #P5 (Q&amp;A, Rubric)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS6.SSS1.p4">
<blockquote class="ltx_quote" id="S5.SS6.SSS1.p4.1">
<p class="ltx_p" id="S5.SS6.SSS1.p4.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS1.p4.1.1.1">The AI judge was more accurate than me on my own faithfulness metric. It helped me identify gaps in my quick assessment.</span> #P9 (Q&amp;A, Rubric)</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.2. </span>Reasons for Distrust</h4>
<div class="ltx_para" id="S5.SS6.SSS2.p1">
<p class="ltx_p" id="S5.SS6.SSS2.p1.1">Conversely, participants who expressed lower trust often felt that the AI model imposed its own criteria or failed to align with their expectations:</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS2.p2">
<blockquote class="ltx_quote" id="S5.SS6.SSS2.p2.1">
<p class="ltx_p" id="S5.SS6.SSS2.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS2.p2.1.1.1">The model was opinionated on the way to evaluate and didn’t enforce my criteria.</span> #P4 (Email Generation, Pairwise)</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.3. </span>Perception of Explanations</h4>
<div class="ltx_para" id="S5.SS6.SSS3.p1">
<p class="ltx_p" id="S5.SS6.SSS3.p1.3">We conducted an ANOVA to determine whether the type of AI Judge Strategy (pairwise vs. rubric) affected participants’ perception of how helpful the explanations were in completing tasks. The results indicated a significant main effect of the type of AI strategy, <math alttext="F(1,14)=7.79,p=0.014" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p1.1.m1.4"><semantics id="S5.SS6.SSS3.p1.1.m1.4a"><mrow id="S5.SS6.SSS3.p1.1.m1.4.4.2" xref="S5.SS6.SSS3.p1.1.m1.4.4.3.cmml"><mrow id="S5.SS6.SSS3.p1.1.m1.3.3.1.1" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.cmml"><mrow id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.2" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.1" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.2" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS3.p1.1.m1.1.1" xref="S5.SS6.SSS3.p1.1.m1.1.1.cmml">1</mn><mo id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS3.p1.1.m1.2.2" xref="S5.SS6.SSS3.p1.1.m1.2.2.cmml">14</mn><mo id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.1" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.3" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.3.cmml">7.79</mn></mrow><mo id="S5.SS6.SSS3.p1.1.m1.4.4.2.3" xref="S5.SS6.SSS3.p1.1.m1.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS3.p1.1.m1.4.4.2.2" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.cmml"><mi id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.2" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.1" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.3" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.3.cmml">0.014</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p1.1.m1.4b"><apply id="S5.SS6.SSS3.p1.1.m1.4.4.3.cmml" xref="S5.SS6.SSS3.p1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS3.p1.1.m1.4.4.3a.cmml" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1"><eq id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.1"></eq><apply id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2"><times id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS3.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS6.SSS3.p1.1.m1.1.1">1</cn><cn id="S5.SS6.SSS3.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS6.SSS3.p1.1.m1.2.2">14</cn></interval></apply><cn id="S5.SS6.SSS3.p1.1.m1.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS3.p1.1.m1.3.3.1.1.3">7.79</cn></apply><apply id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.cmml" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2"><eq id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.1.cmml" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.1"></eq><ci id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.2.cmml" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS3.p1.1.m1.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS3.p1.1.m1.4.4.2.2.3">0.014</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p1.1.m1.4c">F(1,14)=7.79,p=0.014</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p1.1.m1.4d">italic_F ( 1 , 14 ) = 7.79 , italic_p = 0.014</annotation></semantics></math>, with rubric-based explanations rated more favorably overall compared to pairwise. However, there were no significant effects of task type, <math alttext="F(2,28)=0.09,p=0.9152" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p1.2.m2.4"><semantics id="S5.SS6.SSS3.p1.2.m2.4a"><mrow id="S5.SS6.SSS3.p1.2.m2.4.4.2" xref="S5.SS6.SSS3.p1.2.m2.4.4.3.cmml"><mrow id="S5.SS6.SSS3.p1.2.m2.3.3.1.1" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.cmml"><mrow id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.2" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.1" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.2" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS3.p1.2.m2.1.1" xref="S5.SS6.SSS3.p1.2.m2.1.1.cmml">2</mn><mo id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS3.p1.2.m2.2.2" xref="S5.SS6.SSS3.p1.2.m2.2.2.cmml">28</mn><mo id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.1" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.3" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.3.cmml">0.09</mn></mrow><mo id="S5.SS6.SSS3.p1.2.m2.4.4.2.3" xref="S5.SS6.SSS3.p1.2.m2.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS3.p1.2.m2.4.4.2.2" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.cmml"><mi id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.2" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.1" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.3" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.3.cmml">0.9152</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p1.2.m2.4b"><apply id="S5.SS6.SSS3.p1.2.m2.4.4.3.cmml" xref="S5.SS6.SSS3.p1.2.m2.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS3.p1.2.m2.4.4.3a.cmml" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1"><eq id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.1.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.1"></eq><apply id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2"><times id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS3.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS6.SSS3.p1.2.m2.1.1">2</cn><cn id="S5.SS6.SSS3.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS6.SSS3.p1.2.m2.2.2">28</cn></interval></apply><cn id="S5.SS6.SSS3.p1.2.m2.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS3.p1.2.m2.3.3.1.1.3">0.09</cn></apply><apply id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.cmml" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2"><eq id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.1.cmml" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.1"></eq><ci id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.2.cmml" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS3.p1.2.m2.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS3.p1.2.m2.4.4.2.2.3">0.9152</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p1.2.m2.4c">F(2,28)=0.09,p=0.9152</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p1.2.m2.4d">italic_F ( 2 , 28 ) = 0.09 , italic_p = 0.9152</annotation></semantics></math>, nor was there a significant interaction between task and strategy, <math alttext="F(2,28)=0.67,p=0.521" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p1.3.m3.4"><semantics id="S5.SS6.SSS3.p1.3.m3.4a"><mrow id="S5.SS6.SSS3.p1.3.m3.4.4.2" xref="S5.SS6.SSS3.p1.3.m3.4.4.3.cmml"><mrow id="S5.SS6.SSS3.p1.3.m3.3.3.1.1" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.cmml"><mrow id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.2" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.1" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.2" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS3.p1.3.m3.1.1" xref="S5.SS6.SSS3.p1.3.m3.1.1.cmml">2</mn><mo id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS3.p1.3.m3.2.2" xref="S5.SS6.SSS3.p1.3.m3.2.2.cmml">28</mn><mo id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.1" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.3" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.3.cmml">0.67</mn></mrow><mo id="S5.SS6.SSS3.p1.3.m3.4.4.2.3" xref="S5.SS6.SSS3.p1.3.m3.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS3.p1.3.m3.4.4.2.2" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.cmml"><mi id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.2" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.1" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.3" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.3.cmml">0.521</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p1.3.m3.4b"><apply id="S5.SS6.SSS3.p1.3.m3.4.4.3.cmml" xref="S5.SS6.SSS3.p1.3.m3.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS3.p1.3.m3.4.4.3a.cmml" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1"><eq id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.1.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.1"></eq><apply id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2"><times id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS3.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS6.SSS3.p1.3.m3.1.1">2</cn><cn id="S5.SS6.SSS3.p1.3.m3.2.2.cmml" type="integer" xref="S5.SS6.SSS3.p1.3.m3.2.2">28</cn></interval></apply><cn id="S5.SS6.SSS3.p1.3.m3.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS3.p1.3.m3.3.3.1.1.3">0.67</cn></apply><apply id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.cmml" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2"><eq id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.1.cmml" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.1"></eq><ci id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.2.cmml" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS3.p1.3.m3.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS3.p1.3.m3.4.4.2.2.3">0.521</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p1.3.m3.4c">F(2,28)=0.67,p=0.521</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p1.3.m3.4d">italic_F ( 2 , 28 ) = 0.67 , italic_p = 0.521</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS3.p2">
<p class="ltx_p" id="S5.SS6.SSS3.p2.1">To further explore these differences, we performed a post hoc analysis using Tukey’s HSD test. The results revealed that participants rated rubric explanations significantly higher than pairwise explanations (mean difference = 0.73, <math alttext="p=0.008" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p2.1.m1.1"><semantics id="S5.SS6.SSS3.p2.1.m1.1a"><mrow id="S5.SS6.SSS3.p2.1.m1.1.1" xref="S5.SS6.SSS3.p2.1.m1.1.1.cmml"><mi id="S5.SS6.SSS3.p2.1.m1.1.1.2" xref="S5.SS6.SSS3.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS6.SSS3.p2.1.m1.1.1.1" xref="S5.SS6.SSS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS3.p2.1.m1.1.1.3" xref="S5.SS6.SSS3.p2.1.m1.1.1.3.cmml">0.008</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p2.1.m1.1b"><apply id="S5.SS6.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS6.SSS3.p2.1.m1.1.1"><eq id="S5.SS6.SSS3.p2.1.m1.1.1.1.cmml" xref="S5.SS6.SSS3.p2.1.m1.1.1.1"></eq><ci id="S5.SS6.SSS3.p2.1.m1.1.1.2.cmml" xref="S5.SS6.SSS3.p2.1.m1.1.1.2">𝑝</ci><cn id="S5.SS6.SSS3.p2.1.m1.1.1.3.cmml" type="float" xref="S5.SS6.SSS3.p2.1.m1.1.1.3">0.008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p2.1.m1.1c">p=0.008</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p2.1.m1.1d">italic_p = 0.008</annotation></semantics></math>), suggesting that rubric explanations were perceived as more useful across all tasks (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F14" title="Figure 14 ‣ 5.5. RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">14</span></a>). We also analyzed participants’ feedback on when explanations were perceived as helpful versus unhelpful. Explanations were deemed helpful in two key ways: they served as tools for refining criteria or as a means of validating existing criteria.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS3.p3">
<blockquote class="ltx_quote" id="S5.SS6.SSS3.p3.1">
<p class="ltx_p" id="S5.SS6.SSS3.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS3.p3.1.1.1">After looking at the explanations, I realized that criteria can be improved and be made more specific. For the criteria I chose, I think I agree with the explanations provided, and it helps me understand the ranking better.</span> #P10 (Email Generation, Pairwise)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS6.SSS3.p4">
<p class="ltx_p" id="S5.SS6.SSS3.p4.1">Conversely, explanations were considered unhelpful when they were either not used or were found to be ineffective in resolving discrepancies.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS3.p5">
<blockquote class="ltx_quote" id="S5.SS6.SSS3.p5.1">
<p class="ltx_p" id="S5.SS6.SSS3.p5.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS3.p5.1.1.1">Explanations helped me see the shortcoming of AI, but the shortcomings were not able to be fixed with multiple iterations of criteria refinement.</span> #P4 (Email Generation, Pairwise)</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.4. </span>Perception of Positional Bias</h4>
<div class="ltx_para" id="S5.SS6.SSS4.p1">
<p class="ltx_p" id="S5.SS6.SSS4.p1.1">Of the 196 total evaluations run by the 15 users, positional bias appeared in 35% of those evaluations. We conducted an ANOVA to determine whether the type of AI Judge strategy (pairwise vs. rubric) and task type (Q&amp;A, email generation, summarization) affected participants’ perception of how helpful indication of positional bias was in completing the tasks. The analysis revealed no significant main effects, indicating that neither the type of task nor the strategy used by the judge had a significant impact on participants’ perceptions of the helpfulness of positional bias (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F14" title="Figure 14 ‣ 5.5. RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">14</span></a>).
This lack of effect could potentially be explained by the fact that positional bias was only present in 35% percent of evaluations. An independent samples t-test was conducted to compare the perceived helpfulness when participants results included positional bias and when they did not. The results showed a significant difference, t(28) = 2.98, p = 0.003, showing that when positional bias was present, participants reported it being more helpful than it did not appear in the results. The primary reason for positional bias not being helpful was that it did not appear in the results.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS4.p2">
<blockquote class="ltx_quote" id="S5.SS6.SSS4.p2.1">
<p class="ltx_p" id="S5.SS6.SSS4.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS4.p2.1.1.1">I quickly glanced to see if there was positional bias detected but it was not. Hence, I did not follow up on that. </span> #P10 (Q&amp;A, Pairwise)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS6.SSS4.p3">
<p class="ltx_p" id="S5.SS6.SSS4.p3.1">Participants reported that the positional bias was helpful because it alerted them to rephrase their criteria.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS4.p4">
<blockquote class="ltx_quote" id="S5.SS6.SSS4.p4.1">
<p class="ltx_p" id="S5.SS6.SSS4.p4.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS4.p4.1.1.1">Positional bias inspired me to simplify the criteria and rephrase them.</span> #P2 (Summarization, Rubric)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS6.SSS4.p5">
<blockquote class="ltx_quote" id="S5.SS6.SSS4.p5.1">
<p class="ltx_p" id="S5.SS6.SSS4.p5.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS6.SSS4.p5.1.1.1">It alerted me that some rephrasing might be needed before expanding to the whole dataset.</span> #P2 (Q&amp;A, Pairwise)</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.5. </span>Cognitive Load</h4>
<div class="ltx_para" id="S5.SS6.SSS5.p1">
<p class="ltx_p" id="S5.SS6.SSS5.p1.3">We also assessed cognitive load using three items from the NASA TLX (Task Load Index). The repeated measures ANOVA revealed a significant main effect of the task on cognitive load, <math alttext="F(2,28)=9.41,p=0.0008" class="ltx_Math" display="inline" id="S5.SS6.SSS5.p1.1.m1.4"><semantics id="S5.SS6.SSS5.p1.1.m1.4a"><mrow id="S5.SS6.SSS5.p1.1.m1.4.4.2" xref="S5.SS6.SSS5.p1.1.m1.4.4.3.cmml"><mrow id="S5.SS6.SSS5.p1.1.m1.3.3.1.1" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.cmml"><mrow id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.2" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.1" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.2" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS5.p1.1.m1.1.1" xref="S5.SS6.SSS5.p1.1.m1.1.1.cmml">2</mn><mo id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS5.p1.1.m1.2.2" xref="S5.SS6.SSS5.p1.1.m1.2.2.cmml">28</mn><mo id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.1" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.3" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.3.cmml">9.41</mn></mrow><mo id="S5.SS6.SSS5.p1.1.m1.4.4.2.3" xref="S5.SS6.SSS5.p1.1.m1.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS5.p1.1.m1.4.4.2.2" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.cmml"><mi id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.2" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.1" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.3" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.3.cmml">0.0008</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS5.p1.1.m1.4b"><apply id="S5.SS6.SSS5.p1.1.m1.4.4.3.cmml" xref="S5.SS6.SSS5.p1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS5.p1.1.m1.4.4.3a.cmml" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1"><eq id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.1"></eq><apply id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2"><times id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS5.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS6.SSS5.p1.1.m1.1.1">2</cn><cn id="S5.SS6.SSS5.p1.1.m1.2.2.cmml" type="integer" xref="S5.SS6.SSS5.p1.1.m1.2.2">28</cn></interval></apply><cn id="S5.SS6.SSS5.p1.1.m1.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS5.p1.1.m1.3.3.1.1.3">9.41</cn></apply><apply id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.cmml" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2"><eq id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.1.cmml" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.1"></eq><ci id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.2.cmml" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS5.p1.1.m1.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS5.p1.1.m1.4.4.2.2.3">0.0008</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS5.p1.1.m1.4c">F(2,28)=9.41,p=0.0008</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS5.p1.1.m1.4d">italic_F ( 2 , 28 ) = 9.41 , italic_p = 0.0008</annotation></semantics></math>, indicating that the cognitive load varied significantly depending on the task. Post hoc analysis using Tukey’s HSD test showed that cognitive load was significantly higher in the summarization task compared to the Q&amp;A task (meandiff = .779, p = 0.001) (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F14" title="Figure 14 ‣ 5.5. RQ4: How Do Task-Related Factors and Judge Strategy Impact User Perceptions? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">14</span></a>). There was no significant effect of judge strategy on cognitive load, <math alttext="F(1,14)=3.02,p=0.1044" class="ltx_Math" display="inline" id="S5.SS6.SSS5.p1.2.m2.4"><semantics id="S5.SS6.SSS5.p1.2.m2.4a"><mrow id="S5.SS6.SSS5.p1.2.m2.4.4.2" xref="S5.SS6.SSS5.p1.2.m2.4.4.3.cmml"><mrow id="S5.SS6.SSS5.p1.2.m2.3.3.1.1" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.cmml"><mrow id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.2" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.1" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.2" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS5.p1.2.m2.1.1" xref="S5.SS6.SSS5.p1.2.m2.1.1.cmml">1</mn><mo id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS5.p1.2.m2.2.2" xref="S5.SS6.SSS5.p1.2.m2.2.2.cmml">14</mn><mo id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.1" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.3" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.3.cmml">3.02</mn></mrow><mo id="S5.SS6.SSS5.p1.2.m2.4.4.2.3" xref="S5.SS6.SSS5.p1.2.m2.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS5.p1.2.m2.4.4.2.2" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.cmml"><mi id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.2" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.1" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.3" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.3.cmml">0.1044</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS5.p1.2.m2.4b"><apply id="S5.SS6.SSS5.p1.2.m2.4.4.3.cmml" xref="S5.SS6.SSS5.p1.2.m2.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS5.p1.2.m2.4.4.3a.cmml" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1"><eq id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.1.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.1"></eq><apply id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2"><times id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS5.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS6.SSS5.p1.2.m2.1.1">1</cn><cn id="S5.SS6.SSS5.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS6.SSS5.p1.2.m2.2.2">14</cn></interval></apply><cn id="S5.SS6.SSS5.p1.2.m2.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS5.p1.2.m2.3.3.1.1.3">3.02</cn></apply><apply id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.cmml" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2"><eq id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.1.cmml" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.1"></eq><ci id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.2.cmml" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS5.p1.2.m2.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS5.p1.2.m2.4.4.2.2.3">0.1044</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS5.p1.2.m2.4c">F(1,14)=3.02,p=0.1044</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS5.p1.2.m2.4d">italic_F ( 1 , 14 ) = 3.02 , italic_p = 0.1044</annotation></semantics></math>, and the interaction between task and judge strategy also did not reach significance, <math alttext="F(2,28)=2.23,p=0.126" class="ltx_Math" display="inline" id="S5.SS6.SSS5.p1.3.m3.4"><semantics id="S5.SS6.SSS5.p1.3.m3.4a"><mrow id="S5.SS6.SSS5.p1.3.m3.4.4.2" xref="S5.SS6.SSS5.p1.3.m3.4.4.3.cmml"><mrow id="S5.SS6.SSS5.p1.3.m3.3.3.1.1" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.cmml"><mrow id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.cmml"><mi id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.2" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.2.cmml">F</mi><mo id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.1" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.1.cmml">⁢</mo><mrow id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.2" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.1.cmml"><mo id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.2.1" stretchy="false" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.1.cmml">(</mo><mn id="S5.SS6.SSS5.p1.3.m3.1.1" xref="S5.SS6.SSS5.p1.3.m3.1.1.cmml">2</mn><mo id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.2.2" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.1.cmml">,</mo><mn id="S5.SS6.SSS5.p1.3.m3.2.2" xref="S5.SS6.SSS5.p1.3.m3.2.2.cmml">28</mn><mo id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.2.3" stretchy="false" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.1" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.3" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.3.cmml">2.23</mn></mrow><mo id="S5.SS6.SSS5.p1.3.m3.4.4.2.3" xref="S5.SS6.SSS5.p1.3.m3.4.4.3a.cmml">,</mo><mrow id="S5.SS6.SSS5.p1.3.m3.4.4.2.2" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.cmml"><mi id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.2" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.2.cmml">p</mi><mo id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.1" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.1.cmml">=</mo><mn id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.3" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.3.cmml">0.126</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS5.p1.3.m3.4b"><apply id="S5.SS6.SSS5.p1.3.m3.4.4.3.cmml" xref="S5.SS6.SSS5.p1.3.m3.4.4.2"><csymbol cd="ambiguous" id="S5.SS6.SSS5.p1.3.m3.4.4.3a.cmml" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1"><eq id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.1.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.1"></eq><apply id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2"><times id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.1.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.1"></times><ci id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.2.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.2">𝐹</ci><interval closure="open" id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.1.cmml" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.2.3.2"><cn id="S5.SS6.SSS5.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS6.SSS5.p1.3.m3.1.1">2</cn><cn id="S5.SS6.SSS5.p1.3.m3.2.2.cmml" type="integer" xref="S5.SS6.SSS5.p1.3.m3.2.2">28</cn></interval></apply><cn id="S5.SS6.SSS5.p1.3.m3.3.3.1.1.3.cmml" type="float" xref="S5.SS6.SSS5.p1.3.m3.3.3.1.1.3">2.23</cn></apply><apply id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.cmml" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2"><eq id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.1.cmml" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.1"></eq><ci id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.2.cmml" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.2">𝑝</ci><cn id="S5.SS6.SSS5.p1.3.m3.4.4.2.2.3.cmml" type="float" xref="S5.SS6.SSS5.p1.3.m3.4.4.2.2.3">0.126</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS5.p1.3.m3.4c">F(2,28)=2.23,p=0.126</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS5.p1.3.m3.4d">italic_F ( 2 , 28 ) = 2.23 , italic_p = 0.126</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7. </span>RQ5: What do practitioners prefer: Direct Assessment or Pairwise?</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1">Participants were asked to reflect on whether they preferred direct assessment via creating rubrics and whether they preferred pairwise, or whether it was contextual. Participant responses ranged with almost half (7 participants) reporting that they preferred direct assessment, and the remaining being split between pairwise and saying it depends/its contextual. The overview of preference reasons can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#S5.F16" title="Figure 16 ‣ 5.7. RQ5: What do practitioners prefer: Direct Assessment or Pairwise? ‣ 5. Results ‣ Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences"><span class="ltx_text ltx_ref_tag">16</span></a>. Below we report why participants preferred one judge strategy over another.</p>
</div>
<figure class="ltx_figure" id="S5.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F16.fig1" style="width:290.5pt;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.F16.fig1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.F16.fig1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.F16.fig1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.1.1.1.1">
<span class="ltx_p" id="S5.F16.fig1.1.1.1.1.1.1" style="width:34.1pt;"><span class="ltx_text ltx_font_bold" id="S5.F16.fig1.1.1.1.1.1.1.1" style="font-size:80%;">Preference</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.F16.fig1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.1.1.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.1.1.2.1.1" style="width:79.7pt;"><span class="ltx_text ltx_font_bold" id="S5.F16.fig1.1.1.1.2.1.1.1" style="font-size:80%;">Code</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S5.F16.fig1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.1.1.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.1.1.3.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="S5.F16.fig1.1.1.1.3.1.1.1" style="font-size:80%;">Definition of the Code</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F16.fig1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.2.1.1.1">
<span class="ltx_p" id="S5.F16.fig1.1.2.1.1.1.1" style="width:34.1pt;"><span class="ltx_text" id="S5.F16.fig1.1.2.1.1.1.1.1" style="font-size:80%;">Direct</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.2.1.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.2.1.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.2.1.2.1.1.1" style="font-size:80%;">Clarity and Control</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.F16.fig1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.2.1.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.2.1.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.2.1.3.1.1.1" style="font-size:80%;">Clearer and easier to understand, offering more control over the evaluation process and outcomes.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.F16.fig1.1.3.2">
<td class="ltx_td ltx_align_top ltx_border_r" id="S5.F16.fig1.1.3.2.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.F16.fig1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.3.2.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.3.2.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.3.2.2.1.1.1" style="font-size:80%;">Detailed Evaluation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.F16.fig1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.3.2.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.3.2.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.3.2.3.1.1.1" style="font-size:80%;">Allows individual evaluation of responses, making it more useful for tasks requiring thorough review.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.F16.fig1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.4.3.1.1">
<span class="ltx_p" id="S5.F16.fig1.1.4.3.1.1.1" style="width:34.1pt;"><span class="ltx_text" id="S5.F16.fig1.1.4.3.1.1.1.1" style="font-size:80%;">Pairwise</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.4.3.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.4.3.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.4.3.2.1.1.1" style="font-size:80%;">Flexibility and Nuance</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.F16.fig1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.4.3.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.4.3.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.4.3.3.1.1.1" style="font-size:80%;">Provides flexibility and nuance in evaluating abstract or subjective tasks.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.F16.fig1.1.5.4">
<td class="ltx_td ltx_align_top ltx_border_r" id="S5.F16.fig1.1.5.4.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.F16.fig1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.5.4.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.5.4.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.5.4.2.1.1.1" style="font-size:80%;">Ease of Criteria Formulation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.F16.fig1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.5.4.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.5.4.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.5.4.3.1.1.1" style="font-size:80%;">Easier to create evaluation criteria, especially when rubrics are harder to define.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.F16.fig1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.6.5.1.1">
<span class="ltx_p" id="S5.F16.fig1.1.6.5.1.1.1" style="width:34.1pt;"><span class="ltx_text" id="S5.F16.fig1.1.6.5.1.1.1.1" style="font-size:80%;">Neither</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.F16.fig1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.6.5.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.6.5.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.6.5.2.1.1.1" style="font-size:80%;">Combination Approach</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.F16.fig1.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.6.5.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.6.5.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.6.5.3.1.1.1" style="font-size:80%;">A combination of both methods works better for complex tasks, complementing each other through classification (rubric) and ranking (pairwise).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.F16.fig1.1.7.6">
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_r" id="S5.F16.fig1.1.7.6.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.F16.fig1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.7.6.2.1">
<span class="ltx_p" id="S5.F16.fig1.1.7.6.2.1.1" style="width:79.7pt;"><span class="ltx_text" id="S5.F16.fig1.1.7.6.2.1.1.1" style="font-size:80%;">Task-Specific Use Cases</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S5.F16.fig1.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.F16.fig1.1.7.6.3.1">
<span class="ltx_p" id="S5.F16.fig1.1.7.6.3.1.1" style="width:142.3pt;"><span class="ltx_text" id="S5.F16.fig1.1.7.6.3.1.1.1" style="font-size:80%;">Direct Assessment is suited for classification or compliance, while Pairwise is better for identifying preferences or the best result.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F16.fig1.4.1.1" style="font-size:113%;">Figure 15</span>. </span><span class="ltx_text" id="S5.F16.fig1.5.2" style="font-size:113%;">Codes and Reasons for Assessment Preferences</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F16.1" style="width:121.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="609" id="S5.F16.1.g1" src="extracted/5893543/figures/pie4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F16.1.1.1.1" style="font-size:90%;">Figure 16</span>. </span><span class="ltx_text" id="S5.F16.1.2.2" style="font-size:90%;">Assessment Preferences</span></figcaption>
</figure>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S5.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.1. </span>Preferred Direct Assessment</h4>
<div class="ltx_para" id="S5.SS7.SSS1.p1">
<p class="ltx_p" id="S5.SS7.SSS1.p1.1">Participants felt that Direct Assessment gave them more control over how the model outputs were evaluated and appreciated the detailed evaluation of the responses.</p>
</div>
<div class="ltx_para" id="S5.SS7.SSS1.p2">
<blockquote class="ltx_quote" id="S5.SS7.SSS1.p2.1">
<p class="ltx_p" id="S5.SS7.SSS1.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS1.p2.1.1.1">Direct Assessment is more clear task for me to understand the results. Sometime ranks provided by Pairwise Assessment can be set in different order and still be correct.</span> #P1</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS7.SSS1.p3">
<blockquote class="ltx_quote" id="S5.SS7.SSS1.p3.1">
<p class="ltx_p" id="S5.SS7.SSS1.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS1.p3.1.1.1">I prefer to create direct assessments because I feel that I can control more the outcomes of the model.</span> #P14</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS7.SSS1.p4">
<blockquote class="ltx_quote" id="S5.SS7.SSS1.p4.1">
<p class="ltx_p" id="S5.SS7.SSS1.p4.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS1.p4.1.1.1">I would prefer the direct assessment mainly as it involves evaluating each response individually. Rankings are just relative preferences and they do not offer a detailed way of evaluating the model response. As an example in if we have all bad outputs, we would still have some ordering among them but that is essentially irrelevant to judge the response of the model which is not the case in rubric based criteria.</span> #P3</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.2. </span>Preferred Pairwise Assessment</h4>
<div class="ltx_para" id="S5.SS7.SSS2.p1">
<p class="ltx_p" id="S5.SS7.SSS2.p1.1">Of the 15 participants in the study, 4 reported preferring pairwise assessment over direct assessment. The reasons included that participants felt that pairwise assessment allowed for more flexibility and nuance in evaluating subjective tasks where a binary or rigid criteria may not apply. Others reported that the pairwise was simpler and easier to user when formulating evaluation criteria, since detailed rubrics may be more complex.</p>
</div>
<div class="ltx_para" id="S5.SS7.SSS2.p2">
<blockquote class="ltx_quote" id="S5.SS7.SSS2.p2.1">
<p class="ltx_p" id="S5.SS7.SSS2.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS2.p2.1.1.1">”Pairwise is easier to use for crafting an evaluation criteria, especially for the summarization task where the ’goodness’ of a summary is very abstract and can be broad.”</span> #P7</p>
</blockquote>
<blockquote class="ltx_quote" id="S5.SS7.SSS2.p2.2">
<p class="ltx_p" id="S5.SS7.SSS2.p2.2.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS2.p2.2.1.1">It’s easier to formulate a sentence containing the complete criteria and not have to think about rubrics.</span> #P5</p>
</blockquote>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS7.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.7.3. </span>Use of AI Judge Strategy is Contextual</h4>
<div class="ltx_para" id="S5.SS7.SSS3.p1">
<p class="ltx_p" id="S5.SS7.SSS3.p1.1">Participants who preferred neither judge strategy reported that they would either like to use a combination of strategies to accomplish their task, or that their preference would depend on the nature of the task.</p>
</div>
<div class="ltx_para" id="S5.SS7.SSS3.p2">
<blockquote class="ltx_quote" id="S5.SS7.SSS3.p2.1">
<p class="ltx_p" id="S5.SS7.SSS3.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS3.p2.1.1.1">Various tasks are of various complexities, I feel that ranking and classification results coming out of the pairwise and direct assessment can help the user make better decisions which are both relative and independent.</span> #P4</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.SS7.SSS3.p3">
<blockquote class="ltx_quote" id="S5.SS7.SSS3.p3.1">
<p class="ltx_p" id="S5.SS7.SSS3.p3.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS7.SSS3.p3.1.1.1">I believe they have different use cases. Rubric can be used in a case of classification or compliance for example. I might want to consider all answers and see what classifications I can assign to them. Pairwise is about getting the best result for what you want. I don’t care as much about the others, only that one is the correct.</span> #P8</p>
</blockquote>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Variability in Subjective Criteria: Defining Stakeholder Needs in AI-assisted Evaluation</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We observed significant variation in how participants defined criteria within tasks, a finding consistent with prior HCI research on diversity in subjective judgments <cite class="ltx_cite ltx_citemacro_citep">(Karapanos et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib23" title="">2009</a>)</cite>. For example, ”inclusivity” had different interpretations among participants. Some participants believed it meant mentioning every possible holiday to represent all religions, while others thought it meant avoiding specific cultural or holiday references altogether. This aligns with studies in inclusive design, where users with diverse backgrounds interpret fairness and inclusivity differently, highlighting the challenges of capturing subjective values in AI systems <cite class="ltx_cite ltx_citemacro_citep">(Costanza-Chock, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib11" title="">2020</a>)</cite>. This variation underscores the importance of clear stakeholder definitions for criteria definitions, a need echoed in crowdsourcing research, which shows that subjective tasks can lead to inconsistent evaluations without precise guidelines <cite class="ltx_cite ltx_citemacro_citep">(Kittur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib27" title="">2013</a>)</cite>. When developing criteria to evaluate model outputs, it is essential to define stakeholder needs clearly, especially for subjective criteria like inclusivity.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Mitigating Over-Specificity in AI-Assisted Evaluation: Balancing Task Context and Generalization</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">When analyzing how participants adjusted their criteria to align with the AI’s judgments, we observed a consistent tendency to make the criteria increasingly specific to the task at hand. This resulted in overly narrow criteria that worked well for evaluating a single article’s summary but failed to generalize across multiple summaries from different articles. Other AI-assisted evaluation systems <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib26" title="">2023b</a>; Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib39" title="">2024</a>)</cite> do not necessarily prioritize sampling diverse task contexts and outputs, especially when the evaluation heavily depends on prompts. This limits the user’s exposure to varied outputs, increasing the risk of overspecifying criteria to a particular task. To address this issue, it is critical to expose users to diverse task contexts. For instance, in the case of article summarization, this would mean providing users with varied articles and summaries to encourage the development of criteria that can be applied more broadly. A designimprovement could involve enabling users to upload large datasets, with the system recommending or allowing the selection of diverse samples to help develop more robust criteria.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">Conversely, we found that a subset of users kept constructs very general. For example they would rely on the AI evaluator to interpret what it means to be “inclusive” or what it means to be grounded in the document. This lack of specificity can also lead to subpar results. Striking a balance between overly specific and overly vague criteria is essential. Encouraging users to refine their criteria while keeping them general enough for broad application can improve quality. Paired with exposure to diverse outputs, this approach can create more effective evaluation processes that work well across different contexts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Challenges of Natural Language Criteria Formulation</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">When users expressed their criteria in natural language, we observed significant variation in how they structured their inputs, particularly in the pairwise condition where no predefined form or structure was provided. Some participants introduced additional instructions, asking the AI evaluator to consider each response independently, which contradicts the purpose of pairwise comparisons where outputs are meant to be directly compared. Others created complex rule-based systems, such as: <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS2.p1.1.1">If the response includes [insert characteristic], then it is “good”; if it does not, then it is “bad”.</span> Additionally, rather than defining item scales as instructed, participants often provided examples of ”good” and ”bad” responses. These alternative interaction patterns did not align with the intended functionality of the evaluation process and led to suboptimal results. To address this, greater transparency should be provided regarding the prompts used in both direct and pairwise evaluation conditions, ensuring users have a clearer understanding of how their input will be processed. Providing more examples that users can customize to fit their specific needs could also improve outcomes, as well as implementing auto-correction features to guide users in entering the criteria correctly.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Alignment: Refining Criteria, Changing the AI Evaluator, and Altering Expected Results</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We observed various user interactions throughout the evaluation process that mirror principles seen in interactive machine learning (iML), where users iteratively refine models by selecting examples, labeling data, and evaluating outputs <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib35" title="">2010</a>; Simard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib40" title="">2014</a>)</cite>. In line with iML approaches, users in our study adjusted their criteria throughout the evaluation, making them more general or specific as needed. This adaptive behavior reflects the need for flexible interfaces that allow seamless transitions between specifying, revising, and evaluating criteria. As noted in iML, reducing user effort while improving output alignment is crucial <cite class="ltx_cite ltx_citemacro_citep">(Desmond et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00873v1#bib.bib13" title="">2022</a>)</cite>. In some cases, midway through the evaluation, users even changed their expected outcomes to align more closely with the AI’s judgments—especially after reading the explanations, which often convinced them of the AI’s output. Users frequently revised their criteria, whether by adding or removing scales in the rubric condition, or rephrasing their criteria in the pairwise condition. They also experimented with different AI evaluators, switching between models like Mixtral and LLaMA to compare results. This suggests that the path to ”alignment” can take many forms, from adjusting one’s own expectations, to switching the AI model making the judgments, to modifying the criteria itself in search of a better fit. This process could be better supported in the future by automatically testing criteria with multiple models and showing results side-by-side.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Adaptive Evaluation Strategies: Balancing Clarity and Flexibility</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Participants expressed a preference for Direct Assessment due to the clarity and control it offered, allowing for detailed, individualized evaluations. On the other hand, Pairwise Assessment was valued for its flexibility, particularly in more subjective tasks. Some participants preferred a hybrid approach, selecting an evaluation method based on the specific task. The higher number of evaluations in direct assessment suggests that users felt more engaged, likely because it provided them with a greater sense of control. This aligns with the reasons participants cited for favoring direct assessment. These findings highlight the importance of adaptable evaluation strategies tailored to task type. AI systems could benefit from offering multiple evaluation modes or an adaptive assessment strategy that adjusts based on task complexity. For example, users might opt for direct assessment for objective tasks and switch to pairwise ranking for more subjective ones.
Additionally, some users might prefer a combination approach—starting with direct assessment to filter out responses and then using pairwise ranking to further refine the best options. Future research can explore hybrid systems that adapt dynamically based on user workflows.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Bias Awareness and Explanation Visibility in Evaluation Strategies</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Positional bias was considered helpful when present, prompting participants to revise and improve their criteria. This result suggests that highlighting AI biases can enhance the evaluation process and improve alignment between human and AI judgments. While our study focused on positional bias, there is an opportunity to explore the representation of other biases, such as self-enhancement bias and verbosity bias. For example, is the model ranking outputs highly because they were generated by the AI evaluator, or is it favoring longer responses even if they are less accurate or clear? Research is lacking on how users perceive such bias flagging and how it impacts their criteria refinement.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Explanations were perceived as more helpful in the direct assessment condition. One reason, as reported by participants, is that in the pairwise condition, explanations often went unnoticed. Since every comparison generates an explanation, these explanations were only accessible via a modal pop-up, requiring users to click on the result. Even then, users had to sift through multiple explanations to understand the ranking. This limitation in the pairwise condition presents an opportunity to redesign how explanations are presented, making them more concise and digestible. When explanations were more visible, as in the direct assessment condition, they proved to be more effective, suggesting a need for improvement in how pairwise explanations are displayed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We introduced <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.1">EvalAssist</span> a tool designed to help practitioners refine evaluation criteria using both direct and pairwise assessment strategies. The tool provides positional bias metrics and explanations for each AI judgment. In a controlled experiment with 15 machine learning practitioners, we examined how users refine their criteria and identified key differences between the two evaluation approaches. Direct Assessment was preferred for its clarity and control, while Pairwise Assessment was valued for flexibility in subjective tasks. Users often refined their criteria by increasing specificity or simplifying scales to improve alignment with task needs. Human-AI alignment was stronger in the rubric-based (Direct Assessment) condition, highlighting the importance of clear evaluation criteria. Our findings suggest that AI-assisted evaluation systems should offer flexible, adaptive strategies, allowing users to switch between direct and pairwise methods. Such systems should support ongoing criteria refinement, provide clear explanations, and improve alignment between human and AI judgments to enhance evaluation reliability.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock">Claude 3.5 sonnet.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-3-5-sonnet" title="">https://www.anthropic.com/news/claude-3-5-sonnet</a>
</span>
<span class="ltx_bibblock">Accessed: 2024-09-09.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arawjo et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and Elena L Glassman. 2024.

</span>
<span class="ltx_bibblock">ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing.

</span>
<span class="ltx_bibblock">, 18 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2024.

</span>
<span class="ltx_bibblock">Benchmarking foundation models with language-model-as-an-examiner.

</span>
<span class="ltx_bibblock">, 26 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bavaresco et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, E. Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, and Alberto Testoni. 2024.

</span>
<span class="ltx_bibblock">LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.18403" title="">https://arxiv.org/abs/2406.18403</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buçinca et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zana Buçinca, Phoebe Lin, Krzysztof Z Gajos, and Elena L Glassman. 2020.

</span>
<span class="ltx_bibblock">Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems.

</span>
<span class="ltx_bibblock">, 454–464 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024.

</span>
<span class="ltx_bibblock">Humans or llms as the judge? a study on judgement biases.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang and Lee (2023a)</span>
<span class="ltx_bibblock">
Cheng-Han Chiang and Hung-yi Lee. 2023a.

</span>
<span class="ltx_bibblock">Can large language models be an alternative to human evaluations?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang and Lee (2023b)</span>
<span class="ltx_bibblock">
Cheng-Han Chiang and Hung-yi Lee. 2023b.

</span>
<span class="ltx_bibblock">A closer look into using large language models for automatic evaluation.

</span>
<span class="ltx_bibblock">, 8928–8942 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
2022.

</span>
<span class="ltx_bibblock">Scaling Instruction-Finetuned Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/ARXIV.2210.11416" title="">https://doi.org/10.48550/ARXIV.2210.11416</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costanza-Chock (2020)</span>
<span class="ltx_bibblock">
Sasha Costanza-Chock. 2020.

</span>
<span class="ltx_bibblock">Design justice: Community-led practices to build the worlds we need.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desmond et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Michael Desmond, Zahra Ashktorab, Qian Pan, Casey Dugan, and James M Johnson. 2024.

</span>
<span class="ltx_bibblock">EvaluLLM: LLM assisted evaluation of generative outputs.

</span>
<span class="ltx_bibblock">, 30–32 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desmond et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michael Desmond, Michelle Brachman, Evelyn Duesterwald, Casey Dugan, Narendra Nath Joshi, Qian Pan, and Carolina Spina. 2022.

</span>
<span class="ltx_bibblock">AI Assisted Data Labeling with Interactive Auto Label.

</span>
<span class="ltx_bibblock">, 13161–13163 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.

</span>
<span class="ltx_bibblock">Summeval: Re-evaluating summarization evaluation.

</span>
<span class="ltx_bibblock">, 391–409 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floridi and Chiriatti (2020)</span>
<span class="ltx_bibblock">
Luciano Floridi and Massimo Chiriatti. 2020.

</span>
<span class="ltx_bibblock">GPT-3: Its nature, scope, limits, and consequences.

</span>
<span class="ltx_bibblock">, 681–694 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehrmann et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2023.

</span>
<span class="ltx_bibblock">Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text.

</span>
<span class="ltx_bibblock">, 103–166 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazal et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Ahmad Ghazal, Tilmann Rabl, Minqing Hu, Francois Raab, Meikel Poess, Alain Crolotte, and Hans-Arno Jacobsen. 2013.

</span>
<span class="ltx_bibblock">Bigbench: Towards an industry standard benchmark for big data analytics.

</span>
<span class="ltx_bibblock">, 1197–1208 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hart (2006)</span>
<span class="ltx_bibblock">
Sandra G Hart. 2006.

</span>
<span class="ltx_bibblock">NASA-task load index (NASA-TLX); 20 years later.

</span>
<span class="ltx_bibblock">, 904–908 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024.

</span>
<span class="ltx_bibblock">An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib21.3.3.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al<span class="ltx_text" id="bib.bib21.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahng et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, and Lucas Dixon. 2024.

</span>
<span class="ltx_bibblock">Llm comparator: Visual analytics for side-by-side evaluation of large language models.

</span>
<span class="ltx_bibblock">, 7 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karapanos et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Evangelos Karapanos, Jean-Bernard Martens, and Marc Hassenzahl. 2009.

</span>
<span class="ltx_bibblock">Accounting for diversity in subjective judgments.

</span>
<span class="ltx_bibblock">, 639–648 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib24.3.3.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al<span class="ltx_text" id="bib.bib24.4.1">.</span> 2023c.

</span>
<span class="ltx_bibblock">Prometheus: Inducing fine-grained evaluation capability in language models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 2023a.

</span>
<span class="ltx_bibblock">Evallm: Interactive evaluation of large language model prompts on user-defined criteria.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 2023b.

</span>
<span class="ltx_bibblock">Evallm: Interactive evaluation of large language model prompts on user-defined criteria.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kittur et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Aniket Kittur, Jeffrey V Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron Shaw, John Zimmerman, Matt Lease, and John Horton. 2013.

</span>
<span class="ltx_bibblock">The future of crowd work.

</span>
<span class="ltx_bibblock">, 1301–1318 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib28.3.3.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib28.4.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">, 9459–9474 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Margaret Li, Jason Weston, and Stephen Roller. 2019.

</span>
<span class="ltx_bibblock">ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1909.03087 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1909.03087" title="">https://arxiv.org/abs/1909.03087</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib31.3.3.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al<span class="ltx_text" id="bib.bib31.4.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock">G-eval: Nlg evaluation using gpt-4 with better human alignment.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madsen et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Andreas Madsen, Sarath Chandar, and Siva Reddy. 2024.

</span>
<span class="ltx_bibblock">Are self-explanations from Large Language Models faithful?

</span>
<span class="ltx_bibblock">, 295–337 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, and Werner Geyer. 2024.

</span>
<span class="ltx_bibblock">Human-Centered Design Recommendations for LLM-as-a-Judge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Kayur Patel, Naomi Bancroft, Steven M Drucker, James Fogarty, Amy J Ko, and James Landay. 2010.

</span>
<span class="ltx_bibblock">Gestalt: integrated support for implementation and analysis in machine learning.

</span>
<span class="ltx_bibblock">, 37–46 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poursabzi-Sangdeh et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, and Hanna Wallach. 2021.

</span>
<span class="ltx_bibblock">Manipulating and measuring model interpretability.

</span>
<span class="ltx_bibblock">, 52 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raju et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakkar. 2024.

</span>
<span class="ltx_bibblock">Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al<span class="ltx_text" id="bib.bib38.3.3.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al<span class="ltx_text" id="bib.bib38.4.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya G Parameswaran, and Ian Arawjo. 2024.

</span>
<span class="ltx_bibblock">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simard et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Patrice Simard, David Chickering, Aparna Lakshmiratan, Denis Charles, Léon Bottou, Carlos Garcia Jurado Suarez, David Grangier, Saleema Amershi, Johan Verwey, and Jina Suh. 2014.

</span>
<span class="ltx_bibblock">Ice: enabling non-experts to build models interactively for large-scale lopsided problems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomas (2006)</span>
<span class="ltx_bibblock">
David R Thomas. 2006.

</span>
<span class="ltx_bibblock">A general inductive approach for analyzing qualitative evaluation data.

</span>
<span class="ltx_bibblock">, 237–246 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib42.3.3.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib42.4.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">, 24824–24837 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024.

</span>
<span class="ltx_bibblock">Evaluation of Retrieval-Augmented Generation: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib44.3.3.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al<span class="ltx_text" id="bib.bib44.4.1">.</span> 2024a.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024b.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-judge with MT-bench and Chatbot Arena.

</span>
<span class="ltx_bibblock">, 29 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 17:07:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
