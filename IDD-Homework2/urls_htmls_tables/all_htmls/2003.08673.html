<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.08673] Survey of Personalization Techniques for Federated Learning</title><meta property="og:description" content="Federated learning enables machine learning models to learn from private decentralized data without compromising privacy. The standard formulation of federated learning produces one shared model for all clients. Statis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Survey of Personalization Techniques for Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Survey of Personalization Techniques for Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.08673">

<!--Generated on Sun Mar 17 08:29:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Survey of Personalization Techniques for Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Viraj Kulkarni<sup id="id6.2.id1" class="ltx_sup"><span id="id6.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Milind Kulkarni<sup id="id7.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aniruddha Pant<sup id="id8.4.id1" class="ltx_sup">2</sup>
<sup id="id9.5.id2" class="ltx_sup">1</sup>Vishwakarma University
<br class="ltx_break"><sup id="id10.6.id3" class="ltx_sup">2</sup>DeepTek Inc
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p"><span id="id11.id1.1" class="ltx_text">Federated learning enables machine learning models to learn from private decentralized data without compromising privacy. The standard formulation of federated learning produces one shared model for all clients. Statistical heterogeneity due to non-IID distribution of data across devices often leads to scenarios where, for some clients, the local models trained solely on their private data perform better than the global shared model thus taking away their incentive to participate in the process. Several techniques have been proposed to personalize global models to work better for individual clients. This paper highlights the need for personalization and surveys recent research on this topic.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many datasets are inherently decentralized in nature and are distributed across multiple devices owned by different users. Traditional machine learning settings involve aggregating data samples from these users into a central repository and training a machine learning model on it. This movement of data from local devices to a central repository poses two key challenges. Firstly, it compromises the privacy and security of the data. Policies such as the General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">voigt2017eu </a></cite> and Health Insurance Portability and Accountability Act (HIPAA) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">annas2003hipaa </a></cite> stipulate provisions that make such movement difficult. Secondly, it imposes communication overheads which, depending on the setting, may be prohibitively expensive.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">mcmahan2016communication </a></cite> is a framework that enables multiple users known as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">clients</span> to collaboratively train a shared global model on their collective data without moving the data from their local devices. A central server orchestrates the federated learning process which consists of multiple rounds. At the beginning of each round, the server sends the current global model to the participating clients. Each client trains the model on its local data and communicates only the model updates back to the server. The server collects these updates from all clients and makes a single update to the global model thereby concluding the round. By removing the need to aggregate all data on a single device, federated learning overcomes the privacy and communication challenges mentioned above and allows machine learning models to learn on decentralized data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Federated learning has found numerous practical applications where data is decentralized and privacy is important. For example, it has exhibited good performance and robustness for the problem of next-word-prediction on mobile devices <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">hard2018federated </a></cite>. Bonawitz et. al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">bonawitz2019towards </a></cite> propose a scalable system implementing large-scale federated learning for mobile devices. Kairouz et. al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">kairouz2019advances </a></cite> discuss broad challenges and open problems in the field.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The primary incentive for clients to participate in federated learning is obtaining better models. Clients who have insufficient private data to develop accurate local models stand to benefit the most from collaboratively learned models. However, the benefit of participating in federated learning for clients who have sufficient private data to train accurate local models is disputable. Yu et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">yu2020salvaging </a></cite> show that, for many tasks, some participants may gain no benefit by participating since the global shared model is less accurate than the local models they can train on their own. Hanzely et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">hanzely2020federated </a></cite> question the utility of a global model that is too far removed from the typical usage of a user. The distribution of data across clients is highly non-IID for many applications. This statistical heterogeneity makes it difficult to train a single model that will work well for all clients. The purpose of this paper is to survey recent research regarding building personalized models for clients in a federated learning setting that are expected to work better than the global shared model or the local individual models.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Need for Personalization</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Wu et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">wu2020personalized </a></cite> list three challenges faced by federated learning systems related to personalization: (1) device heterogeneity in terms of storage, computation, and communication capabilities; (2) data heterogeneity arising due to non-IID distribution of data; (3) model heterogeneity arising from situations where different clients need models specifically customized to their environment. As an example of model heterogeneity, consider the sentence: “I live in …..”. The next-word-prediction task applied on this sentence needs to predict a different answer customized for each user. If heterogeneity does not exist in the data, it may exist in the labels; different clients may assign different labels to the same data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the original federated learning design of McMahan et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">mcmahan2016communication </a></cite>, the model updates and the final model can leak participant data violating privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">shokri2017membership </a></cite> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">melis2019exploiting </a></cite>. To preserve privacy, McMahan et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">mcmahan2017learning </a></cite> propose differential privacy techniques that limit the information the global model can reveal about individual participants. However, Yu et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">yu2020salvaging </a></cite> argue that such privacy protection mechanisms introduce a fundamental conflict between protecting privacy and achieving higher performance for individual users. Bagdasaryan et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">bagdasaryan2019differential </a></cite> state that the cost of differential privacy mechanisms is the reduction in accuracy, and this cost is borne unequally by clients with the underrepresented or tail participants being affected the worst.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Personalization of the global model becomes necessary to handle the challenges posed by statistical heterogeneity and non-IID distribution of data. Most techniques for personalization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">sim2019investigation </a></cite> generally involve two discrete steps. In the first step, a global model is built in a collaborative fashion. In the second step, the global model is personalized for each client using the client’s private data. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">jiang2019improving </a></cite> argue that optimizing solely for global accuracy yields models that are harder to personalize and propose that, in order to make federated learning personalization useful in practice, the three following objectives must all be addressed <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">simultaneously</span> and not independently: (1) developing improved personalized models that benefit a large majority of clients; (2) developing an accurate global model that benefits clients who have limited private data for personalization; (3) attaining fast model convergence in a small number of training rounds. Out of the local data samples stored with each client, it may happen that only a subset of samples are relevant for a particular task, while the irrelevant samples adversely affect the model training. Tuor et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">tuor2020data </a></cite> propose a method where a relevance model built on a small benchmark set is used to separate relevant and irrelevant samples at each client, and only the relevant samples are used in the federated learning process.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Techniques</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section surveys different methods for adapting global models for individual clients.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Adding User Context</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Before presenting methods to personalize a global model for individual clients, we take a moment to point out that a shared global model can also generate highly personalized predictions if the client’s context and personal information is suitably featurized and incorporated in the dataset. However, most public datasets do not contain contextual features, and developing techniques to effectively incorporate context remains an important open problem that has great potential to improve the utility of federated learning models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">kairouz2019advances </a></cite>. It also remains to be studied if such context featurization can be performed without adversely affecting privacy. As an intermediate approach between a single global model and purely local models, Masour et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">mansour2020three </a></cite> suggest <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">user clustering</span> where similar clients are grouped together and a separate model is trained for each group.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transfer Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Transfer learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">pratt1993discriminability </a></cite> enables deep learning models to utilize the knowledge gained in solving one problem to solve another related problem. Schneider and Vlachos <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">schneider2019mass </a></cite> discuss using transfer learning to achieve model personalization in non-federated settings. Transfer learning has also been used in federated settings, e.g. Wang et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref">wang2019federated </a></cite>, where some or all parameters of a trained global model are re-learned on local data. A learning-theoretic framework with generalization guarantees is provided in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">mansour2020three </a></cite>. By using the parameters of the trained global model to initialize training on local data, transfer learning is able to take advantage of the knowledge extracted by the global model instead of learning it from scratch. To avoid the problem of catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">mccloskey1989catastrophic </a></cite> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">french1999catastrophic </a></cite>, care must be taken to not retrain the model for too long on local data. A variant technique freezes the base layers of the global model and retrains only the top layers on local data. Transfer learning is also known as fine-tuning, and it integrates well into the typical federated learning lifecycle.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-task Learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In multi-task learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">caruana1997multitask </a></cite>, multiple related tasks are solved simultaneously allowing the model to exploit commonalities and differences across the tasks by learning them jointly. Smith et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">smith2017federated </a></cite> show that multi-task learning is a natural choice to build personalized federated models and develop the MOCHA algorithm for multi-task learning in the federated setting that tackles challenges related to communication, stragglers, and fault tolerance. One drawback of using multi-task learning in federated settings is that since it produces one model per task, it is essential that all clients participate in every round.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Meta-Learning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Meta-learning involves training on multiple learning tasks to generate highly-adaptable models that can further learn to solve new tasks with only a small number of training examples. Finn et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">finn2017model </a></cite> propose a model-agnostic meta-learning (MAML) algorithm that is compatible with any model that is trained using gradient descent. MAML builds an internal representation generally suitable for multiple tasks, so that fine tuning the top layers for a new task can produce good results. MAML proceeds in two connected stages: meta-training and meta-testing. Meta-training builds the global model on multiple tasks, and meta-testing adapts the global model individually for separate tasks.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Jiang et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">jiang2019improving </a></cite> point out that if we consider the federated learning process as meta-training and the personalization process as meta-testing, then Federated Averaging <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">mcmahan2016communication </a></cite> is very similar to Reptile <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">nichol2018first </a></cite>, a popular MAML algorithm. The authors also make the observation that careful fine-tuning can produce a global model with high accuracy that can be easily personalized, but naively optimizing for global accuracy can hurt the model’s ability for subsequent personalization. While other personalization approaches for federated learning treat development of the global model and its personalization as two distinct activities, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">jiang2019improving </a></cite> propose a modification to the Federated Averaging algorithm that allows both to be addressed simultaneously resulting in better personalized models.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">A new formulation of the standard federated learning problem proposed by Fallah et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">fallah2020personalized </a></cite> incorporates MAML and seeks to find a global model which performs well <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_italic">after</span> each user updates it with respect to its own loss function. In addition, they propose Per-FedAvg, a personalized variant of Federated Averaging, to solve the above-mentioned formulation. Khodak et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">khodak2019adaptive </a></cite> propose ARUBA, a meta-learning algorithm inspired by online convex optimization, and demonstrate an improvement in performance by applying it to Federated Averaging. Chen et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">chen2018federated </a></cite> present a federated meta-learning framework for building personalized recommendation models where both the algorithm and the model are parameterized and need to be optimized.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Knowledge Distillation</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Caruana et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">caruana1997multitask </a></cite> have demonstrated that it is possible to compress the knowledge of an ensemble of models into a single model which is easier to deploy. Knowledge distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">hinton2015distilling </a></cite> further develops this idea and involves extracting the knowledge of a large <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_italic">teacher</span> network into a smaller <span id="S3.SS5.p1.1.2" class="ltx_text ltx_font_italic">student</span> network by having the student mimic the teacher. Overfitting poses a significant challenge during personalization, especially for clients whose local dataset is small. Yu et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">yu2020salvaging </a></cite> propose that by treating the global federated model as the teacher and the personalized model as the student, the effects of overfitting during personalization can be mitigated. Li et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">li2019fedmd </a></cite> propose FedMD, a federated learning framework based on knowledge distillation and transfer learning that allows clients to independently design their own networks using their local private datasets and a global public dataset.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Base + Personalization Layers</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">In typical federated learning scenarios, data distribution varies greatly across participating devices. To temper the adverse effects of this statistical heterogeneity, Arivazhagan et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">arivazhagan2019federated </a></cite> propose FedPer, a neural network architecture where the base layers are trained centrally by Federated Averaging, and the top layers (also called personalization layers) are trained locally with a variant of gradient descent. As opposed to transfer learning where all the layers are first trained on global data and then all or some layers are retrained on local data, FedPer separately trains the base layers on global data and the personalization layers on local data.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Mixture of Global and Local Models</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">The standard formulation of federated learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">mcmahan2016communication </a></cite> is designed to find a single global model trained on private data across all clients. Hanzely et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">hanzely2020federated </a></cite> propose a different formulation of the problem that seeks an explicit trade-off between the global model and the local models. Instead of learning a single global model, each device learns a mixture of the global model and its own local model. To solve the formulation, the authors develop a new variant of gradient descent called Loopless Local Gradient Descent (LLGD). Instead of performing full averaging, LLGD only takes steps towards averaging thus suggesting that full averaging methods such as Federated Averaging might be too aggressive.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Federated learning encompasses a wide variety of settings, devices, and datasets. When local datasets are small and the data distribution is IID, global models typically outperform local models, and a majority of clients benefit from participating in the federated learning process. However, when clients have sufficiently large private datasets and the data distribution is non-IID, local models exhibit better performance than the shared global model, and clients have no incentive to participate in the federated learning process. An open theoretical question is to determine the conditions under which shared global models can perform better than individual local models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">This paper surveys personalization techniques used to adapt a global federated model to individual clients. With a few exceptions, most prior work is focussed on measuring the performance of the global model on aggregated data instead of measuring its performance as seen by individual clients. Global performance, however, has no relevance if the global model is expected to be subsequently personalized before being put to use.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Personalized models usually show better performance for individual clients than global or local models. In some cases, however, personalized models fail to reach the same performance as local models, especially when differential privacy and robust aggregation is implemented <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">yu2020salvaging </a></cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
P. Voigt and A. Von dem Bussche, “The eu general data protection regulation
(gdpr),” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
G. J. Annas <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Hipaa regulations-a new era of medical-record
privacy?,” <span id="bib.bib2.2.2" class="ltx_text ltx_font_italic">New England Journal of Medicine</span>, vol. 348, no. 15,
pp. 1486–1490, 2003.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Communication-efficient learning of deep networks from decentralized
data,” <span id="bib.bib3.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.05629</span>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein,
H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile
keyboard prediction,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.03604</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Towards
federated learning at scale: System design,” <span id="bib.bib5.2.2" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1902.01046</span>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Advances
and open problems in federated learning,” <span id="bib.bib6.2.2" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
T. Yu, E. Bagdasaryan, and V. Shmatikov, “Salvaging federated learning by
local adaptation,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.04758</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
F. Hanzely and P. Richtárik, “Federated learning of a mixture of global
and local models,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.05516</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Q. Wu, K. He, and X. Chen, “Personalized federated learning for intelligent
iot applications: A cloud-edge based framework,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2002.10671</span>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on
Security and Privacy (SP)</span>, pp. 3–18, IEEE, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended
feature leakage in collaborative learning,” in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">2019 IEEE Symposium on
Security and Privacy (SP)</span>, pp. 691–706, IEEE, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
private recurrent language models,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.06963</span>,
2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
E. Bagdasaryan, O. Poursaeed, and V. Shmatikov, “Differential privacy has
disparate impact on model accuracy,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems</span>, pp. 15453–15462, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
K. C. Sim, P. Zadrazil, and F. Beaufays, “An investigation into on-device
personalization of end-to-end automatic speech recognition models,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.06678</span>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
Y. Jiang, J. Konecny, K. Rush, and S. Kannan, “Improving federated learning
personalization via model agnostic meta learning,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1909.12488</span>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
T. Tuor, S. Wang, B. J. Ko, C. Liu, and K. K. Leung, “Data selection for
federated learning with relevant and irrelevant data at clients,” <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2001.08300</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three approaches for
personalization with applications to federated learning,” <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2002.10619</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
L. Y. Pratt, “Discriminability-based transfer between neural networks,” in
<span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pp. 204–211, 1993.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
J. Schneider and M. Vlachos, “Mass personalization of deep learning,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.02803</span>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
K. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and D. Ramage,
“Federated evaluation of on-device personalization,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1910.10252</span>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
M. McCloskey and N. J. Cohen, “Catastrophic interference in connectionist
networks: The sequential learning problem,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Psychology of learning
and motivation</span>, vol. 24, pp. 109–165, Elsevier, 1989.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
R. M. French, “Catastrophic forgetting in connectionist networks,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Trends in cognitive sciences</span>, vol. 3, no. 4, pp. 128–135, 1999.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
R. Caruana, “Multitask learning,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Machine learning</span>, vol. 28, no. 1,
pp. 41–75, 1997.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task
learning,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
pp. 4424–4434, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast
adaptation of deep networks,” in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the 34th International
Conference on Machine Learning-Volume 70</span>, pp. 1126–1135, JMLR. org, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
A. Nichol, J. Achiam, and J. Schulman, “On first-order meta-learning
algorithms,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.02999</span>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
A. Fallah, A. Mokhtari, and A. Ozdaglar, “Personalized federated learning: A
meta-learning approach,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.07948</span>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar, “Adaptive gradient-based
meta-learning methods,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing
Systems</span>, pp. 5915–5926, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
F. Chen, Z. Dong, Z. Li, and X. He, “Federated meta-learning for
recommendation,” <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.07876</span>, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
D. Li and J. Wang, “Fedmd: Heterogenous federated learning via model
distillation,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.03581</span>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
M. G. Arivazhagan, V. Aggarwal, A. K. Singh, and S. Choudhary, “Federated
learning with personalization layers,” <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1912.00818</span>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.08672" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.08673" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.08673">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.08673" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.08674" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 08:29:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
