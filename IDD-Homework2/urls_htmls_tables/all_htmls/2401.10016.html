<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Gender Bias in Machine Translation and The Era of Large Language Models</title>
<!--Generated on Thu Jan 18 14:32:25 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.10016v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Gender Bias in MT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Gender Bias Sources</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title='3.1 "Conventional" NMT systems ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models'><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>"Conventional" NMT systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>GPT for MT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Preliminary Experiment and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS1" title="4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS1" title="4.1.1 Dataset ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS1.SSS2" title="4.1.2 Prompting Scenarios ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Prompting Scenarios</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Manual error analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS1" title="4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Prompt Scenario 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS2" title="4.2.2 Prompt Scenario 2 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Prompt Scenario 2</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="4.3 Findings &amp; Implications ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Findings &amp; Implications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 Limitations ‣ 4.3 Findings &amp; Implications ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Limitations</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-SA 4.0</div><div id="watermark-tr">arXiv:2401.10016v1 [cs.CL] 18 Jan 2024</div></div>
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Gender Bias in Machine Translation and The Era of Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eva Vanmassenhove
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1">This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT’s current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.
<br class="ltx_break"/></span></p>
<p class="ltx_p" id="id2.id2"><span class="ltx_text ltx_font_bold" id="id2.id2.1">Keywords:<span class="ltx_text ltx_font_medium" id="id2.id2.1.1"> <span class="ltx_text ltx_font_italic" id="id2.id2.1.1.1">Neural Machine Translation, Bias, Gender, Translation Technology, Large Language Models</span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">This chapter investigates the role of Machine Translation (MT) in perpetuating and shaping societal biases, with a specific emphasis on gender bias. Within the realm of MT, biases, specifically gender biases, are particularly prominent due to contrastive linguistic settings that necessitate disambiguation and explicitness in their representation of gender. The (over)reliance of MT systems on statistical patterns, can lead to erroneous disambiguation and thus the generation of morphologically incorrect alternatives <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>. Moreover disambiguation based solely on statistical likelihood combined with the fact that current (traditional) MT systems exhibit limited capability in providing multiple potential translation candidates reinforces or even exacerbates (potentially harmful) stereotypes <cite class="ltx_cite ltx_citemacro_citep">(Cho
et al., <a class="ltx_ref" href="#bib.bib7" title="">2019</a>; Rescigno et al., <a class="ltx_ref" href="#bib.bib28" title="">2020</a>; Prates
et al., <a class="ltx_ref" href="#bib.bib26" title="">2020</a>)</cite>. We advocate for more accountability in the design, development, and deployment of MT systems to address these biases and limitations.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To explore the potential for addressing gender bias in an English-Italian translation setting, we investigate the extent to which ChatGPT (based on GPT-3.5), a powerful Large Language Model (LLM), handles gender across these languages and whether it can contribute and help with bias mitigation. Specifically, we examine whether ChatGPT (GPT-3.5) is capable of generating translations that effectively address gender bias when explicitly prompted to do so. The translations generated by the model, despite specific prompts regarding gender, seem to indicate that there are limitations to the model’s ability to provide a systematic solution for gender bias in translation. While the overall observations aligned with our expectations and the related work, it was surprising that explicitly prompting for gender alternatives often led to additional biases and inaccurate translations rather than more balanced, fair and/or less-biased responses. These findings underscore the need for further exploration and development in this area. In light of the limitations and potential biases of current MT systems, we advocate for continued research and improvements to promote fair and unbiased translations. Given the multidisciplinary nature of the fields, we advocate for more hybridization both in terms of the technology by, for instance, integrating linguistic information into neural models as in the field itself through more collaboration among (computational) linguists, computer scientists, sociolinguists, ethicists and others.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The remainder of this chapter is organized as follows: Section <a class="ltx_ref" href="#S2" title="2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of gender bias in MT and highlights the linguistic settings and statistical dependencies that contribute to its manifestation. Section <a class="ltx_ref" href="#S3" title="3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> presents related work and discusses existing efforts to address gender bias in MT systems. Section <a class="ltx_ref" href="#S4" title="4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> outlines the experiment, including the specific prompts and the analysis. Finally, Section <a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the article by summarizing our findings and their implications, emphasizing the need for further research and advancements in addressing gender bias in MT systems, and advocating for increased accountability in the development and deployment of these technologies. By conducting a systematic analysis of gender bias in MT and evaluating the potential of LLMs in mitigating such bias, we hope to contribute to the ongoing efforts in creating fair, unbiased, and inclusive language technologies.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Gender Bias in MT</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The advent of Neural Machine Translation (NMT) played a pivotal role in breaking down language barriers and facilitating knowledge exchange among diverse (linguistic) communities. With their success and adoption, it also became increasingly important to investigate and address the inherent limitations and shortcomings these systems possess. First, current systems, have a tendency to not only perpetuate but also exacerbate biases and lack control in order to handle contrastive linguistic challenges, such as gender, in a systematic manner <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib41" title="">2019</a>, <a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>. Second, these systems are unable to adhere and abide by language-specific guidelines or policies related to gender-inclusive or gender-neutral language. As the influence of MT systems grows, it becomes increasingly important to scrutinize and analyse the biases embedded within MT systems and their potential impact on societal inequalities.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">One prevalent form of bias, which warrants particular attention in translation contexts, is gender bias. Within the context of MT, we define gender bias as an (explicit or implicit) skew across or within language(s) towards gender(s), leading to systematic associations with gender(s) and thus creating representational (and in some cases allocational) harms that perpetuate unfair, inaccurate and potentially discriminatory stereotypes.
</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Gender biases in MT are often exacerbated by the need for explicit representation and disambiguation of gender in contrastive linguistic settings. For example, the English sentence <span class="ltx_text ltx_font_italic" id="S2.p3.1.1">‘my husband is a <span class="ltx_text ltx_font_bold" id="S2.p3.1.1.1">kindergarten teacher [neutral]</span>’</span> contains a profession noun <span class="ltx_text ltx_font_italic" id="S2.p3.1.2">‘kindergarten teacher’</span> that is not explicitly marked for gender. However, within this sentences, it is clear that it refers to <span class="ltx_text ltx_font_italic" id="S2.p3.1.3">my husband</span>, a masculine nouns with which it semantically agrees. The sentence is translated by Google Translate<span class="ltx_note ltx_role_endnote" id="endnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote1.1.1.1"><sup class="ltx_sup" id="endnote1.1.1.1.1">1</sup></span></span>Accessed on June 12, 2023.</span></span></span> as <span class="ltx_text ltx_font_italic" id="S2.p3.1.4">‘mijn man is <span class="ltx_text ltx_font_bold" id="S2.p3.1.4.1">kleuterjuf [feminine]</span>’</span> in Dutch, where <span class="ltx_text ltx_font_italic" id="S2.p3.1.5">‘juf’</span> is a gender-marked noun that can be translated into English as <span class="ltx_text ltx_font_italic" id="S2.p3.1.6">‘miss’</span>. In many other cases, the MT systems do not produce translation errors but consistently opt for gender stereotypical translations such as the French <span class="ltx_text ltx_font_italic" id="S2.p3.1.7">‘tu es <span class="ltx_text ltx_font_bold" id="S2.p3.1.7.1">belle [feminine]</span>’</span> as a translation for the English gender-neutral <span class="ltx_text ltx_font_italic" id="S2.p3.1.8">‘you are <span class="ltx_text ltx_font_bold" id="S2.p3.1.8.1">beautiful [neutral]</span>’</span> (where the English sentence is besides gender, also ambiguous in terms of number and degree of politeness as opposed to French) and <span class="ltx_text ltx_font_italic" id="S2.p3.1.9">‘tu es <span class="ltx_text ltx_font_bold" id="S2.p3.1.9.1">intelligent</span> [masculine]’</span> as a translation for the English sentence <span class="ltx_text ltx_font_italic" id="S2.p3.1.10">‘you are <span class="ltx_text ltx_font_bold" id="S2.p3.1.10.1">smart</span> [neutral]’</span>. Another illustration, based on a tweet by Dora Vargha <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a class="ltx_ref" href="#bib.bib42" title="">2021</a>)</cite><span class="ltx_note ltx_role_endnote" id="endnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote2.1.1.1"><sup class="ltx_sup" id="endnote2.1.1.1.1">2</sup></span></span>Re-translated on June 12 2023 by the author.</span></span></span>, is presented in Table <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> with Hungarian as the source language and English as the target. Hungarian has a gender-neutral pronoun <span class="ltx_text ltx_font_italic" id="S2.p3.1.11">ö</span> which can be translated into English as either <span class="ltx_text ltx_font_italic" id="S2.p3.1.12">he</span>, <span class="ltx_text ltx_font_italic" id="S2.p3.1.13">she</span> or gender-neutral <span class="ltx_text ltx_font_italic" id="S2.p3.1.14">they</span>. These translations were generated by Google Translate.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T1.1.1.1.1" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Hungarian</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T1.1.1.1.2" style="padding-left:15.0pt;padding-right:15.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">English</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_left" id="S2.T1.1.2.1.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1.1">Ö</span> szép.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.2.1.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.2.1">She</span> is beautiful.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.3.2.1.1">Ö</span> okos.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.2.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.3.2.2.1">He</span> is clever.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.4.3.1.1">Ö</span> olvas.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.4.3.2.1">He</span> reads.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.5.4.1.1">Ö</span> varr.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.4.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.5.4.2.1">She</span> sews.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S2.T1.1.6.5.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.6.5.1.1">Ö</span> tanít.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.6.5.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.6.5.2.1">He</span> teaches.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.6.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.7.6.1.1">Ö</span> főz.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.6.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.7.6.2.1">She</span> cooks.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.8.7.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.8.7.1.1">Ö</span> kutat.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.8.7.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.8.7.2.1">He</span> is researching.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S2.T1.1.9.8.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.9.8.1.1">Ö</span> gyereket nevel.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.9.8.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.9.8.2.1">She</span> is raising a child.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.9.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.10.9.1.1">Ö</span> sok pénzt keres.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.10.9.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.10.9.2.1">He</span> makes a lot of money.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S2.T1.1.11.10.1" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.11.10.1.1">Ö</span> süteményt süt.</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.11.10.2" style="padding-left:15.0pt;padding-right:15.0pt;">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.11.10.2.1">She</span> bakes cakes.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Illustration Gender Bias in Hungarian-English translations provided by Google Translate</figcaption>
</figure>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">While none of the generated translations in Table <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> are wrong from a linguistic point of view, they do exhibit a fair amount of bias and stereotyping when it comes to opting for the feminine or masculine pronoun in the automatically generated English translations. In a systematic examination of this gender bias phenomenon, <cite class="ltx_cite ltx_citemacro_cite">Prates
et al. (<a class="ltx_ref" href="#bib.bib26" title="">2020</a>)</cite> translated sentences containing professions from 12 different gender-neutral language (including Hungarian) into English and examined how often female, male or gender-neutral pronouns appeared in the translations. Their analysis not only revealed a significant imbalance in the distribution of pronouns but also highlighted that this imbalance exceeds the gender disparities observed in society itself for fields such as STEM (Science, Technology, Engineering and Mathematics). Their comparison with results from the U.S. Bureau of Labor Statistics revealed that Google Translate underestimated the expected frequency of female translations, demonstrating an even higher prevalence of male defaults compared to what job occupation data alone would suggest.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Gender Bias Sources</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Different languages handle and mark gender in different ways, resulting in various types of potential ambiguities that are hard to resolve for MT systems, making a one-solution-fits-all very unlikely. The requirement for explicitation and disambiguation in combination with the fact that languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked sets gender aside form other biases in contrastive linguistic settings. By this, it not implied that other biases do not occur, however, they do manifest themselves in different, more implicit ways.<span class="ltx_note ltx_role_endnote" id="endnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote3.1.1.1"><sup class="ltx_sup" id="endnote3.1.1.1.1">3</sup></span></span>Although one could argue, that this implicitness makes it all the more dangerous.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">When state-of-the-art NMT systems are faced with (gender) ambiguous words in cross-linguistic settings, they will generate the most statistically probable translation derived from the training data. This would imply that, one of the main sources of bias in the output are the datasets used as an input, referred to as dataset bias. We want to emphasize that reducing the problem solely to a matter of data bias is an oversimplification since such a reduction neglects the intricate web of factors underlying these biases, including historical legacies and societal injustices <cite class="ltx_cite ltx_citemacro_citep">(Birhane, <a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>. However, given that an in-depth discussion of historical and societal injustices is out of the scope of this chapter, we will further zoom in on data bias where we distinguish between coverage and social bias.<span class="ltx_note ltx_role_endnote" id="endnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote4.1.1.1"><sup class="ltx_sup" id="endnote4.1.1.1.1">4</sup></span></span>For a more comprehensive exploration of this topic, we refer to <cite class="ltx_cite ltx_citemacro_cite">Birhane (<a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite> which provides valuable insights into the ethical dimensions of algorithmic systems and their impact while advocating for a more critical analysis in the field of machine learning and data science.</span></span></span> Additionally, we will briefly discuss statistical and algorithmic bias.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Coverage bias in the context of gender and MT, refers to the underrepresentation and/or the misrepresentation of gender identities and gender-related terminology in the data used for training. For instance, an analysis of <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">Europarl</span> <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="#bib.bib18" title="">2005</a>)</cite>, one of the more popular training datasets for MT systems, revealed a 2:1 male-female speaker ratio. Such an imbalanced dataset in terms of male/female representation, most likely implies a higher frequency of male pronouns, male-endings (nouns, adjectives…), increasing the likelihood of generating "male" translations when facing ambiguity. This can be interpreted in the broad way, since there is also evidence that word embeddings are conforming to an overall androcentric worldview <cite class="ltx_cite ltx_citemacro_citep">(Petreski and
Hashim, <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Taking the example below, a few possible translations for the English word <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">‘teacher’ [neutral]</span> are provided in French: <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.2">‘enseignant’ [masculine singular]</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.3">‘enseignante’ [feminine]</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.4">‘enseignant-e’ [binary alternative]</span> or <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.5">‘le personnel enseignant’ [gender-neutral alternative]</span>.<span class="ltx_note ltx_role_endnote" id="endnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote5.1.1.1"><sup class="ltx_sup" id="endnote5.1.1.1.1">5</sup></span></span>We acknowledge that the gender-neutral alternative <span class="ltx_text ltx_font_italic" id="endnote5.2">‘le personnel enseignant’</span> is not the most literal translation of <span class="ltx_text ltx_font_italic" id="endnote5.3">‘teacher’</span> and furthermore implies multiple people (plural). However, in settings where <span class="ltx_text ltx_font_italic" id="endnote5.4">‘teacher’</span> is used in a generic way, it can be considered a valid gender-neutral alternative.</span></span></span></p>
</div>
<figure class="ltx_figure" id="S2.F1"><svg class="ltx_picture ltx_centering" height="80.21" id="S2.F1.pic1" overflow="visible" version="1.1" width="258.49"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,80.21) matrix(1 0 0 -1 0 0) translate(26.35,0) translate(0,59.79)"><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -21.74 -24.49)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="43.47"><span class="ltx_text" id="S2.F1.pic1.1.1.1.1.1">teacher</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 125.73 6.57)"><foreignobject height="11.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="63.5"><span class="ltx_text" id="S2.F1.pic1.2.2.2.1.1">enseignant</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 122.66 -13.12)"><foreignobject height="11.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.03"><span class="ltx_text" id="S2.F1.pic1.3.3.3.1.1">enseignante</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 87.43 -32.99)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="140.1"><span class="ltx_text" id="S2.F1.pic1.4.4.4.1.1">le personnel enseignant</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 120.35 -52.49)"><foreignobject height="11.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="74.26"><span class="ltx_text" id="S2.F1.pic1.5.5.5.1.1">enseignant-e</span></foreignobject></g><path d="M 26.62 -19.69 L 120.84 9.84" style="fill:none"></path><path d="M 26.62 -19.69 L 117.77 -9.84" style="fill:none"></path><path d="M 26.62 -19.69 L 115.46 -49.21" style="fill:none"></path><path d="M 26.62 -19.69 L 82.54 -29.53" style="fill:none"></path></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Some French translations of the English noun <span class="ltx_text ltx_font_italic" id="S2.F1.2.1">‘teacher’</span>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">An imbalanced dataset could affect the probability of these translations being generated by an MT systems and affect the ability fo handle all possibilities. If the dataset contains a higher proportion of male speakers or masculine forms as opposed to feminine or gender-neutral ones, the MT system will become biased towards producing more masculine translations in case of ambiguity. It would thus consider <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.1">‘enseignant’</span> the more probable translation as opposed to the other valid options (given that there is no additional context to disambiguate). Additionally, a limited or no exposure to forms such as the binary gender inclusive <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.2">‘enseignant-e’</span> or the gender-neutral <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.3">‘le personnel enseignant’</span> in the training data can lead to erroneous or suboptimal translations that are less fluent, accurate and lack naturalness. While balancing training data in terms of male and female genders can be a somewhat viable option in many cases to alleviate these (binary) biases, more and more languages are accommodating for non-binary gender<span class="ltx_note ltx_role_endnote" id="endnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote6.1.1.1"><sup class="ltx_sup" id="endnote6.1.1.1.1">6</sup></span></span>We will use the term <span class="ltx_text ltx_font_italic" id="endnote6.2">non-binary gender</span> to refer to all genders that fall outside the binary female/male distinction.</span></span></span> employing different strategies (for instance, gender-neutral pronouns like <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.4">they</span> in English, gender-neutral endings like <span class="ltx_text ltx_font_italic" id="S2.SS1.p5.1.5">x</span> in Spanish…), a trend that will need to be reflected in MT technology as well. Currently this is not the case, among others, due to a lack of readily available gender-neutral/inclusive data, posing additional challenges for these data-hungry models. Rule-based or hybrid approaches that leverage linguistic knowledge in order to automatically generate gender-neutral alternatives in a post-processing way could be further explored to address the data scarcity issue, especially given the rather systematic nature of gender agreement rules <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib37" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">Social bias in MT, refers to the bias that arises due to societal stereotypes and norms surrounding gender roles, which are embedded in and (un)consciously transmitted via the language in the training data. Such biases may influence the system to generate translations that align with traditional gender stereotypes or assumptions. Even more so because of the use of word embeddings which are used to capture semantic relationships and associations between words based on patterns present in the data. These embeddings can inadvertently reinforce existing biases as they learn from the prevailing language usage, including gendered language and stereotypes and <span class="ltx_text ltx_font_italic" id="S2.SS1.p6.1.1">embed</span> co-occurence patterns in the representations of the word themselves. A recent study by  <cite class="ltx_cite ltx_citemacro_cite">Caliskan et al. (<a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite> investigating the extent of gender bias in static word embeddings revealed that the most common words and concepts are strongly associated with men more so than with women. Additionally, their findings highlighted how various gender biases are still prevalent across multiple dimensions as they observed differences in terms of the part-of-speech (e.g. women are more often associated with adjectives), clusters of concepts (e.g. women are more associated with sexual content and men with big tech), and basic dimensions of the meaning of words (e.g. women are more associated with positive valence and men with dominance).</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">While data bias is a well-acknowledged phenomenon, from a technical point of view, also statistical and algorithmic bias introduced by the translation models are important dimensions of bias in MT since they can introduce, perpetuate and even exacerbate biases observed in the training data. In the context of MT, <cite class="ltx_cite ltx_citemacro_cite">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib41" title="">2019</a>)</cite> investigated the effect of statistical biases on generated translations. A statistically biased MT system may exhibit preferences for frequently occurring words or sub-words, potentially overlooking less common but equally valid synonyms or morphological variants. This bias leads to a loss of morphological variety and can impact the system’s ability to generate diverse and grammatically correct translations. <cite class="ltx_cite ltx_citemacro_cite">Toral (<a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite> reached similar conclusions focusing on automatic post-editing, demonstrating its simpler and more normalized nature compared to human post-edited or translated texts. Both these observations can, in turn, be linked directly to gender bias in translations. Looking back at Figure <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, it is likely that some of the potential translations are way more common in the data than others. The model would strengthen the more frequently occurring connections and <span class="ltx_text ltx_font_italic" id="S2.SS1.p7.1.1">weaken</span> those that are more rarely observed in the data. In a follow-up paper, <cite class="ltx_cite ltx_citemacro_cite">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite> conducted experiments where they train a model with which they then translate the original training data. They give an example where the most commonly occurring form of the French lemma <span class="ltx_text ltx_font_italic" id="S2.SS1.p7.1.2">‘président’</span> in the training data, the masculine singular form, becomes even more common after training. The least common word form of that same lemma, the feminine plural <span class="ltx_text ltx_font_italic" id="S2.SS1.p7.1.3">‘présidentes’</span>, decreases in frequency and is for some models even completely lost in translation. In line with the abovementioned research, recent work by <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà
et al. (<a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite> shows how different architectures trained on the same data exhibit different levels of gender bias. When comparing language-specific encoder-decoders to shared encoder-decoder models, the former are less biased <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà
et al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>. If machine translationese, i.e. the language produced by MT systems, represent simplified, less lexically and morphologically diverse versions of the training data, this also raises more general questions related to the sociolinguistic implications and its potential long-term effects on language <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p" id="S2.SS1.p8.1">We ought to note, however, that while the abovementioned biases might explain the biased outputs observed in automatically generated translations, simply ‘debiasing’ or ’balancing’ the datasets or the model will not enable systems to provide multiple correct alternatives (e.g. the previously provided translations from Hungarian into English) since this would require the generation of multiple alternative translations. More balanced data might however enable systems to more accurately translate sentences where the gender of a referent is present (see the previously given example: <span class="ltx_text ltx_font_italic" id="S2.SS1.p8.1.1">‘My husband is a kindergarten teacher’</span>) and improve the overall quality of translations for all genders.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">While it is out of the scope of this chapter to discuss the entire body of research on gender bias in Natural Language Technology and MT, we will discuss the most pioneering, impactful and/or most closely related work. We will cover some of the existing research on gender bias in both neural MT systems (Section <a class="ltx_ref" href="#S3.SS1" title='3.1 "Conventional" NMT systems ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models'><span class="ltx_text ltx_ref_tag">3.1</span></a>) and more recent work focusing on GPT models used as MT systems (Section <a class="ltx_ref" href="#S3.SS2" title="3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>). NMT models are typically bidirectional sequence-to-sequence (<span class="ltx_text ltx_font_italic" id="S3.p1.1.1">seq2seq</span>)<cite class="ltx_cite ltx_citemacro_citep">(Sutskever
et al., <a class="ltx_ref" href="#bib.bib34" title="">2014</a>)</cite> architectures with encoder-decoder frameworks while GPT (Generative Pre-trained Transformer) models are primary a language models for language generation tasks. Both the more commonly employed NMT systems (the type of model used by Google Translate<span class="ltx_note ltx_role_endnote" id="endnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote7.1.1.1"><sup class="ltx_sup" id="endnote7.1.1.1.1">7</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span></span></span>, DeepL<span class="ltx_note ltx_role_endnote" id="endnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote8.1.1.1"><sup class="ltx_sup" id="endnote8.1.1.1.1">8</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.deepl.com" title="">https://www.deepl.com</a></span></span></span>, and others.) and GPT models are based on Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani
et al., <a class="ltx_ref" href="#bib.bib43" title="">2017</a>)</cite>. However, GPT models are decoder-only models as opposed to the usual encoder-decoder architectures of the conventional MT systems. They are furthermore primarily trained on monolingual data (as opposed to carefully curated parallel corpora) and they require a significantly larger number of parameters <cite class="ltx_cite ltx_citemacro_citep">(Hendy et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>. When using a GPT model for a translation task, it involves relying on the GPT model’s decoding capabilities in order to generate target language translations given a specific input in a unidirectional autogressive way, generating one token at a time, conditioned on the source.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>"Conventional" NMT systems</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The related research on bias in MT can be largely divided into approaches that aim to mitigate bias by addressing the
(i) pre-processing stage (training data and/or representations); (ii) processing stage (algorithm); or (iii) post-processing
stage (output). A large body of research on gender bias in language technology focused on debiasing models by changing the internal representations of separate words (often limited to animate nouns or adjectives) as a pre-processing step (a.o. <cite class="ltx_cite ltx_citemacro_cite">Bolukbasi et al. (<a class="ltx_ref" href="#bib.bib5" title="">2016</a>); Zhao
et al. (<a class="ltx_ref" href="#bib.bib47" title="">2018</a>)</cite>). Debiasing is then usually done by identifying and capturing a gender subspace using seed words (such as <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">‘she’</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">‘he’</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">‘girl’</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">‘boy’</span>…) and subtracting the obtained gender dimension from words such as <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.5">‘plumber’</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.6">‘receptionist’</span> or <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.7">‘doctor’</span> that are gender-neutral and should not carry gender information. The approach has been criticised in subsequent papers due to the fact that there is no clear correlation between removing bias from the internal representations, the so-called intrinsic bias, and the bias observed in downstream tasks or the extrinsic bias <cite class="ltx_cite ltx_citemacro_citep">(Gonen and
Goldberg, <a class="ltx_ref" href="#bib.bib14" title="">2019</a>; Goldfarb-Tarrant et al., <a class="ltx_ref" href="#bib.bib13" title="">2021</a>)</cite>. Aside from the limited impact of these debiasing techniques on the actual translations <cite class="ltx_cite ltx_citemacro_citep">(Font and
Costa-jussà, <a class="ltx_ref" href="#bib.bib10" title="">2019</a>)</cite>, pre-processing techniques have other shortcomings, for instance, these methods (i) rely on lists of manually selected seed words that are ought to capture the ‘gender’ dimension; (ii) are usually applied to animate nouns (and sometimes adjectives) while other part-of-speechs (for instance, verbs, pronouns) might also require debiasing; (iii) offer no control over the eventual output as the MT system will still offer only one translation option; and (iv) do not allow to go beyond the binary gender distinction, i.e. more recent phenomena such as gender-neutral they cannot be handled in an appropriate manner.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Another type of preprocessing techniques that have been explored is the integration of additional features that can help resolve ambiguities from the source into the target language <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib38" title="">2018</a>; Moryossef
et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib38" title="">2018</a>)</cite> built the first speaker-informed NMT system where a tag indicating the gender of the speaker was fed to the NMT system along with the source input. Although the gender tags (that were limited to binary gender) improved the translation in terms of automatic evaluation scores, the manual evaluation revealed some (potentially undesirable) side-effects. While the gender tags were intended to improve the morphological gender agreement with the gender of the speaker, they often resulted in different lexical word choices. Similarly <cite class="ltx_cite ltx_citemacro_cite">Moryossef
et al. (<a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite> employed a parataxis construction such as She told him:’ to help the translation system disambiguate the gender of referents. Both approaches are limited in terms of the referents that can be controlled (first person singular as a subject <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove et al., <a class="ltx_ref" href="#bib.bib38" title="">2018</a>)</cite>, and first person singular as a subject as well as the third person singular as the (in)direct object <cite class="ltx_cite ltx_citemacro_citep">(Moryossef
et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite>. In <cite class="ltx_cite ltx_citemacro_cite">Stafanovičs et al. (<a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite>, a method is presented whereby the gender of the words in the target are projected onto the source. The grammatical gender annotations from the target language in the source data can be leveraged by the MT system. The idea is somewhat similar, although more fine-grained, to the previous approaches presented given that it is assumed that the additional information provided will be a strong enough learning signal for the MT system to generate alternative translation.
The evaluation of their approach is limited to automatic evaluation metrics based on the WinoMT test suite (Stanovsky et al. 2019). WinoMT is a limited test suite given that it contains only sentences that are unambiguous in the source (i.e. the gender of the referents can be inferred from the source).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">To the best of our knowledge, only one work has attempted to control gender by opening the NMT black-box. <cite class="ltx_cite ltx_citemacro_cite">Bau et al. (<a class="ltx_ref" href="#bib.bib2" title="">2019</a>)</cite> developed an unsupervised method to identify and (de)activate the neurons responsible for specific linguistic phenomena. From their experiments, it resulted that gender is the most difficult phenomenon to capture and control with a 21% success rate using the best setup. Gender is particularly hard to control because the ‘gender neurons’ are distributed along the network making it a difficult property to modify.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">We would like to note that most of the related work focuses on gender from a binary point of view (male/female) with exception to the work by <cite class="ltx_cite ltx_citemacro_cite">Saunders
et al. (<a class="ltx_ref" href="#bib.bib29" title="">2020</a>)</cite> who added gender-neutral tags to an existing dataset and replaced gendered endings by placeholders, and the work by <cite class="ltx_cite ltx_citemacro_cite">Vanmassenhove et al. (<a class="ltx_ref" href="#bib.bib37" title="">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Sun
et al. (<a class="ltx_ref" href="#bib.bib33" title="">2021</a>)</cite> which focuses on developing a gender-neutral rewriter for English.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">For a more in-depth overview of concepts related to bias, a summary of work on gender bias and a discussion of mitigation strategies proposed (up until 2021) for MT, we refer the reader to the work by <cite class="ltx_cite ltx_citemacro_cite">Savoldi et al. (<a class="ltx_ref" href="#bib.bib30" title="">2021</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>GPT for MT</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">GPT models are a type of LLMs that started gaining mainstream popularity near the end of 2022, when ChatGPT’s user-friendly interface<span class="ltx_note ltx_role_endnote" id="endnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote9.1.1.1"><sup class="ltx_sup" id="endnote9.1.1.1.1">9</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a></span></span></span> provided easy access to some of the capabilities of GPT models, impressing the public by its human-like responses in a conversational setup <cite class="ltx_cite ltx_citemacro_citep">(Toews, <a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>. GPT models are developed by OpenAI’s<span class="ltx_note ltx_role_endnote" id="endnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote10.1.1.1"><sup class="ltx_sup" id="endnote10.1.1.1.1">10</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="www.openai.com" title="">www.openai.com</a></span></span></span> research lab. The first one, GPT, was developed back in 2018. Since then, various newer versions saw the light of day (GPT-2 (2019), GPT-3 (2020) and GPT-4 (2023)..), which are all iterations of the same GPT language model. The main difference between the different versions being the (exponential) growth in parameters: from 117 million parameters (GPT) to 175 billion for GPT-3, making it at the time the largest LLM. Reuters<span class="ltx_note ltx_role_endnote" id="endnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote11.1.1.1"><sup class="ltx_sup" id="endnote11.1.1.1.1">11</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reuters.com" title="">https://www.reuters.com</a> article accessible via <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://shorturl.at/dewzZ" title="">https://shorturl.at/dewzZ</a></span></span></span> reported that according to an UBS study, ChatGPT became the fastest growing consumer application ever recorded, reaching 100 million monthly active users in January 2023, only two months after its launch on November 30, 2022. Given its popularity and wide-range usage (recreational and professional), ChatGPT is considered the first notable threat to Google’s monopoly<span class="ltx_note ltx_role_endnote" id="endnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote12.1.1.1"><sup class="ltx_sup" id="endnote12.1.1.1.1">12</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nytimes.com" title="">https://www.nytimes.com</a> article accessible via https://shorturl.at/ruRS3</span></span></span><cite class="ltx_cite ltx_citemacro_citep">(Ghosh and
Caliskan, <a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">When talking about LLMs, since 2023, it is common to distinguish between bi-directional Transformer architecture (for instance, Bidirectional Encoder Representations from Transformers (BERT) <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al., <a class="ltx_ref" href="#bib.bib9" title="">2019</a>)</cite>) and uni-directional ones such as the GPT models. While unidirectional language models process and generate strictly in one direction (they process input from left-to-right and generate output left-to-right, token per token), bidirectional ones can attend to both the left and right context when processing input and output sequences, conditioning on preceding and succeeding tokens to generate output. The choice between unidirectional and bidirectional language models depends on the specific requirements of the task at hand. Unidirectional models are well-suited for sequential text generation and are computationally efficient, while bidirectional models tend to excel in tasks that demand a deeper understanding of context and long-range dependencies within the text. Both type of models (e.g. BERT vs GPT-3) have been used for the task of MT, each with their own strengths and weaknesses. BERT itself is not typically used as a direct MT system, it has been incorporated into MT pipelines in various ways to improve translation performance and address certain challenges (see previous section). As for GPT models, the interactive nature of ChatGPT (based on GPT models) allows it to engage in back-and-forth conversations with users which is an interesting ability to experiment with in the context of bias since it implies we can explicitly prompt it to be mindful of biases or to generate multiple alternative translations and thus potentially mitigate biased output. This interactivity opens avenues for experimentation in addressing bias in MT and tailoring translations to meet user preferences and specific contexts. Hereafter, we will therefore focus on related work that specifically explores the use of GPT models for MT. We ought to note that many of these works present very recent and somtimes preliminary findings published on arxiv<span class="ltx_note ltx_role_endnote" id="endnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote13.1.1.1"><sup class="ltx_sup" id="endnote13.1.1.1.1">13</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/" title="">https://arxiv.org/</a></span></span></span> which have been approved for posting after moderation but have not gone through a peer review process yet.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Various studies have already investigated the more general translation abilities of GPT models. Among those are the work by <cite class="ltx_cite ltx_citemacro_cite">Hendy et al. (<a class="ltx_ref" href="#bib.bib16" title="">2023</a>); Jiao
et al. (<a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Wang
et al. (<a class="ltx_ref" href="#bib.bib44" title="">2023</a>)</cite> which evaluates GPT models for MT on a sentence-level <cite class="ltx_cite ltx_citemacro_citep">(Hendy et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>; Jiao
et al., <a class="ltx_ref" href="#bib.bib17" title="">2023</a>; Wang
et al., <a class="ltx_ref" href="#bib.bib44" title="">2023</a>)</cite> and on a document-level <cite class="ltx_cite ltx_citemacro_cite">Hendy et al. (<a class="ltx_ref" href="#bib.bib16" title="">2023</a>); Wang
et al. (<a class="ltx_ref" href="#bib.bib44" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Hendy et al. (<a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite> conducted an extensive automatic and human evaluation. They assessed the performance of three GPT models (ChatGPT,
GPT3.5 (text-davinci-003), and text-davinci002) on 18 language pairs across 4 domains and compared it to state-of-the-art (research and commercial) translation systems. They used the top-ranked systems from the WMT evaluation campaign to identify the state-of-the-art and the respective baselines for comparison. <cite class="ltx_cite ltx_citemacro_cite">Jiao
et al. (<a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite> present a more preliminary study using ChatGPT (GPT-3) for MT. Their analysis is deemed preliminary since it contains only 50 sentences for every set/condition and is additionally limited by the fact that they only employ automatic evaluation metrics. Both studies, however, find that while GPT models can perform competitively with existing MT systems in high-resource<span class="ltx_note ltx_role_endnote" id="endnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote14.1.1.1"><sup class="ltx_sup" id="endnote14.1.1.1.1">14</sup></span></span>We ought to note that this is an ill-defined term.</span></span></span> settings/languages, they significantly underperform when used in low-resource setting or for distant language paris <cite class="ltx_cite ltx_citemacro_citep">(Hendy et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>; Jiao
et al., <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Jiao
et al. (<a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite> re-evaluated the performance with launch of GPT-4 (March 15, 2023) observing that the performance of ChatGPT based on GPT-4 became more comparable to commercial translation systems for more distant languages.<span class="ltx_note ltx_role_endnote" id="endnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote15.1.1.1"><sup class="ltx_sup" id="endnote15.1.1.1.1">15</sup></span></span>Similar to ’low-resource languages’ the terms ’distant languages’ is not very well-defined. In the paper itself, they refer to ‘European’ vs ‘Asian’ languages.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Wang
et al. (<a class="ltx_ref" href="#bib.bib44" title="">2023</a>)</cite> compare commercial MT systems (Google Translate<span class="ltx_note ltx_role_endnote" id="endnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote16.1.1.1"><sup class="ltx_sup" id="endnote16.1.1.1.1">16</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span></span></span>, DeepL<span class="ltx_note ltx_role_endnote" id="endnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote17.1.1.1"><sup class="ltx_sup" id="endnote17.1.1.1.1">17</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.deepl.com/en/translator" title="">https://www.deepl.com/en/translator</a></span></span></span>, and Tencent TranSmart<span class="ltx_note ltx_role_endnote" id="endnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote18.1.1.1"><sup class="ltx_sup" id="endnote18.1.1.1.1">18</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://transmart.qq.com/" title="">https://transmart.qq.com/</a></span></span></span>) and document-level NMT models with GPT models and focus particularly on evaluating their discourse awareness through automatic and human evaluations. They conclude that ChatGPT outperforms the commercial MT systems according to human evaluation and is thus a potentially promising avenue to explore for document-level translation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Rather than concentrating on a direct comparison between existing MT systems and GPT models, other research <cite class="ltx_cite ltx_citemacro_citep">(Peng et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite> has explored how to effectively harness the capabilities and potential of GPT models in the context of translation. These investigations seek to optimize and maximize the benefits that GPT models can offer for improving translation quality and addressing specific translation challenges, for instance by exploring different prompting strategies or the possibility for personalized MT.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">In <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>, ChatGPT is evaluated and compared to Google Translate with an emphasis on how to maximize the potential of ChatGPT. Their findings highlight the importance of clarifying the task information. They find that ChatGPT performs better when you make the task at hand very clear by stating that you are expecting it to serve as an MT system in a particular domain. The temperature, a (hyper)parameter that can be set to control the level of randomness/unpredictibility in the texts it generates, has a relatively small impact when dealing with high-resource languages (i.e. languages for which we generally have a large amount of linguistic resources, e.g. German or French). Additionally, they conduct a ’chain of thought’ experiment where ChatGPT is prompted to not only translate but also explain its ‘thoughts’ while doing so. The chain-of-thought experiments, however, led to word-by-word translation, thus degradeding the overall translation quality. It is important to highlight here that their evaluation consisted only of automatic evaluation metrics (Comet <cite class="ltx_cite ltx_citemacro_citep">(Rei
et al., <a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite> and SacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>. In contrast with the findings presented in <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>, chain-of-thought experiments <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib45" title="">2022</a>)</cite> combined with error analysis <cite class="ltx_cite ltx_citemacro_citep">(Lu
et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>)</cite>, an approach coined <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">error analysis prompting</span>, can lead to human-like MT evaluation at both the system and segment level <cite class="ltx_cite ltx_citemacro_citep">(Lu
et al., <a class="ltx_ref" href="#bib.bib20" title="">2023</a>)</cite>. When prompted to handle multiple translation in one query, they observed unstable scorings and biases. Aside from automatic evaluation metrics (COMET, BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni
et al., <a class="ltx_ref" href="#bib.bib22" title="">2002</a>)</cite>, BERTscore <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib46" title="">2019</a>)</cite> and BLEURT<cite class="ltx_cite ltx_citemacro_citep">(Sellam
et al., <a class="ltx_ref" href="#bib.bib31" title="">2020</a>)</cite>), they refer to a ‘human evaluation’ which, in reality, consists of an evaluation based on a high-quality dataset annotated by humans (and thus not an actual evaluation conducted by humans). Their findings should be considered preliminary given that they sample only 10 segments from each group they investigate (groups are based on the number of tokens) for Chinese–English, English–German and their reliance on automatic evaluation metrics.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">Two other papers investigate the usefulness of additional linguistic information integrated in the prompts fed to ChatGPT. <cite class="ltx_cite ltx_citemacro_cite">Gu (<a class="ltx_ref" href="#bib.bib15" title="">2023</a>)</cite> investigate a two-step prompt strategy where they inject linguistic knowledge into the prompt designed to aid MT. In <cite class="ltx_cite ltx_citemacro_cite">Gao
et al. (<a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>, aside from explicitly mentioning the translation task information (e.g. domain), POS-tags are injected. While these are potentially interesting avenues to explore from an academic point of view in order to explore GPT models’ full potential as an MT system, we can question to what extent it is realistic to explore all possible prompts for various language pairs/settings and effectively expect MT users to employ those strategies.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">While the aforementioned research focused on the quality and potential of GPT models as MT systems, they did not particularly look into its ability to handle specific cross-linguistic phenomenon or biases. In <cite class="ltx_cite ltx_citemacro_cite">Ghosh and
Caliskan (<a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite>, the work most closely related to ours, they examine whether ChatGPT (based on GPT-3) perpetuates gender bias across Bengali and five other low-resource languages: Farsi, Malay, Tagalog, Thai and Turkish. All the investigated languages, aside from English, use gender-neutral pronouns. This implies that, when translating from one of those languages into English, the gender-neutral pronoun can be translated as either <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.1">he</span> or <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.2">she</span> (or gender-neutral <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.3">they</span>). In this translation setup, it becomes apparent that ChatGPT perpetuates and amplifies biases related to gender roles and stereotypes, in terms of actions (e.g. <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.4">‘cooking’</span> is a female activity) and occupations (e.g. <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.5">‘doctor’</span> is a male occupation), when translating gender-neutral pronouns into English. Aside from that, in the context of occupations, it confers more respect to men than to women.<span class="ltx_note ltx_role_endnote" id="endnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote19.1.1.1"><sup class="ltx_sup" id="endnote19.1.1.1.1">19</sup></span></span>Bengali has two pronouns, one of which is the more respectful form and they found it to be used mainly in sentences featuring a male pronoun.</span></span></span> Vice versa, when translating from English, the gender-neutral pronoun <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.6">they</span> could easily be translated into the equivalent gender-neutral pronoun in the other languages, however, their experiments show that currently, ChatGPT is unable to do so.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1"><cite class="ltx_cite ltx_citemacro_cite">Ghosh and
Caliskan (<a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite> furthermore rightfully stress that, while other MT providers have been criticized for the biased outputs produced by their MT systems, OpenAI has claimed to have put extensive bias mitigation measures in place. Yet, the experiments seem to indicate that the same gender biases that have been observed and criticized in commercial systems (e.g. Google Translate or MS Translator<span class="ltx_note ltx_role_endnote" id="endnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote20.1.1.1"><sup class="ltx_sup" id="endnote20.1.1.1.1">20</sup></span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/nl-nl/translator/" title="">https://www.microsoft.com/nl-nl/translator/</a></span></span></span>) persist. Given the popularity of ChatGPT, this warrants further investigation and the authors vouch for a more human-centered approach. While our experiments are still preliminary, we aim to contribute to the investigation of gender biases in ChatGPT used for MT.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Preliminary Experiment and Analysis</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this preliminary experiment, we explore how and to what extent ChatGPT can handle gender across the English-Italian Language pair. While English is a language with pronominal gender, where only the pronouns are gendered<span class="ltx_note ltx_role_endnote" id="endnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote21.1.1.1"><sup class="ltx_sup" id="endnote21.1.1.1.1">21</sup></span></span>A few exceptions are nouns such as <span class="ltx_text ltx_font_italic" id="endnote21.2">’waitress’</span> vs <span class="ltx_text ltx_font_italic" id="endnote21.3">’waiter’</span> or <span class="ltx_text ltx_font_italic" id="endnote21.4">‘actress’</span> vs <span class="ltx_text ltx_font_italic" id="endnote21.5">‘actor’</span>.</span></span></span>, Italian is a more highly inflected language where different categories (nouns, adjectives…) agree in gender with their referent. Additionally, as opposed to English, Italian is a language with grammatical gender meaning that also inanimate nouns are assigned to a gender category. The example below is one taken from the challenge set used for our experiment and it provides a translation for an English gender-neutral sentence into Italian, where the past participle <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">‘sentito/a/<span class="ltx_ERROR undefined" id="S4.p1.1.1.1">\textschwa</span>[‘to feel’]</span> agrees with the gender of its subject <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">you</span>.<span class="ltx_note ltx_role_endnote" id="endnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote22.1.1.1"><sup class="ltx_sup" id="endnote22.1.1.1.1">22</sup></span></span>We added a version that is not included in gENder-IT using the ‘<span class="ltx_ERROR undefined" id="endnote22.2">\textschwa</span>’ which has been recommended by <cite class="ltx_cite ltx_citemacro_cite">Baiocco
et al. (<a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite> as a non-gender-specific ending for Italian.</span></span></span></p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">English
<br class="ltx_break"/></span><span class="ltx_text ltx_font_italic" id="S4.p2.1.2">‘What did <span class="ltx_text ltx_font_bold" id="S4.p2.1.2.1">you</span> expect it to feel like?</span></p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Italian
<br class="ltx_break"/></span><span class="ltx_text ltx_font_italic" id="S4.p3.1.2">‘Como pensavi che ti saresti <span class="ltx_text ltx_font_bold" id="S4.p3.1.2.1">sentito</span>’</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.p3.1.3">‘Como pensavi che ti saresti <span class="ltx_text ltx_font_bold" id="S4.p3.1.3.1">sentita</span>’</span>
<span class="ltx_text ltx_font_italic" id="S4.p3.1.4">‘Como pensavi che ti saresti <span class="ltx_text ltx_font_bold" id="S4.p3.1.4.1">sentit<span class="ltx_ERROR undefined" id="S4.p3.1.4.1.1">\textschwa</span></span>’</span></p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The following sections describe the experimental setup, datasets and promps along with the error analysis conducted. For the experiments, we accessed<span class="ltx_note ltx_role_endnote" id="endnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote23.1.1.1"><sup class="ltx_sup" id="endnote23.1.1.1.1">23</sup></span></span>We accessed ChatGPT to generated these translations between May 30, 2023 and June 5, 2023.</span></span></span> ChatGPT (based on GPT-3.5) through the API. Given that the related work on using GPT models for MT highlighted the importance of (explicit) prompting, we looked at two different prompting scenarios.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.10">We used a subset of an existing challenge dataset, gENder-IT <cite class="ltx_cite ltx_citemacro_citep">(Vanmassenhove and
Monti, <a class="ltx_ref" href="#bib.bib39" title="">2021</a>)</cite>, designed specifically for resolving natural gender phenomena in translation contexts. The challenge set is a word-level (human) annotated, adapted and cleaned version of a subset of the MuST-SHE corpus <cite class="ltx_cite ltx_citemacro_citep">(Bentivogli et al., <a class="ltx_ref" href="#bib.bib3" title="">2020</a>)</cite>, but unlike MuST-SHE which is annotated on the paragraph level, the sentences have been split and annotated on the word-level with <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mo id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><lt id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">&lt;</annotation></semantics></math>F<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.2.m2.1"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mo id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><gt id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.2.m2.1d">&gt;</annotation></semantics></math> (female), <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.3.m3.1"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><mo id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.1b"><lt id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.3.m3.1d">&lt;</annotation></semantics></math>M<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.4.m4.1"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><mo id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><gt id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.4.m4.1d">&gt;</annotation></semantics></math> (male) or <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.5.m5.1"><semantics id="S4.SS1.SSS1.p1.5.m5.1a"><mo id="S4.SS1.SSS1.p1.5.m5.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.5.m5.1b"><lt id="S4.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.5.m5.1d">&lt;</annotation></semantics></math>A<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.6.m6.1"><semantics id="S4.SS1.SSS1.p1.6.m6.1a"><mo id="S4.SS1.SSS1.p1.6.m6.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.6.m6.1b"><gt id="S4.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.6.m6.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.6.m6.1d">&gt;</annotation></semantics></math> (ambiguous) tags. It focuses on sentences containing textually ambiguous words (in terms of gender) in the English source and provides the possible translation alternatives (in terms of binary gender)<span class="ltx_note ltx_role_endnote" id="endnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_note_type">endnote: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="endnote24.1.1.1"><sup class="ltx_sup" id="endnote24.1.1.1.1">24</sup></span></span>There is no crystallized approach to gender-neutral endings in Italian as to date.</span></span></span> on the Italian target side. Two example of tagged sentences from the gENder-IT corpus are provided below for illustration. The first one, Example <a class="ltx_ref" href="#S4.I1.i1" title="1 ‣ 4.1.1 Dataset ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, only contains &lt;F&gt; tags since the gender of ‘ambiguous’ words such as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.10.1">‘I’</span> can be determined based on sentence context. Example <a class="ltx_ref" href="#S4.I1.i2" title="2 ‣ 4.1.1 Dataset ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, is one where within the sentence context, no assumption can or should be made with respect to the gender of the referent. To indicate that not all ambiguous words have the same referent, numbers are appended (e.g. <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.7.m7.1"><semantics id="S4.SS1.SSS1.p1.7.m7.1a"><mo id="S4.SS1.SSS1.p1.7.m7.1.1" xref="S4.SS1.SSS1.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.7.m7.1b"><lt id="S4.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.7.m7.1d">&lt;</annotation></semantics></math>A1<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.8.m8.1"><semantics id="S4.SS1.SSS1.p1.8.m8.1a"><mo id="S4.SS1.SSS1.p1.8.m8.1.1" xref="S4.SS1.SSS1.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.8.m8.1b"><gt id="S4.SS1.SSS1.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.8.m8.1d">&gt;</annotation></semantics></math>, <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.9.m9.1"><semantics id="S4.SS1.SSS1.p1.9.m9.1a"><mo id="S4.SS1.SSS1.p1.9.m9.1.1" xref="S4.SS1.SSS1.p1.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.9.m9.1b"><lt id="S4.SS1.SSS1.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS1.p1.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.9.m9.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.9.m9.1d">&lt;</annotation></semantics></math>A2<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.10.m10.1"><semantics id="S4.SS1.SSS1.p1.10.m10.1a"><mo id="S4.SS1.SSS1.p1.10.m10.1.1" xref="S4.SS1.SSS1.p1.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.10.m10.1b"><gt id="S4.SS1.SSS1.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS1.p1.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.10.m10.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.10.m10.1d">&gt;</annotation></semantics></math>…). We used a subset of gENder-IT for the manual error analysis consisting for which we used the first 50 sentences of the gENderIT corpus.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">“So she turned and she looked at her dad, and she said, “Dad, <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1.1">I &lt;F&gt;</span> know how <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1.2">you &lt;M&gt;</span> feel, but <span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1.3">I &lt;F&gt;</span> don’t believe in the death penalty.”’</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">‘And it was there that another <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.1">nurse &lt;A1&gt;</span>, not the <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.2">nurse &lt;A2&gt;</span> <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.3">who &lt;A2&gt;</span> was looking after Mrs. <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.4">Drucker &lt;F&gt;</span> before, but another <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.5">nurse &lt;A1&gt;</span>, said three words to <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.6">me &lt;A3&gt;</span> that are the three words that most emergency <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.7">physicians &lt;A4&gt;</span> <span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1.8">I &lt;A3&gt;</span> know dread.’</span></p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Prompting Scenarios</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">GPT’s autoregressive approach allows it to produce contextually relevant responses which allow for more engaging interactions or requests as opposed to the conventional MT systems. This combined with the fact that previous research highlighted the importance of explicit prompting, we conducted the experiments with two different prompting scenarios. Furthermore, given that the research by <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite> showed an improvement in performance for MT when prepending information stating the task at hand, in both scenarios, we entered the following prompt before generating any translations: <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">‘You are a Machine translation system’</span>. We generated the translations one by one in both cases. In the first one, we simply prompted ChatGPT to translate sentences containing an ambiguous pronoun or noun into Italian. In the second one, we explicitly prompted it to provide all the possible alternatives in terms of gender.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Prompt Scenario 1</span> <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.2">‘Can you translate the following sentence into Italian: + [insert English sentence]’</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Prompt Scenario 2</span> <span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.2">‘Can you translate the following sentence into Italian providing all the possible alternatives in terms of gender + [insert English sentence]’</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.6">Scenario 1 is probably closer to how an MT-user would prompt ChatGPT since they are not necessarily aware of the fact that the target language might differ from the source in terms of gender marking. It would also give us some insight into how gender is handled ‘by default’. Ideally, ChatGPT would provide alternative translation even when not explicitly prompted to do so. Scenario 2 explicitly asks for alternative translation in terms of gender. In this scenario we hope to explore what the potential of GPT models is with respect to handling gender phenomena across languages. We ought to note that the tags (<math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.1.m1.1"><semantics id="S4.SS1.SSS2.p3.1.m1.1a"><mo id="S4.SS1.SSS2.p3.1.m1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.1.m1.1b"><lt id="S4.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.1.m1.1d">&lt;</annotation></semantics></math>F<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.2.m2.1"><semantics id="S4.SS1.SSS2.p3.2.m2.1a"><mo id="S4.SS1.SSS2.p3.2.m2.1.1" xref="S4.SS1.SSS2.p3.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.2.m2.1b"><gt id="S4.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p3.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.2.m2.1d">&gt;</annotation></semantics></math>, <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.3.m3.1"><semantics id="S4.SS1.SSS2.p3.3.m3.1a"><mo id="S4.SS1.SSS2.p3.3.m3.1.1" xref="S4.SS1.SSS2.p3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.3.m3.1b"><lt id="S4.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.3.m3.1d">&lt;</annotation></semantics></math>M<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.4.m4.1"><semantics id="S4.SS1.SSS2.p3.4.m4.1a"><mo id="S4.SS1.SSS2.p3.4.m4.1.1" xref="S4.SS1.SSS2.p3.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.4.m4.1b"><gt id="S4.SS1.SSS2.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p3.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.4.m4.1d">&gt;</annotation></semantics></math> and <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.5.m5.1"><semantics id="S4.SS1.SSS2.p3.5.m5.1a"><mo id="S4.SS1.SSS2.p3.5.m5.1.1" xref="S4.SS1.SSS2.p3.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.5.m5.1b"><lt id="S4.SS1.SSS2.p3.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p3.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.5.m5.1d">&lt;</annotation></semantics></math>A<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.6.m6.1"><semantics id="S4.SS1.SSS2.p3.6.m6.1a"><mo id="S4.SS1.SSS2.p3.6.m6.1.1" xref="S4.SS1.SSS2.p3.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.6.m6.1b"><gt id="S4.SS1.SSS2.p3.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p3.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.6.m6.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.6.m6.1d">&gt;</annotation></semantics></math>) with which sentences are annotated in the gENder-IT challenge set were removed from the English sentences for the prompt. For convenience, we added the tags to the examples provided in the subsequent section.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Manual error analysis</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We conducted a manual analysis/evaluation of the translations generated by ChatGPT for both scenarios.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Prompt Scenario 1</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">For Scenario 1, the one where the prompt did not explicitly state that alternatives are to be generated, only 5 out of the 50 sentences did not miss valid alternatives in terms of gender. Out of those 5, three times ChatGPT had provided both alternatives (see Example <a class="ltx_ref" href="#S4.I3.i1" title="1 ‣ 4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p1.1.1">‘voltato/a’</span>) in the translation and two times there were no missing alternatives since the translation differed from the original one in such a way that the translation did not contain any forms marking the gender (see Example <a class="ltx_ref" href="#S4.I3.i1" title="1 ‣ 4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I3.i1.p1.1.2">“So I &lt;A&gt; turned around and I &lt;A&gt; looked at her, and I &lt;A&gt; said, "Are you &lt;F&gt; gonna wear this?"”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.3">IT</span> <span class="ltx_text ltx_font_italic" id="S4.I3.i1.p1.1.4">“Mi sono <span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.4.1">voltato/a</span> e l’ho guardata dicendo: ""Lo indosserai?"”</span>
</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.2">“What did you &lt;A&gt; expect it to feel like?”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.3">IT</span> <span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.4">“Cosa ti aspettavi che si sentisse?”</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">For all other 45 sentences, there were either errors or valid translation alternatives in terms of gender missing. Two sentences contained errors with respect to gender in the generated translation. In Example <a class="ltx_ref" href="#S4.I4.i1" title="1 ‣ 4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, it is clear from the sentential context that <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.1">‘classmate’</span> refers to someone with a female gender. Nevertheless, it is translated as <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.2">‘compagno [masculine]’</span> in the Italian translation generated by ChatGPT. Similarly, Example <a class="ltx_ref" href="#S4.I4.i2" title="2 ‣ 4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows how <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.3">child</span> which should be translated into the feminine form according to the given context, is translated into Italian as <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.4">‘figlio’</span> which means <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.5">‘son’</span>. Note that in both cases, the error occurred due to the fact that a masculine form was generated while the correct form was feminine.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<ol class="ltx_enumerate" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.2">“A college <span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.2.1">classmate &lt;F&gt;</span> of mine wrote me &lt;A&gt; a couple weeks ago and said she thought I &lt;A&gt; was a little strident.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.3">IT</span> <span class="ltx_text ltx_font_italic" id="S4.I4.i1.p1.1.4">“Un <span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.4.1">compagno</span> di classe del college mi ha scritto un paio di settimane fa dicendo che pensava che fossi un po’ stridente.”</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.2">“I &lt;A1&gt; knew that if in one hour, <span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.2.1">our child &lt;F&gt;</span> didn’t come, I &lt;A1&gt; was going to leave her there and go back to work and make a presentation about AI to my <span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.2.2"> boss &lt;A2&gt;</span>, Apple’s CEO &lt;A2&gt;.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.3">IT</span> <span class="ltx_text ltx_font_italic" id="S4.I4.i2.p1.1.4">“Sapevo che se <span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.4.1">nostro figlio</span> non fosse arrivato entro un’ora, l’avrei lasciata lì e sarei tornato al lavoro per fare una presentazione sull’IA <span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.4.2">al mio capo</span>, il CEO di Apple.”</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">In the remaining 43 sentences there was at least one alternative translation missing. To illustrate that, we can once more refer back to Example <a class="ltx_ref" href="#S4.I4.i2" title="2 ‣ 4.2.1 Prompt Scenario 1 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> where <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.1">‘boss’</span> in English could be translated into Italian as <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.2">capa [feminine]</span> or <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.3">capo [masculine]</span>, yet only the masculine alternative was provided. When only considering the sentences with two alternative translations, in the vast majority of the cases, the feminine alternatives were lacking with the exception to a few sentences containing the noun ‘nurse’, ‘secretary’ and names stereotypically associates with the female gender (e.g. ‘Vera’). In the gENder-IT challenge set, however, they opted to not assign genders based solely on the names because of potential cultural biases - which all names are considered ‘ambiguous’ unless a full name is provided and the gender of the person could be retrieved through a Wikipedia page (i.e. public/historical figures).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Prompt Scenario 2</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">For Scenario 2, ChatGPT was prompted to translate the same 50 sentences, the only difference being that this time it was explicitly stated that it should generate all possible alternatives in terms of gender for the translation. Out of the 50 sentences, 16 of the translations were entirely correct with no missing alternatives. For half of the sentences, valid alternatives were missing and in 17 sentences there were erroneous translations (in terms of gender). In this scenario, all the alternatives missing were alternatives containing feminine forms. For instance, in <a class="ltx_ref" href="#S4.I5.i1" title="1 ‣ 4.2.2 Prompt Scenario 2 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> only one translation is generated while <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">‘pediatrician’</span> is ambiguous in the source context and could be translated into the feminine or masculine form in Italian (<span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.2">dal pediatra [masculine]</span> or <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.3">dalla pediatra [feminine]</span>) yet only the masculine alternative is provided. Note as well how the proper noun ‘Sarah’ is translated as ‘Laura’.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<ol class="ltx_enumerate" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I5.i1.p1.1.2">“And when Elliot was first born, we &lt;A1&gt; took her to the <span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.2.1">pediatrician &lt;A2&gt;</span> and we met our first nurse &lt;F&gt; — we’ll call her Sarah.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.3">IT</span> <span class="ltx_text ltx_font_italic" id="S4.I5.i1.p1.1.4">“E quando Elliot è nata per la prima volta, l’abbiamo portata <span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.4.1">dal pediatra</span> e abbiamo incontrato la nostra prima infermiera — la chiameremo Laura.”</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Arguably more problematic than the missing alternatives, were the alternatives that were erroneous since they did not stay true to the source. In Example <a class="ltx_ref" href="#S4.I6.i1" title="1 ‣ 4.2.2 Prompt Scenario 2 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, four alternative translations were generated given the English source sentence. While the first alternative stays true to the source by translating <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.1">‘mother’</span> into <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.2">‘madre’</span>, in the subsequent alternatives it is erroneously translated into the Italian as <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.3">‘padre’</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.4">‘fratello’</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.5">‘sorella’</span> which mean <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.6">‘father’</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.7">‘brother’</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.8">‘sister’</span> in English.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<ol class="ltx_enumerate" id="S4.I6">
<li class="ltx_item" id="S4.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I6.i1.p1">
<p class="ltx_p" id="S4.I6.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I6.i1.p1.1.2">“My <span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.2.1">mother</span> had been left behind in the camp, and I &lt;A1&gt; had not seen her since I &lt;A1&gt; left it &lt;A2&gt;.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.3">IT</span>: Alternative 1 <span class="ltx_text ltx_font_italic" id="S4.I6.i1.p1.1.4">“Mia <span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.4.1">madre</span> era stata lasciata indietro nel campo e non l’avevo più vista da quando l’avevo lasciato.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.5">IT</span>: Alternative 2 <span class="ltx_text ltx_font_italic" id="S4.I6.i1.p1.1.6">“Mio <span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.6.1">padre</span> era stato lasciato indietro nel campo e non l’avevo più visto da quando l’avevo lasciato.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.7">IT</span>: Alternative 3 <span class="ltx_text ltx_font_italic" id="S4.I6.i1.p1.1.8">“Mio <span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.8.1">fratello</span> era stato lasciato indietro nel campo e non l’avevo più visto da quando l’avevo lasciato.”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.9">IT</span>: Alternative 4 <span class="ltx_text ltx_font_italic" id="S4.I6.i1.p1.1.10">“Mia <span class="ltx_text ltx_font_bold" id="S4.I6.i1.p1.1.10.1">sorella</span> era stata lasciata indietro nel campo e non l’avevo più vista da quando l’avevo lasciata.”</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1">Similarly, in Example <a class="ltx_ref" href="#S4.I7.i1" title="1 ‣ 4.2.2 Prompt Scenario 2 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, ChatGPT manages to correctly provide two alternatives for the gender-neutral English <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p5.1.1">psychiatrist</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p5.1.2">un/una psichiatra</span>. However, it then proceeds by providing two additional inaccurate alternatives where <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p5.1.3">psychiatrist</span> is translated as <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p5.1.4">ginecologo</span> (<span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p5.1.5">gynaecologist</span>).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p6">
<ol class="ltx_enumerate" id="S4.I7">
<li class="ltx_item" id="S4.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S4.I7.i1.p1">
<p class="ltx_p" id="S4.I7.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.1">EN</span> <span class="ltx_text ltx_font_italic" id="S4.I7.i1.p1.1.2">“I’m a psychiatrist &lt;A&gt; who &lt;A&gt; works with pregnant and postpartum women…”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.3">IT</span>: Alternative 1 <span class="ltx_text ltx_font_italic" id="S4.I7.i1.p1.1.4">“Sono <span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.4.1">uno psichiatra</span> che lavora con donne in gravidanza e nel periodo postpartum…”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.5">IT</span>: Alternative 2 <span class="ltx_text ltx_font_italic" id="S4.I7.i1.p1.1.6">“Sono <span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.6.1">una psichiatra</span> che lavora con donne in gravidanza e nel periodo postpartum…”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.7">IT</span>: Alternative 3 <span class="ltx_text ltx_font_italic" id="S4.I7.i1.p1.1.8">“Sono <span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.8.1">un ginecologo</span> che lavora con donne in gravidanza e nel periodo postpartum…”</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.9">IT</span>: Alternative 4 <span class="ltx_text ltx_font_italic" id="S4.I7.i1.p1.1.10">“Sono <span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.10.1">un ginecologo</span> che lavora con donne in gravidanza e nel periodo postpartum…”</span></p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Findings &amp; Implications</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our preliminary findings on the small challenge set are in line with the findings of <cite class="ltx_cite ltx_citemacro_cite">Ghosh and
Caliskan (<a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite>. Our analysis, of both prompting scenarios, indicates a (strong) male bias which seems to become even more prevalent when asked explicitly to generate alternatives. While ChatGPT has some potential when it comes to generating alternatives when prompted explicitly, it currently cannot handle gender in a systematic manner. Given the current hype surrounding LLMs and their potential, we deem it important to raise awareness among researchers and potential users of these technologies. We will summarize the main findings regarding the model’s handling of gender and its potential implications.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">First of all, in both the Scenario 1 and 2, the analysis revealed serious shortcomings in ChatGPT’s ability to generate multiple possible translations in terms of gender. In Scenario 1, only a very small proportion of the translated sentences contained all binary gender translation alternatives. In Scenario 2, there were numerous instances where feminine gender alternatives were entirely missing. Furthermore, while in Italian, there is no crystallized approach yet to gender-neutral language, gender-neutral markers such as the asterisk or the schwa are used by the transgender Italian community. Such endings or markers were never provided or suggested by the model. Its inability to account for gender alternatives when not explicitly prompted to do so is already problematic as it demonstrates a lack of sensitivity to gender-inclusive language. However, the absence of gender-neutral and feminine alternatives, especially in Scenario 2, where explicit instructions were provided, reveals an actual inability of the model to handle gender systematically across languages. Both observations are leading to incomplete and biased translations and emphasize the need for ongoing improvements and vigilance. Furthermore, the absence of gender-neutral language markers in the model’s output is a missed opportunity for fostering greater inclusivity. The failure to provide such alternatives reflects a lack of awareness of evolving language norms and the specific needs of different user groups.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Second, in Scenario 2, there were various erroneous translations among the alternatives provided. The model would provide alternatives that deviated from the source text and introduced misleading and incorrect information. These erroneous alternatives were sometimes not (directly) related to gender (Example <a class="ltx_ref" href="#S4.I7.i1" title="1 ‣ 4.2.2 Prompt Scenario 2 ‣ 4.2 Manual error analysis ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>) but often the (feminine) terms were inaccurately replaced with masculine alternatives. These translations not only distort the intended message and thus the accuracy of the translation but also often perpetuate gender biases that are deeply ingrained in our society. Erroneous and biased translations that misrepresent the source content furthermore carry the risk of alienating or excluding individuals who do not conform to traditional gender norms.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Overall, these findings underscore the need for continued research and development to enhance our understanding of LLMs’ handling of gender-related nuances in translation. Like more traditional MT models, ChatGPT struggles with gender (biases) across languages. The ability to explicitly prompt it to handle issues related to bias and its tendency to generate answers that at first sight seem to address the issue appropriately furthermore risks creating a feeling of complacency. It is however crucial to recognise that while LLMs, like GPT3.5 have shown remarkable capabilities in various NLP tasks, they are not immune to biases. While explicit prompting might be a useful technique to alleviate some biases, this does not guarantee consistent bias-free or fully accurate responses. When LLMs generate responses that seem to handle gender bias well on the surface, users might become complacent and assume that the issue has been adequately addressed while underlying biases persist. Vigilance is thus required to assess the generated content critically and carefully. Addressing these issues is not only a matter of linguistic accuracy but also a step towards promoting more inclusive and respectful communication in the digital age. The AI community at large, in collaboration with linguists, translators and a diverse set of users should collaborate to further evaluate these models to eventually minimize bias, and ensure that AI-generated content aligns with evolving societal standards of gender-inclusivity and neutrality.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Limitations</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We would like to discuss some of the limitations of our work. First of all, there is a reproducibility problem that occurs when querying ChatGPT. The results of the same query/prompt may vary across multiple trials and the models underlying the app can be updated at any time. This implies that there is a certain randomness to the evaluation and analysis of the translations. Furthermore, currently, a novel LLM (GPT-4) has been developed and can be accessed through a $20 monthly subscription to ChatGPT. According to <cite class="ltx_cite ltx_citemacro_cite">Jiao
et al. (<a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>, GPT-4 outperforms GPT-3.5 as an MT system which warrants for a replication using GPT-4. Nevertheless, the free version of ChatGPT is still based on GPT-3.5, implying that a large amount of users will still be using the currently investigated model for their daily needs.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Aside from the reproducibility limitations, the gENder-IT corpus does not contain any gender-neutral sentences (e.g. using the gender-neutral pronoun <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p2.1.1">they</span> in the English source). We prompted ChatGPT to generate gender alternatives but did not ask specifically for gender-neutral alternatives. While <cite class="ltx_cite ltx_citemacro_cite">Ghosh and
Caliskan (<a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite> finds that ChatGPT cannot handle the English gender-neutral pronoun, they also did not explicitly prompt it to translate in a gender-neutral way.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In conclusion, this chapter has delved into the critical role of MT in perpetuating and shaping societal biases, particularly gender bias. Within the realm of MT, gender biases are especially pronounced due to the need for disambiguation and explicit representation of gender in specific contrastive linguistic settings. The reliance on biased data and statistical patterns in current MT systems can lead to erroneous disambiguation, resulting in the generation of morphologically incorrect alternatives, reinforcing potentially harmful stereotypes.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We provided an overview of existing work related to gender bias in both conventional neural machine translation (NMT) approaches and the use of GPT models as MT systems. We furthermore covered and explained some of the origins/sources of gender bias we observe in language technology. Given the rising popularity of tools like ChatGPT, it is crucial to raise awareness about these issues and take proactive steps to address them. Our experiments with ChatGPT, based on GPT-3.5, in an English-Italian translation setting showed how GPT models perpetuate biases even when explicitly prompted to provide alternative translation. Surprisingly, prompting for gender alternatives often even led to additional biases in the model’s outputs, highlighting the need for further research and development in this area.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The implications of these findings underscore the importance of ongoing research and improvements to promote fair and unbiased translations. We advocate for more hybridization in both technology and collaboration among researchers from various disciplines, including computational linguists, computer scientists, sociolinguists, and ethicists. Such multidisciplinary efforts are essential to address the complex and multifaceted nature of gender bias in MT. We acknowledge that our experiments have been conducted with a relatively small dataset. To gain a deeper understanding of GPT’s ability to deal with gender across languages and to expand the scope of our research, a more detailed analysis with a larger set of sentences across multiple language pairs should be conducted in future work to assess the model’s effectiveness across different linguistic contexts.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Looking ahead, an important area of future work is to investigate how GPT models can be used to generate gender-inclusive datasets. This involves exploring ways to integrate linguistic knowledge into the model to guide the generation of gender alternatives, including those beyond the binary. One potential approach could be to provide tags, similar to those used in the gENder-IT challenge, to influence the model’s output and encourage gender-inclusive translations. As the use of GPT models and other language generation technologies continues to grow, it becomes increasingly vital to ensure that these systems are equipped to handle gender and promote inclusivity. Our research aims to contribute to the ongoing efforts to make MT systems more equitable and sensitive to issues of gender representation.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In closing, this chapter has shed light on the challenges posed by gender bias in MT. It emphasizes the need for continued multidisciplinary research, advancements, and collaborative endeavors to pave the way towards more equitable and unbiased language technologies. As the field progresses, we hope to see the transformation of MT systems into powerful tools that promote diversity, inclusivity, and societal progress.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">I would like to thank Esther Monzó-Nebot for setting up a collaborative initiative gathering expertise from linguistics, computer science and law in order to discuss some of the more pressing issues regarding translation technology. Additionally, I would like to thank Deborah Giustini, Heidi Salaets and Maurizio Veglio for sharing their expertise and broadening my knowledge across diverse fields.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baiocco
et al. (2023)</span>
<span class="ltx_bibblock">
Baiocco, R., F. Rosati, and J. Pistella (2023).

</span>
<span class="ltx_bibblock">Italian proposal for non-binary and inclusive language: The schwa as
a non-gender–specific ending.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Journal of Gay &amp; Lesbian Mental Health</span>, 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bau et al. (2019)</span>
<span class="ltx_bibblock">
Bau, A., Y. Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass (2019,
May).

</span>
<span class="ltx_bibblock">Identifying and Controlling Important Neurons in Neural Machine
Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Seventh International Conference on
Learning Representations (ICLR)</span>, New Orleans, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bentivogli et al. (2020)</span>
<span class="ltx_bibblock">
Bentivogli, L., B. Savoldi, M. Negri, M. A. Di Gangi, R. Cattoni, and M. Turchi
(2020, July).

</span>
<span class="ltx_bibblock">Gender in danger? evaluating speech translation technology on the
MuST-SHE corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, Online, pp.  6923–6933. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane (2021)</span>
<span class="ltx_bibblock">
Birhane, A. (2021).

</span>
<span class="ltx_bibblock">Algorithmic injustice: a relational ethics approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Patterns</span> <span class="ltx_text ltx_font_italic" id="bib.bib4.2.2">2</span>(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al. (2016)</span>
<span class="ltx_bibblock">
Bolukbasi, T., K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai (2016,
December).

</span>
<span class="ltx_bibblock">Man is to Computer Programmer as Woman is to Homemaker? Debiasing
Word Embeddings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of Thirtieth Conference on Neural Information
Processing Systems (NIPS)</span>, Barcelona, Spain, pp.  4349–4357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caliskan et al. (2022)</span>
<span class="ltx_bibblock">
Caliskan, A., P. P. Ajay, T. Charlesworth, R. Wolfe, and M. R. Banaji (2022).

</span>
<span class="ltx_bibblock">Gender bias in word embeddings: a comprehensive analysis of
frequency, syntax, and semantics.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,
and Society</span>, pp.  156–170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho
et al. (2019)</span>
<span class="ltx_bibblock">
Cho, W. I., J. W. Kim, S. M. Kim, and N. S. Kim (2019, August).

</span>
<span class="ltx_bibblock">On measuring gender bias in translation of gender-neutral pronouns.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the First Workshop on Gender Bias in Natural
Language Processing</span>, Florence, Italy, pp.  173–181. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà
et al. (2022)</span>
<span class="ltx_bibblock">
Costa-jussà, M. R., C. Escolano, C. Basta, J. Ferrando, R. Batlle, and
K. Kharitonova (2022).

</span>
<span class="ltx_bibblock">Interpreting gender bias in neural machine translation: Multilingual
architecture matters.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, Volume 36, pp.  11855–11863.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin
et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova (2019).

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, (NAACL-HLT 2019)</span>, Volume 1 (Long and Short Papers),
Minneapolis, Minnesota, USA, pp.  4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Font and
Costa-jussà (2019)</span>
<span class="ltx_bibblock">
Font, J. E. and M. R. Costa-jussà (2019, August).

</span>
<span class="ltx_bibblock">Equalizing Gender Biases in Neural Machine Translation with Word
Embeddings Techniques.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph" id="bib.bib10.1.1">To appear in</em><span class="ltx_text ltx_font_italic" id="bib.bib10.2.2"> Proceedings of the 1st ACL Workshop on
Gender Bias for Natural Language Processing</span>, Florence, Italy.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao
et al. (2023)</span>
<span class="ltx_bibblock">
Gao, Y., R. Wang, and F. Hou (2023).

</span>
<span class="ltx_bibblock">How to design translation prompts for chatgpt: An empirical study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv e-prints</span>, arXiv–2304.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh and
Caliskan (2023)</span>
<span class="ltx_bibblock">
Ghosh, S. and A. Caliskan (2023).

</span>
<span class="ltx_bibblock">Chatgpt perpetuates gender bias in machine translation and ignores
non-gendered pronouns: Findings across bengali and five other low-resource
languages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2305.10510</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldfarb-Tarrant et al. (2021)</span>
<span class="ltx_bibblock">
Goldfarb-Tarrant, S., R. Marchant, R. M. Sánchez, M. Pandya, and A. Lopez
(2021).

</span>
<span class="ltx_bibblock">Intrinsic bias metrics do not correlate with application bias.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pp.  1926–1940.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonen and
Goldberg (2019)</span>
<span class="ltx_bibblock">
Gonen, H. and Y. Goldberg (2019).

</span>
<span class="ltx_bibblock">Lipstick on a pig: Debiasing methods cover up systematic gender
biases in word embeddings but do not remove them.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pp.  609–614.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu (2023)</span>
<span class="ltx_bibblock">
Gu, W. (2023).

</span>
<span class="ltx_bibblock">Linguistically informed chatgpt prompts to enhance japanese-chinese
machine translation: A case study on attributive clauses.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2303.15587</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et al. (2023)</span>
<span class="ltx_bibblock">
Hendy, A., M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Matsushita, Y. J.
Kim, M. Afify, and H. H. Awadalla (2023).

</span>
<span class="ltx_bibblock">How good are gpt models at machine translation? a comprehensive
evaluation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2302.09210</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao
et al. (2023)</span>
<span class="ltx_bibblock">
Jiao, W., W. Wang, J.-t. Huang, X. Wang, and Z. Tu (2023).

</span>
<span class="ltx_bibblock">Is chatgpt a good translator? a preliminary study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2301.08745</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
Koehn, P. (2005, September).

</span>
<span class="ltx_bibblock">Europarl: A Parallel Corpus for Statistical Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of The Tenth Machine Translation Summit (MT
Summit 2005)</span>, Phuket, Thailand, pp.  79–86.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu
et al. (2022)</span>
<span class="ltx_bibblock">
Lu, Q., L. Ding, L. Xie, K. Zhang, D. F. Wong, and D. Tao (2022).

</span>
<span class="ltx_bibblock">Toward human-like evaluation for natural language generation with
error analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2212.10179</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu
et al. (2023)</span>
<span class="ltx_bibblock">
Lu, Q., B. Qiu, L. Ding, L. Xie, and D. Tao (2023).

</span>
<span class="ltx_bibblock">Error analysis prompting enables human-like translation evaluation in
large language models: A case study on chatgpt.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2303.13809</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moryossef
et al. (2019)</span>
<span class="ltx_bibblock">
Moryossef, A., R. Aharoni, and Y. Goldberg (2019, August).

</span>
<span class="ltx_bibblock">Filling Gender &amp; Number Gaps in Neural Machine Translation with
Black-box Context Injection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph" id="bib.bib21.1.1">TO APPEAR IN</em><span class="ltx_text ltx_font_italic" id="bib.bib21.2.2"> 1st ACL Workshop on Gender Bias for
Natural Language Processing</span>, Florence, Italy.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni
et al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu (2002, July).

</span>
<span class="ltx_bibblock">BLEU: A Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics (ACL 2002)</span>, Philadelphia, Pennsylvania, USA, pp. 311–318. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Peng, K., L. Ding, Q. Zhong, L. Shen, X. Liu, M. Zhang, Y. Ouyang, and D. Tao
(2023).

</span>
<span class="ltx_bibblock">Towards making the most of chatgpt for machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2303.13780</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petreski and
Hashim (2023)</span>
<span class="ltx_bibblock">
Petreski, D. and I. C. Hashim (2023).

</span>
<span class="ltx_bibblock">Word embeddings are biased. but whose bias are they reflecting?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">AI &amp; SOCIETY</span> <span class="ltx_text ltx_font_italic" id="bib.bib24.2.2">38</span>(2), 975–982.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Post, M. (2018, October).

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</span>, Belgium, Brussels, pp.  186–191. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prates
et al. (2020)</span>
<span class="ltx_bibblock">
Prates, M. O., P. H. Avelar, and L. C. Lamb (2020).

</span>
<span class="ltx_bibblock">Assessing gender bias in machine translation: a case study with
google translate.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Neural Computing and Applications</span> <span class="ltx_text ltx_font_italic" id="bib.bib26.2.2">32</span>, 6363–6381.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei
et al. (2020)</span>
<span class="ltx_bibblock">
Rei, R., C. Stewart, A. C. Farinha, and A. Lavie (2020).

</span>
<span class="ltx_bibblock">Comet: A neural framework for mt evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pp.  2685–2702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rescigno et al. (2020)</span>
<span class="ltx_bibblock">
Rescigno, A. A., E. Vanmassenhove, J. Monti, and A. Way (2020).

</span>
<span class="ltx_bibblock">A case study of natural gender phenomena in translation a comparison
of google translate, bing microsoft translator and deepl for english to
italian, french and spanish.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Computational Linguistics CLiC-it 2020</span> <span class="ltx_text ltx_font_italic" id="bib.bib28.2.2">359</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders
et al. (2020)</span>
<span class="ltx_bibblock">
Saunders, D., R. Sallis, and B. Byrne (2020, December).

</span>
<span class="ltx_bibblock">Neural machine translation doesn’t translate gender coreference
right unless you make it.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Second Workshop on Gender Bias in Natural
Language Processing</span>, Barcelona, Spain (Online), pp.  35–43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savoldi et al. (2021)</span>
<span class="ltx_bibblock">
Savoldi, B., M. Gaido, L. Bentivogli, M. Negri, and M. Turchi (2021).

</span>
<span class="ltx_bibblock">Gender bias in machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Transactions of the Association for Computational
Linguistics</span> <span class="ltx_text ltx_font_italic" id="bib.bib30.2.2">9</span>, 845–874.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam
et al. (2020)</span>
<span class="ltx_bibblock">
Sellam, T., D. Das, and A. P. Parikh (2020).

</span>
<span class="ltx_bibblock">Bleurt: Learning robust metrics for text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of ACL</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stafanovičs et al. (2020)</span>
<span class="ltx_bibblock">
Stafanovičs, A., T. Bergmanis, and M. Pinnis (2020).

</span>
<span class="ltx_bibblock">Mitigating gender bias in machine translation with target gender
annotations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Fifth Conference on Machine Translation</span>,
pp.  629–638.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
et al. (2021)</span>
<span class="ltx_bibblock">
Sun, T., K. Webster, A. Shah, W. Y. Wang, and M. Johnson (2021).

</span>
<span class="ltx_bibblock">They, them, theirs: Rewriting with gender-neutral english.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2102.06788</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever
et al. (2014)</span>
<span class="ltx_bibblock">
Sutskever, I., O. Vinyals, and Q. V. Le (2014).

</span>
<span class="ltx_bibblock">Sequence to sequence learning with neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Information Processing Systems</span>,
Montreal, Quebec, Canada, pp.  3104–3112.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toews (2023)</span>
<span class="ltx_bibblock">
Toews, R. (2023, Sep).

</span>
<span class="ltx_bibblock">The next generation of large language models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral (2019)</span>
<span class="ltx_bibblock">
Toral, A. (2019).

</span>
<span class="ltx_bibblock">Post-editese: an exacerbated translationese.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of Machine Translation Summit XVII: Research
Track</span>, pp.  273–281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2021)</span>
<span class="ltx_bibblock">
Vanmassenhove, E., C. Emmery, and D. Shterionov (2021).

</span>
<span class="ltx_bibblock">Neutral rewriter: A rule-based and neural approach to automatic
rewriting into gender neutral alternatives.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</span>, pp.  8940–8948.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2018)</span>
<span class="ltx_bibblock">
Vanmassenhove, E., C. Hardmeier, and A. Way (2018).

</span>
<span class="ltx_bibblock">Getting gender right in neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</span>, pp.  3003–3008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove and
Monti (2021)</span>
<span class="ltx_bibblock">
Vanmassenhove, E. and J. Monti (2021).

</span>
<span class="ltx_bibblock">gender-it: An annotated english-italian parallel challenge set for
cross-linguistic natural gender phenomena.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 3rd Workshop on Gender Bias in Natural
Language Processing</span>, pp.  1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2021)</span>
<span class="ltx_bibblock">
Vanmassenhove, E., D. Shterionov, and M. Gwilliam (2021, April).

</span>
<span class="ltx_bibblock">Machine translationese: Effects of algorithmic bias on linguistic
complexity in machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 16th Conference of the European Chapter of
the Association for Computational Linguistics: Main Volume</span>, Online, pp. 2203–2213. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanmassenhove et al. (2019)</span>
<span class="ltx_bibblock">
Vanmassenhove, E., D. Shterionov, and A. Way (2019, August).

</span>
<span class="ltx_bibblock">Lost in translation: Loss and decay of linguistic richness in machine
translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of Machine Translation Summit XVII Volume 1:
Research Track</span>, Dublin, Ireland, pp.  222–232. European Association for
Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vargha (2021)</span>
<span class="ltx_bibblock">
Vargha, D. (2021).

</span>
<span class="ltx_bibblock">Hungarian is a gender-neutral language, it has no gendered pronouns,
so google translate automatically chooses the gender for you. here is how
everyday sexism is..

</span>
<span class="ltx_bibblock">Twitter post.

</span>
<span class="ltx_bibblock">March 20, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani
et al. (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin (2017).

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus,
S. V. N. Vishwanathan, and R. Garnett (Eds.), <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</span>, pp. 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al. (2023)</span>
<span class="ltx_bibblock">
Wang, L., C. Lyu, T. Ji, Z. Zhang, D. Yu, S. Shi, and Z. Tu (2023).

</span>
<span class="ltx_bibblock">Document-level machine translation with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2304.02210</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J., X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,
et al. (2022).

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_font_italic" id="bib.bib45.2.2">35</span>,
24824–24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Zhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi (2019).

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:1904.09675</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al. (2018)</span>
<span class="ltx_bibblock">
Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang (2018, June).

</span>
<span class="ltx_bibblock">Gender bias in coreference resolution: Evaluation and debiasing
methods.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</span>, New Orleans, Louisiana, pp.  15–20.
Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<nav class="ltx_TOC ltx_list_ent ltx_toc_ent"><h6 class="ltx_title ltx_title_contents">Notes</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote1" title="endnote 1 ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span> </a><span class="ltx_text ltx_ref_tag">Accessed on June 12, 2023.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote2" title="endnote 2 ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span> </a><span class="ltx_text ltx_ref_tag">Re-translated on June 12 2023 by the author.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote3" title="endnote 3 ‣ 2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">3</span> </a><span class="ltx_text ltx_ref_tag">Although one could argue, that this implicitness makes it all the more dangerous.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote4" title="endnote 4 ‣ 2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span> </a><span class="ltx_text ltx_ref_tag">For a more comprehensive exploration of this topic, we refer to <cite class="ltx_cite ltx_citemacro_cite">Birhane (<a class="ltx_ref" href="#bib.bib4" title="Birhane (2021) ‣ References ‣ Gender Bias in Machine Translation and The Era of Large Language Models">2021</a>)</cite> which provides valuable insights into the ethical dimensions of algorithmic systems and their impact while advocating for a more critical analysis in the field of machine learning and data science.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote5" title="endnote 5 ‣ 2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5</span> </a><span class="ltx_text ltx_ref_tag">We acknowledge that the gender-neutral alternative <span class="ltx_text ltx_font_italic">‘le personnel enseignant’</span> is not the most literal translation of <span class="ltx_text ltx_font_italic">‘teacher’</span> and furthermore implies multiple people (plural). However, in settings where <span class="ltx_text ltx_font_italic">‘teacher’</span> is used in a generic way, it can be considered a valid gender-neutral alternative.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote6" title="endnote 6 ‣ 2.1 Gender Bias Sources ‣ 2 Gender Bias in MT ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">6</span> </a><span class="ltx_text ltx_ref_tag">We will use the term <span class="ltx_text ltx_font_italic">non-binary gender</span> to refer to all genders that fall outside the binary female/male distinction.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote7" title="endnote 7 ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">7</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote8" title="endnote 8 ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">8</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.deepl.com" title="">https://www.deepl.com</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote9" title="endnote 9 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">9</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote10" title="endnote 10 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">10</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="www.openai.com" title="">www.openai.com</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote11" title="endnote 11 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">11</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reuters.com" title="">https://www.reuters.com</a> article accessible via <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://shorturl.at/dewzZ" title="">https://shorturl.at/dewzZ</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote12" title="endnote 12 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">12</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nytimes.com" title="">https://www.nytimes.com</a> article accessible via https://shorturl.at/ruRS3</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote13" title="endnote 13 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">13</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/" title="">https://arxiv.org/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote14" title="endnote 14 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">14</span> </a><span class="ltx_text ltx_ref_tag">We ought to note that this is an ill-defined term.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote15" title="endnote 15 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">15</span> </a><span class="ltx_text ltx_ref_tag">Similar to ’low-resource languages’ the terms ’distant languages’ is not very well-defined. In the paper itself, they refer to ‘European’ vs ‘Asian’ languages.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote16" title="endnote 16 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">16</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote17" title="endnote 17 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">17</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.deepl.com/en/translator" title="">https://www.deepl.com/en/translator</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote18" title="endnote 18 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">18</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://transmart.qq.com/" title="">https://transmart.qq.com/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote19" title="endnote 19 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">19</span> </a><span class="ltx_text ltx_ref_tag">Bengali has two pronouns, one of which is the more respectful form and they found it to be used mainly in sentences featuring a male pronoun.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote20" title="endnote 20 ‣ 3.2 GPT for MT ‣ 3 Related Work ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">20</span> </a><span class="ltx_text ltx_ref_tag"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/nl-nl/translator/" title="">https://www.microsoft.com/nl-nl/translator/</a></span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote21" title="endnote 21 ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">21</span> </a><span class="ltx_text ltx_ref_tag">A few exceptions are nouns such as <span class="ltx_text ltx_font_italic">’waitress’</span> vs <span class="ltx_text ltx_font_italic">’waiter’</span> or <span class="ltx_text ltx_font_italic">‘actress’</span> vs <span class="ltx_text ltx_font_italic">‘actor’</span>.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote22" title="endnote 22 ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">22</span> </a><span class="ltx_text ltx_ref_tag">We added a version that is not included in gENder-IT using the ‘<span class="ltx_ERROR undefined">\textschwa</span>’ which has been recommended by <cite class="ltx_cite ltx_citemacro_cite">Baiocco
et al. (<a class="ltx_ref" href="#bib.bib1" title="Baiocco et al. (2023) ‣ References ‣ Gender Bias in Machine Translation and The Era of Large Language Models">2023</a>)</cite> as a non-gender-specific ending for Italian.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote23" title="endnote 23 ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">23</span> </a><span class="ltx_text ltx_ref_tag">We accessed ChatGPT to generated these translations between May 30, 2023 and June 5, 2023.</span>
</li>
<li class="ltx_tocentry ltx_tocentry_note">
<a class="ltx_ref" href="#endnote24" title="endnote 24 ‣ 4.1.1 Dataset ‣ 4.1 Experimental Setup ‣ 4 Preliminary Experiment and Analysis ‣ Gender Bias in Machine Translation and The Era of Large Language Models"><span class="ltx_text ltx_ref_tag">24</span> </a><span class="ltx_text ltx_ref_tag">There is no crystallized approach to gender-neutral endings in Italian as to date.</span>
</li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jan 18 14:32:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
