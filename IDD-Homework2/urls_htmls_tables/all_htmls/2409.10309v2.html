<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems</title>
<!--Generated on Tue Oct  8 19:29:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender systems,  Text mining,  Sentence embeddings,  Cold-start recommendation,  Zero-shot recommendation" lang="en" name="keywords"/>
<base href="/html/2409.10309v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S1" title="In beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S2" title="In beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S3" title="In beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Training Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4" title="In beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.SS1" title="In 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S5" title="In beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{textblock}</span>
<p class="ltx_p" id="p1.2">16(0,0.1)</p>
<p class="ltx_p ltx_align_center" id="p1.3"><span class="ltx_text" id="p1.3.1">This paper was published at <span class="ltx_text ltx_font_bold" id="p1.3.1.1">RecSys 2024</span> – please cite the published version <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3640457.3691707" style="font-size:90%;" title="">https://doi.org/10.1145/3640457.3691707</a>.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vojtěch Vančura
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:vancurv@fit.cvut.cz">vancurv@fit.cvut.cz</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-2638-9969" title="ORCID identifier">0000-0003-2638-9969</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Faculty of Information Technology, Czech Technical University in Prague</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Prague</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Czech Republic</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.4.id1">Recombee</span><span class="ltx_text ltx_affiliation_city" id="id5.5.id2">Prague</span><span class="ltx_text ltx_affiliation_country" id="id6.6.id3">Czech Republic</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pavel Kordík
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:pavel.kordik@fit.cvut.cz">pavel.kordik@fit.cvut.cz</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1433-0089" title="ORCID identifier">0000-0003-1433-0089</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Faculty of Information Technology, Czech Technical University in Prague</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Prague</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Czech Republic</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.4.id1">Recombee</span><span class="ltx_text ltx_affiliation_city" id="id11.5.id2">Prague</span><span class="ltx_text ltx_affiliation_country" id="id12.6.id3">Czech Republic</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Milan Straka
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:straka@ufal.mff.cuni.cz">straka@ufal.mff.cuni.cz</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-3295-5576" title="ORCID identifier">0000-0003-3295-5576</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Faculty of Mathematics and Physics, Charles University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Prague</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Czech Republic</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id16.id1">Recommender systems often use text-side information to improve their predictions, especially in cold-start or zero-shot recommendation scenarios, where traditional collaborative filtering approaches cannot be used. Many approaches to text-mining side information for recommender systems have been proposed over recent years, with sentence Transformers being the most prominent one. However, these models are trained to predict semantic similarity without utilizing interaction data with hidden patterns specific to recommender systems. In this paper, we propose beeFormer, a framework for training sentence Transformer models with interaction data. We demonstrate that our models trained with beeFormer can transfer knowledge between datasets while outperforming not only semantic similarity sentence Transformers but also traditional collaborative filtering methods. We also show that training on multiple datasets from different domains accumulates knowledge in a single model, unlocking the possibility of training universal, domain-agnostic sentence Transformer models to mine text representations for recommender systems. We release the source code, trained models, and additional details allowing replication of our experiments at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/recombee/beeformer" title="">https://github.com/recombee/beeformer</a>.</p>
</div>
<div class="ltx_keywords">Recommender systems, Text mining, Sentence embeddings, Cold-start recommendation, Zero-shot recommendation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3691707</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender systems (RS) aim to help users find what they are looking for
in various domains. Many approaches to building algorithms for RS have been proposed over the past years, with Collaborative Filtering (CF) <cite class="ltx_cite ltx_citemacro_citep">(Goldberg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib19" title="">1992</a>)</cite> being the most popular choice. CF methods predict (filter) user preferences by analyzing past interactions. Popular CF techniques include neighborhood-based methods <cite class="ltx_cite ltx_citemacro_citep">(Konstan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib24" title="">1997</a>; Sarwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib36" title="">2001</a>)</cite>, matrix-factorization (MF) <cite class="ltx_cite ltx_citemacro_citep">(Koren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib25" title="">2009</a>; Takács and Tikk, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib47" title="">2012</a>)</cite>, deep neural networks (DNN) <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib28" title="">2018</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib9" title="">2016</a>)</cite>, or shallow linear autoencoders (SLAs) <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib43" title="">2019</a>; Spišák et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib42" title="">2023</a>; Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="434" id="S1.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Training with the beeFormer framework: A sentence Transformer model (up) act as a an encoder to generate item embeddings represented by the matrix <math alttext="\boldsymbol{A}" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">𝑨</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">𝑨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\boldsymbol{A}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">bold_italic_A</annotation></semantics></math>. Then ELSA act as a decoder (down) in a training step on interactions to obtain gradients used to optimize the Transformer model.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.5">SLAs became popular recently, mainly because the EASE <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib43" title="">2019</a>)</cite> model has a closed-form solution while yielding high performance comparable to deep models. Since EASE cannot scale to datasets with a high number of items, scalable variants of EASE have been proposed: SANSA <cite class="ltx_cite ltx_citemacro_citep">(Spišák et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib42" title="">2023</a>)</cite> and ELSA <cite class="ltx_cite ltx_citemacro_citep">(Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite>. SANSA keeps the original structure of EASE but uses (sparse) incomplete Cholesky factorization approximating the inverse of the (potentially) large sparse matrix <math alttext="X^{T}X" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><msup id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml"><mi id="S1.p2.1.m1.1.1.2.2" xref="S1.p2.1.m1.1.1.2.2.cmml">X</mi><mi id="S1.p2.1.m1.1.1.2.3" xref="S1.p2.1.m1.1.1.2.3.cmml">T</mi></msup><mo id="S1.p2.1.m1.1.1.1" xref="S1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">X</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><times id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1"></times><apply id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.2.1.cmml" xref="S1.p2.1.m1.1.1.2">superscript</csymbol><ci id="S1.p2.1.m1.1.1.2.2.cmml" xref="S1.p2.1.m1.1.1.2.2">𝑋</ci><ci id="S1.p2.1.m1.1.1.2.3.cmml" xref="S1.p2.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">X^{T}X</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X</annotation></semantics></math> to construct the asymmetric approximation of the item-to-item weight matrix. ELSA, on the other hand, approximates learned item-to-item weight matrix <math alttext="W" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">W</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">italic_W</annotation></semantics></math> with low-rank approximation <math alttext="W=AA^{T}" class="ltx_Math" display="inline" id="S1.p2.3.m3.1"><semantics id="S1.p2.3.m3.1a"><mrow id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml"><mi id="S1.p2.3.m3.1.1.2" xref="S1.p2.3.m3.1.1.2.cmml">W</mi><mo id="S1.p2.3.m3.1.1.1" xref="S1.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S1.p2.3.m3.1.1.3" xref="S1.p2.3.m3.1.1.3.cmml"><mi id="S1.p2.3.m3.1.1.3.2" xref="S1.p2.3.m3.1.1.3.2.cmml">A</mi><mo id="S1.p2.3.m3.1.1.3.1" xref="S1.p2.3.m3.1.1.3.1.cmml">⁢</mo><msup id="S1.p2.3.m3.1.1.3.3" xref="S1.p2.3.m3.1.1.3.3.cmml"><mi id="S1.p2.3.m3.1.1.3.3.2" xref="S1.p2.3.m3.1.1.3.3.2.cmml">A</mi><mi id="S1.p2.3.m3.1.1.3.3.3" xref="S1.p2.3.m3.1.1.3.3.3.cmml">T</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><apply id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1"><eq id="S1.p2.3.m3.1.1.1.cmml" xref="S1.p2.3.m3.1.1.1"></eq><ci id="S1.p2.3.m3.1.1.2.cmml" xref="S1.p2.3.m3.1.1.2">𝑊</ci><apply id="S1.p2.3.m3.1.1.3.cmml" xref="S1.p2.3.m3.1.1.3"><times id="S1.p2.3.m3.1.1.3.1.cmml" xref="S1.p2.3.m3.1.1.3.1"></times><ci id="S1.p2.3.m3.1.1.3.2.cmml" xref="S1.p2.3.m3.1.1.3.2">𝐴</ci><apply id="S1.p2.3.m3.1.1.3.3.cmml" xref="S1.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S1.p2.3.m3.1.1.3.3.1.cmml" xref="S1.p2.3.m3.1.1.3.3">superscript</csymbol><ci id="S1.p2.3.m3.1.1.3.3.2.cmml" xref="S1.p2.3.m3.1.1.3.3.2">𝐴</ci><ci id="S1.p2.3.m3.1.1.3.3.3.cmml" xref="S1.p2.3.m3.1.1.3.3.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">W=AA^{T}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.3.m3.1d">italic_W = italic_A italic_A start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="\operatorname{diag}(W)=0" class="ltx_Math" display="inline" id="S1.p2.4.m4.2"><semantics id="S1.p2.4.m4.2a"><mrow id="S1.p2.4.m4.2.3" xref="S1.p2.4.m4.2.3.cmml"><mrow id="S1.p2.4.m4.2.3.2.2" xref="S1.p2.4.m4.2.3.2.1.cmml"><mi id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">diag</mi><mo id="S1.p2.4.m4.2.3.2.2a" xref="S1.p2.4.m4.2.3.2.1.cmml">⁡</mo><mrow id="S1.p2.4.m4.2.3.2.2.1" xref="S1.p2.4.m4.2.3.2.1.cmml"><mo id="S1.p2.4.m4.2.3.2.2.1.1" stretchy="false" xref="S1.p2.4.m4.2.3.2.1.cmml">(</mo><mi id="S1.p2.4.m4.2.2" xref="S1.p2.4.m4.2.2.cmml">W</mi><mo id="S1.p2.4.m4.2.3.2.2.1.2" stretchy="false" xref="S1.p2.4.m4.2.3.2.1.cmml">)</mo></mrow></mrow><mo id="S1.p2.4.m4.2.3.1" xref="S1.p2.4.m4.2.3.1.cmml">=</mo><mn id="S1.p2.4.m4.2.3.3" xref="S1.p2.4.m4.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.2b"><apply id="S1.p2.4.m4.2.3.cmml" xref="S1.p2.4.m4.2.3"><eq id="S1.p2.4.m4.2.3.1.cmml" xref="S1.p2.4.m4.2.3.1"></eq><apply id="S1.p2.4.m4.2.3.2.1.cmml" xref="S1.p2.4.m4.2.3.2.2"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">diag</ci><ci id="S1.p2.4.m4.2.2.cmml" xref="S1.p2.4.m4.2.2">𝑊</ci></apply><cn id="S1.p2.4.m4.2.3.3.cmml" type="integer" xref="S1.p2.4.m4.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.2c">\operatorname{diag}(W)=0</annotation><annotation encoding="application/x-llamapun" id="S1.p2.4.m4.2d">roman_diag ( italic_W ) = 0</annotation></semantics></math> to prevent trivial solution such as the identity solution. The matrix <math alttext="A" class="ltx_Math" display="inline" id="S1.p2.5.m5.1"><semantics id="S1.p2.5.m5.1a"><mi id="S1.p2.5.m5.1.1" xref="S1.p2.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S1.p2.5.m5.1b"><ci id="S1.p2.5.m5.1.1.cmml" xref="S1.p2.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S1.p2.5.m5.1d">italic_A</annotation></semantics></math> is optimized using backpropagation.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the popularity and state-of-the-art performance of CF methods in recommendation tasks, they cannot provide any predictions when there are no interactions. In such cases, also known as
cold-start <cite class="ltx_cite ltx_citemacro_citep">(Sethi and Mehrotra, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib37" title="">2021</a>)</cite> and zero-shot <cite class="ltx_cite ltx_citemacro_citep">(DING et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib14" title="">2022</a>)</cite> recommendation, one can use content-based filtering (CBF)  <cite class="ltx_cite ltx_citemacro_citep">(Baeza-Yates et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib5" title="">2015</a>; Anwaar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib4" title="">2018</a>)</cite> using side information (attributes, images, text) directly to produce recommendations or learn a transformation function to transform side information to CF representations <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib52" title="">2020</a>; Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib16" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using texts (item descriptions, user reviews, etc.) as side information has become extremely popular after the invention of the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib49" title="">2017</a>)</cite> neural architecture. Transformer models can be used to encode text into a vector representation, and sentence Transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib35" title="">2019</a>)</cite> were explicitly developed to mine text latent representations from the whole blocks of text (
sentences, paragraphs), which can then be used for various tasks, with the recommendation being one of them.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">However, sentence Transformers trained to predict semantic similarity often fail to capture patterns and user behaviors hidden in the interaction data. In many cases, users may look for a specific item (for example, batteries when buying a kid’s toy, or cables when buying a new printer) with very low semantic similarity compared to other items in the catalog.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.3">To bridge this gap between the semantic and the interaction similarity, we employ the following idea: We use the training procedure from the ELSA model, but instead of optimizing the matrix <math alttext="A" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mi id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">italic_A</annotation></semantics></math>, we generate matrix <math alttext="A" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mi id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><ci id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">italic_A</annotation></semantics></math> with a sentence Transformer model
and optimize parameters of the sentence Transformer instead of optimizing <math alttext="A" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mi id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><ci id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">italic_A</annotation></semantics></math> directly, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a>. However, this approach faces one critical problem: In every training step, we need to generate and optimize embeddings for all items in the catalog, which leads to very high effective batch size for Transformer training, e.g., possibly over a million for some datasets. We propose to overcome this problem by employing the following three techniques: gradient checkpointing <cite class="ltx_cite ltx_citemacro_citep">(Griewank and Walther, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib20" title="">2000</a>)</cite>, gradient accumulation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib8" title="">2016</a>)</cite>, and negative sampling <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib15" title="">2020</a>)</cite>. Combining these techniques with sentence Transformer and ELSA training procedure, we present beeFormer (short for the Recom<span class="ltx_text ltx_font_bold" id="S1.p6.3.1">bee</span> Trans<span class="ltx_text ltx_font_bold" id="S1.p6.3.2">former</span> in camel case), a sentence Transformer training framework that uses text-side information and interactions directly to update the parameters of a Transformer model.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The main contributions of this paper are listed as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose beeFormer, a framework for training sentence Transformers on interaction data with text-side information.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our experiments show that sentence Transformer models trained with beeFormer outperform all baselines in cold-start, zero-shot and time-split recommendation scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We demonstrate the beeFormer’s ability to transfer knowledge between datasets.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We show that training models on combined datasets from various domains further increase performance in the domain-agnostic recommendation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We create and publish LLM-generated item descriptions for all used datasets for reproducibility of our experiments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i6.p1">
<p class="ltx_p" id="S1.I1.i6.p1.1">Models trained with beeFormer are easily deployable into production systems using the sentence Transformers library.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p8.1">We believe the above improvements open a path towards a potentially universal, domain-agnostic, and multi-modal Transformer models for recommender systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">SLAs <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib43" title="">2019</a>; Spišák et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib42" title="">2023</a>; Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite> have recently gained much attention, mainly because of their simplicity while retaining performance comparable to deep models, with EASE <cite class="ltx_cite ltx_citemacro_citep">(Steck, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib43" title="">2019</a>)</cite> being the most promising. ELSA <cite class="ltx_cite ltx_citemacro_citep">(Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite> solved the scalability issue of EASE and enabled its use on large datasets by low-rank approximation of the item-to-item weight matrix of EASE. We use the idea of training the ELSA’s embeddings using backpropagation to obtain the gradients for training the sentence Transformer model.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The first attempt to use Transformers in RS was in sequential recommendation. Bert4rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib44" title="">2019</a>)</cite> used item IDs as tokens and treated the sequential recommendation problem in the same manner as NLP. Several improvements to this approach have been proposed since then <cite class="ltx_cite ltx_citemacro_citep">(de Souza Pereira Moreira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib11" title="">2021</a>; Shalaby et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib38" title="">2022</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib17" title="">2022</a>)</cite>. Promising approach is to create artificial text sequences combining descriptions of interacted items and train a Transformer-based model on them <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib26" title="">2023b</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib22" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Another direction is to use Large Language Models (LLMs), such as the Chat-GPT <cite class="ltx_cite ltx_citemacro_citep">(Di Palma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib13" title="">2023</a>; Manzoor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib29" title="">2024</a>)</cite>. LLMs can be used for various tasks, e.g., as conversational recommendation <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib45" title="">2024</a>; Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib51" title="">2023</a>)</cite>, to generate standardized item text descriptions <cite class="ltx_cite ltx_citemacro_citep">(Acharya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib2" title="">2023</a>)</cite>, recommendation explanation <cite class="ltx_cite ltx_citemacro_citep">(Silva et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib39" title="">2024</a>)</cite>, or to produce recommendations through its text output <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib27" title="">2023a</a>)</cite> directly.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Sentence Transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib35" title="">2019</a>)</cite> use a pooling function on top of the Transformer architecture and provide a robust, easy-to-use framework for tokenization, embedding generation, and training sentence Transformer models. While using sentence Transformers is very popular in the dense-retrieval domain <cite class="ltx_cite ltx_citemacro_citep">(Botha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib6" title="">2020</a>; Gillick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib18" title="">2019</a>)</cite>, using sentence Transformers in recommender systems is limited to generating side information for cold-start methods <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib52" title="">2020</a>; Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib16" title="">2020</a>)</cite> or using neural networks <cite class="ltx_cite ltx_citemacro_citep">(Juarto and Girsang, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib23" title="">2021</a>)</cite> or graph neural networks <cite class="ltx_cite ltx_citemacro_citep">(Spillo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib41" title="">2023</a>)</cite> on top of sentence Transformer models.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Improving sentence Transformers with training on interaction data is crucial for boosting the performance of all methods using sentence embeddings as side information mentioned above. Sadly, to our best knowledge, there is no prior work on training sentence Transformers directly with interaction data in the RS domain.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Training Procedure</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.17">We follow notation from <cite class="ltx_cite ltx_citemacro_citep">(Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite>: assume a set of users <math alttext="\mathbb{U}=\{u_{1},u_{2},\ldots,\\
u_{U}\}" class="ltx_Math" display="inline" id="S3.p1.1.m1.4"><semantics id="S3.p1.1.m1.4a"><mrow id="S3.p1.1.m1.4.4" xref="S3.p1.1.m1.4.4.cmml"><mi id="S3.p1.1.m1.4.4.5" xref="S3.p1.1.m1.4.4.5.cmml">𝕌</mi><mo id="S3.p1.1.m1.4.4.4" xref="S3.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.p1.1.m1.4.4.3.3" xref="S3.p1.1.m1.4.4.3.4.cmml"><mo id="S3.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S3.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.p1.1.m1.2.2.1.1.1" xref="S3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.p1.1.m1.2.2.1.1.1.2" xref="S3.p1.1.m1.2.2.1.1.1.2.cmml">u</mi><mn id="S3.p1.1.m1.2.2.1.1.1.3" xref="S3.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.1.m1.4.4.3.3.5" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p1.1.m1.3.3.2.2.2" xref="S3.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.p1.1.m1.3.3.2.2.2.2" xref="S3.p1.1.m1.3.3.2.2.2.2.cmml">u</mi><mn id="S3.p1.1.m1.3.3.2.2.2.3" xref="S3.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p1.1.m1.4.4.3.3.6" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S3.p1.1.m1.1.1" mathvariant="normal" xref="S3.p1.1.m1.1.1.cmml">…</mi><mo id="S3.p1.1.m1.4.4.3.3.7" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p1.1.m1.4.4.3.3.3" xref="S3.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.p1.1.m1.4.4.3.3.3.2" xref="S3.p1.1.m1.4.4.3.3.3.2.cmml">u</mi><mi id="S3.p1.1.m1.4.4.3.3.3.3" xref="S3.p1.1.m1.4.4.3.3.3.3.cmml">U</mi></msub><mo id="S3.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S3.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.4b"><apply id="S3.p1.1.m1.4.4.cmml" xref="S3.p1.1.m1.4.4"><eq id="S3.p1.1.m1.4.4.4.cmml" xref="S3.p1.1.m1.4.4.4"></eq><ci id="S3.p1.1.m1.4.4.5.cmml" xref="S3.p1.1.m1.4.4.5">𝕌</ci><set id="S3.p1.1.m1.4.4.3.4.cmml" xref="S3.p1.1.m1.4.4.3.3"><apply id="S3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.p1.1.m1.2.2.1.1.1.2">𝑢</ci><cn id="S3.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.p1.1.m1.3.3.2.2.2.cmml" xref="S3.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.p1.1.m1.3.3.2.2.2.2">𝑢</ci><cn id="S3.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">…</ci><apply id="S3.p1.1.m1.4.4.3.3.3.cmml" xref="S3.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.p1.1.m1.4.4.3.3.3.2">𝑢</ci><ci id="S3.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.p1.1.m1.4.4.3.3.3.3">𝑈</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.4c">\mathbb{U}=\{u_{1},u_{2},\ldots,\\
u_{U}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.4d">blackboard_U = { italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_u start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT }</annotation></semantics></math>, a set of items <math alttext="\mathbb{I}=\{i_{1},i_{2},\ldots,i_{I}\}" class="ltx_Math" display="inline" id="S3.p1.2.m2.4"><semantics id="S3.p1.2.m2.4a"><mrow id="S3.p1.2.m2.4.4" xref="S3.p1.2.m2.4.4.cmml"><mi id="S3.p1.2.m2.4.4.5" xref="S3.p1.2.m2.4.4.5.cmml">𝕀</mi><mo id="S3.p1.2.m2.4.4.4" xref="S3.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S3.p1.2.m2.4.4.3.3" xref="S3.p1.2.m2.4.4.3.4.cmml"><mo id="S3.p1.2.m2.4.4.3.3.4" stretchy="false" xref="S3.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S3.p1.2.m2.2.2.1.1.1" xref="S3.p1.2.m2.2.2.1.1.1.cmml"><mi id="S3.p1.2.m2.2.2.1.1.1.2" xref="S3.p1.2.m2.2.2.1.1.1.2.cmml">i</mi><mn id="S3.p1.2.m2.2.2.1.1.1.3" xref="S3.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.2.m2.4.4.3.3.5" xref="S3.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.p1.2.m2.3.3.2.2.2" xref="S3.p1.2.m2.3.3.2.2.2.cmml"><mi id="S3.p1.2.m2.3.3.2.2.2.2" xref="S3.p1.2.m2.3.3.2.2.2.2.cmml">i</mi><mn id="S3.p1.2.m2.3.3.2.2.2.3" xref="S3.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p1.2.m2.4.4.3.3.6" xref="S3.p1.2.m2.4.4.3.4.cmml">,</mo><mi id="S3.p1.2.m2.1.1" mathvariant="normal" xref="S3.p1.2.m2.1.1.cmml">…</mi><mo id="S3.p1.2.m2.4.4.3.3.7" xref="S3.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.p1.2.m2.4.4.3.3.3" xref="S3.p1.2.m2.4.4.3.3.3.cmml"><mi id="S3.p1.2.m2.4.4.3.3.3.2" xref="S3.p1.2.m2.4.4.3.3.3.2.cmml">i</mi><mi id="S3.p1.2.m2.4.4.3.3.3.3" xref="S3.p1.2.m2.4.4.3.3.3.3.cmml">I</mi></msub><mo id="S3.p1.2.m2.4.4.3.3.8" stretchy="false" xref="S3.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.4b"><apply id="S3.p1.2.m2.4.4.cmml" xref="S3.p1.2.m2.4.4"><eq id="S3.p1.2.m2.4.4.4.cmml" xref="S3.p1.2.m2.4.4.4"></eq><ci id="S3.p1.2.m2.4.4.5.cmml" xref="S3.p1.2.m2.4.4.5">𝕀</ci><set id="S3.p1.2.m2.4.4.3.4.cmml" xref="S3.p1.2.m2.4.4.3.3"><apply id="S3.p1.2.m2.2.2.1.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.p1.2.m2.2.2.1.1.1.2">𝑖</ci><cn id="S3.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.p1.2.m2.3.3.2.2.2.cmml" xref="S3.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.3.3.2.2.2.1.cmml" xref="S3.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.2.m2.3.3.2.2.2.2.cmml" xref="S3.p1.2.m2.3.3.2.2.2.2">𝑖</ci><cn id="S3.p1.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S3.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">…</ci><apply id="S3.p1.2.m2.4.4.3.3.3.cmml" xref="S3.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p1.2.m2.4.4.3.3.3.1.cmml" xref="S3.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.p1.2.m2.4.4.3.3.3.2.cmml" xref="S3.p1.2.m2.4.4.3.3.3.2">𝑖</ci><ci id="S3.p1.2.m2.4.4.3.3.3.3.cmml" xref="S3.p1.2.m2.4.4.3.3.3.3">𝐼</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.4c">\mathbb{I}=\{i_{1},i_{2},\ldots,i_{I}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.4d">blackboard_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_i start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT }</annotation></semantics></math>, and a set of token sequences representing corresponding items <math alttext="\mathbb{T}=\{t_{1},t_{2},\ldots,t_{I}\}" class="ltx_Math" display="inline" id="S3.p1.3.m3.4"><semantics id="S3.p1.3.m3.4a"><mrow id="S3.p1.3.m3.4.4" xref="S3.p1.3.m3.4.4.cmml"><mi id="S3.p1.3.m3.4.4.5" xref="S3.p1.3.m3.4.4.5.cmml">𝕋</mi><mo id="S3.p1.3.m3.4.4.4" xref="S3.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.p1.3.m3.4.4.3.3" xref="S3.p1.3.m3.4.4.3.4.cmml"><mo id="S3.p1.3.m3.4.4.3.3.4" stretchy="false" xref="S3.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S3.p1.3.m3.2.2.1.1.1" xref="S3.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.p1.3.m3.2.2.1.1.1.2" xref="S3.p1.3.m3.2.2.1.1.1.2.cmml">t</mi><mn id="S3.p1.3.m3.2.2.1.1.1.3" xref="S3.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.3.m3.4.4.3.3.5" xref="S3.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p1.3.m3.3.3.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.p1.3.m3.3.3.2.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.2.cmml">t</mi><mn id="S3.p1.3.m3.3.3.2.2.2.3" xref="S3.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p1.3.m3.4.4.3.3.6" xref="S3.p1.3.m3.4.4.3.4.cmml">,</mo><mi id="S3.p1.3.m3.1.1" mathvariant="normal" xref="S3.p1.3.m3.1.1.cmml">…</mi><mo id="S3.p1.3.m3.4.4.3.3.7" xref="S3.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p1.3.m3.4.4.3.3.3" xref="S3.p1.3.m3.4.4.3.3.3.cmml"><mi id="S3.p1.3.m3.4.4.3.3.3.2" xref="S3.p1.3.m3.4.4.3.3.3.2.cmml">t</mi><mi id="S3.p1.3.m3.4.4.3.3.3.3" xref="S3.p1.3.m3.4.4.3.3.3.3.cmml">I</mi></msub><mo id="S3.p1.3.m3.4.4.3.3.8" stretchy="false" xref="S3.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.4b"><apply id="S3.p1.3.m3.4.4.cmml" xref="S3.p1.3.m3.4.4"><eq id="S3.p1.3.m3.4.4.4.cmml" xref="S3.p1.3.m3.4.4.4"></eq><ci id="S3.p1.3.m3.4.4.5.cmml" xref="S3.p1.3.m3.4.4.5">𝕋</ci><set id="S3.p1.3.m3.4.4.3.4.cmml" xref="S3.p1.3.m3.4.4.3.3"><apply id="S3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.p1.3.m3.2.2.1.1.1.2">𝑡</ci><cn id="S3.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2.2">𝑡</ci><cn id="S3.p1.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S3.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">…</ci><apply id="S3.p1.3.m3.4.4.3.3.3.cmml" xref="S3.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p1.3.m3.4.4.3.3.3.1.cmml" xref="S3.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.p1.3.m3.4.4.3.3.3.2.cmml" xref="S3.p1.3.m3.4.4.3.3.3.2">𝑡</ci><ci id="S3.p1.3.m3.4.4.3.3.3.3.cmml" xref="S3.p1.3.m3.4.4.3.3.3.3">𝐼</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.4c">\mathbb{T}=\{t_{1},t_{2},\ldots,t_{I}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.4d">blackboard_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT }</annotation></semantics></math> . Let <math alttext="X\in\{0,1\}^{|\mathbb{U}|\times|\mathbb{I}|}" class="ltx_Math" display="inline" id="S3.p1.4.m4.4"><semantics id="S3.p1.4.m4.4a"><mrow id="S3.p1.4.m4.4.5" xref="S3.p1.4.m4.4.5.cmml"><mi id="S3.p1.4.m4.4.5.2" xref="S3.p1.4.m4.4.5.2.cmml">X</mi><mo id="S3.p1.4.m4.4.5.1" xref="S3.p1.4.m4.4.5.1.cmml">∈</mo><msup id="S3.p1.4.m4.4.5.3" xref="S3.p1.4.m4.4.5.3.cmml"><mrow id="S3.p1.4.m4.4.5.3.2.2" xref="S3.p1.4.m4.4.5.3.2.1.cmml"><mo id="S3.p1.4.m4.4.5.3.2.2.1" stretchy="false" xref="S3.p1.4.m4.4.5.3.2.1.cmml">{</mo><mn id="S3.p1.4.m4.3.3" xref="S3.p1.4.m4.3.3.cmml">0</mn><mo id="S3.p1.4.m4.4.5.3.2.2.2" xref="S3.p1.4.m4.4.5.3.2.1.cmml">,</mo><mn id="S3.p1.4.m4.4.4" xref="S3.p1.4.m4.4.4.cmml">1</mn><mo id="S3.p1.4.m4.4.5.3.2.2.3" stretchy="false" xref="S3.p1.4.m4.4.5.3.2.1.cmml">}</mo></mrow><mrow id="S3.p1.4.m4.2.2.2" xref="S3.p1.4.m4.2.2.2.cmml"><mrow id="S3.p1.4.m4.2.2.2.4.2" xref="S3.p1.4.m4.2.2.2.4.1.cmml"><mo id="S3.p1.4.m4.2.2.2.4.2.1" stretchy="false" xref="S3.p1.4.m4.2.2.2.4.1.1.cmml">|</mo><mi id="S3.p1.4.m4.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.cmml">𝕌</mi><mo id="S3.p1.4.m4.2.2.2.4.2.2" rspace="0.055em" stretchy="false" xref="S3.p1.4.m4.2.2.2.4.1.1.cmml">|</mo></mrow><mo id="S3.p1.4.m4.2.2.2.3" rspace="0.222em" xref="S3.p1.4.m4.2.2.2.3.cmml">×</mo><mrow id="S3.p1.4.m4.2.2.2.5.2" xref="S3.p1.4.m4.2.2.2.5.1.cmml"><mo id="S3.p1.4.m4.2.2.2.5.2.1" stretchy="false" xref="S3.p1.4.m4.2.2.2.5.1.1.cmml">|</mo><mi id="S3.p1.4.m4.2.2.2.2" xref="S3.p1.4.m4.2.2.2.2.cmml">𝕀</mi><mo id="S3.p1.4.m4.2.2.2.5.2.2" stretchy="false" xref="S3.p1.4.m4.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.4b"><apply id="S3.p1.4.m4.4.5.cmml" xref="S3.p1.4.m4.4.5"><in id="S3.p1.4.m4.4.5.1.cmml" xref="S3.p1.4.m4.4.5.1"></in><ci id="S3.p1.4.m4.4.5.2.cmml" xref="S3.p1.4.m4.4.5.2">𝑋</ci><apply id="S3.p1.4.m4.4.5.3.cmml" xref="S3.p1.4.m4.4.5.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.4.5.3.1.cmml" xref="S3.p1.4.m4.4.5.3">superscript</csymbol><set id="S3.p1.4.m4.4.5.3.2.1.cmml" xref="S3.p1.4.m4.4.5.3.2.2"><cn id="S3.p1.4.m4.3.3.cmml" type="integer" xref="S3.p1.4.m4.3.3">0</cn><cn id="S3.p1.4.m4.4.4.cmml" type="integer" xref="S3.p1.4.m4.4.4">1</cn></set><apply id="S3.p1.4.m4.2.2.2.cmml" xref="S3.p1.4.m4.2.2.2"><times id="S3.p1.4.m4.2.2.2.3.cmml" xref="S3.p1.4.m4.2.2.2.3"></times><apply id="S3.p1.4.m4.2.2.2.4.1.cmml" xref="S3.p1.4.m4.2.2.2.4.2"><abs id="S3.p1.4.m4.2.2.2.4.1.1.cmml" xref="S3.p1.4.m4.2.2.2.4.2.1"></abs><ci id="S3.p1.4.m4.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1">𝕌</ci></apply><apply id="S3.p1.4.m4.2.2.2.5.1.cmml" xref="S3.p1.4.m4.2.2.2.5.2"><abs id="S3.p1.4.m4.2.2.2.5.1.1.cmml" xref="S3.p1.4.m4.2.2.2.5.2.1"></abs><ci id="S3.p1.4.m4.2.2.2.2.cmml" xref="S3.p1.4.m4.2.2.2.2">𝕀</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.4c">X\in\{0,1\}^{|\mathbb{U}|\times|\mathbb{I}|}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.4d">italic_X ∈ { 0 , 1 } start_POSTSUPERSCRIPT | blackboard_U | × | blackboard_I | end_POSTSUPERSCRIPT</annotation></semantics></math> be a user-item interaction matrix: <math alttext="X_{a,b}=1" class="ltx_Math" display="inline" id="S3.p1.5.m5.2"><semantics id="S3.p1.5.m5.2a"><mrow id="S3.p1.5.m5.2.3" xref="S3.p1.5.m5.2.3.cmml"><msub id="S3.p1.5.m5.2.3.2" xref="S3.p1.5.m5.2.3.2.cmml"><mi id="S3.p1.5.m5.2.3.2.2" xref="S3.p1.5.m5.2.3.2.2.cmml">X</mi><mrow id="S3.p1.5.m5.2.2.2.4" xref="S3.p1.5.m5.2.2.2.3.cmml"><mi id="S3.p1.5.m5.1.1.1.1" xref="S3.p1.5.m5.1.1.1.1.cmml">a</mi><mo id="S3.p1.5.m5.2.2.2.4.1" xref="S3.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.p1.5.m5.2.2.2.2" xref="S3.p1.5.m5.2.2.2.2.cmml">b</mi></mrow></msub><mo id="S3.p1.5.m5.2.3.1" xref="S3.p1.5.m5.2.3.1.cmml">=</mo><mn id="S3.p1.5.m5.2.3.3" xref="S3.p1.5.m5.2.3.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.2b"><apply id="S3.p1.5.m5.2.3.cmml" xref="S3.p1.5.m5.2.3"><eq id="S3.p1.5.m5.2.3.1.cmml" xref="S3.p1.5.m5.2.3.1"></eq><apply id="S3.p1.5.m5.2.3.2.cmml" xref="S3.p1.5.m5.2.3.2"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.3.2.1.cmml" xref="S3.p1.5.m5.2.3.2">subscript</csymbol><ci id="S3.p1.5.m5.2.3.2.2.cmml" xref="S3.p1.5.m5.2.3.2.2">𝑋</ci><list id="S3.p1.5.m5.2.2.2.3.cmml" xref="S3.p1.5.m5.2.2.2.4"><ci id="S3.p1.5.m5.1.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1.1">𝑎</ci><ci id="S3.p1.5.m5.2.2.2.2.cmml" xref="S3.p1.5.m5.2.2.2.2">𝑏</ci></list></apply><cn id="S3.p1.5.m5.2.3.3.cmml" type="integer" xref="S3.p1.5.m5.2.3.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.2c">X_{a,b}=1</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.2d">italic_X start_POSTSUBSCRIPT italic_a , italic_b end_POSTSUBSCRIPT = 1</annotation></semantics></math> if the user <math alttext="u_{a}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">u</mi><mi id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">𝑢</ci><ci id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">u_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_u start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> interacted with item <math alttext="i_{b}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><msub id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">i</mi><mi id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">𝑖</ci><ci id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">i_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_i start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="X_{a,b}=0" class="ltx_Math" display="inline" id="S3.p1.8.m8.2"><semantics id="S3.p1.8.m8.2a"><mrow id="S3.p1.8.m8.2.3" xref="S3.p1.8.m8.2.3.cmml"><msub id="S3.p1.8.m8.2.3.2" xref="S3.p1.8.m8.2.3.2.cmml"><mi id="S3.p1.8.m8.2.3.2.2" xref="S3.p1.8.m8.2.3.2.2.cmml">X</mi><mrow id="S3.p1.8.m8.2.2.2.4" xref="S3.p1.8.m8.2.2.2.3.cmml"><mi id="S3.p1.8.m8.1.1.1.1" xref="S3.p1.8.m8.1.1.1.1.cmml">a</mi><mo id="S3.p1.8.m8.2.2.2.4.1" xref="S3.p1.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.p1.8.m8.2.2.2.2" xref="S3.p1.8.m8.2.2.2.2.cmml">b</mi></mrow></msub><mo id="S3.p1.8.m8.2.3.1" xref="S3.p1.8.m8.2.3.1.cmml">=</mo><mn id="S3.p1.8.m8.2.3.3" xref="S3.p1.8.m8.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.2b"><apply id="S3.p1.8.m8.2.3.cmml" xref="S3.p1.8.m8.2.3"><eq id="S3.p1.8.m8.2.3.1.cmml" xref="S3.p1.8.m8.2.3.1"></eq><apply id="S3.p1.8.m8.2.3.2.cmml" xref="S3.p1.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.p1.8.m8.2.3.2.1.cmml" xref="S3.p1.8.m8.2.3.2">subscript</csymbol><ci id="S3.p1.8.m8.2.3.2.2.cmml" xref="S3.p1.8.m8.2.3.2.2">𝑋</ci><list id="S3.p1.8.m8.2.2.2.3.cmml" xref="S3.p1.8.m8.2.2.2.4"><ci id="S3.p1.8.m8.1.1.1.1.cmml" xref="S3.p1.8.m8.1.1.1.1">𝑎</ci><ci id="S3.p1.8.m8.2.2.2.2.cmml" xref="S3.p1.8.m8.2.2.2.2">𝑏</ci></list></apply><cn id="S3.p1.8.m8.2.3.3.cmml" type="integer" xref="S3.p1.8.m8.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.2c">X_{a,b}=0</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.2d">italic_X start_POSTSUBSCRIPT italic_a , italic_b end_POSTSUBSCRIPT = 0</annotation></semantics></math> otherwise. Assume that <math alttext="M_{a}" class="ltx_Math" display="inline" id="S3.p1.9.m9.1"><semantics id="S3.p1.9.m9.1a"><msub id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml"><mi id="S3.p1.9.m9.1.1.2" xref="S3.p1.9.m9.1.1.2.cmml">M</mi><mi id="S3.p1.9.m9.1.1.3" xref="S3.p1.9.m9.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><apply id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.1.1.1.cmml" xref="S3.p1.9.m9.1.1">subscript</csymbol><ci id="S3.p1.9.m9.1.1.2.cmml" xref="S3.p1.9.m9.1.1.2">𝑀</ci><ci id="S3.p1.9.m9.1.1.3.cmml" xref="S3.p1.9.m9.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">M_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.1d">italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> is a <em class="ltx_emph ltx_font_italic" id="S3.p1.17.1">column</em> vector corresponding to the <math alttext="a" class="ltx_Math" display="inline" id="S3.p1.10.m10.1"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m10.1d">italic_a</annotation></semantics></math>-<math alttext="th" class="ltx_Math" display="inline" id="S3.p1.11.m11.1"><semantics id="S3.p1.11.m11.1a"><mrow id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml"><mi id="S3.p1.11.m11.1.1.2" xref="S3.p1.11.m11.1.1.2.cmml">t</mi><mo id="S3.p1.11.m11.1.1.1" xref="S3.p1.11.m11.1.1.1.cmml">⁢</mo><mi id="S3.p1.11.m11.1.1.3" xref="S3.p1.11.m11.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><apply id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1"><times id="S3.p1.11.m11.1.1.1.cmml" xref="S3.p1.11.m11.1.1.1"></times><ci id="S3.p1.11.m11.1.1.2.cmml" xref="S3.p1.11.m11.1.1.2">𝑡</ci><ci id="S3.p1.11.m11.1.1.3.cmml" xref="S3.p1.11.m11.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">th</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m11.1d">italic_t italic_h</annotation></semantics></math> row of matrix <math alttext="M" class="ltx_Math" display="inline" id="S3.p1.12.m12.1"><semantics id="S3.p1.12.m12.1a"><mi id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><ci id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m12.1d">italic_M</annotation></semantics></math>. Let <math alttext="\operatorname{norm}(M)" class="ltx_Math" display="inline" id="S3.p1.13.m13.2"><semantics id="S3.p1.13.m13.2a"><mrow id="S3.p1.13.m13.2.3.2" xref="S3.p1.13.m13.2.3.1.cmml"><mi id="S3.p1.13.m13.1.1" xref="S3.p1.13.m13.1.1.cmml">norm</mi><mo id="S3.p1.13.m13.2.3.2a" xref="S3.p1.13.m13.2.3.1.cmml">⁡</mo><mrow id="S3.p1.13.m13.2.3.2.1" xref="S3.p1.13.m13.2.3.1.cmml"><mo id="S3.p1.13.m13.2.3.2.1.1" stretchy="false" xref="S3.p1.13.m13.2.3.1.cmml">(</mo><mi id="S3.p1.13.m13.2.2" xref="S3.p1.13.m13.2.2.cmml">M</mi><mo id="S3.p1.13.m13.2.3.2.1.2" stretchy="false" xref="S3.p1.13.m13.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.13.m13.2b"><apply id="S3.p1.13.m13.2.3.1.cmml" xref="S3.p1.13.m13.2.3.2"><ci id="S3.p1.13.m13.1.1.cmml" xref="S3.p1.13.m13.1.1">norm</ci><ci id="S3.p1.13.m13.2.2.cmml" xref="S3.p1.13.m13.2.2">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m13.2c">\operatorname{norm}(M)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.13.m13.2d">roman_norm ( italic_M )</annotation></semantics></math> be a function that
<math alttext="L^{2}" class="ltx_Math" display="inline" id="S3.p1.14.m14.1"><semantics id="S3.p1.14.m14.1a"><msup id="S3.p1.14.m14.1.1" xref="S3.p1.14.m14.1.1.cmml"><mi id="S3.p1.14.m14.1.1.2" xref="S3.p1.14.m14.1.1.2.cmml">L</mi><mn id="S3.p1.14.m14.1.1.3" xref="S3.p1.14.m14.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.p1.14.m14.1b"><apply id="S3.p1.14.m14.1.1.cmml" xref="S3.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.p1.14.m14.1.1.1.cmml" xref="S3.p1.14.m14.1.1">superscript</csymbol><ci id="S3.p1.14.m14.1.1.2.cmml" xref="S3.p1.14.m14.1.1.2">𝐿</ci><cn id="S3.p1.14.m14.1.1.3.cmml" type="integer" xref="S3.p1.14.m14.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.14.m14.1c">L^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.14.m14.1d">italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-normalizes each row of a matrix, so <math alttext="\operatorname{norm}(M)_{a}=M_{a}/\|M_{a}\|" class="ltx_math_unparsed" display="inline" id="S3.p1.15.m15.2"><semantics id="S3.p1.15.m15.2a"><mrow id="S3.p1.15.m15.2b"><mi id="S3.p1.15.m15.1.1">norm</mi><msub id="S3.p1.15.m15.2.3"><mrow id="S3.p1.15.m15.2.3.2"><mo id="S3.p1.15.m15.2.3.2.1" stretchy="false">(</mo><mi id="S3.p1.15.m15.2.2">M</mi><mo id="S3.p1.15.m15.2.3.2.2" stretchy="false">)</mo></mrow><mi id="S3.p1.15.m15.2.3.3">a</mi></msub><mo id="S3.p1.15.m15.2.4">=</mo><msub id="S3.p1.15.m15.2.5"><mi id="S3.p1.15.m15.2.5.2">M</mi><mi id="S3.p1.15.m15.2.5.3">a</mi></msub><mo id="S3.p1.15.m15.2.6" rspace="0em">/</mo><mo id="S3.p1.15.m15.2.7" lspace="0em" rspace="0.167em">∥</mo><msub id="S3.p1.15.m15.2.8"><mi id="S3.p1.15.m15.2.8.2">M</mi><mi id="S3.p1.15.m15.2.8.3">a</mi></msub><mo id="S3.p1.15.m15.2.9" lspace="0em">∥</mo></mrow><annotation encoding="application/x-tex" id="S3.p1.15.m15.2c">\operatorname{norm}(M)_{a}=M_{a}/\|M_{a}\|</annotation><annotation encoding="application/x-llamapun" id="S3.p1.15.m15.2d">roman_norm ( italic_M ) start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT = italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT / ∥ italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT ∥</annotation></semantics></math>.
Finally, let <math alttext="g(\bullet,\theta_{g}):\mathbb{T}\xrightarrow{}\mathbb{R}^{I\times d}" class="ltx_Math" display="inline" id="S3.p1.16.m16.2"><semantics id="S3.p1.16.m16.2a"><mrow id="S3.p1.16.m16.2.2" xref="S3.p1.16.m16.2.2.cmml"><mrow id="S3.p1.16.m16.2.2.1" xref="S3.p1.16.m16.2.2.1.cmml"><mi id="S3.p1.16.m16.2.2.1.3" xref="S3.p1.16.m16.2.2.1.3.cmml">g</mi><mo id="S3.p1.16.m16.2.2.1.2" xref="S3.p1.16.m16.2.2.1.2.cmml">⁢</mo><mrow id="S3.p1.16.m16.2.2.1.1.1" xref="S3.p1.16.m16.2.2.1.1.2.cmml"><mo id="S3.p1.16.m16.2.2.1.1.1.2" stretchy="false" xref="S3.p1.16.m16.2.2.1.1.2.cmml">(</mo><mo id="S3.p1.16.m16.1.1" lspace="0em" rspace="0em" xref="S3.p1.16.m16.1.1.cmml">∙</mo><mo id="S3.p1.16.m16.2.2.1.1.1.3" xref="S3.p1.16.m16.2.2.1.1.2.cmml">,</mo><msub id="S3.p1.16.m16.2.2.1.1.1.1" xref="S3.p1.16.m16.2.2.1.1.1.1.cmml"><mi id="S3.p1.16.m16.2.2.1.1.1.1.2" xref="S3.p1.16.m16.2.2.1.1.1.1.2.cmml">θ</mi><mi id="S3.p1.16.m16.2.2.1.1.1.1.3" xref="S3.p1.16.m16.2.2.1.1.1.1.3.cmml">g</mi></msub><mo id="S3.p1.16.m16.2.2.1.1.1.4" rspace="0.278em" stretchy="false" xref="S3.p1.16.m16.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.p1.16.m16.2.2.2" rspace="0.278em" xref="S3.p1.16.m16.2.2.2.cmml">:</mo><mrow id="S3.p1.16.m16.2.2.3" xref="S3.p1.16.m16.2.2.3.cmml"><mi id="S3.p1.16.m16.2.2.3.2" xref="S3.p1.16.m16.2.2.3.2.cmml">𝕋</mi><mover accent="true" id="S3.p1.16.m16.2.2.3.1" xref="S3.p1.16.m16.2.2.3.1.cmml"><mo id="S3.p1.16.m16.2.2.3.1.2" stretchy="false" xref="S3.p1.16.m16.2.2.3.1.2.cmml">→</mo><mi id="S3.p1.16.m16.2.2.3.1.1" xref="S3.p1.16.m16.2.2.3.1.1.cmml"></mi></mover><msup id="S3.p1.16.m16.2.2.3.3" xref="S3.p1.16.m16.2.2.3.3.cmml"><mi id="S3.p1.16.m16.2.2.3.3.2" xref="S3.p1.16.m16.2.2.3.3.2.cmml">ℝ</mi><mrow id="S3.p1.16.m16.2.2.3.3.3" xref="S3.p1.16.m16.2.2.3.3.3.cmml"><mi id="S3.p1.16.m16.2.2.3.3.3.2" xref="S3.p1.16.m16.2.2.3.3.3.2.cmml">I</mi><mo id="S3.p1.16.m16.2.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p1.16.m16.2.2.3.3.3.1.cmml">×</mo><mi id="S3.p1.16.m16.2.2.3.3.3.3" xref="S3.p1.16.m16.2.2.3.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.16.m16.2b"><apply id="S3.p1.16.m16.2.2.cmml" xref="S3.p1.16.m16.2.2"><ci id="S3.p1.16.m16.2.2.2.cmml" xref="S3.p1.16.m16.2.2.2">:</ci><apply id="S3.p1.16.m16.2.2.1.cmml" xref="S3.p1.16.m16.2.2.1"><times id="S3.p1.16.m16.2.2.1.2.cmml" xref="S3.p1.16.m16.2.2.1.2"></times><ci id="S3.p1.16.m16.2.2.1.3.cmml" xref="S3.p1.16.m16.2.2.1.3">𝑔</ci><interval closure="open" id="S3.p1.16.m16.2.2.1.1.2.cmml" xref="S3.p1.16.m16.2.2.1.1.1"><ci id="S3.p1.16.m16.1.1.cmml" xref="S3.p1.16.m16.1.1">∙</ci><apply id="S3.p1.16.m16.2.2.1.1.1.1.cmml" xref="S3.p1.16.m16.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.16.m16.2.2.1.1.1.1.1.cmml" xref="S3.p1.16.m16.2.2.1.1.1.1">subscript</csymbol><ci id="S3.p1.16.m16.2.2.1.1.1.1.2.cmml" xref="S3.p1.16.m16.2.2.1.1.1.1.2">𝜃</ci><ci id="S3.p1.16.m16.2.2.1.1.1.1.3.cmml" xref="S3.p1.16.m16.2.2.1.1.1.1.3">𝑔</ci></apply></interval></apply><apply id="S3.p1.16.m16.2.2.3.cmml" xref="S3.p1.16.m16.2.2.3"><apply id="S3.p1.16.m16.2.2.3.1.cmml" xref="S3.p1.16.m16.2.2.3.1"><csymbol cd="latexml" id="S3.p1.16.m16.2.2.3.1.1.cmml" xref="S3.p1.16.m16.2.2.3.1.1">absent</csymbol><ci id="S3.p1.16.m16.2.2.3.1.2.cmml" xref="S3.p1.16.m16.2.2.3.1.2">→</ci></apply><ci id="S3.p1.16.m16.2.2.3.2.cmml" xref="S3.p1.16.m16.2.2.3.2">𝕋</ci><apply id="S3.p1.16.m16.2.2.3.3.cmml" xref="S3.p1.16.m16.2.2.3.3"><csymbol cd="ambiguous" id="S3.p1.16.m16.2.2.3.3.1.cmml" xref="S3.p1.16.m16.2.2.3.3">superscript</csymbol><ci id="S3.p1.16.m16.2.2.3.3.2.cmml" xref="S3.p1.16.m16.2.2.3.3.2">ℝ</ci><apply id="S3.p1.16.m16.2.2.3.3.3.cmml" xref="S3.p1.16.m16.2.2.3.3.3"><times id="S3.p1.16.m16.2.2.3.3.3.1.cmml" xref="S3.p1.16.m16.2.2.3.3.3.1"></times><ci id="S3.p1.16.m16.2.2.3.3.3.2.cmml" xref="S3.p1.16.m16.2.2.3.3.3.2">𝐼</ci><ci id="S3.p1.16.m16.2.2.3.3.3.3.cmml" xref="S3.p1.16.m16.2.2.3.3.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.16.m16.2c">g(\bullet,\theta_{g}):\mathbb{T}\xrightarrow{}\mathbb{R}^{I\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.16.m16.2d">italic_g ( ∙ , italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) : blackboard_T start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW blackboard_R start_POSTSUPERSCRIPT italic_I × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> be a Transformer-based neural network with parameters <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p1.17.m17.1"><semantics id="S3.p1.17.m17.1a"><msub id="S3.p1.17.m17.1.1" xref="S3.p1.17.m17.1.1.cmml"><mi id="S3.p1.17.m17.1.1.2" xref="S3.p1.17.m17.1.1.2.cmml">θ</mi><mi id="S3.p1.17.m17.1.1.3" xref="S3.p1.17.m17.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.17.m17.1b"><apply id="S3.p1.17.m17.1.1.cmml" xref="S3.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.p1.17.m17.1.1.1.cmml" xref="S3.p1.17.m17.1.1">subscript</csymbol><ci id="S3.p1.17.m17.1.1.2.cmml" xref="S3.p1.17.m17.1.1.2">𝜃</ci><ci id="S3.p1.17.m17.1.1.3.cmml" xref="S3.p1.17.m17.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.17.m17.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.17.m17.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The training procedure starts with generating latent representations of items – the matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_A</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A=g(\mathbb{T},\theta_{g}).\\
" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">A</mi><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">g</mi><mo id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝕋</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">g</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.4" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" lspace="0em" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">𝐴</ci><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3">𝑔</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝕋</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">𝑔</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">A=g(\mathbb{T},\theta_{g}).\\
</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_A = italic_g ( blackboard_T , italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p2.2">Then, we can compute our loss <cite class="ltx_cite ltx_citemacro_citep">(Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=\big{\|}\operatorname{norm}\big{(}X_{u}\big{)}-\operatorname{norm}\big{(}X_{%
u}(AA^{\top}-\mathcal{I})\big{)}\big{\|}^{2}_{F}.\quad\\
" class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml">L</mi><mo id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">=</mo><msubsup id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">norm</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1a" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">u</mi></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">norm</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1a" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">⁡</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.2" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.2.cmml">X</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.3.cmml">u</mi></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.2.cmml">A</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.1.cmml">⁢</mo><msup id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.2.cmml">A</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.3.cmml">⊤</mo></msup></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.cmml">−</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml">ℐ</mi></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml">F</mi><mn id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">2</mn></msubsup></mrow><mo id="S3.E2.m1.3.3.1.2" lspace="0em" xref="S3.E2.m1.3.3.1.1.cmml">.</mo><mspace id="S3.E2.m1.3.3.1.3" width="1.167em" xref="S3.E2.m1.3.3.1.1.cmml"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"></eq><ci id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3">𝐿</ci><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.3"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">norm</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑢</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1"><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">norm</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.2">𝑋</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.3.3">𝑢</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.2">𝐴</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.2">𝐴</ci><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.3.3">top</csymbol></apply></apply><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.2.1.1.1.1.1.1.3">ℐ</ci></apply></apply></apply></apply></apply><cn id="S3.E2.m1.3.3.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.3">2</cn></apply><ci id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">L=\big{\|}\operatorname{norm}\big{(}X_{u}\big{)}-\operatorname{norm}\big{(}X_{%
u}(AA^{\top}-\mathcal{I})\big{)}\big{\|}^{2}_{F}.\quad\\
</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">italic_L = ∥ roman_norm ( italic_X start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) - roman_norm ( italic_X start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_A italic_A start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT - caligraphic_I ) ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.5">Finally, we compute the gradients of <math alttext="L" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_L</annotation></semantics></math> with respect to <math alttext="A" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_A</annotation></semantics></math>, and then the gradients for <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">θ</mi><mi id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝜃</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> are computed using the chain rule. However, when using common deep learning frameworks, there is a practical problem regarding the memory needed to track gradients for <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><msub id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml">θ</mi><mi id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1">subscript</csymbol><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">𝜃</ci><ci id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> because the size of matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">italic_A</annotation></semantics></math> depends on the number of <span class="ltx_text ltx_font_italic" id="S3.p3.5.1">all</span> items.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.7">We employ the following procedure (described in Algorithm
 <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#alg1" title="Algorithm 1 ‣ 3. Training Procedure ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a>) to address this memory problem: First, we compute the matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p4.1.m1.1"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.1d">italic_A</annotation></semantics></math> in batches without tracking the gradients for <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p4.2.m2.1"><semantics id="S3.p4.2.m2.1a"><msub id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">θ</mi><mi id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">𝜃</ci><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> (lines 1-5 of the algorithm). Then, we compute predictions (line 8), loss (lines 10-13) for a batch of users <math alttext="X_{u}" class="ltx_Math" display="inline" id="S3.p4.3.m3.1"><semantics id="S3.p4.3.m3.1a"><msub id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mi id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml">X</mi><mi id="S3.p4.3.m3.1.1.3" xref="S3.p4.3.m3.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.p4.3.m3.1.1.2.cmml" xref="S3.p4.3.m3.1.1.2">𝑋</ci><ci id="S3.p4.3.m3.1.1.3.cmml" xref="S3.p4.3.m3.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">X_{u}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.3.m3.1d">italic_X start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT</annotation></semantics></math>, and <span class="ltx_text ltx_font_italic" id="S3.p4.7.1">gradient checkpoint</span> <cite class="ltx_cite ltx_citemacro_citep">(Griewank and Walther, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib20" title="">2000</a>)</cite> for the matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p4.4.m4.1"><semantics id="S3.p4.4.m4.1a"><mi id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><ci id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p4.4.m4.1d">italic_A</annotation></semantics></math> (lines 14,16). Finally, we compute the matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p4.5.m5.1"><semantics id="S3.p4.5.m5.1a"><mi id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><ci id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p4.5.m5.1d">italic_A</annotation></semantics></math> in batches again and use the gradient checkpoint to compute the gradients for <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p4.6.m6.1"><semantics id="S3.p4.6.m6.1a"><msub id="S3.p4.6.m6.1.1" xref="S3.p4.6.m6.1.1.cmml"><mi id="S3.p4.6.m6.1.1.2" xref="S3.p4.6.m6.1.1.2.cmml">θ</mi><mi id="S3.p4.6.m6.1.1.3" xref="S3.p4.6.m6.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.6.m6.1b"><apply id="S3.p4.6.m6.1.1.cmml" xref="S3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.1.cmml" xref="S3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.p4.6.m6.1.1.2.cmml" xref="S3.p4.6.m6.1.1.2">𝜃</ci><ci id="S3.p4.6.m6.1.1.3.cmml" xref="S3.p4.6.m6.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m6.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.6.m6.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>. We <span class="ltx_text ltx_font_italic" id="S3.p4.7.2">accumulate gradients</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib8" title="">2016</a>)</cite> during the loop (lines 19-21). Finally, we update <math alttext="\theta_{g}" class="ltx_Math" display="inline" id="S3.p4.7.m7.1"><semantics id="S3.p4.7.m7.1a"><msub id="S3.p4.7.m7.1.1" xref="S3.p4.7.m7.1.1.cmml"><mi id="S3.p4.7.m7.1.1.2" xref="S3.p4.7.m7.1.1.2.cmml">θ</mi><mi id="S3.p4.7.m7.1.1.3" xref="S3.p4.7.m7.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.7.m7.1b"><apply id="S3.p4.7.m7.1.1.cmml" xref="S3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p4.7.m7.1.1.1.cmml" xref="S3.p4.7.m7.1.1">subscript</csymbol><ci id="S3.p4.7.m7.1.1.2.cmml" xref="S3.p4.7.m7.1.1.2">𝜃</ci><ci id="S3.p4.7.m7.1.1.3.cmml" xref="S3.p4.7.m7.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.7.m7.1c">\theta_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.7.m7.1d">italic_θ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> with a PyTorch optimizer (line 22).</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> beeFormer training step procedure in Python using PyTorch</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.3"><span class="ltx_text ltx_font_bold" id="alg1.3.1">Input:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.4"><span class="ltx_text" id="alg1.4.1">batch of interactions <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.4.1.1">X</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.5"><span class="ltx_text" id="alg1.5.1">Transformer model <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.5.1.1">transformer</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.6"><span class="ltx_text" id="alg1.6.1">sequences of tokens <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.6.1.1">tokenized_texts</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.7"><span class="ltx_text" id="alg1.7.1">PyTorch optimizer <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.7.1.1">optimizer</span> optimizing the parameters</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.8"><span class="ltx_text" id="alg1.8.1">of the Transformer model <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.8.1.1">transformer</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.9"><span class="ltx_text ltx_font_bold" id="alg1.9.1">Output:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.10"><span class="ltx_text" id="alg1.10.1">Transformer model with updated weights <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.10.1.1">transformer</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.11"><span class="ltx_text" id="alg1.11.1">predictions <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.11.1.1">X_pred</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_left" id="alg1.12"><span class="ltx_text" id="alg1.12.1">computed loss <span class="ltx_text ltx_lst_identifier ltx_lstlisting" id="alg1.12.1.1">loss</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_listing ltx_figure_panel ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_align_left ltx_listing" id="alg1.13">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,d2l0aCB0b3JjaC5ub19ncmFkKCk6CiAgICBBX2xpc3QgPSBbXQogICAgZm9yIHQgaW4gdG9rZW5pemVkX3RleHRzOgogICAgICAgIEFfbGlzdC5hcHBlbmQodHJhbnNmb3JtZXIodCkpCiAgICBBID0gdG9yY2gudnN0YWNrKEFfbGlzdCkKCkEucmVxdWlyZXNfZ3JhZCA9IFRydWUKWF9wcmVkID0gWCBAIEEgQCBBLlQgLSBYCgpsb3NzID0gdG9yY2gubm4uTVNFTG9zcygKICAgIHRvcmNoLm5uLmZ1bmN0aW9uYWwubm9ybWFsaXplKFgpLAogICAgdG9yY2gubm4uZnVuY3Rpb25hbC5ub3JtYWxpemUoWF9wcmVkKSwKKQpsb3NzLmJhY2t3YXJkKCkKCmNoZWNrcG9pbnQgPSBBLmdyYWQKCm9wdGltaXplci56ZXJvX2dyYWQoKQpmb3IgaSwgdCBpbiBlbnVtZXJhdGUodG9rZW5pemVkX3RleHRzKToKICAgIEFfaSA9IHRyYW5zZm9ybWVyKHQpCiAgICBBX2kuYmFja3dhcmQoZ3JhZGllbnQ9Y2hlY2twb2ludFtpXSkKb3B0aW1pemVyLnN0ZXAoKQ==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_tag ltx_tag_listingline">1</span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx1.1">with</span><span class="ltx_text ltx_lst_space" id="lstnumberx1.2"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx1.3">torch</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx1.4">no_grad</span>():
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_tag ltx_tag_listingline">2</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.2">A_list</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.3"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx2.4"> </span>[]
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_tag ltx_tag_listingline">3</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.1"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx3.2">for</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.3"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.4">t</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.5"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx3.6">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.7"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.8">tokenized_texts</span>:
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_tag ltx_tag_listingline">4</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.2">A_list</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.3">append</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.4">transformer</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.5">t</span>))
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_tag ltx_tag_listingline">5</span><span class="ltx_text ltx_lst_space" id="lstnumberx5.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx5.2">A</span><span class="ltx_text ltx_lst_space" id="lstnumberx5.3"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx5.4"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx5.5">torch</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx5.6">vstack</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx5.7">A_list</span>)
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_tag ltx_tag_listingline">6</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_tag ltx_tag_listingline">7</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx7.1">A</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx7.2">requires_grad</span><span class="ltx_text ltx_lst_space" id="lstnumberx7.3"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx7.4"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx7.5">True</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_tag ltx_tag_listingline">8</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.1">X_pred</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.2"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx8.3"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.4">X</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.5"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.6">@</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.7"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.8">A</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.9"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.10">@</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.11"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.12">A</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx8.13">T</span><span class="ltx_text ltx_lst_space" id="lstnumberx8.14"> </span>-<span class="ltx_text ltx_lst_space" id="lstnumberx8.15"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx8.16">X</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_tag ltx_tag_listingline">9</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_tag ltx_tag_listingline">10</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx10.1">loss</span><span class="ltx_text ltx_lst_space" id="lstnumberx10.2"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx10.3"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx10.4">torch</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx10.5">nn</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx10.6">MSELoss</span>(
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_tag ltx_tag_listingline">11</span><span class="ltx_text ltx_lst_space" id="lstnumberx11.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx11.2">torch</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx11.3">nn</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx11.4">functional</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx11.5">normalize</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx11.6">X</span>),
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_tag ltx_tag_listingline">12</span><span class="ltx_text ltx_lst_space" id="lstnumberx12.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx12.2">torch</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx12.3">nn</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx12.4">functional</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx12.5">normalize</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx12.6">X_pred</span>),
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_tag ltx_tag_listingline">13</span>)
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_tag ltx_tag_listingline">14</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx14.1">loss</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx14.2">backward</span>()
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_tag ltx_tag_listingline">15</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_tag ltx_tag_listingline">16</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx16.1">checkpoint</span><span class="ltx_text ltx_lst_space" id="lstnumberx16.2"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx16.3"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx16.4">A</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx16.5">grad</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_tag ltx_tag_listingline">17</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_tag ltx_tag_listingline">18</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx18.1">optimizer</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx18.2">zero_grad</span>()
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_tag ltx_tag_listingline">19</span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx19.1">for</span><span class="ltx_text ltx_lst_space" id="lstnumberx19.2"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx19.3">i</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx19.4"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx19.5">t</span><span class="ltx_text ltx_lst_space" id="lstnumberx19.6"> </span><span class="ltx_text ltx_lst_keyword ltx_font_bold" id="lstnumberx19.7">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx19.8"> </span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold" id="lstnumberx19.9">enumerate</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx19.10">tokenized_texts</span>):
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_tag ltx_tag_listingline">20</span><span class="ltx_text ltx_lst_space" id="lstnumberx20.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx20.2">A_i</span><span class="ltx_text ltx_lst_space" id="lstnumberx20.3"> </span>=<span class="ltx_text ltx_lst_space" id="lstnumberx20.4"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx20.5">transformer</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx20.6">t</span>)
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_tag ltx_tag_listingline">21</span><span class="ltx_text ltx_lst_space" id="lstnumberx21.1"> </span><span class="ltx_text ltx_lst_identifier" id="lstnumberx21.2">A_i</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx21.3">backward</span>(<span class="ltx_text ltx_lst_identifier" id="lstnumberx21.4">gradient</span>=<span class="ltx_text ltx_lst_identifier" id="lstnumberx21.5">checkpoint</span>[<span class="ltx_text ltx_lst_identifier" id="lstnumberx21.6">i</span>])
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_tag ltx_tag_listingline">22</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx22.1">optimizer</span>.<span class="ltx_text ltx_lst_identifier" id="lstnumberx22.2">step</span>()
</div>
</div>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.19">Using the algorithm above, we effectively enable training of a Transformer model using gradients computed with the ELSA algorithm on top of it, from the memory point of view. However, computing <math alttext="A" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">italic_A</annotation></semantics></math> for all items in every training step quickly becomes time-consuming for datasets with large number of items. At this point, we would like to note one property specific to recommender systems: interaction matrix <math alttext="X" class="ltx_Math" display="inline" id="S3.p5.2.m2.1"><semantics id="S3.p5.2.m2.1a"><mi id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><ci id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">italic_X</annotation></semantics></math> is typically (very) sparse. This property means that when we sample a random batch of user interaction vectors from <math alttext="X" class="ltx_Math" display="inline" id="S3.p5.3.m3.1"><semantics id="S3.p5.3.m3.1a"><mi id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><ci id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">italic_X</annotation></semantics></math>, we obtain interactions only with a limited number of items, meaning that it is not necessary to encode all items to matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p5.4.m4.1"><semantics id="S3.p5.4.m4.1a"><mi id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><ci id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p5.4.m4.1d">italic_A</annotation></semantics></math> in every training step. More formally, let <math alttext="\mathbb{I}_{b}" class="ltx_Math" display="inline" id="S3.p5.5.m5.1"><semantics id="S3.p5.5.m5.1a"><msub id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml"><mi id="S3.p5.5.m5.1.1.2" xref="S3.p5.5.m5.1.1.2.cmml">𝕀</mi><mi id="S3.p5.5.m5.1.1.3" xref="S3.p5.5.m5.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.1b"><apply id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p5.5.m5.1.1.1.cmml" xref="S3.p5.5.m5.1.1">subscript</csymbol><ci id="S3.p5.5.m5.1.1.2.cmml" xref="S3.p5.5.m5.1.1.2">𝕀</ci><ci id="S3.p5.5.m5.1.1.3.cmml" xref="S3.p5.5.m5.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.1c">\mathbb{I}_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.5.m5.1d">blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> be a subset of <math alttext="\mathbb{I}" class="ltx_Math" display="inline" id="S3.p5.6.m6.1"><semantics id="S3.p5.6.m6.1a"><mi id="S3.p5.6.m6.1.1" xref="S3.p5.6.m6.1.1.cmml">𝕀</mi><annotation-xml encoding="MathML-Content" id="S3.p5.6.m6.1b"><ci id="S3.p5.6.m6.1.1.cmml" xref="S3.p5.6.m6.1.1">𝕀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.6.m6.1c">\mathbb{I}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.6.m6.1d">blackboard_I</annotation></semantics></math> derived from the interactions presented in a random batch <math alttext="b" class="ltx_Math" display="inline" id="S3.p5.7.m7.1"><semantics id="S3.p5.7.m7.1a"><mi id="S3.p5.7.m7.1.1" xref="S3.p5.7.m7.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.p5.7.m7.1b"><ci id="S3.p5.7.m7.1.1.cmml" xref="S3.p5.7.m7.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.7.m7.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.p5.7.m7.1d">italic_b</annotation></semantics></math> sampled from <math alttext="X" class="ltx_Math" display="inline" id="S3.p5.8.m8.1"><semantics id="S3.p5.8.m8.1a"><mi id="S3.p5.8.m8.1.1" xref="S3.p5.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p5.8.m8.1b"><ci id="S3.p5.8.m8.1.1.cmml" xref="S3.p5.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.8.m8.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p5.8.m8.1d">italic_X</annotation></semantics></math>. Recent work shows that significant improvement in performance during training CF models can be achieved by negative sampling <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib15" title="">2020</a>)</cite>. We implement negative sampling by adding random items to <math alttext="\mathbb{I}_{b}" class="ltx_Math" display="inline" id="S3.p5.9.m9.1"><semantics id="S3.p5.9.m9.1a"><msub id="S3.p5.9.m9.1.1" xref="S3.p5.9.m9.1.1.cmml"><mi id="S3.p5.9.m9.1.1.2" xref="S3.p5.9.m9.1.1.2.cmml">𝕀</mi><mi id="S3.p5.9.m9.1.1.3" xref="S3.p5.9.m9.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.9.m9.1b"><apply id="S3.p5.9.m9.1.1.cmml" xref="S3.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p5.9.m9.1.1.1.cmml" xref="S3.p5.9.m9.1.1">subscript</csymbol><ci id="S3.p5.9.m9.1.1.2.cmml" xref="S3.p5.9.m9.1.1.2">𝕀</ci><ci id="S3.p5.9.m9.1.1.3.cmml" xref="S3.p5.9.m9.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.9.m9.1c">\mathbb{I}_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.9.m9.1d">blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> and fixing the total number of items observed in each step during training to <math alttext="m" class="ltx_Math" display="inline" id="S3.p5.10.m10.1"><semantics id="S3.p5.10.m10.1a"><mi id="S3.p5.10.m10.1.1" xref="S3.p5.10.m10.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p5.10.m10.1b"><ci id="S3.p5.10.m10.1.1.cmml" xref="S3.p5.10.m10.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.10.m10.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.p5.10.m10.1d">italic_m</annotation></semantics></math>. The <math alttext="m" class="ltx_Math" display="inline" id="S3.p5.11.m11.1"><semantics id="S3.p5.11.m11.1a"><mi id="S3.p5.11.m11.1.1" xref="S3.p5.11.m11.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p5.11.m11.1b"><ci id="S3.p5.11.m11.1.1.cmml" xref="S3.p5.11.m11.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.11.m11.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.p5.11.m11.1d">italic_m</annotation></semantics></math> becomes an additional hyperparameter fulfilling <math alttext="|\mathbb{I}_{b}|\leq m\ll|\mathbb{I}|" class="ltx_Math" display="inline" id="S3.p5.12.m12.2"><semantics id="S3.p5.12.m12.2a"><mrow id="S3.p5.12.m12.2.2" xref="S3.p5.12.m12.2.2.cmml"><mrow id="S3.p5.12.m12.2.2.1.1" xref="S3.p5.12.m12.2.2.1.2.cmml"><mo id="S3.p5.12.m12.2.2.1.1.2" stretchy="false" xref="S3.p5.12.m12.2.2.1.2.1.cmml">|</mo><msub id="S3.p5.12.m12.2.2.1.1.1" xref="S3.p5.12.m12.2.2.1.1.1.cmml"><mi id="S3.p5.12.m12.2.2.1.1.1.2" xref="S3.p5.12.m12.2.2.1.1.1.2.cmml">𝕀</mi><mi id="S3.p5.12.m12.2.2.1.1.1.3" xref="S3.p5.12.m12.2.2.1.1.1.3.cmml">b</mi></msub><mo id="S3.p5.12.m12.2.2.1.1.3" stretchy="false" xref="S3.p5.12.m12.2.2.1.2.1.cmml">|</mo></mrow><mo id="S3.p5.12.m12.2.2.3" xref="S3.p5.12.m12.2.2.3.cmml">≤</mo><mi id="S3.p5.12.m12.2.2.4" xref="S3.p5.12.m12.2.2.4.cmml">m</mi><mo id="S3.p5.12.m12.2.2.5" xref="S3.p5.12.m12.2.2.5.cmml">≪</mo><mrow id="S3.p5.12.m12.2.2.6.2" xref="S3.p5.12.m12.2.2.6.1.cmml"><mo id="S3.p5.12.m12.2.2.6.2.1" stretchy="false" xref="S3.p5.12.m12.2.2.6.1.1.cmml">|</mo><mi id="S3.p5.12.m12.1.1" xref="S3.p5.12.m12.1.1.cmml">𝕀</mi><mo id="S3.p5.12.m12.2.2.6.2.2" stretchy="false" xref="S3.p5.12.m12.2.2.6.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.12.m12.2b"><apply id="S3.p5.12.m12.2.2.cmml" xref="S3.p5.12.m12.2.2"><and id="S3.p5.12.m12.2.2a.cmml" xref="S3.p5.12.m12.2.2"></and><apply id="S3.p5.12.m12.2.2b.cmml" xref="S3.p5.12.m12.2.2"><leq id="S3.p5.12.m12.2.2.3.cmml" xref="S3.p5.12.m12.2.2.3"></leq><apply id="S3.p5.12.m12.2.2.1.2.cmml" xref="S3.p5.12.m12.2.2.1.1"><abs id="S3.p5.12.m12.2.2.1.2.1.cmml" xref="S3.p5.12.m12.2.2.1.1.2"></abs><apply id="S3.p5.12.m12.2.2.1.1.1.cmml" xref="S3.p5.12.m12.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p5.12.m12.2.2.1.1.1.1.cmml" xref="S3.p5.12.m12.2.2.1.1.1">subscript</csymbol><ci id="S3.p5.12.m12.2.2.1.1.1.2.cmml" xref="S3.p5.12.m12.2.2.1.1.1.2">𝕀</ci><ci id="S3.p5.12.m12.2.2.1.1.1.3.cmml" xref="S3.p5.12.m12.2.2.1.1.1.3">𝑏</ci></apply></apply><ci id="S3.p5.12.m12.2.2.4.cmml" xref="S3.p5.12.m12.2.2.4">𝑚</ci></apply><apply id="S3.p5.12.m12.2.2c.cmml" xref="S3.p5.12.m12.2.2"><csymbol cd="latexml" id="S3.p5.12.m12.2.2.5.cmml" xref="S3.p5.12.m12.2.2.5">much-less-than</csymbol><share href="https://arxiv.org/html/2409.10309v2#S3.p5.12.m12.2.2.4.cmml" id="S3.p5.12.m12.2.2d.cmml" xref="S3.p5.12.m12.2.2"></share><apply id="S3.p5.12.m12.2.2.6.1.cmml" xref="S3.p5.12.m12.2.2.6.2"><abs id="S3.p5.12.m12.2.2.6.1.1.cmml" xref="S3.p5.12.m12.2.2.6.2.1"></abs><ci id="S3.p5.12.m12.1.1.cmml" xref="S3.p5.12.m12.1.1">𝕀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.12.m12.2c">|\mathbb{I}_{b}|\leq m\ll|\mathbb{I}|</annotation><annotation encoding="application/x-llamapun" id="S3.p5.12.m12.2d">| blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT | ≤ italic_m ≪ | blackboard_I |</annotation></semantics></math> for any possible <math alttext="\mathbb{I}_{b}" class="ltx_Math" display="inline" id="S3.p5.13.m13.1"><semantics id="S3.p5.13.m13.1a"><msub id="S3.p5.13.m13.1.1" xref="S3.p5.13.m13.1.1.cmml"><mi id="S3.p5.13.m13.1.1.2" xref="S3.p5.13.m13.1.1.2.cmml">𝕀</mi><mi id="S3.p5.13.m13.1.1.3" xref="S3.p5.13.m13.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.13.m13.1b"><apply id="S3.p5.13.m13.1.1.cmml" xref="S3.p5.13.m13.1.1"><csymbol cd="ambiguous" id="S3.p5.13.m13.1.1.1.cmml" xref="S3.p5.13.m13.1.1">subscript</csymbol><ci id="S3.p5.13.m13.1.1.2.cmml" xref="S3.p5.13.m13.1.1.2">𝕀</ci><ci id="S3.p5.13.m13.1.1.3.cmml" xref="S3.p5.13.m13.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.13.m13.1c">\mathbb{I}_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.13.m13.1d">blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, to control the size of the matrix <math alttext="A" class="ltx_Math" display="inline" id="S3.p5.14.m14.1"><semantics id="S3.p5.14.m14.1a"><mi id="S3.p5.14.m14.1.1" xref="S3.p5.14.m14.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p5.14.m14.1b"><ci id="S3.p5.14.m14.1.1.cmml" xref="S3.p5.14.m14.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.14.m14.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p5.14.m14.1d">italic_A</annotation></semantics></math>. Finally, in a situation when <math alttext="|\mathbb{I}_{b}|&lt;m" class="ltx_Math" display="inline" id="S3.p5.15.m15.1"><semantics id="S3.p5.15.m15.1a"><mrow id="S3.p5.15.m15.1.1" xref="S3.p5.15.m15.1.1.cmml"><mrow id="S3.p5.15.m15.1.1.1.1" xref="S3.p5.15.m15.1.1.1.2.cmml"><mo id="S3.p5.15.m15.1.1.1.1.2" stretchy="false" xref="S3.p5.15.m15.1.1.1.2.1.cmml">|</mo><msub id="S3.p5.15.m15.1.1.1.1.1" xref="S3.p5.15.m15.1.1.1.1.1.cmml"><mi id="S3.p5.15.m15.1.1.1.1.1.2" xref="S3.p5.15.m15.1.1.1.1.1.2.cmml">𝕀</mi><mi id="S3.p5.15.m15.1.1.1.1.1.3" xref="S3.p5.15.m15.1.1.1.1.1.3.cmml">b</mi></msub><mo id="S3.p5.15.m15.1.1.1.1.3" stretchy="false" xref="S3.p5.15.m15.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.p5.15.m15.1.1.2" xref="S3.p5.15.m15.1.1.2.cmml">&lt;</mo><mi id="S3.p5.15.m15.1.1.3" xref="S3.p5.15.m15.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.15.m15.1b"><apply id="S3.p5.15.m15.1.1.cmml" xref="S3.p5.15.m15.1.1"><lt id="S3.p5.15.m15.1.1.2.cmml" xref="S3.p5.15.m15.1.1.2"></lt><apply id="S3.p5.15.m15.1.1.1.2.cmml" xref="S3.p5.15.m15.1.1.1.1"><abs id="S3.p5.15.m15.1.1.1.2.1.cmml" xref="S3.p5.15.m15.1.1.1.1.2"></abs><apply id="S3.p5.15.m15.1.1.1.1.1.cmml" xref="S3.p5.15.m15.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p5.15.m15.1.1.1.1.1.1.cmml" xref="S3.p5.15.m15.1.1.1.1.1">subscript</csymbol><ci id="S3.p5.15.m15.1.1.1.1.1.2.cmml" xref="S3.p5.15.m15.1.1.1.1.1.2">𝕀</ci><ci id="S3.p5.15.m15.1.1.1.1.1.3.cmml" xref="S3.p5.15.m15.1.1.1.1.1.3">𝑏</ci></apply></apply><ci id="S3.p5.15.m15.1.1.3.cmml" xref="S3.p5.15.m15.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.15.m15.1c">|\mathbb{I}_{b}|&lt;m</annotation><annotation encoding="application/x-llamapun" id="S3.p5.15.m15.1d">| blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT | &lt; italic_m</annotation></semantics></math>, we select uniformly at random <math alttext="m-|\mathbb{I}_{b}|" class="ltx_Math" display="inline" id="S3.p5.16.m16.1"><semantics id="S3.p5.16.m16.1a"><mrow id="S3.p5.16.m16.1.1" xref="S3.p5.16.m16.1.1.cmml"><mi id="S3.p5.16.m16.1.1.3" xref="S3.p5.16.m16.1.1.3.cmml">m</mi><mo id="S3.p5.16.m16.1.1.2" xref="S3.p5.16.m16.1.1.2.cmml">−</mo><mrow id="S3.p5.16.m16.1.1.1.1" xref="S3.p5.16.m16.1.1.1.2.cmml"><mo id="S3.p5.16.m16.1.1.1.1.2" stretchy="false" xref="S3.p5.16.m16.1.1.1.2.1.cmml">|</mo><msub id="S3.p5.16.m16.1.1.1.1.1" xref="S3.p5.16.m16.1.1.1.1.1.cmml"><mi id="S3.p5.16.m16.1.1.1.1.1.2" xref="S3.p5.16.m16.1.1.1.1.1.2.cmml">𝕀</mi><mi id="S3.p5.16.m16.1.1.1.1.1.3" xref="S3.p5.16.m16.1.1.1.1.1.3.cmml">b</mi></msub><mo id="S3.p5.16.m16.1.1.1.1.3" stretchy="false" xref="S3.p5.16.m16.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.16.m16.1b"><apply id="S3.p5.16.m16.1.1.cmml" xref="S3.p5.16.m16.1.1"><minus id="S3.p5.16.m16.1.1.2.cmml" xref="S3.p5.16.m16.1.1.2"></minus><ci id="S3.p5.16.m16.1.1.3.cmml" xref="S3.p5.16.m16.1.1.3">𝑚</ci><apply id="S3.p5.16.m16.1.1.1.2.cmml" xref="S3.p5.16.m16.1.1.1.1"><abs id="S3.p5.16.m16.1.1.1.2.1.cmml" xref="S3.p5.16.m16.1.1.1.1.2"></abs><apply id="S3.p5.16.m16.1.1.1.1.1.cmml" xref="S3.p5.16.m16.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p5.16.m16.1.1.1.1.1.1.cmml" xref="S3.p5.16.m16.1.1.1.1.1">subscript</csymbol><ci id="S3.p5.16.m16.1.1.1.1.1.2.cmml" xref="S3.p5.16.m16.1.1.1.1.1.2">𝕀</ci><ci id="S3.p5.16.m16.1.1.1.1.1.3.cmml" xref="S3.p5.16.m16.1.1.1.1.1.3">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.16.m16.1c">m-|\mathbb{I}_{b}|</annotation><annotation encoding="application/x-llamapun" id="S3.p5.16.m16.1d">italic_m - | blackboard_I start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT |</annotation></semantics></math> items from the catalog and add them to the sampled batch from <math alttext="X" class="ltx_Math" display="inline" id="S3.p5.17.m17.1"><semantics id="S3.p5.17.m17.1a"><mi id="S3.p5.17.m17.1.1" xref="S3.p5.17.m17.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p5.17.m17.1b"><ci id="S3.p5.17.m17.1.1.cmml" xref="S3.p5.17.m17.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.17.m17.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.p5.17.m17.1d">italic_X</annotation></semantics></math> with zeros in corresponding columns. A notable advantage of our approach is that the asymptotic complexity of beeFormer does not depends on the overall number of items <math alttext="|\mathcal{I}|" class="ltx_Math" display="inline" id="S3.p5.18.m18.1"><semantics id="S3.p5.18.m18.1a"><mrow id="S3.p5.18.m18.1.2.2" xref="S3.p5.18.m18.1.2.1.cmml"><mo id="S3.p5.18.m18.1.2.2.1" stretchy="false" xref="S3.p5.18.m18.1.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.p5.18.m18.1.1" xref="S3.p5.18.m18.1.1.cmml">ℐ</mi><mo id="S3.p5.18.m18.1.2.2.2" stretchy="false" xref="S3.p5.18.m18.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.18.m18.1b"><apply id="S3.p5.18.m18.1.2.1.cmml" xref="S3.p5.18.m18.1.2.2"><abs id="S3.p5.18.m18.1.2.1.1.cmml" xref="S3.p5.18.m18.1.2.2.1"></abs><ci id="S3.p5.18.m18.1.1.cmml" xref="S3.p5.18.m18.1.1">ℐ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.18.m18.1c">|\mathcal{I}|</annotation><annotation encoding="application/x-llamapun" id="S3.p5.18.m18.1d">| caligraphic_I |</annotation></semantics></math> but only on the hyperparameter <math alttext="m" class="ltx_Math" display="inline" id="S3.p5.19.m19.1"><semantics id="S3.p5.19.m19.1a"><mi id="S3.p5.19.m19.1.1" xref="S3.p5.19.m19.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p5.19.m19.1b"><ci id="S3.p5.19.m19.1.1.cmml" xref="S3.p5.19.m19.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.19.m19.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.p5.19.m19.1d">italic_m</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experimental Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate our models on several popular datasets for evaluating recommender systems: <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">MovieLens20M</span> (ML20M) <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib21" title="">2015</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">Goodbooks-10k</span> (GB10k) <cite class="ltx_cite ltx_citemacro_citep">(Zajac, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib50" title="">2017</a>)</cite>, and <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">Amazon Books</span> (AB) <cite class="ltx_cite ltx_citemacro_citep">(Ni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib32" title="">2019</a>)</cite>. Since these datasets contain explicit ratings, we transform them into implicit feedback datasets by considering a rating of four or higher as an interaction between the user and the item, and we keep only users with at least five interactions. Then, we collect movie plots from the IMDB Movies Analysis dataset <cite class="ltx_cite ltx_citemacro_citep">(Mhatre, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib30" title="">2020</a>)</cite> to obtain the descriptions for the items of the MovieLens20M dataset, and we collect book descriptions from the Goodreads books - 31 features dataset <cite class="ltx_cite ltx_citemacro_citep">(Reese, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib34" title="">2020</a>)</cite> and Goodreads 100K books dataset <cite class="ltx_cite ltx_citemacro_citep">(Dhamani, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib12" title="">2021</a>)</cite> to get the descriptions for the Goodbooks-10k dataset; most items in the Amazon Books dataset already contains descriptions. We then remove the items without descriptions.
Finally, we use the Meta-Llama-3.1-8B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>LLAMA 3.1 license allows the use of generated output to train new language models. We add the prefix ”Llama” to the names of our models to comply with license terms.</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(AI@Meta, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib3" title="">2024</a>)</cite> to generate standardized item descriptions from the existing ones to train our models.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Using Llama to generate the item descriptions allows us to publish them. More details about the item description generation are available on our GitHub page.</span></span></span> Since the LLM refuse to generate descriptions for some items (for example, because it refuses to generate explicit content), we remove such items from the dataset. Summary of the resulting dataset’s properties is available in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T1" title="Table 1 ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Detailed statistics of datasets used for evaluation.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.5.6.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.5.6.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.5.6.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">GB10k</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.5.6.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">ML20M</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.5.6.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">AB</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"># of items in <math alttext="X" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mi id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">italic_X</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">9 975</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">16 902</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">63 305</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.2.1" style="padding-left:10.0pt;padding-right:10.0pt;"># of users in <math alttext="X" class="ltx_Math" display="inline" id="S4.T1.2.2.1.m1.1"><semantics id="S4.T1.2.2.1.m1.1a"><mi id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><ci id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.1.m1.1d">italic_X</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">53 365</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">136 589</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">634 964</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.3.1" style="padding-left:10.0pt;padding-right:10.0pt;"># of interactions in <math alttext="X" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><mi id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><ci id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">italic_X</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right" id="S4.T1.3.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">4.20 M</td>
<td class="ltx_td ltx_align_right" id="S4.T1.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">9.69 M</td>
<td class="ltx_td ltx_align_right" id="S4.T1.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">8.29 M</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.4.1" style="padding-left:10.0pt;padding-right:10.0pt;">density of <math alttext="X" class="ltx_Math" display="inline" id="S4.T1.4.4.1.m1.1"><semantics id="S4.T1.4.4.1.m1.1a"><mi id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><ci id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.1.m1.1d">italic_X</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right" id="S4.T1.4.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.77 %</td>
<td class="ltx_td ltx_align_right" id="S4.T1.4.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.42 %</td>
<td class="ltx_td ltx_align_right" id="S4.T1.4.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.02 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.5.5.1" style="padding-left:10.0pt;padding-right:10.0pt;">density of <math alttext="X^{T}X" class="ltx_Math" display="inline" id="S4.T1.5.5.1.m1.1"><semantics id="S4.T1.5.5.1.m1.1a"><mrow id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml"><msup id="S4.T1.5.5.1.m1.1.1.2" xref="S4.T1.5.5.1.m1.1.1.2.cmml"><mi id="S4.T1.5.5.1.m1.1.1.2.2" xref="S4.T1.5.5.1.m1.1.1.2.2.cmml">X</mi><mi id="S4.T1.5.5.1.m1.1.1.2.3" xref="S4.T1.5.5.1.m1.1.1.2.3.cmml">T</mi></msup><mo id="S4.T1.5.5.1.m1.1.1.1" xref="S4.T1.5.5.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.5.5.1.m1.1.1.3" xref="S4.T1.5.5.1.m1.1.1.3.cmml">X</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><apply id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"><times id="S4.T1.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1.1"></times><apply id="S4.T1.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T1.5.5.1.m1.1.1.2.1.cmml" xref="S4.T1.5.5.1.m1.1.1.2">superscript</csymbol><ci id="S4.T1.5.5.1.m1.1.1.2.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2.2">𝑋</ci><ci id="S4.T1.5.5.1.m1.1.1.2.3.cmml" xref="S4.T1.5.5.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S4.T1.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.1.m1.1.1.3">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">X^{T}X</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.1.m1.1d">italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.5.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">41.22 %</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.5.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">26.93 %</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.5.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">7.59 %</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We use two setups for our experiments. First, to test the ability of beeFormer to generalize toward new items, we split ML20M and GB10k datasets item-wise (item-split): we randomly choose 2000 items as the test set, and we use the rest to cross-validate and train our models. Next, we simulate a real-world scenario with the AB dataset: we sorted all interactions by timestamp and used the last 20% of interactions as a test set (time-split). Again, the remaining interactions were used as validation and training sets.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Item-split Setup</h4>
<div class="ltx_para" id="S4.SS0.SSSx1.p1">
<p class="ltx_p" id="S4.SS0.SSSx1.p1.1">In the item-split setup, we use the following scenarios: Firs, a zero-shot scenario, where the models were not trained on the evaluated dataset – they need to transfer knowledge from other datasets. We use CBF in this scenario – we generate item embeddings with a sentence Transformer, and then we use cosine similarities between item embeddings to provide recommendations. Second, in a cold-start scenario,
the models use the text side information to generalize towards new, previously unseen items. We use the Heater model <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib52" title="">2020</a>)</cite> mapping interaction data to side information, to benchmark the baseline models. Again, for our models, we use CBF.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSSx1.p2">
<p class="ltx_p" id="S4.SS0.SSSx1.p2.1">We choose to compare our beeFormer-trained models to three best-performing sentence Transformer models:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">all-mpnet-base-v2</span>, the best performing model from the sentence Transformers <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib35" title="">2019</a>)</cite> library. It is based on MPNET <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib40" title="">2020</a>)</cite>, a model pre-trained with combined masked and permuted language modeling and then finetuned on various datasets.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">BAAI/bge-m3</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib7" title="">2024</a>)</cite>, which uses three-fold versatility (dense, sparse/lexical, and multi-vector) retrieval during training. It is trained and finetuned on various datasets and synthetic data.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">nomic-ai/nomic-embed-text-v1.5</span> <cite class="ltx_cite ltx_citemacro_citep">(Nussbaum et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib33" title="">2024</a>)</cite>, which uses Flash attention <cite class="ltx_cite ltx_citemacro_citep">(Dao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib10" title="">2022</a>)</cite> to handle longer context (up to 8192 tokens for predictions.) It was trained using a Contrastor framework.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nomic-ai/contrastors" title="">https://github.com/nomic-ai/contrastors</a></span></span></span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Time-split Setup</h4>
<div class="ltx_para" id="S4.SS0.SSSx2.p1">
<p class="ltx_p" id="S4.SS0.SSSx2.p1.1">Similarly, for the time-split setup, we use a zero-shot scenario and supervised models trained with interaction data. This setup allows the use of classical CF; we chose KNN <cite class="ltx_cite ltx_citemacro_citep">(Sarwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib36" title="">2001</a>)</cite>, ALS matrix factorization <cite class="ltx_cite ltx_citemacro_citep">(Takács et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib46" title="">2011</a>)</cite>, ELSA <cite class="ltx_cite ltx_citemacro_citep">(Vančura et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib48" title="">2022</a>)</cite>, and SANSA <cite class="ltx_cite ltx_citemacro_citep">(Spišák et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib42" title="">2023</a>)</cite> as baselines.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSSx2.p2">
<p class="ltx_p" id="S4.SS0.SSSx2.p2.1">We train four models by finetuning the initial <span class="ltx_text ltx_font_italic" id="S4.SS0.SSSx2.p2.1.1">all-mpnet-base-v2</span> model: one for each dataset and one model combining the data from the ML20M and GB10k datasets (denoted <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSSx2.p2.1.2">goodlens</em>). The resulting models are available on our Huggingface page.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/beeformer" title="">https://huggingface.co/beeformer</a></span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS0.SSSx2.p3">
<p class="ltx_p" id="S4.SS0.SSSx2.p3.1">We use Recall@20 (R@20), Recall@50 (R@50), and NDCG@100 (N@100) metrics computed using the RecPack <cite class="ltx_cite ltx_citemacro_citep">(Michiels et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#bib.bib31" title="">2022</a>)</cite> framework to compare the models. We also calculate the standard error for each experiment with bootstrap resampling.</p>
</div>
<div class="ltx_para" id="S4.SS0.SSSx2.p4">
<p class="ltx_p" id="S4.SS0.SSSx2.p4.1">We publicly share further details about the datasets, description of the LLM generative procedure, resulting data for reproducibility, hyperparameters, and other technical details on our GitHub page
, along with all the source code.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/recombee/beeformer" title="">https://github.com/recombee/beeformer</a></span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Results</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Item-split setup</h4>
<div class="ltx_para" id="S4.SS1.SSSx1.p1">
<p class="ltx_p" id="S4.SS1.SSSx1.p1.1">In the zero-shot scenario, we observe that the beeFormer models trained on a different domain (books vs. movies) significantly outperform all baselines. Detailed results are in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T2" title="Table 2 ‣ Item-split setup ‣ 4.1. Results ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Results for zero-shot scenario in item-split setup. Names of our models trained with beeFormer are in italics, the best-performing models are represented in bold, and the best baseline for each scenario is underlined. The standard error of all values is below 0.0001 (evaluated via bootstrap resampling).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2" style="padding-left:4.5pt;padding-right:4.5pt;">Sentence Transformer</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3" style="padding-left:4.5pt;padding-right:4.5pt;">R@20</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">R@50</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">N@100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.1" rowspan="4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.1.2.1.1.1">GB10K</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.2" style="padding-left:4.5pt;padding-right:4.5pt;">all-mpnet-base-v2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.3" style="padding-left:4.5pt;padding-right:4.5pt;">0.1017</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">0.1886</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.2.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">0.1739</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.1" style="padding-left:4.5pt;padding-right:4.5pt;">nomic-embed-text-v1.5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.3.2.2.1">0.1146</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.3.2.3.1">0.2069</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.3.2.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.3.2.4.1">0.1896</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.1" style="padding-left:4.5pt;padding-right:4.5pt;">bge-m3</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">0.1134</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.3" style="padding-left:4.5pt;padding-right:4.5pt;">0.1953</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.4.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">0.1838</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.1.5.4.1.1">Llama-movielens-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.2" style="padding-left:4.5pt;padding-right:4.5pt;">0.1782</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.3" style="padding-left:4.5pt;padding-right:4.5pt;">0.2837</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.5.4.4" style="padding-left:4.5pt;padding-right:4.5pt;">0.2719</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td" id="S4.T2.1.6.5.1" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.1.6.5.2.1">Llama-amazbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.3.1">0.2649</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.4.1">0.3957</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.6.5.5" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.5.1">0.3787</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.7.6.1" rowspan="4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text" id="S4.T2.1.7.6.1.1">ML20M</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.7.6.2" style="padding-left:4.5pt;padding-right:4.5pt;">all-mpnet-base-v2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.7.6.3" style="padding-left:4.5pt;padding-right:4.5pt;">0.0788</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.7.6.4" style="padding-left:4.5pt;padding-right:4.5pt;">0.1550</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.7.6.5" style="padding-left:4.5pt;padding-right:4.5pt;">0.1042</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.1" style="padding-left:4.5pt;padding-right:4.5pt;">nomic-embed-text-v1.5</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.2" style="padding-left:4.5pt;padding-right:4.5pt;">0.1113</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.7.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.8.7.3.1">0.2143</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.8.7.4" style="padding-left:4.5pt;padding-right:4.5pt;">0.1511</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.1" style="padding-left:4.5pt;padding-right:4.5pt;">bge-m3</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.9.8.2.1">0.1409</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.3" style="padding-left:4.5pt;padding-right:4.5pt;">0.2125</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.9.8.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.9.8.4.1">0.1578</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.10.9.1" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.1.10.9.1.1">Llama-goodbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.10.9.2" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.2.1">0.1589</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.10.9.3" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.3.1">0.2647</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.1.10.9.4" style="padding-left:4.5pt;padding-right:4.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.4.1">0.2066</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSSx1.p2">
<p class="ltx_p" id="S4.SS1.SSSx1.p2.1">For the cold-start scenario, we observe that beeFormer-trained models outperform all baselines when the Heater model approach is used for mapping semantic embeddings to interactions. The model demonstrate interesting behavior when trained on multiple datasets: <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSSx1.p2.1.1">goodlens</em> model outperforms the models trained solely on the evaluated dataset both for ML20M and GB10k. This indicates the possibility to train one (universal) recommender model on multiple datasets from multiple domains. Detailed results are in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T3" title="Table 3 ‣ Time-split setup ‣ 4.1. Results ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx1.p3">
<p class="ltx_p" id="S4.SS1.SSSx1.p3.1">Comparing results from Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T2" title="Table 2 ‣ Item-split setup ‣ 4.1. Results ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T3" title="Table 3 ‣ Time-split setup ‣ 4.1. Results ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, models trained on different datasets within the same domain yield similar performance to models trained on the evaluated dataset. This demonstrates the critical capability of beeFormer to transfer knowledge from one dataset to another.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Time-split setup</h4>
<div class="ltx_para" id="S4.SS1.SSSx2.p1">
<p class="ltx_p" id="S4.SS1.SSSx2.p1.1">We utilize the time-split setup to compare the beeFormer-trained models with the CF models. The beeFormer models outperform all CF baselines for both training on the evaluated dataset and in the zero-shot scenario within the same domain. The model trained on multiple datasets performs slightly worse in cold-start scenario than pure in-domain knowledge transfer, but the results are still comparable. Detailed results are in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10309v2#S4.T4" title="Table 4 ‣ Time-split setup ‣ 4.1. Results ‣ 4. Experimental Evaluation ‣ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Results for cold-start scenario in item-split setup. Names of our models trained with beeFormer are in italics, the best-performing models are represented in bold, and the best baseline for each scenario is underlined. The standard error of all values is below 0.0001 (evaluated via bootstrap resampling).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.1.1.1.1.1">Dataset</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.1.1.1.1.2.1">Method</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">Sentence Transformer</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">R@20</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">R@50</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5">N@100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.2.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.2.1.1.1.1">
<span class="ltx_tr" id="S4.T3.1.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.2.1.1.1.1.1.1">GB10K</span></span>
<span class="ltx_tr" id="S4.T3.1.2.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.2.1.1.1.1.2.1">CBF</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.2"><span class="ltx_text ltx_font_italic" id="S4.T3.1.2.1.2.1">Llama-goodbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.3">0.2505</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.4">0.3839</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.2.1.5">0.3747</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.3.2.1.1">Llama-goodlens-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.2.1">0.2710</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.3.1">0.4218</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.4.1">0.4066</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T3.1.4.3.1" rowspan="4"><span class="ltx_text" id="S4.T3.1.4.3.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.4.3.1.1.1">
<span class="ltx_tr" id="S4.T3.1.4.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.4.3.1.1.1.1.1">GB10K</span></span>
<span class="ltx_tr" id="S4.T3.1.4.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.4.3.1.1.1.2.1">Heater</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.4.3.2">all-mpnet-base-v2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.4.3.3">0.2078</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.4.3.4">0.3221</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.4.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.4.3.5.1">0.3195</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T3.1.5.4.1">nomic-embed-text-v1.5</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.5.4.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.5.4.2.1">0.2154</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.5.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.5.4.3.1">0.3317</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.5.4.4">0.3193</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T3.1.6.5.1">bge-m3</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.6.5.2">0.2052</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.6.5.3">0.3113</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.6.5.4">0.3099</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<td class="ltx_td ltx_align_left" id="S4.T3.1.7.6.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.7.6.1.1">Llama-movielens-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.7.6.2">0.2060</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.7.6.3">0.3161</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.7.6.4">0.3196</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.8.7.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.8.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.8.7.1.1.1">
<span class="ltx_tr" id="S4.T3.1.8.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.8.7.1.1.1.1.1">ML20M</span></span>
<span class="ltx_tr" id="S4.T3.1.8.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.8.7.1.1.1.2.1">CBF</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.8.7.2"><span class="ltx_text ltx_font_italic" id="S4.T3.1.8.7.2.1">Llama-movielens-mpnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.8.7.3">0.4291</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.8.7.4">0.6108</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.8.7.5">0.4054</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T3.1.9.8.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.9.8.1.1">Llama-goodlens-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.8.2.1">0.4630</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.8.3.1">0.6152</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.9.8.4.1">0.4066</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.10.9.1" rowspan="4"><span class="ltx_text" id="S4.T3.1.10.9.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.1.10.9.1.1.1">
<span class="ltx_tr" id="S4.T3.1.10.9.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.10.9.1.1.1.1.1">ML20M</span></span>
<span class="ltx_tr" id="S4.T3.1.10.9.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.10.9.1.1.1.2.1">Heater</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.10.9.2">all-mpnet-base-v2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.10.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.10.9.3.1">0.3114</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.10.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.10.9.4.1">0.4331</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.10.9.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.10.9.5.1">0.3407</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11.10">
<td class="ltx_td ltx_align_left" id="S4.T3.1.11.10.1">nomic-embed-text-v1.5</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.11.10.2">0.3049</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.11.10.3">0.4285</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.11.10.4">0.3270</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.12.11">
<td class="ltx_td ltx_align_left" id="S4.T3.1.12.11.1">bge-m3</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.12.11.2">0.2847</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.12.11.3">0.3932</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.12.11.4">0.3161</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.13.12.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.13.12.1.1">Llama-goodbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.13.12.2">0.3204</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.13.12.3">0.4669</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.1.13.12.4">0.3381</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Results for time-split setup on the Amazon Books dataset. Names of our models trained with beeFormer are in italics, the best-performing models are represented in bold, and the best baseline for each scenario is underlined. The standard error of all values is below 0.00005 (evaluated via bootstrap resampling).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1" style="padding-left:3.9pt;padding-right:3.9pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.1.1.1.1.1" style="padding-left:3.9pt;padding-right:3.9pt;">Scenario</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.1.1.1.2.1" style="padding-left:3.9pt;padding-right:3.9pt;">Method</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2" style="padding-left:3.9pt;padding-right:3.9pt;">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3" style="padding-left:3.9pt;padding-right:3.9pt;">R@20</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.4" style="padding-left:3.9pt;padding-right:3.9pt;">R@50</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.5" style="padding-left:3.9pt;padding-right:3.9pt;">N@100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1" rowspan="5" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text" id="S4.T4.1.2.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.2.1.1.1.1">
<span class="ltx_tr" id="S4.T4.1.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.2.1.1.1.1.1.1" style="padding-left:3.9pt;padding-right:3.9pt;">zero-shot</span></span>
<span class="ltx_tr" id="S4.T4.1.2.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.2.1.1.1.1.2.1" style="padding-left:3.9pt;padding-right:3.9pt;">CBF</span></span>
</span></span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.1.2" style="padding-left:3.9pt;padding-right:3.9pt;">all-mpnet-base-v2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.1.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0218</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.1.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0336</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.1.2.1.5" style="padding-left:3.9pt;padding-right:3.9pt;">0.0193</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.2.1" style="padding-left:3.9pt;padding-right:3.9pt;">nomic-embed-text-v1.5</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.2.2" style="padding-left:3.9pt;padding-right:3.9pt;">0.0387</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.2.3" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.3.2.3.1">0.0560</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.3.2.4" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.3.2.4.1">0.0320</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.3.1" style="padding-left:3.9pt;padding-right:3.9pt;">bge-m3</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.3.2" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.4.3.2.1">0.0398</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.3.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0546</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.4.3.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0313</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.1" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.5.4.1.1">Llama-goodbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.5.4.2.1">0.0649</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.3" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.5.4.3.1">0.0931</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.5.4.4" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.5.4.4.1">0.0515</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.5.1" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.6.5.1.1">Llama-goodlens-mpnet</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.5.2" style="padding-left:3.9pt;padding-right:3.9pt;">0.0617</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.5.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0891</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.6.5.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0492</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.7.6.1" rowspan="3" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text" id="S4.T4.1.7.6.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.7.6.1.1.1">
<span class="ltx_tr" id="S4.T4.1.7.6.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.7.6.1.1.1.1.1" style="padding-left:3.9pt;padding-right:3.9pt;">supervised</span></span>
<span class="ltx_tr" id="S4.T4.1.7.6.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.7.6.1.1.1.2.1" style="padding-left:3.9pt;padding-right:3.9pt;">CF</span></span>
</span></span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.7.6.2" style="padding-left:3.9pt;padding-right:3.9pt;">KNN</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.7.6.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0370</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.7.6.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0562</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.1.7.6.5" style="padding-left:3.9pt;padding-right:3.9pt;">0.0303</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8.7">
<td class="ltx_td ltx_align_left" id="S4.T4.1.8.7.1" style="padding-left:3.9pt;padding-right:3.9pt;">ALS MF</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.8.7.2" style="padding-left:3.9pt;padding-right:3.9pt;">0.0344</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.8.7.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0580</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.8.7.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0313</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T4.1.9.8.1" style="padding-left:3.9pt;padding-right:3.9pt;">ELSA</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.9.8.2" style="padding-left:3.9pt;padding-right:3.9pt;">0.0367</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.9.8.3" style="padding-left:3.9pt;padding-right:3.9pt;">0.0628</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.9.8.4" style="padding-left:3.9pt;padding-right:3.9pt;">0.0346</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.10.9">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.1.10.9.1" style="padding-left:3.9pt;padding-right:3.9pt;"></th>
<td class="ltx_td ltx_align_left" id="S4.T4.1.10.9.2" style="padding-left:3.9pt;padding-right:3.9pt;">SANSA</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.10.9.3" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.10.9.3.1">0.0421</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.10.9.4" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.10.9.4.1">0.0678</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.10.9.5" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.1.10.9.5.1">0.0362</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.1.11.10.1" style="padding-left:3.9pt;padding-right:3.9pt;">CBF</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.11.10.2" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_italic" id="S4.T4.1.11.10.2.1">Llama-amazbooks-mpnet</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.11.10.3" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.11.10.3.1">0.0706</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.11.10.4" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.11.10.4.1">0.1045</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T4.1.11.10.5" style="padding-left:3.9pt;padding-right:3.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.11.10.5.1">0.0571</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce beeFormer, a novel training procedure that enhances neural representations of items by training sentence encoders on interactions. Our approach is scalable, utilizing ELSA linear autoencoder as the decoder during the training process, enabling it to handle datasets with a large number of items. BeeFormer-trained models are easily deployable into existing production systems since they are compatible with the widely adopted sentence Transformers library and can produce recommendations via embedding tables with cosine similarity criterion.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We observe performance improvements over various state-of-the-art baselines in all evaluated scenarios. Notably, in the time-split setup on the Amazon Books dataset, our models achieves significantly better results over CF methods in both supervised and zero-shot scenarios, demonstrating both superior performance and the ability to transfer knowledge from one dataset to another.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We also demonstrate that training the <em class="ltx_emph ltx_font_italic" id="S5.p3.1.1">Llama-goodlens-mpnet</em> model on two datasets (GB10K and ML20M) from different domains further increases performance when evaluating on individual datasets. This ability to accumulate knowledge from multiple datasets marks an important step towards training universal, domain-agnostic, content-based models for RS.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">In our future work, we plan to build one (big) dataset from several RS domains and train a universal sentence Transformer model on it. We want to also explore the possibility of using beeFormer with computer vision models. Building multi-modal encoders with beeFormer could be especially useful in domains such as fashion recommendation.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We want to thank anonymous reviewers for their suggestions, many of which helped us improve this paper.
Our research has been supported by the Grant Agency of Czech Technical University (SGS23/210/OHK3/3T/18) and by the Grant Agency of the Czech Republic under the EXPRO program as project “LUSyD” (project No. GX20-16819X).

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Arkadeep Acharya, Brijraj
Singh, and Naoyuki Onoe.
2023.

</span>
<span class="ltx_bibblock">LLM Based Generation of Item-Description for
Recommendation System. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 17th
ACM Conference on Recommender Systems</em> (Singapore, Singapore)
<em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(RecSys ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
1204–1207.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3610647" title="">https://doi.org/10.1145/3604915.3610647</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta. 2024.

</span>
<span class="ltx_bibblock">The Llama 3 Herd of Models.

</span>
<span class="ltx_bibblock">(2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models" title="">https://ai.meta.com/research/publications/the-llama-3-herd-of-models</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwaar et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Fahad Anwaar, Naima
Iltaf, Hammad Afzal, and Raheel
Nawaz. 2018.

</span>
<span class="ltx_bibblock">HRS-CE: A hybrid framework to integrate content
embeddings in recommender systems for cold start items.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Journal of computational science</em>
29 (2018), 9–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baeza-Yates et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Ricardo Baeza-Yates, Di
Jiang, Fabrizio Silvestri, and Beverly
Harrison. 2015.

</span>
<span class="ltx_bibblock">Predicting the next app that you are going to use.
In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the eighth ACM international
conference on web search and data mining</em>. 285–294.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Botha et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jan A. Botha, Zifei Shan,
and Daniel Gillick. 2020.

</span>
<span class="ltx_bibblock">Entity Linking in 100 Languages. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>,
Bonnie Webber, Trevor
Cohn, Yulan He, and Yang Liu (Eds.).
Association for Computational Linguistics,
Online, 7833–7845.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.emnlp-main.630" title="">https://doi.org/10.18653/v1/2020.emnlp-main.630</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao,
Peitian Zhang, Kun Luo,
Defu Lian, and Zheng Liu.
2024.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual,
multi-functionality, multi-granularity text embeddings through self-knowledge
distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">arXiv:2402.03216</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu,
Chiyuan Zhang, and Carlos Guestrin.
2016.

</span>
<span class="ltx_bibblock">Training deep nets with sublinear memory cost.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">arXiv preprint arXiv:1604.06174</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Heng-Tze Cheng, Levent
Koc, Jeremiah Harmsen, Tal Shaked,
Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado,
Wei Chai, Mustafa Ispir, et al<span class="ltx_text" id="bib.bib9.3.1">.</span>
2016.

</span>
<span class="ltx_bibblock">Wide &amp; deep learning for recommender systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">Proceedings of the 1st workshop on deep learning
for recommender systems</em>. 7–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu,
Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact
attention with io-awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Advances in Neural Information Processing
Systems</em> 35 (2022),
16344–16359.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Souza Pereira Moreira et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Gabriel de Souza Pereira Moreira,
Sara Rabhi, Jeong Min Lee,
Ronay Ak, and Even Oldridge.
2021.

</span>
<span class="ltx_bibblock">Transformers4Rec: Bridging the Gap between NLP and
Sequential / Session-Based Recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 15th ACM Conference on
Recommender Systems</em> (Amsterdam, Netherlands) <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(RecSys
’21)</em>. Association for Computing Machinery,
New York, NY, USA, 143–153.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3460231.3474255" title="">https://doi.org/10.1145/3460231.3474255</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamani (2021)</span>
<span class="ltx_bibblock">
Manav Dhamani.
2021.

</span>
<span class="ltx_bibblock">Goodreads 100K books.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/mdhamani/goodreads-books-100k" title="">https://www.kaggle.com/datasets/mdhamani/goodreads-books-100k</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Palma et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dario Di Palma,
Giovanni Maria Biancofiore, Vito Walter
Anelli, Fedelucio Narducci, Tommaso
Di Noia, and Eugenio Di Sciascio.
2023.

</span>
<span class="ltx_bibblock">Evaluating chatgpt as a recommender system: A
rigorous approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv:2309.03613</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DING et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
HAO DING, Anoop Deoras,
Bernie Wang, and Hao Wang.
2022.

</span>
<span class="ltx_bibblock">Zero-Shot Recommender Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ICLR Workshop on Deep Generative Models for Highly
Structured Data</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jingtao Ding, Yuhan Quan,
Quanming Yao, Yong Li, and
Depeng Jin. 2020.

</span>
<span class="ltx_bibblock">Simplify and robustify negative sampling for
implicit collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Advances in Neural Information Processing
Systems</em> 33 (2020),
1094–1105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaoyu Du, Xiang Wang,
Xiangnan He, Zechao Li,
Jinhui Tang, and Tat-Seng Chua.
2020.

</span>
<span class="ltx_bibblock">How to learn item representation for cold-start
multimedia recommendation?. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the
28th ACM International Conference on Multimedia</em>.
3469–3477.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang
Liu, Zuohui Fu, Yingqiang Ge, and
Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as Language Processing (RLP): A
Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5). In
<em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 16th ACM Conference on
Recommender Systems</em> (¡conf-loc¿, ¡city¿Seattle¡/city¿, ¡state¿WA¡/state¿,
¡country¿USA¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(RecSys ’22)</em>.
Association for Computing Machinery,
New York, NY, USA, 299–315.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3523227.3546767" title="">https://doi.org/10.1145/3523227.3546767</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gillick et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Daniel Gillick, Sayali
Kulkarni, Larry Lansing, Alessandro
Presta, Jason Baldridge, Eugene Ie,
and Diego Garcia-Olano. 2019.

</span>
<span class="ltx_bibblock">Learning Dense Representations for Entity
Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 23rd Conference
on Computational Natural Language Learning (CoNLL)</em>,
Mohit Bansal and Aline
Villavicencio (Eds.). Association for Computational
Linguistics, Hong Kong, China, 528–537.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/K19-1049" title="">https://doi.org/10.18653/v1/K19-1049</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldberg et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (1992)</span>
<span class="ltx_bibblock">
David Goldberg, David
Nichols, Brian M. Oki, and Douglas
Terry. 1992.

</span>
<span class="ltx_bibblock">Using Collaborative Filtering to Weave an
Information Tapestry.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Commun. ACM</em> (1992),
61–70.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/138859.138867" title="">https://doi.org/10.1145/138859.138867</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Griewank and Walther (2000)</span>
<span class="ltx_bibblock">
Andreas Griewank and
Andrea Walther. 2000.

</span>
<span class="ltx_bibblock">Algorithm 799: revolve: an implementation of
checkpointing for the reverse or adjoint mode of computational
differentiation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM Trans. Math. Softw.</em>
26, 1 (mar
2000), 19–45.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/347837.347846" title="">https://doi.org/10.1145/347837.347846</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)</span>
<span class="ltx_bibblock">
F. Maxwell Harper and
Joseph A. Konstan. 2015.

</span>
<span class="ltx_bibblock">The MovieLens Datasets: History and Context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ACM Trans. Interact. Intell. Syst.</em>
5, 4, Article 19
(Dec. 2015), 19 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2827872" title="">https://doi.org/10.1145/2827872</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yupeng Hou, Shanlei Mu,
Wayne Xin Zhao, Yaliang Li,
Bolin Ding, and Ji-Rong Wen.
2022.

</span>
<span class="ltx_bibblock">Towards Universal Sequence Representation Learning
for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the
28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>
(Washington DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.2">(KDD ’22)</em>.
Association for Computing Machinery,
New York, NY, USA, 585–593.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534678.3539381" title="">https://doi.org/10.1145/3534678.3539381</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juarto and Girsang (2021)</span>
<span class="ltx_bibblock">
Budi Juarto and
Abba Suganda Girsang. 2021.

</span>
<span class="ltx_bibblock">Neural collaborative with sentence BERT for news
recommender system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">JOIV: International Journal on Informatics
Visualization</em> 5, 4
(2021), 448–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konstan et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (1997)</span>
<span class="ltx_bibblock">
Joseph A. Konstan,
Bradley N. Miller, David Maltz,
Jonathan L. Herlocker, Lee R. Gordon,
and John Riedl. 1997.

</span>
<span class="ltx_bibblock">GroupLens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Commun. ACM</em> 40,
3 (1997), 77–87.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/245108.245126" title="">https://doi.org/10.1145/245108.245126</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Yehuda Koren, Robert
Bell, and Chris Volinsky.
2009.

</span>
<span class="ltx_bibblock">Matrix factorization techniques for recommender
systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Computer</em> (2009).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/MC.2009.263" title="">https://doi.org/10.1109/MC.2009.263</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiacheng Li, Ming Wang,
Jin Li, Jinmiao Fu, Xin
Shen, Jingbo Shang, and Julian
McAuley. 2023b.

</span>
<span class="ltx_bibblock">Text Is All You Need: Learning Language
Representations for Sequential Recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining</em> (¡conf-loc¿, ¡city¿Long Beach¡/city¿,
¡state¿CA¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿)
<em class="ltx_emph ltx_font_italic" id="bib.bib26.4.2">(KDD ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
1258–1267.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3580305.3599519" title="">https://doi.org/10.1145/3580305.3599519</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ruyu Li, Wenhao Deng,
Yu Cheng, Zheng Yuan,
Jiaqi Zhang, and Fajie Yuan.
2023a.

</span>
<span class="ltx_bibblock">Exploring the Upper Limits of Text-Based
Collaborative Filtering Using Large Language Models: Discoveries and
Insights.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.11700 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Dawen Liang, Rahul G.
Krishnan, Matthew D. Hoffman, and Tony
Jebara. 2018.

</span>
<span class="ltx_bibblock">Variational autoencoders for collaborative
filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">The Web Conference 2018 -
Proceedings of the World Wide Web Conference, WWW 2018</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3178876.3186150" title="">https://doi.org/10.1145/3178876.3186150</a>
arXiv:1802.05814

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manzoor et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ahtsham Manzoor, Samuel C.
Ziegler, Klaus Maria. Pirker Garcia, and
Dietmar Jannach. 2024.

</span>
<span class="ltx_bibblock">ChatGPT as a Conversational Recommender System: A
User-Centric Analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 32nd
ACM Conference on User Modeling, Adaptation and Personalization</em> (Cagliari,
Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.2">(UMAP ’24)</em>. Association
for Computing Machinery, New York, NY, USA,
267–272.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3627043.3659574" title="">https://doi.org/10.1145/3627043.3659574</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mhatre (2020)</span>
<span class="ltx_bibblock">
Samruddhi Mhatre.
2020.

</span>
<span class="ltx_bibblock">IMDB movies analysis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/samruddhim/imdb-movies-analysis" title="">https://www.kaggle.com/datasets/samruddhim/imdb-movies-analysis</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michiels et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Lien Michiels, Robin
Verachtert, and Bart Goethals.
2022.

</span>
<span class="ltx_bibblock">RecPack: An(Other) Experimentation Toolkit for
Top-N Recommendation Using Implicit Feedback Data. In
<em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 16th ACM Conference on
Recommender Systems</em> (Seattle, WA, USA). Association for
Computing Machinery, New York, NY, USA,
648–651.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3523227.3551472" title="">https://doi.org/10.1145/3523227.3551472</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jianmo Ni, Jiacheng Li,
and Julian McAuley. 2019.

</span>
<span class="ltx_bibblock">Justifying recommendations using distantly-labeled
reviews and fine-grained aspects. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings
of the 2019 conference on empirical methods in natural language processing
and the 9th international joint conference on natural language processing
(EMNLP-IJCNLP)</em>. 188–197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nussbaum et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zach Nussbaum, John X
Morris, Brandon Duderstadt, and Andriy
Mulyar. 2024.

</span>
<span class="ltx_bibblock">Nomic Embed: Training a Reproducible Long Context
Text Embedder.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2402.01613</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reese (2020)</span>
<span class="ltx_bibblock">
Austin Reese.
2020.

</span>
<span class="ltx_bibblock">Goodreads books - 31 features.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/austinreese/goodreads-books" title="">https://www.kaggle.com/datasets/austinreese/goodreads-books</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna
Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese
BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing</em>.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1908.10084" title="">http://arxiv.org/abs/1908.10084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarwar et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Badrul Sarwar, George
Karypis, Joseph Konstan, and John
Riedl. 2001.

</span>
<span class="ltx_bibblock">Item-based collaborative filtering recommendation
algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 10th
international conference on World Wide Web</em>. 285–295.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sethi and Mehrotra (2021)</span>
<span class="ltx_bibblock">
Rachna Sethi and Monica
Mehrotra. 2021.

</span>
<span class="ltx_bibblock">Cold start in recommender systems—A survey from
domain perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Intelligent Data
Communication Technologies and Internet of Things: Proceedings of ICICI
2020</em>. Springer, 223–232.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shalaby et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Walid Shalaby, Sejoon Oh,
Amir Afsharinejad, Srijan Kumar, and
Xiquan Cui. 2022.

</span>
<span class="ltx_bibblock">M2TRec: Metadata-aware Multi-task Transformer for
Large-scale and Cold-start free Session-based Recommendations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 16th ACM Conference on
Recommender Systems</em>. 573–578.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ítallo Silva, Leandro
Marinho, Alan Said, and Martijn C.
Willemsen. 2024.

</span>
<span class="ltx_bibblock">Leveraging ChatGPT for Automated Human-centered
Explanations in Recommender Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 29th International Conference on
Intelligent User Interfaces</em> <em class="ltx_emph ltx_font_italic" id="bib.bib39.4.2">(IUI ’24)</em>.
Association for Computing Machinery,
New York, NY, USA, 597–608.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3640543.3645171" title="">https://doi.org/10.1145/3640543.3645171</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kaitao Song, Xu Tan,
Tao Qin, Jianfeng Lu, and
Tie-Yan Liu. 2020.

</span>
<span class="ltx_bibblock">Mpnet: Masked and permuted pre-training for
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Advances in neural information processing
systems</em> 33 (2020),
16857–16867.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spillo et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Giuseppe Spillo, Cataldo
Musto, Marco Polignano, Pasquale Lops,
Marco de Gemmis, and Giovanni
Semeraro. 2023.

</span>
<span class="ltx_bibblock">Combining Graph Neural Networks and Sentence
Encoders for Knowledge-aware Recommendations. In
<em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 31st ACM Conference on User
Modeling, Adaptation and Personalization</em> (Limassol, Cyprus)
<em class="ltx_emph ltx_font_italic" id="bib.bib41.4.2">(UMAP ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
1–12.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3565472.3592965" title="">https://doi.org/10.1145/3565472.3592965</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spišák et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Martin Spišák,
Radek Bartyzal, Antonín Hoskovec,
Ladislav Peska, and Miroslav Tůma.
2023.

</span>
<span class="ltx_bibblock">Scalable Approximate NonSymmetric Autoencoder for
Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the
17th ACM Conference on Recommender Systems</em> (Singapore, Singapore)
<em class="ltx_emph ltx_font_italic" id="bib.bib42.4.2">(RecSys ’23)</em>. Association for
Computing Machinery, New York, NY, USA,
763–770.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608827" title="">https://doi.org/10.1145/3604915.3608827</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steck (2019)</span>
<span class="ltx_bibblock">
Harald Steck.
2019.

</span>
<span class="ltx_bibblock">Embarrassingly shallow autoencoders for sparse
data. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">The Web Conference 2019 - Proceedings of
the World Wide Web Conference, WWW 2019</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3308558.3313710" title="">https://doi.org/10.1145/3308558.3313710</a>
arXiv:1905.03375

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu,
Jian Wu, Changhua Pei,
Xiao Lin, Wenwu Ou, and
Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential Recommendation with
Bidirectional Encoder Representations from Transformer. In
<em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the 28th ACM International
Conference on Information and Knowledge Management</em> (Beijing, China)
<em class="ltx_emph ltx_font_italic" id="bib.bib44.4.2">(CIKM ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
1441–1450.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3357384.3357895" title="">https://doi.org/10.1145/3357384.3357895</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ruixuan Sun, Xinyi Li,
Avinash Akella, and Joseph A. Konstan.
2024.

</span>
<span class="ltx_bibblock">Large Language Models as Conversational Movie
Recommenders: A User Study.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.19093 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takács et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Gábor Takács,
István Pilászy, and Domonkos
Tikk. 2011.

</span>
<span class="ltx_bibblock">Applications of the Conjugate Gradient Method for
Implicit Feedback Collaborative Filtering. In
<em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of the Fifth ACM Conference on
Recommender Systems</em> (Chicago, Illinois, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib46.4.2">(RecSys
’11)</em>. Association for Computing Machinery,
New York, NY, USA, 297–300.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2043932.2043987" title="">https://doi.org/10.1145/2043932.2043987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takács and Tikk (2012)</span>
<span class="ltx_bibblock">
Gábor Takács and
Domonkos Tikk. 2012.

</span>
<span class="ltx_bibblock">Alternating Least Squares for Personalized
Ranking. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the Sixth ACM
Conference on Recommender Systems</em> (Dublin, Ireland)
<em class="ltx_emph ltx_font_italic" id="bib.bib47.2.2">(RecSys ’12)</em>. Association for
Computing Machinery, New York, NY, USA,
83–90.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2365952.2365972" title="">https://doi.org/10.1145/2365952.2365972</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vančura et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Vojtěch Vančura,
Rodrigo Alves, Petr Kasalický, and
Pavel Kordík. 2022.

</span>
<span class="ltx_bibblock">Scalable Linear Shallow Autoencoder for
Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the
16th ACM Conference on Recommender Systems</em> (Seattle, WA, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib48.4.2">(RecSys ’22)</em>. Association for
Computing Machinery, New York, NY, USA,
604–609.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3523227.3551482" title="">https://doi.org/10.1145/3523227.3551482</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia
Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Advances in neural information processing
systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zajac (2017)</span>
<span class="ltx_bibblock">
Zygmunt Zajac.
2017.

</span>
<span class="ltx_bibblock">Goodbooks-10k: a new dataset for book
recommendations.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://fastml.com/goodbooks-10k" title="">http://fastml.com/goodbooks-10k</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">FastML</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang (2023)</span>
<span class="ltx_bibblock">
Gangyi Zhang.
2023.

</span>
<span class="ltx_bibblock">User-Centric Conversational Recommendation:
Adapting the Need of User with Large Language Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 17th ACM Conference on
Recommender Systems</em> (Singapore, Singapore) <em class="ltx_emph ltx_font_italic" id="bib.bib51.2.2">(RecSys
’23)</em>. Association for Computing Machinery,
New York, NY, USA, 1349–1354.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604915.3608885" title="">https://doi.org/10.1145/3604915.3608885</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ziwei Zhu, Shahin Sefati,
Parsa Saadatpanah, and James Caverlee.
2020.

</span>
<span class="ltx_bibblock">Recommendation for new users and new items via
randomized training and mixture-of-experts transformation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval</em>.
1121–1130.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 19:29:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
