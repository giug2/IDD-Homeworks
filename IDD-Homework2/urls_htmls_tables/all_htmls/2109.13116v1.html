<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.13116] VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering</title><meta property="og:description" content="We present VQA-MHUG â€“ a novel 49-participant dataset of multimodal human gaze on both images and questions during visual question answering (VQA) collected using a high-speed eye tracker.
We use our dataset to analyze â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.13116">

<!--Generated on Sat Mar  2 03:30:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ekta Sood<sup id="id8.8.id1" class="ltx_sup">1</sup>, Fabian KÃ¶gel<sup id="id9.9.id2" class="ltx_sup">1</sup>, Florian Strohm<sup id="id10.10.id3" class="ltx_sup">1</sup>, Prajit Dhar<sup id="id11.11.id4" class="ltx_sup">2</sup>, Andreas Bulling<sup id="id12.12.id5" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id13.13.id6" class="ltx_sup">1</sup>University of Stuttgart, Institute for Visualization and Interactive Systems (VIS), Germany
<br class="ltx_break"><sup id="id14.14.id7" class="ltx_sup">2</sup>University of Groningen, Center for Language and Cognition (CLCG), the Netherlands
<br class="ltx_break"><span id="id15.15.id8" class="ltx_text ltx_font_typewriter">{ekta.sood,fabian.koegel,florian.strohm,andreas.bulling}@vis.uni-stuttgart.de
<br class="ltx_break">p.dhar@rug.nl</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">We present VQA-MHUG â€“ a novel 49-participant dataset of <span id="id16.id1.1" class="ltx_text ltx_framed ltx_framed_underline">m</span>ultimodal <span id="id16.id1.2" class="ltx_text ltx_framed ltx_framed_underline">hu</span>man <span id="id16.id1.3" class="ltx_text ltx_framed ltx_framed_underline">g</span>aze on both images and questions during visual question answering (VQA) collected using a high-speed eye tracker.
We use our dataset to analyze the similarity between human and neural attentive strategies learned by five state-of-the-art VQA models: Modular Co-Attention Network (MCAN) with either grid or region features, Pythia, Bilinear Attention Network (BAN), and the Multimodal Factorized Bilinear Pooling Network (MFB).
While prior work has focused on studying the image modality, our analyses show â€“ for the first time â€“ that for all models, higher correlation with human attention on text is a significant predictor of
VQA performance.
This finding points at a potential for improving VQA performance and, at the same time, calls for further research on neural text attention mechanisms and their integration into architectures for vision and language tasks, including but potentially also beyond VQA.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) has gained popularity as a practically-useful and challenging task at the intersection of natural language processing (NLP) and computer vision (CV)Â <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite>. The key challenge in VQA is to develop computational models that are able to reason over questions and images in order to generate answers that are well-grounded in both modalitiesÂ <cite class="ltx_cite ltx_citemacro_cite">P. Zhang etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2015</a>); Agrawal etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>); Goyal etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2017a</a>); Kafle etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>.
Attention mechanisms originally introduced in NLP for monomodal language tasks have been successfully applied to multimodal tasks (like VQA) and established a new state of the artÂ <cite class="ltx_cite ltx_citemacro_cite">Correia and Colombini (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>); Kim etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>); Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">These advances have, in turn, triggered research into understanding the reasons for these improvements. A body of work has studied similarities between neural and human attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Qiuxia etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2020</a>); Yun etÂ al. (<a href="#bib.bib63" title="" class="ltx_ref">2013</a>); Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>.
Models seem to learn very different attention strategies and similarity to human attention might only improve performance for specific model typesÂ <cite class="ltx_cite ltx_citemacro_cite">Sood etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>.
However, although VQA is an inherently multimodal task, all of these analyses have only focused on image attention.
The most likely reason for this is that existing datasets only offer mono-modal attention on the imageÂ <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>); Fosco etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>); Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.
In addition, due to the challenges involved in recording human gaze data at scale, prior works have instead used mouse data as a proxy to attention <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>.
However, mouse data was shown to over-estimate some image areasÂ <cite class="ltx_cite ltx_citemacro_cite">Tavakoli etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2017b</a>); Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> or to miss relevant background information altogetherÂ <cite class="ltx_cite ltx_citemacro_cite">Sugano and Bulling (<a href="#bib.bib48" title="" class="ltx_ref">2016</a>); Tavakoli etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2017a</a>)</cite>.
As of now, there is no publicly available dataset that offers human gaze data on both the images and questions.
This severely impedes further progress in this emerging area of research.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our work fills this gap by introducing VQA-MHUG â€“ the first dataset of <span id="S1.p3.1.1" class="ltx_text ltx_framed ltx_framed_underline">m</span>ultimodal <span id="S1.p3.1.2" class="ltx_text ltx_framed ltx_framed_underline">hu</span>man <span id="S1.p3.1.3" class="ltx_text ltx_framed ltx_framed_underline">g</span>aze on both images and questions in VQA.
To collect our dataset, we conducted a 49-participant eye tracking study.
We used a commercial, high-speed eye tracker to record gaze data on images and corresponding questions of the VQAv2 validation set.
VQA-MHUG contains 11,970 gaze samples for 3,990 question-image pairs, tagged and balanced by reasoning type and difficulty.
We ensured a large overlap in question-image pairs with nine other VQA datasets to maximize the usefulness of VQA-MHUG for future multimodal studies on human and neural attention mechanisms.
Using our dataset, we conduct detailed analyses of the similarity between human and neural attentive strategies, the latter of which we obtained from five top-performing models in the VQA challenges 2017-2020:
Modulated Co-Attention Network (MCAN) with grid or region features, Pythia, Bilinear Attention Network (BAN), and the Multimodal Factorized Bilinear Pooling Network (MFB).
These analyses show, for the first time, that correlation with human attention on text is a significant predictor of accuracy for all the studied state-of-the-art VQA models.
This suggests a potential for significant performance improvements in VQA by guiding models to "read the questions" more similarly to humans.
In summary, our work contributes:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">VQA-MHUG, a novel 49-participant dataset of multimodal human gaze on both <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">images</span> and <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">questions</span> during visual question answering collected using a high-speed eye tracker.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Detailed analysis of the similarity between human and neural attentive strategies indicating that human-like attention to text could yield significant performance improvements.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work is related to previous work on 1) neural machine attention, 2) attention in VQA, and 3) comparison of neural and human attention.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Neural Machine Attention.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Inspired by the human visual system, neural machine attention
allows neural networks to selectively focus on particular parts of the input, resulting in significant improvements in performance and interpretabilityÂ <cite class="ltx_cite ltx_citemacro_cite">Correia and Colombini (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.
Single-modal attention <cite class="ltx_cite ltx_citemacro_cite">Bahdanau etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2014</a>)</cite> as well as approaches that build on it, such as self attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib54" title="" class="ltx_ref">2015</a>); Vaswani etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> or stacked attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib57" title="" class="ltx_ref">2016a</a>, <a href="#bib.bib58" title="" class="ltx_ref">b</a>); Zhang etÂ al. (<a href="#bib.bib64" title="" class="ltx_ref">2018</a>); Anderson etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, have been shown to be particularly helpful for sequence learning tasks in NLP and CV.
Initially, attention mechanisms were often combined with recurrent and convolutional architectures to encode the input featuresÂ <cite class="ltx_cite ltx_citemacro_cite">Bahdanau etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2014</a>); Yu etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>); Tavakoli etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2017b</a>); Kim etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2016</a>); Lu etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2016</a>); Jabri etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>); Agrawal etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>.
More recently, Transformer-based architectures have been introduced that solely rely on attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2017</a>); Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>); Khan etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>.
Large-scale, pre-trained language models
are a key application of Transformers that enabled their current performance lead in both NLP and multimodal vision-language tasksÂ <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Yang etÂ al. (<a href="#bib.bib56" title="" class="ltx_ref">2019b</a>); Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>); Lu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attention in VQA.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Increased interest into capturing multimodal relationships with attention mechanisms have put focus on VQA as a benchmark taskÂ <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib33" title="" class="ltx_ref">2014</a>); Malinowski etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>); Lu etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2016</a>); Yu etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>); Nguyen and Okatani (<a href="#bib.bib35" title="" class="ltx_ref">2018</a>); Yang etÂ al. (<a href="#bib.bib55" title="" class="ltx_ref">2019a</a>); Li etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>.
In fact, attention mechanisms have been extensively explored in VQA and have repeatedly dominated the important VQAv2 challengeÂ <cite class="ltx_cite ltx_citemacro_citep">(Anderson etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>; Yu etÂ al., <a href="#bib.bib60" title="" class="ltx_ref">2019b</a>; Jiang etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.
Although attention-based models have achieved remarkable success, it often remains unclear how and why different attention mechanisms actually workÂ <cite class="ltx_cite ltx_citemacro_cite">Jain and Wallace (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>); Serrano and Smith (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparing Neural and Human Attention.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Several prior works have proposed datasets of human attention on images to study the differences between neural and human attention in VQAÂ <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>); Fosco etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>); Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.
In particular, free-viewing and task-specific mouse tracking from SALICONÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> and VQA-HATÂ <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>, as well as free-viewing and task-specific gaze data from SBU GazeÂ <cite class="ltx_cite ltx_citemacro_cite">Yun etÂ al. (<a href="#bib.bib62" title="" class="ltx_ref">2015</a>)</cite> and AiR-DÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> have been compared to neural attention.
All of these works were limited to images only and found mouse tracking to overestimate relevant areas and miss scene contextÂ <cite class="ltx_cite ltx_citemacro_cite">Sugano and Bulling (<a href="#bib.bib48" title="" class="ltx_ref">2016</a>); Tavakoli etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2017b</a>, <a href="#bib.bib50" title="" class="ltx_ref">a</a>); He etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>.
Furthermore, while integrating human attention over the image showed performance improvements in VQAÂ <cite class="ltx_cite ltx_citemacro_cite">Park etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>); Qiao etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2018</a>); Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, the influence of integrating human text attention remains unclear.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">There is currently no multimodal dataset including real human gaze on VQA questions and images.
This represents a major limitation for two different aspects of research, i.e. research aiming to better understand and improve neural attention mechanisms and research focusing on integrating human attention to improve VQA performance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The VQA-MHUG Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We present <span id="S3.p1.1.1" class="ltx_text ltx_framed ltx_framed_underline">V</span>isual <span id="S3.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">Q</span>uestion <span id="S3.p1.1.3" class="ltx_text ltx_framed ltx_framed_underline">A</span>nswering with <span id="S3.p1.1.4" class="ltx_text ltx_framed ltx_framed_underline">M</span>ulti-Modal <span id="S3.p1.1.5" class="ltx_text ltx_framed ltx_framed_underline">Hu</span>man <span id="S3.p1.1.6" class="ltx_text ltx_framed ltx_framed_underline">G</span>aze (VQA-MHUG)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset is publicly available at <a target="_blank" href="https://perceptualui.org/publications/sood21_conll/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://perceptualui.org/publications/sood21_conll/</a></span></span></span>.
To the best of our knowledge, this is the first resource containing multimodal human gaze data over a textual question and the corresponding image.
Our corpus encompasses task-specific gaze on a subset of the benchmark dataset VQAv2 val<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://visualqa.org/download.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/download.html</a></span></span></span>Â <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2017b</a>)</cite>.
We specifically focused on question-image pairs that machines struggle with, but humans answer easily (determined by high inter-agreement and confidence in the VQAv2 annotations).
We then balanced the selection by evenly picking questions based on a machine difficulty score and from different reasoning types.
Thus, VQA-MHUG covers a wide range of challenging reasoning capabilities and overlaps with many VQAv2-related datasets (see Table <a href="#A1.T4" title="Table 4 â€£ Appendix A VQA-MHUG Overlap to Related Datasets â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a href="#A1" title="Appendix A VQA-MHUG Overlap to Related Datasets â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reasoning Types.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">VQAv2 groups question-image pairs based on question words: <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">what</span>, <span id="S3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">who</span>, <span id="S3.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">how</span>, <span id="S3.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">when</span> and <span id="S3.SS0.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_italic">where</span>. Instead, we binned our pairs into the reasoning capabilities required to answer them. We incorporated the categories proposed byÂ <cite class="ltx_cite ltx_citemacro_citet">Kafle and Kanan (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> for their task directed image understanding challenge (TDIUC) and extended them with an additional category, <span id="S3.SS0.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_italic">reading</span>, for questions that are answered by reading text on the images. This resulted in 12 reasoning types that align better with commonly-diagnosed error cases<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>See Appendix <a href="#A2" title="Appendix B Reasoning Types â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for details on the reasoning type tagging.</span></span></span>. We binned VQAv2 val pairs accordingly by training a LSTM-based classifier on 1.6â€‰M TDIUC and 145â€‰K VQAv2 train+val samples which we labelled using regular expressions. The classifier predicted the reasoning type for a given question-answer pair. The final model achieved 99.67% accuracy on a 20% held-out test set.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Machine Difficulty Score.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">To assess the difficulty for a machine to answer a question-image pair, we ran two popular VQA models â€“ MFBÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> for multimodal fusion and MCANÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>)</cite> for transformer attention â€“ inspired byÂ <cite class="ltx_cite ltx_citemacro_citet">Sood etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>.
A difficult question results in low answer accuracy, particularly after rephrasing or asking further control questions.
To test this, we evaluated on four datasets and averaged their corresponding normalized metrics: (1) VQAv2 accuracy, (2) VQA-CP accuracy on reduced biasÂ <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2018a</a>)</cite>, (3) VQA-Introspectâ€™s consistency with respect to visual perceptionÂ <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib41" title="" class="ltx_ref">2020a</a>)</cite> and (4) VQA-Rephrasingsâ€™ robustness against linguistic variationsÂ <cite class="ltx_cite ltx_citemacro_cite">Shah etÂ al. (<a href="#bib.bib44" title="" class="ltx_ref">2019a</a>)</cite> (see Appendix <a href="#A3" title="Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Participants and Experimental Setup.</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.2" class="ltx_p">We recruited 49 participants at the local university (18 identified female and 31 male) with normal or corrected-to-normal vision, aged between 19 and 35 years (<math id="S3.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mu=25.8" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">Î¼</mi><mo id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">25.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1"><eq id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1"></eq><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2">ğœ‡</ci><cn type="float" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3">25.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.1.m1.1c">\mu=25.8</annotation></semantics></math>, <math id="S3.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\sigma=2.8" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S3.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">Ïƒ</mi><mo id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">2.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1"><eq id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1"></eq><ci id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2">ğœ</ci><cn type="float" id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.3">2.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.2.m2.1c">\sigma=2.8</annotation></semantics></math>) and compensated them for their participation<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The university ethics committee approved our study.</span></span></span>.
All participants had an English Level of C1 or above (8 were native speakers).<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>After providing their consent, we collected basic demographic information for each participant. The anonymized data is available with the dataset.</span></span></span></p>
</div>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p2.1" class="ltx_p">Questions and images were presented one after each other on a 24.5" monitor with resolution 1920x1080â€‰px.
They were centered on a white background, and scaled/line-wrapped to fit 26.2x11.5Â° of visual angle in the center.
For the questions, we used a monospace font of size 0.6Â° and line spacing such that the bounding boxes around each word covered 1.8Â° vertically.
Binocular gaze data was collected with an EyeLink 1000 Plus remote eye tracker at 2 kHz with an average measured tracking error of 0.62Â° (see Appendix <a href="#A5" title="Appendix E Experimental Setup â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>).</p>
</div>
<div id="S3.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p3.1" class="ltx_p">Participants had unlimited viewing time but were instructed to move on as soon as they understood the question, gave an answer, or decided to skip.
They completed a set of practice recordings to familiarize themselves with the study procedure.
As such, the task was known to the participant, so both the question reading and the subsequent image viewing were conditioned on VQA.
They then completed three blocks of 110 recordings in randomized order with 5 minute breaks in-between.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset Statistics.</h4>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.1" class="ltx_p">VQA-MHUG contains gaze on 3,990 stimuli from VQAv2 val.
For each stimulus, we provide three recordings from different participants over text and image, their corresponding answer, and whether they answered the question correctly (as compared to the VQAv2 annotations).
For 3,177 stimuli (79.6%), the majority of participant answers appear in the VQAv2 annotations.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Human Attention Maps.</h4>

<div id="S3.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px5.p1.1" class="ltx_p">To generate human attention maps, we used the fixation detection algorithm of the EyeLink software with default parameters.
We always picked the eye with the lower validation error to prioritize accuracy <cite class="ltx_cite ltx_citemacro_cite">Hooge etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> and represented fixations by Gaussian kernels with <math id="S3.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\sigma=1\text{Â°}" display="inline"><semantics id="S3.SS0.SSS0.Px5.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">Ïƒ</mi><mo id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml"><mn id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mtext id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3a.cmml">Â°</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1"><eq id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.1"></eq><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.2">ğœ</ci><apply id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3"><times id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.2">1</cn><ci id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3a.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3"><mtext id="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px5.p1.1.m1.1.1.3.3">Â°</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px5.p1.1.m1.1c">\sigma=1\text{Â°}</annotation></semantics></math>.
For our experiments, we assumed that the majority of gaze is valid and averaged the three recordings per stimulus, yielding a single attention map.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset Validation.</h4>

<div id="S3.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px6.p1.1" class="ltx_p">To validate that the attention maps indeed contain relevant image regions, we masked 300 stimuli with our recorded VQA-MHUG maps (see Figure <a href="#S3.F1.sf2" title="In Figure 1 â€£ Dataset Validation. â€£ 3 The VQA-MHUG Dataset â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>).
Then, we showed two additional participants these masked stimuli.
Comparing their answer accuracy with the participants who saw the full images, validation participants achieved comparable accuracy (62.43% vs. 63.87% in the main study).
Therefore our VQA-MHUG maps contain sufficient image areas to answer the questions and mask distracting objects as illustrated in Figure <a href="#S3.F1" title="Figure 1 â€£ Dataset Validation. â€£ 3 The VQA-MHUG Dataset â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.13116/assets/figures/3703.jpg" id="S3.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>collection study</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.13116/assets/figures/3703_masked.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>validation study</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example images for the question "How ripe are the bananas?". Validation images (b) were masked using the attention maps from our VQA-MHUG dataset.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison to Related Datasets.</h4>

<div id="S3.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px7.p1.1" class="ltx_p">We further measured the center bias and compared VQA-MHUG to related human attention datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>); Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>); Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> on their overlapping samples.
All datasets use mouse tracking as a proxy to collect human attention, except for the eye-tracking dataset AiR-DÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> which is similar to our recording paradigm, yet has no overlap with VQAv2.
Therefore, we showed participants 195 additional stimuli from the AiR-D dataset for comparison.
Table <a href="#S3.T1" title="Table 1 â€£ Comparison to Related Datasets. â€£ 3 The VQA-MHUG Dataset â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the mean rank correlation
of VQA-MHUG with a synthetic center fixation, inter-participant, and the other datasets.
The high correlation between VQA-MHUG and AiR-D indicates that our data is of comparable quality.
Our center bias is smaller compared to AiR-D but, as expected from human eye behaviour <cite class="ltx_cite ltx_citemacro_cite">Tatler (<a href="#bib.bib49" title="" class="ltx_ref">2007</a>)</cite>, larger than in the mouse tracking proxies SALICON and VQA-HAT.
We observe that both mouse tracking datasets have significantly lower correlation with VQA-MHUG than the eye-tracking AiR-D corpus.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.12.13.1" class="ltx_tr">
<th id="S3.T1.12.13.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.12.13.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T1.12.13.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.12.13.1.3.1" class="ltx_text ltx_font_bold">VQA-MHUG</span></td>
<td id="S3.T1.12.13.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.12.13.1.4.1" class="ltx_text ltx_font_bold">Center Fixation</span></td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.4.4.5.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T1.4.4.6" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.4.4.6.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">
<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mi id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">ğ†</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">ğ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\rho</annotation></semantics></math><span id="S3.T1.2.2.2.1" class="ltx_text ltx_font_bold"> <math id="S3.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.2.2.2.1.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.1.m1.1b"><ci id="S3.T1.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span>
</td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_left ltx_border_t">
<math id="S3.T1.3.3.3.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.T1.3.3.3.m1.1a"><mi id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">ğ†</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">ğ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\rho</annotation></semantics></math><span id="S3.T1.4.4.4.1" class="ltx_text ltx_font_bold"> <math id="S3.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.4.4.4.1.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><ci id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span>
</td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<th id="S3.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQA-MHUG</th>
<th id="S3.T1.6.6.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">G</th>
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S3.T1.5.5.1.m1.1" class="ltx_Math" alttext="0.769\pm 0.079" display="inline"><semantics id="S3.T1.5.5.1.m1.1a"><mrow id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml"><mn id="S3.T1.5.5.1.m1.1.1.2" xref="S3.T1.5.5.1.m1.1.1.2.cmml">0.769</mn><mo id="S3.T1.5.5.1.m1.1.1.1" xref="S3.T1.5.5.1.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.5.5.1.m1.1.1.3" xref="S3.T1.5.5.1.m1.1.1.3.cmml">0.079</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><apply id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.5.5.1.m1.1.1.2.cmml" xref="S3.T1.5.5.1.m1.1.1.2">0.769</cn><cn type="float" id="S3.T1.5.5.1.m1.1.1.3.cmml" xref="S3.T1.5.5.1.m1.1.1.3">0.079</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">0.769\pm 0.079</annotation></semantics></math></td>
<td id="S3.T1.6.6.2" class="ltx_td ltx_align_left ltx_border_t"><math id="S3.T1.6.6.2.m1.1" class="ltx_Math" alttext="0.473\pm 0.049" display="inline"><semantics id="S3.T1.6.6.2.m1.1a"><mrow id="S3.T1.6.6.2.m1.1.1" xref="S3.T1.6.6.2.m1.1.1.cmml"><mn id="S3.T1.6.6.2.m1.1.1.2" xref="S3.T1.6.6.2.m1.1.1.2.cmml">0.473</mn><mo id="S3.T1.6.6.2.m1.1.1.1" xref="S3.T1.6.6.2.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.6.6.2.m1.1.1.3" xref="S3.T1.6.6.2.m1.1.1.3.cmml">0.049</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.m1.1b"><apply id="S3.T1.6.6.2.m1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1"><csymbol cd="latexml" id="S3.T1.6.6.2.m1.1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.6.6.2.m1.1.1.2.cmml" xref="S3.T1.6.6.2.m1.1.1.2">0.473</cn><cn type="float" id="S3.T1.6.6.2.m1.1.1.3.cmml" xref="S3.T1.6.6.2.m1.1.1.3">0.049</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.m1.1c">0.473\pm 0.049</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.8.8" class="ltx_tr">
<th id="S3.T1.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">AiR-D</th>
<th id="S3.T1.8.8.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">G</th>
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_left"><math id="S3.T1.7.7.1.m1.1" class="ltx_Math" alttext="0.710\pm 0.060" display="inline"><semantics id="S3.T1.7.7.1.m1.1a"><mrow id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml"><mn id="S3.T1.7.7.1.m1.1.1.2" xref="S3.T1.7.7.1.m1.1.1.2.cmml">0.710</mn><mo id="S3.T1.7.7.1.m1.1.1.1" xref="S3.T1.7.7.1.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.7.7.1.m1.1.1.3" xref="S3.T1.7.7.1.m1.1.1.3.cmml">0.060</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><apply id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.1.m1.1.1.2">0.710</cn><cn type="float" id="S3.T1.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.1.m1.1.1.3">0.060</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">0.710\pm 0.060</annotation></semantics></math></td>
<td id="S3.T1.8.8.2" class="ltx_td ltx_align_left"><math id="S3.T1.8.8.2.m1.1" class="ltx_Math" alttext="0.523" display="inline"><semantics id="S3.T1.8.8.2.m1.1a"><mn id="S3.T1.8.8.2.m1.1.1" xref="S3.T1.8.8.2.m1.1.1.cmml">0.523</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.2.m1.1b"><cn type="float" id="S3.T1.8.8.2.m1.1.1.cmml" xref="S3.T1.8.8.2.m1.1.1">0.523</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.2.m1.1c">0.523</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<th id="S3.T1.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-HAT</th>
<th id="S3.T1.10.10.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">M</th>
<td id="S3.T1.9.9.1" class="ltx_td ltx_align_left"><math id="S3.T1.9.9.1.m1.1" class="ltx_Math" alttext="0.612\pm 0.145" display="inline"><semantics id="S3.T1.9.9.1.m1.1a"><mrow id="S3.T1.9.9.1.m1.1.1" xref="S3.T1.9.9.1.m1.1.1.cmml"><mn id="S3.T1.9.9.1.m1.1.1.2" xref="S3.T1.9.9.1.m1.1.1.2.cmml">0.612</mn><mo id="S3.T1.9.9.1.m1.1.1.1" xref="S3.T1.9.9.1.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.9.9.1.m1.1.1.3" xref="S3.T1.9.9.1.m1.1.1.3.cmml">0.145</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.1.m1.1b"><apply id="S3.T1.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.9.9.1.m1.1.1.1.cmml" xref="S3.T1.9.9.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.9.9.1.m1.1.1.2.cmml" xref="S3.T1.9.9.1.m1.1.1.2">0.612</cn><cn type="float" id="S3.T1.9.9.1.m1.1.1.3.cmml" xref="S3.T1.9.9.1.m1.1.1.3">0.145</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.1.m1.1c">0.612\pm 0.145</annotation></semantics></math></td>
<td id="S3.T1.10.10.2" class="ltx_td ltx_align_left"><math id="S3.T1.10.10.2.m1.1" class="ltx_Math" alttext="0.339\pm 0.107" display="inline"><semantics id="S3.T1.10.10.2.m1.1a"><mrow id="S3.T1.10.10.2.m1.1.1" xref="S3.T1.10.10.2.m1.1.1.cmml"><mn id="S3.T1.10.10.2.m1.1.1.2" xref="S3.T1.10.10.2.m1.1.1.2.cmml">0.339</mn><mo id="S3.T1.10.10.2.m1.1.1.1" xref="S3.T1.10.10.2.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.10.10.2.m1.1.1.3" xref="S3.T1.10.10.2.m1.1.1.3.cmml">0.107</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.2.m1.1b"><apply id="S3.T1.10.10.2.m1.1.1.cmml" xref="S3.T1.10.10.2.m1.1.1"><csymbol cd="latexml" id="S3.T1.10.10.2.m1.1.1.1.cmml" xref="S3.T1.10.10.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.10.10.2.m1.1.1.2.cmml" xref="S3.T1.10.10.2.m1.1.1.2">0.339</cn><cn type="float" id="S3.T1.10.10.2.m1.1.1.3.cmml" xref="S3.T1.10.10.2.m1.1.1.3">0.107</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.2.m1.1c">0.339\pm 0.107</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.12.12" class="ltx_tr">
<th id="S3.T1.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row">SALICON</th>
<th id="S3.T1.12.12.4" class="ltx_td ltx_align_right ltx_th ltx_th_row">M</th>
<td id="S3.T1.11.11.1" class="ltx_td ltx_align_left"><math id="S3.T1.11.11.1.m1.1" class="ltx_Math" alttext="0.634\pm 0.063" display="inline"><semantics id="S3.T1.11.11.1.m1.1a"><mrow id="S3.T1.11.11.1.m1.1.1" xref="S3.T1.11.11.1.m1.1.1.cmml"><mn id="S3.T1.11.11.1.m1.1.1.2" xref="S3.T1.11.11.1.m1.1.1.2.cmml">0.634</mn><mo id="S3.T1.11.11.1.m1.1.1.1" xref="S3.T1.11.11.1.m1.1.1.1.cmml">Â±</mo><mn id="S3.T1.11.11.1.m1.1.1.3" xref="S3.T1.11.11.1.m1.1.1.3.cmml">0.063</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.1.m1.1b"><apply id="S3.T1.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1"><csymbol cd="latexml" id="S3.T1.11.11.1.m1.1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.T1.11.11.1.m1.1.1.2.cmml" xref="S3.T1.11.11.1.m1.1.1.2">0.634</cn><cn type="float" id="S3.T1.11.11.1.m1.1.1.3.cmml" xref="S3.T1.11.11.1.m1.1.1.3">0.063</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.1.m1.1c">0.634\pm 0.063</annotation></semantics></math></td>
<td id="S3.T1.12.12.2" class="ltx_td ltx_align_left"><math id="S3.T1.12.12.2.m1.1" class="ltx_Math" alttext="0.479" display="inline"><semantics id="S3.T1.12.12.2.m1.1a"><mn id="S3.T1.12.12.2.m1.1.1" xref="S3.T1.12.12.2.m1.1.1.cmml">0.479</mn><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.2.m1.1b"><cn type="float" id="S3.T1.12.12.2.m1.1.1.cmml" xref="S3.T1.12.12.2.m1.1.1">0.479</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.2.m1.1c">0.479</annotation></semantics></math></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S3.T1.12.14.1" class="ltx_tr">
<th id="S3.T1.12.14.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4">
<span id="S3.T1.12.14.1.1.1" class="ltx_text ltx_font_bold">G</span>: Gaze,
<span id="S3.T1.12.14.1.1.2" class="ltx_text ltx_font_bold">M</span>: Mouse-Tracking</th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Spearmanâ€™s rank correlation (<math id="S3.T1.14.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.T1.14.m1.1b"><mi id="S3.T1.14.m1.1.1" xref="S3.T1.14.m1.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S3.T1.14.m1.1c"><ci id="S3.T1.14.m1.1.1.cmml" xref="S3.T1.14.m1.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.m1.1d">\rho</annotation></semantics></math>)
of VQA-MHUG with itself (inter-participant), related datasets, and a synthetic center fixation â€“ Mean over all samples in the intersection of the datasets and three VQA-MHUG participants. The standard deviation is the mean error over participants.
Only VQA-HAT and VQA-MHUG provide multiple attention maps per sample, allowing us to calculate the standard deviation when comparing to the synthetic center fixation.
</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Comparison of Human and Machine Attention</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The collected data enabled us to analyze whether models achieve a higher accuracy on VQAv2 val the more their attentive behavior over the text and image correlates with human ground-truth attention.
Hence, we investigated the attention weights over text and image features of different SOTA VQA models.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>VQA Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We selected five top performing VQA models of the VQA challenges 2017 to 2020:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">MFBÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> (Runner-up 2017);</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">BANÂ <cite class="ltx_cite ltx_citemacro_cite">Kim etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite> (Runner-up 2018);</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Pythia v0.1Â <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> (Winner 2018);</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">MCAN<sub id="S4.I1.i4.p1.1.1" class="ltx_sub">R</sub> with region image features Â <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>)</cite> (Winner 2019);</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">MCAN<sub id="S4.I1.i5.p1.1.1" class="ltx_sub">G</sub> with grid image featuresÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite> (Winner 2020).</p>
</div>
</li>
</ul>
<p id="S4.SS1.p1.2" class="ltx_p">Instead of using the text and image features directly for classification, these models re-weight the features using linear, bilinear and TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> (co-)attention mechanisms, whose attention maps we extracted and compared to human ground-truths from VQA-MHUG.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Pythia and MFB use co-attention: they first use a projected attention map to re-weight text features, then fuse them with the image features using linear (Pythia) and bilinear (MFB) fusion and subsequently re-weight the image features using an attention map projected from the fused features.
In this way, the text attention influences the image attention.
BAN avoids separating the attention into text and image streams and reduces both input streams simultaneously with a bilinear attention map projected from the fused features.
Finally, MCAN as a Transformer model stacks co-attention modules with multi-headed scaled dot-product attention for each modality.
After the last Transformer layer in both the text and image stream, another attention map is used to project the feature matrix into a single feature vector.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Extracting Model Attention</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We used
an official implementation<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/zwxalgorithm/pythia" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zwxalgorithm/pythia</a></span></span></span> of the Pythia v0.1 architecture and the OpenVQA<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/MILVLG/openvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/openvqa</a></span></span></span> implementationsÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib59" title="" class="ltx_ref">2019a</a>)</cite> for MFB, BAN and MCAN.
We re-implemented the grid image feature loader for MCAN<sub id="S4.SS2.p1.1.1" class="ltx_sub">G</sub>, since it is not available in OpenVQA.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Following previous workÂ <cite class="ltx_cite ltx_citemacro_cite">Sood etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>, we trained each network architecture twelve times with random seeds on the VQAv2 training set and then chose the top nine models based on the validation accuracy.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For models based on region image features, we used the extracted features provided by Anderson et al.Â <cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib5" title="" class="ltx_ref">2018</a></cite>, while we trained MCAN<sub id="S4.SS2.p3.1.1" class="ltx_sub">G</sub> with ResNeXtÂ <cite class="ltx_cite ltx_citemacro_cite">Xie etÂ al. (<a href="#bib.bib53" title="" class="ltx_ref">2017</a>)</cite> grid features as provided by the authorsÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/facebookresearch/grid-feats-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/grid-feats-vqa</a></span></span></span>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">For MFB and Pythia we extracted the two projected attention maps that re-weight text and image features, while we extracted the single bilinear attention map for BAN.
To obtain separate attention maps for text and image from BANâ€™s bilinear attention map, we marginalized over each dimension as suggested by the authorsÂ <cite class="ltx_cite ltx_citemacro_cite">Kim etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>.
MFB, BAN and Pythia generate multiple such attention maps called â€œglimpsesâ€
by using multiple projections.
We averaged the glimpses after extraction, yielding a single attention map for each modality.
Since it is unclear how the Transformer layer weights relate to the original input features, we instead extracted the attention weights of the final projection layer in text and image streams for MCAN<sub id="S4.SS2.p4.1.1" class="ltx_sub">R</sub> and MCAN<sub id="S4.SS2.p4.1.2" class="ltx_sub">G</sub>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">The extracted image attention maps contain one weight per feature.
To compare them with the human spatial attention maps collected in VQA-MHUG, we mapped the features back to their source region in the image.
For region-based features we assigned the attention weights to the corresponding bounding box normalized by region size.
Analogously, for grid-based features, we mapped the attention weights to their corresponding grid cells.
The text attention vector was directly mapped back to the question token sequence.
We excluded 74 samples due to varied tokenization between models.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.13116/assets/figures/model_attention2_dataset_nomds.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="1149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>comparison to other attention datasets</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.13116/assets/figures/model_attention2_model.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="690" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>model attention - text, image, and inter-modal comparison</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Attention maps visualized across question types.
Image attention seems mostly plausible throughout models. Previous datasets lack attention on the questions, but we reveal now that text attention is not always human-like, nor plausible. Mouse tracking datasets, SALICON and VQA-HAT, seem to over-estimate the relevant areas.
</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Performance Metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compared the multimodal attention extracted from five models to our human data in VQA-MHUG using three approaches.
We used Spearmanâ€™s rank correlation to compare importance ranking of image regions and words, Jensen-Shannon divergence to compare the distance between the human, and neural attention distributions
and a regression model to study the suitability of text and image correlation as predictors of per document model accuracy.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Spearmanâ€™s rank correlation and Jensen-Shannon divergence.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Similar to prior work, we downsampled all attention maps to 14x14 matrices and calculated the mean <span id="S4.SS3.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Spearmanâ€™s rank correlation</span> <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">\rho</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> and <span id="S4.SS3.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">Jensen-Shannon divergence</span> (JSD)Â <cite class="ltx_cite ltx_citemacro_cite">Sood etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020a</a>, <a href="#bib.bib47" title="" class="ltx_ref">b</a>)</cite> between the neural attention and the corresponding human attention.
We computed both metrics for both image and text modalities.
We also evaluated the corresponding accuracy scores on the VQAv2 validation setÂ <cite class="ltx_cite ltx_citemacro_cite">Agrawal (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ordinal Logistic Regression.</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">Averaging correlation over the whole dataset is too coarse and obscures the impact that similarity to human attention has on accuracy.
Additionally, rank correlation does not allow to analyze the effect of two independent variables on a dependent variableÂ <cite class="ltx_cite ltx_citemacro_cite">Bewick etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2003</a>)</cite>, e.g. image and text attention correlation on accuracy.
To account for this and to study on a per document basis which modality factors influence the likelihood of a model to predict the answer correctly, we performed an <span id="S4.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Ordinal Linear Regression</span> (OLR).</p>
</div>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p2.1" class="ltx_p">The official VQAv2 evaluationÂ <cite class="ltx_cite ltx_citemacro_cite">Agrawal (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> score per document is based on agreement with ten human annotator answers, where each match increases the score by 0.3 (capped at 1.0 or 4 agreed answers).
Since our response variable (accuracy score) is not necessarily ordered equidistant, we binned accuracy scores for each document into a likelihood scale (accuracy correctness).</p>
</div>
<div id="S4.SS3.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p3.3" class="ltx_p">The model predicts the likelihood of accuracy correctness for each document with three different predictors â€” the text correlation (<math id="S4.SS3.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS3.SSS0.Px2.p3.1.m1.1a"><mi id="S4.SS3.SSS0.Px2.p3.1.m1.1.1" xref="S4.SS3.SSS0.Px2.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p3.1.m1.1b"><ci id="S4.SS3.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p3.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p3.1.m1.1c">x</annotation></semantics></math>), the image correlation (<math id="S4.SS3.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS3.SSS0.Px2.p3.2.m2.1a"><mi id="S4.SS3.SSS0.Px2.p3.2.m2.1.1" xref="S4.SS3.SSS0.Px2.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p3.2.m2.1b"><ci id="S4.SS3.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px2.p3.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p3.2.m2.1c">y</annotation></semantics></math>), and the interaction between the text and image correlation (<math id="S4.SS3.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS3.SSS0.Px2.p3.3.m3.1a"><mi id="S4.SS3.SSS0.Px2.p3.3.m3.1.1" xref="S4.SS3.SSS0.Px2.p3.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p3.3.m3.1b"><ci id="S4.SS3.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S4.SS3.SSS0.Px2.p3.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p3.3.m3.1c">z</annotation></semantics></math>). The latter we deem inter-modal correlation predictor, as it allows us to test if the interaction between the correlation of text and image impacts accuracy. Given that the dependant variables are ranked we opted for using ordered logistic regression to predict for each accuracy bin.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Human and Neural Attention Relationship â€“ Averaged Over Documents</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">TableÂ <a href="#S5.T2" title="Table 2 â€£ Text attention. â€£ 5.1 Human and Neural Attention Relationship â€“ Averaged Over Documents â€£ 5 Results â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall accuracy scores of the five models on the VQAv2 validation set when trained only on the training partition. The models improved over the challenge years â€“ MCAN grid is the current SOTAÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>.
For each model and modality, we report the Spearmanâ€™s rank correlation and JSD scores averaged over the entire VQA-MHUG corpus (cf. SectionÂ <a href="#S4.SS3" title="4.3 Performance Metrics â€£ 4 Comparison of Human and Machine Attention â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
All figures were averaged over nine model runs and the standard deviation is given over those instances.
Given that one cannot average p-values we used a paired t-test to check if the differences in correlation and JSD per document and between models were statistically significant at <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><lt id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></lt><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">ğ‘</ci><cn type="float" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math> (see Appendix <a href="#A4" title="Appendix D Significance between models â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image attention.</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.3" class="ltx_p">Models using region features, i.e. excluding MCAN<math id="S5.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1a" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1a.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>, are more correlated with human visual attention on images.
MCAN<math id="S5.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.2.m2.1.1a" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1"><ci id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1a.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">{}_{\text{R}}</annotation></semantics></math> achieves the highest correlation, MFB the lowest, and the general trend shows that models with higher correlation had higher overall validation accuracy.
Although MCAN<math id="S5.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.3.m3.1a"><msub id="S5.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.3.m3.1.1a" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"></mi><mtext id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1"><ci id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1a.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.3.m3.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.3.m3.1c">{}_{\text{G}}</annotation></semantics></math> achieves the highest accuracy, it had the lowest correlation with human image attention.
For all model types, the difference between image correlation scores is significant, except between Pythia and BAN (see Appendix <a href="#A4" title="Appendix D Significance between models â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).
With respect to the JSD, we observed similar patterns except for the Pythia model, which was more dissimilar to human attention (had a higher overall JSD) compared to BAN.
For all model types, the difference between image JSD scores was statistically significant (see Appendix <a href="#A4" title="Appendix D Significance between models â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text attention.</h4>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.2" class="ltx_p">Both the correlation and JSD scores indicate that Pythia is the most similar to human text attention, followed by MFB.
Models with higher overall accuracy do not have high similarity to human visual attention over text on the JSD and correlation metrics.
For both metrics, the difference in text attention between every model pairing is statistically significant, except for the JSD scores between pairings of BAN, MCAN<math id="S5.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><msub id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS0.Px2.p1.1.m1.1.1a" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1"><ci id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1a.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>, and MCAN<math id="S5.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.1a"><msub id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS0.Px2.p1.2.m2.1.1a" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1"><ci id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1a.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.1c">{}_{\text{R}}</annotation></semantics></math> (see Appendix <a href="#A4" title="Appendix D Significance between models â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.28" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.6pt,8.9pt) scale(0.858929646688029,0.858929646688029) ;">
<table id="S5.T2.28.28" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.28.28.29.1" class="ltx_tr">
<th id="S5.T2.28.28.29.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.28.28.29.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.28.28.29.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T2.28.28.29.1.3.1" class="ltx_text ltx_font_bold">Image</span></th>
<th id="S5.T2.28.28.29.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S5.T2.28.28.29.1.4.1" class="ltx_text ltx_font_bold">Text</span></th>
</tr>
<tr id="S5.T2.6.6.6" class="ltx_tr">
<th id="S5.T2.6.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T2.6.6.6.7.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T2.6.6.6.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S5.T2.6.6.6.8.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="S5.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">ğ†</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">ğ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\rho</annotation></semantics></math><span id="S5.T2.2.2.2.2.1" class="ltx_text ltx_font_bold"> <math id="S5.T2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span>
</th>
<th id="S5.T2.3.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.3.3.3.3.1" class="ltx_text ltx_font_bold">JSD <math id="S5.T2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T2.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.1b"><ci id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S5.T2.5.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<math id="S5.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T2.4.4.4.4.m1.1a"><mi id="S5.T2.4.4.4.4.m1.1.1" xref="S5.T2.4.4.4.4.m1.1.1.cmml">ğ†</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.4.m1.1.1">ğ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.m1.1c">\rho</annotation></semantics></math><span id="S5.T2.5.5.5.5.1" class="ltx_text ltx_font_bold"> <math id="S5.T2.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.5.5.5.5.1.m1.1a"><mo stretchy="false" id="S5.T2.5.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.5.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.1.m1.1c">\uparrow</annotation></semantics></math></span>
</th>
<th id="S5.T2.6.6.6.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.6.6.6.6.1" class="ltx_text ltx_font_bold">JSD <math id="S5.T2.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.6.6.6.6.1.m1.1a"><mo stretchy="false" id="S5.T2.6.6.6.6.1.m1.1.1" xref="S5.T2.6.6.6.6.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.1.m1.1b"><ci id="S5.T2.6.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.6.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.11.11.11" class="ltx_tr">
<th id="S5.T2.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MCAN<math id="S5.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.T2.7.7.7.1.m1.1a"><msub id="S5.T2.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.cmml"><mi id="S5.T2.7.7.7.1.m1.1.1a" xref="S5.T2.7.7.7.1.m1.1.1.cmml"></mi><mtext id="S5.T2.7.7.7.1.m1.1.1.1" xref="S5.T2.7.7.7.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.1.m1.1b"><apply id="S5.T2.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1"><ci id="S5.T2.7.7.7.1.m1.1.1.1a.cmml" xref="S5.T2.7.7.7.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.7.7.7.1.m1.1.1.1.cmml" xref="S5.T2.7.7.7.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>
</th>
<th id="S5.T2.11.11.11.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">70.24%</th>
<td id="S5.T2.11.11.11.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<math id="S5.T2.8.8.8.2.m1.3" class="ltx_Math" alttext="0.509\pm 0.026\text{\,}\hfil\lx@intercol&amp;" display="inline"><semantics id="S5.T2.8.8.8.2.m1.3a"><mrow id="S5.T2.8.8.8.2.m1.3.3" xref="S5.T2.8.8.8.2.m1.3.3.cmml"><mrow id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.cmml"><mn id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.509</mn><mo id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.3" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">0.026</mn></mrow><mtext id="S5.T2.8.8.8.2.m1.2.2.2.2.2.2" xref="S5.T2.8.8.8.2.m1.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.8.8.8.2.m1.3.3.3.3.3.3" xref="S5.T2.8.8.8.2.m1.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.2.m1.3b"><apply id="S5.T2.8.8.8.2.m1.3.3.cmml" xref="S5.T2.8.8.8.2.m1.3.3"><csymbol cd="latexml" id="S5.T2.8.8.8.2.m1.2.2.2.2.2.2.cmml" xref="S5.T2.8.8.8.2.m1.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.3.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.1.1.1.1.1">0.509</cn><cn type="float" id="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.8.8.8.2.m1.1.1.1.1.1.1.2.2.2.2.2">0.026</cn></apply><ci id="S5.T2.8.8.8.2.m1.3.3.3.3.3.3.cmml" xref="S5.T2.8.8.8.2.m1.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.2.m1.3c">0.509\pm 0.026\text{\,}\hfil\lx@intercol&amp;</annotation></semantics></math> <math id="S5.T2.9.9.9.3.m2.3" class="ltx_Math" alttext="0.537\pm 0.003\text{\,}&amp;" display="inline"><semantics id="S5.T2.9.9.9.3.m2.3a"><mrow id="S5.T2.9.9.9.3.m2.3.3" xref="S5.T2.9.9.9.3.m2.3.3.cmml"><mrow id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.cmml"><mn id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.1.1.1.1.1.cmml">0.537</mn><mo id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.3" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.2.2.2.2.2.cmml">0.003</mn></mrow><mtext id="S5.T2.9.9.9.3.m2.2.2.2.2.2.2" xref="S5.T2.9.9.9.3.m2.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.9.9.9.3.m2.3.3.3.3.3.3" xref="S5.T2.9.9.9.3.m2.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.9.3.m2.3b"><apply id="S5.T2.9.9.9.3.m2.3.3.cmml" xref="S5.T2.9.9.9.3.m2.3.3"><csymbol cd="latexml" id="S5.T2.9.9.9.3.m2.2.2.2.2.2.2.cmml" xref="S5.T2.9.9.9.3.m2.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.cmml" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.3.cmml" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.1.1.1.1.1">0.537</cn><cn type="float" id="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.9.9.9.3.m2.1.1.1.1.1.1.2.2.2.2.2">0.003</cn></apply><ci id="S5.T2.9.9.9.3.m2.3.3.3.3.3.3.cmml" xref="S5.T2.9.9.9.3.m2.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.9.3.m2.3c">0.537\pm 0.003\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.10.10.10.4.m3.3" class="ltx_Math" alttext="-0.059\pm 0.012\text{\,}&amp;" display="inline"><semantics id="S5.T2.10.10.10.4.m3.3a"><mrow id="S5.T2.10.10.10.4.m3.3.3" xref="S5.T2.10.10.10.4.m3.3.3.cmml"><mrow id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.cmml"><mrow id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.cmml"><mo id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.2a" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.059</mn></mrow><mo id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.3" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.2.2.2.2.2.cmml">0.012</mn></mrow><mtext id="S5.T2.10.10.10.4.m3.2.2.2.2.2.2" xref="S5.T2.10.10.10.4.m3.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.10.10.10.4.m3.3.3.3.3.3.3" xref="S5.T2.10.10.10.4.m3.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.10.4.m3.3b"><apply id="S5.T2.10.10.10.4.m3.3.3.cmml" xref="S5.T2.10.10.10.4.m3.3.3"><csymbol cd="latexml" id="S5.T2.10.10.10.4.m3.2.2.2.2.2.2.cmml" xref="S5.T2.10.10.10.4.m3.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.cmml" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.3.cmml" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.1.1.1.1.1.2a">-0.059</cn><cn type="float" id="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.10.10.10.4.m3.1.1.1.1.1.1.2.2.2.2.2">0.012</cn></apply><ci id="S5.T2.10.10.10.4.m3.3.3.3.3.3.3.cmml" xref="S5.T2.10.10.10.4.m3.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.10.4.m3.3c">-0.059\pm 0.012\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.11.11.11.5.m4.3" class="ltx_Math" alttext="0.402\pm 0.007\text{\,}\ " display="inline"><semantics id="S5.T2.11.11.11.5.m4.3a"><mrow id="S5.T2.11.11.11.5.m4.3.3" xref="S5.T2.11.11.11.5.m4.3.3.cmml"><mrow id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.cmml"><mn id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.1.1.1.1.1.cmml">0.402</mn><mo id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.3" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.2.2.2.2.2.cmml">0.007</mn></mrow><mtext id="S5.T2.11.11.11.5.m4.2.2.2.2.2.2" xref="S5.T2.11.11.11.5.m4.2.2.2.2.2.2.cmml">Â </mtext><mi id="S5.T2.11.11.11.5.m4.3.3.3.3.3.3" xref="S5.T2.11.11.11.5.m4.3.3.3.3.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.11.11.11.5.m4.3b"><apply id="S5.T2.11.11.11.5.m4.3.3.cmml" xref="S5.T2.11.11.11.5.m4.3.3"><csymbol cd="latexml" id="S5.T2.11.11.11.5.m4.2.2.2.2.2.2.cmml" xref="S5.T2.11.11.11.5.m4.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.cmml" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.3.cmml" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.1.1.1.1.1">0.402</cn><cn type="float" id="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.11.11.11.5.m4.1.1.1.1.1.1.2.2.2.2.2">0.007</cn></apply><csymbol cd="latexml" id="S5.T2.11.11.11.5.m4.3.3.3.3.3.3.cmml" xref="S5.T2.11.11.11.5.m4.3.3.3.3.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.11.11.5.m4.3c">0.402\pm 0.007\text{\,}\ </annotation></semantics></math>
</td>
<td id="S5.T2.11.11.11.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.11.11.11.8" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.11.11.11.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T2.16.16.16" class="ltx_tr">
<th id="S5.T2.12.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCAN<math id="S5.T2.12.12.12.1.m1.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.T2.12.12.12.1.m1.1a"><msub id="S5.T2.12.12.12.1.m1.1.1" xref="S5.T2.12.12.12.1.m1.1.1.cmml"><mi id="S5.T2.12.12.12.1.m1.1.1a" xref="S5.T2.12.12.12.1.m1.1.1.cmml"></mi><mtext id="S5.T2.12.12.12.1.m1.1.1.1" xref="S5.T2.12.12.12.1.m1.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.12.12.12.1.m1.1b"><apply id="S5.T2.12.12.12.1.m1.1.1.cmml" xref="S5.T2.12.12.12.1.m1.1.1"><ci id="S5.T2.12.12.12.1.m1.1.1.1a.cmml" xref="S5.T2.12.12.12.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T2.12.12.12.1.m1.1.1.1.cmml" xref="S5.T2.12.12.12.1.m1.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.12.12.1.m1.1c">{}_{\text{R}}</annotation></semantics></math>
</th>
<th id="S5.T2.16.16.16.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">67.24%</th>
<td id="S5.T2.16.16.16.5" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S5.T2.13.13.13.2.m1.3" class="ltx_Math" alttext="0.602\pm 0.003\text{\,}\hfil\lx@intercol&amp;" display="inline"><semantics id="S5.T2.13.13.13.2.m1.3a"><mrow id="S5.T2.13.13.13.2.m1.3.3" xref="S5.T2.13.13.13.2.m1.3.3.cmml"><mrow id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.cmml"><mn id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.602</mn><mo id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.3" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">0.003</mn></mrow><mtext id="S5.T2.13.13.13.2.m1.2.2.2.2.2.2" xref="S5.T2.13.13.13.2.m1.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.13.13.13.2.m1.3.3.3.3.3.3" xref="S5.T2.13.13.13.2.m1.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.13.13.13.2.m1.3b"><apply id="S5.T2.13.13.13.2.m1.3.3.cmml" xref="S5.T2.13.13.13.2.m1.3.3"><csymbol cd="latexml" id="S5.T2.13.13.13.2.m1.2.2.2.2.2.2.cmml" xref="S5.T2.13.13.13.2.m1.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.cmml" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.3.cmml" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.1.1.1.1.1">0.602</cn><cn type="float" id="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.13.13.13.2.m1.1.1.1.1.1.1.2.2.2.2.2">0.003</cn></apply><ci id="S5.T2.13.13.13.2.m1.3.3.3.3.3.3.cmml" xref="S5.T2.13.13.13.2.m1.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.13.13.2.m1.3c">0.602\pm 0.003\text{\,}\hfil\lx@intercol&amp;</annotation></semantics></math> <math id="S5.T2.14.14.14.3.m2.3" class="ltx_Math" alttext="0.467\pm 0.002\text{\,}&amp;" display="inline"><semantics id="S5.T2.14.14.14.3.m2.3a"><mrow id="S5.T2.14.14.14.3.m2.3.3" xref="S5.T2.14.14.14.3.m2.3.3.cmml"><mrow id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.cmml"><mn id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.1.1.1.1.1.cmml">0.467</mn><mo id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.3" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.2.2.2.2.2.cmml">0.002</mn></mrow><mtext id="S5.T2.14.14.14.3.m2.2.2.2.2.2.2" xref="S5.T2.14.14.14.3.m2.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.14.14.14.3.m2.3.3.3.3.3.3" xref="S5.T2.14.14.14.3.m2.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.14.14.14.3.m2.3b"><apply id="S5.T2.14.14.14.3.m2.3.3.cmml" xref="S5.T2.14.14.14.3.m2.3.3"><csymbol cd="latexml" id="S5.T2.14.14.14.3.m2.2.2.2.2.2.2.cmml" xref="S5.T2.14.14.14.3.m2.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.cmml" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.3.cmml" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.1.1.1.1.1">0.467</cn><cn type="float" id="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.14.14.14.3.m2.1.1.1.1.1.1.2.2.2.2.2">0.002</cn></apply><ci id="S5.T2.14.14.14.3.m2.3.3.3.3.3.3.cmml" xref="S5.T2.14.14.14.3.m2.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.14.14.3.m2.3c">0.467\pm 0.002\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.15.15.15.4.m3.3" class="ltx_Math" alttext="-0.042\pm 0.018\text{\,}&amp;" display="inline"><semantics id="S5.T2.15.15.15.4.m3.3a"><mrow id="S5.T2.15.15.15.4.m3.3.3" xref="S5.T2.15.15.15.4.m3.3.3.cmml"><mrow id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.cmml"><mrow id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.cmml"><mo id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.2a" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.042</mn></mrow><mo id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.3" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.2.2.2.2.2.cmml">0.018</mn></mrow><mtext id="S5.T2.15.15.15.4.m3.2.2.2.2.2.2" xref="S5.T2.15.15.15.4.m3.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.15.15.15.4.m3.3.3.3.3.3.3" xref="S5.T2.15.15.15.4.m3.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.15.15.15.4.m3.3b"><apply id="S5.T2.15.15.15.4.m3.3.3.cmml" xref="S5.T2.15.15.15.4.m3.3.3"><csymbol cd="latexml" id="S5.T2.15.15.15.4.m3.2.2.2.2.2.2.cmml" xref="S5.T2.15.15.15.4.m3.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.cmml" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.3.cmml" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.1.1.1.1.1.2a">-0.042</cn><cn type="float" id="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.15.15.15.4.m3.1.1.1.1.1.1.2.2.2.2.2">0.018</cn></apply><ci id="S5.T2.15.15.15.4.m3.3.3.3.3.3.3.cmml" xref="S5.T2.15.15.15.4.m3.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.15.15.4.m3.3c">-0.042\pm 0.018\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.16.16.16.5.m4.3" class="ltx_Math" alttext="0.398\pm 0.017\text{\,}\ " display="inline"><semantics id="S5.T2.16.16.16.5.m4.3a"><mrow id="S5.T2.16.16.16.5.m4.3.3" xref="S5.T2.16.16.16.5.m4.3.3.cmml"><mrow id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.cmml"><mn id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.1.1.1.1.1.cmml">0.398</mn><mo id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.3" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.2.2.2.2.2.cmml">0.017</mn></mrow><mtext id="S5.T2.16.16.16.5.m4.2.2.2.2.2.2" xref="S5.T2.16.16.16.5.m4.2.2.2.2.2.2.cmml">Â </mtext><mi id="S5.T2.16.16.16.5.m4.3.3.3.3.3.3" xref="S5.T2.16.16.16.5.m4.3.3.3.3.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.16.16.16.5.m4.3b"><apply id="S5.T2.16.16.16.5.m4.3.3.cmml" xref="S5.T2.16.16.16.5.m4.3.3"><csymbol cd="latexml" id="S5.T2.16.16.16.5.m4.2.2.2.2.2.2.cmml" xref="S5.T2.16.16.16.5.m4.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.cmml" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.3.cmml" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.1.1.1.1.1">0.398</cn><cn type="float" id="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.16.16.16.5.m4.1.1.1.1.1.1.2.2.2.2.2">0.017</cn></apply><csymbol cd="latexml" id="S5.T2.16.16.16.5.m4.3.3.3.3.3.3.cmml" xref="S5.T2.16.16.16.5.m4.3.3.3.3.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.16.16.5.m4.3c">0.398\pm 0.017\text{\,}\ </annotation></semantics></math>
</td>
<td id="S5.T2.16.16.16.7" class="ltx_td"></td>
<td id="S5.T2.16.16.16.8" class="ltx_td"></td>
<td id="S5.T2.16.16.16.9" class="ltx_td"></td>
</tr>
<tr id="S5.T2.20.20.20" class="ltx_tr">
<th id="S5.T2.20.20.20.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pythia</th>
<th id="S5.T2.20.20.20.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">66.00%</th>
<td id="S5.T2.20.20.20.4" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S5.T2.17.17.17.1.m1.3" class="ltx_Math" alttext="0.584\pm 0.003\text{\,}\hfil\lx@intercol&amp;" display="inline"><semantics id="S5.T2.17.17.17.1.m1.3a"><mrow id="S5.T2.17.17.17.1.m1.3.3" xref="S5.T2.17.17.17.1.m1.3.3.cmml"><mrow id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.cmml"><mn id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.584</mn><mo id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.3" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">0.003</mn></mrow><mtext id="S5.T2.17.17.17.1.m1.2.2.2.2.2.2" xref="S5.T2.17.17.17.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.17.17.17.1.m1.3.3.3.3.3.3" xref="S5.T2.17.17.17.1.m1.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.17.17.17.1.m1.3b"><apply id="S5.T2.17.17.17.1.m1.3.3.cmml" xref="S5.T2.17.17.17.1.m1.3.3"><csymbol cd="latexml" id="S5.T2.17.17.17.1.m1.2.2.2.2.2.2.cmml" xref="S5.T2.17.17.17.1.m1.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.cmml" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.1.1.1.1.1">0.584</cn><cn type="float" id="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.17.17.17.1.m1.1.1.1.1.1.1.2.2.2.2.2">0.003</cn></apply><ci id="S5.T2.17.17.17.1.m1.3.3.3.3.3.3.cmml" xref="S5.T2.17.17.17.1.m1.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.17.17.1.m1.3c">0.584\pm 0.003\text{\,}\hfil\lx@intercol&amp;</annotation></semantics></math> <math id="S5.T2.18.18.18.2.m2.3" class="ltx_Math" alttext="0.479\pm 0.001\text{\,}&amp;" display="inline"><semantics id="S5.T2.18.18.18.2.m2.3a"><mrow id="S5.T2.18.18.18.2.m2.3.3" xref="S5.T2.18.18.18.2.m2.3.3.cmml"><mrow id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.cmml"><mn id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml">0.479</mn><mo id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.3" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml">0.001</mn></mrow><mtext id="S5.T2.18.18.18.2.m2.2.2.2.2.2.2" xref="S5.T2.18.18.18.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.18.18.18.2.m2.3.3.3.3.3.3" xref="S5.T2.18.18.18.2.m2.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.18.18.18.2.m2.3b"><apply id="S5.T2.18.18.18.2.m2.3.3.cmml" xref="S5.T2.18.18.18.2.m2.3.3"><csymbol cd="latexml" id="S5.T2.18.18.18.2.m2.2.2.2.2.2.2.cmml" xref="S5.T2.18.18.18.2.m2.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.cmml" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.3.cmml" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.1.1.1.1.1">0.479</cn><cn type="float" id="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.18.18.18.2.m2.1.1.1.1.1.1.2.2.2.2.2">0.001</cn></apply><ci id="S5.T2.18.18.18.2.m2.3.3.3.3.3.3.cmml" xref="S5.T2.18.18.18.2.m2.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.18.18.2.m2.3c">0.479\pm 0.001\text{\,}&amp;</annotation></semantics></math> Â Â  <math id="S5.T2.19.19.19.3.m3.3" class="ltx_Math" alttext="0.251\pm 0.016\text{\,}&amp;" display="inline"><semantics id="S5.T2.19.19.19.3.m3.3a"><mrow id="S5.T2.19.19.19.3.m3.3.3" xref="S5.T2.19.19.19.3.m3.3.3.cmml"><mrow id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.cmml"><mn id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml">0.251</mn><mo id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.3" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml">0.016</mn></mrow><mtext id="S5.T2.19.19.19.3.m3.2.2.2.2.2.2" xref="S5.T2.19.19.19.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.19.19.19.3.m3.3.3.3.3.3.3" xref="S5.T2.19.19.19.3.m3.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.19.19.19.3.m3.3b"><apply id="S5.T2.19.19.19.3.m3.3.3.cmml" xref="S5.T2.19.19.19.3.m3.3.3"><csymbol cd="latexml" id="S5.T2.19.19.19.3.m3.2.2.2.2.2.2.cmml" xref="S5.T2.19.19.19.3.m3.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.cmml" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.1.1.1.1.1">0.251</cn><cn type="float" id="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.19.19.19.3.m3.1.1.1.1.1.1.2.2.2.2.2">0.016</cn></apply><ci id="S5.T2.19.19.19.3.m3.3.3.3.3.3.3.cmml" xref="S5.T2.19.19.19.3.m3.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.19.19.3.m3.3c">0.251\pm 0.016\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.20.20.20.4.m4.3" class="ltx_Math" alttext="0.337\pm 0.015\text{\,}\ " display="inline"><semantics id="S5.T2.20.20.20.4.m4.3a"><mrow id="S5.T2.20.20.20.4.m4.3.3" xref="S5.T2.20.20.20.4.m4.3.3.cmml"><mrow id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.cmml"><mn id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml">0.337</mn><mo id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.3" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml">0.015</mn></mrow><mtext id="S5.T2.20.20.20.4.m4.2.2.2.2.2.2" xref="S5.T2.20.20.20.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi id="S5.T2.20.20.20.4.m4.3.3.3.3.3.3" xref="S5.T2.20.20.20.4.m4.3.3.3.3.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.20.20.20.4.m4.3b"><apply id="S5.T2.20.20.20.4.m4.3.3.cmml" xref="S5.T2.20.20.20.4.m4.3.3"><csymbol cd="latexml" id="S5.T2.20.20.20.4.m4.2.2.2.2.2.2.cmml" xref="S5.T2.20.20.20.4.m4.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.cmml" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.3.cmml" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.1.1.1.1.1">0.337</cn><cn type="float" id="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.20.20.20.4.m4.1.1.1.1.1.1.2.2.2.2.2">0.015</cn></apply><csymbol cd="latexml" id="S5.T2.20.20.20.4.m4.3.3.3.3.3.3.cmml" xref="S5.T2.20.20.20.4.m4.3.3.3.3.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.20.20.20.4.m4.3c">0.337\pm 0.015\text{\,}\ </annotation></semantics></math>
</td>
<td id="S5.T2.20.20.20.7" class="ltx_td"></td>
<td id="S5.T2.20.20.20.8" class="ltx_td"></td>
<td id="S5.T2.20.20.20.9" class="ltx_td"></td>
</tr>
<tr id="S5.T2.24.24.24" class="ltx_tr">
<th id="S5.T2.24.24.24.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">BAN</th>
<th id="S5.T2.24.24.24.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">65.91%</th>
<td id="S5.T2.24.24.24.4" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S5.T2.21.21.21.1.m1.3" class="ltx_Math" alttext="0.582\pm 0.004\text{\,}\hfil\lx@intercol&amp;" display="inline"><semantics id="S5.T2.21.21.21.1.m1.3a"><mrow id="S5.T2.21.21.21.1.m1.3.3" xref="S5.T2.21.21.21.1.m1.3.3.cmml"><mrow id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.cmml"><mn id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.582</mn><mo id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.3" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">0.004</mn></mrow><mtext id="S5.T2.21.21.21.1.m1.2.2.2.2.2.2" xref="S5.T2.21.21.21.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.21.21.21.1.m1.3.3.3.3.3.3" xref="S5.T2.21.21.21.1.m1.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.21.21.21.1.m1.3b"><apply id="S5.T2.21.21.21.1.m1.3.3.cmml" xref="S5.T2.21.21.21.1.m1.3.3"><csymbol cd="latexml" id="S5.T2.21.21.21.1.m1.2.2.2.2.2.2.cmml" xref="S5.T2.21.21.21.1.m1.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.cmml" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.1.1.1.1.1">0.582</cn><cn type="float" id="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.21.21.21.1.m1.1.1.1.1.1.1.2.2.2.2.2">0.004</cn></apply><ci id="S5.T2.21.21.21.1.m1.3.3.3.3.3.3.cmml" xref="S5.T2.21.21.21.1.m1.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.21.21.1.m1.3c">0.582\pm 0.004\text{\,}\hfil\lx@intercol&amp;</annotation></semantics></math> <math id="S5.T2.22.22.22.2.m2.3" class="ltx_Math" alttext="0.469\pm 0.002\text{\,}&amp;" display="inline"><semantics id="S5.T2.22.22.22.2.m2.3a"><mrow id="S5.T2.22.22.22.2.m2.3.3" xref="S5.T2.22.22.22.2.m2.3.3.cmml"><mrow id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.cmml"><mn id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml">0.469</mn><mo id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.3" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml">0.002</mn></mrow><mtext id="S5.T2.22.22.22.2.m2.2.2.2.2.2.2" xref="S5.T2.22.22.22.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.22.22.22.2.m2.3.3.3.3.3.3" xref="S5.T2.22.22.22.2.m2.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.22.22.22.2.m2.3b"><apply id="S5.T2.22.22.22.2.m2.3.3.cmml" xref="S5.T2.22.22.22.2.m2.3.3"><csymbol cd="latexml" id="S5.T2.22.22.22.2.m2.2.2.2.2.2.2.cmml" xref="S5.T2.22.22.22.2.m2.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.cmml" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.3.cmml" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.1.1.1.1.1">0.469</cn><cn type="float" id="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.22.22.22.2.m2.1.1.1.1.1.1.2.2.2.2.2">0.002</cn></apply><ci id="S5.T2.22.22.22.2.m2.3.3.3.3.3.3.cmml" xref="S5.T2.22.22.22.2.m2.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.22.22.22.2.m2.3c">0.469\pm 0.002\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.23.23.23.3.m3.3" class="ltx_Math" alttext="-0.132\pm 0.030\text{\,}&amp;" display="inline"><semantics id="S5.T2.23.23.23.3.m3.3a"><mrow id="S5.T2.23.23.23.3.m3.3.3" xref="S5.T2.23.23.23.3.m3.3.3.cmml"><mrow id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.cmml"><mrow id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.cmml"><mo id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.2a" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.132</mn></mrow><mo id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.3" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml">0.030</mn></mrow><mtext id="S5.T2.23.23.23.3.m3.2.2.2.2.2.2" xref="S5.T2.23.23.23.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.23.23.23.3.m3.3.3.3.3.3.3" xref="S5.T2.23.23.23.3.m3.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.23.23.23.3.m3.3b"><apply id="S5.T2.23.23.23.3.m3.3.3.cmml" xref="S5.T2.23.23.23.3.m3.3.3"><csymbol cd="latexml" id="S5.T2.23.23.23.3.m3.2.2.2.2.2.2.cmml" xref="S5.T2.23.23.23.3.m3.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.cmml" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.1.1.1.1.1.2a">-0.132</cn><cn type="float" id="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.23.23.23.3.m3.1.1.1.1.1.1.2.2.2.2.2">0.030</cn></apply><ci id="S5.T2.23.23.23.3.m3.3.3.3.3.3.3.cmml" xref="S5.T2.23.23.23.3.m3.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.23.23.23.3.m3.3c">-0.132\pm 0.030\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.24.24.24.4.m4.3" class="ltx_Math" alttext="0.398\pm 0.021\text{\,}\ " display="inline"><semantics id="S5.T2.24.24.24.4.m4.3a"><mrow id="S5.T2.24.24.24.4.m4.3.3" xref="S5.T2.24.24.24.4.m4.3.3.cmml"><mrow id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.cmml"><mn id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml">0.398</mn><mo id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.3" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml">0.021</mn></mrow><mtext id="S5.T2.24.24.24.4.m4.2.2.2.2.2.2" xref="S5.T2.24.24.24.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi id="S5.T2.24.24.24.4.m4.3.3.3.3.3.3" xref="S5.T2.24.24.24.4.m4.3.3.3.3.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.24.24.24.4.m4.3b"><apply id="S5.T2.24.24.24.4.m4.3.3.cmml" xref="S5.T2.24.24.24.4.m4.3.3"><csymbol cd="latexml" id="S5.T2.24.24.24.4.m4.2.2.2.2.2.2.cmml" xref="S5.T2.24.24.24.4.m4.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.cmml" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.3.cmml" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.1.1.1.1.1">0.398</cn><cn type="float" id="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.24.24.24.4.m4.1.1.1.1.1.1.2.2.2.2.2">0.021</cn></apply><csymbol cd="latexml" id="S5.T2.24.24.24.4.m4.3.3.3.3.3.3.cmml" xref="S5.T2.24.24.24.4.m4.3.3.3.3.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.24.24.24.4.m4.3c">0.398\pm 0.021\text{\,}\ </annotation></semantics></math>
</td>
<td id="S5.T2.24.24.24.7" class="ltx_td"></td>
<td id="S5.T2.24.24.24.8" class="ltx_td"></td>
<td id="S5.T2.24.24.24.9" class="ltx_td"></td>
</tr>
<tr id="S5.T2.28.28.28" class="ltx_tr">
<th id="S5.T2.28.28.28.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MFB</th>
<th id="S5.T2.28.28.28.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">65.06%</th>
<td id="S5.T2.28.28.28.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">
<math id="S5.T2.25.25.25.1.m1.3" class="ltx_Math" alttext="0.530\pm 0.003\text{\,}\hfil\lx@intercol&amp;" display="inline"><semantics id="S5.T2.25.25.25.1.m1.3a"><mrow id="S5.T2.25.25.25.1.m1.3.3" xref="S5.T2.25.25.25.1.m1.3.3.cmml"><mrow id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.cmml"><mn id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">0.530</mn><mo id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.3" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">0.003</mn></mrow><mtext id="S5.T2.25.25.25.1.m1.2.2.2.2.2.2" xref="S5.T2.25.25.25.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.25.25.25.1.m1.3.3.3.3.3.3" xref="S5.T2.25.25.25.1.m1.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.25.25.25.1.m1.3b"><apply id="S5.T2.25.25.25.1.m1.3.3.cmml" xref="S5.T2.25.25.25.1.m1.3.3"><csymbol cd="latexml" id="S5.T2.25.25.25.1.m1.2.2.2.2.2.2.cmml" xref="S5.T2.25.25.25.1.m1.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.cmml" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.1.1.1.1.1">0.530</cn><cn type="float" id="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.25.25.25.1.m1.1.1.1.1.1.1.2.2.2.2.2">0.003</cn></apply><ci id="S5.T2.25.25.25.1.m1.3.3.3.3.3.3.cmml" xref="S5.T2.25.25.25.1.m1.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.25.25.25.1.m1.3c">0.530\pm 0.003\text{\,}\hfil\lx@intercol&amp;</annotation></semantics></math> <math id="S5.T2.26.26.26.2.m2.3" class="ltx_Math" alttext="0.523\pm 0.004\text{\,}&amp;" display="inline"><semantics id="S5.T2.26.26.26.2.m2.3a"><mrow id="S5.T2.26.26.26.2.m2.3.3" xref="S5.T2.26.26.26.2.m2.3.3.cmml"><mrow id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.cmml"><mn id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml">0.523</mn><mo id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.3" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml">0.004</mn></mrow><mtext id="S5.T2.26.26.26.2.m2.2.2.2.2.2.2" xref="S5.T2.26.26.26.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.26.26.26.2.m2.3.3.3.3.3.3" xref="S5.T2.26.26.26.2.m2.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.26.26.26.2.m2.3b"><apply id="S5.T2.26.26.26.2.m2.3.3.cmml" xref="S5.T2.26.26.26.2.m2.3.3"><csymbol cd="latexml" id="S5.T2.26.26.26.2.m2.2.2.2.2.2.2.cmml" xref="S5.T2.26.26.26.2.m2.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.cmml" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.3.cmml" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.1.1.1.1.1">0.523</cn><cn type="float" id="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.26.26.26.2.m2.1.1.1.1.1.1.2.2.2.2.2">0.004</cn></apply><ci id="S5.T2.26.26.26.2.m2.3.3.3.3.3.3.cmml" xref="S5.T2.26.26.26.2.m2.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.26.26.26.2.m2.3c">0.523\pm 0.004\text{\,}&amp;</annotation></semantics></math> Â Â  <math id="S5.T2.27.27.27.3.m3.3" class="ltx_Math" alttext="0.225\pm 0.055\text{\,}&amp;" display="inline"><semantics id="S5.T2.27.27.27.3.m3.3a"><mrow id="S5.T2.27.27.27.3.m3.3.3" xref="S5.T2.27.27.27.3.m3.3.3.cmml"><mrow id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.cmml"><mn id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml">0.225</mn><mo id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.3" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml">0.055</mn></mrow><mtext id="S5.T2.27.27.27.3.m3.2.2.2.2.2.2" xref="S5.T2.27.27.27.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi mathvariant="normal" id="S5.T2.27.27.27.3.m3.3.3.3.3.3.3" xref="S5.T2.27.27.27.3.m3.3.3.3.3.3.3.cmml">&amp;</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.27.27.27.3.m3.3b"><apply id="S5.T2.27.27.27.3.m3.3.3.cmml" xref="S5.T2.27.27.27.3.m3.3.3"><csymbol cd="latexml" id="S5.T2.27.27.27.3.m3.2.2.2.2.2.2.cmml" xref="S5.T2.27.27.27.3.m3.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.cmml" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.1.1.1.1.1">0.225</cn><cn type="float" id="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.27.27.27.3.m3.1.1.1.1.1.1.2.2.2.2.2">0.055</cn></apply><ci id="S5.T2.27.27.27.3.m3.3.3.3.3.3.3.cmml" xref="S5.T2.27.27.27.3.m3.3.3.3.3.3.3">&amp;</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.27.27.27.3.m3.3c">0.225\pm 0.055\text{\,}&amp;</annotation></semantics></math> <math id="S5.T2.28.28.28.4.m4.3" class="ltx_Math" alttext="0.352\pm 0.011\text{\,}\ " display="inline"><semantics id="S5.T2.28.28.28.4.m4.3a"><mrow id="S5.T2.28.28.28.4.m4.3.3" xref="S5.T2.28.28.28.4.m4.3.3.cmml"><mrow id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.cmml"><mn id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.1.1.1.1.1" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml">0.352</mn><mo id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.3" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.3.cmml">Â±</mo><mn id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.2.2.2.2.2" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml">0.011</mn></mrow><mtext id="S5.T2.28.28.28.4.m4.2.2.2.2.2.2" xref="S5.T2.28.28.28.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi id="S5.T2.28.28.28.4.m4.3.3.3.3.3.3" xref="S5.T2.28.28.28.4.m4.3.3.3.3.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.28.28.28.4.m4.3b"><apply id="S5.T2.28.28.28.4.m4.3.3.cmml" xref="S5.T2.28.28.28.4.m4.3.3"><csymbol cd="latexml" id="S5.T2.28.28.28.4.m4.2.2.2.2.2.2.cmml" xref="S5.T2.28.28.28.4.m4.2.2.2.2.2.2">times</csymbol><apply id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.cmml" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.3.cmml" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.3">uncertain</csymbol><cn type="float" id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.1.1.1.1.1">0.352</cn><cn type="float" id="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.T2.28.28.28.4.m4.1.1.1.1.1.1.2.2.2.2.2">0.011</cn></apply><csymbol cd="latexml" id="S5.T2.28.28.28.4.m4.3.3.3.3.3.3.cmml" xref="S5.T2.28.28.28.4.m4.3.3.3.3.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.28.28.28.4.m4.3c">0.352\pm 0.011\text{\,}\ </annotation></semantics></math>
</td>
<td id="S5.T2.28.28.28.7" class="ltx_td ltx_border_bb"></td>
<td id="S5.T2.28.28.28.8" class="ltx_td ltx_border_bb"></td>
<td id="S5.T2.28.28.28.9" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy of the five models as well as the Spearmanâ€™s rank correlation (<math id="S5.T2.32.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S5.T2.32.m1.1b"><mi id="S5.T2.32.m1.1.1" xref="S5.T2.32.m1.1.1.cmml">Ï</mi><annotation-xml encoding="MathML-Content" id="S5.T2.32.m1.1c"><ci id="S5.T2.32.m1.1.1.cmml" xref="S5.T2.32.m1.1.1">ğœŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.32.m1.1d">\rho</annotation></semantics></math>) and the Jensenâ€“Shannon divergence (<span id="S5.T2.36.1" class="ltx_text ltx_font_italic">JSD</span>) between neural and human attention over images (left) and text (right). Standard deviation was calculated over nine model runs and indicates the attention variability between different instances of the same architecture. All correlation and JSD scores between models differ significantly (p&lt;0.05), except for the image correlation between Pythia and BAN as well as the JSD text scores between BAN, MCAN<math id="S5.T2.33.m2.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.T2.33.m2.1b"><msub id="S5.T2.33.m2.1.1" xref="S5.T2.33.m2.1.1.cmml"><mi id="S5.T2.33.m2.1.1b" xref="S5.T2.33.m2.1.1.cmml"></mi><mtext id="S5.T2.33.m2.1.1.1" xref="S5.T2.33.m2.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.33.m2.1c"><apply id="S5.T2.33.m2.1.1.cmml" xref="S5.T2.33.m2.1.1"><ci id="S5.T2.33.m2.1.1.1a.cmml" xref="S5.T2.33.m2.1.1.1"><mtext mathsize="70%" id="S5.T2.33.m2.1.1.1.cmml" xref="S5.T2.33.m2.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.33.m2.1d">{}_{\text{G}}</annotation></semantics></math> and MCAN<math id="S5.T2.34.m3.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.T2.34.m3.1b"><msub id="S5.T2.34.m3.1.1" xref="S5.T2.34.m3.1.1.cmml"><mi id="S5.T2.34.m3.1.1b" xref="S5.T2.34.m3.1.1.cmml"></mi><mtext id="S5.T2.34.m3.1.1.1" xref="S5.T2.34.m3.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T2.34.m3.1c"><apply id="S5.T2.34.m3.1.1.cmml" xref="S5.T2.34.m3.1.1"><ci id="S5.T2.34.m3.1.1.1a.cmml" xref="S5.T2.34.m3.1.1.1"><mtext mathsize="70%" id="S5.T2.34.m3.1.1.1.cmml" xref="S5.T2.34.m3.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.34.m3.1d">{}_{\text{R}}</annotation></semantics></math></figcaption>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ordinal Logistic Regression</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">By averaging evaluation metrics (correlation and JSD) across documents, we obscure the impact that similarity has <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">on each document</span> with respect to accuracy. The Ordinal Logistic Regression model results uncover the importance of the text and image correlation scores as predictors on per document accuracy.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text Correlation.</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.3" class="ltx_p">We show (cf. TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.3 Discussion â€£ 5 Results â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) <span id="S5.SS2.SSS0.Px1.p1.3.1" class="ltx_text ltx_font_bold">for all five different VQA models</span>, that as the correlation to human text attention decreases, the likelihood that the models will be able to correctly predict the answer significantly decreases/ Our findings show that correlation to human text attention is a significant predictor on accuracy.
The MCAN<math id="S5.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS0.Px1.p1.1.m1.1.1a" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1"><ci id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1a.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>, MCAN<math id="S5.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.1a"><msub id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S5.SS2.SSS0.Px1.p1.2.m2.1.1a" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1"><ci id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1a.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.1c">{}_{\text{R}}</annotation></semantics></math>, and MFB model have the strongest relationship (<math id="S5.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.3.m3.1a"><mi id="S5.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.3.m3.1c">p</annotation></semantics></math> &lt; 0.001) to text correlation being a significant predictor on accuracy. This indicates that for these models in particular, the less the model is correlated with human text attention, the less likely the model will predict the answer correctly.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image Correlation.</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.3" class="ltx_p">Interestingly, we observe the same trend as text correlation, in which image attention correlation is also a significant predictor on accuracy, but <span id="S5.SS2.SSS0.Px2.p1.3.1" class="ltx_text ltx_font_bold">not consistently across all models</span>.
It is a significant predictor for three (MCAN<math id="S5.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S5.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.1.m1.1.1a" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1"><ci id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>, Pythia, and BAN) out of the five total models. Notably, the MCAN<math id="S5.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S5.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.2.m2.1.1a" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1"><ci id="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.2.m2.1c">{}_{\text{G}}</annotation></semantics></math> model has a significantly strong relationship to image correlation.
This indicates that when the Pythia, BAN, and in particular MCAN<math id="S5.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.3.m3.1a"><msub id="S5.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.3.m3.1.1a" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1"><ci id="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.3.m3.1c">{}_{\text{G}}</annotation></semantics></math> learn attention which is less correlated to human image attention, then the model is more less likely to be able to predict the answer correctly.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Inter-Modal Correlation.</h4>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p1.6" class="ltx_p">We paired the text correlation <math id="S5.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.1.m1.1c">x</annotation></semantics></math> and the image correlation <math id="S5.SS2.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.2.m2.1a"><mi id="S5.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S5.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.2.m2.1c">y</annotation></semantics></math> together as an inter-modal predictor <math id="S5.SS2.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.3.m3.1a"><mi id="S5.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px3.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.3.m3.1b"><ci id="S5.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.3.m3.1c">z</annotation></semantics></math>. Inter-modal correlation tests whether the interaction between the two correlation scores, as the predictor <math id="S5.SS2.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.4.m4.1a"><mi id="S5.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S5.SS2.SSS0.Px3.p1.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.4.m4.1b"><ci id="S5.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.4.m4.1c">z</annotation></semantics></math>, has an effect on accuracy.
Interestingly, inter-modal correlation <math id="S5.SS2.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.5.m5.1a"><mi id="S5.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S5.SS2.SSS0.Px3.p1.5.m5.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.5.m5.1b"><ci id="S5.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.5.m5.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.5.m5.1c">z</annotation></semantics></math> is a significant predictor on accuracy for the MCAN<math id="S5.SS2.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.SS2.SSS0.Px3.p1.6.m6.1a"><msub id="S5.SS2.SSS0.Px3.p1.6.m6.1.1" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1.cmml"><mi id="S5.SS2.SSS0.Px3.p1.6.m6.1.1a" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1.cmml"></mi><mtext id="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px3.p1.6.m6.1b"><apply id="S5.SS2.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1"><ci id="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1a.cmml" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1.cmml" xref="S5.SS2.SSS0.Px3.p1.6.m6.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px3.p1.6.m6.1c">{}_{\text{G}}</annotation></semantics></math> and Pythia models but not for the other 3 model types.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attention Maps â€“ Qualitative Analysis.</h4>

<div id="S5.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px4.p1.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 â€£ 4.2 Extracting Model Attention â€£ 4 Comparison of Human and Machine Attention â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> visualizes the human as well as neural attention distributions of five VQA models for a selection of examples from the benchmark VQAv2 dataset.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>See Appendix <a href="#A7" title="Appendix G More Model Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a> for additional examples.</span></span></span>
As can be seen, all previous datasets only uncover the differences between human and neural image attention, while VQA-MHUG (ours) allows for studying multimodal neural VQA models attention.
We also find our attention maps to be highly relevant and confirm that the mouse tracking datasets SALICON and VQA-HAT seem to over-estimate relevant areas.
As the AiR-D dataset does not overlap with VQAv2, we separately visualize a selection of examples (see SectionÂ <a href="#S3.SS0.SSS0.Px7" title="Comparison to Related Datasets. â€£ 3 The VQA-MHUG Dataset â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> overlapping with our VQA-MHUG data (see Appendix <a href="#A6" title="Appendix F MHUG vs. AiR-D Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>).</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Discussion</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">When averaging metrics across all documents in VQA-MHUG, our results regarding similarity between machine and human image attention and performance follow insights derived from previous workÂ <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>, where they observed that as the models improved with respect to accuracy they were also more correlated to human attention on the images. However, notably we only observe this trend with the models which use region features. That is, though the MCAN grid is the highest performing model with respect to accuracy, it is also the model which is least similar to human image attention. Such an observation was also reported in previous work which compared the XLNet transformer to human attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Sood etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Analysis from the Ordinal Logistic Regression model shows, for the first time, that correlation to human text attention is a significant predictor across all VQA model types, where dissimilarity between human and neural text attention decreases the likelihood of the models ability to predict the answer correctly.
We conclude that striving to enhance neural attention to more similarly emulate human attention on text will improve performance in the five VQA models.
As can be observed in Figure <a href="#S4.F2" title="Figure 2 â€£ 4.2 Extracting Model Attention â€£ 4 Comparison of Human and Machine Attention â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, text attention is not always human-like, especially for the otherwise high performing MCAN models, suggesting that increased similarity to human text attention might lead to further improvements with respect to accuracy.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Due to the lack of human attention data over text, researchers were not able to uncover the limitations or relevance of high correlation to human text attention on VQA model accuracy.
In addition, our analysis on the role of image attention and inter-modal attention as a predictor on accuracy indicates that for certain model types it would be beneficial to improve image and inter-modal correlation.
These findings are consistent withÂ <cite class="ltx_cite ltx_citemacro_citet">Sood etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2020b</a>)</cite> which found that different model types learn different attention strategies and similarity of machine to human attention does not guarantee best performance. This may be due to factors such as features used (grid versus region), the different learned attention strategies across model types and how the architectures model the interactions between the multimodal input features. For example, the MCAN grid model applies self- and guided attention to model the interplay between grid-based image and text feature representations. On the other hand, the Pythia model uses both bottom up attention (image features extracted on the region level) and top down attention (text attention applied over the images), where the text attention weights are not learned by the image feature representations.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.3.1" class="ltx_tr">
<th id="S5.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.2.3.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T3.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.3.1.2.1" class="ltx_text ltx_font_bold">Text</span></th>
<th id="S5.T3.2.3.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.3.1.3.1" class="ltx_text ltx_font_bold">Image</span></th>
<th id="S5.T3.2.3.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.2.3.1.4.1" class="ltx_text ltx_font_bold">Inter-Modal</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MCAN<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.m1.1.1a" xref="S5.T3.1.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.m1.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"><ci id="S5.T3.1.1.1.m1.1.1.1a.cmml" xref="S5.T3.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">{}_{\text{G}}</annotation></semantics></math>
</th>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_t">-4.60***</td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_left ltx_border_t">-8.32***</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-8.33***</td>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<th id="S5.T3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCAN<math id="S5.T3.2.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{R}}" display="inline"><semantics id="S5.T3.2.2.1.m1.1a"><msub id="S5.T3.2.2.1.m1.1.1" xref="S5.T3.2.2.1.m1.1.1.cmml"><mi id="S5.T3.2.2.1.m1.1.1a" xref="S5.T3.2.2.1.m1.1.1.cmml"></mi><mtext id="S5.T3.2.2.1.m1.1.1.1" xref="S5.T3.2.2.1.m1.1.1.1a.cmml">R</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.m1.1b"><apply id="S5.T3.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1"><ci id="S5.T3.2.2.1.m1.1.1.1a.cmml" xref="S5.T3.2.2.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.2.2.1.m1.1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1.1">R</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.m1.1c">{}_{\text{R}}</annotation></semantics></math>
</th>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_left">-5.50***</td>
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_left">0.21</td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_nopad_r ltx_align_left">0.05</td>
</tr>
<tr id="S5.T3.2.4.1" class="ltx_tr">
<th id="S5.T3.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pythia</th>
<td id="S5.T3.2.4.1.2" class="ltx_td ltx_align_left">-2.83**</td>
<td id="S5.T3.2.4.1.3" class="ltx_td ltx_align_left">-1.83*</td>
<td id="S5.T3.2.4.1.4" class="ltx_td ltx_nopad_r ltx_align_left">-1.81*</td>
</tr>
<tr id="S5.T3.2.5.2" class="ltx_tr">
<th id="S5.T3.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BAN</th>
<td id="S5.T3.2.5.2.2" class="ltx_td ltx_align_left">-2.20*</td>
<td id="S5.T3.2.5.2.3" class="ltx_td ltx_align_left">-3.62***</td>
<td id="S5.T3.2.5.2.4" class="ltx_td ltx_nopad_r ltx_align_left">0.53</td>
</tr>
<tr id="S5.T3.2.6.3" class="ltx_tr">
<th id="S5.T3.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MFB</th>
<td id="S5.T3.2.6.3.2" class="ltx_td ltx_align_left ltx_border_bb">-3.32***</td>
<td id="S5.T3.2.6.3.3" class="ltx_td ltx_align_left ltx_border_bb">-0.05</td>
<td id="S5.T3.2.6.3.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">-0.208</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ordinal Logistic regression model t-values
such that for every one unit decrease in correlation, each respective model is less likely to predict the answer correctly. Significance is denoted as *<math id="S5.T3.8.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.T3.8.m1.1b"><mrow id="S5.T3.8.m1.1.1" xref="S5.T3.8.m1.1.1.cmml"><mi id="S5.T3.8.m1.1.1.2" xref="S5.T3.8.m1.1.1.2.cmml">p</mi><mo id="S5.T3.8.m1.1.1.1" xref="S5.T3.8.m1.1.1.1.cmml">&lt;</mo><mn id="S5.T3.8.m1.1.1.3" xref="S5.T3.8.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.8.m1.1c"><apply id="S5.T3.8.m1.1.1.cmml" xref="S5.T3.8.m1.1.1"><lt id="S5.T3.8.m1.1.1.1.cmml" xref="S5.T3.8.m1.1.1.1"></lt><ci id="S5.T3.8.m1.1.1.2.cmml" xref="S5.T3.8.m1.1.1.2">ğ‘</ci><cn type="float" id="S5.T3.8.m1.1.1.3.cmml" xref="S5.T3.8.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.m1.1d">p&lt;0.05</annotation></semantics></math>, **<math id="S5.T3.9.m2.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.T3.9.m2.1b"><mrow id="S5.T3.9.m2.1.1" xref="S5.T3.9.m2.1.1.cmml"><mi id="S5.T3.9.m2.1.1.2" xref="S5.T3.9.m2.1.1.2.cmml">p</mi><mo id="S5.T3.9.m2.1.1.1" xref="S5.T3.9.m2.1.1.1.cmml">&lt;</mo><mn id="S5.T3.9.m2.1.1.3" xref="S5.T3.9.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.9.m2.1c"><apply id="S5.T3.9.m2.1.1.cmml" xref="S5.T3.9.m2.1.1"><lt id="S5.T3.9.m2.1.1.1.cmml" xref="S5.T3.9.m2.1.1.1"></lt><ci id="S5.T3.9.m2.1.1.2.cmml" xref="S5.T3.9.m2.1.1.2">ğ‘</ci><cn type="float" id="S5.T3.9.m2.1.1.3.cmml" xref="S5.T3.9.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.m2.1d">p&lt;0.01</annotation></semantics></math>, ***<math id="S5.T3.10.m3.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S5.T3.10.m3.1b"><mrow id="S5.T3.10.m3.1.1" xref="S5.T3.10.m3.1.1.cmml"><mi id="S5.T3.10.m3.1.1.2" xref="S5.T3.10.m3.1.1.2.cmml">p</mi><mo id="S5.T3.10.m3.1.1.1" xref="S5.T3.10.m3.1.1.1.cmml">&lt;</mo><mn id="S5.T3.10.m3.1.1.3" xref="S5.T3.10.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.10.m3.1c"><apply id="S5.T3.10.m3.1.1.cmml" xref="S5.T3.10.m3.1.1"><lt id="S5.T3.10.m3.1.1.1.cmml" xref="S5.T3.10.m3.1.1.1"></lt><ci id="S5.T3.10.m3.1.1.2.cmml" xref="S5.T3.10.m3.1.1.2">ğ‘</ci><cn type="float" id="S5.T3.10.m3.1.1.3.cmml" xref="S5.T3.10.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.m3.1d">p&lt;0.001</annotation></semantics></math>.
Correlation to human text attention is a significant predictor of accuracy for <span id="S5.T3.14.1" class="ltx_text ltx_font_bold">all five models</span>. Correlation to human image attention is an important for the accuracy of MCAN<math id="S5.T3.11.m4.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.T3.11.m4.1b"><msub id="S5.T3.11.m4.1.1" xref="S5.T3.11.m4.1.1.cmml"><mi id="S5.T3.11.m4.1.1b" xref="S5.T3.11.m4.1.1.cmml"></mi><mtext id="S5.T3.11.m4.1.1.1" xref="S5.T3.11.m4.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.11.m4.1c"><apply id="S5.T3.11.m4.1.1.cmml" xref="S5.T3.11.m4.1.1"><ci id="S5.T3.11.m4.1.1.1a.cmml" xref="S5.T3.11.m4.1.1.1"><mtext mathsize="70%" id="S5.T3.11.m4.1.1.1.cmml" xref="S5.T3.11.m4.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.m4.1d">{}_{\text{G}}</annotation></semantics></math>, Pythia, and BAN
while inter-modal correlation
is a significant predictor of accuracy for both MCAN<math id="S5.T3.12.m5.1" class="ltx_Math" alttext="{}_{\text{G}}" display="inline"><semantics id="S5.T3.12.m5.1b"><msub id="S5.T3.12.m5.1.1" xref="S5.T3.12.m5.1.1.cmml"><mi id="S5.T3.12.m5.1.1b" xref="S5.T3.12.m5.1.1.cmml"></mi><mtext id="S5.T3.12.m5.1.1.1" xref="S5.T3.12.m5.1.1.1a.cmml">G</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.12.m5.1c"><apply id="S5.T3.12.m5.1.1.cmml" xref="S5.T3.12.m5.1.1"><ci id="S5.T3.12.m5.1.1.1a.cmml" xref="S5.T3.12.m5.1.1.1"><mtext mathsize="70%" id="S5.T3.12.m5.1.1.1.cmml" xref="S5.T3.12.m5.1.1.1">G</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.m5.1d">{}_{\text{G}}</annotation></semantics></math> and Pythia.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work we have presented VQA-MHUG â€“ a new, fully annotated 49-participant dataset for visual question answering that includes nearly 4,000 question-answer pairs.
Our dataset is unique in that it is the first to provide real human gaze data on both images and corresponding questions and, as such, allows researchers to jointly study human and machine attention.
Revealed through a detailed comparison of multiple leading VQA models, we showed that higher correlation between neural and human text attention is a significant predictor of high VQA performance.
This novel finding highlights the potential to improve VQA performance with human-like attention biases and simultaneously calls for further investigation of neural text attention mechanisms, as we find these are an indicator for success on language and vision tasks, including VQA.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Statement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We identified a number of potential benefits and risks of our approach.</p>
</div>
<section id="Sx1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Potential benefits</h4>

<div id="Sx1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px1.p1.1" class="ltx_p">By leveraging human behavioral data, our method could be used to guide intelligent user interfaces using human attentive abilities within the context of reading behaviors.
We see significant potential of approach to interpret text attention to enable a new generation of attentive text interfaces, particularly when jointly modelling with user task specific eye movement behaviors during comprehension tasks. We see potential for e-learning multimodal applications approach could be used to qualify reader actions and provide feedback to encourage improvement in comprehension. By bridging the gap between human and neural attention, we see a potential positive impact in improving attention strategies in users.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Potential risks</h4>

<div id="Sx1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px2.p1.1" class="ltx_p">Though we see the aforementioned potential benefits, we also identified a some risks and ethical concerns.
By aiming to interpret the gap between human and machine attention, we open the door for potentially exploiting user biases. In addition, one can conceive that there is potential for using the findings of our work to develop tool which discriminate against specific users given their eye movement behaviors. This leads to the discussion
about the behavioral data collection, it is conceivable that one could generate a system which might predicts cognitive impairments in order to filter out individuals from some program or opportunity.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset Curation</h4>

<div id="Sx1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px3.p1.1" class="ltx_p">To protect the privacy of our participants we saved all data anonymized and collected only directly relevant data and demographic information in compliance with our universityâ€™s code of ethics and the General Data Protection Regulation (GDPR) of the European Union (EU). Our study was approved by the ethics committee (institutional review board) of the university. Additional measures for safety during the COVID-19 pandemic were taken with disinfection of the material, obligatory masks and breaks between scheduled recording sessions. All participants signed a consent form that included details about the purpose, goal, procedure, risks, benefits and privacy measures of our research. For the 45-60 minute study an above average compensation of 20â‚¬ was paid. At any point the participant could abort the study without penalty. The study took place in a standard university lab and the participantâ€™s head was not fixed. Every 15 minutes a 5 minute break was scheduled.</p>
</div>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">E. Sood was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanyâ€™s Excellence Strategy - EXC 2075 â€“ 390740016;
F. Strohm and A. Bulling were funded by the European Research Council (ERC; grant agreement 801708);
<br class="ltx_break">We would like to especially thank Simon Tannert for his valuable insights and support, as well as Dr. Philipp MÃ¼ller and Dr. Paul BÃ¼rkner for their helpful suggestions. Lastly, we would like to thank the anonymous reviewers for their useful feedback.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal (2015)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal. 2015.

</span>
<span class="ltx_bibblock">Visualqa official evaluation code.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/GT-Vision-Lab/VQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/GT-Vision-Lab/VQA</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2016)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1203" title="" class="ltx_ref ltx_href">Analyzing the behavior
of visual question answering models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1955â€“1960, Austin, Texas. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2018a)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
2018a.

</span>
<span class="ltx_bibblock">Donâ€™t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 4971â€“4980.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2018b)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
2018b.

</span>
<span class="ltx_bibblock">Donâ€™t just assume; look and answer: Overcoming priors for visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077â€“6086.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al. (2015)</span>
<span class="ltx_bibblock">
S.Â Antol, A.Â Agrawal, J.Â Lu, M.Â Mitchell, D.Â Batra, C.Â L. Zitnick, and
D.Â Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href">Vqa: Visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision
(ICCV)</em>, pages 2425â€“2433.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau etÂ al. (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.0473</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bewick etÂ al. (2003)</span>
<span class="ltx_bibblock">
Viv Bewick, Liz Cheek, and J.Â Ball. 2003.

</span>
<span class="ltx_bibblock">Statistics review 7: Correlation and regression.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Critical Care</em>, 7:451 â€“ 459.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Shi Chen, Ming Jiang, Jinhui Yang, and QiÂ Zhao. 2020.

</span>
<span class="ltx_bibblock">Air: Attention with reasoning capability.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 91â€“107.
Springer.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Correia and Colombini (2021)</span>
<span class="ltx_bibblock">
A.Â S. Correia and E.Â Colombini. 2021.

</span>
<span class="ltx_bibblock">Attention, please! a survey of neural attention models in deep
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2103.16775.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al. (2016)</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1092" title="" class="ltx_ref ltx_href">Human attention in
visual question answering: Do humans and deep networks look at the same
regions?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 932â€“937, Austin, Texas. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fosco etÂ al. (2020)</span>
<span class="ltx_bibblock">
Camilo Fosco, Anelise Newman, Pat Sukhum, YunÂ Bin Zhang, Nanxuan Zhao, Aude
Oliva, and Zoya Bylinskii. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.00453" title="" class="ltx_ref ltx_href">How Much
Time Do You Have? Modeling Multi-Duration Saliency</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, volumeÂ 1, pages
4473â€“4482.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan etÂ al. (2017)</span>
<span class="ltx_bibblock">
Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_iccv_2017/html/Gan_VQS_Linking_Segmentations_ICCV_2017_paper.html" title="" class="ltx_ref ltx_href">Vqs: Linking segmentations to questions and answers for supervised attention
in vqa and question-focused semantic segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 1811â€“1820.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017a)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html" title="" class="ltx_ref ltx_href">Making the v in vqa matter: Elevating the role of image understanding in
visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6904â€“6913.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017b)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017b.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6904â€“6913.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sen He, HamedÂ Rezazadegan Tavakoli, Ali Borji, and Nicolas Pugeault. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2019.00862" title="" class="ltx_ref ltx_href">Human attention in
image captioning: Dataset and analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Computer Vision</em>, pages
8528â€“8537, Piscataway, NJ. IEEE.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooge etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ignace T.Â C. Hooge, GijsÂ A. Holleman, NinaÂ C. Haukes, and RoyÂ S. Hessels. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3758/s13428-018-1135-3" title="" class="ltx_ref ltx_href">Gaze tracking
accuracy in humans: One eye is sometimes better than two</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Behavior Research Methods</em>, 51(6):2712â€“2721.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri etÂ al. (2016)</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens van der Maaten. 2016.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2016</em>, Lecture Notes in Computer
Science, pages 727â€“739, Cham and s.l. Springer International Publishing.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain and Wallace (2019)</span>
<span class="ltx_bibblock">
Sarthak Jain and ByronÂ C Wallace. 2019.

</span>
<span class="ltx_bibblock">Attention is not explanation.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.10186</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
Chen. 2020.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10267â€“10276.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2015)</span>
<span class="ltx_bibblock">
Ming Jiang, Shengsheng Huang, Juanyong Duan, and QiÂ Zhao. 2015.

</span>
<span class="ltx_bibblock">Salicon: Saliency in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2018)</span>
<span class="ltx_bibblock">
YuÂ Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh. 2018.

</span>
<span class="ltx_bibblock">Pythia v0. 1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09956</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_iccv_2017/html/Kafle_An_Analysis_of_ICCV_2017_paper.html" title="" class="ltx_ref ltx_href">An analysis of visual question answering algorithms</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 1965â€“1973.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kushal Kafle, Robik Shrestha, and Christopher Kanan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3389/frai.2019.00028" title="" class="ltx_ref ltx_href">Challenges and
prospects in vision and language research</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Frontiers in Artificial Intelligence</em>, 2:28.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan etÂ al. (2020)</span>
<span class="ltx_bibblock">
AishaÂ Urooj Khan, Amir Mazaheri, Niels DaÂ Vitoria Lobo, and Mubarak Shah. 2020.

</span>
<span class="ltx_bibblock">Mmft-bert: Multimodal fusion transformer with bert encodings for
visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.14095</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on Neural
Information Processing Systems</em>, pages 1571â€“1581.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2016)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang. 2016.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual qa.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01455</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, DavidÂ A. Shamma,
MichaelÂ S. Bernstein, and Li Fei-Fei. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s11263-016-0981-7" title="" class="ltx_ref ltx_href">Visual genome:
Connecting language and vision using crowdsourced dense image annotations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):32â€“73.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2019)</span>
<span class="ltx_bibblock">
Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan
He, and Chuang Gan. 2019.

</span>
<span class="ltx_bibblock">Beyond rnns: Positional self-attention with co-attention for video
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 33, pages 8658â€“8665.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.02265</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/1606.00061" title="" class="ltx_ref ltx_href">Hierarchical question-image
co-attention for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 29 (NIPS
2016)</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
27:1682â€“1690.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski etÂ al. (2015)</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1â€“9.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Okatani (2018)</span>
<span class="ltx_bibblock">
Duy-Kien Nguyen and Takayuki Okatani. 2018.

</span>
<span class="ltx_bibblock">Improved fusion of visual and language representations by dense
symmetric co-attention for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6087â€“6096.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">P. Zhang etÂ al. (2015)</span>
<span class="ltx_bibblock">
P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.542" title="" class="ltx_ref ltx_href">Yin and yang:
Balancing and answering binary visual questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 5014â€“5022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al. (2018)</span>
<span class="ltx_bibblock">
DongÂ Huk Park, LisaÂ Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele,
Trevor Darrell, and Marcus Rohrbach. 2018.

</span>
<span class="ltx_bibblock">Multimodal explanations: Justifying decisions and pointing to the
evidence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington etÂ al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/D14-1162" title="" class="ltx_ref ltx_href">GloVe: Global
vectors for word representation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1532â€“1543, Doha, Qatar.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao etÂ al. (2018)</span>
<span class="ltx_bibblock">
Tingting Qiao, Jianfeng Dong, and Duanqing Xu. 2018.

</span>
<span class="ltx_bibblock">Exploring human-like attention supervision in visual question
answering: Hlat.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Thirty-Second AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiuxia etÂ al. (2020)</span>
<span class="ltx_bibblock">
LAI Qiuxia, Salman Khan, Yongwei Nie, Sun Hanqiu, Jianbing Shen, and Ling Shao.
2020.

</span>
<span class="ltx_bibblock">Understanding more about human and machine attention in deep neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju etÂ al. (2020a)</span>
<span class="ltx_bibblock">
RamprasaathÂ R Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,
MarcoÂ Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020a.

</span>
<span class="ltx_bibblock">Squinting at vqa models: Introspecting vqa models with sub-questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10003â€“10011.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju etÂ al. (2020b)</span>
<span class="ltx_bibblock">
RamprasaathÂ R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,
MarcoÂ Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020b.

</span>
<span class="ltx_bibblock">Squinting at vqa models: Introspecting vqa models with sub-questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serrano and Smith (2019)</span>
<span class="ltx_bibblock">
Sofia Serrano and NoahÂ A Smith. 2019.

</span>
<span class="ltx_bibblock">Is attention interpretable?

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.03731</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah etÂ al. (2019a)</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019a.

</span>
<span class="ltx_bibblock">Cycle-consistency for robust visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6649â€“6658.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah etÂ al. (2019b)</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Shah_Cycle-Consistency_for_Robust_Visual_Question_Answering_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">Cycle-consistency for robust visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 6649â€“6658.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sood etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling, and NgocÂ Thang
Vu. 2020a.

</span>
<span class="ltx_bibblock">Interpreting attention models with human visual attention in machine
reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proc. ACL SIGNLL Conference on Computational Natural
Language Learning (CoNLL)</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sood etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Ekta Sood, Simon Tannert, Philipp MÃ¼ller, and Andreas Bulling.
2020b.

</span>
<span class="ltx_bibblock">Improving natural language processing tasks with human gaze-guided
neural attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sugano and Bulling (2016)</span>
<span class="ltx_bibblock">
Yusuke Sugano and Andreas Bulling. 2016.

</span>
<span class="ltx_bibblock">Seeing with humans: Gaze-assisted neural image captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1608.05203</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tatler (2007)</span>
<span class="ltx_bibblock">
B.Â Tatler. 2007.

</span>
<span class="ltx_bibblock">The central fixation bias in scene viewing: selecting an optimal
viewing position independently of motor biases and image feature
distributions.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Journal of vision</em>, 7 14:4.1â€“17.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tavakoli etÂ al. (2017a)</span>
<span class="ltx_bibblock">
HamedÂ R. Tavakoli, Fawad Ahmed, Ali Borji, and Jorma Laaksonen.
2017a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Tavakoli_Saliency_Revisited_Analysis_CVPR_2017_paper.html" title="" class="ltx_ref ltx_href">Saliency revisited: Analysis of mouse movements versus fixations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 1774â€“1782.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tavakoli etÂ al. (2017b)</span>
<span class="ltx_bibblock">
HamedÂ R. Tavakoli, Rakshith Shetty, Ali Borji, and Jorma Laaksonen.
2017b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_iccv_2017/html/Tavakoli_Paying_Attention_to_ICCV_2017_paper.html" title="" class="ltx_ref ltx_href">Paying attention to descriptions generated by image captioning models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 2487â€“2496.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st International Conference on Neural
Information Processing Systems</em>, NIPSâ€™17, page 6000â€“6010, Red Hook, NY,
USA. Curran Associates Inc.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2017)</span>
<span class="ltx_bibblock">
Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. 2017.

</span>
<span class="ltx_bibblock">Aggregated residual transformations for deep neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 1492â€“1500.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2015)</span>
<span class="ltx_bibblock">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
2048â€“2057. PMLR.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019a)</span>
<span class="ltx_bibblock">
Chao Yang, Mengqi Jiang, Bin Jiang, Weixin Zhou, and Keqin Li.
2019a.

</span>
<span class="ltx_bibblock">Co-attention network with question type for visual question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 7:40771â€“40781.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019b)</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
and QuocÂ V Le. 2019b.

</span>
<span class="ltx_bibblock">Xlnet: Generalized autoregressive pretraining for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.08237</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, LiÂ Deng, and Alex Smola.
2016a.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 21â€“29.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy.
2016b.

</span>
<span class="ltx_bibblock">Hierarchical attention networks for document classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 conference of the North American
chapter of the association for computational linguistics: human language
technologies</em>, pages 1480â€“1489.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2019a)</span>
<span class="ltx_bibblock">
Zhou Yu, Yuhao Cui, Zhenwei Shao, Pengbing Gao, and Jun Yu. 2019a.

</span>
<span class="ltx_bibblock">Openvqa.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/MILVLG/openvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/openvqa</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2019b)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and QiÂ Tian. 2019b.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6281â€“6290.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2017)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017.

</span>
<span class="ltx_bibblock">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1821â€“1830.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yun etÂ al. (2015)</span>
<span class="ltx_bibblock">
K.Â Yun, Y.Â Peng, D.Â Samaras, G.Â J. Zelinsky, and T.Â L. Berg. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2013.101" title="" class="ltx_ref ltx_href">Studying relationships
between human gaze, description, and computer vision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 739â€“746.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yun etÂ al. (2013)</span>
<span class="ltx_bibblock">
Kiwon Yun, Yifan Peng, Dimitris Samaras, GregoryÂ J Zelinsky, and TamaraÂ L Berg.
2013.

</span>
<span class="ltx_bibblock">Studying relationships between human gaze, description, and computer
vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conference on Computer Vision and Pattern
Recognition</em>, pages 739â€“746.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yan Zhang, JonathonÂ S. Hare, and Adam PrÃ¼gel-Bennett. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=B12Js_yRb" title="" class="ltx_ref ltx_href">Learning to count
objects in natural images for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings</em>. OpenReview.net.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>VQA-MHUG Overlap to Related Datasets</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">During the selection of stimuli for VQA-MHUG, we maintained large overlaps with other benchmark and attention datasets that also used subsets of VQAv2 questions/images to allow for easy integration and comparison of our data with existing approaches (see Table <a href="#A1.T4" title="Table 4 â€£ Appendix A VQA-MHUG Overlap to Related Datasets â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="A1.T4" class="ltx_table">
<table id="A1.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.1.1.1" class="ltx_tr">
<th id="A1.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="A1.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="A1.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">âˆ©</span></td>
</tr>
<tr id="A1.T4.1.2.2" class="ltx_tr">
<th id="A1.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQAv2 val <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2017a</a>)</cite>
</th>
<td id="A1.T4.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">3,990</td>
</tr>
<tr id="A1.T4.1.3.3" class="ltx_tr">
<th id="A1.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VG <cite class="ltx_cite ltx_citemacro_cite">Krishna etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="A1.T4.1.3.3.2" class="ltx_td ltx_align_left">2,238</td>
</tr>
<tr id="A1.T4.1.4.4" class="ltx_tr">
<th id="A1.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-CP2 <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite>
</th>
<td id="A1.T4.1.4.4.2" class="ltx_td ltx_align_left">1,904</td>
</tr>
<tr id="A1.T4.1.5.5" class="ltx_tr">
<th id="A1.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-Rephrasings <cite class="ltx_cite ltx_citemacro_cite">Shah etÂ al. (<a href="#bib.bib45" title="" class="ltx_ref">2019b</a>)</cite>
</th>
<td id="A1.T4.1.5.5.2" class="ltx_td ltx_align_left">1,373</td>
</tr>
<tr id="A1.T4.1.6.6" class="ltx_tr">
<th id="A1.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-Introspect <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2020b</a>)</cite>
</th>
<td id="A1.T4.1.6.6.2" class="ltx_td ltx_align_left">1,213</td>
</tr>
<tr id="A1.T4.1.7.7" class="ltx_tr">
<th id="A1.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SALICON <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite>
</th>
<td id="A1.T4.1.7.7.2" class="ltx_td ltx_align_left">1,134</td>
</tr>
<tr id="A1.T4.1.8.8" class="ltx_tr">
<th id="A1.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TDIUC <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="A1.T4.1.8.8.2" class="ltx_td ltx_align_left">1,125</td>
</tr>
<tr id="A1.T4.1.9.9" class="ltx_tr">
<th id="A1.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQS <cite class="ltx_cite ltx_citemacro_cite">Gan etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="A1.T4.1.9.9.2" class="ltx_td ltx_align_left">695</td>
</tr>
<tr id="A1.T4.1.10.10" class="ltx_tr">
<th id="A1.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VQA-X <cite class="ltx_cite ltx_citemacro_cite">Park etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite>
</th>
<td id="A1.T4.1.10.10.2" class="ltx_td ltx_align_left">491</td>
</tr>
<tr id="A1.T4.1.11.11" class="ltx_tr">
<th id="A1.T4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">VQA-HAT <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>
</th>
<td id="A1.T4.1.11.11.2" class="ltx_td ltx_align_left ltx_border_bb">410</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Overlap of VQA-MHUG question-image pairs with different established VQA related datasets.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Reasoning Types</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We binned question-image pairs by 12 reasoning types, as they align better with potential error classes than the VQAv2 question types. Figure <a href="#A2.F4" title="Figure 4 â€£ B.1 Tagger â€£ Appendix B Reasoning Types â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the relationship of reasoning types to question types. The reasoning types incorporate the categories proposed by <cite class="ltx_cite ltx_citemacro_citet">Kafle and Kanan (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, except the absurd category and adding a new <span id="A2.p1.1.1" class="ltx_text ltx_font_italic">reading</span> category for questions that ask about text on the images.</p>
</div>
<div id="A2.p2" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p">Scene Recognition</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p">Object Presence</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p">Colour</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p">Positional Reasoning</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p">Counting</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p">Utility Affordance</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p">Object Recognition</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i8.p1" class="ltx_para">
<p id="A2.I1.i8.p1.1" class="ltx_p">Activity Recognition</p>
</div>
</li>
<li id="A2.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i9.p1" class="ltx_para">
<p id="A2.I1.i9.p1.1" class="ltx_p">Attribute</p>
</div>
</li>
<li id="A2.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i10.p1" class="ltx_para">
<p id="A2.I1.i10.p1.1" class="ltx_p">Reading</p>
</div>
</li>
<li id="A2.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i11.p1" class="ltx_para">
<p id="A2.I1.i11.p1.1" class="ltx_p">Sentiment Understanding</p>
</div>
</li>
<li id="A2.I1.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i12.p1" class="ltx_para">
<p id="A2.I1.i12.p1.1" class="ltx_p">Sport Recognition</p>
</div>
</li>
</ul>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Tagger</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">To label VQA-MHUG with our reasoning types we used a LSTM-based classifier to predict the reasoning type given the question-answer pair.
The input text is encoded using 300D glove embeddingsÂ <cite class="ltx_cite ltx_citemacro_cite">Pennington etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2014</a>)</cite>, which are passed though a single LSTM layer with hidden size 256 and a final softmax classification layer.
We labeled 145â€‰K VQAv2 train-val questions and extended the 1.6â€‰M TDIUC questions by the <span id="A2.SS1.p1.1.1" class="ltx_text ltx_font_italic">reading</span> category using regular expressions and manual work.
We trained the network using this data by optimizing cross-entropy loss with the Adam optimizer and a batch size of 128.
The final model achieves an accuracy of 99.67% on a held-out set of 20% of the training data.
The trained tagger was then used to label the question-image pairs in VQA-MHUG.
Figure <a href="#A2.F3" title="Figure 3 â€£ B.1 Tagger â€£ Appendix B Reasoning Types â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the label distribution.</p>
</div>
<figure id="A2.F3" class="ltx_figure"><img src="/html/2109.13116/assets/figures/rtypes_in_mhug.png" id="A2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Final distribution of tagged reasoning types in VQA-MHUG. When no other type fit, the tagger assigned <span id="A2.F3.2.1" class="ltx_text ltx_font_italic">utility affordance</span>, which had the least training data. This indicates that there could be clusters that do not fit any current type.</figcaption>
</figure>
<figure id="A2.F4" class="ltx_figure"><img src="/html/2109.13116/assets/figures/rtypes_qtypes.png" id="A2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Relationship of our VQA-MHUG reasoning types (left) and VQAv2 question types
(right). Question types are not good categories for error case analysis since they mix many
reasoning capabilities.</figcaption>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Machine Difficulty Score</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">For our machine difficulty score we evaluated the Multimodal Factorized Bilinear Pooling Model (MFB) <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> for multimodal fusion and the Multimodal Co-Attention Network (MCAN) <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2019b</a>)</cite> for transformer attention on four datasets (VQAv2, VQA-CPv2, VQA-Introspect and VQA-Rephrasings).
The standard VQAv2 and VQA-CPv2 use simple accuracy, but VQA-CPv2 has intentionally dissimilar answer distributions in the train and validation splits to not allow exploitation of priors.
In general, low accuracy on both model architectures indicates a harder question (Equation <a href="#A3.E1" title="In Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="A3.p2" class="ltx_para">
<table id="A3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.E1.m1.24" class="ltx_Math" alttext="\begin{split}&amp;\text{score}_{\text{VQA}}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{ans}})=\\
&amp;\text{score}_{\text{VQA-CP}}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{ans}})=\\
&amp;1-(\min(\frac{\text{\# of annotators that said {\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}ans}}}{3},1))\end{split}" display="block"><semantics id="A3.E1.m1.24a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="A3.E1.m1.24.24.2" xref="A3.E1.m1.23.23.1.cmml"><mtr id="A3.E1.m1.24.24.2a" xref="A3.E1.m1.23.23.1.cmml"><mtd id="A3.E1.m1.24.24.2b" xref="A3.E1.m1.23.23.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E1.m1.24.24.2c" xref="A3.E1.m1.23.23.1.cmml"><mrow id="A3.E1.m1.6.6.6.6.6" xref="A3.E1.m1.23.23.1.cmml"><mrow id="A3.E1.m1.6.6.6.6.6.7" xref="A3.E1.m1.23.23.1.cmml"><msub id="A3.E1.m1.6.6.6.6.6.7.2" xref="A3.E1.m1.23.23.1.cmml"><mtext id="A3.E1.m1.1.1.1.1.1.1" xref="A3.E1.m1.1.1.1.1.1.1a.cmml">score</mtext><mtext id="A3.E1.m1.2.2.2.2.2.2.1" xref="A3.E1.m1.2.2.2.2.2.2.1a.cmml">VQA</mtext></msub><mo lspace="0em" rspace="0em" id="A3.E1.m1.6.6.6.6.6.7.1" xref="A3.E1.m1.23.23.1a.cmml">â€‹</mo><mrow id="A3.E1.m1.6.6.6.6.6.7.3" xref="A3.E1.m1.23.23.1.cmml"><mo stretchy="false" id="A3.E1.m1.3.3.3.3.3.3" xref="A3.E1.m1.23.23.1a.cmml">(</mo><mtext mathcolor="#FF0000" id="A3.E1.m1.4.4.4.4.4.4" xref="A3.E1.m1.4.4.4.4.4.4a.cmml">ans</mtext><mo stretchy="false" id="A3.E1.m1.5.5.5.5.5.5" xref="A3.E1.m1.23.23.1a.cmml">)</mo></mrow></mrow><mo id="A3.E1.m1.6.6.6.6.6.6" xref="A3.E1.m1.6.6.6.6.6.6.cmml">=</mo><mi id="A3.E1.m1.6.6.6.6.6.8" xref="A3.E1.m1.23.23.1a.cmml"></mi></mrow></mtd></mtr><mtr id="A3.E1.m1.24.24.2d" xref="A3.E1.m1.23.23.1.cmml"><mtd id="A3.E1.m1.24.24.2e" xref="A3.E1.m1.23.23.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E1.m1.24.24.2f" xref="A3.E1.m1.23.23.1.cmml"><mrow id="A3.E1.m1.12.12.12.6.6" xref="A3.E1.m1.23.23.1.cmml"><mrow id="A3.E1.m1.12.12.12.6.6.7" xref="A3.E1.m1.23.23.1.cmml"><msub id="A3.E1.m1.12.12.12.6.6.7.2" xref="A3.E1.m1.23.23.1.cmml"><mtext id="A3.E1.m1.7.7.7.1.1.1" xref="A3.E1.m1.7.7.7.1.1.1a.cmml">score</mtext><mtext id="A3.E1.m1.8.8.8.2.2.2.1" xref="A3.E1.m1.8.8.8.2.2.2.1a.cmml">VQA-CP</mtext></msub><mo lspace="0em" rspace="0em" id="A3.E1.m1.12.12.12.6.6.7.1" xref="A3.E1.m1.23.23.1a.cmml">â€‹</mo><mrow id="A3.E1.m1.12.12.12.6.6.7.3" xref="A3.E1.m1.23.23.1.cmml"><mo stretchy="false" id="A3.E1.m1.9.9.9.3.3.3" xref="A3.E1.m1.23.23.1a.cmml">(</mo><mtext mathcolor="#FF0000" id="A3.E1.m1.10.10.10.4.4.4" xref="A3.E1.m1.10.10.10.4.4.4a.cmml">ans</mtext><mo stretchy="false" id="A3.E1.m1.11.11.11.5.5.5" xref="A3.E1.m1.23.23.1a.cmml">)</mo></mrow></mrow><mo id="A3.E1.m1.12.12.12.6.6.6" xref="A3.E1.m1.12.12.12.6.6.6.cmml">=</mo><mi id="A3.E1.m1.12.12.12.6.6.8" xref="A3.E1.m1.23.23.1a.cmml"></mi></mrow></mtd></mtr><mtr id="A3.E1.m1.24.24.2g" xref="A3.E1.m1.23.23.1.cmml"><mtd id="A3.E1.m1.24.24.2h" xref="A3.E1.m1.23.23.1a.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E1.m1.24.24.2i" xref="A3.E1.m1.23.23.1.cmml"><mrow id="A3.E1.m1.24.24.2.23.11.11" xref="A3.E1.m1.23.23.1.cmml"><mn id="A3.E1.m1.13.13.13.1.1.1" xref="A3.E1.m1.13.13.13.1.1.1.cmml">1</mn><mo id="A3.E1.m1.14.14.14.2.2.2" xref="A3.E1.m1.14.14.14.2.2.2.cmml">âˆ’</mo><mrow id="A3.E1.m1.24.24.2.23.11.11.11.1" xref="A3.E1.m1.23.23.1.cmml"><mo stretchy="false" id="A3.E1.m1.15.15.15.3.3.3" xref="A3.E1.m1.23.23.1a.cmml">(</mo><mrow id="A3.E1.m1.24.24.2.23.11.11.11.1.1" xref="A3.E1.m1.23.23.1.cmml"><mi id="A3.E1.m1.16.16.16.4.4.4" xref="A3.E1.m1.16.16.16.4.4.4.cmml">min</mi><mo id="A3.E1.m1.24.24.2.23.11.11.11.1.1a" xref="A3.E1.m1.23.23.1a.cmml">â¡</mo><mrow id="A3.E1.m1.24.24.2.23.11.11.11.1.1.1" xref="A3.E1.m1.23.23.1.cmml"><mo stretchy="false" id="A3.E1.m1.17.17.17.5.5.5" xref="A3.E1.m1.23.23.1a.cmml">(</mo><mfrac id="A3.E1.m1.18.18.18.6.6.6" xref="A3.E1.m1.18.18.18.6.6.6.cmml"><mrow id="A3.E1.m1.18.18.18.6.6.6.2" xref="A3.E1.m1.18.18.18.6.6.6.2c.cmml"><mtext id="A3.E1.m1.18.18.18.6.6.6.2a" xref="A3.E1.m1.18.18.18.6.6.6.2c.cmml"># of annotators that saidÂ </mtext><mtext mathcolor="#FF0000" id="A3.E1.m1.18.18.18.6.6.6.2b" xref="A3.E1.m1.18.18.18.6.6.6.2c.cmml">ans</mtext></mrow><mn id="A3.E1.m1.18.18.18.6.6.6.3" xref="A3.E1.m1.18.18.18.6.6.6.3.cmml">3</mn></mfrac><mo id="A3.E1.m1.19.19.19.7.7.7" xref="A3.E1.m1.23.23.1a.cmml">,</mo><mn id="A3.E1.m1.20.20.20.8.8.8" xref="A3.E1.m1.20.20.20.8.8.8.cmml">1</mn><mo stretchy="false" id="A3.E1.m1.21.21.21.9.9.9" xref="A3.E1.m1.23.23.1a.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A3.E1.m1.22.22.22.10.10.10" xref="A3.E1.m1.23.23.1a.cmml">)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="A3.E1.m1.24b"><apply id="A3.E1.m1.23.23.1.cmml" xref="A3.E1.m1.24.24.2"><and id="A3.E1.m1.23.23.1a.cmml" xref="A3.E1.m1.24.24.2b"></and><apply id="A3.E1.m1.23.23.1b.cmml" xref="A3.E1.m1.24.24.2"><eq id="A3.E1.m1.6.6.6.6.6.6.cmml" xref="A3.E1.m1.6.6.6.6.6.6"></eq><apply id="A3.E1.m1.23.23.1.3.cmml" xref="A3.E1.m1.24.24.2"><times id="A3.E1.m1.23.23.1.3.1.cmml" xref="A3.E1.m1.24.24.2b"></times><apply id="A3.E1.m1.23.23.1.3.2.cmml" xref="A3.E1.m1.24.24.2"><csymbol cd="ambiguous" id="A3.E1.m1.23.23.1.3.2.1.cmml" xref="A3.E1.m1.24.24.2b">subscript</csymbol><ci id="A3.E1.m1.1.1.1.1.1.1a.cmml" xref="A3.E1.m1.1.1.1.1.1.1"><mtext id="A3.E1.m1.1.1.1.1.1.1.cmml" xref="A3.E1.m1.1.1.1.1.1.1">score</mtext></ci><ci id="A3.E1.m1.2.2.2.2.2.2.1a.cmml" xref="A3.E1.m1.2.2.2.2.2.2.1"><mtext mathsize="70%" id="A3.E1.m1.2.2.2.2.2.2.1.cmml" xref="A3.E1.m1.2.2.2.2.2.2.1">VQA</mtext></ci></apply><ci id="A3.E1.m1.4.4.4.4.4.4a.cmml" xref="A3.E1.m1.4.4.4.4.4.4"><mtext mathcolor="#FF0000" id="A3.E1.m1.4.4.4.4.4.4.cmml" xref="A3.E1.m1.4.4.4.4.4.4">ans</mtext></ci></apply><apply id="A3.E1.m1.23.23.1.5.cmml" xref="A3.E1.m1.24.24.2"><times id="A3.E1.m1.23.23.1.5.1.cmml" xref="A3.E1.m1.24.24.2b"></times><apply id="A3.E1.m1.23.23.1.5.2.cmml" xref="A3.E1.m1.24.24.2"><csymbol cd="ambiguous" id="A3.E1.m1.23.23.1.5.2.1.cmml" xref="A3.E1.m1.24.24.2b">subscript</csymbol><ci id="A3.E1.m1.7.7.7.1.1.1a.cmml" xref="A3.E1.m1.7.7.7.1.1.1"><mtext id="A3.E1.m1.7.7.7.1.1.1.cmml" xref="A3.E1.m1.7.7.7.1.1.1">score</mtext></ci><ci id="A3.E1.m1.8.8.8.2.2.2.1a.cmml" xref="A3.E1.m1.8.8.8.2.2.2.1"><mtext mathsize="70%" id="A3.E1.m1.8.8.8.2.2.2.1.cmml" xref="A3.E1.m1.8.8.8.2.2.2.1">VQA-CP</mtext></ci></apply><ci id="A3.E1.m1.10.10.10.4.4.4a.cmml" xref="A3.E1.m1.10.10.10.4.4.4"><mtext mathcolor="#FF0000" id="A3.E1.m1.10.10.10.4.4.4.cmml" xref="A3.E1.m1.10.10.10.4.4.4">ans</mtext></ci></apply></apply><apply id="A3.E1.m1.23.23.1c.cmml" xref="A3.E1.m1.24.24.2"><eq id="A3.E1.m1.12.12.12.6.6.6.cmml" xref="A3.E1.m1.12.12.12.6.6.6"></eq><share href="#A3.E1.m1.23.23.1.5.cmml" id="A3.E1.m1.23.23.1d.cmml" xref="A3.E1.m1.24.24.2b"></share><apply id="A3.E1.m1.23.23.1.1.cmml" xref="A3.E1.m1.24.24.2"><minus id="A3.E1.m1.14.14.14.2.2.2.cmml" xref="A3.E1.m1.14.14.14.2.2.2"></minus><cn type="integer" id="A3.E1.m1.13.13.13.1.1.1.cmml" xref="A3.E1.m1.13.13.13.1.1.1">1</cn><apply id="A3.E1.m1.23.23.1.1.1.1.1.cmml" xref="A3.E1.m1.24.24.2"><min id="A3.E1.m1.16.16.16.4.4.4.cmml" xref="A3.E1.m1.16.16.16.4.4.4"></min><apply id="A3.E1.m1.18.18.18.6.6.6.cmml" xref="A3.E1.m1.18.18.18.6.6.6"><divide id="A3.E1.m1.18.18.18.6.6.6.1.cmml" xref="A3.E1.m1.18.18.18.6.6.6"></divide><ci id="A3.E1.m1.18.18.18.6.6.6.2c.cmml" xref="A3.E1.m1.18.18.18.6.6.6.2"><mrow id="A3.E1.m1.18.18.18.6.6.6.2.cmml" xref="A3.E1.m1.18.18.18.6.6.6.2"><mtext id="A3.E1.m1.18.18.18.6.6.6.2a.cmml" xref="A3.E1.m1.18.18.18.6.6.6.2"># of annotators that saidÂ </mtext><mtext mathcolor="#FF0000" id="A3.E1.m1.18.18.18.6.6.6.2b.cmml" xref="A3.E1.m1.18.18.18.6.6.6.2">ans</mtext></mrow></ci><cn type="integer" id="A3.E1.m1.18.18.18.6.6.6.3.cmml" xref="A3.E1.m1.18.18.18.6.6.6.3">3</cn></apply><cn type="integer" id="A3.E1.m1.20.20.20.8.8.8.cmml" xref="A3.E1.m1.20.20.20.8.8.8">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E1.m1.24c">\begin{split}&amp;\text{score}_{\text{VQA}}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{ans}})=\\
&amp;\text{score}_{\text{VQA-CP}}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\text{ans}})=\\
&amp;1-(\min(\frac{\text{\# of annotators that said {\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}ans}}}{3},1))\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">VQA-Introspect asks additional perceptual sub-questions to VQAv2 and tests consistency w.r.t. visual grounding.
If a model is correct on the main question "Can birds fly?", but fails the perceptual sub-question "Are the birds in the air?", it is inconsistent and the question is potentially too easy, as it can be answered from the question alone.
We assign the (binary encoded) four combinations of "main correct/incorrect" and "all sub-questions correct/incorrect" numerical values to combine it with the other metrics.
In Equation <a href="#A3.E2" title="In Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we purposefully assign a high difficulty to a question where the perceptual sub-question is correctly answered, but the main reasoning question is not (01) and a low difficulty for a question that seems to exploit question bias (10).</p>
</div>
<div id="A3.p4" class="ltx_para">
<table id="A3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.E2.m1.1" class="ltx_Math" alttext="\text{score}_{\text{Intro}}=\left\{\begin{array}[]{ll}1.0&amp;00\text{ or }01\\
0.25&amp;10\\
0.0&amp;11\end{array}\right." display="block"><semantics id="A3.E2.m1.1a"><mrow id="A3.E2.m1.1.2" xref="A3.E2.m1.1.2.cmml"><msub id="A3.E2.m1.1.2.2" xref="A3.E2.m1.1.2.2.cmml"><mtext id="A3.E2.m1.1.2.2.2" xref="A3.E2.m1.1.2.2.2a.cmml">score</mtext><mtext id="A3.E2.m1.1.2.2.3" xref="A3.E2.m1.1.2.2.3a.cmml">Intro</mtext></msub><mo id="A3.E2.m1.1.2.1" xref="A3.E2.m1.1.2.1.cmml">=</mo><mrow id="A3.E2.m1.1.2.3.2" xref="A3.E2.m1.1.2.3.1.cmml"><mo id="A3.E2.m1.1.2.3.2.1" xref="A3.E2.m1.1.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="A3.E2.m1.1.1" xref="A3.E2.m1.1.1.cmml"><mtr id="A3.E2.m1.1.1a" xref="A3.E2.m1.1.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1b" xref="A3.E2.m1.1.1.cmml"><mn id="A3.E2.m1.1.1.1.1.1" xref="A3.E2.m1.1.1.1.1.1.cmml">1.0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1c" xref="A3.E2.m1.1.1.cmml"><mrow id="A3.E2.m1.1.1.1.2.1" xref="A3.E2.m1.1.1.1.2.1.cmml"><mn id="A3.E2.m1.1.1.1.2.1.2" xref="A3.E2.m1.1.1.1.2.1.2.cmml">00</mn><mo lspace="0em" rspace="0em" id="A3.E2.m1.1.1.1.2.1.1" xref="A3.E2.m1.1.1.1.2.1.1.cmml">â€‹</mo><mtext id="A3.E2.m1.1.1.1.2.1.3" xref="A3.E2.m1.1.1.1.2.1.3a.cmml">Â orÂ </mtext><mo lspace="0em" rspace="0em" id="A3.E2.m1.1.1.1.2.1.1a" xref="A3.E2.m1.1.1.1.2.1.1.cmml">â€‹</mo><mn id="A3.E2.m1.1.1.1.2.1.4" xref="A3.E2.m1.1.1.1.2.1.4.cmml">01</mn></mrow></mtd></mtr><mtr id="A3.E2.m1.1.1d" xref="A3.E2.m1.1.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1e" xref="A3.E2.m1.1.1.cmml"><mn id="A3.E2.m1.1.1.2.1.1" xref="A3.E2.m1.1.1.2.1.1.cmml">0.25</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1f" xref="A3.E2.m1.1.1.cmml"><mn id="A3.E2.m1.1.1.2.2.1" xref="A3.E2.m1.1.1.2.2.1.cmml">10</mn></mtd></mtr><mtr id="A3.E2.m1.1.1g" xref="A3.E2.m1.1.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1h" xref="A3.E2.m1.1.1.cmml"><mn id="A3.E2.m1.1.1.3.1.1" xref="A3.E2.m1.1.1.3.1.1.cmml">0.0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E2.m1.1.1i" xref="A3.E2.m1.1.1.cmml"><mn id="A3.E2.m1.1.1.3.2.1" xref="A3.E2.m1.1.1.3.2.1.cmml">11</mn></mtd></mtr></mtable><mi id="A3.E2.m1.1.2.3.2.2" xref="A3.E2.m1.1.2.3.1.1.cmml"></mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.E2.m1.1b"><apply id="A3.E2.m1.1.2.cmml" xref="A3.E2.m1.1.2"><eq id="A3.E2.m1.1.2.1.cmml" xref="A3.E2.m1.1.2.1"></eq><apply id="A3.E2.m1.1.2.2.cmml" xref="A3.E2.m1.1.2.2"><csymbol cd="ambiguous" id="A3.E2.m1.1.2.2.1.cmml" xref="A3.E2.m1.1.2.2">subscript</csymbol><ci id="A3.E2.m1.1.2.2.2a.cmml" xref="A3.E2.m1.1.2.2.2"><mtext id="A3.E2.m1.1.2.2.2.cmml" xref="A3.E2.m1.1.2.2.2">score</mtext></ci><ci id="A3.E2.m1.1.2.2.3a.cmml" xref="A3.E2.m1.1.2.2.3"><mtext mathsize="70%" id="A3.E2.m1.1.2.2.3.cmml" xref="A3.E2.m1.1.2.2.3">Intro</mtext></ci></apply><apply id="A3.E2.m1.1.2.3.1.cmml" xref="A3.E2.m1.1.2.3.2"><csymbol cd="latexml" id="A3.E2.m1.1.2.3.1.1.cmml" xref="A3.E2.m1.1.2.3.2.1">cases</csymbol><matrix id="A3.E2.m1.1.1.cmml" xref="A3.E2.m1.1.1"><matrixrow id="A3.E2.m1.1.1a.cmml" xref="A3.E2.m1.1.1"><cn type="float" id="A3.E2.m1.1.1.1.1.1.cmml" xref="A3.E2.m1.1.1.1.1.1">1.0</cn><apply id="A3.E2.m1.1.1.1.2.1.cmml" xref="A3.E2.m1.1.1.1.2.1"><times id="A3.E2.m1.1.1.1.2.1.1.cmml" xref="A3.E2.m1.1.1.1.2.1.1"></times><cn type="integer" id="A3.E2.m1.1.1.1.2.1.2.cmml" xref="A3.E2.m1.1.1.1.2.1.2">00</cn><ci id="A3.E2.m1.1.1.1.2.1.3a.cmml" xref="A3.E2.m1.1.1.1.2.1.3"><mtext id="A3.E2.m1.1.1.1.2.1.3.cmml" xref="A3.E2.m1.1.1.1.2.1.3">Â orÂ </mtext></ci><cn type="integer" id="A3.E2.m1.1.1.1.2.1.4.cmml" xref="A3.E2.m1.1.1.1.2.1.4">01</cn></apply></matrixrow><matrixrow id="A3.E2.m1.1.1b.cmml" xref="A3.E2.m1.1.1"><cn type="float" id="A3.E2.m1.1.1.2.1.1.cmml" xref="A3.E2.m1.1.1.2.1.1">0.25</cn><cn type="integer" id="A3.E2.m1.1.1.2.2.1.cmml" xref="A3.E2.m1.1.1.2.2.1">10</cn></matrixrow><matrixrow id="A3.E2.m1.1.1c.cmml" xref="A3.E2.m1.1.1"><cn type="float" id="A3.E2.m1.1.1.3.1.1.cmml" xref="A3.E2.m1.1.1.3.1.1">0.0</cn><cn type="integer" id="A3.E2.m1.1.1.3.2.1.cmml" xref="A3.E2.m1.1.1.3.2.1">11</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E2.m1.1c">\text{score}_{\text{Intro}}=\left\{\begin{array}[]{ll}1.0&amp;00\text{ or }01\\
0.25&amp;10\\
0.0&amp;11\end{array}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.p5" class="ltx_para">
<p id="A3.p5.3" class="ltx_p">Finally VQA-Rephrasings tests robustness against 3 linguistic variations per question and measures this with a "consensus score" â€“ the share of fully correctly answered subsets of size <math id="A3.p5.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A3.p5.1.m1.1a"><mi id="A3.p5.1.m1.1.1" xref="A3.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p5.1.m1.1b"><ci id="A3.p5.1.m1.1.1.cmml" xref="A3.p5.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.1.m1.1c">k</annotation></semantics></math> of a question and its rephrasings.
It is unclear how to interpret different settings of <math id="A3.p5.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A3.p5.2.m2.1a"><mi id="A3.p5.2.m2.1.1" xref="A3.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p5.2.m2.1b"><ci id="A3.p5.2.m2.1.1.cmml" xref="A3.p5.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.2.m2.1c">k</annotation></semantics></math>, so we set <math id="A3.p5.3.m3.1" class="ltx_Math" alttext="k=1" display="inline"><semantics id="A3.p5.3.m3.1a"><mrow id="A3.p5.3.m3.1.1" xref="A3.p5.3.m3.1.1.cmml"><mi id="A3.p5.3.m3.1.1.2" xref="A3.p5.3.m3.1.1.2.cmml">k</mi><mo id="A3.p5.3.m3.1.1.1" xref="A3.p5.3.m3.1.1.1.cmml">=</mo><mn id="A3.p5.3.m3.1.1.3" xref="A3.p5.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p5.3.m3.1b"><apply id="A3.p5.3.m3.1.1.cmml" xref="A3.p5.3.m3.1.1"><eq id="A3.p5.3.m3.1.1.1.cmml" xref="A3.p5.3.m3.1.1.1"></eq><ci id="A3.p5.3.m3.1.1.2.cmml" xref="A3.p5.3.m3.1.1.2">ğ‘˜</ci><cn type="integer" id="A3.p5.3.m3.1.1.3.cmml" xref="A3.p5.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p5.3.m3.1c">k=1</annotation></semantics></math>, which simplifies to simple accuracy over the rephrasings (Equation <a href="#A3.E3" title="In Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="A3.p6" class="ltx_para">
<table id="A3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.E3.m1.7" class="ltx_math_unparsed" alttext="\begin{split}&amp;\text{score}_{\text{Rep}}=\\
&amp;1-(\frac{\text{\# of correct k-sized subsets}}{\text{\# of k-sized subset}}\end{split}" display="block"><semantics id="A3.E3.m1.7a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="A3.E3.m1.7.7"><mtr id="A3.E3.m1.7.7a"><mtd id="A3.E3.m1.7.7b"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E3.m1.7.7c"><mrow id="A3.E3.m1.3.3.3.3.3"><msub id="A3.E3.m1.3.3.3.3.3.4"><mtext id="A3.E3.m1.1.1.1.1.1.1">score</mtext><mtext id="A3.E3.m1.2.2.2.2.2.2.1">Rep</mtext></msub><mo id="A3.E3.m1.3.3.3.3.3.3">=</mo><mi id="A3.E3.m1.3.3.3.3.3.5"></mi></mrow></mtd></mtr><mtr id="A3.E3.m1.7.7d"><mtd id="A3.E3.m1.7.7e"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E3.m1.7.7f"><mrow id="A3.E3.m1.7.7.7.4.4"><mn id="A3.E3.m1.4.4.4.1.1.1">1</mn><mo id="A3.E3.m1.5.5.5.2.2.2">âˆ’</mo><mrow id="A3.E3.m1.7.7.7.4.4.5"><mo stretchy="false" id="A3.E3.m1.6.6.6.3.3.3">(</mo><mfrac id="A3.E3.m1.7.7.7.4.4.4"><mtext id="A3.E3.m1.7.7.7.4.4.4.2"># of correct k-sized subsets</mtext><mtext id="A3.E3.m1.7.7.7.4.4.4.3"># of k-sized subset</mtext></mfrac></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="A3.E3.m1.7b">\begin{split}&amp;\text{score}_{\text{Rep}}=\\
&amp;1-(\frac{\text{\# of correct k-sized subsets}}{\text{\# of k-sized subset}}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.p7" class="ltx_para">
<p id="A3.p7.4" class="ltx_p">We combined the four resulting scores of each MFB and MCAN in equal parts (Equation <a href="#A3.E4" title="In Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
Since not all our candidate stimuli are present in all four datasets used in the difficulty score, there is a set <math id="A3.p7.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="A3.p7.1.m1.1a"><mi id="A3.p7.1.m1.1.1" xref="A3.p7.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="A3.p7.1.m1.1b"><ci id="A3.p7.1.m1.1.1.cmml" xref="A3.p7.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p7.1.m1.1c">S</annotation></semantics></math> of <math id="A3.p7.2.m2.3" class="ltx_Math" alttext="|S|\in[1,4]" display="inline"><semantics id="A3.p7.2.m2.3a"><mrow id="A3.p7.2.m2.3.4" xref="A3.p7.2.m2.3.4.cmml"><mrow id="A3.p7.2.m2.3.4.2.2" xref="A3.p7.2.m2.3.4.2.1.cmml"><mo stretchy="false" id="A3.p7.2.m2.3.4.2.2.1" xref="A3.p7.2.m2.3.4.2.1.1.cmml">|</mo><mi id="A3.p7.2.m2.1.1" xref="A3.p7.2.m2.1.1.cmml">S</mi><mo stretchy="false" id="A3.p7.2.m2.3.4.2.2.2" xref="A3.p7.2.m2.3.4.2.1.1.cmml">|</mo></mrow><mo id="A3.p7.2.m2.3.4.1" xref="A3.p7.2.m2.3.4.1.cmml">âˆˆ</mo><mrow id="A3.p7.2.m2.3.4.3.2" xref="A3.p7.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="A3.p7.2.m2.3.4.3.2.1" xref="A3.p7.2.m2.3.4.3.1.cmml">[</mo><mn id="A3.p7.2.m2.2.2" xref="A3.p7.2.m2.2.2.cmml">1</mn><mo id="A3.p7.2.m2.3.4.3.2.2" xref="A3.p7.2.m2.3.4.3.1.cmml">,</mo><mn id="A3.p7.2.m2.3.3" xref="A3.p7.2.m2.3.3.cmml">4</mn><mo stretchy="false" id="A3.p7.2.m2.3.4.3.2.3" xref="A3.p7.2.m2.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p7.2.m2.3b"><apply id="A3.p7.2.m2.3.4.cmml" xref="A3.p7.2.m2.3.4"><in id="A3.p7.2.m2.3.4.1.cmml" xref="A3.p7.2.m2.3.4.1"></in><apply id="A3.p7.2.m2.3.4.2.1.cmml" xref="A3.p7.2.m2.3.4.2.2"><abs id="A3.p7.2.m2.3.4.2.1.1.cmml" xref="A3.p7.2.m2.3.4.2.2.1"></abs><ci id="A3.p7.2.m2.1.1.cmml" xref="A3.p7.2.m2.1.1">ğ‘†</ci></apply><interval closure="closed" id="A3.p7.2.m2.3.4.3.1.cmml" xref="A3.p7.2.m2.3.4.3.2"><cn type="integer" id="A3.p7.2.m2.2.2.cmml" xref="A3.p7.2.m2.2.2">1</cn><cn type="integer" id="A3.p7.2.m2.3.3.cmml" xref="A3.p7.2.m2.3.3">4</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p7.2.m2.3c">|S|\in[1,4]</annotation></semantics></math> scores per question-image pair.
We penalize the cases where only one score is available by normalizing over <math id="A3.p7.3.m3.2" class="ltx_Math" alttext="\text{avg}(|S|)" display="inline"><semantics id="A3.p7.3.m3.2a"><mrow id="A3.p7.3.m3.2.2" xref="A3.p7.3.m3.2.2.cmml"><mtext id="A3.p7.3.m3.2.2.3" xref="A3.p7.3.m3.2.2.3a.cmml">avg</mtext><mo lspace="0em" rspace="0em" id="A3.p7.3.m3.2.2.2" xref="A3.p7.3.m3.2.2.2.cmml">â€‹</mo><mrow id="A3.p7.3.m3.2.2.1.1" xref="A3.p7.3.m3.2.2.cmml"><mo stretchy="false" id="A3.p7.3.m3.2.2.1.1.2" xref="A3.p7.3.m3.2.2.cmml">(</mo><mrow id="A3.p7.3.m3.2.2.1.1.1.2" xref="A3.p7.3.m3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="A3.p7.3.m3.2.2.1.1.1.2.1" xref="A3.p7.3.m3.2.2.1.1.1.1.1.cmml">|</mo><mi id="A3.p7.3.m3.1.1" xref="A3.p7.3.m3.1.1.cmml">S</mi><mo stretchy="false" id="A3.p7.3.m3.2.2.1.1.1.2.2" xref="A3.p7.3.m3.2.2.1.1.1.1.1.cmml">|</mo></mrow><mo stretchy="false" id="A3.p7.3.m3.2.2.1.1.3" xref="A3.p7.3.m3.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p7.3.m3.2b"><apply id="A3.p7.3.m3.2.2.cmml" xref="A3.p7.3.m3.2.2"><times id="A3.p7.3.m3.2.2.2.cmml" xref="A3.p7.3.m3.2.2.2"></times><ci id="A3.p7.3.m3.2.2.3a.cmml" xref="A3.p7.3.m3.2.2.3"><mtext id="A3.p7.3.m3.2.2.3.cmml" xref="A3.p7.3.m3.2.2.3">avg</mtext></ci><apply id="A3.p7.3.m3.2.2.1.1.1.1.cmml" xref="A3.p7.3.m3.2.2.1.1.1.2"><abs id="A3.p7.3.m3.2.2.1.1.1.1.1.cmml" xref="A3.p7.3.m3.2.2.1.1.1.2.1"></abs><ci id="A3.p7.3.m3.1.1.cmml" xref="A3.p7.3.m3.1.1">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p7.3.m3.2c">\text{avg}(|S|)</annotation></semantics></math> instead of <math id="A3.p7.4.m4.1" class="ltx_Math" alttext="|S|" display="inline"><semantics id="A3.p7.4.m4.1a"><mrow id="A3.p7.4.m4.1.2.2" xref="A3.p7.4.m4.1.2.1.cmml"><mo stretchy="false" id="A3.p7.4.m4.1.2.2.1" xref="A3.p7.4.m4.1.2.1.1.cmml">|</mo><mi id="A3.p7.4.m4.1.1" xref="A3.p7.4.m4.1.1.cmml">S</mi><mo stretchy="false" id="A3.p7.4.m4.1.2.2.2" xref="A3.p7.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.p7.4.m4.1b"><apply id="A3.p7.4.m4.1.2.1.cmml" xref="A3.p7.4.m4.1.2.2"><abs id="A3.p7.4.m4.1.2.1.1.cmml" xref="A3.p7.4.m4.1.2.2.1"></abs><ci id="A3.p7.4.m4.1.1.cmml" xref="A3.p7.4.m4.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p7.4.m4.1c">|S|</annotation></semantics></math> to counterweight the uncertainty it brings.</p>
</div>
<div id="A3.p8" class="ltx_para">
<table id="A3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.E4.m1.9" class="ltx_Math" alttext="\begin{split}&amp;\text{difficulty}=\\
&amp;\frac{1}{max(|S|,\text{avg}(|S|))}\cdot\sum_{s\in S}(\frac{\text{score}_{s}^{\text{MFB}}+\text{score}_{s}^{\text{MCAN}}}{2})\end{split}" display="block"><semantics id="A3.E4.m1.9a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="A3.E4.m1.9.9" xref="A3.E4.m1.9.10.1.cmml"><mtr id="A3.E4.m1.9.9a" xref="A3.E4.m1.9.10.1.cmml"><mtd id="A3.E4.m1.9.9b" xref="A3.E4.m1.9.10.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E4.m1.9.9c" xref="A3.E4.m1.9.10.1.cmml"><mrow id="A3.E4.m1.2.2.2.2.2" xref="A3.E4.m1.9.10.1.cmml"><mtext id="A3.E4.m1.1.1.1.1.1.1" xref="A3.E4.m1.1.1.1.1.1.1a.cmml">difficulty</mtext><mo id="A3.E4.m1.2.2.2.2.2.2" xref="A3.E4.m1.2.2.2.2.2.2.cmml">=</mo><mi id="A3.E4.m1.2.2.2.2.2.3" xref="A3.E4.m1.9.10.1.cmml"></mi></mrow></mtd></mtr><mtr id="A3.E4.m1.9.9d" xref="A3.E4.m1.9.10.1.cmml"><mtd id="A3.E4.m1.9.9e" xref="A3.E4.m1.9.10.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="A3.E4.m1.9.9f" xref="A3.E4.m1.9.10.1.cmml"><mrow id="A3.E4.m1.9.9.9.7.7" xref="A3.E4.m1.9.10.1.cmml"><mfrac id="A3.E4.m1.3.3.3.1.1.1" xref="A3.E4.m1.3.3.3.1.1.1.cmml"><mn id="A3.E4.m1.3.3.3.1.1.1.6" xref="A3.E4.m1.3.3.3.1.1.1.6.cmml">1</mn><mrow id="A3.E4.m1.3.3.3.1.1.1.4" xref="A3.E4.m1.3.3.3.1.1.1.4.cmml"><mi id="A3.E4.m1.3.3.3.1.1.1.4.6" xref="A3.E4.m1.3.3.3.1.1.1.4.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="A3.E4.m1.3.3.3.1.1.1.4.5" xref="A3.E4.m1.3.3.3.1.1.1.4.5.cmml">â€‹</mo><mi id="A3.E4.m1.3.3.3.1.1.1.4.7" xref="A3.E4.m1.3.3.3.1.1.1.4.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="A3.E4.m1.3.3.3.1.1.1.4.5a" xref="A3.E4.m1.3.3.3.1.1.1.4.5.cmml">â€‹</mo><mi id="A3.E4.m1.3.3.3.1.1.1.4.8" xref="A3.E4.m1.3.3.3.1.1.1.4.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="A3.E4.m1.3.3.3.1.1.1.4.5b" xref="A3.E4.m1.3.3.3.1.1.1.4.5.cmml">â€‹</mo><mrow id="A3.E4.m1.3.3.3.1.1.1.4.4.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.3.cmml"><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.3" xref="A3.E4.m1.3.3.3.1.1.1.4.4.3.cmml">(</mo><mrow id="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.2" xref="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.1.cmml"><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.2.1" xref="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.1.1.cmml">|</mo><mi id="A3.E4.m1.3.3.3.1.1.1.1.1" xref="A3.E4.m1.3.3.3.1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.2.2" xref="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.1.1.cmml">|</mo></mrow><mo id="A3.E4.m1.3.3.3.1.1.1.4.4.2.4" xref="A3.E4.m1.3.3.3.1.1.1.4.4.3.cmml">,</mo><mrow id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.cmml"><mtext id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3a.cmml">avg</mtext><mo lspace="0em" rspace="0em" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.2.cmml">â€‹</mo><mrow id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.cmml"><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.cmml">(</mo><mrow id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.1.cmml"><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.2.1" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.1.1.cmml">|</mo><mi id="A3.E4.m1.3.3.3.1.1.1.2.2" xref="A3.E4.m1.3.3.3.1.1.1.2.2.cmml">S</mi><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.2.2" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.1.1.cmml">|</mo></mrow><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.3" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A3.E4.m1.3.3.3.1.1.1.4.4.2.5" xref="A3.E4.m1.3.3.3.1.1.1.4.4.3.cmml">)</mo></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.055em" id="A3.E4.m1.4.4.4.2.2.2" xref="A3.E4.m1.4.4.4.2.2.2.cmml">â‹…</mo><mrow id="A3.E4.m1.9.9.9.7.7.8" xref="A3.E4.m1.9.10.1.cmml"><munder id="A3.E4.m1.9.9.9.7.7.8.1" xref="A3.E4.m1.9.10.1.cmml"><mo movablelimits="false" rspace="0em" id="A3.E4.m1.5.5.5.3.3.3" xref="A3.E4.m1.5.5.5.3.3.3.cmml">âˆ‘</mo><mrow id="A3.E4.m1.6.6.6.4.4.4.1" xref="A3.E4.m1.6.6.6.4.4.4.1.cmml"><mi id="A3.E4.m1.6.6.6.4.4.4.1.2" xref="A3.E4.m1.6.6.6.4.4.4.1.2.cmml">s</mi><mo id="A3.E4.m1.6.6.6.4.4.4.1.1" xref="A3.E4.m1.6.6.6.4.4.4.1.1.cmml">âˆˆ</mo><mi id="A3.E4.m1.6.6.6.4.4.4.1.3" xref="A3.E4.m1.6.6.6.4.4.4.1.3.cmml">S</mi></mrow></munder><mrow id="A3.E4.m1.9.9.9.7.7.8.2" xref="A3.E4.m1.9.10.1.cmml"><mo stretchy="false" id="A3.E4.m1.7.7.7.5.5.5" xref="A3.E4.m1.9.10.1.cmml">(</mo><mfrac id="A3.E4.m1.8.8.8.6.6.6" xref="A3.E4.m1.8.8.8.6.6.6.cmml"><mrow id="A3.E4.m1.8.8.8.6.6.6.2" xref="A3.E4.m1.8.8.8.6.6.6.2.cmml"><msubsup id="A3.E4.m1.8.8.8.6.6.6.2.2" xref="A3.E4.m1.8.8.8.6.6.6.2.2.cmml"><mtext id="A3.E4.m1.8.8.8.6.6.6.2.2.2.2" xref="A3.E4.m1.8.8.8.6.6.6.2.2.2.2a.cmml">score</mtext><mi id="A3.E4.m1.8.8.8.6.6.6.2.2.2.3" xref="A3.E4.m1.8.8.8.6.6.6.2.2.2.3.cmml">s</mi><mtext id="A3.E4.m1.8.8.8.6.6.6.2.2.3" xref="A3.E4.m1.8.8.8.6.6.6.2.2.3a.cmml">MFB</mtext></msubsup><mo id="A3.E4.m1.8.8.8.6.6.6.2.1" xref="A3.E4.m1.8.8.8.6.6.6.2.1.cmml">+</mo><msubsup id="A3.E4.m1.8.8.8.6.6.6.2.3" xref="A3.E4.m1.8.8.8.6.6.6.2.3.cmml"><mtext id="A3.E4.m1.8.8.8.6.6.6.2.3.2.2" xref="A3.E4.m1.8.8.8.6.6.6.2.3.2.2a.cmml">score</mtext><mi id="A3.E4.m1.8.8.8.6.6.6.2.3.2.3" xref="A3.E4.m1.8.8.8.6.6.6.2.3.2.3.cmml">s</mi><mtext id="A3.E4.m1.8.8.8.6.6.6.2.3.3" xref="A3.E4.m1.8.8.8.6.6.6.2.3.3a.cmml">MCAN</mtext></msubsup></mrow><mn id="A3.E4.m1.8.8.8.6.6.6.3" xref="A3.E4.m1.8.8.8.6.6.6.3.cmml">2</mn></mfrac><mo stretchy="false" id="A3.E4.m1.9.9.9.7.7.7" xref="A3.E4.m1.9.10.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="A3.E4.m1.9b"><apply id="A3.E4.m1.9.10.1.cmml" xref="A3.E4.m1.9.9"><eq id="A3.E4.m1.2.2.2.2.2.2.cmml" xref="A3.E4.m1.2.2.2.2.2.2"></eq><ci id="A3.E4.m1.1.1.1.1.1.1a.cmml" xref="A3.E4.m1.1.1.1.1.1.1"><mtext id="A3.E4.m1.1.1.1.1.1.1.cmml" xref="A3.E4.m1.1.1.1.1.1.1">difficulty</mtext></ci><apply id="A3.E4.m1.9.10.1.3.cmml" xref="A3.E4.m1.9.9"><ci id="A3.E4.m1.4.4.4.2.2.2.cmml" xref="A3.E4.m1.4.4.4.2.2.2">â‹…</ci><apply id="A3.E4.m1.3.3.3.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1"><divide id="A3.E4.m1.3.3.3.1.1.1.5.cmml" xref="A3.E4.m1.3.3.3.1.1.1"></divide><cn type="integer" id="A3.E4.m1.3.3.3.1.1.1.6.cmml" xref="A3.E4.m1.3.3.3.1.1.1.6">1</cn><apply id="A3.E4.m1.3.3.3.1.1.1.4.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4"><times id="A3.E4.m1.3.3.3.1.1.1.4.5.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.5"></times><ci id="A3.E4.m1.3.3.3.1.1.1.4.6.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.6">ğ‘š</ci><ci id="A3.E4.m1.3.3.3.1.1.1.4.7.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.7">ğ‘</ci><ci id="A3.E4.m1.3.3.3.1.1.1.4.8.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.8">ğ‘¥</ci><interval closure="open" id="A3.E4.m1.3.3.3.1.1.1.4.4.3.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2"><apply id="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.2"><abs id="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1.3.3.1.1.2.1"></abs><ci id="A3.E4.m1.3.3.3.1.1.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1.1.1">ğ‘†</ci></apply><apply id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2"><times id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.2.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.2"></times><ci id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3a.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3"><mtext id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.3">avg</mtext></ci><apply id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.2"><abs id="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.1.1.cmml" xref="A3.E4.m1.3.3.3.1.1.1.4.4.2.2.1.1.1.2.1"></abs><ci id="A3.E4.m1.3.3.3.1.1.1.2.2.cmml" xref="A3.E4.m1.3.3.3.1.1.1.2.2">ğ‘†</ci></apply></apply></interval></apply></apply><apply id="A3.E4.m1.9.10.1.3.3.cmml" xref="A3.E4.m1.9.9"><apply id="A3.E4.m1.9.10.1.3.3.1.cmml" xref="A3.E4.m1.9.9"><csymbol cd="ambiguous" id="A3.E4.m1.9.10.1.3.3.1.1.cmml" xref="A3.E4.m1.9.9">subscript</csymbol><sum id="A3.E4.m1.5.5.5.3.3.3.cmml" xref="A3.E4.m1.5.5.5.3.3.3"></sum><apply id="A3.E4.m1.6.6.6.4.4.4.1.cmml" xref="A3.E4.m1.6.6.6.4.4.4.1"><in id="A3.E4.m1.6.6.6.4.4.4.1.1.cmml" xref="A3.E4.m1.6.6.6.4.4.4.1.1"></in><ci id="A3.E4.m1.6.6.6.4.4.4.1.2.cmml" xref="A3.E4.m1.6.6.6.4.4.4.1.2">ğ‘ </ci><ci id="A3.E4.m1.6.6.6.4.4.4.1.3.cmml" xref="A3.E4.m1.6.6.6.4.4.4.1.3">ğ‘†</ci></apply></apply><apply id="A3.E4.m1.8.8.8.6.6.6.cmml" xref="A3.E4.m1.8.8.8.6.6.6"><divide id="A3.E4.m1.8.8.8.6.6.6.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6"></divide><apply id="A3.E4.m1.8.8.8.6.6.6.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2"><plus id="A3.E4.m1.8.8.8.6.6.6.2.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.1"></plus><apply id="A3.E4.m1.8.8.8.6.6.6.2.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2"><csymbol cd="ambiguous" id="A3.E4.m1.8.8.8.6.6.6.2.2.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2">superscript</csymbol><apply id="A3.E4.m1.8.8.8.6.6.6.2.2.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2"><csymbol cd="ambiguous" id="A3.E4.m1.8.8.8.6.6.6.2.2.2.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2">subscript</csymbol><ci id="A3.E4.m1.8.8.8.6.6.6.2.2.2.2a.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2.2.2"><mtext id="A3.E4.m1.8.8.8.6.6.6.2.2.2.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2.2.2">score</mtext></ci><ci id="A3.E4.m1.8.8.8.6.6.6.2.2.2.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2.2.3">ğ‘ </ci></apply><ci id="A3.E4.m1.8.8.8.6.6.6.2.2.3a.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2.3"><mtext mathsize="70%" id="A3.E4.m1.8.8.8.6.6.6.2.2.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.2.3">MFB</mtext></ci></apply><apply id="A3.E4.m1.8.8.8.6.6.6.2.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3"><csymbol cd="ambiguous" id="A3.E4.m1.8.8.8.6.6.6.2.3.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3">superscript</csymbol><apply id="A3.E4.m1.8.8.8.6.6.6.2.3.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3"><csymbol cd="ambiguous" id="A3.E4.m1.8.8.8.6.6.6.2.3.2.1.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3">subscript</csymbol><ci id="A3.E4.m1.8.8.8.6.6.6.2.3.2.2a.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3.2.2"><mtext id="A3.E4.m1.8.8.8.6.6.6.2.3.2.2.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3.2.2">score</mtext></ci><ci id="A3.E4.m1.8.8.8.6.6.6.2.3.2.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3.2.3">ğ‘ </ci></apply><ci id="A3.E4.m1.8.8.8.6.6.6.2.3.3a.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3.3"><mtext mathsize="70%" id="A3.E4.m1.8.8.8.6.6.6.2.3.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.2.3.3">MCAN</mtext></ci></apply></apply><cn type="integer" id="A3.E4.m1.8.8.8.6.6.6.3.cmml" xref="A3.E4.m1.8.8.8.6.6.6.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.E4.m1.9c">\begin{split}&amp;\text{difficulty}=\\
&amp;\frac{1}{max(|S|,\text{avg}(|S|))}\cdot\sum_{s\in S}(\frac{\text{score}_{s}^{\text{MFB}}+\text{score}_{s}^{\text{MCAN}}}{2})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="A3.p9" class="ltx_para">
<p id="A3.p9.1" class="ltx_p">Figure <a href="#A3.F5" title="Figure 5 â€£ Appendix C Machine Difficulty Score â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the resulting distribution of difficulty scores.</p>
</div>
<figure id="A3.F5" class="ltx_figure"><img src="/html/2109.13116/assets/figures/mhug_diff_per_rtype.png" id="A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Final distribution of difficulty per tagged reasoning type in VQA-MHUG. Clearly some types like <span id="A3.F5.3.1" class="ltx_text ltx_font_italic">reading</span> and <span id="A3.F5.4.2" class="ltx_text ltx_font_italic">counting</span> are harder than others.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Significance between models</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Table <a href="#A4.T5" title="Table 5 â€£ Appendix D Significance between models â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the significance of the differences in Rank Correlation and JSD for pairs of models.</p>
</div>
<figure id="A4.T5" class="ltx_table">
<div id="A4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:186.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.6pt,-3.3pt) scale(1.0361273583633,1.0361273583633) ;">
<table id="A4.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T5.1.1.1.1" class="ltx_tr">
<td id="A4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Between Model Comparison</td>
<td id="A4.T5.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Image Correlation</td>
<td id="A4.T5.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Image JSD</td>
<td id="A4.T5.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">Text Correlation</td>
<td id="A4.T5.1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">Text JSD</td>
</tr>
<tr id="A4.T5.1.1.2.2" class="ltx_tr">
<td id="A4.T5.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">MCAN_G vs MCAN_R</td>
<td id="A4.T5.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">***</td>
<td id="A4.T5.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">***</td>
<td id="A4.T5.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">***</td>
<td id="A4.T5.1.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">***</td>
</tr>
<tr id="A4.T5.1.1.3.3" class="ltx_tr">
<td id="A4.T5.1.1.3.3.1" class="ltx_td ltx_align_left">MCAN_G vs PYTHIA</td>
<td id="A4.T5.1.1.3.3.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.3.3.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.3.3.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.3.3.5" class="ltx_td ltx_nopad_r ltx_align_left">***</td>
</tr>
<tr id="A4.T5.1.1.4.4" class="ltx_tr">
<td id="A4.T5.1.1.4.4.1" class="ltx_td ltx_align_left">MCAN_G vs BAN</td>
<td id="A4.T5.1.1.4.4.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.4.4.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.4.4.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.4.4.5" class="ltx_td ltx_nopad_r ltx_align_left">p&gt;0.05</td>
</tr>
<tr id="A4.T5.1.1.5.5" class="ltx_tr">
<td id="A4.T5.1.1.5.5.1" class="ltx_td ltx_align_left">MCAN_G vs MFB</td>
<td id="A4.T5.1.1.5.5.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.5.5.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.5.5.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.5.5.5" class="ltx_td ltx_nopad_r ltx_align_left">***</td>
</tr>
<tr id="A4.T5.1.1.6.6" class="ltx_tr">
<td id="A4.T5.1.1.6.6.1" class="ltx_td ltx_align_left">MCAN_R vs PYTHIA</td>
<td id="A4.T5.1.1.6.6.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.6.6.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.6.6.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.6.6.5" class="ltx_td ltx_nopad_r ltx_align_left">***</td>
</tr>
<tr id="A4.T5.1.1.7.7" class="ltx_tr">
<td id="A4.T5.1.1.7.7.1" class="ltx_td ltx_align_left">MCAN_R vs BAN</td>
<td id="A4.T5.1.1.7.7.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.7.7.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.7.7.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.7.7.5" class="ltx_td ltx_nopad_r ltx_align_left">p&gt;0.05</td>
</tr>
<tr id="A4.T5.1.1.8.8" class="ltx_tr">
<td id="A4.T5.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t">PYTHIA vs BAN</td>
<td id="A4.T5.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t">p&gt;0.05</td>
<td id="A4.T5.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_t">***</td>
<td id="A4.T5.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_t">***</td>
<td id="A4.T5.1.1.8.8.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">***</td>
</tr>
<tr id="A4.T5.1.1.9.9" class="ltx_tr">
<td id="A4.T5.1.1.9.9.1" class="ltx_td ltx_align_left">PYTHIA vs MFB</td>
<td id="A4.T5.1.1.9.9.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.9.9.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.9.9.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.9.9.5" class="ltx_td ltx_nopad_r ltx_align_left">***</td>
</tr>
<tr id="A4.T5.1.1.10.10" class="ltx_tr">
<td id="A4.T5.1.1.10.10.1" class="ltx_td ltx_align_left">BAN vs MFB</td>
<td id="A4.T5.1.1.10.10.2" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.10.10.3" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.10.10.4" class="ltx_td ltx_align_left">***</td>
<td id="A4.T5.1.1.10.10.5" class="ltx_td ltx_nopad_r ltx_align_left">***</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>We performed a paired t-test to indicate if the differences between correlation and JSD scores is statistically significant where p&lt;0.05 (*), p&lt;0.01 (**), p&lt;0.001 (***). We show that for all models, the image correlation scores are statistically differ except when comparing the Pythia and BAN models. The image JSD scores and text correlation scores are significantly different for all models. The difference between models text JSD scores are significant, except for between BAN and both MCAN networks.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Experimental Setup</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">Binocular gaze data was collected with an EyeLink 1000 plus remote eye tracker at 2kHz. To ensure gaze estimation accuracy, participants were asked to use a mounted chin rest (see Figure <a href="#A5.F6" title="Figure 6 â€£ Appendix E Experimental Setup â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The stimuli was shown on a 24.5â€ screen with resolution of <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mn id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><times id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></times><cn type="integer" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="A5.p1.1.m1.1.1.3.cmml" xref="A5.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">1920\times 1080</annotation></semantics></math> pixels. The monitor was placed 90cm in front of the participants.</p>
</div>
<figure id="A5.F6" class="ltx_figure"><img src="/html/2109.13116/assets/figures/tracker_setup.jpg" id="A5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="538" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Setup of the eye tracker in our lab</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>MHUG vs. AiR-D Examples</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">The AiR-D dataset does not overlap with VQAv2, as such we separately visualize a selection of examples (see Figures <a href="#A6.F7" title="Figure 7 â€£ Appendix F MHUG vs. AiR-D Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#A6.F8" title="Figure 8 â€£ Appendix F MHUG vs. AiR-D Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) from the overlapping 195 additional stimuli presented to humans during the VQA-MHUG data collection.</p>
</div>
<figure id="A6.F7" class="ltx_figure"><img src="/html/2109.13116/assets/figures/air-mhug-example1.png" id="A6.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="763" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of MHUG gaze vs. AiR-D gaze</figcaption>
</figure>
<figure id="A6.F8" class="ltx_figure"><img src="/html/2109.13116/assets/figures/air-mhug-example2.png" id="A6.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="741" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Examples of MHUG gaze vs. AiR-D gaze</figcaption>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>More Model Examples</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">Figures <a href="#A7.F9" title="Figure 9 â€£ Appendix G More Model Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#A7.F10" title="Figure 10 â€£ Appendix G More Model Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> show additional visualization examples of VQA-MHUG data in comparison with the extracted model data. We randomly sampled question-image pairs with high image attention correlation (Figure <a href="#A7.F9" title="Figure 9 â€£ Appendix G More Model Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) and high text attention correlation (Figure <a href="#A7.F10" title="Figure 10 â€£ Appendix G More Model Examples â€£ VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<figure id="A7.F9" class="ltx_figure"><img src="/html/2109.13116/assets/figures/high_img_corr.png" id="A7.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="384" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Comparison of VQA-MHUG attention maps and the model extracted attention maps on text and images, <span id="A7.F9.2.1" class="ltx_text ltx_font_bold">where image attention correlation is high</span>.</figcaption>
</figure>
<figure id="A7.F10" class="ltx_figure"><img src="/html/2109.13116/assets/figures/high_txt_corr2.png" id="A7.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="384" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison of VQA-MHUG attention maps and the model extracted attention maps on text and images, <span id="A7.F10.2.1" class="ltx_text ltx_font_bold">where text attention correlation is high</span>.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.13115" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.13116" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.13116">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.13116" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.13117" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:30:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
