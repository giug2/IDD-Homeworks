<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Music Genre Classification: Training an AI model</title>
<!--Generated on Fri May 24 13:44:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Multi-layer perceptron,  Convolutional Neural Network,  K-Nearest Neighbours,  Random Forest Classifier
" lang="en" name="keywords"/>
<base href="/html/2405.15096v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S1" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S1.SS1" title="In I Introduction ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_italic">Is the problem solved?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S1.SS2" title="In I Introduction ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-B</span> </span><span class="ltx_text ltx_font_italic">A possible solution</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S2" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Intended Experiment methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S3" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Data Pre-Processing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S4" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Baseline Multilayer Perceptron</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S4.SS1" title="In IV Baseline Multilayer Perceptron ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Model Specifications</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S5" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Deep Neural Network: CNN</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S5.SS1" title="In V Deep Neural Network: CNN ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Model Specifications and results</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S6" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">K-Nearest Neighbours From Scratch</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S6.SS1" title="In VI K-Nearest Neighbours From Scratch ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Model Specifications and results</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S7" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Random Forest Wide model</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S7.SS1" title="In VII Random Forest Wide model ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Model Specifications and results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S7.SS2" title="In VII Random Forest Wide model ‣ Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Comparisons</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#S8" title="In Music Genre Classification: Training an AI model"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Music Genre Classification: Training an AI model
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup class="ltx_sup" id="id1.1.id1">st</sup> Keoikantse Mogonediwa
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.2.id1">Academy of computer science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.3.id2">University of Johannesburg
<br class="ltx_break"/></span>Auckland Park JHB 2092, South Africa 
<br class="ltx_break"/>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Music genre classification is an area that utilizes machine learning models and techniques for the processing of audio signals, in which applications range from content recommendation systems to music recommendation systems. In this research I explore various machine learning algorithms for the purpose of music genre classification, using features extracted from audio signals.The systems are namely, a Multilayer Perceptron (built from scratch), a k-Nearest Neighbours (also built from scratch), a Convolutional Neural Network and lastly a Random Forest wide model. In order to process the audio signals, feature extraction methods such as Short-Time Fourier Transform, and the extraction of Mel Cepstral Coefficients (MFCCs), is performed. Through this extensive research, I aim to asses the robustness of machine learning models for genre classification, and to compare their results.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Multi-layer perceptron, Convolutional Neural Network, K-Nearest Neighbours, Random Forest Classifier

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music is a form of expression, a universal language that is easy to translate into cultural stories and different emotions. Communities and societies all over the world unite through music. The emergence of digital music in the industry has resulted in music being easily accessible from any technology device <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib9" title="">9</a>]</cite>. Online streaming platforms consist of different types of musical genre’s. This research paper studies a gap in this industry, where genre classification algorithms are developed to categorize musical genres as a added feature to enhance a user’s experience. Genre classification can be used in different areas within the music industry, such as assisting music producers in identifying which musical genres are more popular.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A significant amount of work has gone into music genre classification models. Genre classification models that have been developed depend on the quality of the provided labels as part of the training process. Even though these models are advanced, there remains a gap in real world applications, where some data may not contain the necessary text labels required for genre classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.4.1.1">I-A</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS1.5.2">Is the problem solved?</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Classification models mainly follow a supervised learning approach, thus meaning they require text labels that are clear and easily definable. The study of this paper, aims to cover an area where ambiguity could exist within genre classification. The niche research area is to establish if different machine learning algorithms can perform more accurately if they use input data extracted as features from audio files, instead of using text labels.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS2.4.1.1">I-B</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS2.5.2">A possible solution</span>
</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">A possible solution is to use ”Short-Time Fourier Transform” (STFT). STFT is the practice of using audio signals to process and analyse the frequencies of an audio file over a certain time period <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib6" title="">6</a>]</cite>. Fourier transformation consists of different concepts such as overlapping windows, which is a practice that helps to capture smoother transitions between the time segments. This study proposes the use of Short-Time Fourier Transformation to extract features from an audio file. The features are then used as an input into different machine learning algorithms that are trained to classify audio files into their respective genre.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="341" id="S1.F1.g1" src="extracted/5616371/Flowchart-music-genre-classification.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Typical flowchart demonstrating the steps involved in music genre classification</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Intended Experiment methods</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Music genre classification within the machine learning niche creates a specialized research area that aims to develop algorithms that can automatically detect and categorize music. The algorithms include the Multilayer Perceptron, a Convolutional Neural Network (CNN), which is part of the deep learning models that are applied for various tasks such as computer vision or classification, the K-Nearest Neighbour (K-NN), and finally a Random Forest wide model that incorporates feature engineering.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The four models are compared to identify which model produces the most accurate results. This research paper provides a deep understanding of the challenges with regards to genre classification. The strategy used in this research integrates machine learning algorithms and windows forms applications as a user interface to make an impact on the music industry.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">To ensure the results of this study are efficient, this study first looks at a Multilayer Perceptron (MLP) algorithm as a baseline model, which has an standard architecture of just 3 layers. The 3 layers comprise of an input, which receives pre-processed data, typically presented to the model as a feature vector, a hidden and output layer, where the model is trained to reduce errors between the target output and the computed values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib5" title="">5</a>]</cite>. An example of the features used for a multilayer perceptron are Mel-Frequency Cepstral Coefficients which are mostly used for the purpose of music analysis. The second algorithm is a CNN, which is a deep neural network that has multiple layers, with reduced parameters which provides researchers with the opportunity to work with larger models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib1" title="">1</a>]</cite>. To name a few, the hyper-parameters typically used for these models are, namely batch sizes, epochs and learning rates. CNN models also make use of a split between the training, validation and test data.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">K-Nearest Neighbour is a classification model that classifies instances based on their similarities. It assigns a label to data point by identifying the class that is predominantly recognized within the nearest k-neighbours <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib3" title="">3</a>]</cite>. K-Nearest Neighbour models includes hyper-parameter tuning of the different values of k, to find the optimal value that provide the best classification result. The last model is a wider model, which is considered as a type of neural network that consists of nodes and layers. Wide models focus on the different patterns within the data.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="151" id="S2.F2.g1" src="extracted/5616371/Images/metadata.png" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Metadata information retrieved from the dataset.</figcaption>
</figure>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The proposed dataset for this research is the GTZAN Dataset, which is applied specifically for the purpose of training machine learning models for classification. The dataset consists of 10 genres, with 100 audio files for each genre, that are 30 seconds long <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib4" title="">4</a>]</cite>. The different genres are ”blues”, ”classical”, ”country”, ”disco”, ”hiphop”, ”jazz”, ”metal”, ”pop”, ”reggae”, ”rock”. During the course of this research, I discovered that the dataset actually has 1 or 2 corrupted files that could not be processed, therefore entailing that those files had to be excluded from the dataset. The files in question were identified within the ”jazz” genre.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Data Pre-Processing</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Figure 3 is the Short-Time Fourier Transform (STFT) representation that is extracted from a randomly selected audio file. It contains the meaningful information that can be used to classify audio. It is basically the analysis of frequency data of a signal over short time intervals. Padding is applied to the extracted features, to ensure that the audio features that are extracted are of the same length, which is represented in figure 4. This also ensures that the model captures only the necessary information for the training process.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The process of STFT includes the division of the input audio signal into short frames, that overlap each other, capturing temporal information. Fourier Transform is then applied to each window frame, whereby a conversion from a time domain to a frequency domain occurs, which reveals the frequency of the signal in each frame. It is also key to highlight that STFT operates with fixed length segments (windows), therefore it may be necessary to apply padding, to ensure that the length of the signal remains constant during the transformation process.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The GTZAN dataset also consists csv metadata that is utilized for the purpose of this research. The training data is split into 80% for the training data, and 20% for the testing phase. Figure 2 presents a sample of the metadata information, whereby a complete view of the metadata is a total of 60 columns which all contain information extracted from audio signals through STFT.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S3.F3.g1" src="extracted/5616371/CNN_Model/reggae_STFT.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>STFT of a randomly selected Reggae audio file</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="100" id="S3.F4.g1" src="extracted/5616371/CNN_Model/reggae_STFT_truncated_.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>STFT of a randomly selected Reggae audio file in which padding has been applied</figcaption>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The spectrogram representation which is obtained through STFT, contains the necessary features required for classification. The most important feature to highlight in this regard is the Mel-Frequency Cepstral Coefficients (MFCCs). MFCCS is typically used for speech recognition, speaker identification and musical genre classification. This feature contains the necessary frequency information that used to process audio signals of different genres which is important for classification. The numerical values determined from this features are converted into a 1D feature vector and provided as input into the models. Figure 5 represents the spectrogram.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="S3.F5.g1" src="extracted/5616371/CNN_Model/reggae_Spectogram.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Spectrogram of the selected reggae genre audio file</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="100" id="S3.F6.g1" src="extracted/5616371/CNN_Model/reggae_wave.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Wave form representation of a randomly selected Reggae audio file</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Baseline Multilayer Perceptron</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Model Specifications</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The MLP used in this research comprises of an input layer, a hidden layer with 1024 neurons, and an output layer in which a softmax function is applied to it. A ReLU activation function is applied, along with gradient descent with the purpose of fine-tuning hyperparameters.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In the context of this model, the complexity limits the model’s capabilities to learn meaningful representations. The model has not been tested with techniques such as batch normalization, or drop out. The results presented by the model showcase that the prescribed idea proposed in this research of using an MLP with only 3 hidden layers for classification, is not an efficient solution.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The similarly related work by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib8" title="">8</a>]</cite>, outlines the use of techniques such as feature selection, or a combination of computing space-time decomposition. In their research paper, the MLP’s performance is able to produce results up to 56.40%, which signifies the idea of using different techniques to improve performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Deep Neural Network: CNN</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Model Specifications and results</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The model consists of 7 layers. The first convolutional layer uses 32 filters with a kernel size of (3,3). The second convolutional layer consists of 64 filters, also with the kernel size of (3,3). A max pooling layer is included, and the output is then flattened to ensure that it is contained within a 1D array, which is the feature vector containing the necessary input values required for computation. The selected activation function is a Rectifier Linear Unit. When process the data, because the CNN uses the windows frames as input for performing classification, which are the time segments, the feature extraction process had to include padding, in which a specified max length is determined, and if the length of the time segment exceeds the max length, the values are truncated to fit the max length, or to add 0 values in a case where the time segments are not equal to the max length. This is done to ensure that the values remain constant for the CNN algorithm.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In terms of calculating the loss, a categorical cross entropy function is used as it is most suitable for multiple output classifications. The selected optimizer is ”adam” optimizer, with a softmax function being applied on the last layer of the model. The results from this architecture return a 0.25% accuracy when the model is trained for 10 epochs, with a dense layer consisting of 128 neurons. The possible reason for this low accuracy rate could be that the model might benefit from an increase in complexity, which is what I further researched and noted, and indeed the accuracy rating increased.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">When the model is provided with 2 dense layers of 256 and 128 neurons, along with a dropout rate of 0.3 which randomly sets the input values to 0 to prevent overfitting, its accuracy rate increases to around 0.275%. This could mean that increasing the dense layers improves the performance of the model.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="S5.F7.g1" src="extracted/5616371/CNN_Model/accuracy.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Training accuracy of CNN Model</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="194" id="S5.F8.g1" src="extracted/5616371/CNN_Model/train.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Training loss of CNN model.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Figure 7 essentially represents an accuracy rating of about 23%, in which the validation data shows that it is possible that, at some point, the model was not learning any information before a significant rise in accuracy. In comparison to the loss outlined in figure 8, this could mean the model was stuck in a local minima before continuing onwards. The change in accuracy is affected by the addition of dropout and an inclusion of another layer with 64 neurons. The model is trained for 10 epochs, whereby the increase in accuracy displayed on the chart, could signify that provided the training time is increased, the model accuracy could increase.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">K-Nearest Neighbours From Scratch</span>
</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.4.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.5.2">Model Specifications and results</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The K-Nearest model presented a decent result of 55% for music genre classification. The models specifications include a K = 1 value, in which, increasing this value decreases the performance of the model significantly.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">K-fold cross validation is technique that can be used for improving the KNN’s performance. A similar study highlighted the use of K-fold cross validation for performing classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib10" title="">10</a>]</cite>, which improves the performance, and also ensures that the accuracy is not biased.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">The KNN model is essentially a model that requires less training time compared to other existing models. Selecting a k value ranges from 1 to 100 in which the different values can be tested to identify the most optimal accuracy. The similarly related research work on genre classification presented a KNN with an accuracy rating of 77.18%. The author opted for the Spotify dataset which contains all the values for the metadata of each genre <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.15096v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Random Forest Wide model</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.4.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.5.2">Model Specifications and results</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">The random forest is defined with 35 estimators and a depth of 25 for complexity. In this regard, the more complex the model is, the more it is able to interpret data and improve its accuracy, similarly for its number of estimators. The model’s results are present a positive outcome of about 84% for genre classification.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">The confusion matrix entails that a lighter color essentially represent how well the model has learned meaningful representations. Interestingly, with regards to this particular model, the confusion matrix demonstrates the models ability to classify classical music most accurately compared to other genres. Figure 9 presents the confusion matrix.</p>
</div>
<figure class="ltx_figure" id="S7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S7.F9.g1" src="extracted/5616371/RandomForest/cm.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Confusion Matrix for Random Forest.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.4.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.5.2">Comparisons</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S7.SS2.p1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.SS2.p1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S7.SS2.p1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S7.SS2.p1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1.1.2.1">Results</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.SS2.p1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.SS2.p1.1.2.1.1">MLP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.SS2.p1.1.2.1.2">0%</td>
</tr>
<tr class="ltx_tr" id="S7.SS2.p1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.SS2.p1.1.3.2.1">CNN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.SS2.p1.1.3.2.2">40%</td>
</tr>
<tr class="ltx_tr" id="S7.SS2.p1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.SS2.p1.1.4.3.1">KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.SS2.p1.1.4.3.2">55%</td>
</tr>
<tr class="ltx_tr" id="S7.SS2.p1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S7.SS2.p1.1.5.4.1">RandomForest</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.SS2.p1.1.5.4.2">84%</td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_figure" id="S7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="121" id="S7.F10.g1" src="extracted/5616371/Images/count.png" width="120"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Count of the audio files for each genre</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">To summarise the findings, the Random Forest model presented a higher accuracy rating compared to the MLP, CNN and KNN. The Random Forest’s ensemble of decision trees allowed it to capture complex relationships in the data more effectively than the other models. In contrast, the MLP’s architecture and limited capacity prevent the model from learning intricate patterns and useful information to perform classification. This is also similar to the covolutional neural network, whereby the models complexity plays a role in its ability to classify accurately. It is also possible that the CNN’s fixed kernel sizes were not the most optimal solution for the purpose of this research.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">In terms of feature engineering, STFT is a feature extraction method that can be applied to audio files to depict information that can be transposed into feature vectors, to train machine learning models. The dataset contains a few files that are corrupt, and it is possible that those files can impact the ability of the models to generalise when shown new data. The identified files were recognised in the ”jazz” class, which could also result in a class imbalance compared to the rest of the classes.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Figure 10 present a count of the number of audio files for each class that are processed and used for this research. The distribution of samples across classes provides considerations when interpreting the model’s performance and generalization. To conclude this research, machine learning models present an avenue in music that can be further explored and utilized.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi.

</span>
<span class="ltx_bibblock">Understanding of a convolutional neural network.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">2017 international conference on engineering and technology (ICET)</span>, pages 1–6. Ieee, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Morgan Buisson, Pablo Alonso-Jiménez, and Dmitry Bogdanov.

</span>
<span class="ltx_bibblock">Ambiguity modelling with label distribution learning for music classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 611–615. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Aman Kataria and MD Singh.

</span>
<span class="ltx_bibblock">A review of data classification using k-nearest neighbour algorithm.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">International Journal of Emerging Technology and Advanced Engineering</span>, 3(6):354–360, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Andrada Olteanu.

</span>
<span class="ltx_bibblock">Gtzan dataset-music genre classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Kaggle. com</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y-S Park and S Lek.

</span>
<span class="ltx_bibblock">Artificial neural networks: Multilayer perceptron for ecological modeling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Developments in environmental modelling</span>, volume 28, pages 123–140. Elsevier, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Michael Portnoff.

</span>
<span class="ltx_bibblock">Time-frequency representation of digital signals and systems based on short-time fourier analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Acoustics, Speech, and Signal Processing</span>, 28(1):55–69, 1980.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dewangga Satriya Rahardwika, Eko Hari Rachmawanto, Christy Atika Sari, Candra Irawan, Desi Purwanti Kusumaningrum, Swapaka Listya Trusthi, et al.

</span>
<span class="ltx_bibblock">Comparison of svm, knn, and nb classifier for genre music classification based on metadata.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2020 international seminar on application for technology of information and communication (iSemantic)</span>, pages 12–16. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Carlos N Silla, Alessandro L Koerich, and Celso AA Kaestner.

</span>
<span class="ltx_bibblock">A machine learning approach to automatic music genre classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Journal of the Brazilian Computer Society</span>, 14:7–18, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Sadie A Stafford.

</span>
<span class="ltx_bibblock">Music in the digital age: The emergence of digital music and its repercussions on the music industry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">The Elon Journal of Undergraduate Research in Communications</span>, 1(2):112–120, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
George Tzanetakis and Perry Cook.

</span>
<span class="ltx_bibblock">Musical genre classification of audio signals.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on speech and audio processing</span>, 10(5):293–302, 2002.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 13:44:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
