<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Revisit Anything: Visual Place Recognition via Image Segment Retrieval</title>
<!--Generated on Wed Sep 25 16:40:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2409.18049v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S1" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.SS1" title="In 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Whole Image Encoders</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.SS2" title="In 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Region/Patch Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.SS3" title="In 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Segments-Enhanced Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.SS4" title="In 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Open-set VPR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.SS1" title="In 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.SS2" title="In 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Super Segments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.SS3" title="In 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>SuperSegment Descriptors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.SS4" title="In 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Image Retrieval via Segments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S4" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S4.SS0.SSS0.Px1" title="In 4 Experimental Setup ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Datasets</span>:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S4.SS0.SSS0.Px2" title="In 4 Experimental Setup ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Evaluation and Benchmarking</span>:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS1" title="In 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>State-of-the-art comparisons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS1.SSS1" title="In 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Aggregating Segments vs Whole Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS1.SSS2" title="In 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>VPR Fine-tuned Encoders + Segments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS2" title="In 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Revisiting Objects of Interest (OOI): Object Instance Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3" title="In 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3.SSS1" title="In 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>Aggregation method &amp; Order of Neighborhood Expansion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3.SSS2" title="In 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>Segment to Image Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3.SSS3" title="In 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>Patches vs Segment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS4" title="In 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Limitations, Storage, Compute Time &amp; IOU Based Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS5" title="In 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Qualitative Analyses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S6" title="In Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S7" title="In Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S8" title="In Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Storage, Compute Time &amp; IOU Based Filtering Additional Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9" title="In Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Additional Results and Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS1" title="In 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Local Feature Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS2" title="In 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>SAM vs FastSAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS3" title="In 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>HardVLAD vs (Soft) NetVLAD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS4" title="In 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4 </span>Different feature extractor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS5" title="In 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.5 </span>17Places dataset: Ground Truth Vagueness</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10" title="In Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Implementation and Benchmarking Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1" title="In 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span>Factorized Aggregation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1.SSS0.Px1" title="In 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">SegVLAD:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1.SSS0.Px2" title="In 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">GlobalVLAD:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1.SSS0.Px3" title="In 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">Segment Average Pooling (SAP):</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1.SSS0.Px4" title="In 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">Global Average Pooling (GAP):</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1.SSS0.Px5" title="In 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">Generalized Mean Pooling (GeM):</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS2" title="In 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span>Backbone Networks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS2.SSS0.Px1" title="In 10.2 Backbone Networks ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">DINOv2:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS2.SSS0.Px2" title="In 10.2 Backbone Networks ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">SAM:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS2.SSS0.Px3" title="In 10.2 Backbone Networks ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title">Evaluation:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS3" title="In 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3 </span>Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S11" title="In Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>More Qualitative Examples</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">(eccv)                Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is *not* recommended for camera-ready version</p>
</div>
<span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Indian Institute of Science (IISc), Bengaluru, India </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>International Institute of Information Technology, Hyderabad, India </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>University of Adelaide, Australia
<br class="ltx_break"/><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>* Equal contribution</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Revisit Anything: Visual Place Recognition via Image Segment Retrieval</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kartik Garg* <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0002-8585-3939" title=""><span class="ltx_ERROR undefined" id="id6.2.2.id1">\XeTeXLinkBox</span> <svg class="ltx_picture" height="1" id="id1.1.1.pic1" overflow="visible" version="1.1" width="1"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1 0 0)"><g color="#A6CE39" fill="#A6CE39" stroke="#A6CE39"><path d="M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF"><path d="M 0 0 L 0 0 L 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g></g></svg></a>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sai Shubodh Puligilla* <a class="ltx_ref ltx_href" href="https://orcid.org/0009-0000-7643-4479" title=""><span class="ltx_ERROR undefined" id="id7.2.2.id1">\XeTeXLinkBox</span> <svg class="ltx_picture" height="1" id="id2.1.1.pic1" overflow="visible" version="1.1" width="1"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1 0 0)"><g color="#A6CE39" fill="#A6CE39" stroke="#A6CE39"><path d="M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF"><path d="M 0 0 L 0 0 L 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g></g></svg></a>
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shishir Kolathaya  <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0001-8689-2318" title=""><span class="ltx_ERROR undefined" id="id8.2.2.id1">\XeTeXLinkBox</span> <svg class="ltx_picture" height="1" id="id3.1.1.pic1" overflow="visible" version="1.1" width="1"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1 0 0)"><g color="#A6CE39" fill="#A6CE39" stroke="#A6CE39"><path d="M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF"><path d="M 0 0 L 0 0 L 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g></g></svg></a>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Madhava Krishna  <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0001-7846-7901" title=""><span class="ltx_ERROR undefined" id="id9.2.2.id1">\XeTeXLinkBox</span> <svg class="ltx_picture" height="1" id="id4.1.1.pic1" overflow="visible" version="1.1" width="1"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1 0 0)"><g color="#A6CE39" fill="#A6CE39" stroke="#A6CE39"><path d="M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF"><path d="M 0 0 L 0 0 L 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g></g></svg></a>
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sourav Garg  <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0001-6068-3307" title=""><span class="ltx_ERROR undefined" id="id10.2.2.id1">\XeTeXLinkBox</span> <svg class="ltx_picture" height="1" id="id5.1.1.pic1" overflow="visible" version="1.1" width="1"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1) matrix(1 0 0 -1 0 0)"><g color="#A6CE39" fill="#A6CE39" stroke="#A6CE39"><path d="M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF"><path d="M 0 0 L 0 0 L 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 L 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 L 0 0 L 0 0 Z M 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 C 0 0 0 0 0 0 Z" style="stroke:none"></path></g></g></svg></a>
</span><span class="ltx_author_notes">33</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1"><span class="ltx_text" id="id11.id1.1">Accurately recognizing a revisited place is crucial for embodied agents to localize and navigate. This requires visual representations to be distinct, despite strong variations in camera viewpoint and scene appearance. Existing visual place recognition pipelines encode the <span class="ltx_text ltx_font_italic" id="id11.id1.1.1">whole</span> image and search for matches.
This poses a fundamental challenge in matching two images of the same place captured from different camera viewpoints: <span class="ltx_text ltx_font_italic" id="id11.id1.1.2">the similarity of what overlaps can be dominated by the dissimilarity of what does not overlap</span>.
We address this by encoding and searching for <span class="ltx_text ltx_font_italic" id="id11.id1.1.3">image segments</span> instead of the whole images. We propose to use open-set image segmentation to decompose an image into ‘meaningful’ entities (i.e., things and stuff). This enables us to create a novel image representation as a collection of multiple overlapping subgraphs connecting a segment with its neighboring segments, dubbed SuperSegment. Furthermore, to efficiently encode these SuperSegments into compact vector representations, we propose a novel factorized representation of feature aggregation. We show that retrieving these partial representations leads to significantly higher recognition recall than the typical whole image based retrieval. Our segments-based approach, dubbed SegVLAD, sets a new state-of-the-art in place recognition on a diverse selection of benchmark datasets, while being applicable to <span class="ltx_text ltx_font_italic" id="id11.id1.1.4">both</span> generic and task-specialized image encoders. Finally, we demonstrate the potential of our method to “revisit anything” by evaluating our method on an object instance retrieval task, which bridges the two disparate areas of research: visual place recognition and object-goal navigation, through their common aim of recognizing goal objects specific to a place.

<br class="ltx_break"/>Source code: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AnyLoc/Revisit-Anything" title="">https://github.com/AnyLoc/Revisit-Anything</a>.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span class="ltx_text" id="id12.id1">Visual Place Recognition Image Segmentation Robotics</span>
</div>
<section class="ltx_section" id="S1">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h3>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Visual Place Recognition (VPR) is an important capability for embodied agents to localize and navigate autonomously. A predominant solution for VPR is to encode an image into a global vector and retrieve similar vectors as coarse localization hypotheses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib44" title="">44</a>]</cite>.
Thus, for almost a decade, researchers have focused on learning/finetuning image encoders so that global descriptors are induced with invariance to appearance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib59" title="">59</a>]</cite>, viewpoint <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite>, and clutter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib27" title="">27</a>]</cite>. On the other hand, there is a vast literature on local descriptors (point/pixel-level), mainly relevant for geometric reranking in hierarchical VPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib71" title="">71</a>]</cite>. In the middle of local and global descriptors exists a variety of methods that use regions/patches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib4" title="">4</a>]</cite>, lines/planes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib17" title="">17</a>]</cite>, objects (things/stuff) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib21" title="">21</a>]</cite>, and segments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib33" title="">33</a>]</cite> to represent images. However, these methods are still only aimed at either improving global descriptors based coarse retrieval or local feature matching based reranking. In this work, in contrast to conventional retrieval-based VPR, we explore an alternative: <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">retrieval via encoding segments instead of the whole image</span>. This is particularly enabled by recent advances in open-set image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib37" title="">37</a>]</cite> which can meaningfully deconstruct a place into ‘things’ (and/or ‘stuff’) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib10" title="">10</a>]</cite>. Thus, we reformulate the VPR problem of revisiting places as that to <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">revisiting things</span> by enabling recognition of these specific things within the context of their place. While such a segment-level place recognition approach provides a direct link to higher-level semantic tasks, such as object-goal navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib43" title="">43</a>]</cite>, it also addresses a fundamental issue in matching partially-overlapping images from across significant viewpoint change. Segments-based partial image representation avoids the mismatches caused by the whole-image representation when <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">the similarity of what overlaps is dominated by the dissimilarity of what does not overlap</span>. Our novel segments-based VPR method, dubbed <span class="ltx_text ltx_font_italic" id="S1.p1.1.4">SegVLAD</span> (Segment based Vector of Locally Aggregated Descriptors), is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.F1" title="Figure 1 ‣ 2.4 Open-set VPR ‣ 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>, which makes the following novel contributions:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">an image representation as a collection of multiple overlapping subgraphs of segments, dubbed <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">SuperSegments</span>, which enables accurate recognition across partially-overlapping images;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">a factorized representation of feature aggregation to effectively accommodate both segment-level information as well as segment neighborhood information; and</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">a similarity-weighted ranking method to convert segment-level retrieval into image-level retrieval.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Using a diverse set of data sources, we demonstrate that our proposed segments-based retrieval enables place recognition under wide viewpoint variations, where global descriptor based retrieval suffers. SegVLAD achieves a new state-of-the-art on multiple challenging datasets. We also introduce an evaluation of our method on an instance-level object retrieval task – a novel capability of our pipeline unlike conventional VPR methods. We conduct several ablations and parameter studies to justify the design choices and emphasize the effectiveness of our method as an open-set segments-based coarse retriever.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h3>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Image retrieval-based Visual Place Recognition (VPR) is a well-established area of research in visual localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib70" title="">70</a>]</cite>. It is important both during mapping for loop closures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib70" title="">70</a>]</cite> as well as for relocalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib53" title="">53</a>]</cite>. The underlying task in both the scenarios remains the same: how to recognize a previously seen place. The state-of-the-art methods in VPR use a global descriptor-based approach which converts an image into a compact vector to enable fast retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite>. The top retrieved hypotheses are often then re-ranked through compute-intensive local feature matching using geometric information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib11" title="">11</a>]</cite>. In contrast to previous approaches, we aim to explore image segment level descriptors in this work. This representation falls between point-based local descriptors and the whole-image based global descriptors. Our approach can be considered as ‘semi-global’, with the proposed segment (and SuperSegment) based descriptors being a ‘spatially-reduced’ form of whole-image global representation. This is motivated by our hypothesis that to deal with viewpoint variations in VPR with partially-overlapping images, we need a way to partially represent and match them.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Whole Image Encoders</h4>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Earlier works in whole-image representation used methods like Gist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib50" title="">50</a>]</cite>, BoW (Bag of Word) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib66" title="">66</a>]</cite>, and VLAD (Vector of Locally Aggregated Descriptors) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib30" title="">30</a>]</cite>, often defined using hand-crafted features such as SIFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib41" title="">41</a>]</cite>. In recent years, deep learning based methods have demonstrated remarkable performance, with initial successful methods like NetVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite> now rapidly outperformed by better alternatives such as CosPlace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite>, MixVPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>]</cite>, EigenPlaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>]</cite>, TransVPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib71" title="">71</a>]</cite>, and more recently AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>, SALAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite> and VLAD-BuFF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib35" title="">35</a>]</cite>. All these learning-based methods improve different aspects of representation learning: training datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib1" title="">1</a>]</cite>, objective/loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>]</cite>, aggregation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib71" title="">71</a>]</cite>, and generalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>. Our approach complements these existing methods as we mainly focus on the use of segment-based information, where the segments can be described by any of the image encoders from the aforementioned techniques. In particular, we demonstrate that <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">both</span> – an off-the-shelf encoder, e.g., DINOv2-AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> or that finetuned specifically for the VPR task, e.g., DINOv2-NetVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib35" title="">35</a>]</cite> – can be used in conjunction with our segment-based approach to further elevate place recognition capability.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Region/Patch Based Methods</h4>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There exist several methods that employ region or patch level information to enhance representational power <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib74" title="">74</a>]</cite>. However, most of these methods only use this additional information to generate a single (or concatenated) compact vector representation of an image. Other methods such as Patch-NetVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib25" title="">25</a>]</cite> create multiple features per image but their primary purpose is to perform local matching based reranking. In contrast to these methods, we aim to use multiple segment descriptors per image to directly retrieve from a database of segments, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">without</span> using any geometric information or reranking. The motivation behind this stems from the very nature of hierarchical VPR pipelines: reranking recall is upper bounded by the coarse retriever. A better coarse retriever can improve reranking performance without needing to rerank from a longer list of top hypotheses. MultiVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib4" title="">4</a>]</cite> is similar to our method in the spirit of retrieving multiple features per query image. However, like aforementioned methods, MultiVLAD defines regions arbitrarily, whereas we use image segments obtained from Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib37" title="">37</a>]</cite> which are semantically meaningful.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Segments-Enhanced Methods</h4>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">There exist several methods that use semantic segmentation information to improve VPR, as also surveyed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib22" title="">22</a>]</cite>. These methods vary in terms of type of segmentation used and the specific ways in which it is integrated in the VPR pipeline, e.g., planes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib17" title="">17</a>]</cite>, objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib31" title="">31</a>]</cite>, landmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib68" title="">68</a>]</cite>, outdoor semantics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib47" title="">47</a>]</cite>, utility/confusion based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib33" title="">33</a>]</cite>, domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib26" title="">26</a>]</cite> and even learning to segment for VPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib52" title="">52</a>]</cite>. However, neither these methods aim to perform segment-level retrieval nor do they use open-set segmentation.
We also review two concurrent works:
MESA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib78" title="">78</a>]</cite> and Region-Revisited <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite>. Similar to our method, they both use SAM to segment images but for different specific tasks. MESA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib78" title="">78</a>]</cite> proposes a graph-based local feature/area matching method to obtain point correspondences. Our method complements this, as we perform coarse retrieval for VPR, which could potentially use MESA for reranking. Regions Revisted <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite> delves into the advantages of using SAM masks in conjunction with SLIC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib36" title="">36</a>]</cite> to improve semantic segmentation, activity recognition and object <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">category</span> retrieval. In contrast, we aim to improve <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">instance-level</span> recognition by recognizing specific things belonging to specific places that a robot encounters during a revisit. Similar to our work, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib20" title="">20</a>]</cite> creates an image sequence-based topological graph of segments where its segment neighbourhood aggregation is based on average pooling, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3" title="5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5.3</span></a>, we show that such segment average pooling deteriorates recognition performance for the VPR task.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Open-set VPR</h4>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Researchers have recently started to shift their focus to open-set, generally-applicable techniques, including that for VPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib65" title="">65</a>]</cite>. FM-Loc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib46" title="">46</a>]</cite> uses GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib9" title="">9</a>]</cite> to recognize object and place labels, whereas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib31" title="">31</a>]</cite> uses CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib57" title="">57</a>]</cite> for open-set place recognition. LIP-Loc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib65" title="">65</a>]</cite> proposes pretraining for cross-modal VPR, but is limited in its zero-shot capabilities. AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> proposes to use DINOv2 with domain-level vocabularies and hard-assignment based VLAD. It achieves state-of-the-art performance particularly on non-streetview datasets, where current VPR-trained methods tend to fail. In this work, we propose a generally-applicable approach which is built on top of models like SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib37" title="">37</a>]</cite> and DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib51" title="">51</a>]</cite>, and works with both VPR-agnostic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> and VPR-specific <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib35" title="">35</a>]</cite> backbone models. We particularly aim for a new paradigm in retrieval based VPR, where we move away from the conventional whole image global descriptors to segments based descriptors and retrieval, which achieves a new state-of-the-art on diverse domains under wide viewpoint variations.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S2.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Overview of our segment-retrieval based VPR pipeline, dubbed SegVLAD: We use open-set segmentor (SAM) to extract segment masks, which are converted into <span class="ltx_text ltx_font_bold" id="S2.F1.4.2.1">SuperSegments</span> using the neighbouring image segments. Using pixel-level DINOv2 descriptors with VLAD-based aggregation, we obtain SuperSegment descriptors, which are matched against a flat index of SuperSegment descriptors obtained from all the images of the entire reference database.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h3>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Despite recent advances in place recognition, viewpoint variations continue to be an open challenge for an embodied agent to recognize the same specific things in its environment. Current methods in visual place recognition tackle this problem by converting an image <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">as a whole</span> into a global descriptor, which does not explicitly deal with the problem of partial visual overlap caused by viewpoint variations. We propose an alternative solution by representing images <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">partially</span> with the help of image segments. In the following subsections, we describe our representation and retrieval method, which deviates from the conventional VPR techniques but creates a new capability in terms of recognizing objects/things that constitute a place.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Formulation</h4>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.9">We represent an image as a set of segment descriptors instead of a single global descriptor. For an image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_I</annotation></semantics></math>, we obtain binary image segment masks <math alttext="M\in\{0,1\}^{S\times N}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.2"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3" xref="S3.SS1.p1.2.m2.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.2.cmml">M</mi><mo id="S3.SS1.p1.2.m2.2.3.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">∈</mo><msup id="S3.SS1.p1.2.m2.2.3.3" xref="S3.SS1.p1.2.m2.2.3.3.cmml"><mrow id="S3.SS1.p1.2.m2.2.3.3.2.2" xref="S3.SS1.p1.2.m2.2.3.3.2.1.cmml"><mo id="S3.SS1.p1.2.m2.2.3.3.2.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.2.3.3.2.1.cmml">{</mo><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS1.p1.2.m2.2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.2.3.3.2.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">1</mn><mo id="S3.SS1.p1.2.m2.2.3.3.2.2.3" stretchy="false" xref="S3.SS1.p1.2.m2.2.3.3.2.1.cmml">}</mo></mrow><mrow id="S3.SS1.p1.2.m2.2.3.3.3" xref="S3.SS1.p1.2.m2.2.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.2.3.3.3.2" xref="S3.SS1.p1.2.m2.2.3.3.3.2.cmml">S</mi><mo id="S3.SS1.p1.2.m2.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.2.m2.2.3.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.2.3.3.3.3" xref="S3.SS1.p1.2.m2.2.3.3.3.3.cmml">N</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.3"><in id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.1"></in><ci id="S3.SS1.p1.2.m2.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.3.2">𝑀</ci><apply id="S3.SS1.p1.2.m2.2.3.3.cmml" xref="S3.SS1.p1.2.m2.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3">superscript</csymbol><set id="S3.SS1.p1.2.m2.2.3.3.2.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3.2.2"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">0</cn><cn id="S3.SS1.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2">1</cn></set><apply id="S3.SS1.p1.2.m2.2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.2.3.3.3"><times id="S3.SS1.p1.2.m2.2.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3.3.1"></times><ci id="S3.SS1.p1.2.m2.2.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.2.3.3.3.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.2.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.2.3.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">M\in\{0,1\}^{S\times N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.2d">italic_M ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_S × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> and dense pixel-level descriptors <math alttext="f_{p}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">f</mi><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">p</mi></msub><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><in id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></in><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝑓</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">𝑝</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">f_{p}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_S</annotation></semantics></math> represents the number of segments per image, <math alttext="D" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_D</annotation></semantics></math> is the descriptor dimension, and <math alttext="p\in[1,N]" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.2"><semantics id="S3.SS1.p1.6.m6.2a"><mrow id="S3.SS1.p1.6.m6.2.3" xref="S3.SS1.p1.6.m6.2.3.cmml"><mi id="S3.SS1.p1.6.m6.2.3.2" xref="S3.SS1.p1.6.m6.2.3.2.cmml">p</mi><mo id="S3.SS1.p1.6.m6.2.3.1" xref="S3.SS1.p1.6.m6.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p1.6.m6.2.3.3.2" xref="S3.SS1.p1.6.m6.2.3.3.1.cmml"><mo id="S3.SS1.p1.6.m6.2.3.3.2.1" stretchy="false" xref="S3.SS1.p1.6.m6.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">1</mn><mo id="S3.SS1.p1.6.m6.2.3.3.2.2" xref="S3.SS1.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.6.m6.2.2" xref="S3.SS1.p1.6.m6.2.2.cmml">N</mi><mo id="S3.SS1.p1.6.m6.2.3.3.2.3" stretchy="false" xref="S3.SS1.p1.6.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.2b"><apply id="S3.SS1.p1.6.m6.2.3.cmml" xref="S3.SS1.p1.6.m6.2.3"><in id="S3.SS1.p1.6.m6.2.3.1.cmml" xref="S3.SS1.p1.6.m6.2.3.1"></in><ci id="S3.SS1.p1.6.m6.2.3.2.cmml" xref="S3.SS1.p1.6.m6.2.3.2">𝑝</ci><interval closure="closed" id="S3.SS1.p1.6.m6.2.3.3.1.cmml" xref="S3.SS1.p1.6.m6.2.3.3.2"><cn id="S3.SS1.p1.6.m6.1.1.cmml" type="integer" xref="S3.SS1.p1.6.m6.1.1">1</cn><ci id="S3.SS1.p1.6.m6.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2">𝑁</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.2c">p\in[1,N]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.2d">italic_p ∈ [ 1 , italic_N ]</annotation></semantics></math> represents spatial elements across the width (<math alttext="W" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_W</annotation></semantics></math>) and height (<math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_H</annotation></semantics></math>) of the image encoder’s output, flattened into <math alttext="N=W\times H" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">N</mi><mo id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">=</mo><mrow id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mi id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">W</mi><mo id="S3.SS1.p1.9.m9.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.9.m9.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p1.9.m9.1.1.3.3" xref="S3.SS1.p1.9.m9.1.1.3.3.cmml">H</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><eq id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1"></eq><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝑁</ci><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><times id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3.1"></times><ci id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">𝑊</ci><ci id="S3.SS1.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3.3">𝐻</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">N=W\times H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_N = italic_W × italic_H</annotation></semantics></math> for convenience. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S2.F1" title="Figure 1 ‣ 2.4 Open-set VPR ‣ 2 Related Works ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> shows an illustration of our proposed pipeline, as explained in the following subsections.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Super Segments</h4>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">Humans are remarkable at visual recognition, where existing studies suggest that we often leverage spatial associations among objects in an environment to represent it internally <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib28" title="">28</a>]</cite>. This enables us to distinguish between two different scenes through the surrounding context of the objects of interest. In this work, we imbibe this context through the spatial neighbourhood of the image segments.
For each image, we construct a graph of segments through their pixel centers using Delaunay Triangulation. This provides us with a binary adjacency matrix <math alttext="A\in\{0,1\}^{S\times S}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.2"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3" xref="S3.SS2.p1.1.m1.2.3.cmml"><mi id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.2.cmml">A</mi><mo id="S3.SS2.p1.1.m1.2.3.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.2.3.3" xref="S3.SS2.p1.1.m1.2.3.3.cmml"><mrow id="S3.SS2.p1.1.m1.2.3.3.2.2" xref="S3.SS2.p1.1.m1.2.3.3.2.1.cmml"><mo id="S3.SS2.p1.1.m1.2.3.3.2.2.1" stretchy="false" xref="S3.SS2.p1.1.m1.2.3.3.2.1.cmml">{</mo><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p1.1.m1.2.3.3.2.2.2" xref="S3.SS2.p1.1.m1.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">1</mn><mo id="S3.SS2.p1.1.m1.2.3.3.2.2.3" stretchy="false" xref="S3.SS2.p1.1.m1.2.3.3.2.1.cmml">}</mo></mrow><mrow id="S3.SS2.p1.1.m1.2.3.3.3" xref="S3.SS2.p1.1.m1.2.3.3.3.cmml"><mi id="S3.SS2.p1.1.m1.2.3.3.3.2" xref="S3.SS2.p1.1.m1.2.3.3.3.2.cmml">S</mi><mo id="S3.SS2.p1.1.m1.2.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.2.3.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.2.3.3.3.3" xref="S3.SS2.p1.1.m1.2.3.3.3.3.cmml">S</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><apply id="S3.SS2.p1.1.m1.2.3.cmml" xref="S3.SS2.p1.1.m1.2.3"><in id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.1"></in><ci id="S3.SS2.p1.1.m1.2.3.2.cmml" xref="S3.SS2.p1.1.m1.2.3.2">𝐴</ci><apply id="S3.SS2.p1.1.m1.2.3.3.cmml" xref="S3.SS2.p1.1.m1.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.3.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.3">superscript</csymbol><set id="S3.SS2.p1.1.m1.2.3.3.2.1.cmml" xref="S3.SS2.p1.1.m1.2.3.3.2.2"><cn id="S3.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1">0</cn><cn id="S3.SS2.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.2.2">1</cn></set><apply id="S3.SS2.p1.1.m1.2.3.3.3.cmml" xref="S3.SS2.p1.1.m1.2.3.3.3"><times id="S3.SS2.p1.1.m1.2.3.3.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.3.3.1"></times><ci id="S3.SS2.p1.1.m1.2.3.3.3.2.cmml" xref="S3.SS2.p1.1.m1.2.3.3.3.2">𝑆</ci><ci id="S3.SS2.p1.1.m1.2.3.3.3.3.cmml" xref="S3.SS2.p1.1.m1.2.3.3.3.3">𝑆</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">A\in\{0,1\}^{S\times S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.2d">italic_A ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_S × italic_S end_POSTSUPERSCRIPT</annotation></semantics></math> to define the neighborhood for individual segments. We use this adjacency information to expand the context of individual segments to generate new <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.2.1">SuperSegment</span> masks (<math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">caligraphic_M</annotation></semantics></math>) as below:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{M}_{S\times N}={}^{\mathds{1}}(A_{S\times S}^{o}\cdot M_{S\times N})\ " class="ltx_math_unparsed" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1b"><msub id="S3.E1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.2">ℳ</mi><mrow id="S3.E1.m1.1.1.3"><mi id="S3.E1.m1.1.1.3.2">S</mi><mo id="S3.E1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E1.m1.1.1.3.3">N</mi></mrow></msub><mo id="S3.E1.m1.1.2">=</mo><mmultiscripts id="S3.E1.m1.1.3"><mrow id="S3.E1.m1.1.3.2"><mo id="S3.E1.m1.1.3.2.1" stretchy="false">(</mo><msubsup id="S3.E1.m1.1.3.2.2"><mi id="S3.E1.m1.1.3.2.2.2.2">A</mi><mrow id="S3.E1.m1.1.3.2.2.2.3"><mi id="S3.E1.m1.1.3.2.2.2.3.2">S</mi><mo id="S3.E1.m1.1.3.2.2.2.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E1.m1.1.3.2.2.2.3.3">S</mi></mrow><mi id="S3.E1.m1.1.3.2.2.3">o</mi></msubsup><mo id="S3.E1.m1.1.3.2.3" lspace="0.222em" rspace="0.222em">⋅</mo><msub id="S3.E1.m1.1.3.2.4"><mi id="S3.E1.m1.1.3.2.4.2">M</mi><mrow id="S3.E1.m1.1.3.2.4.3"><mi id="S3.E1.m1.1.3.2.4.3.2">S</mi><mo id="S3.E1.m1.1.3.2.4.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E1.m1.1.3.2.4.3.3">N</mi></mrow></msub><mo id="S3.E1.m1.1.3.2.5" stretchy="false">)</mo></mrow><mprescripts id="S3.E1.m1.1.3a"></mprescripts><mrow id="S3.E1.m1.1.3b"></mrow><mn id="S3.E1.m1.1.3.3">𝟙</mn></mmultiscripts></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{M}_{S\times N}={}^{\mathds{1}}(A_{S\times S}^{o}\cdot M_{S\times N})\ </annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_M start_POSTSUBSCRIPT italic_S × italic_N end_POSTSUBSCRIPT = start_FLOATSUPERSCRIPT blackboard_1 end_FLOATSUPERSCRIPT ( italic_A start_POSTSUBSCRIPT italic_S × italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT ⋅ italic_M start_POSTSUBSCRIPT italic_S × italic_N end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.7">where <math alttext="o\geq 0" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m1.1"><semantics id="S3.SS2.p1.3.m1.1a"><mrow id="S3.SS2.p1.3.m1.1.1" xref="S3.SS2.p1.3.m1.1.1.cmml"><mi id="S3.SS2.p1.3.m1.1.1.2" xref="S3.SS2.p1.3.m1.1.1.2.cmml">o</mi><mo id="S3.SS2.p1.3.m1.1.1.1" xref="S3.SS2.p1.3.m1.1.1.1.cmml">≥</mo><mn id="S3.SS2.p1.3.m1.1.1.3" xref="S3.SS2.p1.3.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m1.1b"><apply id="S3.SS2.p1.3.m1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1"><geq id="S3.SS2.p1.3.m1.1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1.1"></geq><ci id="S3.SS2.p1.3.m1.1.1.2.cmml" xref="S3.SS2.p1.3.m1.1.1.2">𝑜</ci><cn id="S3.SS2.p1.3.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m1.1c">o\geq 0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m1.1d">italic_o ≥ 0</annotation></semantics></math> refers to the order for expanding the neighborhood by multiplying the adjacency matrix by itself as <math alttext="A^{o+1}=A^{o}\cdot A" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m2.1"><semantics id="S3.SS2.p1.4.m2.1a"><mrow id="S3.SS2.p1.4.m2.1.1" xref="S3.SS2.p1.4.m2.1.1.cmml"><msup id="S3.SS2.p1.4.m2.1.1.2" xref="S3.SS2.p1.4.m2.1.1.2.cmml"><mi id="S3.SS2.p1.4.m2.1.1.2.2" xref="S3.SS2.p1.4.m2.1.1.2.2.cmml">A</mi><mrow id="S3.SS2.p1.4.m2.1.1.2.3" xref="S3.SS2.p1.4.m2.1.1.2.3.cmml"><mi id="S3.SS2.p1.4.m2.1.1.2.3.2" xref="S3.SS2.p1.4.m2.1.1.2.3.2.cmml">o</mi><mo id="S3.SS2.p1.4.m2.1.1.2.3.1" xref="S3.SS2.p1.4.m2.1.1.2.3.1.cmml">+</mo><mn id="S3.SS2.p1.4.m2.1.1.2.3.3" xref="S3.SS2.p1.4.m2.1.1.2.3.3.cmml">1</mn></mrow></msup><mo id="S3.SS2.p1.4.m2.1.1.1" xref="S3.SS2.p1.4.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.4.m2.1.1.3" xref="S3.SS2.p1.4.m2.1.1.3.cmml"><msup id="S3.SS2.p1.4.m2.1.1.3.2" xref="S3.SS2.p1.4.m2.1.1.3.2.cmml"><mi id="S3.SS2.p1.4.m2.1.1.3.2.2" xref="S3.SS2.p1.4.m2.1.1.3.2.2.cmml">A</mi><mi id="S3.SS2.p1.4.m2.1.1.3.2.3" xref="S3.SS2.p1.4.m2.1.1.3.2.3.cmml">o</mi></msup><mo id="S3.SS2.p1.4.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.4.m2.1.1.3.1.cmml">⋅</mo><mi id="S3.SS2.p1.4.m2.1.1.3.3" xref="S3.SS2.p1.4.m2.1.1.3.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m2.1b"><apply id="S3.SS2.p1.4.m2.1.1.cmml" xref="S3.SS2.p1.4.m2.1.1"><eq id="S3.SS2.p1.4.m2.1.1.1.cmml" xref="S3.SS2.p1.4.m2.1.1.1"></eq><apply id="S3.SS2.p1.4.m2.1.1.2.cmml" xref="S3.SS2.p1.4.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m2.1.1.2.1.cmml" xref="S3.SS2.p1.4.m2.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.4.m2.1.1.2.2.cmml" xref="S3.SS2.p1.4.m2.1.1.2.2">𝐴</ci><apply id="S3.SS2.p1.4.m2.1.1.2.3.cmml" xref="S3.SS2.p1.4.m2.1.1.2.3"><plus id="S3.SS2.p1.4.m2.1.1.2.3.1.cmml" xref="S3.SS2.p1.4.m2.1.1.2.3.1"></plus><ci id="S3.SS2.p1.4.m2.1.1.2.3.2.cmml" xref="S3.SS2.p1.4.m2.1.1.2.3.2">𝑜</ci><cn id="S3.SS2.p1.4.m2.1.1.2.3.3.cmml" type="integer" xref="S3.SS2.p1.4.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S3.SS2.p1.4.m2.1.1.3.cmml" xref="S3.SS2.p1.4.m2.1.1.3"><ci id="S3.SS2.p1.4.m2.1.1.3.1.cmml" xref="S3.SS2.p1.4.m2.1.1.3.1">⋅</ci><apply id="S3.SS2.p1.4.m2.1.1.3.2.cmml" xref="S3.SS2.p1.4.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m2.1.1.3.2.1.cmml" xref="S3.SS2.p1.4.m2.1.1.3.2">superscript</csymbol><ci id="S3.SS2.p1.4.m2.1.1.3.2.2.cmml" xref="S3.SS2.p1.4.m2.1.1.3.2.2">𝐴</ci><ci id="S3.SS2.p1.4.m2.1.1.3.2.3.cmml" xref="S3.SS2.p1.4.m2.1.1.3.2.3">𝑜</ci></apply><ci id="S3.SS2.p1.4.m2.1.1.3.3.cmml" xref="S3.SS2.p1.4.m2.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m2.1c">A^{o+1}=A^{o}\cdot A</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m2.1d">italic_A start_POSTSUPERSCRIPT italic_o + 1 end_POSTSUPERSCRIPT = italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT ⋅ italic_A</annotation></semantics></math>. This is matrix-multiplied with the original segmentation masks <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m3.1"><semantics id="S3.SS2.p1.5.m3.1a"><mi id="S3.SS2.p1.5.m3.1.1" xref="S3.SS2.p1.5.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m3.1b"><ci id="S3.SS2.p1.5.m3.1.1.cmml" xref="S3.SS2.p1.5.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m3.1d">italic_M</annotation></semantics></math> to expand the neighborhood <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.7.1">at pixel level</span>. <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m4.1"><semantics id="S3.SS2.p1.6.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.6.m4.1.1" xref="S3.SS2.p1.6.m4.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m4.1b"><ci id="S3.SS2.p1.6.m4.1.1.cmml" xref="S3.SS2.p1.6.m4.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m4.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m4.1d">caligraphic_M</annotation></semantics></math> is obtained after element-wise binarization (denoted with <math alttext="{}^{\mathds{1}}()" class="ltx_math_unparsed" display="inline" id="S3.SS2.p1.7.m5.1"><semantics id="S3.SS2.p1.7.m5.1a"><mmultiscripts id="S3.SS2.p1.7.m5.1.1"><mrow id="S3.SS2.p1.7.m5.1.1.2"><mo id="S3.SS2.p1.7.m5.1.1.2.1" stretchy="false">(</mo><mo id="S3.SS2.p1.7.m5.1.1.2.2" stretchy="false">)</mo></mrow><mprescripts id="S3.SS2.p1.7.m5.1.1a"></mprescripts><mrow id="S3.SS2.p1.7.m5.1.1b"></mrow><mn id="S3.SS2.p1.7.m5.1.1.3">𝟙</mn></mmultiscripts><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m5.1b">{}^{\mathds{1}}()</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m5.1c">start_FLOATSUPERSCRIPT blackboard_1 end_FLOATSUPERSCRIPT ( )</annotation></semantics></math>) so that all pixels in the SuperSegment mask may only contribute once to the subsequent feature aggregation. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.F2" title="Figure 2 ‣ 3.2 Super Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>, we illustrate the extent of image area covered with different orders of mask expansion. Unlike, a patch or regular grid-based approach, the expanded mask of the window in the leftmost image covers a meaningful entity (building) in the rightmost image.
Our approach to creating SuperSegments differs from <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.7.2">coarse</span> segmentation methods or superpixels in terms of the ‘self-overlap’. By expanding neighborhood of each individual segment, we obtain several <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.7.3">partially overlapping</span> SuperSegments. A coarse segmentor will need to make assumptions about the right sub-segments to be coalesced so that it can enable accurate recognition from a different viewpoint, which could otherwise lead to the same limitation as that of the whole-image descriptors. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.F3" title="Figure 3 ‣ 3.2 Super Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a> presents examples of multiple overlapping SuperSegments from the same image.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.4" style="width:433.6pt;height:78.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-239.4pt,43.5pt) scale(0.475278548620347,0.475278548620347) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.F2.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.F2.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.F2.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F2.1.1.1.1.g1" src="extracted/5879622/nbragg_single/order_0_query_idx_00151_seg_id_00010.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F2.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F2.2.2.2.2.g1" src="extracted/5879622/nbragg_single/order_1_query_idx_00151_seg_id_00010.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F2.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F2.3.3.3.3.g1" src="extracted/5879622/nbragg_single/order_2_query_idx_00151_seg_id_00010.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F2.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="222" id="S3.F2.4.4.4.4.g1" src="extracted/5879622/nbragg_single/order_3_query_idx_00151_seg_id_00010.png" width="299"/></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.6.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.7.2" style="font-size:90%;">Neighborhood expansion (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.E1" title="Equation 1 ‣ 3.2 Super Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>) of a window in the leftmost image to the whole building in the rightmost image, progressing from no neighborhood aggregation to a third-order aggregation. This neighborhood expansion is in stark contrast with a typical regular grid- or patch-based approach which may not capture semantically-meaningful SuperSegments.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F3.4" style="width:433.6pt;height:60.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-239.4pt,33.3pt) scale(0.475278548620347,0.475278548620347) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.F3.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.F3.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.F3.1.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="168" id="S3.F3.1.1.1.1.g1" src="extracted/5879622/overlap_single/cropped_query_idx_00327_seg_id_00013.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.2.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="168" id="S3.F3.2.2.2.2.g1" src="extracted/5879622/overlap_single/cropped_query_idx_00327_seg_id_00050.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.3.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="168" id="S3.F3.3.3.3.3.g1" src="extracted/5879622/overlap_single/cropped_query_idx_00327_seg_id_00100.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S3.F3.4.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="168" id="S3.F3.4.4.4.4.g1" src="extracted/5879622/overlap_single/cropped_query_idx_00327_seg_id_00135.png" width="299"/></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.6.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.7.2" style="font-size:90%;">Illustration of four SuperSegments obtained from the same image. All four of these spatially overlap with each other, which is different from coarse segmentation methods that do not typically allow overlap across segments.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>SuperSegment Descriptors</h4>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.12">In this section, we describe our feature aggregation method to obtain SuperSegment descriptors. Recent state-of-the-art method AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> demonstrated that using off-the-shelf powerful image encoders such as DINOv2 with hard assignment based VLAD aggregation achieves superior recognition performance. However, AnyLoc does not use segmentation information and only operates at the whole-image level. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite> showed that average pooling works well for segment-level descriptors, but it didn’t consider segment neighborhood information. In this work, we propose a unified formulation for feature aggregation that can easily switch across segments, segment neighborhood and the whole image as well as different aggregation types (see supplementary for details). This simply extends Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.E1" title="Equation 1 ‣ 3.2 Super Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> as below:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{S\times D}={}^{\mathds{1}}(A_{S\times S}^{o}\cdot M_{S\times N})\ \cdot T_{%
N\times D}" class="ltx_math_unparsed" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1b"><msub id="S3.E2.m1.1.1"><mi id="S3.E2.m1.1.1.2">F</mi><mrow id="S3.E2.m1.1.1.3"><mi id="S3.E2.m1.1.1.3.2">S</mi><mo id="S3.E2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E2.m1.1.1.3.3">D</mi></mrow></msub><mo id="S3.E2.m1.1.2">=</mo><mmultiscripts id="S3.E2.m1.1.3"><mrow id="S3.E2.m1.1.3.2"><mo id="S3.E2.m1.1.3.2.1" stretchy="false">(</mo><msubsup id="S3.E2.m1.1.3.2.2"><mi id="S3.E2.m1.1.3.2.2.2.2">A</mi><mrow id="S3.E2.m1.1.3.2.2.2.3"><mi id="S3.E2.m1.1.3.2.2.2.3.2">S</mi><mo id="S3.E2.m1.1.3.2.2.2.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E2.m1.1.3.2.2.2.3.3">S</mi></mrow><mi id="S3.E2.m1.1.3.2.2.3">o</mi></msubsup><mo id="S3.E2.m1.1.3.2.3" lspace="0.222em" rspace="0.222em">⋅</mo><msub id="S3.E2.m1.1.3.2.4"><mi id="S3.E2.m1.1.3.2.4.2">M</mi><mrow id="S3.E2.m1.1.3.2.4.3"><mi id="S3.E2.m1.1.3.2.4.3.2">S</mi><mo id="S3.E2.m1.1.3.2.4.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E2.m1.1.3.2.4.3.3">N</mi></mrow></msub><mo id="S3.E2.m1.1.3.2.5" rspace="0.555em" stretchy="false">)</mo></mrow><mprescripts id="S3.E2.m1.1.3a"></mprescripts><mrow id="S3.E2.m1.1.3b"></mrow><mn id="S3.E2.m1.1.3.3">𝟙</mn></mmultiscripts><mo id="S3.E2.m1.1.4" rspace="0.222em">⋅</mo><msub id="S3.E2.m1.1.5"><mi id="S3.E2.m1.1.5.2">T</mi><mrow id="S3.E2.m1.1.5.3"><mi id="S3.E2.m1.1.5.3.2">N</mi><mo id="S3.E2.m1.1.5.3.1" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.E2.m1.1.5.3.3">D</mi></mrow></msub></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1c">F_{S\times D}={}^{\mathds{1}}(A_{S\times S}^{o}\cdot M_{S\times N})\ \cdot T_{%
N\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_F start_POSTSUBSCRIPT italic_S × italic_D end_POSTSUBSCRIPT = start_FLOATSUPERSCRIPT blackboard_1 end_FLOATSUPERSCRIPT ( italic_A start_POSTSUBSCRIPT italic_S × italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT ⋅ italic_M start_POSTSUBSCRIPT italic_S × italic_N end_POSTSUBSCRIPT ) ⋅ italic_T start_POSTSUBSCRIPT italic_N × italic_D end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.7">where <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_T</annotation></semantics></math> represents the features to be aggregated. By replacing <math alttext="A" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_A</annotation></semantics></math> and <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_M</annotation></semantics></math> with ones matrices, one can obtain the whole-image global descriptor for <math alttext="S=1" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">S</mi><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><eq id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></eq><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑆</ci><cn id="S3.SS3.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.SS3.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">S=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_S = 1</annotation></semantics></math>. For methods like Global Average Pooling (GAP), <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_T</annotation></semantics></math> can be directly used as the output of the image encoder. In our work, we use Hard-VLAD, for which <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_T</annotation></semantics></math> is the residual feature matrix <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.7.1">per cluster</span> and is obtained as below with respect to each of the cluster centers <math alttext="c_{k}" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><msub id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml"><mi id="S3.SS3.p1.7.m7.1.1.2" xref="S3.SS3.p1.7.m7.1.1.2.cmml">c</mi><mi id="S3.SS3.p1.7.m7.1.1.3" xref="S3.SS3.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><apply id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.2">𝑐</ci><ci id="S3.SS3.p1.7.m7.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">c_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T^{k}_{N_{k}\times D}=\{\alpha_{k}(f_{p})(f_{p}-c_{k})\;|\;\alpha_{k}(f_{p})=1%
\};\quad N_{k}=\sum_{p}\alpha_{k}(f_{p})" class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.3.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.1.4.2.2" xref="S3.E3.m1.1.1.1.1.4.2.2.cmml">T</mi><mrow id="S3.E3.m1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.4.3.cmml"><msub id="S3.E3.m1.1.1.1.1.4.3.2" xref="S3.E3.m1.1.1.1.1.4.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.4.3.2.2" xref="S3.E3.m1.1.1.1.1.4.3.2.2.cmml">N</mi><mi id="S3.E3.m1.1.1.1.1.4.3.2.3" xref="S3.E3.m1.1.1.1.1.4.3.2.3.cmml">k</mi></msub><mo id="S3.E3.m1.1.1.1.1.4.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E3.m1.1.1.1.1.4.3.1.cmml">×</mo><mi id="S3.E3.m1.1.1.1.1.4.3.3" xref="S3.E3.m1.1.1.1.1.4.3.3.cmml">D</mi></mrow><mi id="S3.E3.m1.1.1.1.1.4.2.3" xref="S3.E3.m1.1.1.1.1.4.2.3.cmml">k</mi></msubsup><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.3.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">{</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.1.1.1.4.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.1.1.1.4.3.cmml">k</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.3a" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.2.cmml">f</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.3.cmml">p</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml">−</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.2.cmml">c</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.2.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.2.2.4" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">|</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.2.2.2.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2.2.2.1.3" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.1.3.2" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.1.3.3" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3.3.cmml">k</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.2" xref="S3.E3.m1.1.1.1.1.2.2.2.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.2.cmml">f</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.3.cmml">1</mn></mrow><mo id="S3.E3.m1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">}</mo></mrow></mrow><mo id="S3.E3.m1.2.2.2.3" rspace="1.167em" xref="S3.E3.m1.2.2.3a.cmml">;</mo><mrow id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml"><msub id="S3.E3.m1.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.3.cmml"><mi id="S3.E3.m1.2.2.2.2.3.2" xref="S3.E3.m1.2.2.2.2.3.2.cmml">N</mi><mi id="S3.E3.m1.2.2.2.2.3.3" xref="S3.E3.m1.2.2.2.2.3.3.cmml">k</mi></msub><mo id="S3.E3.m1.2.2.2.2.2" rspace="0.111em" xref="S3.E3.m1.2.2.2.2.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><munder id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml"><mo id="S3.E3.m1.2.2.2.2.1.2.2" movablelimits="false" xref="S3.E3.m1.2.2.2.2.1.2.2.cmml">∑</mo><mi id="S3.E3.m1.2.2.2.2.1.2.3" xref="S3.E3.m1.2.2.2.2.1.2.3.cmml">p</mi></munder><mrow id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><msub id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.3.2" xref="S3.E3.m1.2.2.2.2.1.1.3.2.cmml">α</mi><mi id="S3.E3.m1.2.2.2.2.1.1.3.3" xref="S3.E3.m1.2.2.2.2.1.1.3.3.cmml">k</mi></msub><mo id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.2.2.1.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.cmml"><mo id="S3.E3.m1.2.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E3.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E3.m1.2.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.3a.cmml" xref="S3.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><apply id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.4">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.2.1.cmml" xref="S3.E3.m1.1.1.1.1.4">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.2.2.cmml" xref="S3.E3.m1.1.1.1.1.4.2.2">𝑇</ci><ci id="S3.E3.m1.1.1.1.1.4.2.3.cmml" xref="S3.E3.m1.1.1.1.1.4.2.3">𝑘</ci></apply><apply id="S3.E3.m1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3"><times id="S3.E3.m1.1.1.1.1.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.4.3.1"></times><apply id="S3.E3.m1.1.1.1.1.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.4.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.4.3.2.2">𝑁</ci><ci id="S3.E3.m1.1.1.1.1.4.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3.2.3">𝑘</ci></apply><ci id="S3.E3.m1.1.1.1.1.4.3.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3.3">𝐷</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3">conditional-set</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.4.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.4.3">𝑘</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.1"></minus><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.2">𝑓</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.2.3">𝑝</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.2">𝑐</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.2.1.1.3.3">𝑘</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"><eq id="S3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2"></eq><apply id="S3.E3.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1"><times id="S3.E3.m1.1.1.1.1.2.2.2.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.2"></times><apply id="S3.E3.m1.1.1.1.1.2.2.2.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.3.3">𝑘</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.2">𝑓</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.1.1.1.1.3">𝑝</ci></apply></apply><cn id="S3.E3.m1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.2.2.2.3">1</cn></apply></apply></apply><apply id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2"><eq id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2"></eq><apply id="S3.E3.m1.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.3.1.cmml" xref="S3.E3.m1.2.2.2.2.3">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.3.2.cmml" xref="S3.E3.m1.2.2.2.2.3.2">𝑁</ci><ci id="S3.E3.m1.2.2.2.2.3.3.cmml" xref="S3.E3.m1.2.2.2.2.3.3">𝑘</ci></apply><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><apply id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2">subscript</csymbol><sum id="S3.E3.m1.2.2.2.2.1.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2.2"></sum><ci id="S3.E3.m1.2.2.2.2.1.2.3.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3">𝑝</ci></apply><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><times id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2"></times><apply id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3.2">𝛼</ci><ci id="S3.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3.3">𝑘</ci></apply><apply id="S3.E3.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.2">𝑓</ci><ci id="S3.E3.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.1.1.1.3">𝑝</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">T^{k}_{N_{k}\times D}=\{\alpha_{k}(f_{p})(f_{p}-c_{k})\;|\;\alpha_{k}(f_{p})=1%
\};\quad N_{k}=\sum_{p}\alpha_{k}(f_{p})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × italic_D end_POSTSUBSCRIPT = { italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) = 1 } ; italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.11">where <math alttext="\alpha_{k}(f_{p})\in\{0,1\}" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m1.3"><semantics id="S3.SS3.p1.8.m1.3a"><mrow id="S3.SS3.p1.8.m1.3.3" xref="S3.SS3.p1.8.m1.3.3.cmml"><mrow id="S3.SS3.p1.8.m1.3.3.1" xref="S3.SS3.p1.8.m1.3.3.1.cmml"><msub id="S3.SS3.p1.8.m1.3.3.1.3" xref="S3.SS3.p1.8.m1.3.3.1.3.cmml"><mi id="S3.SS3.p1.8.m1.3.3.1.3.2" xref="S3.SS3.p1.8.m1.3.3.1.3.2.cmml">α</mi><mi id="S3.SS3.p1.8.m1.3.3.1.3.3" xref="S3.SS3.p1.8.m1.3.3.1.3.3.cmml">k</mi></msub><mo id="S3.SS3.p1.8.m1.3.3.1.2" xref="S3.SS3.p1.8.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.SS3.p1.8.m1.3.3.1.1.1" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.cmml"><mo id="S3.SS3.p1.8.m1.3.3.1.1.1.2" stretchy="false" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p1.8.m1.3.3.1.1.1.1" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.cmml"><mi id="S3.SS3.p1.8.m1.3.3.1.1.1.1.2" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.2.cmml">f</mi><mi id="S3.SS3.p1.8.m1.3.3.1.1.1.1.3" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.SS3.p1.8.m1.3.3.1.1.1.3" stretchy="false" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p1.8.m1.3.3.2" xref="S3.SS3.p1.8.m1.3.3.2.cmml">∈</mo><mrow id="S3.SS3.p1.8.m1.3.3.3.2" xref="S3.SS3.p1.8.m1.3.3.3.1.cmml"><mo id="S3.SS3.p1.8.m1.3.3.3.2.1" stretchy="false" xref="S3.SS3.p1.8.m1.3.3.3.1.cmml">{</mo><mn id="S3.SS3.p1.8.m1.1.1" xref="S3.SS3.p1.8.m1.1.1.cmml">0</mn><mo id="S3.SS3.p1.8.m1.3.3.3.2.2" xref="S3.SS3.p1.8.m1.3.3.3.1.cmml">,</mo><mn id="S3.SS3.p1.8.m1.2.2" xref="S3.SS3.p1.8.m1.2.2.cmml">1</mn><mo id="S3.SS3.p1.8.m1.3.3.3.2.3" stretchy="false" xref="S3.SS3.p1.8.m1.3.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m1.3b"><apply id="S3.SS3.p1.8.m1.3.3.cmml" xref="S3.SS3.p1.8.m1.3.3"><in id="S3.SS3.p1.8.m1.3.3.2.cmml" xref="S3.SS3.p1.8.m1.3.3.2"></in><apply id="S3.SS3.p1.8.m1.3.3.1.cmml" xref="S3.SS3.p1.8.m1.3.3.1"><times id="S3.SS3.p1.8.m1.3.3.1.2.cmml" xref="S3.SS3.p1.8.m1.3.3.1.2"></times><apply id="S3.SS3.p1.8.m1.3.3.1.3.cmml" xref="S3.SS3.p1.8.m1.3.3.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m1.3.3.1.3.1.cmml" xref="S3.SS3.p1.8.m1.3.3.1.3">subscript</csymbol><ci id="S3.SS3.p1.8.m1.3.3.1.3.2.cmml" xref="S3.SS3.p1.8.m1.3.3.1.3.2">𝛼</ci><ci id="S3.SS3.p1.8.m1.3.3.1.3.3.cmml" xref="S3.SS3.p1.8.m1.3.3.1.3.3">𝑘</ci></apply><apply id="S3.SS3.p1.8.m1.3.3.1.1.1.1.cmml" xref="S3.SS3.p1.8.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS3.p1.8.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.8.m1.3.3.1.1.1.1.2.cmml" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.2">𝑓</ci><ci id="S3.SS3.p1.8.m1.3.3.1.1.1.1.3.cmml" xref="S3.SS3.p1.8.m1.3.3.1.1.1.1.3">𝑝</ci></apply></apply><set id="S3.SS3.p1.8.m1.3.3.3.1.cmml" xref="S3.SS3.p1.8.m1.3.3.3.2"><cn id="S3.SS3.p1.8.m1.1.1.cmml" type="integer" xref="S3.SS3.p1.8.m1.1.1">0</cn><cn id="S3.SS3.p1.8.m1.2.2.cmml" type="integer" xref="S3.SS3.p1.8.m1.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m1.3c">\alpha_{k}(f_{p})\in\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m1.3d">italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ∈ { 0 , 1 }</annotation></semantics></math> is 1 if <math alttext="f_{p}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m2.1"><semantics id="S3.SS3.p1.9.m2.1a"><msub id="S3.SS3.p1.9.m2.1.1" xref="S3.SS3.p1.9.m2.1.1.cmml"><mi id="S3.SS3.p1.9.m2.1.1.2" xref="S3.SS3.p1.9.m2.1.1.2.cmml">f</mi><mi id="S3.SS3.p1.9.m2.1.1.3" xref="S3.SS3.p1.9.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m2.1b"><apply id="S3.SS3.p1.9.m2.1.1.cmml" xref="S3.SS3.p1.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m2.1.1.1.cmml" xref="S3.SS3.p1.9.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.9.m2.1.1.2.cmml" xref="S3.SS3.p1.9.m2.1.1.2">𝑓</ci><ci id="S3.SS3.p1.9.m2.1.1.3.cmml" xref="S3.SS3.p1.9.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m2.1c">f_{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m2.1d">italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> belongs to <math alttext="c_{k}" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m3.1"><semantics id="S3.SS3.p1.10.m3.1a"><msub id="S3.SS3.p1.10.m3.1.1" xref="S3.SS3.p1.10.m3.1.1.cmml"><mi id="S3.SS3.p1.10.m3.1.1.2" xref="S3.SS3.p1.10.m3.1.1.2.cmml">c</mi><mi id="S3.SS3.p1.10.m3.1.1.3" xref="S3.SS3.p1.10.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m3.1b"><apply id="S3.SS3.p1.10.m3.1.1.cmml" xref="S3.SS3.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m3.1.1.1.cmml" xref="S3.SS3.p1.10.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m3.1.1.2.cmml" xref="S3.SS3.p1.10.m3.1.1.2">𝑐</ci><ci id="S3.SS3.p1.10.m3.1.1.3.cmml" xref="S3.SS3.p1.10.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m3.1c">c_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m3.1d">italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, and . The cluster centers (vocabulary) can be constructed using the map or the domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>. The SuperSegment VLAD descriptors obtained from Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.E2" title="Equation 2 ‣ 3.3 SuperSegment Descriptors ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a> for each cluster center <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m4.1"><semantics id="S3.SS3.p1.11.m4.1a"><mi id="S3.SS3.p1.11.m4.1.1" xref="S3.SS3.p1.11.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m4.1b"><ci id="S3.SS3.p1.11.m4.1.1.cmml" xref="S3.SS3.p1.11.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m4.1d">italic_k</annotation></semantics></math> are l2-normalized per cluster (i.e., intra-normalization), concatenated across clusters and then finally l2-normalized, following existing works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Image Retrieval via Segments</h4>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.7">Existing global descriptor based VPR techniques produce a single vector per image to search against a database of reference image vectors. In our method, we obtain multiple SuperSegment descriptors per image. We perform retrieval at segment-level, that is, we search for the top matches for each query segment against a flat index of all segments from all the images of the reference database/map. To evaluate in the form of image retrieval-based VPR, we convert the top retrieved segment indices across all segments of a query image into top reference image indices. This is achieved through a weighted frequency measure (i.e., weighted bin/word counting). We first map the top <math alttext="K^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msup id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝐾</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">K^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> retrieved segment indices for each of the query segments <math alttext="s\in[1,S]" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.2"><semantics id="S3.SS4.p1.2.m2.2a"><mrow id="S3.SS4.p1.2.m2.2.3" xref="S3.SS4.p1.2.m2.2.3.cmml"><mi id="S3.SS4.p1.2.m2.2.3.2" xref="S3.SS4.p1.2.m2.2.3.2.cmml">s</mi><mo id="S3.SS4.p1.2.m2.2.3.1" xref="S3.SS4.p1.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS4.p1.2.m2.2.3.3.2" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml"><mo id="S3.SS4.p1.2.m2.2.3.3.2.1" stretchy="false" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS4.p1.2.m2.2.3.3.2.2" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.SS4.p1.2.m2.2.2" xref="S3.SS4.p1.2.m2.2.2.cmml">S</mi><mo id="S3.SS4.p1.2.m2.2.3.3.2.3" stretchy="false" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.2b"><apply id="S3.SS4.p1.2.m2.2.3.cmml" xref="S3.SS4.p1.2.m2.2.3"><in id="S3.SS4.p1.2.m2.2.3.1.cmml" xref="S3.SS4.p1.2.m2.2.3.1"></in><ci id="S3.SS4.p1.2.m2.2.3.2.cmml" xref="S3.SS4.p1.2.m2.2.3.2">𝑠</ci><interval closure="closed" id="S3.SS4.p1.2.m2.2.3.3.1.cmml" xref="S3.SS4.p1.2.m2.2.3.3.2"><cn id="S3.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1">1</cn><ci id="S3.SS4.p1.2.m2.2.2.cmml" xref="S3.SS4.p1.2.m2.2.2">𝑆</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.2c">s\in[1,S]</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.2d">italic_s ∈ [ 1 , italic_S ]</annotation></semantics></math> to their respective reference image indices, denoted with <math alttext="r" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">italic_r</annotation></semantics></math>. Then, for each of the unique retrieved image indices <math alttext="r_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">r</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝑟</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">r_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, we accumulate its segment similarity <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mi id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">italic_θ</annotation></semantics></math> and then use the cumulative similarity score <math alttext="\hat{\theta}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><mover accent="true" id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">θ</mi><mo id="S3.SS4.p1.6.m6.1.1.1" xref="S3.SS4.p1.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><ci id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1.1">^</ci><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\hat{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">over^ start_ARG italic_θ end_ARG</annotation></semantics></math> to rank the image indices to obtain the top image match <math alttext="r_{j}^{*}" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><msubsup id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2.2" xref="S3.SS4.p1.7.m7.1.1.2.2.cmml">r</mi><mi id="S3.SS4.p1.7.m7.1.1.2.3" xref="S3.SS4.p1.7.m7.1.1.2.3.cmml">j</mi><mo id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">superscript</csymbol><apply id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.2.1.cmml" xref="S3.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.2.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2.2">𝑟</ci><ci id="S3.SS4.p1.7.m7.1.1.2.3.cmml" xref="S3.SS4.p1.7.m7.1.1.2.3">𝑗</ci></apply><times id="S3.SS4.p1.7.m7.1.1.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">r_{j}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{j}^{*}=\underset{r_{j}}{\text{argmax}}\;\hat{\theta}(r_{j});\quad\hat{%
\theta}(r_{j})=\sum_{s=1}^{S}\sum_{k=1}^{K^{\prime}}\theta_{sk}\cdot\mathds{1}%
_{\{r_{sk}=r_{j}\}}" class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.2" xref="S3.E4.m1.3.3.3.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><msubsup id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.3.2.2" xref="S3.E4.m1.2.2.1.1.3.2.2.cmml">r</mi><mi id="S3.E4.m1.2.2.1.1.3.2.3" xref="S3.E4.m1.2.2.1.1.3.2.3.cmml">j</mi><mo id="S3.E4.m1.2.2.1.1.3.3" xref="S3.E4.m1.2.2.1.1.3.3.cmml">∗</mo></msubsup><mo id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><munder accentunder="true" id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.3.cmml"><mtext id="S3.E4.m1.2.2.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.3.2a.cmml">argmax</mtext><msub id="S3.E4.m1.2.2.1.1.1.3.1" xref="S3.E4.m1.2.2.1.1.1.3.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.3.1.2" xref="S3.E4.m1.2.2.1.1.1.3.1.2.cmml">r</mi><mi id="S3.E4.m1.2.2.1.1.1.3.1.3" xref="S3.E4.m1.2.2.1.1.1.3.1.3.cmml">j</mi></msub></munder><mo id="S3.E4.m1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.2.cmml">⁢</mo><mover accent="true" id="S3.E4.m1.2.2.1.1.1.4" xref="S3.E4.m1.2.2.1.1.1.4.cmml"><mi id="S3.E4.m1.2.2.1.1.1.4.2" xref="S3.E4.m1.2.2.1.1.1.4.2.cmml">θ</mi><mo id="S3.E4.m1.2.2.1.1.1.4.1" xref="S3.E4.m1.2.2.1.1.1.4.1.cmml">^</mo></mover><mo id="S3.E4.m1.2.2.1.1.1.2a" xref="S3.E4.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.cmml">r</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.2.3" rspace="1.167em" xref="S3.E4.m1.3.3.3a.cmml">;</mo><mrow id="S3.E4.m1.3.3.2.2" xref="S3.E4.m1.3.3.2.2.cmml"><mrow id="S3.E4.m1.3.3.2.2.1" xref="S3.E4.m1.3.3.2.2.1.cmml"><mover accent="true" id="S3.E4.m1.3.3.2.2.1.3" xref="S3.E4.m1.3.3.2.2.1.3.cmml"><mi id="S3.E4.m1.3.3.2.2.1.3.2" xref="S3.E4.m1.3.3.2.2.1.3.2.cmml">θ</mi><mo id="S3.E4.m1.3.3.2.2.1.3.1" xref="S3.E4.m1.3.3.2.2.1.3.1.cmml">^</mo></mover><mo id="S3.E4.m1.3.3.2.2.1.2" xref="S3.E4.m1.3.3.2.2.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.2.2.1.1.1" xref="S3.E4.m1.3.3.2.2.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.2.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.3.3.2.2.1.1.1.1" xref="S3.E4.m1.3.3.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.2.2.1.1.1.1.2" xref="S3.E4.m1.3.3.2.2.1.1.1.1.2.cmml">r</mi><mi id="S3.E4.m1.3.3.2.2.1.1.1.1.3" xref="S3.E4.m1.3.3.2.2.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E4.m1.3.3.2.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.2.2.2" rspace="0.111em" xref="S3.E4.m1.3.3.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.2.2.3" xref="S3.E4.m1.3.3.2.2.3.cmml"><munderover id="S3.E4.m1.3.3.2.2.3.1" xref="S3.E4.m1.3.3.2.2.3.1.cmml"><mo id="S3.E4.m1.3.3.2.2.3.1.2.2" movablelimits="false" rspace="0em" xref="S3.E4.m1.3.3.2.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E4.m1.3.3.2.2.3.1.2.3" xref="S3.E4.m1.3.3.2.2.3.1.2.3.cmml"><mi id="S3.E4.m1.3.3.2.2.3.1.2.3.2" xref="S3.E4.m1.3.3.2.2.3.1.2.3.2.cmml">s</mi><mo id="S3.E4.m1.3.3.2.2.3.1.2.3.1" xref="S3.E4.m1.3.3.2.2.3.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.3.3.2.2.3.1.2.3.3" xref="S3.E4.m1.3.3.2.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.3.3.2.2.3.1.3" xref="S3.E4.m1.3.3.2.2.3.1.3.cmml">S</mi></munderover><mrow id="S3.E4.m1.3.3.2.2.3.2" xref="S3.E4.m1.3.3.2.2.3.2.cmml"><munderover id="S3.E4.m1.3.3.2.2.3.2.1" xref="S3.E4.m1.3.3.2.2.3.2.1.cmml"><mo id="S3.E4.m1.3.3.2.2.3.2.1.2.2" movablelimits="false" xref="S3.E4.m1.3.3.2.2.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E4.m1.3.3.2.2.3.2.1.2.3" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.cmml"><mi id="S3.E4.m1.3.3.2.2.3.2.1.2.3.2" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.2.cmml">k</mi><mo id="S3.E4.m1.3.3.2.2.3.2.1.2.3.1" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E4.m1.3.3.2.2.3.2.1.2.3.3" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.3.cmml">1</mn></mrow><msup id="S3.E4.m1.3.3.2.2.3.2.1.3" xref="S3.E4.m1.3.3.2.2.3.2.1.3.cmml"><mi id="S3.E4.m1.3.3.2.2.3.2.1.3.2" xref="S3.E4.m1.3.3.2.2.3.2.1.3.2.cmml">K</mi><mo id="S3.E4.m1.3.3.2.2.3.2.1.3.3" xref="S3.E4.m1.3.3.2.2.3.2.1.3.3.cmml">′</mo></msup></munderover><mrow id="S3.E4.m1.3.3.2.2.3.2.2" xref="S3.E4.m1.3.3.2.2.3.2.2.cmml"><msub id="S3.E4.m1.3.3.2.2.3.2.2.2" xref="S3.E4.m1.3.3.2.2.3.2.2.2.cmml"><mi id="S3.E4.m1.3.3.2.2.3.2.2.2.2" xref="S3.E4.m1.3.3.2.2.3.2.2.2.2.cmml">θ</mi><mrow id="S3.E4.m1.3.3.2.2.3.2.2.2.3" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.cmml"><mi id="S3.E4.m1.3.3.2.2.3.2.2.2.3.2" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.2.cmml">s</mi><mo id="S3.E4.m1.3.3.2.2.3.2.2.2.3.1" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.3.3.2.2.3.2.2.2.3.3" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.3.cmml">k</mi></mrow></msub><mo id="S3.E4.m1.3.3.2.2.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E4.m1.3.3.2.2.3.2.2.1.cmml">⋅</mo><msub id="S3.E4.m1.3.3.2.2.3.2.2.3" xref="S3.E4.m1.3.3.2.2.3.2.2.3.cmml"><mn id="S3.E4.m1.3.3.2.2.3.2.2.3.2" xref="S3.E4.m1.3.3.2.2.3.2.2.3.2.cmml">𝟙</mn><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.2.cmml">{</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.2.2.cmml">r</mi><mrow id="S3.E4.m1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.2.3.2.cmml">s</mi><mo id="S3.E4.m1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.2.3.3.cmml">k</mi></mrow></msub><mo id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">=</mo><msub id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.3.2.cmml">r</mi><mi id="S3.E4.m1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.1.1.1.2.cmml">}</mo></mrow></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.3.cmml" xref="S3.E4.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.3a.cmml" xref="S3.E4.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1"><eq id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"></eq><apply id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.2.1.cmml" xref="S3.E4.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.3.2.2.cmml" xref="S3.E4.m1.2.2.1.1.3.2.2">𝑟</ci><ci id="S3.E4.m1.2.2.1.1.3.2.3.cmml" xref="S3.E4.m1.2.2.1.1.3.2.3">𝑗</ci></apply><times id="S3.E4.m1.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3"></times></apply><apply id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2"></times><apply id="S3.E4.m1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.3"><apply id="S3.E4.m1.2.2.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.3.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.3.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.3.1.2">𝑟</ci><ci id="S3.E4.m1.2.2.1.1.1.3.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.3.1.3">𝑗</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.3.2a.cmml" xref="S3.E4.m1.2.2.1.1.1.3.2"><mtext id="S3.E4.m1.2.2.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.3.2">argmax</mtext></ci></apply><apply id="S3.E4.m1.2.2.1.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.1.4"><ci id="S3.E4.m1.2.2.1.1.1.4.1.cmml" xref="S3.E4.m1.2.2.1.1.1.4.1">^</ci><ci id="S3.E4.m1.2.2.1.1.1.4.2.cmml" xref="S3.E4.m1.2.2.1.1.1.4.2">𝜃</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2">𝑟</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3">𝑗</ci></apply></apply></apply><apply id="S3.E4.m1.3.3.2.2.cmml" xref="S3.E4.m1.3.3.2.2"><eq id="S3.E4.m1.3.3.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2"></eq><apply id="S3.E4.m1.3.3.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.1"><times id="S3.E4.m1.3.3.2.2.1.2.cmml" xref="S3.E4.m1.3.3.2.2.1.2"></times><apply id="S3.E4.m1.3.3.2.2.1.3.cmml" xref="S3.E4.m1.3.3.2.2.1.3"><ci id="S3.E4.m1.3.3.2.2.1.3.1.cmml" xref="S3.E4.m1.3.3.2.2.1.3.1">^</ci><ci id="S3.E4.m1.3.3.2.2.1.3.2.cmml" xref="S3.E4.m1.3.3.2.2.1.3.2">𝜃</ci></apply><apply id="S3.E4.m1.3.3.2.2.1.1.1.1.cmml" xref="S3.E4.m1.3.3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.2.2.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.2.2.1.1.1.1.2">𝑟</ci><ci id="S3.E4.m1.3.3.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.2.2.1.1.1.1.3">𝑗</ci></apply></apply><apply id="S3.E4.m1.3.3.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.3"><apply id="S3.E4.m1.3.3.2.2.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.1.1.cmml" xref="S3.E4.m1.3.3.2.2.3.1">superscript</csymbol><apply id="S3.E4.m1.3.3.2.2.3.1.2.cmml" xref="S3.E4.m1.3.3.2.2.3.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.1.2.1.cmml" xref="S3.E4.m1.3.3.2.2.3.1">subscript</csymbol><sum id="S3.E4.m1.3.3.2.2.3.1.2.2.cmml" xref="S3.E4.m1.3.3.2.2.3.1.2.2"></sum><apply id="S3.E4.m1.3.3.2.2.3.1.2.3.cmml" xref="S3.E4.m1.3.3.2.2.3.1.2.3"><eq id="S3.E4.m1.3.3.2.2.3.1.2.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.1.2.3.1"></eq><ci id="S3.E4.m1.3.3.2.2.3.1.2.3.2.cmml" xref="S3.E4.m1.3.3.2.2.3.1.2.3.2">𝑠</ci><cn id="S3.E4.m1.3.3.2.2.3.1.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.2.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.3.3.2.2.3.1.3.cmml" xref="S3.E4.m1.3.3.2.2.3.1.3">𝑆</ci></apply><apply id="S3.E4.m1.3.3.2.2.3.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2"><apply id="S3.E4.m1.3.3.2.2.3.2.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.2.1.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1">superscript</csymbol><apply id="S3.E4.m1.3.3.2.2.3.2.1.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.2.1.2.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1">subscript</csymbol><sum id="S3.E4.m1.3.3.2.2.3.2.1.2.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.2.2"></sum><apply id="S3.E4.m1.3.3.2.2.3.2.1.2.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3"><eq id="S3.E4.m1.3.3.2.2.3.2.1.2.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.1"></eq><ci id="S3.E4.m1.3.3.2.2.3.2.1.2.3.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.2">𝑘</ci><cn id="S3.E4.m1.3.3.2.2.3.2.1.2.3.3.cmml" type="integer" xref="S3.E4.m1.3.3.2.2.3.2.1.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.3.3.2.2.3.2.1.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.2.1.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.3">superscript</csymbol><ci id="S3.E4.m1.3.3.2.2.3.2.1.3.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.3.2">𝐾</ci><ci id="S3.E4.m1.3.3.2.2.3.2.1.3.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.1.3.3">′</ci></apply></apply><apply id="S3.E4.m1.3.3.2.2.3.2.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2"><ci id="S3.E4.m1.3.3.2.2.3.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.1">⋅</ci><apply id="S3.E4.m1.3.3.2.2.3.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.2.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2">subscript</csymbol><ci id="S3.E4.m1.3.3.2.2.3.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2.2">𝜃</ci><apply id="S3.E4.m1.3.3.2.2.3.2.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3"><times id="S3.E4.m1.3.3.2.2.3.2.2.2.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.1"></times><ci id="S3.E4.m1.3.3.2.2.3.2.2.2.3.2.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.2">𝑠</ci><ci id="S3.E4.m1.3.3.2.2.3.2.2.2.3.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.2.3.3">𝑘</ci></apply></apply><apply id="S3.E4.m1.3.3.2.2.3.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.3.2.2.3.1.cmml" xref="S3.E4.m1.3.3.2.2.3.2.2.3">subscript</csymbol><cn id="S3.E4.m1.3.3.2.2.3.2.2.3.2.cmml" type="integer" xref="S3.E4.m1.3.3.2.2.3.2.2.3.2">1</cn><set id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"></eq><apply id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.2">𝑟</ci><apply id="S3.E4.m1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.3"><times id="S3.E4.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.2.3.1"></times><ci id="S3.E4.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2.3.2">𝑠</ci><ci id="S3.E4.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.2.3.3">𝑘</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.3.2">𝑟</ci><ci id="S3.E4.m1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></set></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">r_{j}^{*}=\underset{r_{j}}{\text{argmax}}\;\hat{\theta}(r_{j});\quad\hat{%
\theta}(r_{j})=\sum_{s=1}^{S}\sum_{k=1}^{K^{\prime}}\theta_{sk}\cdot\mathds{1}%
_{\{r_{sk}=r_{j}\}}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT = start_UNDERACCENT italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_UNDERACCENT start_ARG argmax end_ARG over^ start_ARG italic_θ end_ARG ( italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ; over^ start_ARG italic_θ end_ARG ( italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = ∑ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_s italic_k end_POSTSUBSCRIPT ⋅ blackboard_1 start_POSTSUBSCRIPT { italic_r start_POSTSUBSCRIPT italic_s italic_k end_POSTSUBSCRIPT = italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p1.8">In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.SS3.SSS2" title="5.3.2 Segment to Image Retrieval ‣ 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5.3.2</span></a>, we compare our similarity-weighted ranking with other alternatives based on frequency or similarity alone.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h3>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px1.1.1">Datasets</span>:</h6>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">VPR datasets are in abundance, as can be found in several benchmarks including VPR-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib77" title="">77</a>]</cite>, Deep Visual GeoLocalization Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib7" title="">7</a>]</cite>, and AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>. In this work, we used a variety of datasets covering both outdoor and indoor environments. Outdoor datasets include Pitts30k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib69" title="">69</a>]</cite>, AmsterTime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib75" title="">75</a>]</cite>, Mapillary Street Level Sequences (MSLS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib72" title="">72</a>]</cite>, SF-XL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite>, VPAir <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib62" title="">62</a>]</cite>, Revisted Oxford5K and Revisited Paris6k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib55" title="">55</a>]</cite> . Indoor datasets include Baidu Mall <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib67" title="">67</a>]</cite>, 17Places <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib80" title="">80</a>]</cite> and InsideOut <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib27" title="">27</a>]</cite>. Additional datasets-related details are provided in the supplementary.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.1.1">Evaluation and Benchmarking</span>:</h6>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.4">We evaluate our method as an image retrieval task using Recall@K metric, where top K<sup class="ltx_sup" id="S4.SS0.SSS0.Px2.p1.4.1"><span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.1.1">′</span></sup> (<math alttext="=50" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1"></eq><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2">absent</csymbol><cn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">=50</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">= 50</annotation></semantics></math>) retrieved segments per query segment are used to obtain top K (<math alttext="=5" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml"></mi><mo id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1"><eq id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1"></eq><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2">absent</csymbol><cn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">=5</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m3.1d">= 5</annotation></semantics></math>) images (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.E4" title="Equation 4 ‣ 3.4 Image Retrieval via Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a>). We compare against the most recent and high-performing VPR baseline methods. This includes CosPlace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite>, MixVPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>]</cite> and EigenPlaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>]</cite>, which are trained on large-scale urban datasets for VPR tasks. We further include two very recent state-of-the-art methods that use DINOv2 as the backbone. These include AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> which uses an <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.2">off-the-shelf</span> DINOv2 model and SALAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite> which uses a <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.3">finetuned</span> DINOv2 backbone.
Given the dichotomy between general-purpose VPR benchmarking of AnyLoc and the typical outdoor-focused benchmarking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib7" title="">7</a>]</cite>, we evaluated our method using two different backbones. <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.4">a)</span> <span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.p1.4.5">SegVLAD-PreT:</span> we use the same backbone and aggregation as AnyLoc, i.e., off-the-shelf <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.6">pretrained</span> DINOv2 (ViT-G) backbone with hard VLAD assignment, but the key difference is in the use of SuperSegments for our method as opposed to whole-image description of AnyLoc. <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.7">b)</span> <span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.p1.4.8">SegVLAD-FineT:</span> as our default aggregation method is VLAD, we use a <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.9">finetuned</span> DINOv2 (ViT-B) backbone which is similar to SALAD but we replace its aggregation layer with the original NetVLAD aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite> using 64 clusters, as described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib35" title="">35</a>]</cite>. We use this finetuned backbone with hard VLAD based assignment, similar to AnyLoc.
For both these models, we reduce the descriptor dimensions of the VLAD descriptor to <math alttext="1024" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><cn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m4.1d">1024</annotation></semantics></math> using PCA, as commonly done in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>. We train PCA transform in a map-specific manner using the database images of the dataset. Following AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>, we report results using two different sources of VLAD vocabulary: map-specific <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.10">(M)</span> and domain-specific <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.4.11">(D)</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h3>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We first present benchmark comparison of our method against state-of-the-art VPR methods.
This is followed by detailed analysis of our proposed aggregation technique. Lastly, we demonstrate results on a downstream task of Object-of-Interest (OOI) retrieval, showcasing the versatility of our method.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>State-of-the-art comparisons</h4>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S5.T1.3.2" style="font-size:90%;">Recall@1/5 benchmark comparison on outdoor street-view datasets.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.4" style="width:390.3pt;height:103.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-177.8pt,47.2pt) scale(0.523266057710479,0.523266057710479) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T1.4.1.1.1.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.2.1">Pitts-</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.3.1">MSLS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.4" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.4.1">MSLS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.5" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.5.1">SF-XL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.6" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.6.1">RO5k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.7" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.7.1">RO5k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.8.1">RP6k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.4.1.1.1.9" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.9.1">RP6k</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.2.2">
<td class="ltx_td" id="S5.T1.4.1.2.2.1" style="padding-left:18.0pt;padding-right:18.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.2.1">30K</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.3.1">SF</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.4" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.4.1">CPH</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.5" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.5.1">Val</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.6" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.6.1">Med</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.7" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.7.1">Hard</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.8.1">Med</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.2.2.9" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.2.2.9.1">Hard</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.1.3.3.1" style="padding-left:18.0pt;padding-right:18.0pt;">      CosPlace</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.2" style="padding-left:18.0pt;padding-right:18.0pt;">      90.4/95.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.3.3.3.1">93.4</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.3.3.3.2">97.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.4" style="padding-left:18.0pt;padding-right:18.0pt;">      84.9/92.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.5" style="padding-left:18.0pt;padding-right:18.0pt;">      94.6/97.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.6" style="padding-left:18.0pt;padding-right:18.0pt;">      85.7/87.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.7" style="padding-left:18.0pt;padding-right:18.0pt;">      27.1/45.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.8" style="padding-left:18.0pt;padding-right:18.0pt;">      94.3/95.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.3.9" style="padding-left:18.0pt;padding-right:18.0pt;">      7.1/15.7</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.4.4">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.4.4.1" style="padding-left:18.0pt;padding-right:18.0pt;">      MixVPR</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.2" style="padding-left:18.0pt;padding-right:18.0pt;">      91.5/95.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.3" style="padding-left:18.0pt;padding-right:18.0pt;">      91.3/95.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.4" style="padding-left:18.0pt;padding-right:18.0pt;">      87.1/92.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.5" style="padding-left:18.0pt;padding-right:18.0pt;">      87.8/93.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.6" style="padding-left:18.0pt;padding-right:18.0pt;">      68.6/80.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.7" style="padding-left:18.0pt;padding-right:18.0pt;">      32.9/54.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.8" style="padding-left:18.0pt;padding-right:18.0pt;">      94.3/100</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.4.4.9" style="padding-left:18.0pt;padding-right:18.0pt;">      10.0/32.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.5.5">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.5.5.1" style="padding-left:18.0pt;padding-right:18.0pt;">      EigenPlaces</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.2" style="padding-left:18.0pt;padding-right:18.0pt;">      92.6/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.5.5.2.1">96.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.5.5.3.1">92.6</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.5.5.3.2">97.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.4" style="padding-left:18.0pt;padding-right:18.0pt;">      87.1/92.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.5" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.5.5.5.1">96.4/98.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.6" style="padding-left:18.0pt;padding-right:18.0pt;">      85.7/88.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.7" style="padding-left:18.0pt;padding-right:18.0pt;">      42.8/57.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.8" style="padding-left:18.0pt;padding-right:18.0pt;">      95.7/98.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.5.5.9" style="padding-left:18.0pt;padding-right:18.0pt;">      4.3/11.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.6.6">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.6.6.1" style="padding-left:18.0pt;padding-right:18.0pt;">      AnyLoc</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.2" style="padding-left:18.0pt;padding-right:18.0pt;">      87.7/94.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.3" style="padding-left:18.0pt;padding-right:18.0pt;">      83.4/94.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.4" style="padding-left:18.0pt;padding-right:18.0pt;">      79.9/89.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.5" style="padding-left:18.0pt;padding-right:18.0pt;">      84.4/91.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.6" style="padding-left:18.0pt;padding-right:18.0pt;">      88.6/92.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.7" style="padding-left:18.0pt;padding-right:18.0pt;">      40.0/58.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.6.6.8.1">97.1</span>/100</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.6.6.9" style="padding-left:18.0pt;padding-right:18.0pt;">      11.4/44.3</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.7.7">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.7.7.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SALAD</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.2" style="padding-left:18.0pt;padding-right:18.0pt;">      92.6/96.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.3" style="padding-left:18.0pt;padding-right:18.0pt;">      91.7/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.7.7.3.1">97.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.4" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.7.7.4.1">92.3</span>/96.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.5" style="padding-left:18.0pt;padding-right:18.0pt;">      93.6/97.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.6" style="padding-left:18.0pt;padding-right:18.0pt;">      82.9/90.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.7" style="padding-left:18.0pt;padding-right:18.0pt;">      37.1/54.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.8" style="padding-left:18.0pt;padding-right:18.0pt;">      95.7/98.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.7.7.9" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.7.7.9.1">14.3</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.7.7.9.2">58.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.1.8.8.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.8.8.1.1">SegVLAD-PreT</span> (D)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.2" style="padding-left:18.0pt;padding-right:18.0pt;">      86.7/94.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.3" style="padding-left:18.0pt;padding-right:18.0pt;">      88.4/94.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.4" style="padding-left:18.0pt;padding-right:18.0pt;">      81.7/90.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.5" style="padding-left:18.0pt;padding-right:18.0pt;">      90.9/96.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.6" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.8.8.6.1">90.0</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.8.8.6.2">97.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.7" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.8.8.7.1">47.1</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.8.8.7.2">72.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.8.8.8.1">98.5</span>/100</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.8.8.9" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.8.8.9.1">18.6</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.8.8.9.2">55.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.9.9">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.9.9.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.9.9.1.1">SegVLAD-PreT</span> (M)</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.2" style="padding-left:18.0pt;padding-right:18.0pt;">      83.9/93.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.3" style="padding-left:18.0pt;padding-right:18.0pt;">      81.0/92.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.4" style="padding-left:18.0pt;padding-right:18.0pt;">      76.1/89.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.5" style="padding-left:18.0pt;padding-right:18.0pt;">      90.2/95.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.6" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.9.9.6.1">92.9</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.9.9.6.2">95.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.7" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.9.9.7.1">61.4</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.9.9.7.2">81.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.9.9.8.1">97.1</span>/100</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.9.9.9" style="padding-left:18.0pt;padding-right:18.0pt;">      11.4/40.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.10.10">
<td class="ltx_td ltx_align_left" id="S5.T1.4.1.10.10.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.10.10.1.1">SegVLAD-FineT</span> (D)</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.2.1">92.9</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.10.10.2.2">96.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.10.10.3.1">93.4</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.3.2">97.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.4" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.4.1">91.8</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.4.2">96.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.5" style="padding-left:18.0pt;padding-right:18.0pt;">      94.2/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.5.1">97.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.6" style="padding-left:18.0pt;padding-right:18.0pt;">      82.9/92.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.7" style="padding-left:18.0pt;padding-right:18.0pt;">      40.0/60.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.10.10.8.1">97.1</span>/100</td>
<td class="ltx_td ltx_align_center" id="S5.T1.4.1.10.10.9" style="padding-left:18.0pt;padding-right:18.0pt;">      5.7/52.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.1.11.11.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.11.11.1.1">SegVLAD-FineT</span> (M)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S5.T1.4.1.11.11.2.1">93.1</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.11.11.2.2">96.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.3" style="padding-left:18.0pt;padding-right:18.0pt;">      92.2/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.11.11.3.1">97.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.4" style="padding-left:18.0pt;padding-right:18.0pt;">      91.6/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.11.11.4.1">96.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.5" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.1.11.11.5.1">95.6</span>/<span class="ltx_text ltx_font_bold" id="S5.T1.4.1.11.11.5.2">98.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.6" style="padding-left:18.0pt;padding-right:18.0pt;">      84.3/91.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.7" style="padding-left:18.0pt;padding-right:18.0pt;">      44.3/61.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.8" style="padding-left:18.0pt;padding-right:18.0pt;">      95.7/100</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.4.1.11.11.9" style="padding-left:18.0pt;padding-right:18.0pt;">      10.0/57.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">Recall@1/5 benchmark comparison on ‘out-of-distribution’ datasets.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.4" style="width:281.9pt;height:110.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-89.5pt,35.0pt) scale(0.611461746383129,0.611461746383129) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.4.1.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.2" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.2.1">Baidu</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.3" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.3.1">AmsterTime</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.4" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.4.1">InsideOut</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.5" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.5.1">17Places</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.4.1.1.1.6" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.6.1">VPAir</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.4.1.2.2.1" style="padding-left:12.0pt;padding-right:12.0pt;">CosPlace</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.2.2.2" style="padding-left:12.0pt;padding-right:12.0pt;">41.6/55.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.2.2.3" style="padding-left:12.0pt;padding-right:12.0pt;">47.7/69.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.2.2.4" style="padding-left:12.0pt;padding-right:12.0pt;">0.2/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.2.2.5" style="padding-left:12.0pt;padding-right:12.0pt;">81.3/88.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.2.2.6" style="padding-left:12.0pt;padding-right:12.0pt;">4.6/13.7</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.3.3.1" style="padding-left:12.0pt;padding-right:12.0pt;">MixVPR</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.3.3.2" style="padding-left:12.0pt;padding-right:12.0pt;">64.4/80.3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.3.3.3" style="padding-left:12.0pt;padding-right:12.0pt;">40.2/59.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.3.3.4" style="padding-left:12.0pt;padding-right:12.0pt;">0.0/1.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.3.3.5" style="padding-left:12.0pt;padding-right:12.0pt;">85.2/90.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.3.3.6" style="padding-left:12.0pt;padding-right:12.0pt;">6.8/16.1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.4.4.1" style="padding-left:12.0pt;padding-right:12.0pt;">EigenPlaces</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.4.4.2" style="padding-left:12.0pt;padding-right:12.0pt;">56.5/72.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.4.4.3" style="padding-left:12.0pt;padding-right:12.0pt;">48.9/69.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.4.4.4" style="padding-left:12.0pt;padding-right:12.0pt;">0.4/1.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.4.4.5" style="padding-left:12.0pt;padding-right:12.0pt;">83.0/90.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.4.4.6" style="padding-left:12.0pt;padding-right:12.0pt;">6.5/17.9</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.5.5.1" style="padding-left:12.0pt;padding-right:12.0pt;">AnyLoc</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.5.5.2" style="padding-left:12.0pt;padding-right:12.0pt;">75.2/87.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.5.5.3" style="padding-left:12.0pt;padding-right:12.0pt;">50.3/73.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.5.5.4" style="padding-left:12.0pt;padding-right:12.0pt;">2.4/8.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.5.5.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.5.5.5.1">95.3</span>/97.3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.5.5.6" style="padding-left:12.0pt;padding-right:12.0pt;">66.7/79.2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.6.6.1" style="padding-left:12.0pt;padding-right:12.0pt;">SALAD</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.6.6.2" style="padding-left:12.0pt;padding-right:12.0pt;">74.8/86.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.6.6.3" style="padding-left:12.0pt;padding-right:12.0pt;">55.4/75.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.6.6.4" style="padding-left:12.0pt;padding-right:12.0pt;">0.6/1.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.6.6.5" style="padding-left:12.0pt;padding-right:12.0pt;">82.5/88.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.6.6.6" style="padding-left:12.0pt;padding-right:12.0pt;">25.8/38.7</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.4.1.7.7.1" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.7.7.1.1">SegVLAD-PreT</span> (D)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.7.7.2" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.7.7.2.1">78.5</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.7.7.2.2">93.8</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.7.7.3" style="padding-left:12.0pt;padding-right:12.0pt;">48.3/72.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.7.7.4" style="padding-left:12.0pt;padding-right:12.0pt;">4.2/9.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.7.7.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.7.7.5.1">95.3</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.7.7.5.2">98.0</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.1.7.7.6" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.7.7.6.1">69.8</span>/<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.7.7.6.2">83.7</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.8.8.1" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.8.8.1.1">SegVLAD-PreT</span> (M)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.8.8.2" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.8.8.2.1">80.4</span>/<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.8.8.2.2">94.0</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.8.8.3" style="padding-left:12.0pt;padding-right:12.0pt;">54.3/76.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.8.8.4" style="padding-left:12.0pt;padding-right:12.0pt;">3.2/10.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.8.8.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.8.8.5.1">95.3</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.8.8.5.2">98.0</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.8.8.6" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.8.8.6.1">67.2</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.8.8.6.2">82.8</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.1.9.9.1" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.9.9.1.1">SegVLAD-FineT</span> (D)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.9.9.2" style="padding-left:12.0pt;padding-right:12.0pt;">69.9/89.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.9.9.3" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.9.9.3.1">56.7</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.9.9.3.2">76.8</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.9.9.4" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.9.9.4.1">7.0</span>/<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.4.1.9.9.4.2">14.0</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.9.9.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.9.9.5.1">95.3/</span>97.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.1.9.9.6" style="padding-left:12.0pt;padding-right:12.0pt;">33.9/52.4</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.4.1.10.10.1" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.1.1">SegVLAD-FineT</span> (M)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.10.10.2" style="padding-left:12.0pt;padding-right:12.0pt;">69.7/90.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.10.10.3" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.3.1">60.2</span>/<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.3.2">78.2</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.10.10.4" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.4.1">7.2</span>/<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.4.2">17.2</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.10.10.5" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.5.1">95.3</span>/<span class="ltx_text ltx_font_bold" id="S5.T2.4.1.10.10.5.2">98.3</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.1.10.10.6" style="padding-left:12.0pt;padding-right:12.0pt;">34.2/52.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T1" title="Table 1 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> presents Recall@1/5 comparison against state-of-the-art VPR methods on standard outdoor street-view datasets, which are similar to the typical training datasets used for VPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T2" title="Table 2 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a> covers ‘out-of-distribution’ datasets, inspired by AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>, covering indoor environments (Baidu Mall and 17 Places), aerial imagery (VPAir), indoor-to-outdoor viewing (InsideOut), and historical image matching (AmsterTime). Below, we discuss two key aspects of this comparative analysis: <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">i)</span> how our segment-based approach compares against whole-image global descriptor based methods, and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">ii)</span> how performance trends vary depending on the choices of feature backbone with regards to task-specific (VPR) training.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Aggregating Segments vs Whole Images</h5>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T1" title="Table 1 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T2" title="Table 2 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a> show that our proposed method SegVLAD achieves a new state-of-the-art on the majority of datasets, considering both the backbone variants: PreT and FineT. AnyLoc and SALAD respectively differ from SegVLAD-PreT and SegVLAD-FineT in terms of the aggregation scope (global vs segments). Thus, the superior performance of SegVLAD clearly highlights the role of segments based retrieval over whole-image based approach. On the Baidu Mall dataset – highly-aliased indoor environment – our method (pre-trained) improves over AnyLoc by <math alttext="3-5\%" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.1.m1.1"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><mrow id="S5.SS1.SSS1.p1.1.m1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS1.p1.1.m1.1.1.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml">3</mn><mo id="S5.SS1.SSS1.p1.1.m1.1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S5.SS1.SSS1.p1.1.m1.1.1.3" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml"><mn id="S5.SS1.SSS1.p1.1.m1.1.1.3.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.2.cmml">5</mn><mo id="S5.SS1.SSS1.p1.1.m1.1.1.3.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.1.m1.1b"><apply id="S5.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1"><minus id="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.1"></minus><cn id="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS1.p1.1.m1.1.1.2">3</cn><apply id="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS1.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.1">percent</csymbol><cn id="S5.SS1.SSS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.1.m1.1c">3-5\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.1.m1.1d">3 - 5 %</annotation></semantics></math> for R@1 and around <math alttext="6\%" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.2.m2.1"><semantics id="S5.SS1.SSS1.p1.2.m2.1a"><mrow id="S5.SS1.SSS1.p1.2.m2.1.1" xref="S5.SS1.SSS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.SSS1.p1.2.m2.1.1.2" xref="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml">6</mn><mo id="S5.SS1.SSS1.p1.2.m2.1.1.1" xref="S5.SS1.SSS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.2.m2.1b"><apply id="S5.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS1.p1.2.m2.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.2.m2.1c">6\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.2.m2.1d">6 %</annotation></semantics></math> for R@5 in absolute gains. On the InsideOut dataset – matching outdoor images viewed from within indoors – our method leads to a ‘meaningful’ recall, unlike all other baselines. Overall, these results highlight that even with the use of powerful image encoders (DINOv2), global aggregation struggles to deal with the challenges of matching images across major viewpoint shifts – it is thus the partial image representation and matching which is needed to obtain superior recognition performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>VPR Fine-tuned Encoders + Segments</h5>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">For SegVLAD-FineT, we used a DINOv2 backbone finetuned for the purpose of VPR, mainly to observe the benefit of segments over global descriptor based approach in a <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p1.1.1">task-specific manner</span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T1" title="Table 1 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T2" title="Table 2 ‣ 5.1 State-of-the-art comparisons ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a> show that, on the outdoor street-view datasets, SALAD (finetuned DINOv2) generally performs better than AnyLoc (its pretrained counterpart), whereas the latter generally outperforms the former on ‘out-of-distribution’ datasets. It can be clearly observed that these performance patterns translate well from global- to segment-level results.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Revisiting Objects of Interest (OOI): Object Instance Retrieval</h4>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">A typical requirement of an embodied agent is to understand the context of its task through its memory/map information, which is composed of visual and/or semantic cues. For example, navigating to a given object goal requires a robot to visually recognize the goal and not be confused by perceptually-similar items. In this section, we demonstrate our method’s ability to retrieve the correct image given just an Object Of Interest (OOI) as a query segment. For this purpose, we use an extended version of the Baidu dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib73" title="">73</a>]</cite> which annotates OOI as various discriminative areas that can be reliably detected under variable viewpoint and lighting conditions. In total, there are 220 OOI, which cover various things such as logos, brand names, posters, etc., in a highly cluttered mall environment. To cast this dataset in terms of revisiting things, we use the original query images of the Baidu Mall <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib67" title="">67</a>]</cite> dataset as the database and the images with OOI as the queries. This allows us to evaluate the OOIs directly. This is similar to VPR evaluation of recall in terms of image retrieval but with querying of a specific segment instead of using all the segments of the query image.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Recall@1 results for various approaches on Object-Instance Retrieval Task</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.4.1.1.1" style="padding-left:24.0pt;padding-right:24.0pt;">        <span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.1.2" style="padding-left:24.0pt;padding-right:24.0pt;">        SegVLAD NoNbrAgg</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.1.3" style="padding-left:24.0pt;padding-right:24.0pt;">        SegVLAD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.1.1.4" style="padding-left:24.0pt;padding-right:24.0pt;">        Segment-to-Global</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.1.5" style="padding-left:24.0pt;padding-right:24.0pt;">        Global-to-Global</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.2.1" style="padding-left:24.0pt;padding-right:24.0pt;">        <span class="ltx_text ltx_font_bold" id="S5.T3.4.2.2.1.1">R@1</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.2.2" style="padding-left:24.0pt;padding-right:24.0pt;">        64.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.2.3" style="padding-left:24.0pt;padding-right:24.0pt;">        <span class="ltx_text ltx_font_bold" id="S5.T3.4.2.2.3.1">92.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.4.2.2.4" style="padding-left:24.0pt;padding-right:24.0pt;">        30.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.4.2.2.5" style="padding-left:24.0pt;padding-right:24.0pt;">        86.4</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We consider four different methods of recognizing known objects in this study. i) <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.1">Global-to-Global</span>: as a baseline method, we use whole images to represent and retrieve, i.e., without using the OOI mask; this resembles object-goal recognition problem for an InstanceImageNavigation task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib39" title="">39</a>]</cite>. ii) <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.2">Segment-to-Global</span>: this is the same as the previous setting except that the query image descriptor is aggregated only using the OOI mask; this tests the ability of the image encoder/aggregator to match segment-level descriptor against global descriptors. iii) <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.3">SegVLAD</span> and iv) <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.4">SegVLAD NoNbrAgg</span>, which are our proposed methods but the latter does not use any neighborhood information; this highlights the relevance of spatial context around the OOI for recognition. For SegVLAD, we create a virtual segment mask for the OOI, append it to the other masks of the image, and then perform our neighborhood expansion and feature aggregation, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3" title="3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T3" title="Table 3 ‣ 5.2 Revisiting Objects of Interest (OOI): Object Instance Retrieval ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a> reports Recall@1 for different recognition methods. It can be observed that <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.1">SegVLAD</span> outperforms <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.2">Global-to-Global</span> matching by a large margin, which shows that recognizing specific object instances through their images (as in InstanceImageNav) is more prone to failures. It can further be observed from low recall of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.3">SegVLAD NoNbrAgg</span> that neighborhood aggregation around segments is crucial to capture the required context. Finally, poor recall of <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p3.1.4">Segment-to-Global</span> highlights that matching a part of an image (OOI) with the whole image is not a viable solution for object instance recognition.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T5.2" style="width:433.6pt;height:124.5pt;vertical-align:-116.9pt;"><span class="ltx_transformed_inner" style="transform:translate(16.2pt,-0.3pt) scale(1.08065722768799,1.08065722768799) ;">
<figure class="ltx_figure ltx_minipage ltx_align_top" id="S5.T5.1.fig1" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.1.fig1.1.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T5.1.fig1.2.2" style="font-size:90%;">Recall@1/5 for Baidu mall dataset for different aggregation methods and different orders of neighborhood expansion.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1.fig1.3" style="width:346.9pt;height:105.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(25.1pt,-7.6pt) scale(1.16909684889502,1.16909684889502) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_top" id="S5.T5.1.fig1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.fig1.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.fig1.3.1.1.1.1" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.1.1.1.1">Order</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.fig1.3.1.1.1.2" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.1.1.2.1">SegVLAD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.fig1.3.1.1.1.3" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.1.1.3.1">SAP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.fig1.3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.fig1.3.1.2.1.1" style="padding-left:30.0pt;padding-right:30.0pt;">         0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.fig1.3.1.2.1.2" style="padding-left:30.0pt;padding-right:30.0pt;">         73.1/89.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.fig1.3.1.2.1.3" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.2.1.3.1"> 74.6</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.2.1.3.2">91.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.fig1.3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.1.fig1.3.1.3.2.1" style="padding-left:30.0pt;padding-right:30.0pt;">         1</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.fig1.3.1.3.2.2" style="padding-left:30.0pt;padding-right:30.0pt;">         77.4/91.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.fig1.3.1.3.2.3" style="padding-left:30.0pt;padding-right:30.0pt;">         65.6/87.2</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.fig1.3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.1.fig1.3.1.4.3.1" style="padding-left:30.0pt;padding-right:30.0pt;">         2</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.fig1.3.1.4.3.2" style="padding-left:30.0pt;padding-right:30.0pt;">         76.3/92.4</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.fig1.3.1.4.3.3" style="padding-left:30.0pt;padding-right:30.0pt;">         53.2/81.3</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.fig1.3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.fig1.3.1.5.4.1" style="padding-left:30.0pt;padding-right:30.0pt;">         3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.fig1.3.1.5.4.2" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.5.4.2.1">77.7</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.1.fig1.3.1.5.4.2.2">92.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.fig1.3.1.5.4.3" style="padding-left:30.0pt;padding-right:30.0pt;">         49.8/78.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure ltx_minipage ltx_align_top" id="S5.T5.2.fig2" style="width:195.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.T5.2.fig2.1.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.2.fig2.2.2" style="font-size:90%;">Recall@1/5 comparison between different methods for ranking images based on segment-level retrieval.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T5.2.fig2.3" style="width:390.3pt;height:88.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.1pt,-8.2pt) scale(1.22668106696296,1.22668106696296) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_top" id="S5.T5.2.fig2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.2.fig2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.2.fig2.3.1.1.1.1" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.2.fig2.3.1.1.1.2" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.1.1.2.1">Baidu</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.2.fig2.3.1.1.1.3" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.1.1.3.1">AmsterTime</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.2.fig2.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.2.fig2.3.1.2.1.1" style="padding-left:30.0pt;padding-right:30.0pt;">         Max Seg</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.fig2.3.1.2.1.2" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.2.1.2.1">78.5</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.2.1.2.2">93.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.fig2.3.1.2.1.3" style="padding-left:30.0pt;padding-right:30.0pt;">         53.9/70.4</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.fig2.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.fig2.3.1.3.2.1" style="padding-left:30.0pt;padding-right:30.0pt;">         Max Sim</th>
<td class="ltx_td ltx_align_center" id="S5.T5.2.fig2.3.1.3.2.2" style="padding-left:30.0pt;padding-right:30.0pt;">         65.2/92.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.fig2.3.1.3.2.3" style="padding-left:30.0pt;padding-right:30.0pt;">         34.4/62.4</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.fig2.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.2.fig2.3.1.4.3.1" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T5.2.fig2.3.1.4.3.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.2.fig2.3.1.4.3.2" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.4.3.2.1">78.5</span>/93.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.2.fig2.3.1.4.3.3" style="padding-left:30.0pt;padding-right:30.0pt;">         <span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.4.3.3.1">54.4</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.2.fig2.3.1.4.3.3.2">76.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h4>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Aggregation method &amp; Order of Neighborhood Expansion</h5>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Previous studies on VPR such as AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> have shown VLAD to be better than other aggregation methods for whole-image based global descriptors. However, in an increasing number of segment-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite>, segment <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.1">average</span> pooling (SAP) is used more commonly. Thus, we compare hard-assignment VLAD against SAP on Baidu dataset. For SAP, we upsample the DINOv2 features to match the resolution of our SAM masks – this upsampling is shown to enhance performance in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite>. For VLAD aggregation, we use our proposed method SegVLAD, where we downsample masks to match with the low resolution of DINOv2. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T5" title="Table 5 ‣ 5.2 Revisiting Objects of Interest (OOI): Object Instance Retrieval ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> shows that SAP performs well for order 0 aggregation (i.e., no neighborhood aggregation) but its performance reduces as the neighborhood expands. On the other hand, SegVLAD has low recall when no neighborhood is considered but benefits significantly even with its immediate neighborhood (order 1). We attribute these inverted trends of SegVLAD and SAP to the very nature of these aggregation methods: as more information becomes available SAP smooths out the overall information content whereas SegVLAD benefits from additional information which gets distributed across its clusters, thus minimizing any possible smoothing effect. It can be observed that R@5 increases for SegVLAD with an increasing order of neighborhood expansion but margins diminish for higher orders. Overall, SegVLAD (order 3) achieves the best results, despite aggregating at a <math alttext="14\times" class="ltx_math_unparsed" display="inline" id="S5.SS3.SSS1.p1.1.m1.1"><semantics id="S5.SS3.SSS1.p1.1.m1.1a"><mrow id="S5.SS3.SSS1.p1.1.m1.1b"><mn id="S5.SS3.SSS1.p1.1.m1.1.1">14</mn><mo id="S5.SS3.SSS1.p1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.1.m1.1c">14\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS1.p1.1.m1.1d">14 ×</annotation></semantics></math> lower resolution than SAP’s upsampling based aggregation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Segment to Image Retrieval</h5>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Unlike conventional global descriptor retrieval based VPR, we perform retrieval for multiple SuperSegments of the query image. To obtain retrieval output in terms of images (as that is what VPR is typically evaluated on), there exist multiple ways to combine the top segment-level matches across all the query segments. We consider the following alternative options. i) <span class="ltx_text ltx_font_typewriter" id="S5.SS3.SSS2.p1.1.1">MaxSeg</span>: we obtain the best matching segment for each query segment, associate the matched segments to their respective reference image indices, and then rank these image indices based on their frequency; this method <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS2.p1.1.2">weakly</span> resembles an inverted index list based counting of common segments between the query and the reference. ii) <span class="ltx_text ltx_font_typewriter" id="S5.SS3.SSS2.p1.1.3">MaxSim</span>: across the best matching reference segments, we order their image indices based on the segment similarity; this method is similar to that used in MultiVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib4" title="">4</a>]</cite>. iii) <span class="ltx_text ltx_font_typewriter" id="S5.SS3.SSS2.p1.1.4">Similarity-Weighted Frequency</span>: this is our proposed method as defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S3.SS4" title="3.4 Image Retrieval via Segments ‣ 3 Proposed Approach ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.T5" title="Table 5 ‣ 5.2 Revisiting Objects of Interest (OOI): Object Instance Retrieval ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> shows that our proposed method for combining segment-level hypotheses consistently achieves superior results for both the datasets. While MaxSeg achieves a similar performance on Baidu, it suffers a drop in recall for AmsterTime. Both the methods outperform MaxSim at R@1 by a large margin.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.F4.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F4.3.3">
<td class="ltx_td ltx_align_center" id="S5.F4.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="99" id="S5.F4.1.1.1.g1" src="extracted/5879622/qual_single/single_cs_query_idx_00005_seg_id_00002.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.2.2.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.2.2.2.g1" src="extracted/5879622/qual_single/match_cs_query_idx_00005_seg_id_00002.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.3.3.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.3.3.3.g1" src="extracted/5879622/qual_single/anyloc_match_query_idx_00005_seg_id_00002.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.6.6">
<td class="ltx_td ltx_align_center" id="S5.F4.4.4.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="101" id="S5.F4.4.4.1.g1" src="extracted/5879622/qual_single/cropped_single_cs_query_idx_00019_seg_id_00001.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.5.5.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="120" id="S5.F4.5.5.2.g1" src="extracted/5879622/qual_single/cropped_match_cs_query_idx_00019_seg_id_00001.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.6.6.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.6.6.3.g1" src="extracted/5879622/qual_single/anyloc_match_query_idx_00019_seg_id_00001.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="S5.F4.9.9">
<td class="ltx_td ltx_align_center" id="S5.F4.7.7.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.7.7.1.g1" src="extracted/5879622/qual_single/single_cs_query_idx_02143_seg_id_00010.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.8.8.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.8.8.2.g1" src="extracted/5879622/qual_single/match_cs_query_idx_02143_seg_id_00010.png" width="180"/></td>
<td class="ltx_td ltx_align_center" id="S5.F4.9.9.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="135" id="S5.F4.9.9.3.g1" src="extracted/5879622/qual_single/anyloc_cs_match_query_idx_02143_seg_id_00010.png" width="180"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.11.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.12.2" style="font-size:90%;">Qualitative results: Columns respectively represent the query image, correct match of SegVLAD and incorrect match of AnyLoc. Examples from different datasets: AmsterTime, Baidu Mall, Pitts-30K are presented across the rows. </span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h5 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Patches vs Segment</h5>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">While open-set segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib37" title="">37</a>]</cite> is aimed at a meaningful segregation of visual entities, a simple alternative to our segment-based retrieval is to use uniformly defined regions/patches. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.F5" title="Figure 5 ‣ 5.3.3 Patches vs Segment ‣ 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> compares SegVLAD with a patch-based approach, where we consider arbitrary square patch sizes to segment an image. It can be observed that SegVLAD outperforms its patch-based counterparts, where smaller patches perform the worst, and for larger patches, R@1 saturates while R@5 reduces. These results are also in line with similar findings of a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib64" title="">64</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F5.1" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="323" id="S5.F5.1.g1" src="extracted/5879622/figures/r1_vs_time_amstertime.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.1.1.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.1.2.2" style="font-size:90%;">Recall vs storage/retrieval time on AmsterTime.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F5.5" style="width:208.1pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.4.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.2.1" style="font-size:90%;">Patch vs Segments on AmsterTime (Reso. 256<math alttext="\times" class="ltx_Math" display="inline" id="S5.T6.2.1.m1.1"><semantics id="S5.T6.2.1.m1.1b"><mo id="S5.T6.2.1.m1.1.1" xref="S5.T6.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.1.m1.1c"><times id="S5.T6.2.1.m1.1.1.cmml" xref="S5.T6.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.1.m1.1e">×</annotation></semantics></math>256)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.F5.5.4" style="width:390.3pt;height:179pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.4pt,-35.5pt) scale(1.65713230513248,1.65713230513248) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.F5.5.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.F5.5.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F5.5.4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.5.1.1.1">Patch Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F5.5.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.5.1.2.1">NumSegDb</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F5.5.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.5.1.3.1">NumSegIm</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F5.5.4.4.5.1.4"><span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.5.1.4.1">R@1/5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F5.2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.2.1.1.1.1">16<math alttext="\times" class="ltx_Math" display="inline" id="S5.F5.2.1.1.1.1.m1.1"><semantics id="S5.F5.2.1.1.1.1.m1.1a"><mo id="S5.F5.2.1.1.1.1.m1.1.1" xref="S5.F5.2.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.F5.2.1.1.1.1.m1.1b"><times id="S5.F5.2.1.1.1.1.m1.1.1.cmml" xref="S5.F5.2.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.2.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.F5.2.1.1.1.1.m1.1d">×</annotation></semantics></math>16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.2.1.1.1.2">315136</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.2.1.1.1.3">256</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.2.1.1.1.4">35.7/62.1</td>
</tr>
<tr class="ltx_tr" id="S5.F5.3.2.2.2">
<td class="ltx_td ltx_align_center" id="S5.F5.3.2.2.2.1">32<math alttext="\times" class="ltx_Math" display="inline" id="S5.F5.3.2.2.2.1.m1.1"><semantics id="S5.F5.3.2.2.2.1.m1.1a"><mo id="S5.F5.3.2.2.2.1.m1.1.1" xref="S5.F5.3.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.F5.3.2.2.2.1.m1.1b"><times id="S5.F5.3.2.2.2.1.m1.1.1.cmml" xref="S5.F5.3.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.F5.3.2.2.2.1.m1.1d">×</annotation></semantics></math>32</td>
<td class="ltx_td ltx_align_center" id="S5.F5.3.2.2.2.2"> 78784</td>
<td class="ltx_td ltx_align_center" id="S5.F5.3.2.2.2.3"> 64</td>
<td class="ltx_td ltx_align_center" id="S5.F5.3.2.2.2.4">45.9/72.5</td>
</tr>
<tr class="ltx_tr" id="S5.F5.4.3.3.3">
<td class="ltx_td ltx_align_center" id="S5.F5.4.3.3.3.1">64<math alttext="\times" class="ltx_Math" display="inline" id="S5.F5.4.3.3.3.1.m1.1"><semantics id="S5.F5.4.3.3.3.1.m1.1a"><mo id="S5.F5.4.3.3.3.1.m1.1.1" xref="S5.F5.4.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.F5.4.3.3.3.1.m1.1b"><times id="S5.F5.4.3.3.3.1.m1.1.1.cmml" xref="S5.F5.4.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.4.3.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.F5.4.3.3.3.1.m1.1d">×</annotation></semantics></math>64</td>
<td class="ltx_td ltx_align_center" id="S5.F5.4.3.3.3.2"> 19696</td>
<td class="ltx_td ltx_align_center" id="S5.F5.4.3.3.3.3">  16</td>
<td class="ltx_td ltx_align_center" id="S5.F5.4.3.3.3.4">52.0/75.1</td>
</tr>
<tr class="ltx_tr" id="S5.F5.5.4.4.4">
<td class="ltx_td ltx_align_center" id="S5.F5.5.4.4.4.1">128<math alttext="\times" class="ltx_Math" display="inline" id="S5.F5.5.4.4.4.1.m1.1"><semantics id="S5.F5.5.4.4.4.1.m1.1a"><mo id="S5.F5.5.4.4.4.1.m1.1.1" xref="S5.F5.5.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.F5.5.4.4.4.1.m1.1b"><times id="S5.F5.5.4.4.4.1.m1.1.1.cmml" xref="S5.F5.5.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.5.4.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.F5.5.4.4.4.1.m1.1d">×</annotation></semantics></math>128</td>
<td class="ltx_td ltx_align_center" id="S5.F5.5.4.4.4.2">   4924</td>
<td class="ltx_td ltx_align_center" id="S5.F5.5.4.4.4.3">   4</td>
<td class="ltx_td ltx_align_center" id="S5.F5.5.4.4.4.4">53.5/65.6</td>
</tr>
<tr class="ltx_tr" id="S5.F5.5.4.4.6.1">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.F5.5.4.4.6.1.1">SegVLAD</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.F5.5.4.4.6.1.2">129637</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.F5.5.4.4.6.1.3">105</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.F5.5.4.4.6.1.4">
<span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.6.1.4.1">56.8</span>/<span class="ltx_text ltx_font_bold" id="S5.F5.5.4.4.6.1.4.2">77.7</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Limitations, Storage, Compute Time &amp; IOU Based Filtering</h4>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.9">A key limitation of our method is its large map size, i.e., large storage requirement for segment-level descriptors (see supplementary for further details).
In this section, we analyze the resource requirements in terms of database (index) storage and query retrieval time for our method, along with a preliminary study on IOU (Intersection over Union) based filtering of SuperSegments to reduce such costs. We compute IOU between all pairs of SuperSegments in a given image, and remove segments with <math alttext="IOU(s_{i},s_{j})&gt;\psi\;\forall\;i\in[1,S],\,j\in[i,S]" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.6"><semantics id="S5.SS4.p1.1.m1.6a"><mrow id="S5.SS4.p1.1.m1.6.6.2" xref="S5.SS4.p1.1.m1.6.6.3.cmml"><mrow id="S5.SS4.p1.1.m1.5.5.1.1" xref="S5.SS4.p1.1.m1.5.5.1.1.cmml"><mrow id="S5.SS4.p1.1.m1.5.5.1.1.2" xref="S5.SS4.p1.1.m1.5.5.1.1.2.cmml"><mi id="S5.SS4.p1.1.m1.5.5.1.1.2.4" xref="S5.SS4.p1.1.m1.5.5.1.1.2.4.cmml">I</mi><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.3" xref="S5.SS4.p1.1.m1.5.5.1.1.2.3.cmml">⁢</mo><mi id="S5.SS4.p1.1.m1.5.5.1.1.2.5" xref="S5.SS4.p1.1.m1.5.5.1.1.2.5.cmml">O</mi><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.3a" xref="S5.SS4.p1.1.m1.5.5.1.1.2.3.cmml">⁢</mo><mi id="S5.SS4.p1.1.m1.5.5.1.1.2.6" xref="S5.SS4.p1.1.m1.5.5.1.1.2.6.cmml">U</mi><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.3b" xref="S5.SS4.p1.1.m1.5.5.1.1.2.3.cmml">⁢</mo><mrow id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.3.cmml"><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.3" stretchy="false" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.3.cmml">(</mo><msub id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.2" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.2.cmml">s</mi><mi id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.3" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.4" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.3.cmml">,</mo><msub id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.2" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.2.cmml">s</mi><mi id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.3" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.3.cmml">j</mi></msub><mo id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.5" stretchy="false" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.SS4.p1.1.m1.5.5.1.1.4" xref="S5.SS4.p1.1.m1.5.5.1.1.4.cmml">&gt;</mo><mrow id="S5.SS4.p1.1.m1.5.5.1.1.5" xref="S5.SS4.p1.1.m1.5.5.1.1.5.cmml"><mi id="S5.SS4.p1.1.m1.5.5.1.1.5.2" xref="S5.SS4.p1.1.m1.5.5.1.1.5.2.cmml">ψ</mi><mo id="S5.SS4.p1.1.m1.5.5.1.1.5.1" lspace="0.447em" xref="S5.SS4.p1.1.m1.5.5.1.1.5.1.cmml">⁢</mo><mrow id="S5.SS4.p1.1.m1.5.5.1.1.5.3" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3.cmml"><mo id="S5.SS4.p1.1.m1.5.5.1.1.5.3.1" rspace="0.447em" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3.1.cmml">∀</mo><mi id="S5.SS4.p1.1.m1.5.5.1.1.5.3.2" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3.2.cmml">i</mi></mrow></mrow><mo id="S5.SS4.p1.1.m1.5.5.1.1.6" xref="S5.SS4.p1.1.m1.5.5.1.1.6.cmml">∈</mo><mrow id="S5.SS4.p1.1.m1.5.5.1.1.7.2" xref="S5.SS4.p1.1.m1.5.5.1.1.7.1.cmml"><mo id="S5.SS4.p1.1.m1.5.5.1.1.7.2.1" stretchy="false" xref="S5.SS4.p1.1.m1.5.5.1.1.7.1.cmml">[</mo><mn id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">1</mn><mo id="S5.SS4.p1.1.m1.5.5.1.1.7.2.2" xref="S5.SS4.p1.1.m1.5.5.1.1.7.1.cmml">,</mo><mi id="S5.SS4.p1.1.m1.2.2" xref="S5.SS4.p1.1.m1.2.2.cmml">S</mi><mo id="S5.SS4.p1.1.m1.5.5.1.1.7.2.3" stretchy="false" xref="S5.SS4.p1.1.m1.5.5.1.1.7.1.cmml">]</mo></mrow></mrow><mo id="S5.SS4.p1.1.m1.6.6.2.3" rspace="0.337em" xref="S5.SS4.p1.1.m1.6.6.3a.cmml">,</mo><mrow id="S5.SS4.p1.1.m1.6.6.2.2" xref="S5.SS4.p1.1.m1.6.6.2.2.cmml"><mi id="S5.SS4.p1.1.m1.6.6.2.2.2" xref="S5.SS4.p1.1.m1.6.6.2.2.2.cmml">j</mi><mo id="S5.SS4.p1.1.m1.6.6.2.2.1" xref="S5.SS4.p1.1.m1.6.6.2.2.1.cmml">∈</mo><mrow id="S5.SS4.p1.1.m1.6.6.2.2.3.2" xref="S5.SS4.p1.1.m1.6.6.2.2.3.1.cmml"><mo id="S5.SS4.p1.1.m1.6.6.2.2.3.2.1" stretchy="false" xref="S5.SS4.p1.1.m1.6.6.2.2.3.1.cmml">[</mo><mi id="S5.SS4.p1.1.m1.3.3" xref="S5.SS4.p1.1.m1.3.3.cmml">i</mi><mo id="S5.SS4.p1.1.m1.6.6.2.2.3.2.2" xref="S5.SS4.p1.1.m1.6.6.2.2.3.1.cmml">,</mo><mi id="S5.SS4.p1.1.m1.4.4" xref="S5.SS4.p1.1.m1.4.4.cmml">S</mi><mo id="S5.SS4.p1.1.m1.6.6.2.2.3.2.3" stretchy="false" xref="S5.SS4.p1.1.m1.6.6.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.6b"><apply id="S5.SS4.p1.1.m1.6.6.3.cmml" xref="S5.SS4.p1.1.m1.6.6.2"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.6.6.3a.cmml" xref="S5.SS4.p1.1.m1.6.6.2.3">formulae-sequence</csymbol><apply id="S5.SS4.p1.1.m1.5.5.1.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1"><and id="S5.SS4.p1.1.m1.5.5.1.1a.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1"></and><apply id="S5.SS4.p1.1.m1.5.5.1.1b.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1"><gt id="S5.SS4.p1.1.m1.5.5.1.1.4.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.4"></gt><apply id="S5.SS4.p1.1.m1.5.5.1.1.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2"><times id="S5.SS4.p1.1.m1.5.5.1.1.2.3.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.3"></times><ci id="S5.SS4.p1.1.m1.5.5.1.1.2.4.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.4">𝐼</ci><ci id="S5.SS4.p1.1.m1.5.5.1.1.2.5.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.5">𝑂</ci><ci id="S5.SS4.p1.1.m1.5.5.1.1.2.6.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.6">𝑈</ci><interval closure="open" id="S5.SS4.p1.1.m1.5.5.1.1.2.2.3.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2"><apply id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.2">𝑠</ci><ci id="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2">subscript</csymbol><ci id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.2">𝑠</ci><ci id="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="S5.SS4.p1.1.m1.5.5.1.1.5.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5"><times id="S5.SS4.p1.1.m1.5.5.1.1.5.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5.1"></times><ci id="S5.SS4.p1.1.m1.5.5.1.1.5.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5.2">𝜓</ci><apply id="S5.SS4.p1.1.m1.5.5.1.1.5.3.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3"><csymbol cd="latexml" id="S5.SS4.p1.1.m1.5.5.1.1.5.3.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3.1">for-all</csymbol><ci id="S5.SS4.p1.1.m1.5.5.1.1.5.3.2.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.5.3.2">𝑖</ci></apply></apply></apply><apply id="S5.SS4.p1.1.m1.5.5.1.1c.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1"><in id="S5.SS4.p1.1.m1.5.5.1.1.6.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.6"></in><share href="https://arxiv.org/html/2409.18049v1#S5.SS4.p1.1.m1.5.5.1.1.5.cmml" id="S5.SS4.p1.1.m1.5.5.1.1d.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1"></share><interval closure="closed" id="S5.SS4.p1.1.m1.5.5.1.1.7.1.cmml" xref="S5.SS4.p1.1.m1.5.5.1.1.7.2"><cn id="S5.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1">1</cn><ci id="S5.SS4.p1.1.m1.2.2.cmml" xref="S5.SS4.p1.1.m1.2.2">𝑆</ci></interval></apply></apply><apply id="S5.SS4.p1.1.m1.6.6.2.2.cmml" xref="S5.SS4.p1.1.m1.6.6.2.2"><in id="S5.SS4.p1.1.m1.6.6.2.2.1.cmml" xref="S5.SS4.p1.1.m1.6.6.2.2.1"></in><ci id="S5.SS4.p1.1.m1.6.6.2.2.2.cmml" xref="S5.SS4.p1.1.m1.6.6.2.2.2">𝑗</ci><interval closure="closed" id="S5.SS4.p1.1.m1.6.6.2.2.3.1.cmml" xref="S5.SS4.p1.1.m1.6.6.2.2.3.2"><ci id="S5.SS4.p1.1.m1.3.3.cmml" xref="S5.SS4.p1.1.m1.3.3">𝑖</ci><ci id="S5.SS4.p1.1.m1.4.4.cmml" xref="S5.SS4.p1.1.m1.4.4">𝑆</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.6c">IOU(s_{i},s_{j})&gt;\psi\;\forall\;i\in[1,S],\,j\in[i,S]</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.6d">italic_I italic_O italic_U ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) &gt; italic_ψ ∀ italic_i ∈ [ 1 , italic_S ] , italic_j ∈ [ italic_i , italic_S ]</annotation></semantics></math>, where <math alttext="\psi\in[0,1]" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.2"><semantics id="S5.SS4.p1.2.m2.2a"><mrow id="S5.SS4.p1.2.m2.2.3" xref="S5.SS4.p1.2.m2.2.3.cmml"><mi id="S5.SS4.p1.2.m2.2.3.2" xref="S5.SS4.p1.2.m2.2.3.2.cmml">ψ</mi><mo id="S5.SS4.p1.2.m2.2.3.1" xref="S5.SS4.p1.2.m2.2.3.1.cmml">∈</mo><mrow id="S5.SS4.p1.2.m2.2.3.3.2" xref="S5.SS4.p1.2.m2.2.3.3.1.cmml"><mo id="S5.SS4.p1.2.m2.2.3.3.2.1" stretchy="false" xref="S5.SS4.p1.2.m2.2.3.3.1.cmml">[</mo><mn id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml">0</mn><mo id="S5.SS4.p1.2.m2.2.3.3.2.2" xref="S5.SS4.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S5.SS4.p1.2.m2.2.2" xref="S5.SS4.p1.2.m2.2.2.cmml">1</mn><mo id="S5.SS4.p1.2.m2.2.3.3.2.3" stretchy="false" xref="S5.SS4.p1.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.2b"><apply id="S5.SS4.p1.2.m2.2.3.cmml" xref="S5.SS4.p1.2.m2.2.3"><in id="S5.SS4.p1.2.m2.2.3.1.cmml" xref="S5.SS4.p1.2.m2.2.3.1"></in><ci id="S5.SS4.p1.2.m2.2.3.2.cmml" xref="S5.SS4.p1.2.m2.2.3.2">𝜓</ci><interval closure="closed" id="S5.SS4.p1.2.m2.2.3.3.1.cmml" xref="S5.SS4.p1.2.m2.2.3.3.2"><cn id="S5.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS4.p1.2.m2.1.1">0</cn><cn id="S5.SS4.p1.2.m2.2.2.cmml" type="integer" xref="S5.SS4.p1.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.2c">\psi\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.2d">italic_ψ ∈ [ 0 , 1 ]</annotation></semantics></math> is a threshold on IOU and <math alttext="s_{i}" class="ltx_Math" display="inline" id="S5.SS4.p1.3.m3.1"><semantics id="S5.SS4.p1.3.m3.1a"><msub id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mi id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml">s</mi><mi id="S5.SS4.p1.3.m3.1.1.3" xref="S5.SS4.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.p1.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2">𝑠</ci><ci id="S5.SS4.p1.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.3.m3.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> refers to the list of SuperSegments sorted by their pixel area in a descending order. We only perform this culling on the database segments.
We use the outdoor-finetuned DINOv2 backbone for this purpose and compare SegVLAD with SALAD on AmsterTime (see supplementary for additional results on Pitts30K). Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.F5" title="Figure 5 ‣ 5.3.3 Patches vs Segment ‣ 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> shows that SegVLAD outperforms SALAD while requiring less storage (annotated on points) and comparable retrieval time (excludes extraction time), using IoU-based filtering threshold ranging from <math alttext="0.2" class="ltx_Math" display="inline" id="S5.SS4.p1.4.m4.1"><semantics id="S5.SS4.p1.4.m4.1a"><mn id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><cn id="S5.SS4.p1.4.m4.1.1.cmml" type="float" xref="S5.SS4.p1.4.m4.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">0.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.4.m4.1d">0.2</annotation></semantics></math> to <math alttext="0.8" class="ltx_Math" display="inline" id="S5.SS4.p1.5.m5.1"><semantics id="S5.SS4.p1.5.m5.1a"><mn id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><cn id="S5.SS4.p1.5.m5.1.1.cmml" type="float" xref="S5.SS4.p1.5.m5.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.5.m5.1d">0.8</annotation></semantics></math> (left to right) with a step size of <math alttext="0.2" class="ltx_Math" display="inline" id="S5.SS4.p1.6.m6.1"><semantics id="S5.SS4.p1.6.m6.1a"><mn id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><cn id="S5.SS4.p1.6.m6.1.1.cmml" type="float" xref="S5.SS4.p1.6.m6.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">0.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.6.m6.1d">0.2</annotation></semantics></math>. In particular, at <math alttext="0.4" class="ltx_Math" display="inline" id="S5.SS4.p1.7.m7.1"><semantics id="S5.SS4.p1.7.m7.1a"><mn id="S5.SS4.p1.7.m7.1.1" xref="S5.SS4.p1.7.m7.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.7.m7.1b"><cn id="S5.SS4.p1.7.m7.1.1.cmml" type="float" xref="S5.SS4.p1.7.m7.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.7.m7.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.7.m7.1d">0.4</annotation></semantics></math> IOU threshold, only <math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS4.p1.8.m8.1"><semantics id="S5.SS4.p1.8.m8.1a"><mrow id="S5.SS4.p1.8.m8.1.1" xref="S5.SS4.p1.8.m8.1.1.cmml"><mn id="S5.SS4.p1.8.m8.1.1.2" xref="S5.SS4.p1.8.m8.1.1.2.cmml">20</mn><mo id="S5.SS4.p1.8.m8.1.1.1" xref="S5.SS4.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.8.m8.1b"><apply id="S5.SS4.p1.8.m8.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1"><csymbol cd="latexml" id="S5.SS4.p1.8.m8.1.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1.1">percent</csymbol><cn id="S5.SS4.p1.8.m8.1.1.2.cmml" type="integer" xref="S5.SS4.p1.8.m8.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.8.m8.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.8.m8.1d">20 %</annotation></semantics></math> of SuperSegments are retained (<math alttext="0.05" class="ltx_Math" display="inline" id="S5.SS4.p1.9.m9.1"><semantics id="S5.SS4.p1.9.m9.1a"><mn id="S5.SS4.p1.9.m9.1.1" xref="S5.SS4.p1.9.m9.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.9.m9.1b"><cn id="S5.SS4.p1.9.m9.1.1.cmml" type="float" xref="S5.SS4.p1.9.m9.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.9.m9.1c">0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.9.m9.1d">0.05</annotation></semantics></math> GB) while still outperforming the baseline.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Qualitative Analyses</h4>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">In this section, we further demonstrate the capabilities of our method through qualitative visualizations. We compare our method against AnyLoc, where the only difference between the two methods is <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">Global</span> aggregation/retrieval vs <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">SuperSegment</span> aggregation/retrieval. We particularly consider the queries for which our approach successfully retrieved the correct match but AnyLoc failed to do so (additional examples can be found in the supplementary).
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S5.F4" title="Figure 4 ‣ 5.3.2 Segment to Image Retrieval ‣ 5.3 Ablation Studies ‣ 5 Results ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a> shows triplets of images in the order of query, correct match (ours), and incorrect match (Anyloc). The segmented part shows one of the correctly matched SuperSegments, displayed as a subgraph in white color overlaid on the corresponding segment masks.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">The first row shows a triplet from AmsterTime where our proposed method is able to correctly recognize a subgraph of building across the image pair, whereas Anyloc retrieves an incorrect image of a street-view with buildings, cars, and road, laid out similarly across the image pair. This highlights that a global descriptor can get confused with <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.1">perceptually-aliased global context</span>. In contrast, our SuperSegment based SegVLAD is not only able to retrieve the correct image but it also correctly finds the mutually-overlapping area. A similar trend follows for the Baidu Mall (middle row) and Pitts30K (last row). In the Baidu Mall example, our approach identifies the piano and the region around it in the query image. It correctly retrieves an image having similar spatial context with the piano. This is akin to how humans use spatial context to identify places. AnyLoc, on the other hand, retrieves an image with similar floor tiles and railings. This example particularly highlights our hypothesis that dissimilarity of non-overlapping regions can dominate the similarity of overlapping regions in global whole-image descriptors. Finally, the Pitts30K example (last row) shows a case of strong viewpoint change. While SegVLAD correctly matches the traffic signal and signboards to retrieve the correct match, AnyLoc retrieves a similar looking image while missing the finer details. This example particularly reinforces the idea of ‘revisiting things’, as even though the background mountain is common across the triplet, it is the context of the things near the camera/robot which helps in uniquely recognizing a place.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h3>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we presented a novel visual place recognition pipeline <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">SegVLAD</span> based on image segments-based description and retrieval, which is akin to ‘revisting things’ as a means to recognize specific instances of what constitute a place. Our proposed <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">SuperSegments</span> based image representation and a novel factorization based feature aggregation enables us to effectively represent and retrieve images using our segment similarity-weighted image ranking. Our results show that despite using powerful image encoders such as DINOv2 (pretrained or VPR-finetuned), existing global descriptor based techniques are unable to deal with the challenges of viewpoint variations. In contrast, SegVLAD is able to correctly retrieve images through its ability to match partially-overlapping images with its partial image representation in the form of semi-global subgraphs of segments, i.e., SuperSegments. Thus, our method achieves state-of-the-art results on three diverse datasets (indoor and outdoor) that exhibit strong viewpoint variations on top of other challenges of appearance shift and high perceptual aliasing. Through an additional object instance retrieval study, we demonstrate the unique ability of our method to recognize objects instances within their specific place contexts – an open-set recognition capability that existing VPR methods lack.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our approach shifts the paradigm in retrieval based VPR research, as the conventional methods predominantly classify into either whole-image global descriptor based coarse retrieval or local feature based geometric reranking. Our approach complements recent concurrent works like MESA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib78" title="">78</a>]</cite>; future work can explore a hierarchical VPR pipeline that closely integrates a segment-based coarse retriever with segments-based reranker such as MESA, thus doing away entirely with global whole-image descriptors. Furthermore, segments-based representation with implicitly baked semantics provide a natural way for creating <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">text-based</span> interfaces through CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib57" title="">57</a>]</cite> and LLMs (Large Language Models) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib9" title="">9</a>]</cite>, which can be easily integrated with recent efforts in this direction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_chapter" id="Ptx1">
<h2 class="ltx_title ltx_title_chapter">Supplementary Material</h2>
<div class="ltx_para" id="Ptx1.p1">
<p class="ltx_p" id="Ptx1.p1.1">In this supplementary, we first present the limitations of our work, coupled with additional results on IOU-based filtering of SuperSegments (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S8" title="8 Storage, Compute Time &amp; IOU Based Filtering Additional Results ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">8</span></a>). This is followed by ablation studies on local feature based retriever (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS1" title="9.1 Local Feature Retrieval ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">9.1</span></a>) and an efficient version of SAM (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.SS2" title="9.2 SAM vs FastSAM ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">9.2</span></a>). We then present implementation and benchmarking details relating to the proposed factorized feature aggregation method (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS1" title="10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">10.1</span></a>), backbone models (SAM and DINOv2) (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS2" title="10.2 Backbone Networks ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">10.2</span></a>), and benchmark datasets (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.SS3" title="10.3 Datasets ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">10.3</span></a>). Finally, we present additional qualitative examples for retrieval (<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S11" title="11 More Qualitative Examples ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<section class="ltx_section" id="S7">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h3>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While our proposed method SegVLAD achieves state-of-the-art results on a diverse set of VPR benchmark datasets, there are notable limitations of our approach. i) Redundancy: we create several overlapping SuperSegments per image. While these are somewhat necessary to enable accurate partial image matching via partial representations, SuperSegments formed through neighboring central segments will have a very high overlap. ii) Map size, we need to store several SuperSegment descriptors (far more than a typical global descriptors database). These limitations to some extent can be addressed through simple measures, e.g., masks IOU based filtering of the database segments to reduce both the redundancy and storage, as detailed in the subsequent section. The value of our research primarily lies in the demonstration of a novel approach to VPR that not only addresses the fundamental limitation of global descriptors but is also characterized as an open-set, object-based, text-interface-friendly representation, which is more likely to plug in to similar recent approaches aimed at embodied intelligence.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Storage, Compute Time &amp; IOU Based Filtering Additional Results</h3>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S8.T7" title="Table 7 ‣ 8 Storage, Compute Time &amp; IOU Based Filtering Additional Results ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">7</span></a> shows results for IOU-based filtering for different thresholds, corresponding number of database segments, their storage consumption, and the average retrieval time per query. As can be observed from the results for AmsterTime, 0.4 IOU threshold removes up to <math alttext="80\%" class="ltx_Math" display="inline" id="S8.p1.1.m1.1"><semantics id="S8.p1.1.m1.1a"><mrow id="S8.p1.1.m1.1.1" xref="S8.p1.1.m1.1.1.cmml"><mn id="S8.p1.1.m1.1.1.2" xref="S8.p1.1.m1.1.1.2.cmml">80</mn><mo id="S8.p1.1.m1.1.1.1" xref="S8.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.p1.1.m1.1b"><apply id="S8.p1.1.m1.1.1.cmml" xref="S8.p1.1.m1.1.1"><csymbol cd="latexml" id="S8.p1.1.m1.1.1.1.cmml" xref="S8.p1.1.m1.1.1.1">percent</csymbol><cn id="S8.p1.1.m1.1.1.2.cmml" type="integer" xref="S8.p1.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.1.m1.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S8.p1.1.m1.1d">80 %</annotation></semantics></math> of the original segments, and our method still outperforms the global descriptor baseline while only requiring roughly half its storage. For Pitts30K, both 0.4 and 0.6 IOU thresholds remain reasonable choices, with improved recall at reduced storage and time. Furthermore, in absolute terms, both storage and retrieval time
for our method are practically viable, and comparable to the global descriptor baseline.</p>
</div>
<figure class="ltx_table" id="S8.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S8.T7.5.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S8.T7.6.2" style="font-size:90%;">Storage (GB) and Retrieval Time (ms) analysis coupled with IOU based filtering of SuperSegments, compared to typical global descriptor based retrieval pipeline. Both global and segment based approaches use finetuned DINOv2 encoder.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S8.T7.3" style="width:433.6pt;height:85.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-147.3pt,29.1pt) scale(0.595403959104493,0.595403959104493) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S8.T7.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S8.T7.3.3.4.1">
<td class="ltx_td ltx_border_tt" id="S8.T7.3.3.4.1.1" style="padding-left:18.0pt;padding-right:18.0pt;"></td>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S8.T7.3.3.4.1.2" style="padding-left:18.0pt;padding-right:18.0pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S8.T7.3.3.4.1.3" style="padding-left:18.0pt;padding-right:18.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S8.T7.3.3.4.1.4" style="padding-left:18.0pt;padding-right:18.0pt;">      AmsterTime</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S8.T7.3.3.4.1.5" style="padding-left:18.0pt;padding-right:18.0pt;">      Pitts30K</th>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S8.T7.3.3.3.4" style="padding-left:18.0pt;padding-right:18.0pt;">      Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S8.T7.1.1.1.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <math alttext="\psi" class="ltx_Math" display="inline" id="S8.T7.1.1.1.1.m1.1"><semantics id="S8.T7.1.1.1.1.m1.1a"><mi id="S8.T7.1.1.1.1.m1.1.1" xref="S8.T7.1.1.1.1.m1.1.1.cmml">ψ</mi><annotation-xml encoding="MathML-Content" id="S8.T7.1.1.1.1.m1.1b"><ci id="S8.T7.1.1.1.1.m1.1.1.cmml" xref="S8.T7.1.1.1.1.m1.1.1">𝜓</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.T7.1.1.1.1.m1.1c">\psi</annotation><annotation encoding="application/x-llamapun" id="S8.T7.1.1.1.1.m1.1d">italic_ψ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S8.T7.3.3.3.5" style="padding-left:18.0pt;padding-right:18.0pt;">      Dim</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.2.2.2.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <math alttext="N_{Db}" class="ltx_Math" display="inline" id="S8.T7.2.2.2.2.m1.1"><semantics id="S8.T7.2.2.2.2.m1.1a"><msub id="S8.T7.2.2.2.2.m1.1.1" xref="S8.T7.2.2.2.2.m1.1.1.cmml"><mi id="S8.T7.2.2.2.2.m1.1.1.2" xref="S8.T7.2.2.2.2.m1.1.1.2.cmml">N</mi><mrow id="S8.T7.2.2.2.2.m1.1.1.3" xref="S8.T7.2.2.2.2.m1.1.1.3.cmml"><mi id="S8.T7.2.2.2.2.m1.1.1.3.2" xref="S8.T7.2.2.2.2.m1.1.1.3.2.cmml">D</mi><mo id="S8.T7.2.2.2.2.m1.1.1.3.1" xref="S8.T7.2.2.2.2.m1.1.1.3.1.cmml">⁢</mo><mi id="S8.T7.2.2.2.2.m1.1.1.3.3" xref="S8.T7.2.2.2.2.m1.1.1.3.3.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S8.T7.2.2.2.2.m1.1b"><apply id="S8.T7.2.2.2.2.m1.1.1.cmml" xref="S8.T7.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S8.T7.2.2.2.2.m1.1.1.1.cmml" xref="S8.T7.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S8.T7.2.2.2.2.m1.1.1.2.cmml" xref="S8.T7.2.2.2.2.m1.1.1.2">𝑁</ci><apply id="S8.T7.2.2.2.2.m1.1.1.3.cmml" xref="S8.T7.2.2.2.2.m1.1.1.3"><times id="S8.T7.2.2.2.2.m1.1.1.3.1.cmml" xref="S8.T7.2.2.2.2.m1.1.1.3.1"></times><ci id="S8.T7.2.2.2.2.m1.1.1.3.2.cmml" xref="S8.T7.2.2.2.2.m1.1.1.3.2">𝐷</ci><ci id="S8.T7.2.2.2.2.m1.1.1.3.3.cmml" xref="S8.T7.2.2.2.2.m1.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T7.2.2.2.2.m1.1c">N_{Db}</annotation><annotation encoding="application/x-llamapun" id="S8.T7.2.2.2.2.m1.1d">italic_N start_POSTSUBSCRIPT italic_D italic_b end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.6" style="padding-left:18.0pt;padding-right:18.0pt;">      R@1/5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.7" style="padding-left:18.0pt;padding-right:18.0pt;">      Storage</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.8" style="padding-left:18.0pt;padding-right:18.0pt;">      Time</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <math alttext="N_{Db}" class="ltx_Math" display="inline" id="S8.T7.3.3.3.3.m1.1"><semantics id="S8.T7.3.3.3.3.m1.1a"><msub id="S8.T7.3.3.3.3.m1.1.1" xref="S8.T7.3.3.3.3.m1.1.1.cmml"><mi id="S8.T7.3.3.3.3.m1.1.1.2" xref="S8.T7.3.3.3.3.m1.1.1.2.cmml">N</mi><mrow id="S8.T7.3.3.3.3.m1.1.1.3" xref="S8.T7.3.3.3.3.m1.1.1.3.cmml"><mi id="S8.T7.3.3.3.3.m1.1.1.3.2" xref="S8.T7.3.3.3.3.m1.1.1.3.2.cmml">D</mi><mo id="S8.T7.3.3.3.3.m1.1.1.3.1" xref="S8.T7.3.3.3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="S8.T7.3.3.3.3.m1.1.1.3.3" xref="S8.T7.3.3.3.3.m1.1.1.3.3.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S8.T7.3.3.3.3.m1.1b"><apply id="S8.T7.3.3.3.3.m1.1.1.cmml" xref="S8.T7.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S8.T7.3.3.3.3.m1.1.1.1.cmml" xref="S8.T7.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S8.T7.3.3.3.3.m1.1.1.2.cmml" xref="S8.T7.3.3.3.3.m1.1.1.2">𝑁</ci><apply id="S8.T7.3.3.3.3.m1.1.1.3.cmml" xref="S8.T7.3.3.3.3.m1.1.1.3"><times id="S8.T7.3.3.3.3.m1.1.1.3.1.cmml" xref="S8.T7.3.3.3.3.m1.1.1.3.1"></times><ci id="S8.T7.3.3.3.3.m1.1.1.3.2.cmml" xref="S8.T7.3.3.3.3.m1.1.1.3.2">𝐷</ci><ci id="S8.T7.3.3.3.3.m1.1.1.3.3.cmml" xref="S8.T7.3.3.3.3.m1.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.T7.3.3.3.3.m1.1c">N_{Db}</annotation><annotation encoding="application/x-llamapun" id="S8.T7.3.3.3.3.m1.1d">italic_N start_POSTSUBSCRIPT italic_D italic_b end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.9" style="padding-left:18.0pt;padding-right:18.0pt;">      R@1/5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.10" style="padding-left:18.0pt;padding-right:18.0pt;">      Storage</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S8.T7.3.3.3.11" style="padding-left:18.0pt;padding-right:18.0pt;">      Time</th>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.3.3.5.2.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SALAD</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.2" style="padding-left:18.0pt;padding-right:18.0pt;">      -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.3" style="padding-left:18.0pt;padding-right:18.0pt;">      8448</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.4" style="padding-left:18.0pt;padding-right:18.0pt;">      1231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.5" style="padding-left:18.0pt;padding-right:18.0pt;">      55.4/75.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.6" style="padding-left:18.0pt;padding-right:18.0pt;">      0.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.7" style="padding-left:18.0pt;padding-right:18.0pt;">      2.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.8" style="padding-left:18.0pt;padding-right:18.0pt;">      10000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.9" style="padding-left:18.0pt;padding-right:18.0pt;">      92.6/96.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.10" style="padding-left:18.0pt;padding-right:18.0pt;">      0.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.3.3.5.2.11" style="padding-left:18.0pt;padding-right:18.0pt;">      25.1</td>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.6.3">
<td class="ltx_td ltx_align_left" id="S8.T7.3.3.6.3.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SegVLAD</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.2" style="padding-left:18.0pt;padding-right:18.0pt;">      0.2</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.3" style="padding-left:18.0pt;padding-right:18.0pt;">      1024</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.4" style="padding-left:18.0pt;padding-right:18.0pt;">      3886</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.5" style="padding-left:18.0pt;padding-right:18.0pt;">      54.0/69.2</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.6" style="padding-left:18.0pt;padding-right:18.0pt;">      0.03</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.7" style="padding-left:18.0pt;padding-right:18.0pt;">      1.2</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.8" style="padding-left:18.0pt;padding-right:18.0pt;">      25704</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.9" style="padding-left:18.0pt;padding-right:18.0pt;">      91.8/96.2</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.10" style="padding-left:18.0pt;padding-right:18.0pt;">      0.19</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.6.3.11" style="padding-left:18.0pt;padding-right:18.0pt;">      8.0</td>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.7.4">
<td class="ltx_td ltx_align_left" id="S8.T7.3.3.7.4.1" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.1.1">SegVLAD</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.2" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.2.1">0.4</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.3" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.3.1">1024</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.4" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.4.1">6200</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.5" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.5.1">58.9/76.2</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.6" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.6.1"> 0.05</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.7" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.7.1">3.1</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.8" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.8.1">40507</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.9" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.9.1">92.6/96.7</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.10" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.10.1"> 0.31</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.7.4.11" style="padding-left:18.0pt;padding-right:18.0pt;">      <span class="ltx_text ltx_font_bold" id="S8.T7.3.3.7.4.11.1">12.3</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.8.5">
<td class="ltx_td ltx_align_left" id="S8.T7.3.3.8.5.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SegVLAD</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.2" style="padding-left:18.0pt;padding-right:18.0pt;">      0.6</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.3" style="padding-left:18.0pt;padding-right:18.0pt;">      1024</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.4" style="padding-left:18.0pt;padding-right:18.0pt;">      9986</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.5" style="padding-left:18.0pt;padding-right:18.0pt;">      58.1/77.3</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.6" style="padding-left:18.0pt;padding-right:18.0pt;">      0.08</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.7" style="padding-left:18.0pt;padding-right:18.0pt;">      4.2</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.8" style="padding-left:18.0pt;padding-right:18.0pt;">      65699</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.9" style="padding-left:18.0pt;padding-right:18.0pt;">      92.8/96.8</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.10" style="padding-left:18.0pt;padding-right:18.0pt;">      0.51</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.8.5.11" style="padding-left:18.0pt;padding-right:18.0pt;">      19.2</td>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.9.6">
<td class="ltx_td ltx_align_left" id="S8.T7.3.3.9.6.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SegVLAD</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.2" style="padding-left:18.0pt;padding-right:18.0pt;">      0.8</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.3" style="padding-left:18.0pt;padding-right:18.0pt;">      1024</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.4" style="padding-left:18.0pt;padding-right:18.0pt;">      23807</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.5" style="padding-left:18.0pt;padding-right:18.0pt;">      58.2/79.5</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.6" style="padding-left:18.0pt;padding-right:18.0pt;">      0.18</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.7" style="padding-left:18.0pt;padding-right:18.0pt;">      9.3</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.8" style="padding-left:18.0pt;padding-right:18.0pt;">      154854</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.9" style="padding-left:18.0pt;padding-right:18.0pt;">      92.4/96.8</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.10" style="padding-left:18.0pt;padding-right:18.0pt;">      1.18</td>
<td class="ltx_td ltx_align_center" id="S8.T7.3.3.9.6.11" style="padding-left:18.0pt;padding-right:18.0pt;">      43.2</td>
</tr>
<tr class="ltx_tr" id="S8.T7.3.3.10.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S8.T7.3.3.10.7.1" style="padding-left:18.0pt;padding-right:18.0pt;">      SegVLAD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.2" style="padding-left:18.0pt;padding-right:18.0pt;">      1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.3" style="padding-left:18.0pt;padding-right:18.0pt;">      1024</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.4" style="padding-left:18.0pt;padding-right:18.0pt;">      129637</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.5" style="padding-left:18.0pt;padding-right:18.0pt;">      58.9/79.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.6" style="padding-left:18.0pt;padding-right:18.0pt;">      0.98</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.7" style="padding-left:18.0pt;padding-right:18.0pt;">      42.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.8" style="padding-left:18.0pt;padding-right:18.0pt;">      871150</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.9" style="padding-left:18.0pt;padding-right:18.0pt;">      93.2/96.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.10" style="padding-left:18.0pt;padding-right:18.0pt;">      6.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.3.3.10.7.11" style="padding-left:18.0pt;padding-right:18.0pt;">      251.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S9">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Additional Results and Ablation Studies</h3>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">This section describes two ablation studies: <span class="ltx_text ltx_font_italic" id="S9.p1.1.1">i)</span> local feature based retrieval, which highlights the role of SuperSegments in capturing sufficient pixel scope to retrieve correctly, and <span class="ltx_text ltx_font_italic" id="S9.p1.1.2">ii)</span> efficient version of SAM, which emphasizes the lack of strict dependence on a particular segmentation model while also addressing its computational bottleneck.</p>
</div>
<section class="ltx_subsection" id="S9.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Local Feature Retrieval</h4>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.2">There exist retrieval-based place recognition methods that directly use the local features with efficient inverted indexing and searching, e.g., DeLF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib49" title="">49</a>]</cite>. In this section, we compare our segments-based retriever against such a local feature-based retriever, while using the same feature backbone (AnyLoc’s DINOv2). We sample <math alttext="S" class="ltx_Math" display="inline" id="S9.SS1.p1.1.m1.1"><semantics id="S9.SS1.p1.1.m1.1a"><mi id="S9.SS1.p1.1.m1.1.1" xref="S9.SS1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S9.SS1.p1.1.m1.1b"><ci id="S9.SS1.p1.1.m1.1.1.cmml" xref="S9.SS1.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p1.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S9.SS1.p1.1.m1.1d">italic_S</annotation></semantics></math> local features uniformly at random, where <math alttext="S" class="ltx_Math" display="inline" id="S9.SS1.p1.2.m2.1"><semantics id="S9.SS1.p1.2.m2.1a"><mi id="S9.SS1.p1.2.m2.1.1" xref="S9.SS1.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S9.SS1.p1.2.m2.1b"><ci id="S9.SS1.p1.2.m2.1.1.cmml" xref="S9.SS1.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p1.2.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="S9.SS1.p1.2.m2.1d">italic_S</annotation></semantics></math> is the average number of segments for that image. As we intend to compare the role of local features in contrast to aggregation at segment/global level, we directly construct a flat index of these local features using the reference images of the given dataset. We then use our retrieval pipeline, considering local features a drop-in replacement of segment descriptors.</p>
</div>
<div class="ltx_para" id="S9.SS1.p2">
<p class="ltx_p" id="S9.SS1.p2.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.T8" title="Table 8 ‣ 9.1 Local Feature Retrieval ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">8</span></a> compares recall across local (pixel/point) features, segment descriptors and global descriptors on Baidu (indoor) and AmsterTime (outdoor). <math alttext="S" class="ltx_Math" display="inline" id="S9.SS1.p2.1.m1.1"><semantics id="S9.SS1.p2.1.m1.1a"><mi id="S9.SS1.p2.1.m1.1.1" xref="S9.SS1.p2.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S9.SS1.p2.1.m1.1b"><ci id="S9.SS1.p2.1.m1.1.1.cmml" xref="S9.SS1.p2.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p2.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S9.SS1.p2.1.m1.1d">italic_S</annotation></semantics></math> is set respectively to <math alttext="130" class="ltx_Math" display="inline" id="S9.SS1.p2.2.m2.1"><semantics id="S9.SS1.p2.2.m2.1a"><mn id="S9.SS1.p2.2.m2.1.1" xref="S9.SS1.p2.2.m2.1.1.cmml">130</mn><annotation-xml encoding="MathML-Content" id="S9.SS1.p2.2.m2.1b"><cn id="S9.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S9.SS1.p2.2.m2.1.1">130</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p2.2.m2.1c">130</annotation><annotation encoding="application/x-llamapun" id="S9.SS1.p2.2.m2.1d">130</annotation></semantics></math> and <math alttext="100" class="ltx_Math" display="inline" id="S9.SS1.p2.3.m3.1"><semantics id="S9.SS1.p2.3.m3.1a"><mn id="S9.SS1.p2.3.m3.1.1" xref="S9.SS1.p2.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S9.SS1.p2.3.m3.1b"><cn id="S9.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S9.SS1.p2.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p2.3.m3.1c">100</annotation><annotation encoding="application/x-llamapun" id="S9.SS1.p2.3.m3.1d">100</annotation></semantics></math> for the two datasets. It can be observed that local features perform inferior to both segments and global descriptors. This showcases that without any locally-aggregated information <span class="ltx_text ltx_font_italic" id="S9.SS1.p2.3.1">local features lack sufficient spatial context</span> needed to differentiate between two places. On the other hand, <span class="ltx_text ltx_font_italic" id="S9.SS1.p2.3.2">global descriptors carry additional spatial context</span> due to the non-overlapping parts of the image pair, which leads to mismatches. As a middle-order aggregation approach, our SuperSegments based SegVLAD descriptors achieve the required balance between aggregating sufficient spatial context and maintaining the distinctiveness necessary for viewpoint-invariant VPR task.</p>
</div>
<figure class="ltx_table" id="S9.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S9.T8.2.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S9.T8.3.2" style="font-size:90%;">Comparison of local, segments and global retrieval methods using AnyLoc’s DINOv2 encoder.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S9.T8.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T8.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S9.T8.4.1.1.1" rowspan="2" style="padding-left:16.5pt;padding-right:16.5pt;">     <span class="ltx_text" id="S9.T8.4.1.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.4.1.1.2" style="padding-left:16.5pt;padding-right:16.5pt;">     Baidu</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.4.1.1.3" style="padding-left:16.5pt;padding-right:16.5pt;">     AmsterTime</td>
</tr>
<tr class="ltx_tr" id="S9.T8.4.2.2">
<td class="ltx_td ltx_align_center" id="S9.T8.4.2.2.1" style="padding-left:16.5pt;padding-right:16.5pt;">     R@1/5</td>
<td class="ltx_td ltx_align_center" id="S9.T8.4.2.2.2" style="padding-left:16.5pt;padding-right:16.5pt;">     R@1/5</td>
</tr>
<tr class="ltx_tr" id="S9.T8.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T8.4.3.3.1" style="padding-left:16.5pt;padding-right:16.5pt;">     Local (Pixel/Point)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T8.4.3.3.2" style="padding-left:16.5pt;padding-right:16.5pt;">     69.1/88.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T8.4.3.3.3" style="padding-left:16.5pt;padding-right:16.5pt;">     42.2/66.1</td>
</tr>
<tr class="ltx_tr" id="S9.T8.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S9.T8.4.4.4.1" style="padding-left:16.5pt;padding-right:16.5pt;">     Segment (SegVLAD)</th>
<td class="ltx_td ltx_align_center" id="S9.T8.4.4.4.2" style="padding-left:16.5pt;padding-right:16.5pt;">     <span class="ltx_text ltx_font_bold" id="S9.T8.4.4.4.2.1">78.5</span>/<span class="ltx_text ltx_font_bold" id="S9.T8.4.4.4.2.2">93.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.4.4.4.3" style="padding-left:16.5pt;padding-right:16.5pt;">     <span class="ltx_text ltx_font_bold" id="S9.T8.4.4.4.3.1">56.8</span>/<span class="ltx_text ltx_font_bold" id="S9.T8.4.4.4.3.2">77.7</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S9.T8.4.5.5.1" style="padding-left:16.5pt;padding-right:16.5pt;">     Global (AnyLoc)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.4.5.5.2" style="padding-left:16.5pt;padding-right:16.5pt;">     75.2/87.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.4.5.5.3" style="padding-left:16.5pt;padding-right:16.5pt;">     50.3/73.0</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S9.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S9.T9.5.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S9.T9.6.2" style="font-size:90%;">Comparing R@1/5 and segmentation inference time on the Baidu dataset using different segmentation methods: SAM vs FastSAM.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S9.T9.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T9.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.2" style="padding-left:4.5pt;padding-right:4.5pt;">Scope</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.3" style="padding-left:4.5pt;padding-right:4.5pt;">Backbone</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">Seg. Time (s)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">Resolution</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T9.3.4.1.6" style="padding-left:4.5pt;padding-right:4.5pt;">R@1/5</th>
</tr>
<tr class="ltx_tr" id="S9.T9.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.1" style="padding-left:4.5pt;padding-right:4.5pt;">AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.2" style="padding-left:4.5pt;padding-right:4.5pt;">Global</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.3" style="padding-left:4.5pt;padding-right:4.5pt;">DINOv2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.4" style="padding-left:4.5pt;padding-right:4.5pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.5" style="padding-left:4.5pt;padding-right:4.5pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T9.3.5.2.6" style="padding-left:4.5pt;padding-right:4.5pt;">75.2/87.6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T9.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T9.1.1.2" style="padding-left:4.5pt;padding-right:4.5pt;">SegVLAD with SAM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T9.1.1.3" style="padding-left:4.5pt;padding-right:4.5pt;">Segment</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T9.1.1.4" style="padding-left:4.5pt;padding-right:4.5pt;">DINOv2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T9.1.1.5" style="padding-left:4.5pt;padding-right:4.5pt;">3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T9.1.1.1" style="padding-left:4.5pt;padding-right:4.5pt;">240<math alttext="\times" class="ltx_Math" display="inline" id="S9.T9.1.1.1.m1.1"><semantics id="S9.T9.1.1.1.m1.1a"><mo id="S9.T9.1.1.1.m1.1.1" xref="S9.T9.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S9.T9.1.1.1.m1.1b"><times id="S9.T9.1.1.1.m1.1.1.cmml" xref="S9.T9.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S9.T9.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S9.T9.1.1.1.m1.1d">×</annotation></semantics></math>320</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T9.1.1.6" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text ltx_font_bold" id="S9.T9.1.1.6.1">78.5</span>/<span class="ltx_text ltx_font_bold" id="S9.T9.1.1.6.2">93.8</span>
</td>
</tr>
<tr class="ltx_tr" id="S9.T9.2.2">
<td class="ltx_td ltx_align_left" id="S9.T9.2.2.2" style="padding-left:4.5pt;padding-right:4.5pt;">SegVLAD with FastSAM</td>
<td class="ltx_td ltx_align_center" id="S9.T9.2.2.3" style="padding-left:4.5pt;padding-right:4.5pt;">Segment</td>
<td class="ltx_td ltx_align_center" id="S9.T9.2.2.4" style="padding-left:4.5pt;padding-right:4.5pt;">DINOv2</td>
<td class="ltx_td ltx_align_center" id="S9.T9.2.2.5" style="padding-left:4.5pt;padding-right:4.5pt;">0.28</td>
<td class="ltx_td ltx_align_center" id="S9.T9.2.2.1" style="padding-left:4.5pt;padding-right:4.5pt;">240<math alttext="\times" class="ltx_Math" display="inline" id="S9.T9.2.2.1.m1.1"><semantics id="S9.T9.2.2.1.m1.1a"><mo id="S9.T9.2.2.1.m1.1.1" xref="S9.T9.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S9.T9.2.2.1.m1.1b"><times id="S9.T9.2.2.1.m1.1.1.cmml" xref="S9.T9.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S9.T9.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S9.T9.2.2.1.m1.1d">×</annotation></semantics></math>320</td>
<td class="ltx_td ltx_align_center" id="S9.T9.2.2.6" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text ltx_font_bold" id="S9.T9.2.2.6.1">76.2</span>/<span class="ltx_text ltx_font_bold" id="S9.T9.2.2.6.2">91.9</span>
</td>
</tr>
<tr class="ltx_tr" id="S9.T9.3.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S9.T9.3.3.2" style="padding-left:4.5pt;padding-right:4.5pt;">SegVLAD with FastSAM</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T9.3.3.3" style="padding-left:4.5pt;padding-right:4.5pt;">Segment</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T9.3.3.4" style="padding-left:4.5pt;padding-right:4.5pt;">DINOv2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T9.3.3.5" style="padding-left:4.5pt;padding-right:4.5pt;">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T9.3.3.1" style="padding-left:4.5pt;padding-right:4.5pt;">480<math alttext="\times" class="ltx_Math" display="inline" id="S9.T9.3.3.1.m1.1"><semantics id="S9.T9.3.3.1.m1.1a"><mo id="S9.T9.3.3.1.m1.1.1" xref="S9.T9.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S9.T9.3.3.1.m1.1b"><times id="S9.T9.3.3.1.m1.1.1.cmml" xref="S9.T9.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S9.T9.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S9.T9.3.3.1.m1.1d">×</annotation></semantics></math>640</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T9.3.3.6" style="padding-left:4.5pt;padding-right:4.5pt;">
<span class="ltx_text ltx_font_bold" id="S9.T9.3.3.6.1">77.5</span>/<span class="ltx_text ltx_font_bold" id="S9.T9.3.3.6.2">93.6</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>SAM vs FastSAM</h4>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10" title="10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">10</span></a> provides details of the configuration used for the original SAM model, where segment extraction becomes time consuming due to grid-based sampling/prompting (up to 3.5 seconds per image). Therefore, in our pipeline, we drop-in replace the original SAM with an efficient version of SAM, i.e., FastSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib79" title="">79</a>]</cite> to analyse the recall-speed trade-off for our proposed method. FastSAM replaces the transformer architecture of SAM with a CNN-based detector trained on an instance segmentation task using 1/50th of the original training dataset of SAM. Additionally, it uses a prompt-free approach to attain high efficiency to <span class="ltx_text ltx_font_italic" id="S9.SS2.p1.1.1">segment everything</span>, which refers to extraction of all possible masks, as opposed to point/box based sparse prompting.</p>
</div>
<div class="ltx_para" id="S9.SS2.p2">
<p class="ltx_p" id="S9.SS2.p2.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.T9" title="Table 9 ‣ 9.1 Local Feature Retrieval ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">9</span></a> (Baidu) shows that the drop in recall values is quite marginal (up to <math alttext="2\%" class="ltx_Math" display="inline" id="S9.SS2.p2.1.m1.1"><semantics id="S9.SS2.p2.1.m1.1a"><mrow id="S9.SS2.p2.1.m1.1.1" xref="S9.SS2.p2.1.m1.1.1.cmml"><mn id="S9.SS2.p2.1.m1.1.1.2" xref="S9.SS2.p2.1.m1.1.1.2.cmml">2</mn><mo id="S9.SS2.p2.1.m1.1.1.1" xref="S9.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S9.SS2.p2.1.m1.1b"><apply id="S9.SS2.p2.1.m1.1.1.cmml" xref="S9.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S9.SS2.p2.1.m1.1.1.1.cmml" xref="S9.SS2.p2.1.m1.1.1.1">percent</csymbol><cn id="S9.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="S9.SS2.p2.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.SS2.p2.1.m1.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="S9.SS2.p2.1.m1.1d">2 %</annotation></semantics></math>) when replacing SAM with FastSAM, while being <math alttext="13\times" class="ltx_math_unparsed" display="inline" id="S9.SS2.p2.2.m2.1"><semantics id="S9.SS2.p2.2.m2.1a"><mrow id="S9.SS2.p2.2.m2.1b"><mn id="S9.SS2.p2.2.m2.1.1">13</mn><mo id="S9.SS2.p2.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S9.SS2.p2.2.m2.1c">13\times</annotation><annotation encoding="application/x-llamapun" id="S9.SS2.p2.2.m2.1d">13 ×</annotation></semantics></math> faster in segmentation inference and still outperforming the second best method, i.e., AnyLoc (global descriptor baseline). In the last row, we further show that we can use FastSAM at higher input resolution with almost negligible increase in time and it further reduces the recall gap with original SAM.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3 </span>HardVLAD vs (Soft) NetVLAD</h4>
<div class="ltx_para" id="S9.SS3.p1">
<p class="ltx_p" id="S9.SS3.p1.1">In the main paper, we used hard assignment based VLAD aggregation for both pretrained and finetuned DINOv2 backbones. The alternative choice, particularly for the finetuned version, is to use soft assignment as defined in NetVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.T11" title="Table 11 ‣ 9.4 Different feature extractor ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">11</span></a> shows that hard assignment of VLAD performs similar to NetVLAD.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS4">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4 </span>Different feature extractor</h4>
<div class="ltx_para" id="S9.SS4.p1">
<p class="ltx_p" id="S9.SS4.p1.1">Other than the DINOv2 as our backbone feature extractor, here we consider another similar pretrained off-the-shelf backbone to highlight that our method is generally applicable to any dense feature extractor.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.T11" title="Table 11 ‣ 9.4 Different feature extractor ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">11</span></a> shows results for a more recent feature extractor, RADIOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib58" title="">58</a>]</cite>. Our segments-based approach applied on top of this encoder substantially improves its performance, although our DINOv2-based SegVLAD achieves the best performance.</p>
</div>
<figure class="ltx_table" id="S9.T11">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S9.T11.fig1" style="width:208.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.T11.fig1.1.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S9.T11.fig1.2.2" style="font-size:90%;">R@1/5 Comparison between HardVLAD and SoftVLAD.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S9.T11.fig1.3" style="width:303.5pt;height:95.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(66.2pt,-20.9pt) scale(1.77342597580113,1.77342597580113) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S9.T11.fig1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T11.fig1.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S9.T11.fig1.3.1.1.1.1" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig1.3.1.1.1.2" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.1.1.2.1">AmsterTime</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig1.3.1.1.1.3" style="padding-left:3.6pt;padding-right:3.6pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.1.1.3.1">Pitts30K</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T11.fig1.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S9.T11.fig1.3.1.2.1.1" style="padding-left:3.6pt;padding-right:3.6pt;">HardVLAD</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig1.3.1.2.1.2" style="padding-left:3.6pt;padding-right:3.6pt;">58.9/<span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.2.1.2.1">79.3</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig1.3.1.2.1.3" style="padding-left:3.6pt;padding-right:3.6pt;">
<span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.2.1.3.1">93.2</span>/96.8</td>
</tr>
<tr class="ltx_tr" id="S9.T11.fig1.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S9.T11.fig1.3.1.3.2.1" style="padding-left:3.6pt;padding-right:3.6pt;">SoftVLAD</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T11.fig1.3.1.3.2.2" style="padding-left:3.6pt;padding-right:3.6pt;">
<span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.3.2.2.1">60.2</span>/78.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T11.fig1.3.1.3.2.3" style="padding-left:3.6pt;padding-right:3.6pt;">93/<span class="ltx_text ltx_font_bold" id="S9.T11.fig1.3.1.3.2.3.1">96.9</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S9.T11.fig2" style="width:208.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.T11.fig2.1.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="S9.T11.fig2.2.2" style="font-size:90%;">Using Different feature extractor on AmsterTime.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S9.T11.fig2.3" style="width:433.6pt;height:123.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.0pt,-16.8pt) scale(1.37366210672867,1.37366210672867) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S9.T11.fig2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T11.fig2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig2.3.1.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig2.3.1.1.1.2" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.1.1.2.1">Backbone</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig2.3.1.1.1.3" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.1.1.3.1">#Params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S9.T11.fig2.3.1.1.1.4" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.1.1.4.1">R@1/5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T11.fig2.3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T11.fig2.3.1.2.1.1" style="padding-left:12.0pt;padding-right:12.0pt;">RADIO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.2.1.2" style="padding-left:12.0pt;padding-right:12.0pt;">RADIOv2 ViT-H/16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.2.1.3" style="padding-left:12.0pt;padding-right:12.0pt;">0.6B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.2.1.4" style="padding-left:12.0pt;padding-right:12.0pt;">47.8/72.1</td>
</tr>
<tr class="ltx_tr" id="S9.T11.fig2.3.1.3.2">
<td class="ltx_td ltx_align_left" id="S9.T11.fig2.3.1.3.2.1" style="padding-left:12.0pt;padding-right:12.0pt;">SegVLAD</td>
<td class="ltx_td ltx_align_center" id="S9.T11.fig2.3.1.3.2.2" style="padding-left:12.0pt;padding-right:12.0pt;">RADIOv2 ViT-H/16</td>
<td class="ltx_td ltx_align_center" id="S9.T11.fig2.3.1.3.2.3" style="padding-left:12.0pt;padding-right:12.0pt;">0.6B</td>
<td class="ltx_td ltx_align_center" id="S9.T11.fig2.3.1.3.2.4" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_italic" id="S9.T11.fig2.3.1.3.2.4.1">52.9</span>/<span class="ltx_text ltx_font_italic" id="S9.T11.fig2.3.1.3.2.4.2">74.9</span>
</td>
</tr>
<tr class="ltx_tr" id="S9.T11.fig2.3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T11.fig2.3.1.4.3.1" style="padding-left:12.0pt;padding-right:12.0pt;">AnyLoc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.4.3.2" style="padding-left:12.0pt;padding-right:12.0pt;">DINOv2 ViT-G/14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.4.3.3" style="padding-left:12.0pt;padding-right:12.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T11.fig2.3.1.4.3.4" style="padding-left:12.0pt;padding-right:12.0pt;">50.3/73.0</td>
</tr>
<tr class="ltx_tr" id="S9.T11.fig2.3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S9.T11.fig2.3.1.5.4.1" style="padding-left:12.0pt;padding-right:12.0pt;">SegVLAD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T11.fig2.3.1.5.4.2" style="padding-left:12.0pt;padding-right:12.0pt;">DINOv2 ViT-G/14</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T11.fig2.3.1.5.4.3" style="padding-left:12.0pt;padding-right:12.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T11.fig2.3.1.5.4.4" style="padding-left:12.0pt;padding-right:12.0pt;">
<span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.5.4.4.1">56.8</span>/<span class="ltx_text ltx_font_bold" id="S9.T11.fig2.3.1.5.4.4.2">77.7</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S9.F6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S9.F6.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.F6.2.2">
<td class="ltx_td ltx_align_center" id="S9.F6.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="162" id="S9.F6.1.1.1.g1" src="extracted/5879622/17P_plots/recall1_vs_loc_rad_k_50_paper2.png" width="269"/></td>
<td class="ltx_td ltx_align_center" id="S9.F6.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="162" id="S9.F6.2.2.2.g1" src="extracted/5879622/17P_plots/recall5_vs_loc_rad_k_50_paper2.png" width="269"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F6.4.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S9.F6.5.2" style="font-size:90%;">Recall vs localization radius (in terms of frame separation) for 17 Places. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S9.SS5">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.5 </span>17Places dataset: Ground Truth Vagueness</h4>
<div class="ltx_para" id="S9.SS5.p1">
<p class="ltx_p" id="S9.SS5.p1.1">For VPR, ground truth is often defined in terms of either GPS coordinates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite> or image frame separation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib45" title="">45</a>]</cite>. There is also often a discrepancy in defining ground truth for the VPR task, depending on how a ‘place’ is defined and whether the camera position or visual overlap is used as a criterion for correct recognition, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib19" title="">19</a>]</cite>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S9.F6" title="Figure 6 ‣ 9.4 Different feature extractor ‣ 9 Additional Results and Ablation Studies ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a>, we present extended results for 17Places dataset to indicate how relative ranking of two methods can potentially switch depending on the choice of localization radius (in terms of frame separation).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Implementation and Benchmarking Details</h3>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">In this section, we provide detailed implementation of our factorized representation for aggregation, backbone networks and benchmark datasets.</p>
</div>
<section class="ltx_subsection" id="S10.SS1">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>Factorized Aggregation</h4>
<div class="ltx_para" id="S10.SS1.p1">
<p class="ltx_p" id="S10.SS1.p1.1">We proposed a <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.1">factorized</span> representation for feature aggregation as a unified method to aggregate at segment/global level for different aggregation types (see Section 3.3, Equation 2 in the main paper). In this section, we further elaborate this with explicit formulations for computing <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.2">SegVLAD</span>, <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.3">GlobalVLAD</span>, <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.4">Global Average Pooling (GAP)</span>, <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.5">Segment Average Pooling (SAP)</span>, and <span class="ltx_text ltx_font_italic" id="S10.SS1.p1.1.6">Generalized Mean Pooling (GeM)</span> using the proposed factorization:</p>
<table class="ltx_equation ltx_eqn_table" id="S10.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{S\times D}=A_{S\times S}^{o}\cdot M_{S\times N}\ \cdot T_{N\times D}" class="ltx_Math" display="block" id="S10.E5.m1.1"><semantics id="S10.E5.m1.1a"><mrow id="S10.E5.m1.1.1" xref="S10.E5.m1.1.1.cmml"><msub id="S10.E5.m1.1.1.2" xref="S10.E5.m1.1.1.2.cmml"><mi id="S10.E5.m1.1.1.2.2" xref="S10.E5.m1.1.1.2.2.cmml">F</mi><mrow id="S10.E5.m1.1.1.2.3" xref="S10.E5.m1.1.1.2.3.cmml"><mi id="S10.E5.m1.1.1.2.3.2" xref="S10.E5.m1.1.1.2.3.2.cmml">S</mi><mo id="S10.E5.m1.1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.2.3.1.cmml">×</mo><mi id="S10.E5.m1.1.1.2.3.3" xref="S10.E5.m1.1.1.2.3.3.cmml">D</mi></mrow></msub><mo id="S10.E5.m1.1.1.1" xref="S10.E5.m1.1.1.1.cmml">=</mo><mrow id="S10.E5.m1.1.1.3" xref="S10.E5.m1.1.1.3.cmml"><msubsup id="S10.E5.m1.1.1.3.2" xref="S10.E5.m1.1.1.3.2.cmml"><mi id="S10.E5.m1.1.1.3.2.2.2" xref="S10.E5.m1.1.1.3.2.2.2.cmml">A</mi><mrow id="S10.E5.m1.1.1.3.2.2.3" xref="S10.E5.m1.1.1.3.2.2.3.cmml"><mi id="S10.E5.m1.1.1.3.2.2.3.2" xref="S10.E5.m1.1.1.3.2.2.3.2.cmml">S</mi><mo id="S10.E5.m1.1.1.3.2.2.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.3.2.2.3.1.cmml">×</mo><mi id="S10.E5.m1.1.1.3.2.2.3.3" xref="S10.E5.m1.1.1.3.2.2.3.3.cmml">S</mi></mrow><mi id="S10.E5.m1.1.1.3.2.3" xref="S10.E5.m1.1.1.3.2.3.cmml">o</mi></msubsup><mo id="S10.E5.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.3.1.cmml">⋅</mo><msub id="S10.E5.m1.1.1.3.3" xref="S10.E5.m1.1.1.3.3.cmml"><mi id="S10.E5.m1.1.1.3.3.2" xref="S10.E5.m1.1.1.3.3.2.cmml">M</mi><mrow id="S10.E5.m1.1.1.3.3.3" xref="S10.E5.m1.1.1.3.3.3.cmml"><mi id="S10.E5.m1.1.1.3.3.3.2" xref="S10.E5.m1.1.1.3.3.3.2.cmml">S</mi><mo id="S10.E5.m1.1.1.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.3.3.3.1.cmml">×</mo><mi id="S10.E5.m1.1.1.3.3.3.3" xref="S10.E5.m1.1.1.3.3.3.3.cmml">N</mi></mrow></msub><mo id="S10.E5.m1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.3.1.cmml">⋅</mo><msub id="S10.E5.m1.1.1.3.4" xref="S10.E5.m1.1.1.3.4.cmml"><mi id="S10.E5.m1.1.1.3.4.2" xref="S10.E5.m1.1.1.3.4.2.cmml">T</mi><mrow id="S10.E5.m1.1.1.3.4.3" xref="S10.E5.m1.1.1.3.4.3.cmml"><mi id="S10.E5.m1.1.1.3.4.3.2" xref="S10.E5.m1.1.1.3.4.3.2.cmml">N</mi><mo id="S10.E5.m1.1.1.3.4.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E5.m1.1.1.3.4.3.1.cmml">×</mo><mi id="S10.E5.m1.1.1.3.4.3.3" xref="S10.E5.m1.1.1.3.4.3.3.cmml">D</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S10.E5.m1.1b"><apply id="S10.E5.m1.1.1.cmml" xref="S10.E5.m1.1.1"><eq id="S10.E5.m1.1.1.1.cmml" xref="S10.E5.m1.1.1.1"></eq><apply id="S10.E5.m1.1.1.2.cmml" xref="S10.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S10.E5.m1.1.1.2.1.cmml" xref="S10.E5.m1.1.1.2">subscript</csymbol><ci id="S10.E5.m1.1.1.2.2.cmml" xref="S10.E5.m1.1.1.2.2">𝐹</ci><apply id="S10.E5.m1.1.1.2.3.cmml" xref="S10.E5.m1.1.1.2.3"><times id="S10.E5.m1.1.1.2.3.1.cmml" xref="S10.E5.m1.1.1.2.3.1"></times><ci id="S10.E5.m1.1.1.2.3.2.cmml" xref="S10.E5.m1.1.1.2.3.2">𝑆</ci><ci id="S10.E5.m1.1.1.2.3.3.cmml" xref="S10.E5.m1.1.1.2.3.3">𝐷</ci></apply></apply><apply id="S10.E5.m1.1.1.3.cmml" xref="S10.E5.m1.1.1.3"><ci id="S10.E5.m1.1.1.3.1.cmml" xref="S10.E5.m1.1.1.3.1">⋅</ci><apply id="S10.E5.m1.1.1.3.2.cmml" xref="S10.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S10.E5.m1.1.1.3.2.1.cmml" xref="S10.E5.m1.1.1.3.2">superscript</csymbol><apply id="S10.E5.m1.1.1.3.2.2.cmml" xref="S10.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S10.E5.m1.1.1.3.2.2.1.cmml" xref="S10.E5.m1.1.1.3.2">subscript</csymbol><ci id="S10.E5.m1.1.1.3.2.2.2.cmml" xref="S10.E5.m1.1.1.3.2.2.2">𝐴</ci><apply id="S10.E5.m1.1.1.3.2.2.3.cmml" xref="S10.E5.m1.1.1.3.2.2.3"><times id="S10.E5.m1.1.1.3.2.2.3.1.cmml" xref="S10.E5.m1.1.1.3.2.2.3.1"></times><ci id="S10.E5.m1.1.1.3.2.2.3.2.cmml" xref="S10.E5.m1.1.1.3.2.2.3.2">𝑆</ci><ci id="S10.E5.m1.1.1.3.2.2.3.3.cmml" xref="S10.E5.m1.1.1.3.2.2.3.3">𝑆</ci></apply></apply><ci id="S10.E5.m1.1.1.3.2.3.cmml" xref="S10.E5.m1.1.1.3.2.3">𝑜</ci></apply><apply id="S10.E5.m1.1.1.3.3.cmml" xref="S10.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S10.E5.m1.1.1.3.3.1.cmml" xref="S10.E5.m1.1.1.3.3">subscript</csymbol><ci id="S10.E5.m1.1.1.3.3.2.cmml" xref="S10.E5.m1.1.1.3.3.2">𝑀</ci><apply id="S10.E5.m1.1.1.3.3.3.cmml" xref="S10.E5.m1.1.1.3.3.3"><times id="S10.E5.m1.1.1.3.3.3.1.cmml" xref="S10.E5.m1.1.1.3.3.3.1"></times><ci id="S10.E5.m1.1.1.3.3.3.2.cmml" xref="S10.E5.m1.1.1.3.3.3.2">𝑆</ci><ci id="S10.E5.m1.1.1.3.3.3.3.cmml" xref="S10.E5.m1.1.1.3.3.3.3">𝑁</ci></apply></apply><apply id="S10.E5.m1.1.1.3.4.cmml" xref="S10.E5.m1.1.1.3.4"><csymbol cd="ambiguous" id="S10.E5.m1.1.1.3.4.1.cmml" xref="S10.E5.m1.1.1.3.4">subscript</csymbol><ci id="S10.E5.m1.1.1.3.4.2.cmml" xref="S10.E5.m1.1.1.3.4.2">𝑇</ci><apply id="S10.E5.m1.1.1.3.4.3.cmml" xref="S10.E5.m1.1.1.3.4.3"><times id="S10.E5.m1.1.1.3.4.3.1.cmml" xref="S10.E5.m1.1.1.3.4.3.1"></times><ci id="S10.E5.m1.1.1.3.4.3.2.cmml" xref="S10.E5.m1.1.1.3.4.3.2">𝑁</ci><ci id="S10.E5.m1.1.1.3.4.3.3.cmml" xref="S10.E5.m1.1.1.3.4.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.E5.m1.1c">F_{S\times D}=A_{S\times S}^{o}\cdot M_{S\times N}\ \cdot T_{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S10.E5.m1.1d">italic_F start_POSTSUBSCRIPT italic_S × italic_D end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_S × italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT ⋅ italic_M start_POSTSUBSCRIPT italic_S × italic_N end_POSTSUBSCRIPT ⋅ italic_T start_POSTSUBSCRIPT italic_N × italic_D end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<section class="ltx_paragraph" id="S10.SS1.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">SegVLAD:</h6>
<div class="ltx_para" id="S10.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S10.SS1.SSS0.Px1.p1.4">Given the segment adjacency matrix <math alttext="A^{o}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S10.SS1.SSS0.Px1.p1.1.m1.1a"><msup id="S10.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">A</mi><mi id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><ci id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐴</ci><ci id="S10.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S10.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.1.m1.1c">A^{o}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.1.m1.1d">italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT</annotation></semantics></math> and binary masks <math alttext="M" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S10.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S10.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S10.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S10.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.2.m2.1d">italic_M</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px1.p1.4.1">each</span> cluster (<math alttext="k" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S10.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S10.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S10.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S10.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.3.m3.1d">italic_k</annotation></semantics></math>) undergoes segment-wise aggregation by setting <math alttext="T^{k}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S10.SS1.SSS0.Px1.p1.4.m4.1a"><msup id="S10.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">T</mi><mi id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><ci id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1.2">𝑇</ci><ci id="S10.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S10.SS1.SSS0.Px1.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.4.m4.1c">T^{k}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.4.m4.1d">italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> as below:</p>
<table class="ltx_equation ltx_eqn_table" id="S10.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="T^{k}_{{N_{k}}\times D}=[T^{k}_{1},T^{k}_{2}\dots T^{k}_{N_{k}}]^{\top};\quad T%
^{k}_{p}=\alpha_{k}(f_{p})(f_{p}-c_{k})" class="ltx_Math" display="block" id="S10.E6.m1.2"><semantics id="S10.E6.m1.2a"><mrow id="S10.E6.m1.2.2.2" xref="S10.E6.m1.2.2.3.cmml"><mrow id="S10.E6.m1.1.1.1.1" xref="S10.E6.m1.1.1.1.1.cmml"><msubsup id="S10.E6.m1.1.1.1.1.4" xref="S10.E6.m1.1.1.1.1.4.cmml"><mi id="S10.E6.m1.1.1.1.1.4.2.2" xref="S10.E6.m1.1.1.1.1.4.2.2.cmml">T</mi><mrow id="S10.E6.m1.1.1.1.1.4.3" xref="S10.E6.m1.1.1.1.1.4.3.cmml"><msub id="S10.E6.m1.1.1.1.1.4.3.2" xref="S10.E6.m1.1.1.1.1.4.3.2.cmml"><mi id="S10.E6.m1.1.1.1.1.4.3.2.2" xref="S10.E6.m1.1.1.1.1.4.3.2.2.cmml">N</mi><mi id="S10.E6.m1.1.1.1.1.4.3.2.3" xref="S10.E6.m1.1.1.1.1.4.3.2.3.cmml">k</mi></msub><mo id="S10.E6.m1.1.1.1.1.4.3.1" lspace="0.222em" rspace="0.222em" xref="S10.E6.m1.1.1.1.1.4.3.1.cmml">×</mo><mi id="S10.E6.m1.1.1.1.1.4.3.3" xref="S10.E6.m1.1.1.1.1.4.3.3.cmml">D</mi></mrow><mi id="S10.E6.m1.1.1.1.1.4.2.3" xref="S10.E6.m1.1.1.1.1.4.2.3.cmml">k</mi></msubsup><mo id="S10.E6.m1.1.1.1.1.3" xref="S10.E6.m1.1.1.1.1.3.cmml">=</mo><msup id="S10.E6.m1.1.1.1.1.2" xref="S10.E6.m1.1.1.1.1.2.cmml"><mrow id="S10.E6.m1.1.1.1.1.2.2.2" xref="S10.E6.m1.1.1.1.1.2.2.3.cmml"><mo id="S10.E6.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S10.E6.m1.1.1.1.1.2.2.3.cmml">[</mo><msubsup id="S10.E6.m1.1.1.1.1.1.1.1.1" xref="S10.E6.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S10.E6.m1.1.1.1.1.1.1.1.1.2.2" xref="S10.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mn id="S10.E6.m1.1.1.1.1.1.1.1.1.3" xref="S10.E6.m1.1.1.1.1.1.1.1.1.3.cmml">1</mn><mi id="S10.E6.m1.1.1.1.1.1.1.1.1.2.3" xref="S10.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo id="S10.E6.m1.1.1.1.1.2.2.2.4" xref="S10.E6.m1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S10.E6.m1.1.1.1.1.2.2.2.2" xref="S10.E6.m1.1.1.1.1.2.2.2.2.cmml"><msubsup id="S10.E6.m1.1.1.1.1.2.2.2.2.2" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">T</mi><mn id="S10.E6.m1.1.1.1.1.2.2.2.2.2.3" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.3.cmml">2</mn><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">k</mi></msubsup><mo id="S10.E6.m1.1.1.1.1.2.2.2.2.1" xref="S10.E6.m1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.3" mathvariant="normal" xref="S10.E6.m1.1.1.1.1.2.2.2.2.3.cmml">…</mi><mo id="S10.E6.m1.1.1.1.1.2.2.2.2.1a" xref="S10.E6.m1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><msubsup id="S10.E6.m1.1.1.1.1.2.2.2.2.4" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.cmml"><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.2" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.2.cmml">T</mi><msub id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.cmml"><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.2" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.2.cmml">N</mi><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.3" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.3.cmml">k</mi></msub><mi id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.3" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.3.cmml">k</mi></msubsup></mrow><mo id="S10.E6.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S10.E6.m1.1.1.1.1.2.2.3.cmml">]</mo></mrow><mo id="S10.E6.m1.1.1.1.1.2.4" xref="S10.E6.m1.1.1.1.1.2.4.cmml">⊤</mo></msup></mrow><mo id="S10.E6.m1.2.2.2.3" rspace="1.167em" xref="S10.E6.m1.2.2.3a.cmml">;</mo><mrow id="S10.E6.m1.2.2.2.2" xref="S10.E6.m1.2.2.2.2.cmml"><msubsup id="S10.E6.m1.2.2.2.2.4" xref="S10.E6.m1.2.2.2.2.4.cmml"><mi id="S10.E6.m1.2.2.2.2.4.2.2" xref="S10.E6.m1.2.2.2.2.4.2.2.cmml">T</mi><mi id="S10.E6.m1.2.2.2.2.4.3" xref="S10.E6.m1.2.2.2.2.4.3.cmml">p</mi><mi id="S10.E6.m1.2.2.2.2.4.2.3" xref="S10.E6.m1.2.2.2.2.4.2.3.cmml">k</mi></msubsup><mo id="S10.E6.m1.2.2.2.2.3" xref="S10.E6.m1.2.2.2.2.3.cmml">=</mo><mrow id="S10.E6.m1.2.2.2.2.2" xref="S10.E6.m1.2.2.2.2.2.cmml"><msub id="S10.E6.m1.2.2.2.2.2.4" xref="S10.E6.m1.2.2.2.2.2.4.cmml"><mi id="S10.E6.m1.2.2.2.2.2.4.2" xref="S10.E6.m1.2.2.2.2.2.4.2.cmml">α</mi><mi id="S10.E6.m1.2.2.2.2.2.4.3" xref="S10.E6.m1.2.2.2.2.2.4.3.cmml">k</mi></msub><mo id="S10.E6.m1.2.2.2.2.2.3" xref="S10.E6.m1.2.2.2.2.2.3.cmml">⁢</mo><mrow id="S10.E6.m1.2.2.2.2.1.1.1" xref="S10.E6.m1.2.2.2.2.1.1.1.1.cmml"><mo id="S10.E6.m1.2.2.2.2.1.1.1.2" stretchy="false" xref="S10.E6.m1.2.2.2.2.1.1.1.1.cmml">(</mo><msub id="S10.E6.m1.2.2.2.2.1.1.1.1" xref="S10.E6.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S10.E6.m1.2.2.2.2.1.1.1.1.2" xref="S10.E6.m1.2.2.2.2.1.1.1.1.2.cmml">f</mi><mi id="S10.E6.m1.2.2.2.2.1.1.1.1.3" xref="S10.E6.m1.2.2.2.2.1.1.1.1.3.cmml">p</mi></msub><mo id="S10.E6.m1.2.2.2.2.1.1.1.3" stretchy="false" xref="S10.E6.m1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S10.E6.m1.2.2.2.2.2.3a" xref="S10.E6.m1.2.2.2.2.2.3.cmml">⁢</mo><mrow id="S10.E6.m1.2.2.2.2.2.2.1" xref="S10.E6.m1.2.2.2.2.2.2.1.1.cmml"><mo id="S10.E6.m1.2.2.2.2.2.2.1.2" stretchy="false" xref="S10.E6.m1.2.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S10.E6.m1.2.2.2.2.2.2.1.1" xref="S10.E6.m1.2.2.2.2.2.2.1.1.cmml"><msub id="S10.E6.m1.2.2.2.2.2.2.1.1.2" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2.cmml"><mi id="S10.E6.m1.2.2.2.2.2.2.1.1.2.2" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2.2.cmml">f</mi><mi id="S10.E6.m1.2.2.2.2.2.2.1.1.2.3" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2.3.cmml">p</mi></msub><mo id="S10.E6.m1.2.2.2.2.2.2.1.1.1" xref="S10.E6.m1.2.2.2.2.2.2.1.1.1.cmml">−</mo><msub id="S10.E6.m1.2.2.2.2.2.2.1.1.3" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3.cmml"><mi id="S10.E6.m1.2.2.2.2.2.2.1.1.3.2" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3.2.cmml">c</mi><mi id="S10.E6.m1.2.2.2.2.2.2.1.1.3.3" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S10.E6.m1.2.2.2.2.2.2.1.3" stretchy="false" xref="S10.E6.m1.2.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S10.E6.m1.2b"><apply id="S10.E6.m1.2.2.3.cmml" xref="S10.E6.m1.2.2.2"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.3a.cmml" xref="S10.E6.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S10.E6.m1.1.1.1.1.cmml" xref="S10.E6.m1.1.1.1.1"><eq id="S10.E6.m1.1.1.1.1.3.cmml" xref="S10.E6.m1.1.1.1.1.3"></eq><apply id="S10.E6.m1.1.1.1.1.4.cmml" xref="S10.E6.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.4.1.cmml" xref="S10.E6.m1.1.1.1.1.4">subscript</csymbol><apply id="S10.E6.m1.1.1.1.1.4.2.cmml" xref="S10.E6.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.4.2.1.cmml" xref="S10.E6.m1.1.1.1.1.4">superscript</csymbol><ci id="S10.E6.m1.1.1.1.1.4.2.2.cmml" xref="S10.E6.m1.1.1.1.1.4.2.2">𝑇</ci><ci id="S10.E6.m1.1.1.1.1.4.2.3.cmml" xref="S10.E6.m1.1.1.1.1.4.2.3">𝑘</ci></apply><apply id="S10.E6.m1.1.1.1.1.4.3.cmml" xref="S10.E6.m1.1.1.1.1.4.3"><times id="S10.E6.m1.1.1.1.1.4.3.1.cmml" xref="S10.E6.m1.1.1.1.1.4.3.1"></times><apply id="S10.E6.m1.1.1.1.1.4.3.2.cmml" xref="S10.E6.m1.1.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.4.3.2.1.cmml" xref="S10.E6.m1.1.1.1.1.4.3.2">subscript</csymbol><ci id="S10.E6.m1.1.1.1.1.4.3.2.2.cmml" xref="S10.E6.m1.1.1.1.1.4.3.2.2">𝑁</ci><ci id="S10.E6.m1.1.1.1.1.4.3.2.3.cmml" xref="S10.E6.m1.1.1.1.1.4.3.2.3">𝑘</ci></apply><ci id="S10.E6.m1.1.1.1.1.4.3.3.cmml" xref="S10.E6.m1.1.1.1.1.4.3.3">𝐷</ci></apply></apply><apply id="S10.E6.m1.1.1.1.1.2.cmml" xref="S10.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.3.cmml" xref="S10.E6.m1.1.1.1.1.2">superscript</csymbol><interval closure="closed" id="S10.E6.m1.1.1.1.1.2.2.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2"><apply id="S10.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S10.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S10.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S10.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S10.E6.m1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><cn id="S10.E6.m1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S10.E6.m1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2"><times id="S10.E6.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.1"></times><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.2">𝑇</ci><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.2.3">𝑘</ci></apply><cn id="S10.E6.m1.1.1.1.1.2.2.2.2.2.3.cmml" type="integer" xref="S10.E6.m1.1.1.1.1.2.2.2.2.2.3">2</cn></apply><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.3">…</ci><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.4.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.2.2.2.4.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4">subscript</csymbol><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4">superscript</csymbol><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.2">𝑇</ci><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.2.3">𝑘</ci></apply><apply id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3"><csymbol cd="ambiguous" id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.1.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3">subscript</csymbol><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.2.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.2">𝑁</ci><ci id="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.3.cmml" xref="S10.E6.m1.1.1.1.1.2.2.2.2.4.3.3">𝑘</ci></apply></apply></apply></interval><csymbol cd="latexml" id="S10.E6.m1.1.1.1.1.2.4.cmml" xref="S10.E6.m1.1.1.1.1.2.4">top</csymbol></apply></apply><apply id="S10.E6.m1.2.2.2.2.cmml" xref="S10.E6.m1.2.2.2.2"><eq id="S10.E6.m1.2.2.2.2.3.cmml" xref="S10.E6.m1.2.2.2.2.3"></eq><apply id="S10.E6.m1.2.2.2.2.4.cmml" xref="S10.E6.m1.2.2.2.2.4"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.4.1.cmml" xref="S10.E6.m1.2.2.2.2.4">subscript</csymbol><apply id="S10.E6.m1.2.2.2.2.4.2.cmml" xref="S10.E6.m1.2.2.2.2.4"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.4.2.1.cmml" xref="S10.E6.m1.2.2.2.2.4">superscript</csymbol><ci id="S10.E6.m1.2.2.2.2.4.2.2.cmml" xref="S10.E6.m1.2.2.2.2.4.2.2">𝑇</ci><ci id="S10.E6.m1.2.2.2.2.4.2.3.cmml" xref="S10.E6.m1.2.2.2.2.4.2.3">𝑘</ci></apply><ci id="S10.E6.m1.2.2.2.2.4.3.cmml" xref="S10.E6.m1.2.2.2.2.4.3">𝑝</ci></apply><apply id="S10.E6.m1.2.2.2.2.2.cmml" xref="S10.E6.m1.2.2.2.2.2"><times id="S10.E6.m1.2.2.2.2.2.3.cmml" xref="S10.E6.m1.2.2.2.2.2.3"></times><apply id="S10.E6.m1.2.2.2.2.2.4.cmml" xref="S10.E6.m1.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.2.4.1.cmml" xref="S10.E6.m1.2.2.2.2.2.4">subscript</csymbol><ci id="S10.E6.m1.2.2.2.2.2.4.2.cmml" xref="S10.E6.m1.2.2.2.2.2.4.2">𝛼</ci><ci id="S10.E6.m1.2.2.2.2.2.4.3.cmml" xref="S10.E6.m1.2.2.2.2.2.4.3">𝑘</ci></apply><apply id="S10.E6.m1.2.2.2.2.1.1.1.1.cmml" xref="S10.E6.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S10.E6.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S10.E6.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S10.E6.m1.2.2.2.2.1.1.1.1.2">𝑓</ci><ci id="S10.E6.m1.2.2.2.2.1.1.1.1.3.cmml" xref="S10.E6.m1.2.2.2.2.1.1.1.1.3">𝑝</ci></apply><apply id="S10.E6.m1.2.2.2.2.2.2.1.1.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1"><minus id="S10.E6.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.1"></minus><apply id="S10.E6.m1.2.2.2.2.2.2.1.1.2.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.2.2.1.1.2.1.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S10.E6.m1.2.2.2.2.2.2.1.1.2.2.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2.2">𝑓</ci><ci id="S10.E6.m1.2.2.2.2.2.2.1.1.2.3.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.2.3">𝑝</ci></apply><apply id="S10.E6.m1.2.2.2.2.2.2.1.1.3.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S10.E6.m1.2.2.2.2.2.2.1.1.3.1.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S10.E6.m1.2.2.2.2.2.2.1.1.3.2.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3.2">𝑐</ci><ci id="S10.E6.m1.2.2.2.2.2.2.1.1.3.3.cmml" xref="S10.E6.m1.2.2.2.2.2.2.1.1.3.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.E6.m1.2c">T^{k}_{{N_{k}}\times D}=[T^{k}_{1},T^{k}_{2}\dots T^{k}_{N_{k}}]^{\top};\quad T%
^{k}_{p}=\alpha_{k}(f_{p})(f_{p}-c_{k})</annotation><annotation encoding="application/x-llamapun" id="S10.E6.m1.2d">italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT × italic_D end_POSTSUBSCRIPT = [ italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT … italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ; italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ( italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.SS1.SSS0.Px1.p1.6">For <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px1.p1.6.1">SegVLAD</span>, without neighborhood aggregation, <math alttext="A^{o}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.5.m1.1"><semantics id="S10.SS1.SSS0.Px1.p1.5.m1.1a"><msup id="S10.SS1.SSS0.Px1.p1.5.m1.1.1" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1.cmml"><mi id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.2" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1.2.cmml">A</mi><mi id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.3" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.5.m1.1b"><apply id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1">superscript</csymbol><ci id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.2.cmml" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1.2">𝐴</ci><ci id="S10.SS1.SSS0.Px1.p1.5.m1.1.1.3.cmml" xref="S10.SS1.SSS0.Px1.p1.5.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.5.m1.1c">A^{o}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.5.m1.1d">italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT</annotation></semantics></math> is set as an identity matrix <math alttext="I_{S}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px1.p1.6.m2.1"><semantics id="S10.SS1.SSS0.Px1.p1.6.m2.1a"><msub id="S10.SS1.SSS0.Px1.p1.6.m2.1.1" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1.cmml"><mi id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.2" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1.2.cmml">I</mi><mi id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.3" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px1.p1.6.m2.1b"><apply id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.1.cmml" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1">subscript</csymbol><ci id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.2.cmml" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1.2">𝐼</ci><ci id="S10.SS1.SSS0.Px1.p1.6.m2.1.1.3.cmml" xref="S10.SS1.SSS0.Px1.p1.6.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px1.p1.6.m2.1c">I_{S}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px1.p1.6.m2.1d">italic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS1.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">GlobalVLAD:</h6>
<div class="ltx_para" id="S10.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S10.SS1.SSS0.Px2.p1.7"><math alttext="T" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S10.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S10.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S10.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S10.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.1.m1.1d">italic_T</annotation></semantics></math> is defined in the same way as Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.E6" title="Equation 6 ‣ SegVLAD: ‣ 10.1 Factorized Aggregation ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a>; segment adjacency matrix is not applicable and set to identity (<math alttext="A^{o}=I_{1}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S10.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S10.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><msup id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mi id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">A</mi><mi id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">o</mi></msup><mo id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><msub id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">I</mi><mn id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1"><eq id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.1"></eq><apply id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.2">𝐴</ci><ci id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.2.3">𝑜</ci></apply><apply id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.2">𝐼</ci><cn id="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S10.SS1.SSS0.Px2.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.2.m2.1c">A^{o}=I_{1}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.2.m2.1d">italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT = italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>); and the segmentation mask is set as an all-ones matrix <math alttext="M=J_{1\times N}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S10.SS1.SSS0.Px2.p1.3.m3.1a"><mrow id="S10.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">M</mi><mo id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.1" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml">=</mo><msub id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">J</mi><mrow id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.cmml"><mn id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">1</mn><mo id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1"><eq id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.1"></eq><ci id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.2">𝑀</ci><apply id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.2">𝐽</ci><apply id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3"><times id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.1"></times><cn id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.2">1</cn><ci id="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.3.m3.1.1.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.3.m3.1c">M=J_{1\times N}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.3.m3.1d">italic_M = italic_J start_POSTSUBSCRIPT 1 × italic_N end_POSTSUBSCRIPT</annotation></semantics></math>. Interestingly, the proposed factorization can further be adapted to obtain a <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px2.p1.7.1">GlobalVLAD</span> representation for all cluster centers in a <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px2.p1.7.2">single shot</span> by setting <math alttext="M_{S\times N}=M^{\prime}_{C\times N}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S10.SS1.SSS0.Px2.p1.4.m4.1a"><mrow id="S10.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><msub id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.2" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">M</mi><mrow id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.2" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.2.cmml">S</mi><mo id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.3.cmml">N</mi></mrow></msub><mo id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.1" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml">=</mo><msubsup id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.2" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.2.cmml">M</mi><mrow id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml">C</mi><mo id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml">N</mi></mrow><mo id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.3" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.3.cmml">′</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1"><eq id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.1"></eq><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.2">𝑀</ci><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3"><times id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.1"></times><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.2">𝑆</ci><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.2.3.3">𝑁</ci></apply></apply><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3">subscript</csymbol><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.2">𝑀</ci><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.3">′</ci></apply><apply id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3"><times id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1"></times><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2">𝐶</ci><ci id="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.4.m4.1c">M_{S\times N}=M^{\prime}_{C\times N}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.4.m4.1d">italic_M start_POSTSUBSCRIPT italic_S × italic_N end_POSTSUBSCRIPT = italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C × italic_N end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="M^{\prime}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S10.SS1.SSS0.Px2.p1.5.m5.1a"><msup id="S10.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.2" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml">M</mi><mo id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.3" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1">superscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1.2">𝑀</ci><ci id="S10.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S10.SS1.SSS0.Px2.p1.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.5.m5.1c">M^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.5.m5.1d">italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> represents membership of <math alttext="f_{p}\;(p\in[1,N])" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.6.m6.3"><semantics id="S10.SS1.SSS0.Px2.p1.6.m6.3a"><mrow id="S10.SS1.SSS0.Px2.p1.6.m6.3.3" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.cmml"><msub id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.2" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.2.cmml">f</mi><mi id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.3" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.3.cmml">p</mi></msub><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.2" lspace="0.280em" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.2.cmml">⁢</mo><mrow id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.cmml"><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.2" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.cmml">(</mo><mrow id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.cmml"><mi id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.2" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.2.cmml">p</mi><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.1" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.1.cmml">∈</mo><mrow id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.2" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.1.cmml"><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.2.1" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.1.cmml">[</mo><mn id="S10.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S10.SS1.SSS0.Px2.p1.6.m6.1.1.cmml">1</mn><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.2.2" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.1.cmml">,</mo><mi id="S10.SS1.SSS0.Px2.p1.6.m6.2.2" xref="S10.SS1.SSS0.Px2.p1.6.m6.2.2.cmml">N</mi><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.2.3" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.1.cmml">]</mo></mrow></mrow><mo id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.3" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.6.m6.3b"><apply id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3"><times id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.2"></times><apply id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.2">𝑓</ci><ci id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.3.3">𝑝</ci></apply><apply id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1"><in id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.1"></in><ci id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.2">𝑝</ci><interval closure="closed" id="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.3.3.1.1.1.3.2"><cn id="S10.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="S10.SS1.SSS0.Px2.p1.6.m6.1.1">1</cn><ci id="S10.SS1.SSS0.Px2.p1.6.m6.2.2.cmml" xref="S10.SS1.SSS0.Px2.p1.6.m6.2.2">𝑁</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.6.m6.3c">f_{p}\;(p\in[1,N])</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.6.m6.3d">italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_p ∈ [ 1 , italic_N ] )</annotation></semantics></math> in cluster <math alttext="c_{k}\;(k\in[1,C])" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px2.p1.7.m7.3"><semantics id="S10.SS1.SSS0.Px2.p1.7.m7.3a"><mrow id="S10.SS1.SSS0.Px2.p1.7.m7.3.3" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.cmml"><msub id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.cmml"><mi id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.2" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.2.cmml">c</mi><mi id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.3" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.3.cmml">k</mi></msub><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.2" lspace="0.280em" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.2.cmml">⁢</mo><mrow id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.cmml"><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.2" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.cmml">(</mo><mrow id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.cmml"><mi id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.2" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.2.cmml">k</mi><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.1" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.1.cmml">∈</mo><mrow id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.2" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.1.cmml"><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.2.1" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.1.cmml">[</mo><mn id="S10.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S10.SS1.SSS0.Px2.p1.7.m7.1.1.cmml">1</mn><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.2.2" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.1.cmml">,</mo><mi id="S10.SS1.SSS0.Px2.p1.7.m7.2.2" xref="S10.SS1.SSS0.Px2.p1.7.m7.2.2.cmml">C</mi><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.2.3" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.1.cmml">]</mo></mrow></mrow><mo id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.3" stretchy="false" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px2.p1.7.m7.3b"><apply id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3"><times id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.2"></times><apply id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.2.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.2">𝑐</ci><ci id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.3.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.3.3">𝑘</ci></apply><apply id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1"><in id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.1.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.1"></in><ci id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.2.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.2">𝑘</ci><interval closure="closed" id="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.3.3.1.1.1.3.2"><cn id="S10.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="S10.SS1.SSS0.Px2.p1.7.m7.1.1">1</cn><ci id="S10.SS1.SSS0.Px2.p1.7.m7.2.2.cmml" xref="S10.SS1.SSS0.Px2.p1.7.m7.2.2">𝐶</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px2.p1.7.m7.3c">c_{k}\;(k\in[1,C])</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px2.p1.7.m7.3d">italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_k ∈ [ 1 , italic_C ] )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS1.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">Segment Average Pooling (SAP):</h6>
<div class="ltx_para" id="S10.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S10.SS1.SSS0.Px3.p1.3">Given the segment adjacency matrix <math alttext="A^{o}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S10.SS1.SSS0.Px3.p1.1.m1.1a"><msup id="S10.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">A</mi><mi id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1">superscript</csymbol><ci id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1.2">𝐴</ci><ci id="S10.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S10.SS1.SSS0.Px3.p1.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px3.p1.1.m1.1c">A^{o}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px3.p1.1.m1.1d">italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT</annotation></semantics></math> and binary masks <math alttext="M" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S10.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S10.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S10.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S10.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S10.SS1.SSS0.Px3.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px3.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px3.p1.2.m2.1d">italic_M</annotation></semantics></math>, the output of the image encoder is directly used as <math alttext="T_{N\times D}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px3.p1.3.m3.1"><semantics id="S10.SS1.SSS0.Px3.p1.3.m3.1a"><msub id="S10.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.2" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml">T</mi><mrow id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">N</mi><mo id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px3.p1.3.m3.1b"><apply id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.2">𝑇</ci><apply id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3"><times id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.1"></times><ci id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.2">𝑁</ci><ci id="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px3.p1.3.m3.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px3.p1.3.m3.1c">T_{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px3.p1.3.m3.1d">italic_T start_POSTSUBSCRIPT italic_N × italic_D end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS1.SSS0.Px4">
<h6 class="ltx_title ltx_title_paragraph">Global Average Pooling (GAP):</h6>
<div class="ltx_para" id="S10.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S10.SS1.SSS0.Px4.p1.3">Similar to <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px4.p1.3.1">SAP</span>, <math alttext="T_{N\times D}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px4.p1.1.m1.1"><semantics id="S10.SS1.SSS0.Px4.p1.1.m1.1a"><msub id="S10.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">T</mi><mrow id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">N</mi><mo id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.3" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.2">𝑇</ci><apply id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3"><times id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.1"></times><ci id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.2">𝑁</ci><ci id="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px4.p1.1.m1.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px4.p1.1.m1.1c">T_{N\times D}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px4.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_N × italic_D end_POSTSUBSCRIPT</annotation></semantics></math> is the direct output of the image encoder, whereas the adjacency matrix is set to identity (<math alttext="A^{o}=I_{1}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px4.p1.2.m2.1"><semantics id="S10.SS1.SSS0.Px4.p1.2.m2.1a"><mrow id="S10.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><msup id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">A</mi><mi id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">o</mi></msup><mo id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.1" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><msub id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.2" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml">I</mi><mn id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.3" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1"><eq id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2">superscript</csymbol><ci id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.2">𝐴</ci><ci id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.2.3">𝑜</ci></apply><apply id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.2">𝐼</ci><cn id="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S10.SS1.SSS0.Px4.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px4.p1.2.m2.1c">A^{o}=I_{1}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px4.p1.2.m2.1d">italic_A start_POSTSUPERSCRIPT italic_o end_POSTSUPERSCRIPT = italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>). The segmentation mask is set as an all-ones matrix <math alttext="M=J_{1\times N}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px4.p1.3.m3.1"><semantics id="S10.SS1.SSS0.Px4.p1.3.m3.1a"><mrow id="S10.SS1.SSS0.Px4.p1.3.m3.1.1" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.2" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml">M</mi><mo id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.1" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.1.cmml">=</mo><msub id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.2" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.2.cmml">J</mi><mrow id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.cmml"><mn id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.2" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.2.cmml">1</mn><mo id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.3" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.3.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px4.p1.3.m3.1b"><apply id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1"><eq id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.1"></eq><ci id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.2">𝑀</ci><apply id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3">subscript</csymbol><ci id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.2">𝐽</ci><apply id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3"><times id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.1.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.1"></times><cn id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.2">1</cn><ci id="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.3.cmml" xref="S10.SS1.SSS0.Px4.p1.3.m3.1.1.3.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px4.p1.3.m3.1c">M=J_{1\times N}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px4.p1.3.m3.1d">italic_M = italic_J start_POSTSUBSCRIPT 1 × italic_N end_POSTSUBSCRIPT</annotation></semantics></math>, which is similar to <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS0.Px4.p1.3.2">GlobalVLAD</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS1.SSS0.Px5">
<h6 class="ltx_title ltx_title_paragraph">Generalized Mean Pooling (GeM):</h6>
<div class="ltx_para" id="S10.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S10.SS1.SSS0.Px5.p1.5">The above formulations at both segments and global level can be easily extended to GeM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib56" title="">56</a>]</cite> through <math alttext="T=T^{p}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px5.p1.1.m1.1"><semantics id="S10.SS1.SSS0.Px5.p1.1.m1.1a"><mrow id="S10.SS1.SSS0.Px5.p1.1.m1.1.1" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.2" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml">T</mi><mo id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.1" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml">=</mo><msup id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.2" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.2.cmml">T</mi><mi id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.3" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.3.cmml">p</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px5.p1.1.m1.1b"><apply id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1"><eq id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.1"></eq><ci id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.2">𝑇</ci><apply id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3">superscript</csymbol><ci id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.2">𝑇</ci><ci id="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px5.p1.1.m1.1.1.3.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px5.p1.1.m1.1c">T=T^{p}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px5.p1.1.m1.1d">italic_T = italic_T start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="F=F^{1/p}" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px5.p1.2.m2.1"><semantics id="S10.SS1.SSS0.Px5.p1.2.m2.1a"><mrow id="S10.SS1.SSS0.Px5.p1.2.m2.1.1" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.cmml"><mi id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.2" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml">F</mi><mo id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.1" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.1.cmml">=</mo><msup id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml"><mi id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.2" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.2.cmml">F</mi><mrow id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.cmml"><mn id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.2" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.2.cmml">1</mn><mo id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.1" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.1.cmml">/</mo><mi id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.3" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.3.cmml">p</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px5.p1.2.m2.1b"><apply id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1"><eq id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.1"></eq><ci id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.2.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.2">𝐹</ci><apply id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.1.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3">superscript</csymbol><ci id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.2.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.2">𝐹</ci><apply id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3"><divide id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.1.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.1"></divide><cn id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.2">1</cn><ci id="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.3.cmml" xref="S10.SS1.SSS0.Px5.p1.2.m2.1.1.3.3.3">𝑝</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px5.p1.2.m2.1c">F=F^{1/p}</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px5.p1.2.m2.1d">italic_F = italic_F start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="p=1" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px5.p1.3.m3.1"><semantics id="S10.SS1.SSS0.Px5.p1.3.m3.1a"><mrow id="S10.SS1.SSS0.Px5.p1.3.m3.1.1" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.cmml"><mi id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.2" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml">p</mi><mo id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.1" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.3" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px5.p1.3.m3.1b"><apply id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1"><eq id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.1"></eq><ci id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.2.cmml" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.2">𝑝</ci><cn id="S10.SS1.SSS0.Px5.p1.3.m3.1.1.3.cmml" type="integer" xref="S10.SS1.SSS0.Px5.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px5.p1.3.m3.1c">p=1</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px5.p1.3.m3.1d">italic_p = 1</annotation></semantics></math> represents average pooling, <math alttext="p=\infty" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px5.p1.4.m4.1"><semantics id="S10.SS1.SSS0.Px5.p1.4.m4.1a"><mrow id="S10.SS1.SSS0.Px5.p1.4.m4.1.1" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.cmml"><mi id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.2" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.2.cmml">p</mi><mo id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.1" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.1.cmml">=</mo><mi id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.3" mathvariant="normal" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.3.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px5.p1.4.m4.1b"><apply id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1"><eq id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.1"></eq><ci id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.2.cmml" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.2">𝑝</ci><infinity id="S10.SS1.SSS0.Px5.p1.4.m4.1.1.3.cmml" xref="S10.SS1.SSS0.Px5.p1.4.m4.1.1.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px5.p1.4.m4.1c">p=\infty</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px5.p1.4.m4.1d">italic_p = ∞</annotation></semantics></math> represents max pooling and <math alttext="p=3" class="ltx_Math" display="inline" id="S10.SS1.SSS0.Px5.p1.5.m5.1"><semantics id="S10.SS1.SSS0.Px5.p1.5.m5.1a"><mrow id="S10.SS1.SSS0.Px5.p1.5.m5.1.1" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.cmml"><mi id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.2" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.2.cmml">p</mi><mo id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.1" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.1.cmml">=</mo><mn id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.3" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS1.SSS0.Px5.p1.5.m5.1b"><apply id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1"><eq id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.1.cmml" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.1"></eq><ci id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.2.cmml" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.2">𝑝</ci><cn id="S10.SS1.SSS0.Px5.p1.5.m5.1.1.3.cmml" type="integer" xref="S10.SS1.SSS0.Px5.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS1.SSS0.Px5.p1.5.m5.1c">p=3</annotation><annotation encoding="application/x-llamapun" id="S10.SS1.SSS0.Px5.p1.5.m5.1d">italic_p = 3</annotation></semantics></math> represents its typical use in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S10.SS2">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>Backbone Networks</h4>
<section class="ltx_paragraph" id="S10.SS2.SSS0.Px1">
<h6 class="ltx_title ltx_title_paragraph">DINOv2:</h6>
<div class="ltx_para" id="S10.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S10.SS2.SSS0.Px1.p1.3">We follow AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> and use its default ViT-G backbone with the <span class="ltx_text ltx_font_typewriter" id="S10.SS2.SSS0.Px1.p1.3.1">value</span> facet features from layer <math alttext="31" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S10.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="S10.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S10.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">31</mn><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px1.p1.1.m1.1b"><cn id="S10.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S10.SS2.SSS0.Px1.p1.1.m1.1.1">31</cn></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px1.p1.1.m1.1c">31</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px1.p1.1.m1.1d">31</annotation></semantics></math>. For the DINOv2 <span class="ltx_text ltx_font_italic" id="S10.SS2.SSS0.Px1.p1.3.2">finetuned</span> model, we followed SALAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite> which by default uses ViT-B backbone. Note that SALAD’s aggregation is different from the soft assignment based VLAD aggregation proposed in NetVLAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib3" title="">3</a>]</cite>. Since our method is based on Hard-VLAD aggregation, we followed SALAD’s finetuning approach but replaced their aggregation with NetVLAD. Similar to SALAD, we only train the last 4 layers of DINOv2 (ViT-B) on the GSV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib1" title="">1</a>]</cite> with training image resolution as <math alttext="224\times 224" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S10.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S10.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">224</mn><mo id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1"><times id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.1"></times><cn id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.2">224</cn><cn id="S10.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S10.SS2.SSS0.Px1.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px1.p1.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px1.p1.2.m2.1d">224 × 224</annotation></semantics></math>. Similar to NetVLAD, we used <math alttext="64" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S10.SS2.SSS0.Px1.p1.3.m3.1a"><mn id="S10.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S10.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px1.p1.3.m3.1b"><cn id="S10.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="S10.SS2.SSS0.Px1.p1.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px1.p1.3.m3.1c">64</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px1.p1.3.m3.1d">64</annotation></semantics></math> clusters which were initialized by randomly sampling images from the GSV training set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS2.SSS0.Px2">
<h6 class="ltx_title ltx_title_paragraph">SAM:</h6>
<div class="ltx_para" id="S10.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S10.SS2.SSS0.Px2.p1.1">We use its ViT-H model with default parameters for segmentation. It generates masks for the entire image, using a grid of point prompts (32 along each edge), which are subsequently filtered based on IOU and stability score.</p>
</div>
</section>
<section class="ltx_paragraph" id="S10.SS2.SSS0.Px3">
<h6 class="ltx_title ltx_title_paragraph">Evaluation:</h6>
<div class="ltx_para" id="S10.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S10.SS2.SSS0.Px3.p1.3">For evaluation, we used <math alttext="640\times 480" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S10.SS2.SSS0.Px3.p1.1.m1.1a"><mrow id="S10.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">640</mn><mo id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1"><times id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.1"></times><cn id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.2">640</cn><cn id="S10.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px3.p1.1.m1.1c">640\times 480</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px3.p1.1.m1.1d">640 × 480</annotation></semantics></math> image resolution for DINOv2 encoder and <math alttext="320\times 240" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S10.SS2.SSS0.Px3.p1.2.m2.1a"><mrow id="S10.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.2" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml">320</mn><mo id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.3" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml">240</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px3.p1.2.m2.1b"><apply id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1"><times id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.1"></times><cn id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.2">320</cn><cn id="S10.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.2.m2.1.1.3">240</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px3.p1.2.m2.1c">320\times 240</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px3.p1.2.m2.1d">320 × 240</annotation></semantics></math> for SAM. For AmsterTime, we followed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib75" title="">75</a>]</cite> and used a fixed resolution of <math alttext="256\times 256" class="ltx_Math" display="inline" id="S10.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S10.SS2.SSS0.Px3.p1.3.m3.1a"><mrow id="S10.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.2" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml">256</mn><mo id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.3" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S10.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1"><times id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.1"></times><cn id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.2">256</cn><cn id="S10.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml" type="integer" xref="S10.SS2.SSS0.Px3.p1.3.m3.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.SS2.SSS0.Px3.p1.3.m3.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S10.SS2.SSS0.Px3.p1.3.m3.1d">256 × 256</annotation></semantics></math> for both the models. Note that we follow the exact same procedure of image resizing when comparing our segments based approach with their global counterparts, i.e., AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite> and SALAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite>. Additionally, for the baseline methods EigenPlaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib8" title="">8</a>]</cite>, CosPlace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite> and MixVPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib2" title="">2</a>]</cite>, we used ResNet50 backbone for all three methods with output descriptor dimensions of 2048 for EigenPlaces and Cosplace, and 4096 for MixVPR.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S10.SS3">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span>Datasets</h4>
<div class="ltx_para" id="S10.SS3.p1">
<p class="ltx_p" id="S10.SS3.p1.1">In this work, we used a variety of datasets covering both outdoor and indoor environments. Outdoor datasets include Pitts30k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib69" title="">69</a>]</cite>, AmsterTime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib75" title="">75</a>]</cite>, Mapillary Street Level Sequences (MSLS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib72" title="">72</a>]</cite>, SF-XL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite>, Revisted Oxford5K and Revisited Paris6k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib55" title="">55</a>]</cite> . Indoor datasets include Baidu Mall <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib67" title="">67</a>]</cite>, 17Places <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib80" title="">80</a>]</cite> and InsideOut <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib27" title="">27</a>]</cite>. While all these datasets exhibit strong viewpoint variations, they are significantly diverse in terms of appearance changes, perceptual aliasing (repetitive elements), type of environment/domain (indoor vs outdoor) and extent of temporal changes (e.g., matching historical images). We elaborate on each of the datasets in detail in the supplementary. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.T12" title="Table 12 ‣ 10.3 Datasets ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">12</span></a>, we report the number of reference and query images in each of these datasets. Furthermore, for our segments-based approach, we also note the total number of segments (based on SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib37" title="">37</a>]</cite>) across all the queries and references for each dataset, with the final columns listing the average number of segments per image. This highlights the scale at which we perform segments based retrieval, as opposed to global descriptors based retrieval.</p>
</div>
<figure class="ltx_table" id="S10.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S10.T12.10.1.1" style="font-size:90%;">Table 12</span>: </span><span class="ltx_text" id="S10.T12.11.2" style="font-size:90%;">An overview of the datasets in terms of the number of images (Ref/Qry), segments (Ref/Qry) and resolution used for image processing.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S10.T12.8" style="width:303.5pt;height:104.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-84.3pt,28.9pt) scale(0.643001192888492,0.643001192888492) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S10.T12.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S10.T12.8.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S10.T12.8.8.9.1.1" style="padding-left:16.5pt;padding-right:16.5pt;">     Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T12.8.8.9.1.2" style="padding-left:16.5pt;padding-right:16.5pt;">     Num. Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T12.8.8.9.1.3" style="padding-left:16.5pt;padding-right:16.5pt;">      Num. Segments</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T12.8.8.9.1.4" style="padding-left:16.5pt;padding-right:16.5pt;">     Avg. Seg./Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S10.T12.8.8.9.1.5" style="padding-left:16.5pt;padding-right:16.5pt;">     Resolution</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S10.T12.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S10.T12.1.1.1.2" style="padding-left:16.5pt;padding-right:16.5pt;">     Baidu</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T12.1.1.1.3" style="padding-left:16.5pt;padding-right:16.5pt;">       689 / 2292</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T12.1.1.1.4" style="padding-left:16.5pt;padding-right:16.5pt;">      92K / 295K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T12.1.1.1.5" style="padding-left:16.5pt;padding-right:16.5pt;">       134 / 129</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S10.T12.1.1.1.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.1.1.1.1.m1.1"><semantics id="S10.T12.1.1.1.1.m1.1a"><mo id="S10.T12.1.1.1.1.m1.1.1" xref="S10.T12.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.1.1.1.1.m1.1b"><times id="S10.T12.1.1.1.1.m1.1.1.cmml" xref="S10.T12.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.1.1.1.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
<tr class="ltx_tr" id="S10.T12.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.2.2.2.2" style="padding-left:16.5pt;padding-right:16.5pt;">     AmsterTime</th>
<td class="ltx_td ltx_align_center" id="S10.T12.2.2.2.3" style="padding-left:16.5pt;padding-right:16.5pt;">     1231 / 1231</td>
<td class="ltx_td ltx_align_center" id="S10.T12.2.2.2.4" style="padding-left:16.5pt;padding-right:16.5pt;">     129K / 119K</td>
<td class="ltx_td ltx_align_center" id="S10.T12.2.2.2.5" style="padding-left:16.5pt;padding-right:16.5pt;">     105 / 96</td>
<td class="ltx_td ltx_align_center" id="S10.T12.2.2.2.1" style="padding-left:16.5pt;padding-right:16.5pt;">     256<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.2.2.2.1.m1.1"><semantics id="S10.T12.2.2.2.1.m1.1a"><mo id="S10.T12.2.2.2.1.m1.1.1" xref="S10.T12.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.2.2.2.1.m1.1b"><times id="S10.T12.2.2.2.1.m1.1.1.cmml" xref="S10.T12.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.2.2.2.1.m1.1d">×</annotation></semantics></math>256</td>
</tr>
<tr class="ltx_tr" id="S10.T12.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.3.3.3.2" style="padding-left:16.5pt;padding-right:16.5pt;">     Pitts30K</th>
<td class="ltx_td ltx_align_center" id="S10.T12.3.3.3.3" style="padding-left:16.5pt;padding-right:16.5pt;">     10K / 6816</td>
<td class="ltx_td ltx_align_center" id="S10.T12.3.3.3.4" style="padding-left:16.5pt;padding-right:16.5pt;">     873K / 592K</td>
<td class="ltx_td ltx_align_center" id="S10.T12.3.3.3.5" style="padding-left:16.5pt;padding-right:16.5pt;">     87 / 86</td>
<td class="ltx_td ltx_align_center" id="S10.T12.3.3.3.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.3.3.3.1.m1.1"><semantics id="S10.T12.3.3.3.1.m1.1a"><mo id="S10.T12.3.3.3.1.m1.1.1" xref="S10.T12.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.3.3.3.1.m1.1b"><times id="S10.T12.3.3.3.1.m1.1.1.cmml" xref="S10.T12.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.3.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.3.3.3.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
<tr class="ltx_tr" id="S10.T12.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.4.4.4.2" style="padding-left:16.5pt;padding-right:16.5pt;">     MSLS CPH</th>
<td class="ltx_td ltx_align_center" id="S10.T12.4.4.4.3" style="padding-left:16.5pt;padding-right:16.5pt;">     6315 / 242</td>
<td class="ltx_td ltx_align_center" id="S10.T12.4.4.4.4" style="padding-left:16.5pt;padding-right:16.5pt;">     578k/23k</td>
<td class="ltx_td ltx_align_center" id="S10.T12.4.4.4.5" style="padding-left:16.5pt;padding-right:16.5pt;">     91/96</td>
<td class="ltx_td ltx_align_center" id="S10.T12.4.4.4.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.4.4.4.1.m1.1"><semantics id="S10.T12.4.4.4.1.m1.1a"><mo id="S10.T12.4.4.4.1.m1.1.1" xref="S10.T12.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.4.4.4.1.m1.1b"><times id="S10.T12.4.4.4.1.m1.1.1.cmml" xref="S10.T12.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.4.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.4.4.4.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
<tr class="ltx_tr" id="S10.T12.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.5.5.5.2" style="padding-left:16.5pt;padding-right:16.5pt;">     MSLS SF</th>
<td class="ltx_td ltx_align_center" id="S10.T12.5.5.5.3" style="padding-left:16.5pt;padding-right:16.5pt;">     12556/ 498</td>
<td class="ltx_td ltx_align_center" id="S10.T12.5.5.5.4" style="padding-left:16.5pt;padding-right:16.5pt;">     1.1M / 43k</td>
<td class="ltx_td ltx_align_center" id="S10.T12.5.5.5.5" style="padding-left:16.5pt;padding-right:16.5pt;">     89/87</td>
<td class="ltx_td ltx_align_center" id="S10.T12.5.5.5.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.5.5.5.1.m1.1"><semantics id="S10.T12.5.5.5.1.m1.1a"><mo id="S10.T12.5.5.5.1.m1.1.1" xref="S10.T12.5.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.5.5.5.1.m1.1b"><times id="S10.T12.5.5.5.1.m1.1.1.cmml" xref="S10.T12.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.5.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.5.5.5.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
<tr class="ltx_tr" id="S10.T12.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.6.6.6.2" style="padding-left:16.5pt;padding-right:16.5pt;">     SF-XL (Val)</th>
<td class="ltx_td ltx_align_center" id="S10.T12.6.6.6.3" style="padding-left:16.5pt;padding-right:16.5pt;">     8015/7993</td>
<td class="ltx_td ltx_align_center" id="S10.T12.6.6.6.4" style="padding-left:16.5pt;padding-right:16.5pt;">     648k/ 646k</td>
<td class="ltx_td ltx_align_center" id="S10.T12.6.6.6.5" style="padding-left:16.5pt;padding-right:16.5pt;">     81/ 81</td>
<td class="ltx_td ltx_align_center" id="S10.T12.6.6.6.1" style="padding-left:16.5pt;padding-right:16.5pt;">     512<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.6.6.6.1.m1.1"><semantics id="S10.T12.6.6.6.1.m1.1a"><mo id="S10.T12.6.6.6.1.m1.1.1" xref="S10.T12.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.6.6.6.1.m1.1b"><times id="S10.T12.6.6.6.1.m1.1.1.cmml" xref="S10.T12.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.6.6.6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.6.6.6.1.m1.1d">×</annotation></semantics></math>512</td>
</tr>
<tr class="ltx_tr" id="S10.T12.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S10.T12.7.7.7.2" style="padding-left:16.5pt;padding-right:16.5pt;">     InsideOut</th>
<td class="ltx_td ltx_align_center" id="S10.T12.7.7.7.3" style="padding-left:16.5pt;padding-right:16.5pt;">     10886 /500</td>
<td class="ltx_td ltx_align_center" id="S10.T12.7.7.7.4" style="padding-left:16.5pt;padding-right:16.5pt;">     821k/52k</td>
<td class="ltx_td ltx_align_center" id="S10.T12.7.7.7.5" style="padding-left:16.5pt;padding-right:16.5pt;">     75/106</td>
<td class="ltx_td ltx_align_center" id="S10.T12.7.7.7.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.7.7.7.1.m1.1"><semantics id="S10.T12.7.7.7.1.m1.1a"><mo id="S10.T12.7.7.7.1.m1.1.1" xref="S10.T12.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.7.7.7.1.m1.1b"><times id="S10.T12.7.7.7.1.m1.1.1.cmml" xref="S10.T12.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.7.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.7.7.7.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
<tr class="ltx_tr" id="S10.T12.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S10.T12.8.8.8.2" style="padding-left:16.5pt;padding-right:16.5pt;">     17Places</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T12.8.8.8.3" style="padding-left:16.5pt;padding-right:16.5pt;">     406/406</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T12.8.8.8.4" style="padding-left:16.5pt;padding-right:16.5pt;">     34k/34k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T12.8.8.8.5" style="padding-left:16.5pt;padding-right:16.5pt;">     84/86</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S10.T12.8.8.8.1" style="padding-left:16.5pt;padding-right:16.5pt;">     480<math alttext="\times" class="ltx_Math" display="inline" id="S10.T12.8.8.8.1.m1.1"><semantics id="S10.T12.8.8.8.1.m1.1a"><mo id="S10.T12.8.8.8.1.m1.1.1" xref="S10.T12.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S10.T12.8.8.8.1.m1.1b"><times id="S10.T12.8.8.8.1.m1.1.1.cmml" xref="S10.T12.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S10.T12.8.8.8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S10.T12.8.8.8.1.m1.1d">×</annotation></semantics></math>640</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S10.F7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S10.F7.28">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S10.F7.28.29.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S10.F7.28.29.1.1">Query</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S10.F7.28.29.1.2">SegVLAD (Ours)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S10.F7.28.29.1.3">AnyLoc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib32" title="">32</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S10.F7.28.29.1.4">SALAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib29" title="">29</a>]</cite>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S10.F7.4.4">
<td class="ltx_td ltx_align_center" id="S10.F7.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="372" id="S10.F7.1.1.1.g1" src="x2.png" width="215"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.2.2.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_match_idx_00006_seg_id_00013.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.3.3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.3.3.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_anyloc_cs_match_query_idx_00006_seg_id_00013.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.4.4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.4.4.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_salad_cs_match_query_idx_00006_seg_id_00013.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.8.8">
<td class="ltx_td ltx_align_center" id="S10.F7.5.5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="61" id="S10.F7.5.5.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_query_idx_00322_seg_id_00003.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.6.6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S10.F7.6.6.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_match_idx_00322_seg_id_00003.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.7.7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.7.7.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_anyloc_cs_match_query_idx_00322_seg_id_00003.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.8.8.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S10.F7.8.8.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_salad_cs_match_query_idx_00322_seg_id_00003.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.12.12">
<td class="ltx_td ltx_align_center" id="S10.F7.9.9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="192" id="S10.F7.9.9.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_query_idx_00832_seg_id_00000.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.10.10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S10.F7.10.10.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_match_idx_00832_seg_id_00000.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.11.11.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.11.11.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_anyloc_cs_match_query_idx_00832_seg_id_00000.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.12.12.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="72" id="S10.F7.12.12.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_salad_cs_match_query_idx_00832_seg_id_00000.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.16.16">
<td class="ltx_td ltx_align_center" id="S10.F7.13.13.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.13.13.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/single_cs_query_idx_02603_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.14.14.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.14.14.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/single_cs_match_idx_02603_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.15.15.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.15.15.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/anyloc_cs_match_query_idx_02603_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.16.16.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.16.16.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/salad_cs_match_query_idx_02603_seg_id_-0001.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.20.20">
<td class="ltx_td ltx_align_center" id="S10.F7.17.17.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="144" id="S10.F7.17.17.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/single_cs_query_idx_00407_seg_id_00010.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.18.18.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.18.18.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/single_cs_match_idx_00407_seg_id_00010.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.19.19.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.19.19.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/anyloc_cs_match_query_idx_00407_seg_id_00010.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.20.20.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.20.20.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/salad_cs_match_query_idx_00407_seg_id_00010.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.24.24">
<td class="ltx_td ltx_align_center" id="S10.F7.21.21.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.21.21.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/query_vpair.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.22.22.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.22.22.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/segvlad_vpair.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.23.23.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.23.23.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/anyloc_vpair.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.24.24.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.24.24.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/salad_vpair.png" width="108"/></td>
</tr>
<tr class="ltx_tr" id="S10.F7.28.28">
<td class="ltx_td ltx_align_center" id="S10.F7.25.25.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="107" id="S10.F7.25.25.1.g1" src="extracted/5879622/supp_qual_eg_downsampled/AmsterTime_nvsingle_cs_fail_query_idx_00376_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.26.26.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.26.26.2.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_single_cs_fail_match_idx_00376_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.27.27.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.27.27.3.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_anyloc_cs_match_fail_query_idx_00376_seg_id_-0001.png" width="108"/></td>
<td class="ltx_td ltx_align_center" id="S10.F7.28.28.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="81" id="S10.F7.28.28.4.g1" src="extracted/5879622/supp_qual_eg_downsampled/cropped_salad_cs_match_fail_query_idx_00376_seg_id_-0001.png" width="108"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F7.30.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S10.F7.31.2" style="font-size:90%;">Qualitative results: Columns respectively represent the query image, and predictions from SegVLAD, AnyLoc, and SALAD. Examples from different datasets: AmsterTime, Baidu Mall, Pitts-30K, InsideOut and VPAir are presented across the rows. </span></figcaption>
</figure>
<div class="ltx_para" id="S10.SS3.p2">
<ol class="ltx_enumerate" id="S10.I1">
<li class="ltx_item" id="S10.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S10.I1.i1.p1">
<p class="ltx_p" id="S10.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i1.p1.1.1">Baidu Mall</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib67" title="">67</a>]</cite> has images taken in a mall environment with varying camera poses. We use 2292 query images and 689 reference images. The dataset is characterized by extreme viewpoint shifts, cluttered environments, and semantic-rich information (logos, brand names, etc). It exhibits difficult appearance conditions of a commercial mall, characterized by reflective surfaces, non-uniform lighting, pedestrians, and perceptual aliasing caused by repetitive elements such as floor tiles, walls, stairs, and glass panes.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S10.I1.i2.p1">
<p class="ltx_p" id="S10.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i2.p1.1.1">AmsterTime</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib75" title="">75</a>]</cite> consists of 1231 pairs of reference and query images in the city of Amsterdam. It consists of grayscale historical images that are used as queries and modern RGB images of the same places (as confirmed by human experts) that are used as references. This dataset is characterized by long temporal changes, viewpoint shifts and modality (RGB vs grayscale). These drastic domain shifts make it a difficult dataset for VPR research.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S10.I1.i3.p1">
<p class="ltx_p" id="S10.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i3.p1.1.1">Pitts30K</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib69" title="">69</a>]</cite> is one of the most used benchmarking datasets for VPR research. It consists of images taken from Google Street View showing different locations in downtown Pittsburgh with varying camera poses. We use the test split having 10000 references and 6816 query images. It is a large-scale dataset with several similar-appearing buildings and visual distractors like cars and pedestrians.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S10.I1.i4.p1">
<p class="ltx_p" id="S10.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i4.p1.1.1">Mapillary Street-level Sequences</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib72" title="">72</a>]</cite> (MSLS) is a large-scale, diverse dataset containing 1.6M street-level images from 30 cities worldwide. The dataset is divided into training (22 cities, 1.4M images), validation (2 cities, 30K images), and test (6 cities, 66K images) sets across different times of day, seasons, and new/old (after several years). This is especially useful as a non-saturated benchmark. We use the 2 cities from validation dataset, i.e. Copenhagen (CPH) and San Francisco (SF). The reference/query split of Copenhagen is 6315/242 whereas that of San Francisco is 12556/498.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S10.I1.i5.p1">
<p class="ltx_p" id="S10.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i5.p1.1.1">SF-XL</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib6" title="">6</a>]</cite> This is a large scale dataset from the city of San Francisco with large-viewpoint changes. We use the validation split of this dataset with 8015 reference and 7993 query images.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S10.I1.i6.p1">
<p class="ltx_p" id="S10.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i6.p1.1.1">17Places</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib80" title="">80</a>]</cite> This is an indoor dataset which consists of buildings at Coast Capri Hotel (British Columbia) as well as York University (Canada). Both reference and query datasets have 406 images. This dataset is challenging because of significant changes in lighting conditions as well as cluttered indoor environments. On this dataset particularly, we report results for both 5 (default) and 15 frames GT radius (r) as it incorrectly penalized correct retrieval for the former.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S10.I1.i7.p1">
<p class="ltx_p" id="S10.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i7.p1.1.1">VPAir</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib62" title="">62</a>]</cite> This is an aerial dataset, consisting of 2706 database-query image pairs and 10,000 distractor images. These images are captured on an aircraft with a downward-facing camera at an altitude of 300 metres. Do note that we do not use the distractor set and our database and query images consist of 2706 images each, following AnyLoc’s approach. Note that the dataset covers extremely challenging landscapes such as urban regions, farmlands and forests over more than 100 km and can be considered an out-of-distribution (OOD) dataset. Here, we use a localization radius of 3 frames.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S10.I1.i8.p1">
<p class="ltx_p" id="S10.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i8.p1.1.1">InsideOut</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib27" title="">27</a>]</cite> This is a very interesting indoor/outdoor recognition dataset, wherein the task is to localize images based on outdoor scenes while viewing from indoor through windows. The images were taken in Amsterdam, the original dataset consisting of 6.4 million panoramic street-view images and 1000 user-generated indoor queries. However, we curate a smaller split of this dataset as follows: We use the test indoor queries set as our query images. For each query, we define a correct match if it is within a 50m radius; otherwise, it’s classified as a distractor. To curate this reference set, we use 7 correct matches and 15 distractors per query image. After removing repetitive images the final dataset consists of 500 queries and 10886 reference images.</p>
</div>
</li>
<li class="ltx_item" id="S10.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S10.I1.i9.p1">
<p class="ltx_p" id="S10.I1.i9.p1.1"><span class="ltx_text ltx_font_bold" id="S10.I1.i9.p1.1.1">RO5k and RP6k</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#bib.bib55" title="">55</a>]</cite> ROxford5K and RParis6k are classical instance retrieval benchmarks. Revisited Oxford5K has a reference/ query split of 4993/70 for 11 Oxford buildings and Paris6K has a reference query split of 6322/70 for 12 architectural groups. Each image of the same building is labelled as Good (i.e., positive), OK (i.e., positive), Junk, or Bad (i.e., negative) based on relevance. Junk images can be discarded or regarded as negative examples.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S11">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>More Qualitative Examples</h3>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S10.F7" title="Figure 7 ‣ 10.3 Datasets ‣ 10 Implementation and Benchmarking Details ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">7</span></a>, we present additional qualitative results for multiple datasets: Baidu, AmsterTime, Pitts30K, InsideOut, and VPAir, which not only highlight the extremities of viewpoint variations found in these datasets but also the capability of our method to retrieve correctly under such conditions. We compare SegVLAD with both AnyLoc and SALAD. The first column shows all the query images followed by the correct match predicted by SegVLAD, and incorrect matches predicted by AnyLoc and SALAD.
In the fourth row, we show an example from Pitts30K where SegVLAD and SALAD retrieve correctly but AnyLoc fails. The last row shows a case where all the methods fail.
We show all the qualitative results at the original resolution of the images.</p>
</div>
<div class="ltx_para" id="S11.p2">
<p class="ltx_p" id="S11.p2.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18049v1#S11.F8" title="Figure 8 ‣ 11 More Qualitative Examples ‣ Supplementary Material ‣ Revisit Anything: Visual Place Recognition via Image Segment Retrieval"><span class="ltx_text ltx_ref_tag">8</span></a>, we visualize the segments matched to the OOI queries using segment descriptors. While SegVLAD easily succeeds at recognizing large objects, it is particularly better than the global approach on recognizing small objects with the help of its expanded neighborhood.</p>
</div>
<figure class="ltx_figure" id="S11.F8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S11.F8.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S11.F8.4.4">
<td class="ltx_td ltx_align_center" id="S11.F8.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="178" id="S11.F8.1.1.1.g1" src="x3.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.2.2.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="100" id="S11.F8.2.2.2.g1" src="extracted/5879622/ooi/adidas_logo_img_cropped.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.3.3.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.3.3.3.g1" src="extracted/5879622/ooi/seg_query_idx_00192.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.4.4.4" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.4.4.4.g1" src="extracted/5879622/ooi/seg_query_idx_00192_match_idx_01244.png" width="150"/></td>
</tr>
<tr class="ltx_tr" id="S11.F8.8.8">
<td class="ltx_td ltx_align_center" id="S11.F8.5.5.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="189" id="S11.F8.5.5.1.g1" src="x4.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.6.6.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="100" id="S11.F8.6.6.2.g1" src="extracted/5879622/ooi/miss_sixty_photo_img_cropped_blurred.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.7.7.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.7.7.3.g1" src="extracted/5879622/ooi/seg_query_idx_00018.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.8.8.4" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.8.8.4.g1" src="extracted/5879622/ooi/seg_query_idx_00018_match_idx_02222.png" width="150"/></td>
</tr>
<tr class="ltx_tr" id="S11.F8.12.12">
<td class="ltx_td ltx_align_center" id="S11.F8.9.9.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="192" id="S11.F8.9.9.1.g1" src="x5.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.10.10.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="100" id="S11.F8.10.10.2.g1" src="extracted/5879622/ooi/CK_jew_photo_img_cropped_blurred.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.11.11.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.11.11.3.g1" src="extracted/5879622/ooi/seg_query_idx_00011.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.12.12.4" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.12.12.4.g1" src="extracted/5879622/ooi/seg_query_idx_00011_match_idx_01309.png" width="150"/></td>
</tr>
<tr class="ltx_tr" id="S11.F8.16.16">
<td class="ltx_td ltx_align_center" id="S11.F8.13.13.1" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="179" id="S11.F8.13.13.1.g1" src="x6.png" width="299"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.14.14.2" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="100" id="S11.F8.14.14.2.g1" src="extracted/5879622/ooi/Hazzys_img_cropped.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.15.15.3" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.15.15.3.g1" src="extracted/5879622/ooi/seg_query_idx_00090.png" width="150"/></td>
<td class="ltx_td ltx_align_center" id="S11.F8.16.16.4" style="padding-left:12.0pt;padding-right:12.0pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S11.F8.16.16.4.g1" src="extracted/5879622/ooi/seg_query_idx_00090_match_idx_01306.png" width="150"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S11.F8.18.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S11.F8.19.2" style="font-size:90%;">Qualitative results for OOI retrieval. Column 1 and 2 show OOI (marked in red bounding box). Column 3 shows the OOI as a segment along with its neighbors. Column 4 shows the matched segment.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h3 class="ltx_title ltx_title_section">Acknowledgements</h3>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the Centre for Augmented Reasoning (CAR) at the Australian Institute for Machine Learning (AIML), University of Adelaide, Australia. The authors acknowledge the computational support provided by the Indian Institute of Science (IISc), Bengaluru, India, and the
International Institute of Information Technology, Hyderabad (IIITH), India. The authors thank Ahmad Khaliq for technical support, Sarah Ibrahimi for sharing the InsideOut dataset, and Martin Humenberger for sharing the OOI annotations for the Baidu Mall dataset.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ali-bey, A., Chaib-draa, B., Giguère, P.: Gsv-cities: Toward appropriate supervised visual place recognition. Neurocomputing <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">513</span>, 194–203 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ali-bey, A., Chaib-draa, B., Giguère, P.: Mixvpr: Feature mixing for visual place recognition. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 2998–3007 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: Netvlad: Cnn architecture for weakly supervised place recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5297–5307 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Arandjelovic, R., Zisserman, A.: All about vlad. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 1578–1585 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bar, M.: Visual objects in context. Nature Reviews Neuroscience <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">5</span>(8), 617–629 (2004)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Berton, G., Masone, C., Caputo, B.: Rethinking visual geo-localization for large-scale applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4878–4888 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Berton, G., Mereu, R., Trivigno, G., Masone, C., Csurka, G., Sattler, T., Caputo, B.: Deep visual geo-localization benchmark. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5396–5407 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Berton, G., Trivigno, G., Caputo, B., Masone, C.: Eigenplaces: Training viewpoint robust models for visual place recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 11080–11090 (October 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">33</span>, 1877–1901 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1209–1218 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Cao, B., Araujo, A., Sim, J.: Unifying deep local and global features for image search. In: European Conference on Computer Vision. pp. 726–743. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chang, M., Gervet, T., Khanna, M., Yenamandra, S., Shah, D., Min, S.Y., Shah, K., Paxton, C., Gupta, S., Batra, D., et al.: Goat: Go to any thing. arXiv preprint arXiv:2311.06430 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chen, L.C., Yang, Y., Wang, J., Xu, W., Yuille, A.L.: Attention to scale: Scale-aware semantic image segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3640–3649 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Chen, Z., Maffra, F., Sa, I., Chli, M.: Only look once, mining distinctive landmarks from convnet for visual place recognition. In: Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. pp. 9–16. IEEE (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Cheng, C., Page, D.L., Abidi, M.A.: Object-based place recognition and loop closing with jigsaw puzzle image segmentation algorithm. In: 2008 IEEE International Conference on Robotics and Automation. pp. 557–562. IEEE (2008)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Cummins, M., Newman, P.: Appearance-only slam at large scale with fab-map 2.0. The International Journal of Robotics Research <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">30</span>(9), 1100–1123 (2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Cupec, R., Nyarko, E.K., Filko, D., Kitanov, A., Petrović, I.: Place recognition based on matching of planar surfaces and line segments. The International Journal of Robotics Research <span class="ltx_text ltx_font_bold" id="bib.bib17.1.1">34</span>(4-5), 674–704 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Folorunsho, S.O.: Semantic segmentation-based approach for autonomous navigation in challenging farm terrains. algorithms <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">15</span>,  3 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Garg, S., Fischer, T., Milford, M.: Where is your place, visual place recognition? In: IJCAI (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Garg, S., Rana, K., Hosseinzadeh, M., Mares, L., Suenderhauf, N., Dayoub, F., Reid, I.: Robohop: Segment-based topological map representation for open-world visual navigation. In: 2024 IEEE International Conference on Robotics and Automation (ICRA) (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Garg, S., Suenderhauf, N., Milford, M.: Lost? appearance-invariant place recognition for opposite viewpoints using visual semantics. In: Proceedings of Robotics: Science and Systems XIV (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Garg, S., Sünderhauf, N., Dayoub, F., Morrison, D., Cosgun, A., Carneiro, G., Wu, Q., Chin, T.J., Reid, I., Gould, S., Corke, P., Milford, M.: Semantics for robotic mapping, perception and interaction: A survey. Foundations and Trends® in Robotics <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">8</span>(1–2), 1–224 (2020). https://doi.org/10.1561/2300000059, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1561/2300000059" title="">http://dx.doi.org/10.1561/2300000059</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Gawel, A., Del Don, C., Siegwart, R., Nieto, J., Cadena, C.: X-view: Graph-based semantic multi-view localization. IEEE Robotics and Automation Letters <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">3</span>(3), 1687–1694 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Gu, Q., Kuwajerwala, A., Morin, S., Jatavallabhula, K.M., Sen, B., Agarwal, A., Rivera, C., Paul, W., Ellis, K., Chellappa, R., et al.: Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. arXiv preprint arXiv:2309.16650 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Hausler, S., Garg, S., Xu, M., Milford, M., Fischer, T.: Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14141–14152 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hu, H., Qiao, Z., Cheng, M., Liu, Z., Wang, H.: Dasgil: Domain adaptation for semantic and geometric-aware image-based localization. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">30</span>, 1342–1353 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Ibrahimi, S., Van Noord, N., Alpherts, T., Worring, M.: Inside out visual place recognition. arXiv preprint arXiv:2111.13546 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Intraub, H.: The representation of visual scenes. Trends in cognitive sciences <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">1</span>(6), 217–222 (1997)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Izquierdo, S., Civera, J.: Optimal transport aggregation for visual place recognition (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jégou, H., Douze, M., Schmid, C., Pérez, P.: Aggregating local descriptors into a compact image representation. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3304–3311 (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kassab, C., Mattamala, M., Fallon, M.: Clip-based features achieve competitive zero-shot visual localization. OpenReview preprint arXiv:2306.14846 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Keetha, N., Mishra, A., Karhade, J., Jatavallabhula, K.M., Scherer, S., Krishna, M., Garg, S.: Anyloc: Towards universal visual place recognition. IEEE Robotics and Automation Letters (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Keetha, N.V., Milford, M., Garg, S.: A hierarchical dual model of environment-and place-specific utility for visual place recognition. IEEE Robotics and Automation Letters <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">6</span>(4), 6969–6976 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Khaliq, A., Ehsan, S., Milford, M., McDonald-Maier, K.: A holistic visual place recognition approach using lightweight cnns for severe viewpoint and appearance changes. arXiv preprint arXiv:1811.03032 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Khaliq, A., Xu, M., Hausler, S., Milford, M., Garg, S.: Vlad-buff: Burst-aware fast feature aggregation for visual place recognition. In: European Conference on Computer Vision. Springer (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Khorasgani, S.H., Chen, Y., Shkurti, F.: Slic: Self-supervised learning with iterative clustering for human action videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16091–16101 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Knopp, J., Sivic, J., Pajdla, T.: Avoiding confusing features in place recognition. In: European Conference on Computer Vision. pp. 748–761. Springer (2010)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Krantz, J., Lee, S., Malik, J., Batra, D., Chaplot, D.S.: Instance-specific image goal navigation: Training embodied agents to find object instances. arXiv preprint arXiv:2211.15876 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Le, D.C., Youn, C.H.: City-scale visual place recognition with deep local features based on multi-scale ordered VLAD pooling. arXiv preprint arXiv:2009.09255 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Lowe, D.G.: Distinctive image features from scale-invariant keypoints. International journal of computer vision <span class="ltx_text ltx_font_bold" id="bib.bib41.1.1">60</span>(2), 91–110 (2004)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Maalouf, A., Jadhav, N., Jatavallabhula, K.M., Chahine, M., Vogt, D.M., Wood, R.J., Torralba, A., Rus, D.: Follow anything: Open-set detection, tracking, and following in real-time. IEEE Robotics and Automation Letters <span class="ltx_text ltx_font_bold" id="bib.bib42.1.1">9</span>(4), 3283–3290 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Manglani, S.: Real-time vision-based navigation for a robot in an indoor environment. arXiv preprint arXiv:2307.00666 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Masone, C., Caputo, B.: A survey on deep visual place recognition. IEEE Access <span class="ltx_text ltx_font_bold" id="bib.bib44.1.1">9</span>, 19516–19547 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Milford, M.J., Wyeth, G.F.: Seqslam: Visual route-based navigation for sunny summer days and stormy winter nights. In: Robotics and Automation (ICRA), 2012 IEEE International Conference on. pp. 1643–1649. IEEE (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Mirjalili, R., Krawez, M., Burgard, W.: Fm-loc: Using foundation models for improved vision-based localization. In: Robotics and Automation (ICRA), 2023 IEEE International Conference on. IEEE (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Mousavian, A., Kosecka, J.: Semantic image based geolocation given a map (author’s initial manuscript). Tech. rep., George Mason University Fairfax United States (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Naseer, T., Oliveira, G.L., Brox, T., Burgard, W.: Semantics-aware visual localization under challenging perceptual conditions. In: IEEE International Conference on Robotics and Automation (ICRA) (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Noh, H., Araujo, A., Sim, J., Weyand, T., Han, B.: Large-scale image retrieval with attentive deep local features. In: Int. Conf. Comput. Vis. pp. 3456–3465 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Oliva, A.: Gist of the scene. Neurobiology of attention <span class="ltx_text ltx_font_bold" id="bib.bib50.1.1">696</span>(64), 251–258 (2005)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.Y., Xu, H., Sharma, V., Li, S.W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without supervision (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Paolicelli, V., Tavera, A., Masone, C., Berton, G., Caputo, B.: Learning semantics for visual place recognition through multi-scale attention. In: Image Analysis and Processing–ICIAP 2022: 21st International Conference, Lecce, Italy, May 23–27, 2022, Proceedings, Part II. pp. 454–466. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Pion, N., Humenberger, M., Csurka, G., Cabon, Y., Sattler, T.: Benchmarking image retrieval for visual localization. In: 2020 International Conference on 3D Vision (3DV). pp. 483–494. IEEE (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Puligilla, S.S., Tourani, S., Vaidya, T., Parihar, U.S., Sarvadevabhatla, R.K., Krishna, K.M.: Topological mapping for manhattan-like repetitive environments. In: 2020 IEEE International Conference on Robotics and Automation (ICRA). pp. 6268–6274. IEEE (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Radenović, F., Iscen, A., Tolias, G., Avrithis, Y., Chum, O.: Revisiting oxford and paris: Large-scale image retrieval benchmarking. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5706–5715 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Radenović, F., Tolias, G., Chum, O.: Fine-tuning cnn image retrieval with no human annotation. IEEE Trans. Pattern Anal. Mach. Intell. <span class="ltx_text ltx_font_bold" id="bib.bib56.1.1">41</span>(7), 1655–1668 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Ranzinger, M., Heinrich, G., Kautz, J., Molchanov, P.: Am-radio: Agglomerative vision foundation model reduce all domains into one. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12490–12500 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Revaud, J., Almazán, J., Rezende, R.S., Souza, C.R.d.: Learning with average precision: Training image retrieval with a listwise loss. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 5107–5116 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Sarlin, P.E., Cadena, C., Siegwart, R., Dymczyk, M.: From coarse to fine: Robust hierarchical localization at large scale. arXiv preprint arXiv:1812.03506 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Sattler, T., Maddern, W., Toft, C., Torii, A., Hammarstrand, L., Stenborg, E., Safari, D., Okutomi, M., M, P., J, S., F, K., T, P.: Benchmarking 6dof outdoor visual localization in changing conditions. In: Proc. CVPR (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Schleiss, M., Rouatbi, F., Cremers, D.: Vpair – aerial visual place recognition and localization in large-scale outdoor environments (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Schubert, S., Neubert, P., Garg, S., Milford, M., Fischer, T.: Visual place recognition: A tutorial. RAM (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Shlapentokh-Rothman, M., Blume, A., Xiao, Y., Wu, Y., TV, S., Tao, H., Lee, J.Y., Torres, W., Wang, Y.X., Hoiem, D.: Region-based representations revisited. arXiv preprint arXiv:2402.02352 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Shubodh, S., Omama, M., Zaidi, H., Parihar, U.S., Krishna, M.: Lip-loc: Lidar image pretraining for cross-modal localization. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 948–957 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Sivic, J., Zisserman, A.: Video google: A text retrieval approach to object matching in videos. In: Proceedings of International Conference on Computer Vision (ICCV). p. 1470. IEEE (2003)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Sun, X., Xie, Y., Luo, P., Wang, L.: A dataset for benchmarking image-based localization. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5641–5649 (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:20531893" title="">https://api.semanticscholar.org/CorpusID:20531893</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Sünderhauf, N., Shirazi, S., Jacobson, A., Dayoub, F., Pepperell, E., Upcroft, B., Milford, M.: Place recognition with convnet landmarks: Viewpoint-robust, condition-robust, training-free. Proceedings of Robotics: Science and Systems XII (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Torii, A., Sivic, J., Pajdla, T., Okutomi, M.: Visual place recognition with repetitive structures. In: 2013 IEEE Conference on Computer Vision and Pattern Recognition. pp. 883–890 (2013). https://doi.org/10.1109/CVPR.2013.119

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Tsintotas, K.A., Bampis, L., Gasteratos, A.: The revisiting problem in simultaneous localization and mapping: A survey on visual loop closure detection. IEEE Transactions on Intelligent Transportation Systems <span class="ltx_text ltx_font_bold" id="bib.bib70.1.1">23</span>(11), 19929–19953 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Wang, R., Shen, Y., Zuo, W., Zhou, S., Zheng, N.: Transvpr: Transformer-based place recognition with multi-level attention aggregation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13648–13657 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Warburg, F., Hauberg, S., López-Antequera, M., Gargallo, P., Kuang, Y., Civera, J.: Mapillary street-level sequences: A dataset for lifelong place recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2626–2635 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Weinzaepfel, P., Csurka, G., Cabon, Y., Humenberger, M.: Visual localization by learning objects-of-interest dense match regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5634–5643 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Xin, Z., Cai, Y., Lu, T., Xing, X., Cai, S., Zhang, J., Yang, Y., Wang, Y.: Localizing discriminative visual landmarks for place recognition. In: 2019 International conference on robotics and automation (ICRA). pp. 5979–5985. IEEE (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Yildiz, B., Khademi, S., Siebes, R.M., Van Gemert, J.: Amstertime: A visual place recognition benchmark dataset for severe domain shift. In: 2022 26th International Conference on Pattern Recognition (ICPR). IEEE (Aug 2022). https://doi.org/10.1109/icpr56361.2022.9956049, <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/ICPR56361.2022.9956049" title="">http://dx.doi.org/10.1109/ICPR56361.2022.9956049</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Yu, J., Zhu, C., Zhang, J., Huang, Q., Tao, D.: Spatial pyramid-enhanced netvlad with weighted triplet loss for place recognition. IEEE Trans. Neural Netw. Learn. Syst. <span class="ltx_text ltx_font_bold" id="bib.bib76.1.1">31</span>(2), 661–674 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Zaffar, M., Garg, S., Milford, M., Kooij, J., Flynn, D., McDonald-Maier, K., Ehsan, S.: Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change. International Journal of Computer Vision pp. 1–39 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Zhang, Y., Zhao, X.: Mesa: Matching everything by segmenting anything. arXiv preprint arXiv:2401.16741 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Zhao, X., Ding, W., An, Y., Du, Y., Yu, T., Li, M., Tang, M., Wang, J.: Fast segment anything. arXiv preprint arXiv:2306.12156 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee, Y.J.: Segment everything everywhere all at once. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib81.1.1">36</span> (2024)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 16:40:15 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
