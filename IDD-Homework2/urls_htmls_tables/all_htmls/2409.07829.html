<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat</title>
<!--Generated on Thu Sep 12 08:23:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="UI automation test,  large language model,  retrieval-augmented generation,  cost optimization" lang="en" name="keywords"/>
<base href="/html/2409.07829v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S1" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS1" title="In 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Task Description Decomposition</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS1.SSS1" title="In 2.1. Task Description Decomposition ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Retrieving Few-shot Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS1.SSS2" title="In 2.1. Task Description Decomposition ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Generating Decomposed Actions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS2" title="In 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>UI Automation Execution</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS2.SSS1" title="In 2.2. UI Automation Execution ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>UI Element Mapping</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS2.SSS2" title="In 2.2. UI Automation Execution ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>LLMs Optimizer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS3" title="In 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.SS1" title="In 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>RQ1: Performance of UI Automation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.SS2" title="In 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>RQ2: Comparison with State-of-the-Art</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.SS3" title="In 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>RQ3: Usefulness of <span class="ltx_text ltx_font_smallcaps">CAT</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S4" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implication and Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S5" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S6" title="In Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sidong Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Monash University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Melbourne</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sidong.feng@monash.edu">sidong.feng@monash.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haochuan Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hudsonhclu@tencent.com">hudsonhclu@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianqin Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:janetjiang@tencent.com">janetjiang@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ting Xiong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:candyxiong@tencent.com">candyxiong@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Likun Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:likunhuang@tencent.com">likunhuang@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yinglin Liang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dickylliang@tencent.com">dickylliang@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoqin Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:allysali@tencent.com">allysali@tencent.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuetang Deng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">Tencent Inc.</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Guangzhou</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yuetangdeng@tencent.com">yuetangdeng@tencent.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aldeida Aleti
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">Monash University</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Melbourne</span><span class="ltx_text ltx_affiliation_country" id="id27.3.id3">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:aldeida.aleti@monash.edu%20">aldeida.aleti@monash.edu </a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id28.id1">UI automation tests play a crucial role in ensuring the quality of mobile applications. Despite the growing popularity of machine learning techniques to generate these tests, they still face several challenges, such as the mismatch of UI elements. The recent advances in Large Language Models (LLMs) have addressed these issues by leveraging their semantic understanding capabilities. However, a significant gap remains in applying these models to industrial-level app testing, particularly in terms of cost optimization and knowledge limitation. To address this, we introduce <span class="ltx_text ltx_font_smallcaps" id="id28.id1.1">CAT</span> to create cost-effective UI automation tests for industry apps by combining machine learning and LLMs with best practices. Given the task description, <span class="ltx_text ltx_font_smallcaps" id="id28.id1.2">CAT</span> employs Retrieval Augmented Generation (RAG) to source examples of industrial app usage as the few-shot learning context, assisting LLMs in generating the specific sequence of actions. <span class="ltx_text ltx_font_smallcaps" id="id28.id1.3">CAT</span> then employs machine learning techniques, with LLMs serving as a complementary optimizer, to map the target element on the UI screen. Our evaluations on the WeChat testing dataset demonstrate the <span class="ltx_text ltx_font_smallcaps" id="id28.id1.4">CAT</span>’s performance and cost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming the state-of-the-art. We have also integrated our approach into the real-world WeChat testing platform, demonstrating its usefulness in detecting 141 bugs and enhancing the developers’ testing process.</p>
</div>
<div class="ltx_keywords">UI automation test, large language model, retrieval-augmented generation, cost optimization
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>39th IEEE/ACM International Conference on Automated Software Engineering ; October 27-November 1, 2024; Sacramento, CA, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>39th IEEE/ACM International Conference on Automated Software Engineering (ASE ’24), October 27-November 1, 2024, Sacramento, CA, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3691620.3695260</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-1248-7/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Software and its engineering Software testing and debugging</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Mobile apps have become increasingly popular over the past decade, with millions of apps available for download from app stores like the Apple App Store and Google Play Store.
With the rise of app importance in our daily life, it has become increasingly critical for app developers to ensure that their apps are of high quality and perform as expected for users.
One common practice for quality assurance is writing UI automation tests, enabling developers to use a pre-defined criterion as the test’s oracle to discover bugs or maximize code coverage.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Hardcoding UI automation tests, such as record and replay <cite class="ltx_cite ltx_citemacro_citep">(Zadgaonkar, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib38" title="">2013</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib18" title="">2023b</a>)</cite>, can quickly become obsolete due to the fast-paced evolution of platforms and the industry’s demand for frequent releases.
As a result, industry developers often favor writing UI automation tests with high-level task objectives, such as “sharing a picture with Pony” for regression or performance testing <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib11" title="">2018</a>)</cite>.
The research community has made substantial contributions toward automating these testing activities.
For instance, AppFlow <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib27" title="">2018</a>)</cite> introduces a system that leverages test libraries of analogous apps and uses machine learning methods to synthesize UI automation tests.
However, existing machine learning techniques in this field often face practical adoption challenges <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib19" title="">2024</a>)</cite>, particularly the issues of mismatched UI elements, which undermine the effectiveness of UI automation tests.
Large Language Models (LLMs) pave the way for numerous software development tasks, including UI automation. A recent success in this field is AdbGPT <cite class="ltx_cite ltx_citemacro_citep">(Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a>)</cite>, which uses prompt engineering to extract semantic understanding and logical reasoning from LLMs, significantly improving the performance of UI automation tests.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Yet, applying LLMs for UI automation tests in the industry involves two main challenges, in terms of cost optimization and knowledge integration.
First, while numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib19" title="">2024</a>)</cite> focus on utilizing LLMs for UI automation tests, the associated costs can be prohibitive for industry-level testing.
Even with the deployment of a private LLM service, the computational cost remains high.
For instance, inferring a single token with LLaMA-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib34" title="">2023</a>)</cite> requires 6.7 billion FLOPs, and the entire UI automation process may use over millions of tokens.
Second, LLMs often lack specific knowledge and experience regarding industrial apps, which can lead to ineffective UI automation.
This is primarily due to the potential license copyright violations from the LLMs <cite class="ltx_cite ltx_citemacro_citep">(web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib4" title="">2024c</a>)</cite>, which prevent the inclusion of industry resources in the training.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we propose <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">CAT</span>, designed to facilitate <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">C</span>ost-effective UI <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">A</span>utomation <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">T</span>ests for industrial apps.
This is achieved by integrating well-established machine learning methods with cutting-edge advancements in LLMs.
<span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.5">CAT</span> operates in two phases: i) <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">Task Description Decomposition</span> and ii) <span class="ltx_text ltx_font_italic" id="S1.p4.1.7">UI Automation Execution</span>.
When presented with a task description, the first phase is to break down it into a specific sequence of executable actions.
Given the limited knowledge of industrial apps, we aim to provide one example to aid LLMs in comprehending the app usage and abstracting task description decomposition.
As a result, we leverage the Retrieval Augmented Generation (RAG) method, which performs neural searches across previous app testing datasets to select the most analogous examples as the few-shot learning context, encouraging the LLMs to formulate possible actions.
Once the actions are determined, the second phase is to automatically execute them on the device by associating them with the UI element mapping.
To accomplish this, we propose a machine learning method, with the LLMs serving as a complementary optimizer, to map the UI element on the screen.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To evaluate the performance of the <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">CAT</span>, we carry out a large-scale experiment involving 39k tasks in the WeChat dataset.
First, we conduct an ablation study to assess the performance of the <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">CAT</span> in UI automation, using five variations of the approach.
The results show that <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">CAT</span> significantly increases the completion rate at a reduced cost, successfully completing 90% of the tasks at an average cost of $0.34.
Second, we evaluate the performance of the <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">CAT</span> against two state-of-the-art UI automation methods.
Our findings reveal that our approach can save more cost and time without sacrificing the completion rate, saving $1,467 cost even when compared to the best baseline.
Beyond the performance of the approach, we also evaluate its practical usefulness.
We integrate the <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.5">CAT</span> with the WeChat testing platform, triggering it whenever new functionalities need testing or a new version is released, accompanied by new task descriptions.
During the testing period from December 2023 to June 2024, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.6">CAT</span> automatically executes 6k of UI automation tests, detecting 141 bugs.
This alleviates the developers’ burden in bug detection, saving substantial time for subsequent bug fixing.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The contributions of this paper are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">CAT</span>, a novel approach that combines machine learning and large language models with the adoption of best practices to generate UI automation tests. To the best of our knowledge, this is the first work that contemplates cost optimization and knowledge integration of LLMs in industrial-level app testing.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We conduct extensive experiments, including ablation studies and comparisons with state-of-the-art methods, to showcase the performance and cost-effectiveness of <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">CAT</span>. Additionally, we incorporate our approach into a real-world testing platform to illustrate its practical usefulness.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Approach</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Given a high-level task description, we propose an automated approach to break down the actions and execute them on the dynamic UI to trigger the app activity for testing.
The overview of our approach <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">CAT</span> is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.F1" title="Figure 1 ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">1</span></a>, which is divided into two main phases: (i) the <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">Task Description Decomposition</span> phase, which decomposes task description into multiple potential action steps, including action types, target elements, and input values;
(ii) the <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">UI Automation Execution</span> phase which aligns the actions with the dynamic UI elements to accomplish UI automation.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="397" id="S2.F1.g1" src="extracted/5850018/images/overview.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overview of our approach.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Task Description Decomposition</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The first phase of our approach is to understand, analyze, organize, and generate the actions from the high-level task description by using the advance of LLMs.
Given the potential knowledge gap in LLMs regarding the specifics of industrial app usage, we adopt the widely recognized Retrieval Augmented Generation (RAG) technique.
It first employs a machine learning mechanism to retrieve a few examples detailing the inputs and outputs associated with industrial app usage.
Then, these examples serve as a context for few-shot learning, assisting the LLMs in generating potential actions for the task description.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Retrieving Few-shot Examples</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">A representative example helps the model in eliciting specific knowledge and abstractions.
A randomly chosen example might not capture the intricacies of industrial app usage, thereby limiting the LLMs’ ability to fully comprehend the task.
To address this, we utilize examples from previous app testing datasets, a common industry practice for testing app functionalities across iterative versions.
The dataset usually comprises testing objectives with high-level task descriptions and their specific sequence of steps.
Consequently, we apply a neural search and a similarity-based retrieval technique to select representative examples from the dataset.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">First, we utilize the transformer encoder model T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib31" title="">2020</a>)</cite> to encode the descriptions into a vector space.
This allows for the handling of variable-sized inputs: when an input sequence is provided, it is mapped to a sequence of embeddings that are then passed into the encoder.
All encoders share an identical structure, each consisting of two subcomponents: a self-attention layer followed by a compact feed-forward network.
Layer normalization is applied to each subcomponent’s input, while a residual skip connection adds each subcomponent’s input to its output.
Dropout is implemented within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.3">Based on the vectors, we employ the sparse retrieval method known as cosine similarity <cite class="ltx_cite ltx_citemacro_citep">(Singhal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib32" title="">2001</a>)</cite>, to identify representative examples that align closely with the testing task description, as indicated by the highest relevance score.
In detail, it is measured as:
<math alttext="cos(v_{1},v_{2})=\frac{v_{1}v_{2}}{\left|v_{1}\right|\left|v_{2}\right|}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p3.1.m1.4"><semantics id="S2.SS1.SSS1.p3.1.m1.4a"><mrow id="S2.SS1.SSS1.p3.1.m1.4.4" xref="S2.SS1.SSS1.p3.1.m1.4.4.cmml"><mrow id="S2.SS1.SSS1.p3.1.m1.4.4.2" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.4.4.2.4" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.4.cmml">c</mi><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.3" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.3.cmml">⁢</mo><mi id="S2.SS1.SSS1.p3.1.m1.4.4.2.5" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.5.cmml">o</mi><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.3a" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.3.cmml">⁢</mo><mi id="S2.SS1.SSS1.p3.1.m1.4.4.2.6" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.6.cmml">s</mi><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.3b" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.3.cmml">⁢</mo><mrow id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.3.cmml"><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.3" stretchy="false" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.3.cmml">(</mo><msub id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.2" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.3" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.4" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.3.cmml">,</mo><msub id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.2" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.3" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.5" stretchy="false" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS1.p3.1.m1.4.4.3" xref="S2.SS1.SSS1.p3.1.m1.4.4.3.cmml">=</mo><mfrac id="S2.SS1.SSS1.p3.1.m1.2.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.cmml"><mrow id="S2.SS1.SSS1.p3.1.m1.2.2.4" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.cmml"><msub id="S2.SS1.SSS1.p3.1.m1.2.2.4.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2.3.cmml">1</mn></msub><mo id="S2.SS1.SSS1.p3.1.m1.2.2.4.1" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.1.cmml">⁢</mo><msub id="S2.SS1.SSS1.p3.1.m1.2.2.4.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3.3.cmml">2</mn></msub></mrow><mrow id="S2.SS1.SSS1.p3.1.m1.2.2.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.cmml"><mrow id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.2.cmml"><mo id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.2" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.2" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.3" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.3" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.SS1.SSS1.p3.1.m1.2.2.2.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.2.cmml"><mo id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.cmml"><mi id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.2" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.3.cmml">2</mn></msub><mo id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.3" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p3.1.m1.4b"><apply id="S2.SS1.SSS1.p3.1.m1.4.4.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4"><eq id="S2.SS1.SSS1.p3.1.m1.4.4.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.3"></eq><apply id="S2.SS1.SSS1.p3.1.m1.4.4.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2"><times id="S2.SS1.SSS1.p3.1.m1.4.4.2.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.3"></times><ci id="S2.SS1.SSS1.p3.1.m1.4.4.2.4.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.4">𝑐</ci><ci id="S2.SS1.SSS1.p3.1.m1.4.4.2.5.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.5">𝑜</ci><ci id="S2.SS1.SSS1.p3.1.m1.4.4.2.6.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.6">𝑠</ci><interval closure="open" id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2"><apply id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.3.3.1.1.1.1.3">1</cn></apply><apply id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.4.4.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S2.SS1.SSS1.p3.1.m1.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2"><divide id="S2.SS1.SSS1.p3.1.m1.2.2.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2"></divide><apply id="S2.SS1.SSS1.p3.1.m1.2.2.4.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4"><times id="S2.SS1.SSS1.p3.1.m1.2.2.4.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.1"></times><apply id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.2.2.4.2.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.2.3">1</cn></apply><apply id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.2.2.4.3.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.2.2.4.3.3">2</cn></apply></apply><apply id="S2.SS1.SSS1.p3.1.m1.2.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2"><times id="S2.SS1.SSS1.p3.1.m1.2.2.2.3.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.3"></times><apply id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1"><abs id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.2"></abs><apply id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1"><abs id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.2.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.2"></abs><apply id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.1.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.2.cmml" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.1.m1.2.2.2.2.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p3.1.m1.4c">cos(v_{1},v_{2})=\frac{v_{1}v_{2}}{\left|v_{1}\right|\left|v_{2}\right|}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p3.1.m1.4d">italic_c italic_o italic_s ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = divide start_ARG italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG | italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | | italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | end_ARG</annotation></semantics></math>, where <math alttext="v_{1}v_{2}" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p3.2.m2.1"><semantics id="S2.SS1.SSS1.p3.2.m2.1a"><mrow id="S2.SS1.SSS1.p3.2.m2.1.1" xref="S2.SS1.SSS1.p3.2.m2.1.1.cmml"><msub id="S2.SS1.SSS1.p3.2.m2.1.1.2" xref="S2.SS1.SSS1.p3.2.m2.1.1.2.cmml"><mi id="S2.SS1.SSS1.p3.2.m2.1.1.2.2" xref="S2.SS1.SSS1.p3.2.m2.1.1.2.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.2.m2.1.1.2.3" xref="S2.SS1.SSS1.p3.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="S2.SS1.SSS1.p3.2.m2.1.1.1" xref="S2.SS1.SSS1.p3.2.m2.1.1.1.cmml">⁢</mo><msub id="S2.SS1.SSS1.p3.2.m2.1.1.3" xref="S2.SS1.SSS1.p3.2.m2.1.1.3.cmml"><mi id="S2.SS1.SSS1.p3.2.m2.1.1.3.2" xref="S2.SS1.SSS1.p3.2.m2.1.1.3.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.2.m2.1.1.3.3" xref="S2.SS1.SSS1.p3.2.m2.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p3.2.m2.1b"><apply id="S2.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1"><times id="S2.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.1"></times><apply id="S2.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.2.m2.1.1.2.1.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p3.2.m2.1.1.2.2.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.2.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.2.m2.1.1.2.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.2.m2.1.1.2.3">1</cn></apply><apply id="S2.SS1.SSS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.2.m2.1.1.3.1.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS1.p3.2.m2.1.1.3.2.cmml" xref="S2.SS1.SSS1.p3.2.m2.1.1.3.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.2.m2.1.1.3.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p3.2.m2.1c">v_{1}v_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p3.2.m2.1d">italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is the inner product of two vectors and <math alttext="\left|v_{1}\right|\left|v_{2}\right|" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p3.3.m3.2"><semantics id="S2.SS1.SSS1.p3.3.m3.2a"><mrow id="S2.SS1.SSS1.p3.3.m3.2.2" xref="S2.SS1.SSS1.p3.3.m3.2.2.cmml"><mrow id="S2.SS1.SSS1.p3.3.m3.1.1.1.1" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.2.cmml"><mo id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.2" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.2.1.cmml">|</mo><msub id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.2" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.3" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.3" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.SS1.SSS1.p3.3.m3.2.2.3" xref="S2.SS1.SSS1.p3.3.m3.2.2.3.cmml">⁢</mo><mrow id="S2.SS1.SSS1.p3.3.m3.2.2.2.1" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.2.cmml"><mo id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.2" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.2.1.cmml">|</mo><msub id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.cmml"><mi id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.2" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.2.cmml">v</mi><mn id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.3" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.3.cmml">2</mn></msub><mo id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.3" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p3.3.m3.2b"><apply id="S2.SS1.SSS1.p3.3.m3.2.2.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2"><times id="S2.SS1.SSS1.p3.3.m3.2.2.3.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.3"></times><apply id="S2.SS1.SSS1.p3.3.m3.1.1.1.2.cmml" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1"><abs id="S2.SS1.SSS1.p3.3.m3.1.1.1.2.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.2"></abs><apply id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.3.m3.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS1.SSS1.p3.3.m3.2.2.2.2.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1"><abs id="S2.SS1.SSS1.p3.3.m3.2.2.2.2.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.2"></abs><apply id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.1.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.2.cmml" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.2">𝑣</ci><cn id="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.SSS1.p3.3.m3.2.2.2.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p3.3.m3.2c">\left|v_{1}\right|\left|v_{2}\right|</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p3.3.m3.2d">| italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | | italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT |</annotation></semantics></math> is the product of 2-norm for these vectors.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Prompt example for task description decomposition. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.3">
<tr class="ltx_tr" id="S2.T1.3.4" style="background-color:#C4C4C4;">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.3.4.1"><span class="ltx_text ltx_font_bold" id="S2.T1.3.4.1.1" style="font-size:80%;background-color:#C4C4C4;">COMPONENT</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.3.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.4.2.1" style="background-color:#C4C4C4;">
<span class="ltx_p" id="S2.T1.3.4.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.3.4.2.1.1.1" style="font-size:80%;">DETAILS</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.2"><span class="ltx_text" id="S2.T1.1.1.2.1" style="font-size:80%;">⟨Instructions⟩</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.1"><span class="ltx_inline-logical-block ltx_align_top" id="S2.T1.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S2.T1.1.1.1.1.p1">
<span class="ltx_p" id="S2.T1.1.1.1.1.p1.1"><span class="ltx_text" id="S2.T1.1.1.1.1.p1.1.1"></span><span class="ltx_text ltx_font_italic" id="S2.T1.1.1.1.1.p1.1.2" style="font-size:80%;">
<span class="ltx_tabular ltx_align_top" id="S2.T1.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S2.T1.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.1.1.1.1.p1.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.p1.1.2.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.p1.1.2.1.1.1.1.1">I want you to act as a professional developer. I expect you to analyze the task description I provide and respond with potential actions that could interact with the device. Please enumerate these actions and encapsulate the operations within brackets [like this].</span>
</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.1.1.p1.1.3"></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.2.2.2"><span class="ltx_text" id="S2.T1.2.2.2.1" style="font-size:80%;">⟨One-shot Example⟩</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.2.2.1"><span class="ltx_inline-logical-block ltx_align_top" id="S2.T1.2.2.1.1">
<span class="ltx_para ltx_noindent" id="S2.T1.2.2.1.1.p1">
<span class="ltx_p" id="S2.T1.2.2.1.1.p1.1"><span class="ltx_text" id="S2.T1.2.2.1.1.p1.1.1"></span><span class="ltx_text ltx_font_italic" id="S2.T1.2.2.1.1.p1.1.2" style="font-size:80%;">
<span class="ltx_tabular ltx_align_top" id="S2.T1.2.2.1.1.p1.1.2.1">
<span class="ltx_tr" id="S2.T1.2.2.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.2.2.1.1.p1.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.1.1.p1.1.2.1.1.1.1">
<span class="ltx_p" id="S2.T1.2.2.1.1.p1.1.2.1.1.1.1.1">Here is an example:</span>
</span></span></span>
<span class="ltx_tr" id="S2.T1.2.2.1.1.p1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.2.2.1.1.p1.1.2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.1.1.p1.1.2.1.2.1.1">
<span class="ltx_p" id="S2.T1.2.2.1.1.p1.1.2.1.2.1.1.1">Task description -¿ “open settings”</span>
</span></span></span>
<span class="ltx_tr" id="S2.T1.2.2.1.1.p1.1.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.2.2.1.1.p1.1.2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.2.2.1.1.p1.1.2.1.3.1.1">
<span class="ltx_p" id="S2.T1.2.2.1.1.p1.1.2.1.3.1.1.1">Actions -¿ 1. [tap] [me] 2. [tap] [settings] …</span>
</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.1.1.p1.1.3"></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.3.3.2"><span class="ltx_text" id="S2.T1.3.3.2.1" style="font-size:80%;">⟨Testing Task Description⟩</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="S2.T1.3.3.1"><span class="ltx_inline-logical-block ltx_align_top" id="S2.T1.3.3.1.1">
<span class="ltx_para ltx_noindent" id="S2.T1.3.3.1.1.p1">
<span class="ltx_p" id="S2.T1.3.3.1.1.p1.1"><span class="ltx_text" id="S2.T1.3.3.1.1.p1.1.1"></span><span class="ltx_text ltx_font_italic" id="S2.T1.3.3.1.1.p1.1.2" style="font-size:80%;">
<span class="ltx_tabular ltx_align_top" id="S2.T1.3.3.1.1.p1.1.2.1">
<span class="ltx_tr" id="S2.T1.3.3.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.3.3.1.1.p1.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.1.1.p1.1.2.1.1.1.1">
<span class="ltx_p" id="S2.T1.3.3.1.1.p1.1.2.1.1.1.1.1">Here is the testing task:</span>
</span></span></span>
<span class="ltx_tr" id="S2.T1.3.3.1.1.p1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.3.3.1.1.p1.1.2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.1.1.p1.1.2.1.2.1.1">
<span class="ltx_p" id="S2.T1.3.3.1.1.p1.1.2.1.2.1.1.1">Task description -¿ “change username to TEST”</span>
</span></span></span>
<span class="ltx_tr" id="S2.T1.3.3.1.1.p1.1.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T1.3.3.1.1.p1.1.2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.3.3.1.1.p1.1.2.1.3.1.1">
<span class="ltx_p" id="S2.T1.3.3.1.1.p1.1.2.1.3.1.1.1">Actions -¿</span>
</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.3.3.1.1.p1.1.3"></span></span>
</span></span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Generating Decomposed Actions</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">A LLMs prompt example to generate potential actions for task description is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.T1" title="Table 1 ‣ 2.1.1. Retrieving Few-shot Examples ‣ 2.1. Task Description Decomposition ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">1</span></a>, including <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS2.p1.1.1">⟨Instructions⟩</span> + <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS2.p1.1.2">⟨One-shot Example⟩</span> + <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS2.p1.1.3">⟨Testing Task Description⟩</span>.
Specifically, we first instruct the LLMs to outline the objective goal, which is to dissect the task description into the potential sequence of actions.
According to a small pilot study, we retrieve the top-1 representative examples for few-shot learning (as known as one-shot learning), aiding in the recognition of industrial app usage and output patterns.
Next, we present the testing task description as the test prompt and ask for the decomposed actions.
Due to the advantage of instruction prompting and few-shot learning, the LLMs will consistently generate a numeric list to represent the sequence of actions in the same format as our example output, which can be inferred using regular expressions.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>UI Automation Execution</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The second phase involves matching the decomposed actions with the UI elements to automate execution.
A common approach is to use lexical computation to match the displayed text of the elements on the current UI screen.
However, there may be mismatches with the target element due to the dynamic nature of the UI screen.
For instance, “sharing a moment” might be synthesized into a “moment A” element due to previous app usage, while the current element is “moment B” on the UI screen.
This can hinder the process of machine learning-based UI element mapping, leading to UI automation failure.
To address this issue, we utilize the ability of LLMs to semantically explore the app and understand the semantic correlation towards the target element.
Note that, we use the machine learning method as the primary UI element mapping and employ the LLMs as a complementary optimizer to save expenditure costs for industrial-level testing.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>UI Element Mapping</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.3">Consider the target element (<math alttext="e_{target}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.1.m1.1"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><msub id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml">e</mi><mrow id="S2.SS2.SSS1.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.3.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.3.cmml">a</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.3.1a" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.4" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.4.cmml">r</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.3.1b" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.5" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.5.cmml">g</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.3.1c" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.6" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.6.cmml">e</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.3.1d" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3.7" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.7.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><apply id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.2">𝑒</ci><apply id="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3"><times id="S2.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.2">𝑡</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.4">𝑟</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.5.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.5">𝑔</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.6.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.6">𝑒</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.7.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">e_{target}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.1d">italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) in the executive actions and the elements in the current UI screen (<math alttext="\{e_{1},e_{2},...,e_{n}\}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.2.m2.4"><semantics id="S2.SS2.SSS1.p1.2.m2.4a"><mrow id="S2.SS2.SSS1.p1.2.m2.4.4.3" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml"><mo id="S2.SS2.SSS1.p1.2.m2.4.4.3.4" stretchy="false" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml">{</mo><msub id="S2.SS2.SSS1.p1.2.m2.2.2.1.1" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.2" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1.2.cmml">e</mi><mn id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.3" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p1.2.m2.4.4.3.5" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS1.p1.2.m2.3.3.2.2" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.2" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2.2.cmml">e</mi><mn id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.3" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS1.p1.2.m2.4.4.3.6" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml">,</mo><mi id="S2.SS2.SSS1.p1.2.m2.1.1" mathvariant="normal" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">…</mi><mo id="S2.SS2.SSS1.p1.2.m2.4.4.3.7" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS1.p1.2.m2.4.4.3.3" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.2" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3.2.cmml">e</mi><mi id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.3" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3.3.cmml">n</mi></msub><mo id="S2.SS2.SSS1.p1.2.m2.4.4.3.8" stretchy="false" xref="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.4b"><set id="S2.SS2.SSS1.p1.2.m2.4.4.4.cmml" xref="S2.SS2.SSS1.p1.2.m2.4.4.3"><apply id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1.2">𝑒</ci><cn id="S2.SS2.SSS1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2.2">𝑒</ci><cn id="S2.SS2.SSS1.p1.2.m2.3.3.2.2.3.cmml" type="integer" xref="S2.SS2.SSS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">…</ci><apply id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3.2">𝑒</ci><ci id="S2.SS2.SSS1.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.4.4.3.3.3">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.4c">\{e_{1},e_{2},...,e_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.2.m2.4d">{ italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>).
Similar to the previous methods in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS1.SSS1" title="2.1.1. Retrieving Few-shot Examples ‣ 2.1. Task Description Decomposition ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>, we first use the transformer-based model to encode the UI elements into vectors.
We then use the similarity measurement method to compute the lexical similarity between UI elements and the target element: <math alttext="Similarity(e_{n},e_{target}))=cos(Encode(e_{n}),Encode(e_{target}))" class="ltx_math_unparsed" display="inline" id="S2.SS2.SSS1.p1.3.m3.1"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mrow id="S2.SS2.SSS1.p1.3.m3.1b"><mi id="S2.SS2.SSS1.p1.3.m3.1.1">S</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.2">i</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.3">m</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.4">i</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.5">l</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.6">a</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.7">r</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.8">i</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.9">t</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.10">y</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.11"><mo id="S2.SS2.SSS1.p1.3.m3.1.11.1" stretchy="false">(</mo><msub id="S2.SS2.SSS1.p1.3.m3.1.11.2"><mi id="S2.SS2.SSS1.p1.3.m3.1.11.2.2">e</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.11.2.3">n</mi></msub><mo id="S2.SS2.SSS1.p1.3.m3.1.11.3">,</mo><msub id="S2.SS2.SSS1.p1.3.m3.1.11.4"><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.2">e</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.11.4.3"><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.2">t</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.1">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.3">a</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.1a">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.4">r</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.1b">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.5">g</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.1c">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.6">e</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.1d">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.11.4.3.7">t</mi></mrow></msub><mo id="S2.SS2.SSS1.p1.3.m3.1.11.5" stretchy="false">)</mo></mrow><mo id="S2.SS2.SSS1.p1.3.m3.1.12" stretchy="false">)</mo><mo id="S2.SS2.SSS1.p1.3.m3.1.13">=</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.14">c</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.15">o</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.16">s</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.17" stretchy="false">(</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.18">E</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.19">n</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.20">c</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.21">o</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.22">d</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.23">e</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.24"><mo id="S2.SS2.SSS1.p1.3.m3.1.24.1" stretchy="false">(</mo><msub id="S2.SS2.SSS1.p1.3.m3.1.24.2"><mi id="S2.SS2.SSS1.p1.3.m3.1.24.2.2">e</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.24.2.3">n</mi></msub><mo id="S2.SS2.SSS1.p1.3.m3.1.24.3" stretchy="false">)</mo></mrow><mo id="S2.SS2.SSS1.p1.3.m3.1.25">,</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.26">E</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.27">n</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.28">c</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.29">o</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.30">d</mi><mi id="S2.SS2.SSS1.p1.3.m3.1.31">e</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.32"><mo id="S2.SS2.SSS1.p1.3.m3.1.32.1" stretchy="false">(</mo><msub id="S2.SS2.SSS1.p1.3.m3.1.32.2"><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.2">e</mi><mrow id="S2.SS2.SSS1.p1.3.m3.1.32.2.3"><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.2">t</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.1">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.3">a</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.1a">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.4">r</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.1b">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.5">g</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.1c">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.6">e</mi><mo id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.1d">⁢</mo><mi id="S2.SS2.SSS1.p1.3.m3.1.32.2.3.7">t</mi></mrow></msub><mo id="S2.SS2.SSS1.p1.3.m3.1.32.3" stretchy="false">)</mo></mrow><mo id="S2.SS2.SSS1.p1.3.m3.1.33" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">Similarity(e_{n},e_{target}))=cos(Encode(e_{n}),Encode(e_{target}))</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.3.m3.1d">italic_S italic_i italic_m italic_i italic_l italic_a italic_r italic_i italic_t italic_y ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) ) = italic_c italic_o italic_s ( italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) )</annotation></semantics></math>.
To match the most similar UI element on the UI screen to the target element, we identify the UI element with the highest similarity value.
Additionally, we establish a threshold value to ascertain whether the target element is matched, or if it may contain semantic mismatches, which would necessitate optimization by LLMs.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Prompt example for UI element mapping. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tr class="ltx_tr" id="S2.T2.2.3" style="background-color:#C4C4C4;">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.2.3.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.3.1.1" style="font-size:80%;background-color:#C4C4C4;">COMPONENT</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T2.2.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.3.2.1" style="background-color:#C4C4C4;">
<span class="ltx_p" id="S2.T2.2.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.3.2.1.1.1" style="font-size:80%;">DETAILS</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.2"><span class="ltx_text" id="S2.T2.1.1.2.1" style="font-size:80%;">⟨Instructions⟩</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T2.1.1.1"><span class="ltx_inline-logical-block ltx_align_top" id="S2.T2.1.1.1.1">
<span class="ltx_para ltx_noindent" id="S2.T2.1.1.1.1.p1">
<span class="ltx_p" id="S2.T2.1.1.1.1.p1.1"><span class="ltx_text" id="S2.T2.1.1.1.1.p1.1.1"></span><span class="ltx_text ltx_font_italic" id="S2.T2.1.1.1.1.p1.1.2" style="font-size:80%;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.1.1.1.1.p1.1.2.1">
<span class="ltx_tr" id="S2.T2.1.1.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T2.1.1.1.1.p1.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.1.p1.1.2.1.1.1.1">
<span class="ltx_p" id="S2.T2.1.1.1.1.p1.1.2.1.1.1.1.1">I want you to act as a professional developer. I would like you to analyze a given target element and the current UI screen, provided in the format of an XML view hierarchy. Please identify and respond with an element on the UI screen within brackets [like this], that is semantically related to the target element.</span>
</span></span></span>
</span></span><span class="ltx_text" id="S2.T2.1.1.1.1.p1.1.3"></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.2.2.2"><span class="ltx_text" id="S2.T2.2.2.2.1" style="font-size:80%;">⟨Testing Element⟩</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_t" id="S2.T2.2.2.1"><span class="ltx_inline-logical-block ltx_align_top" id="S2.T2.2.2.1.1">
<span class="ltx_para ltx_noindent" id="S2.T2.2.2.1.1.p1">
<span class="ltx_p" id="S2.T2.2.2.1.1.p1.1"><span class="ltx_text" id="S2.T2.2.2.1.1.p1.1.1"></span><span class="ltx_text ltx_font_italic" id="S2.T2.2.2.1.1.p1.1.2" style="font-size:80%;">
<span class="ltx_tabular ltx_align_top" id="S2.T2.2.2.1.1.p1.1.2.1">
<span class="ltx_tr" id="S2.T2.2.2.1.1.p1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T2.2.2.1.1.p1.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.2.1.1.p1.1.2.1.1.1.1">
<span class="ltx_p" id="S2.T2.2.2.1.1.p1.1.2.1.1.1.1.1">Target element -¿ “username”</span>
</span></span></span>
<span class="ltx_tr" id="S2.T2.2.2.1.1.p1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T2.2.2.1.1.p1.1.2.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.2.1.1.p1.1.2.1.2.1.1">
<span class="ltx_p" id="S2.T2.2.2.1.1.p1.1.2.1.2.1.1.1">Current UI screen -¿ ¡view hierarchy¿</span>
</span></span></span>
<span class="ltx_tr" id="S2.T2.2.2.1.1.p1.1.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_justify" id="S2.T2.2.2.1.1.p1.1.2.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.2.2.1.1.p1.1.2.1.3.1.1">
<span class="ltx_p" id="S2.T2.2.2.1.1.p1.1.2.1.3.1.1.1">Identified element -¿</span>
</span></span></span>
</span></span><span class="ltx_text" id="S2.T2.2.2.1.1.p1.1.3"></span></span>
</span></span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>LLMs Optimizer</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">LLMs are employed to specifically address occasional mismatches in UI elements.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.T2" title="Table 2 ‣ 2.2.1. UI Element Mapping ‣ 2.2. UI Automation Execution ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">2</span></a> presents an example of the prompts given to LLMs.
The prompt begins with an instruction outlining the objective of identifying UI elements on the screen that are semantically related to the target element.
A challenge in using LLMs for UI element mapping is their limitation in processing large text inputs, while the UI representation (i.e., view hierarchy) is typically lengthy - averaging thousands of tokens for each UI.
Although recent LLMs can handle visual inputs such as UI screens, recent research <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib8" title="">2023</a>)</cite> has highlighted constraints in visual UI understanding.
To address this, we propose a heuristic method to simplify nested layouts and extract atomic elements by traversing the view hierarchy tree using a depth-first search.
Specifically, we iterate through each node, starting from the root of the view hierarchy, and remove layouts that contain only one node, continuing to search its child node.
With the simplified UI representation, we prompt the LLMs to identify which UI element is semantically related to the target element.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Implementation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Our <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.1">CAT</span> is implemented as a fully automated UI automation testing tool.
According to a small-scale pilot study, we use the pre-trained ChatGPT model as the LLMs which was released on the OpenAI website <cite class="ltx_cite ltx_citemacro_citep">(web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib6" title="">2024e</a>)</cite>.
The basic model of ChatGPT is the gpt-4 model, representing the state-of-the-art LLMs.
To execute the UI automation on the device, we use Genymotion <cite class="ltx_cite ltx_citemacro_citep">(web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib5" title="">2024d</a>)</cite> for running and controlling the virtual Android device, Android UIAutomator <cite class="ltx_cite ltx_citemacro_citep">(web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib3" title="">2024b</a>)</cite> for dumping the UI view hierarchy, and Android Debug Bridge (ADB) <cite class="ltx_cite ltx_citemacro_citep">(web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib2" title="">2024a</a>)</cite> for executing the operations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Evaluation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we describe the procedure we used to evaluate <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">CAT</span>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">RQ1:</span> How effective is our approach in UI automation?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">RQ2:</span> How does our approach compare to state-of-the-art?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">RQ3:</span> How useful is our approach in a real-world testing environment?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>RQ1: Performance of UI Automation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Experimental Setup.</span>
We collect 39,981 task descriptions from the WeChat app<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>WeChat is among the most popular messenger apps in the world with over 1.67 billion monthly active users.</span></span></span> as our experimental dataset.
Each description, averaging 18.7 words, is provided by 71 internal developers and each invokes a core functionality of the app (on average 7.3 action steps).
These descriptions and their corresponding ground-truth action steps are utilized for UI automation tests across 24 app development cycles over a span of one year.
We divide the experimental dataset into testing and retrieval datasets for our evaluation.
Note that a simple random split may not adequately evaluate generalizability, as the same activities may have very similar descriptions.
To avoid this data leakage problem <cite class="ltx_cite ltx_citemacro_citep">(Kaufman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib28" title="">2012</a>)</cite>, we partition the descriptions in the dataset by app activities.
The resulting split includes 2,010 (5%) descriptions in the testing dataset, and 37,971 (95%) in the retrieval dataset.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Baselines.</span>
We set up four ablation studies as baselines to compare with our approach.
Given that the <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.2">CAT</span> comprises two main phases, we perform variations of our approach for each phase.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS1" title="2.1. Task Description Decomposition ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">2.1</span></a>, we introduce a retrieval-based method to select the top-1 examples to be used in the prompt for few-shot learning.
Thus, we examine the search space of no examples (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">0-shot RAG</span>) and N selected examples (<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">N-shot RAG</span>).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS2" title="2.2. UI Automation Execution ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we present an optimizer that utilizes LLMs to address the issues of mismatched UI elements on the dynamic UI screen.
Thus, we consider a variant of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p3.1.1">CAT<span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1.1"> (no optimizer)</span></span> to compare the performance of our approach with and without LLMs as the complementary method.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S2.SS3" title="2.3. Implementation ‣ 2. Approach ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we detail the implementation of our approach, employing ChatGPT as the LLMs. As a variant, we set up an ablation study using the open-sourced LLMs LLaMA70B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib34" title="">2023</a>)</cite>, referred to as <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p4.1.1">CAT<span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1.1"> (LLaMA70B)</span></span>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Evaluation Metrics.</span>
We employ three evaluation metrics: completion rate, financial cost, and time overhead.
The completion rate assesses the ability of the approach to successfully automate the task within the app.
A higher completion rate indicates a more effective approach to executing UI automation tests.
As the ultimate goal is cost-effectiveness, we calculate the average expenditure incurred for inferring the LLMs throughout the UI automation tests.
Lastly, we also measure the duration of time spent, in minutes.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Results.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.T3" title="Table 3 ‣ 3.1. RQ1: Performance of UI Automation ‣ 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the performance of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p6.1.2">CAT</span> in executing UI automation for task descriptions.
Our approach achieves an average completion rate of 90% at a cost of $0.34, outperforming the ablation baselines.
We observe that the method excluding few-shot learning (0-shot RAG) only attains a 50% completion rate due to the reason that the LLMs’ lack of knowledge and experience with certain apps.
In comparison to 0-shot RAG, the use of few-shot examples can significantly help LLMs in understanding app usage knowledge, increasing the completion rate by 40% for 1-shot learning in our approach.
However, supplementing extensive examples (N-shot RAG) does not enhance the approach’s performance.
This is because the longer context of the examples might lead the LLMs to display tendencies towards instruction forgetting, format errors, and abnormal reasoning.
Instead, it incurs a much higher cost, making it 150% more expensive than our approach.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">In addition, optimizing UI element mapping using LLMs can further improve the approach’s performance of 38% completion rate compared to the ablation baseline of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p7.1.1">CAT</span> without an optimizer.
This suggests that the LLMs possess the ability to comprehend the semantic correlation between the target elements and the elements in the dynamic UI, potentially identifying the likely operations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">Despite the fact that using open-sourced LLMs does not involve financial costs, the state-of-the-art model LLaMA70B only achieves a 71% completion rate.
However, we are optimistic that once these open-sourced LLMs can provide comparable performance with the ChatGPT model, we could integrate our base model with the open-sourced LLMs, thereby further reducing costs for future UI automation tests.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Performance comparison of ablation studies.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.3">
<tr class="ltx_tr" id="S3.T3.3.1">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T3.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.1.1" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.2.1" style="font-size:90%;">Completion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.3.1" style="font-size:90%;">Avg. Cost</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.4.1" style="font-size:90%;">Avg. Time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.2">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T3.3.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T3.3.2.1.1" style="font-size:90%;">CAT</span><span class="ltx_text" id="S3.T3.3.2.1.2" style="font-size:90%;"> (0-shot RAG)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.2.2"><span class="ltx_text" id="S3.T3.3.2.2.1" style="font-size:90%;">50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.2.3"><span class="ltx_text" id="S3.T3.3.2.3.1" style="font-size:90%;">$0.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.2.4"><span class="ltx_text" id="S3.T3.3.2.4.1" style="font-size:90%;">2.61 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T3.3.3.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T3.3.3.1.1" style="font-size:90%;">CAT</span><span class="ltx_text" id="S3.T3.3.3.1.2" style="font-size:90%;"> (N-shot RAG)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.3.2"><span class="ltx_text" id="S3.T3.3.3.2.1" style="font-size:90%;">66%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.3.3"><span class="ltx_text" id="S3.T3.3.3.3.1" style="font-size:90%;">$0.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.3.4"><span class="ltx_text" id="S3.T3.3.3.4.1" style="font-size:90%;">3.20 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.4">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T3.3.4.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T3.3.4.1.1" style="font-size:90%;">CAT</span><span class="ltx_text" id="S3.T3.3.4.1.2" style="font-size:90%;"> (no optimizer)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.4.2"><span class="ltx_text" id="S3.T3.3.4.2.1" style="font-size:90%;">52%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.4.3"><span class="ltx_text" id="S3.T3.3.4.3.1" style="font-size:90%;">$0.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.4.4"><span class="ltx_text" id="S3.T3.3.4.4.1" style="font-size:90%;">5.26 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.5">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T3.3.5.1">
<span class="ltx_text ltx_font_smallcaps" id="S3.T3.3.5.1.1" style="font-size:90%;">CAT</span><span class="ltx_text" id="S3.T3.3.5.1.2" style="font-size:90%;"> (LLaMA70B)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.5.2"><span class="ltx_text" id="S3.T3.3.5.2.1" style="font-size:90%;">71%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.5.3"><span class="ltx_text" id="S3.T3.3.5.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.3.5.4"><span class="ltx_text" id="S3.T3.3.5.4.1" style="font-size:90%;">4.07 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.6" style="background-color:#C4C4C4;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_tt" id="S3.T3.3.6.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T3.3.6.1.1" style="font-size:90%;background-color:#C4C4C4;">CAT</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S3.T3.3.6.2"><span class="ltx_text" id="S3.T3.3.6.2.1" style="font-size:90%;background-color:#C4C4C4;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S3.T3.3.6.3"><span class="ltx_text" id="S3.T3.3.6.3.1" style="font-size:90%;background-color:#C4C4C4;">$0.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_tt" id="S3.T3.3.6.4"><span class="ltx_text" id="S3.T3.3.6.4.1" style="font-size:90%;background-color:#C4C4C4;">2.65 min</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>RQ2: Comparison with State-of-the-Art</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Experimental Setup.</span>
To answer RQ2, we evaluate the comparison of our approach to the state-of-the-art baselines.
We also use the experimental dataset collected in RQ1 (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.SS1" title="3.1. RQ1: Performance of UI Automation ‣ 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Baselines.</span>
We set up two state-of-the-art methods as baselines for comparison with our approach.
These include two machine-learning-based methods and one LLMs-based method, all of which are widely used for UI automation.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Seq2Act</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib29" title="">2020</a>)</cite> utilizes models to extract actions and object targets from task descriptions, associating them with UI elements to facilitate UI automation.
<span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">AdbGPT</span> <cite class="ltx_cite ltx_citemacro_citep">(Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a>)</cite> employs the recent advancements of LLMs for automating UI tasks to extract action entities from task descriptions and make decisions for selecting executable UI elements.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Evaluation Metrics.</span>
To compare with the state-of-the-art baselines, we also employ three evaluation metrics, i.e., completion rate, financial cost, and time overhead.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Results.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#S3.T4" title="Table 4 ‣ 3.2. RQ2: Comparison with State-of-the-Art ‣ 3. Evaluation ‣ Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"><span class="ltx_text ltx_ref_tag">4</span></a> presents the performance comparison with the baselines.
The machine learning-based method (Seq2Act), while not incurring any financial costs, only attains completion rates of 35%, which is 55% lower than our approach <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p4.1.2">CAT</span>.
Among the baselines, AdbGPT exhibits the best performance, achieving a completion rate of 90%. However, it is costly, averaging $1.07 for UI automation due to the extensive use of LLMs.
In contrast, our approach, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p4.1.3">CAT</span>, which integrates machine-learning methods and LLMs optimally, saves $1,467 without compromising the completion rate.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Performance comparison of state-of-the-art.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.3">
<tr class="ltx_tr" id="S3.T4.3.1">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T4.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.3.1.1.1" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.3.1.2.1" style="font-size:90%;">Completion</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.3.1.3.1" style="font-size:90%;">Avg. Cost</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.3.1.4"><span class="ltx_text ltx_font_bold" id="S3.T4.3.1.4.1" style="font-size:90%;">Avg. Time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.2">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T4.3.2.1">
<span class="ltx_text" id="S3.T4.3.2.1.1" style="font-size:90%;">Seq2Act </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.3.2.1.2.1" style="font-size:90%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.3.2.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib29" title="">2020</a><span class="ltx_text" id="S3.T4.3.2.1.4.3" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.2.2"><span class="ltx_text" id="S3.T4.3.2.2.1" style="font-size:90%;">35%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.2.3"><span class="ltx_text" id="S3.T4.3.2.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.3.2.4"><span class="ltx_text" id="S3.T4.3.2.4.1" style="font-size:90%;">5.89 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3">
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S3.T4.3.3.1">
<span class="ltx_text" id="S3.T4.3.3.1.1" style="font-size:90%;">AdbGPT </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.3.3.1.2.1" style="font-size:90%;">(</span>Feng and Chen<span class="ltx_text" id="S3.T4.3.3.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a><span class="ltx_text" id="S3.T4.3.3.1.4.3" style="font-size:90%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.3.2"><span class="ltx_text" id="S3.T4.3.3.2.1" style="font-size:90%;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.3.3"><span class="ltx_text" id="S3.T4.3.3.3.1" style="font-size:90%;">$1.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.3.3.4"><span class="ltx_text" id="S3.T4.3.3.4.1" style="font-size:90%;">2.91 min</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.4" style="background-color:#C4C4C4;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t" id="S3.T4.3.4.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T4.3.4.1.1" style="font-size:90%;background-color:#C4C4C4;">CAT</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.3.4.2"><span class="ltx_text" id="S3.T4.3.4.2.1" style="font-size:90%;background-color:#C4C4C4;">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.3.4.3"><span class="ltx_text" id="S3.T4.3.4.3.1" style="font-size:90%;background-color:#C4C4C4;">$0.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T4.3.4.4"><span class="ltx_text" id="S3.T4.3.4.4.1" style="font-size:90%;background-color:#C4C4C4;">2.63 min</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>RQ3: Usefulness of <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.1.1">CAT</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Industrial Usage.</span>
We collaborate with Tencent to incorporate our approach, <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.1.2">CAT</span>, into the WeChat testing platform.
<span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.1.3">CAT</span> is integrated into the internal UI automation process and is activated whenever new features are proposed or a new app version is released, using the new task descriptions.
As the ultimate goal aim of UI automation is to detect bugs, we also combine our approach with internal bug detection methods through UI automation tests.
Specifically, we use heuristics to monitor event logs to identify crash bugs.
For non-crash bugs, we leverage previous studies such as UI display bug detection <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib36" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib37" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib9" title="">2019</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib15" title="">2022a</a>)</cite> and functional bug detection <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib10" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib20" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib17" title="">2022b</a>)</cite>.
We set up the number of bugs as the evaluation metric to assess the usefulness of our approach.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Results.</span>
We run the experiment in the WeChat testing system, with over 6,300 task descriptions from December 2023 to June 2024.
The completion rate for UI automation reached 88%, indicating that most task descriptions can be successfully automated and tested without the need for developer interaction.
In addition, we automatically detect 141 bugs in these task descriptions.
Note that the detected bugs are also reviewed by internal developers to verify their validity.
This indicates the usefulness of our <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.2">CAT</span> in reducing human effort in bug detection and significantly saving time in subsequent bug fixes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Implication and Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Although many tools <cite class="ltx_cite ltx_citemacro_citep">(Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib35" title="">2024</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib19" title="">2024</a>)</cite> have been proposed that rely solely on LLMs for UI automation, we have opted to combine mature machine learning with LLMs for industrial-level app testing, based on two practical lessons learned.
First, the computational cost for LLMs is high, which is not feasible in an industry that often requires hundreds or thousands of testing tasks.
Second, the LLMs are not entirely reliable.
Due to the nature of generative models, LLMs may often hallucinate, i.e., respond with unrelated concepts. In contrast, a hybrid of machine learning methods and LLMs can significantly mitigate these issues, making the testing system more robust for industrial use.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Another potential interest is the generalizability of our approach to other industrial apps.
In this study, we focus on the WeChat app, using the 39k tasks in the WeChat dataset to assess the performance of our approach.
We believe that our approach should be easily adapted to other industrial apps, given the commonality of the dataset used in industrial app testing <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib23" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Existing automated test generation techniques <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib25" title="">2019</a>; Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib33" title="">2017</a>; web, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib7" title="">2024f</a>; Mao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib30" title="">2016</a>)</cite> share a complementary objective to ours: they mainly focus on generating tests to maximize code coverage and detect bugs.
As opposed, we aim to generate tests that cover specific functionalities (similar to our definition of tasks), guided by manually written descriptions.
The area of written descriptions that most closely aligns with test generation is the work on script-based record and replay.
For example, RERAN <cite class="ltx_cite ltx_citemacro_citep">(Gomez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib24" title="">2013</a>)</cite> is among the earliest record-and-replay tools utilizing the Linux kernel.
Guo et al. <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib26" title="">2019</a>)</cite> introduce an industrial-scale record-and-replay tool, SARA, designed for widget-sensitive and time-sensitive recording and replaying.
Over the past few years, several tools like Robotium <cite class="ltx_cite ltx_citemacro_citep">(Zadgaonkar, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib38" title="">2013</a>)</cite>, and WeReplay <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib18" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib21" title="">c</a>)</cite>, have been developed.
Additionally, there are numerous supplementary tools <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib22" title="">2023d</a>; Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib13" title="">2022b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib12" title="">a</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib16" title="">2023a</a>)</cite> that assist in script writing.
However, these scripts often refer to UI elements with absolute positions or rely on fragile rules, which hinders industries from adopting these methods.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Consequently, many studies aim to advance upon this by simplifying the process, relying on high-level natural language descriptions that outline the desired test tasks.
For instance, AppFlow <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib27" title="">2018</a>)</cite> synthesizes UI automation tests according to the test library.
However, these synthesized tests may lack robustness and contain specific issues like mismatched UI elements, leading to test failure.
Recently, some studies have utilized LLMs to address these failures.
For example, Feng et al. <cite class="ltx_cite ltx_citemacro_citep">(Feng and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2409.07829v1#bib.bib14" title="">2024</a>)</cite> introduce AdbGPT which equips the LLMs with the semantic understanding to guide the replay of certain descriptions with impressive performance.
However, using LLMs for these stochastic app explorations might be expensive and ineffective for industrial app testing due to cost optimization and knowledge integration.
In contrast, we propose a hybrid approach, <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">CAT</span>, that combines machine learning methods and LLMs to adopt the best practice for generating robust UI automation tests for cost-effective industrial-level testing.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We present <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">CAT</span>, a practical solution for generating UI automation tests for mobile apps.
Specifically, given a high-level task description, <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">CAT</span> first applies RAG to retrieve relevant app usages as examples to elicit specific knowledge for LLMs to generate a concrete sequence of actions.
It then employs machine learning and LLMs to adapt these actions to the app’s dynamic UI, correcting any element discrepancies.
Our evaluation of the WeChat dataset, comprising 39k tasks, shows a 90% completion rate in executing UI automation tests at just $0.34 per test.
Additionally, its integration into the WeChat testing platform has led to the automatic identification of 141 bugs, easing the test-writing burden for developers.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We appreciate the assistance from Chunyang Chen and the WeCom team for their valuable contributions to the methodology discussion and some experimental processes. This research is supported by the Australian Research Council under grant DP210100041.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024a)</span>
<span class="ltx_bibblock">
2024a.

</span>
<span class="ltx_bibblock">Android Debug Bridge (adb) - Android Developers.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.android.com/studio/command-line/adb" title="">https://developer.android.com/studio/command-line/adb</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024b)</span>
<span class="ltx_bibblock">
2024b.

</span>
<span class="ltx_bibblock">Android Uiautomator2 Python Wrapper.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openatx/uiautomator2" title="">https://github.com/openatx/uiautomator2</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024c)</span>
<span class="ltx_bibblock">
2024c.

</span>
<span class="ltx_bibblock">Developers warned: GitHub Copilot code may be licensed.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.techtarget.com/searchsoftwarequality/news/252526359/Developers-warned-GitHub-Copilot-code-may-be-licensed" title="">https://www.techtarget.com/searchsoftwarequality/news/252526359/Developers-warned-GitHub-Copilot-code-may-be-licensed</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024d)</span>
<span class="ltx_bibblock">
2024d.

</span>
<span class="ltx_bibblock">Genymotion – Android Emulator for app testing.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.genymotion.com/" title="">https://www.genymotion.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024e)</span>
<span class="ltx_bibblock">
2024e.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">web (2024f)</span>
<span class="ltx_bibblock">
2024f.

</span>
<span class="ltx_bibblock">UI/Application Exerciser Monkey.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.android.com/studio/test/other-testing-tools/monkey" title="">https://developer.android.com/studio/test/other-testing-tools/monkey</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al<span class="ltx_text" id="bib.bib8.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.4.1">arXiv preprint arXiv:2303.12712</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao, and Jinshui Wang. 2019.

</span>
<span class="ltx_bibblock">Gallery dc: Design search and knowledge discovery through auto-created gui component gallery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the ACM on Human-Computer Interaction</em> 3, CSCW (2019), 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jieshan Chen, Jiamou Sun, Sidong Feng, Zhenchang Xing, Qinghua Lu, Xiwei Xu, and Chunyang Chen. 2023.

</span>
<span class="ltx_bibblock">Unveiling the tricks: Automated detection of dark patterns in mobile applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Wontae Choi, Koushik Sen, George Necula, and Wenyu Wang. 2018.

</span>
<span class="ltx_bibblock">DetReduce: minimizing Android GUI test suites for regression testing. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 40th International Conference on Software Engineering</em>. 445–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Chen (2022a)</span>
<span class="ltx_bibblock">
Sidong Feng and Chunyang Chen. 2022a.

</span>
<span class="ltx_bibblock">GIFdroid: an automated light-weight tool for replaying visual bug reports. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings</em>. 95–99.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Chen (2022b)</span>
<span class="ltx_bibblock">
Sidong Feng and Chunyang Chen. 2022b.

</span>
<span class="ltx_bibblock">Gifdroid: Automated replay of visual bug reports for android apps. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 44th International Conference on Software Engineering</em>. 1045–1057.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Chen (2024)</span>
<span class="ltx_bibblock">
Sidong Feng and Chunyang Chen. 2024.

</span>
<span class="ltx_bibblock">Prompting is all you need: Automated android bug replay with large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 46th IEEE/ACM International Conference on Software Engineering</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Sidong Feng, Chunyang Chen, and Zhenchang Xing. 2022a.

</span>
<span class="ltx_bibblock">Gallery dc: Auto-created gui component gallery for design search and knowledge discovery. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings</em>. 80–84.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Sidong Feng, Chunyang Chen, and Zhenchang Xing. 2023a.

</span>
<span class="ltx_bibblock">Video2Action: Reducing human interactions in action annotation of app tutorial videos. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Sidong Feng, Minmin Jiang, Tingting Zhou, Yankun Zhen, and Chunyang Chen. 2022b.

</span>
<span class="ltx_bibblock">Auto-icon+: An automated end-to-end code generation tool for icon designs in ui development.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">ACM Transactions on Interactive Intelligent Systems</em> 12, 4 (2022), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Sidong Feng, Haochuan Lu, Ting Xiong, Yuetang Deng, and Chunyang Chen. 2023b.

</span>
<span class="ltx_bibblock">Towards Efficient Record and Replay: A Case Study in WeChat. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>. 1681–1692.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sidong Feng, Suyu Ma, Han Wang, David Kong, and Chunyang Chen. 2024.

</span>
<span class="ltx_bibblock">MUD: Towards a Large-Scale and Noise-Filtered UI Dataset for Modern Style UI Modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Sidong Feng, Suyu Ma, Jinzhong Yu, Chunyang Chen, Tingting Zhou, and Yankun Zhen. 2021.

</span>
<span class="ltx_bibblock">Auto-icon: An automated code generation tool for icon designs assisting in ui development. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 26th International Conference on Intelligent User Interfaces</em>. 59–69.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Sidong Feng, Mulong Xie, and Chunyang Chen. 2023c.

</span>
<span class="ltx_bibblock">Efficiency matters: Speeding up automated testing with gui rendering inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</em>. IEEE, 906–918.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Sidong Feng, Mulong Xie, Yinxing Xue, and Chunyang Chen. 2023d.

</span>
<span class="ltx_bibblock">Read It, Don’t Watch It: Captioning Bug Recordings Automatically. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</em>. IEEE, 2349–2361.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Cuiyun Gao, Wujie Zheng, Yuetang Deng, David Lo, Jichuan Zeng, Michael R Lyu, and Irwin King. 2019.

</span>
<span class="ltx_bibblock">Emerging app issue identification from user feedback: Experience on wechat. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</em>. IEEE, 279–288.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gomez et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Lorenzo Gomez, Iulian Neamtiu, Tanzirul Azim, and Todd Millstein. 2013.

</span>
<span class="ltx_bibblock">Reran: Timing-and touch-sensitive record and replay for android. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">2013 35th International Conference on Software Engineering (ICSE)</em>. IEEE, 72–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, and Zhendong Su. 2019.

</span>
<span class="ltx_bibblock">Practical GUI testing of Android applications via model abstraction and refinement. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)</em>. IEEE, 269–280.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiaqi Guo, Shuyue Li, Jian-Guang Lou, Zijiang Yang, and Ting Liu. 2019.

</span>
<span class="ltx_bibblock">Sara: self-replay augmented record and replay for Android in industrial cases. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 28th acm sigsoft international symposium on software testing and analysis</em>. 90–100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Gang Hu, Linjie Zhu, and Junfeng Yang. 2018.

</span>
<span class="ltx_bibblock">AppFlow: using machine learning to synthesize robust, reusable UI tests. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>. 269–282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaufman et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Shachar Kaufman, Saharon Rosset, Claudia Perlich, and Ori Stitelman. 2012.

</span>
<span class="ltx_bibblock">Leakage in data mining: Formulation, detection, and avoidance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">ACM Transactions on Knowledge Discovery from Data (TKDD)</em> 6, 4 (2012), 1–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020.

</span>
<span class="ltx_bibblock">Mapping natural language instructions to mobile UI action sequences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2005.03776</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Ke Mao, Mark Harman, and Yue Jia. 2016.

</span>
<span class="ltx_bibblock">Sapienz: Multi-objective automated testing for android applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 25th International Symposium on Software Testing and Analysis</em>. 94–105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">The Journal of Machine Learning Research</em> 21, 1 (2020), 5485–5551.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Amit Singhal et al<span class="ltx_text" id="bib.bib32.3.1">.</span> 2001.

</span>
<span class="ltx_bibblock">Modern information retrieval: A brief overview.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.1">IEEE Data Eng. Bull.</em> 24, 4 (2001), 35–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ting Su, Guozhu Meng, Yuting Chen, Ke Wu, Weiming Yang, Yao Yao, Geguang Pu, Yang Liu, and Zhendong Su. 2017.

</span>
<span class="ltx_bibblock">Guided, stochastic model-based GUI testing of Android apps. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</em>. 245–256.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al<span class="ltx_text" id="bib.bib34.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.4.1">arXiv preprint arXiv:2302.13971</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dingbang Wang, Yu Zhao, Sidong Feng, Zhaoxu Zhang, William GJ Halfond, Chunyang Chen, Xiaoxia Sun, Jiangfan Shi, and Tingting Yu. 2024.

</span>
<span class="ltx_bibblock">Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2407.05165</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen. 2020.

</span>
<span class="ltx_bibblock">UIED: a hybrid tool for GUI element detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>. 1655–1659.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mulong Xie, Zhenchang Xing, Sidong Feng, Xiwei Xu, Liming Zhu, and Chunyang Chen. 2022.

</span>
<span class="ltx_bibblock">Psychologically-inspired, unsupervised inference of perceptual groups of GUI widgets from GUI images. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>. 332–343.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadgaonkar (2013)</span>
<span class="ltx_bibblock">
Hrushikesh Zadgaonkar. 2013.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Robotium automated testing for android</em>.

</span>
<span class="ltx_bibblock">Packt Publishing.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 08:23:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
