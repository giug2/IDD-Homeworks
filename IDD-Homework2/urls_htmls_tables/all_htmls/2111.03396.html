<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.03396] FedLess: Secure and Scalable Federated Learning Using Serverless Computing</title><meta property="og:description" content="The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedLess: Secure and Scalable Federated Learning Using Serverless Computing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedLess: Secure and Scalable Federated Learning Using Serverless Computing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.03396">

<!--Generated on Wed Mar  6 20:30:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Function-as-a-service (FaaS),  serverless computing,  federated learning,  deep learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\epstopdfDeclareGraphicsRule</span>
<p id="p1.2" class="ltx_p">.tifpng.pngconvert #1 <span id="p1.2.1" class="ltx_ERROR undefined">\OutputFile</span>
<span id="p1.2.2" class="ltx_ERROR undefined">\epstopdfDeclareGraphicsRule</span>.tiffpng.pngconvert #1 <span id="p1.2.3" class="ltx_ERROR undefined">\OutputFile</span>
<span id="p1.2.4" class="ltx_ERROR undefined">\PrependGraphicsExtensions</span>.tif, .tiff
</p>
</div>
<h1 class="ltx_title ltx_title_document">FedLess: Secure and Scalable Federated Learning Using Serverless Computing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Grafberger1, Mohak Chadha1, Anshul Jindal1, Jianfeng Gu1, Michael Gerndt1
<br class="ltx_break">
Email: {andreas.grafberger, mohak.chadha, anshul.jindal, jianfeng.gu}@tum.de, gerndt@in.tum.de
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1Chair of Computer Architecture and Parallel Systems, Technische Universität München 
<br class="ltx_break">Garching (near Munich), Germany 
<br class="ltx_break">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables remote clients to learn a shared ML model while keeping the data local. However, conventional FL systems face several challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called <em id="id1.id1.1" class="ltx_emph ltx_font_italic">FedLess</em>. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Function-as-a-service (FaaS), serverless computing, federated learning, deep learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Heterogeneous remote edge devices or siloed data centers such as mobile phones or hospitals generate manifolds of data each day <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Using deep learning, the data generated by these devices and institutions can be used to enable smart applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In the conventional deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> approach, all the raw training data is collected and stored on a central server. However, increasing privacy concerns of data holders and recent legislation on data protection and privacy such as the European General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> prevent the transmission of data to a centralized location, thus making it impossible to train DNNs on data stored and processed in different locations. Towards this, a new paradigm for distributed ML training called Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> has been introduced. FL enables the collaborative training of ML models and addresses the fundamental problems of privacy and ownership of data. In FL, remote clients learn a shared model by optimizing its parameters on their local data and sending back the updated parameters. These local model updates are then aggregated to form the new, updated shared model. Clients in FL can be mobile, edge devices, institutions operating their own data centers, or Virtual Machines managed by Infrastructure-as-a-Service (IaaS) providers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. In this paper, we argue that both components in a traditional FL system, i.e., clients and server, can immensely benefit from a new computing paradigm called serverless computing.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In serverless computing, developers do not have to manage infrastructure themselves but completely hand over this responsibility to a Function-as-a-Service Platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Several open-source and commercial FaaS platforms such as OpenWhisk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, OpenFaaS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, AWS Lambda <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and Google Cloud Functions (GCF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> are currently available. Applications are developed as small units of code, called functions that are independently packaged and uploaded to a FaaS platform and executed on event triggers such as HTTP requests. In the context of serverless, FL clients are functions deployed on a FaaS platform that are capable of calculating the shared model updates. A connected group of different FaaS platforms is referred to as a FaaS fabric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">On the client-side, using FaaS technologies can lead to improvements in resource-efficiency and cost. For instance, a common practice in FL is to only select a small fraction of available clients for each training round. Increasing client parallelism is only beneficial up to a certain point with diminishing returns when more clients are included in each training round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. As a result, most clients wait until they are selected in a later round and stay idle, leading to wastage of resources and incurred costs if the clients use an IaaS provider. The idle times are further enhanced due to the potential heterogeneity in clients’ computing capabilities or local dataset size (§<a href="#S2.SS2" title="II-B Federated Learning (FL) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>). On the FL server-side, one common challenge is to efficiently handle sudden bursts of compute-intensive workloads whenever clients report their local updates for aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In addition to wasted compute or scalability challenges, requiring all data holders to manage the complex infrastructure for their clients can be an enormous burden. These challenges of current FL systems closely align with the core problems that serverless technologies and FaaS platforms aim to solve. These include rapid scalability during request bursts, automatic scaling to zero when resources are unused, and an attractive pricing and development model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Towards efficient, scalable, and enabling ease-of-use for FL, our key contributions are:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present a novel system and framework called <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/andreas-grafberger/fedless</span></span></span> to perform FL on a heterogeneous fabric of FaaS platforms with support for arbitrary DNN models using Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, proper authentication and authorization of clients, and a privacy-protecting training mechanism.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> supports four major commercial FaaS platforms, i.e., AWS Lambda, GCF, Azure Functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and IBM Cloud Functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and two major open-source platforms, i.e., OpenWhisk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and OpenFaaS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We provide examples and helper functions to run FL client functions on all these platforms.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We demonstrate with extensive experiments the features and scalability of <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> on three popular FL benchmark datasets using different DNN models.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We compare <em id="S1.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> with an optimized reimplementation of an existing FaaS based FL system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and demonstrate that our system is significantly faster.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">We practically demonstrate the viability of <em id="S1.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> by comparing it with a traditional IaaS based FL system using Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> with respect to performance and cost.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The rest of this paper is structured as follows. §<a href="#S2" title="II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides a background on FaaS and FL. In §<a href="#S3" title="III Related Work ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the previous work on serverless ML, FL, and other FL frameworks are described. §<a href="#S4" title="IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> describes our system and framework <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">FedLess</em>. In §<a href="#S5" title="V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, our experimental setup is described, while §<a href="#S6" title="VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> presents our experimental results. Finally, §<a href="#S7" title="VII Conclusion and Future Work ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes the paper and presents an outlook.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2111.03396/assets/Images/Diagrams/faas_typical_workflow.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Workflow demonstrating functioning of the FaaS paradigm.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Function-as-a-Service (FaaS)</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">Due to its simplicity, client-friendly cost model, and automatic scaling, FaaS is emerging as a preferred cloud-based programming paradigm. It has gained popularity and widespread adoption in different application domains such as linear algebra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and heterogeneous computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. An overview of the functioning of the FaaS paradigm is shown in Figure 1. In FaaS, the user implements fine-grained functions that are executed in response to event triggers or HTTP requests ( <svg id="S2.SS1.p1.1.pic1" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS1.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>). On function invocation, the FaaS platform is responsible for providing resources to the function and its isolation in ephemeral, stateless containers. These containers are commonly referred to as function instances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and contain the function’s language-specific runtime environment. For commercial FaaS platforms, the function instances are launched on the FaaS platforms’ traditional Infrastructure-as-a-Service Virtual Machines as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The runtime is responsible for relaying the invocation events, context information, and responses between the FaaS platform and the function. On the first invocation of a function, the FaaS platform creates a new function instance, i.e., cold start <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and runs its handler method to process the event ( <svg id="S2.SS1.p1.2.pic2" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS1.p1.2.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>- <svg id="S2.SS1.p1.3.pic3" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS1.p1.3.pic3.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>). When the handler returns a response or exits ( <svg id="S2.SS1.p1.4.pic4" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS1.p1.4.pic4.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>), the function instance remains active for a specific duration to handle successive events. The FaaS platform can create concurrent function instances to handle multiple events or requests on-demand. When the number of requests decreases, the FaaS platform automatically scales down the number of active function instances. For commercial FaaS platforms, users are billed based on the execution time of the functions measured in 100ms or 1ms intervals, i.e., pay-per-use. Moreover, most commercial FaaS platforms limit the maximum execution time and memory a function can have. For instance, with AWS Lambda, these limits are <math id="S2.SS1.p1.5.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S2.SS1.p1.5.m1.1a"><mn id="S2.SS1.p1.5.m1.1.1" xref="S2.SS1.p1.5.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m1.1b"><cn type="integer" id="S2.SS1.p1.5.m1.1.1.cmml" xref="S2.SS1.p1.5.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m1.1c">15</annotation></semantics></math> minutes and <math id="S2.SS1.p1.6.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.SS1.p1.6.m2.1a"><mn id="S2.SS1.p1.6.m2.1.1" xref="S2.SS1.p1.6.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m2.1b"><cn type="integer" id="S2.SS1.p1.6.m2.1.1.cmml" xref="S2.SS1.p1.6.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m2.1c">10</annotation></semantics></math>GB, respectively. However, due to the rapid development in FaaS, these limits might disappear or be relaxed soon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2111.03396/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">General synchronous training workflow in FL.</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Federated Learning (FL)</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.3" class="ltx_p">FL describes a new learning paradigm for data holders to collaboratively train an ML model while keeping their data private. A typical FL system has two main entities, i.e., <em id="S2.SS2.p1.3.1" class="ltx_emph ltx_font_italic">clients</em> and a <em id="S2.SS2.p1.3.2" class="ltx_emph ltx_font_italic">central server</em>. Depending on the FL category, i.e., <em id="S2.SS2.p1.3.3" class="ltx_emph ltx_font_italic">cross-device</em> or <em id="S2.SS2.p1.3.4" class="ltx_emph ltx_font_italic">cross-silo</em>, the clients in FL can range from mobile, edge devices to machines running in cloud data centers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. In <em id="S2.SS2.p1.3.5" class="ltx_emph ltx_font_italic">cross-device</em> setting, the number of clients can be significantly large with limited network and compute capabilities, while in <em id="S2.SS2.p1.3.6" class="ltx_emph ltx_font_italic">cross-silo</em> setting, there are few clients, all with abundant computing resources. Figure <a href="#S2.F2" title="Figure 2 ‣ II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the general synchronous training workflow for both FL categories. At the start of each round ( <svg id="S2.SS2.p1.1.pic1" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS2.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>), the server sends the current (at the start randomly initialized) global model to all the clients. Following this, each client improves the shared model by optimizing it on its local dataset and sends back only the updated model parameters ( <svg id="S2.SS2.p1.2.pic2" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS2.p1.2.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>). Finally, the local model updates from all participating clients are collected and aggregated ( <svg id="S2.SS2.p1.3.pic3" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S2.SS2.p1.3.pic3.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>). The server usually does not possess its own dataset and is primarily responsible for organizing the training and deciding which clients contribute in a new round. The default implementation in <em id="S2.SS2.p1.3.7" class="ltx_emph ltx_font_italic">FedLess</em> uses the <span id="S2.SS2.p1.3.8" class="ltx_text ltx_font_typewriter">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> algorithm for aggregating the model parameters. In contrast to traditional distributed ML, the training data present on clients in FL is <em id="S2.SS2.p1.3.9" class="ltx_emph ltx_font_italic">non-IID</em>, i.e., one client’s dataset is not representative of the full data distribution across all clients, and <em id="S2.SS2.p1.3.10" class="ltx_emph ltx_font_italic">unbalanced</em>, i.e., a small number of clients can have large local datasets while other clients can contain only a few records.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold">Serverless Machine Learning</span>.
The majority of the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> in this domain have focused on distributed ML training using FaaS functions. <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">Siren</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, allows users to train ML models in the cloud using fine-grained functions, thereby removing the burden of non-trivial cluster provisioning and management from the developers. Furthermore, it enables asynchronous distributed training and features an algorithm based on Deep Reinforcement Learning that continuously tunes the number of functions and their memory to optimize performance and cost. In contrast to <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">Siren</em>, <em id="S3.p1.1.4" class="ltx_emph ltx_font_italic">Cirrus</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> supports complete end-to-end ML workflows including data preprocessing and hyperparameter tuning. It provides a lightweight worker runtime for the cloud functions that support various ML models. Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> analyze the cost-performance trade-offs between Infrastructure-as-a-Service (IaaS) and FaaS for distributed ML. Towards this, they developed <em id="S3.p1.1.5" class="ltx_emph ltx_font_italic">LambdaML</em>, which supports different distributed ML variants such as synchronous vs. asynchronous training, purely FaaS-based, or a hybrid (FaaS/IaaS) training approach. In contrast to previous works, <em id="S3.p1.1.6" class="ltx_emph ltx_font_italic">FedLess</em> differs in several ways. First, although FL and distributed ML share some similarities, they have key fundamental differences. For instance, data in FL is private to the client and not accessible by the central server. Furthermore, in distributed ML, workers are rarely idle, whereas, in FL, usually not all workers participate in a round of training. To this end, we designed <em id="S3.p1.1.7" class="ltx_emph ltx_font_italic">FedLess</em> from the ground up, keeping FL in mind. Second, none of the tools mentioned above are vendor-agnostic and support only one FaaS platform, i.e., AWS Lambda. In contrast, <em id="S3.p1.1.8" class="ltx_emph ltx_font_italic">FedLess</em> supports clients distributed across all major commercial and open-source FaaS platforms. Third, none of the tools consider security aspects for function invocations and model training. On the other hand, <em id="S3.p1.1.9" class="ltx_emph ltx_font_italic">FedLess</em> supports cloud-agnostic function authentication/authorization. Finally, <em id="S3.p1.1.10" class="ltx_emph ltx_font_italic">FedLess</em> is built on top of Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which allows easy integration and usage for developers. In contrast, <em id="S3.p1.1.11" class="ltx_emph ltx_font_italic">Siren</em> features a runtime based on a pruned version of MXNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, while <em id="S3.p1.1.12" class="ltx_emph ltx_font_italic">Cirrus</em> does not support any DL framework.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Federated Learning Frameworks</span>.
There exists several open-source frameworks for FL such as <em id="S3.p2.1.2" class="ltx_emph ltx_font_italic">PySyft</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">Tensorflow Federated</em> (TFF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">Flower</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <em id="S3.p2.1.5" class="ltx_emph ltx_font_italic">FedML</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, <em id="S3.p2.1.6" class="ltx_emph ltx_font_italic">PaddleFL</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and <em id="S3.p2.1.7" class="ltx_emph ltx_font_italic">Fate</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Although many of these frameworks provide implementations for different FL algorithms and privacy mechanisms by default, none of them have been developed to be used in a serverless context. Moreover, they require all participating clients to rely on specific communication protocols such as <span id="S3.p2.1.8" class="ltx_text ltx_font_typewriter">gRPC</span>. As a result, for maximum flexibility and to optimize our implementation for the serverless use-case, we do not use them.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Serverless Federated Learning</span>. To the best of our knowledge, we are the only ones to have tried FaaS for FL. In our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we presented <em id="S3.p3.1.2" class="ltx_emph ltx_font_italic">FedKeeper</em>, a tool to facilitate FL of ML models across a fabric of heterogeneous FaaS platforms. However, <em id="S3.p3.1.3" class="ltx_emph ltx_font_italic">FedKeeper</em> lacks crucial features that are required by FL systems in practice. For instance, it only supports relatively small models, does not provide support for security or a privacy protection mechanism, and is not optimized for performance. In this paper, we present a new system, <em id="S3.p3.1.4" class="ltx_emph ltx_font_italic">FedLess</em> that addresses the drawbacks present in <em id="S3.p3.1.5" class="ltx_emph ltx_font_italic">FedKeeper</em>. Moreover, <em id="S3.p3.1.6" class="ltx_emph ltx_font_italic">FedLess</em> is designed to be modular and can be easily extended to support additional data sources, models, and optimization algorithms.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Fedless</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> is a system and framework to perform FL on a heterogeneous fabric of FaaS platforms. In this section, we first describe the overall architecture of <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">FedLess</em>. Following this, we present an overview of its implementation, describe its security features, the mechanism for privacy-preserving training of clients, and performance optimizations that improve scalability and reliability in detail. Finally, we describe the workflow for training clients using <em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">FedLess</em>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2111.03396/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">FedLess System Architecture.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">System Design</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives an overview of the different components of <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>. In the serverless context, we refer to an FL <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">client</em> as the associated <em id="S4.SS1.p1.1.3" class="ltx_emph ltx_font_italic">client function</em>, i.e., the function deployed on a FaaS platform that calculates the local model updates. A <em id="S4.SS1.p1.1.4" class="ltx_emph ltx_font_italic">client administrator</em> is the person who is responsible for managing and deploying the function and is usually also the data holder. The FL training process and the registration of clients are managed by the FL <em id="S4.SS1.p1.1.5" class="ltx_emph ltx_font_italic">administrator</em> (FL admin).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">FedLess</em> supports both <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">cross-silo</em> and <em id="S4.SS1.p2.1.3" class="ltx_emph ltx_font_italic">cross-device</em> FL use-cases (§<a href="#S2.SS2" title="II-B Federated Learning (FL) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>). In the <em id="S4.SS1.p2.1.4" class="ltx_emph ltx_font_italic">cross-silo</em> setting, client functions would belong to a few institutions and either run in on-premise data centers with self-hosted FaaS platforms or on the cloud. In the latter, client functions would run on edge devices with lightweight FaaS platforms such as <em id="S4.SS1.p2.1.5" class="ltx_emph ltx_font_italic">faasd</em> (OpenFaaS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. All clients are externally deployed functions that are callable through an HTTP interface. By default, <em id="S4.SS1.p2.1.6" class="ltx_emph ltx_font_italic">FedLess</em> supports four commercial and two open-source FaaS platforms. We provide scripts for deploying client functions on these FaaS platforms. Apart from setting up optional parameters, such as request limits, no further action is required from the user.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The central component in <em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">FedLess</em> is the control plane which comprises of the <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Parameter Server</em>, the <em id="S4.SS1.p3.1.3" class="ltx_emph ltx_font_italic">Client Database</em>, an OpenWhisk cluster to run the <em id="S4.SS1.p3.1.4" class="ltx_emph ltx_font_italic">Aggregator Function</em>, and the <em id="S4.SS1.p3.1.5" class="ltx_emph ltx_font_italic">FedLess Controller</em>. The controller comprises of a <em id="S4.SS1.p3.1.6" class="ltx_emph ltx_font_italic">Client Registry</em>, <em id="S4.SS1.p3.1.7" class="ltx_emph ltx_font_italic">Client Invoker</em>, and the <em id="S4.SS1.p3.1.8" class="ltx_emph ltx_font_italic">FL Strategy</em>. The default FL strategy implemented in our system is the <span id="S4.SS1.p3.1.9" class="ltx_text ltx_font_typewriter">FedAvg</span> algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Due to its ease-of-use, reliability, replication support, and security features, we chose MongoDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> as our parameter server. Moreover, we make use of MongoDB’s <em id="S4.SS1.p3.1.10" class="ltx_emph ltx_font_italic">GridFS</em> specification to support models larger than the document size limit, i.e., <span id="S4.SS1.p3.1.11" class="ltx_text ltx_font_typewriter">16MB</span>. The global model architecture and its hyperparameters are also stored in the parameter server. The <em id="S4.SS1.p3.1.12" class="ltx_emph ltx_font_italic">Client Database</em> is also a MongoDB instance that serves as a persistent registry for the client functions, their hyperparameters, and dataset locations, along with any additional information supplied by the client administrator during sign-up. The <em id="S4.SS1.p3.1.13" class="ltx_emph ltx_font_italic">Controller</em> is a lightweight process that manages and monitors the entire training cycle. It is responsible for selecting the clients that participate in each round, invoking them and the aggregator. The client registry component in the controller interacts with the client database for managing the clients. The aggregator is a FaaS function that is invoked by the controller after all clients that participated in a training round have finished. It aggregates the updated client model parameters according to the FL strategy.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">New FL strategies other than <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> can easily be implemented by modifying how the controller samples clients for each round and writing a custom aggregator function, taking the one provided as a blueprint. Having the parameter server physically close to the aggregation process is a crucial advantage of our approach due to minimized networking overhead, as is also presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Although we deploy the aggregator function using OpenWhisk as shown in Fig <a href="#S4.F3" title="Figure 3 ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it can be easily deployed using another FaaS platform. The <em id="S4.SS1.p4.1.2" class="ltx_emph ltx_font_italic">Auth Server</em> is an external entity responsible for authentication and is described in detail in §<a href="#S4.SS3.SSS2" title="IV-C2 Authentication and Authorization ‣ IV-C Security ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span>2</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementation Overview</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> framework was implemented using Python3 and also includes a command line tool to orchestrate the entire FL training process. It relies on Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and Keras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to support arbitrary DNN models. Our framework provides:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Low-level, easily exchangeable implementations for serialization methods, helper functions for tasks related to security and general optimizations, as well as classes for easy database access and interaction with the parameter server.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Functions and decorators to abstract away differences of supported FaaS platforms, in turn, unifying the implementation of client functions.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Extensive example implementations of client and aggregation functions that serve as easily modifiable blueprints.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Custom data types and models using the <span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">Pydantic</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> library, which ensures that all clients and the FL server rely on the same data formats and schemata.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Security</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Since the clients in FL belong to separate institutions and networks and have to expose their functionality to the public internet to be accessible by the FL server, it is significantly important that only authenticated and authorized entities can invoke client functions. The security features in <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> are described in the following four aspects.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Function Ownership</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In <em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>, all participating institutions and users that manage one or multiple clients are responsible for deploying the client functions themselves. This is because data holders have complete control over what happens with their data. Moreover, this approach provides greater flexibility for institutions to adjust their complete end-to-end local training workflows. In addition, full oversight on security and access control is essential, especially for domains such as healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> with special legal regulations for the data. Clients deployed on different FaaS platforms can directly use the authentication system provided by <em id="S4.SS3.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">FedLess</em> (§<a href="#S4.SS3.SSS2" title="IV-C2 Authentication and Authorization ‣ IV-C Security ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span>2</span></a>). However, since data holders are free to use different FaaS platforms, they can require additional, platform-specific authentication secrets. A data holder could, for example, deploy a client function with OpenWhisk and specify an API token that has to be supplied by <em id="S4.SS3.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">FedLess</em> with every request. Our system supports additional API tokens in client invocation requests. We provide several examples for writing client functions using our framework for multiple FaaS platforms.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Authentication and Authorization</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In a centralized FL system, like <em id="S4.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>, multiple security interests come together. For instance, data holders require that only the central FL server they integrate with can call their client functions. This means that the central server has to authenticate itself and possess proper authorization. At the same time, before being added to the list of valid client functions for an FL training session, data holders have to identify themselves to the FL server and provide appropriate proof. As a result, we need to support authentication, authorization, and integration with external identity providers through protocols like <em id="S4.SS3.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">OAuth 2.0</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and <em id="S4.SS3.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">SAML 2.0</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Integration with external identity providers enables easy and direct support for identity providers already used by academic institutions and enterprises. Towards this, we use <em id="S4.SS3.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">AWS Cognito</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. It is a service that provides all the required features, has been used in large-scale production systems, and has a convenient pricing structure that scales with the size of the system. Furthermore, it does not restrict <em id="S4.SS3.SSS2.p1.1.5" class="ltx_emph ltx_font_italic">FedLess</em> to a specific cloud provider’s ecosystem since the interaction with Cognito is the same for all the clients.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">The authentication and authorization system is based on <em id="S4.SS3.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">JSON Web Tokens</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> (JWTs) attached to all HTTP requests between the FL server and the clients. Client administrators manually register through a web interface with a Cognito User Pool, offering sign-up via external identity providers. Once they submit their registration request, a central FL server administrator checks the validity of the registration information and confirms it. After the registration process, all necessary information for the client function to check the authenticity and permissions of requests by the FL server is then communicated to the client administrator via a web interface. This includes the public key of the key-pair used to sign the authentication headers, the FL server’s client-id, and required authorization scopes. At the start of each training round, the FL server uses its credentials to fetch a token from Cognito with sufficient privileges to invoke all client functions. For each new request, all clients first check if the token supplied with the request by the FL server was signed by the trusted Cognito User Pool and has the proper authorization.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.5.1.1" class="ltx_text">IV-C</span>3 </span>Parameter Server Access</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">The different client functions need to access the parameter server without compromising security and privacy. For instance, one client function should not be able to read the result of another client or modify the global parameters. To this end, the FL server creates temporary users in the parameter server for each client function that can only read the shared global model and write back results without access to other clients. In every request to a client, the FL server attaches the credentials with which the client can retrieve the global model and write back its results to the parameter server. Furthermore, because the credentials given to the client function belong to a custom user, the access to the parameter server could be easily revoked for individual client functions. Our parameter server, i.e., MongoDB (§<a href="#S4.SS1" title="IV-A System Design ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>) allows the definition of such custom users and role policies.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS4.5.1.1" class="ltx_text">IV-C</span>4 </span>General Security Features</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">The Open Web Application Security Project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> represents the top ten issues that are widely accepted to be most common and critical in software applications. While most security concerns depend on the system and function implementation details and lie in the hands of the data holders, we counter some of the most common vulnerabilities with our system design and implementation in <em id="S4.SS3.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>. First, since all function requests use HTTP, we can easily enforce all communication to use Transport Layer Security (TLS) to encrypt the exchanged data. Second, to protect against insecure serialization and injection attacks, we advise data holders to use the model schemata provided in our framework implemented using the <span id="S4.SS3.SSS4.p1.1.2" class="ltx_text ltx_font_typewriter">Pydantic</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> library. Third, we provide examples and demonstrate how to isolate all authentication and authorization by using separate functions as suggested by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Privacy</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The two main concerns in an FL system in which not all participants can be trusted are <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">poisoning</em> and <em id="S4.SS4.p1.1.2" class="ltx_emph ltx_font_italic">inference</em> attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. As a result, several strategies to protect the privacy of clients for FL have been proposed. These include <em id="S4.SS4.p1.1.3" class="ltx_emph ltx_font_italic">Secure Multiparty Computation</em> (SMC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, <em id="S4.SS4.p1.1.4" class="ltx_emph ltx_font_italic">Homomorphic Encryption</em> (HE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, and <em id="S4.SS4.p1.1.5" class="ltx_emph ltx_font_italic">Differential Privacy</em> (DP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. While SMC and HE involve encrypting the client updates, DP involves adding Gaussian noise to them. Due to its reliable privacy guarantees at the cost of degraded prediction accuracy, it is a common practice to combine DP with SMC or HE methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. This is because due to encryption, less noise would have to be added to the client updates. However, a drawback of these encryption-based approaches is that they require a large amount of memory and have significant computational and networking costs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. This makes them unsuitable for most FL applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. For FaaS, using these methods leads to function timeouts due to fixed limits (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>). As a result, we do not employ secure aggregation in our system. By default, <em id="S4.SS4.p1.1.6" class="ltx_emph ltx_font_italic">FedLess</em> supports <em id="S4.SS4.p1.1.7" class="ltx_emph ltx_font_italic">Local Differential Privacy</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> (LDP). LDP is a form of <em id="S4.SS4.p1.1.8" class="ltx_emph ltx_font_italic">record-level</em> privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> in which the client functions add noise to their parameters before uploading them to the parameter server.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We use the <em id="S4.SS4.p2.1.1" class="ltx_emph ltx_font_italic">Tensorflow Privacy</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> library that provides DP versions of popular optimization algorithms that are also compatible with Keras. A minor caveat of using LDP with ephemeral serverless functions is keeping track of past invocations to a client function through an external storage system. Subsequent invocations of the client function degrade the privacy guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. As a result, data holders would want to prohibit requests by the FL server after a specific privacy budget has been exhausted. For this, they would need to store their state between requests. All of this, combined with our reference implementation of using DP optimizers with client functions, makes it easy for data holders to slightly modify their client function code to benefit from DP and provide proven privacy guarantees for their user data. Investigating the use of other secure aggregation methods such as <em id="S4.SS4.p2.1.2" class="ltx_emph ltx_font_italic">Additive Masking</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> which require complex interaction between clients, is of our interest in the future.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Performance Optimizations</span>
</h3>

<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS5.SSS1.5.1.1" class="ltx_text">IV-E</span>1 </span>Global namespace caching</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">Due to the ephemeral, stateless nature of FaaS functions, they need to load and preprocess the dataset, compile and instantiate the model on every invocation. This overhead becomes significantly large for large local datasets and models. To mitigate this, we exploit that all cloud providers and FaaS frameworks retain global variables in the function instances between invocations. Towards this, we implement a custom Python caching decorator that, after the first function invocation, stores the output of the Python function in an in-memory LRU cache. All subsequent calls to the function return the cached result. Note that the cached result is only available until the duration of the function instance (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>).</p>
</div>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS5.SSS2.5.1.1" class="ltx_text">IV-E</span>2 </span>Running average model aggregation</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">In a naive implementation of <span id="S4.SS5.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>, the aggregator function has to load all model updates in memory for the aggregation. However, due to the limitations on the amount of memory configurable for FaaS functions (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>) on most commercial FaaS providers, this approach is not feasible. To this end, we implement a running average mode in the aggregator function. This is done by loading the client updates from the parameter server only in small batches. After a batch is loaded, we use it to update the current average global model parameters and free up the memory occupied by the processed batch.</p>
</div>
</section>
<section id="S4.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS5.SSS3.5.1.1" class="ltx_text">IV-E</span>3 </span>Federated Evaluation</h4>

<div id="S4.SS5.SSS3.p1" class="ltx_para">
<p id="S4.SS5.SSS3.p1.1" class="ltx_p">The aggregator function can evaluate the updated global model after each round if a global test exists. However, in FL, since this is rare, we also support client-side evaluation as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Each client can be invoked independently to run the global model on a local test set and return the evaluation metrics. The metrics can then be aggregated by the controller. Moreover, independent selection of clients for training and evaluation between rounds minimizes function cold starts.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2111.03396/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">FedLess workflow for a complete FL round.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Putting It All Together</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.11" class="ltx_p">The workflow for training multiple clients using <em id="S4.SS6.p1.11.1" class="ltx_emph ltx_font_italic">FedLess</em> in a single FL round is presented in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-E3 Federated Evaluation ‣ IV-E Performance Optimizations ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. First, the FL admin (§<a href="#S4.SS1" title="IV-A System Design ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>) selects the model to be trained, the registered client functions that will participate in the training, and hyperparameters such as the number of clients per round ( <svg id="S4.SS6.p1.1.pic1" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>). Following this, the training is started by the FL admin. The <em id="S4.SS6.p1.11.2" class="ltx_emph ltx_font_italic">FedLess</em> controller first requests a new invocation token from the Auth Server using the credentials configured by the FL admin ( <svg id="S4.SS6.p1.2.pic2" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.2.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>). Using this token along with the credentials to access the parameter server, the controller invokes the clients it selected for this round ( <svg id="S4.SS6.p1.3.pic3" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.3.pic3.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>). The clients involved in an FL training round are selected randomly by the controller. To ensure that the invocation is valid, the clients validate the signature and authorization of the token using the public key from the Auth Server ( <svg id="S4.SS6.p1.4.pic4" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.4.pic4.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>). The clients then use the supplied parameter server credentials to load the latest global model ( <svg id="S4.SS6.p1.5.pic5" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.5.pic5.1.1.1.1.1" class="ltx_text">5</span></foreignObject></g></g></svg>). Following this, the clients load their local datasets and perform the local training, optionally using LDP ( <svg id="S4.SS6.p1.6.pic6" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.6.pic6.1.1.1.1.1" class="ltx_text">6</span></foreignObject></g></g></svg>). Once the clients have finished training, they again use their credentials to upload their parameters to the parameter server ( <svg id="S4.SS6.p1.7.pic7" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.7.pic7.1.1.1.1.1" class="ltx_text">7</span></foreignObject></g></g></svg>). The <em id="S4.SS6.p1.11.3" class="ltx_emph ltx_font_italic">FedLess</em> controller waits until all clients are either finished, reached a configured timeout, or failed. It then starts the model aggregation by invoking the aggregator function ( <svg id="S4.SS6.p1.8.pic8" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.8.pic8.1.1.1.1.1" class="ltx_text">8</span></foreignObject></g></g></svg>). The aggregator loads the client results from the parameter server, aggregates the parameters, and stores the new global model ( <svg id="S4.SS6.p1.9.pic9" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.9.pic9.1.1.1.1.1" class="ltx_text">9</span></foreignObject></g></g></svg>). Finally, the controller starts the evaluation ( <svg id="S4.SS6.p1.10.pic10" class="ltx_picture" height="19.86" overflow="visible" version="1.1" width="19.86"><g transform="translate(0,19.86) matrix(1 0 0 -1 0 0) translate(9.93,0) translate(0,9.93)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 9.65 0 C 9.65 5.33 5.33 9.65 0 9.65 C -5.33 9.65 -9.65 5.33 -9.65 0 C -9.65 -5.33 -5.33 -9.65 0 -9.65 C 5.33 -9.65 9.65 -5.33 9.65 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -6.92 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.10.pic10.1.1.1.1.1" class="ltx_text">10</span></foreignObject></g></g></svg>). If a global test set exists, then the evaluation was already done in the aggregator. If not, a new selection of clients is invoked for evaluation in which they load the updated model and evaluate on their test set (§<a href="#S4.SS5.SSS3" title="IV-E3 Federated Evaluation ‣ IV-E Performance Optimizations ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span>3</span></a>). The controller aggregates the returned metrics and resumes the training process from ( <svg id="S4.SS6.p1.11.pic11" class="ltx_picture" height="14.77" overflow="visible" version="1.1" width="14.77"><g transform="translate(0,14.77) matrix(1 0 0 -1 0 0) translate(7.38,0) translate(0,7.38)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.11 0 C 7.11 3.92 3.92 7.11 0 7.11 C -3.92 7.11 -7.11 3.92 -7.11 0 C -7.11 -3.92 -3.92 -7.11 0 -7.11 C 3.92 -7.11 7.11 -3.92 7.11 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.SS6.p1.11.pic11.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>) if a configured accuracy threshold is not reached.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we describe the datasets and DNN model architectures we used for evaluating <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>. Furthermore, we describe the distribution and configuration of client functions across the different FaaS platforms.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">MNIST</span>: The MNIST Handwritten Image Database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> comprises 70,000 images of handwritten digits with ten classes corresponding to the respective digits. We use all 10,000 test images of the dataset to evaluate the global model after each training round centrally. To simulate a non-IID setting as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
we sort the 60,000 images in the training set by label, split them into 200 shards of 300 images each, and distribute the shards to the clients. This dataset represents an image classification task.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We also perform experiments on more realistic datasets from the <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">LEAF</em> benchmarking framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. The datasets chosen are described below and are by nature non-IID. For both datasets, we use the same preprocessing steps as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Shakespeare</span>: This dataset contains sentences from the <em id="S5.SS1.p3.1.2" class="ltx_emph ltx_font_italic">The Complete Works of William Shakespeare</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, partitioned by the speaker and play. In total, it contains 4,226,158 utterances of length 80 by 1,129 different <em id="S5.SS1.p3.1.3" class="ltx_emph ltx_font_italic">users</em>, with on-average <math id="S5.SS1.p3.1.m1.2" class="ltx_Math" alttext="\sim 3,700" display="inline"><semantics id="S5.SS1.p3.1.m1.2a"><mrow id="S5.SS1.p3.1.m1.2.3" xref="S5.SS1.p3.1.m1.2.3.cmml"><mi id="S5.SS1.p3.1.m1.2.3.2" xref="S5.SS1.p3.1.m1.2.3.2.cmml"></mi><mo id="S5.SS1.p3.1.m1.2.3.1" xref="S5.SS1.p3.1.m1.2.3.1.cmml">∼</mo><mrow id="S5.SS1.p3.1.m1.2.3.3.2" xref="S5.SS1.p3.1.m1.2.3.3.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">3</mn><mo id="S5.SS1.p3.1.m1.2.3.3.2.1" xref="S5.SS1.p3.1.m1.2.3.3.1.cmml">,</mo><mn id="S5.SS1.p3.1.m1.2.2" xref="S5.SS1.p3.1.m1.2.2.cmml">700</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.2b"><apply id="S5.SS1.p3.1.m1.2.3.cmml" xref="S5.SS1.p3.1.m1.2.3"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.2.3.1.cmml" xref="S5.SS1.p3.1.m1.2.3.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS1.p3.1.m1.2.3.2.cmml" xref="S5.SS1.p3.1.m1.2.3.2">absent</csymbol><list id="S5.SS1.p3.1.m1.2.3.3.1.cmml" xref="S5.SS1.p3.1.m1.2.3.3.2"><cn type="integer" id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">3</cn><cn type="integer" id="S5.SS1.p3.1.m1.2.2.cmml" xref="S5.SS1.p3.1.m1.2.2">700</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.2c">\sim 3,700</annotation></semantics></math> for each. The task of this dataset is to predict the next character in a sentence given the previous 80.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">FEMNIST</span>: It is a modification of the
EMNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> dataset and contains over 800,000 images of digits and characters partitioned by the writer. In total, it contains contributions from 3,550 writers with <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="\sim 226" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mi id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml"></mi><mo id="S5.SS1.p4.1.m1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.cmml">∼</mo><mn id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml">226</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3">226</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\sim 226</annotation></semantics></math> images from each on average. Similar to MNIST, the task of this dataset is to classify images.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">In our experiments, we serve all datasets using an <em id="S5.SS1.p5.1.1" class="ltx_emph ltx_font_italic">nginx</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> store with each dataset partition being available at a separate URL and assign it to one client.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Model Architectures and Hyperparameters</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">MNIST</span>: We use the same CNN as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, consisting of 2 convolutional layers with kernels of size 5x5, a fully connected layer with 512 neurons, and the output layer with ten neurons. The network uses ReLU as the sole activation function and the categorical-cross-entropy loss function.
In total, the model has 582,026 trainable parameters, which amount to 2.3 MB of memory when serialized. We use a batch size of 10 as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> with five local epochs for each client. Because we experienced faster convergence with momentum-based optimization methods, we used Adam as the optimizer.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Shakespeare</span>: Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, we use an <em id="S5.SS2.p2.1.2" class="ltx_emph ltx_font_italic">LSTM</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. First, the characters in sequences of length 80 are each embedded in an eight-dimensional space, followed by two LSTM layers, each comprising 256 units. In the end, the output layer has the same size as the vocabulary and a softmax activation function. Like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, we optimize the model using standard Stochastic Gradient Descent with a learning rate of <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">0.8</span> and use the categorical cross-entropy as the loss function. The LSTM has 818,402 trainable parameters, which take <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\sim 3" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">∼</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\sim 3</annotation></semantics></math>MB in serialized form. Due to long training times of <em id="S5.SS2.p2.1.4" class="ltx_emph ltx_font_italic">LSTMS</em>, and limitations with function timeouts in current FaaS providers (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>), we use a batch size of 32 with one local epoch.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">FEMNIST</span>: As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, we use a large CNN model with two convolutional layers, each followed by a max-pooling layer, followed by a fully-connected layer of size 2048 before resulting in the output layer of size 62. The final model size comes down to 6,603,710 trainable parameters, <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="\sim 26.4" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mi id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">∼</mo><mn id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">26.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">absent</csymbol><cn type="float" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">26.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\sim 26.4</annotation></semantics></math>MB when serialized. Similar to MNIST, we use a batch size of 10 with five local epochs and Adam as the optimizer. For both MNIST and FEMNIST, we use the default values for the learning rate.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">To calculate a global test accuracy and loss for FEMNIST and Shakespeare, we use the reported test set evaluation metrics from the clients and calculate a weighted average based on the test set cardinalities from the clients (§<a href="#S4.SS5.SSS3" title="IV-E3 Federated Evaluation ‣ IV-E Performance Optimizations ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span>3</span></a>,§<a href="#S4.SS6" title="IV-F Putting It All Together ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Serverless client functions configuration and distribution</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To host our parameter server, we used a virtual machine (VM) with 40vCPUs and 177GB RAM on the LRZ compute cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. On the same VM, we ran an OpenWhisk cluster on top of Kubernetes. All client functions are configured with a memory limit of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">2048</span>MB, and functions using public cloud providers are deployed in the same region, i.e., <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">Frankfurt</span>, Germany. Only the memory limit for functions on Azure could not be directly configured since it uses dynamic memory allocation with a maximum memory value of 1.5GB per function. We set the memory limit for the aggregation function of <em id="S5.SS3.p1.1.3" class="ltx_emph ltx_font_italic">FedLess</em> to <span id="S5.SS3.p1.1.4" class="ltx_text ltx_font_typewriter">4096</span>MB (§<a href="#S4.SS1" title="IV-A System Design ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">For all the three datasets (§<a href="#S5.SS1" title="V-A Datasets ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>) we ran experiments with 200 clients in total and varied the number of clients sampled in each training round. For MNIST and FEMNIST, we deployed 170 Google Cloud Functions (GCF), 10 AWS Lambda functions, 10 IBM Cloud Functions, 5 OpenWhisk functions on an on-premise cluster, and 5 Azure Functions. Training LSTMs using a large number of simultaneous client functions up to convergence is out of our cloud budget. As a result, for the Shakespeare experiments, we only use up to 25 simultaneous clients.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We repeat all our experiments five times and follow best practices while presenting our results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2111.03396/assets/x4.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S6.F5.3.2" class="ltx_text" style="font-size:90%;">
Decreased memory footprint of our aggregator implementation.
Results are generated in a simulation with 200 client updates of the FEMNIST model.
</span></figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Analyzing FedLess Performance</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">As described in §<a href="#S4.SS5.SSS2" title="IV-E2 Running average model aggregation ‣ IV-E Performance Optimizations ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span>2</span></a>, due to the limitations on the amount of memory configurable for a FaaS function, using a standard FaaS function for the aggregation would not be possible for large-scale scenarios. Figure <a href="#S6.F5" title="Figure 5 ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the behaviour of our modified <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> implementation in a simulated setting. Aggregating the parameters of 200 clients for the FEMNIST CNN (§<a href="#S5.SS2" title="V-B Model Architectures and Hyperparameters ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>) would require more than 6GB of memory for the vanilla <span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> implementation. In contrast, using batches of 20 updates, our batched running average calculation requires slightly more than 2GB of memory. Note that the overhead created by our implemented security features (§<a href="#S4.SS3.SSS4" title="IV-C4 General Security Features ‣ IV-C Security ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span>4</span></a>) is minimal. The actual token validation and checking takes only a few milliseconds in our client functions. Moreover, after a client fetches the token from the Cognito server, we cache it in the function instance’s memory, requiring no further requests (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>).</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2111.03396/assets/x5.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S6.F6.3.2" class="ltx_text" style="font-size:90%;">
Convergence speed of FedLess for different datasets and numbers of clients per round.
Each dataset is shown in a separate column.
</span></figcaption>
</figure>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2111.03396/assets/x6.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S6.F7.3.2" class="ltx_text" style="font-size:90%;">FedLess test accuracies over time for FEMNIST and Shakespeare.</span></figcaption>
</figure>
<figure id="S6.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.03396/assets/x7.png" id="S6.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="462" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">MNIST.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.03396/assets/x8.png" id="S6.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="462" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">Shakespeare.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.03396/assets/x9.png" id="S6.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="460" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S6.F8.sf3.3.2" class="ltx_text" style="font-size:90%;">FEMNIST.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.4.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S6.F8.5.2" class="ltx_text" style="font-size:90%;">Mean required time for different steps in FedLess for the different datasets. <em id="S6.F8.5.2.1" class="ltx_emph ltx_font_italic">Straggler</em> shows the runtime of the slowest client. If present, <em id="S6.F8.5.2.2" class="ltx_emph ltx_font_italic">Agg. + Eval.</em> shows the runtime of the aggregator function that also performs evaluation (§<a href="#S4.SS6" title="IV-F Putting It All Together ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>).</span></figcaption>
</figure>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The number of training rounds and the overall time taken by <em id="S6.SS1.p2.1.1" class="ltx_emph ltx_font_italic">FedLess</em> to reach a specific target accuracy for the three datasets is shown in Figure <a href="#S6.F6" title="Figure 6 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Results are shown for different numbers of clients involved in each round. For MNIST, we observe that increasing the number of clients involved in each round did not significantly influence the overall number of required rounds. Using a higher number of clients per round did speed up training in terms of required rounds and time for reaching accuracies less than <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><cn type="integer" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">99</annotation></semantics></math>%. However, for higher accuracies, the difference is not that evident. Similarly, for Shakespeare, we observe that more clients per round lead to faster convergence only with respect to communication rounds. This behavior is also visible from Figure <a href="#S6.F7" title="Figure 7 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and can be attributed to the remarkable variance in the execution time of each client in this dataset due to the non-IID distribution (§<a href="#S2.SS2" title="II-B Federated Learning (FL) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a>). As a result, using more clients per round increases the chance of waiting longer for stragglers, increasing the overall time until a certain accuracy is reached. For the FEMNIST dataset, using more clients per round leads to slower convergence in both dimensions, i.e., communication rounds and time to reach the target accuracy. This can be attributed to the large variance in the average reported test set accuracy as shown in Figure <a href="#S6.F7" title="Figure 7 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Using fewer clients in each round for evaluation leads to a larger variance in the average evaluation metrics.
The overall training behavior for different numbers of clients is approximately the same, showing no significant difference in using more clients per round for our chosen hyperparameters.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2111.03396/assets/x10.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="253" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S6.F9.3.2" class="ltx_text" style="font-size:90%;">
Time distributions for FedLess client functions.
Numbers are based on one client function and multiple training rounds for all three datasets.
</span></figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Figure <a href="#S6.F8" title="Figure 8 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows how long the different actors in our system take over the course of the training for the different datasets. Across all experiments, the total round time is mainly determined by the slowest client of each round, the <em id="S6.SS1.p3.1.1" class="ltx_emph ltx_font_italic">straggler</em>. In each training round, vanilla <span id="S6.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> waits for each client to finish or reach a timeout. As a result, only one straggler can drastically increase the total round time. For MNIST, increasing the number of clients from 100 to 200 per round does not impact their execution time, as shown in Figure <a href="#S6.F8.sf1" title="In Figure 8 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>. The aggregator function becomes slightly slower since it has to load and aggregate results of more clients, slightly increasing the overall round time. Based on these findings, we see that for small local datasets and small model sizes, <em id="S6.SS1.p3.1.3" class="ltx_emph ltx_font_italic">FedLess</em> scales relatively easily. A crucial benefit of our system is that longer round times due to stragglers have almost zero influence on the overall costs since client functions are only billed for their actual runtime and not the time they wait for a new round. Similarly, for the Shakespeare dataset, increasing the number of clients from 10 to 25 did not impact the execution time of the clients, as shown in Figure <a href="#S6.F8.sf2" title="In Figure 8 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>. However, sampling more clients increases the chance of including clients that take a long time to finish. Thus, the overall round time increases. Note that the time required for aggregation or client-side evaluation is negligible for this dataset compared to the time it requires to wait for stragglers. Using an FL strategy that accounts for stragglers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> is of our interest in the future but out of scope for this work.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">For the FEMNIST dataset with the large CNN model (§<a href="#S5.SS2" title="V-B Model Architectures and Hyperparameters ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>), increasing the number of clients per round from 100 to 200 almost doubles the total time per round. Unlike the other two datasets, we observe an increase in the execution time of both the aggregator and the clients. The increase in the aggregator’s runtime can be attributed to the networking overhead of loading 200 <math id="S6.SS1.p4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.1.m1.1a"><mo id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><times id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">\times</annotation></semantics></math> 26MB of parameters and calculating the aggregated model using a running average. The reason for the increase in the mean execution time of the clients can be seen from Figure <a href="#S6.F9" title="Figure 9 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. While the clients training on Shakespeare and MNIST spend most of their time on actual model training and very little on loading or writing parameters from and to the parameter server, FEMNIST clients spend a significant portion of their execution time writing back their results to the parameter server.
Since the parameter download time is much less than the upload time and shows little variability, we can confidently say that the overall networking capabilities of the clients are not the problem. Therefore, the ability of MongoDB’s GridFS specification to handle multiple simultaneous uploads of large models is the root cause for the increase in the mean execution time of clients. Investigating the use of another high-performance parameter server with role-based access control rules (§<a href="#S4.SS3.SSS3" title="IV-C3 Parameter Server Access ‣ IV-C Security ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span>3</span></a>) is part of future work. For all datasets, the time for initial rounds is high due to cold starts associated with FaaS functions (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>).</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2111.03396/assets/x11.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S6.F10.3.2" class="ltx_text" style="font-size:90%;">
Test accuracies and increased round durations for MNIST when using FedLess with LDP across 200 client functions with 25 clients per round.
</span></figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.5.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.6.2" class="ltx_text ltx_font_italic">Local Differential Privacy</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">To demonstrate that it is feasible to use LDP with our system, we train a model on MNIST with 200 client functions (§<a href="#S5.SS2" title="V-B Model Architectures and Hyperparameters ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>, §<a href="#S5.SS3" title="V-C Serverless client functions configuration and distribution ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>). We use 25 clients per round, five local epochs, and a batch size of five. After a grid search for the hyperparameters specific to the Adam-DP optimizer in the TF-Privacy library, we use a noise multiplier and L2-clip norm of 1.0, and ten microbatches. The results with LDP as compared to a default MNIST run are shown in Figure <a href="#S6.F10" title="Figure 10 ‣ VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Due to budget constraints, we do not train the LDP model until convergence but demonstrate that it is possible to use LDP with our system and different supported cloud providers. The significant decrease in test set accuracy reached by our model using LDP in 100 rounds as compared to without can be attributed to the addition of Gaussian noise to the model parameters. However, the same approach might not work for all model sizes directly due to the slower training using DP optimizers. Due to the function execution time limits (§<a href="#S2.SS1" title="II-A Function-as-a-Service (FaaS) ‣ II Background ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>) on most commercial FaaS providers, training larger models such as LSTMs with LDP will often lead to a function timeout. One could mitigate this problem by only training on a certain number of batches instead of whole epochs or using a smaller microbatch size.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T1.6.7.1" class="ltx_tr">
<th id="S6.T1.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding:1.25pt 10.0pt;"></th>
<th id="S6.T1.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.25pt 10.0pt;" colspan="2">FL Round Duration (s)</th>
</tr>
<tr id="S6.T1.6.8.2" class="ltx_tr">
<th id="S6.T1.6.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding:1.25pt 10.0pt;">Clients</th>
<th id="S6.T1.6.8.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:1.25pt 10.0pt;">FedKeeper</th>
<th id="S6.T1.6.8.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:1.25pt 10.0pt;">FedLess</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T1.2.2" class="ltx_tr">
<th id="S6.T1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.25pt 10.0pt;">25</th>
<td id="S6.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.25pt 10.0pt;"><math id="S6.T1.1.1.1.m1.1" class="ltx_Math" alttext="18.4\pm 9.6" display="inline"><semantics id="S6.T1.1.1.1.m1.1a"><mrow id="S6.T1.1.1.1.m1.1.1" xref="S6.T1.1.1.1.m1.1.1.cmml"><mn id="S6.T1.1.1.1.m1.1.1.2" xref="S6.T1.1.1.1.m1.1.1.2.cmml">18.4</mn><mo id="S6.T1.1.1.1.m1.1.1.1" xref="S6.T1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S6.T1.1.1.1.m1.1.1.3" xref="S6.T1.1.1.1.m1.1.1.3.cmml">9.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.1.1.1.m1.1b"><apply id="S6.T1.1.1.1.m1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S6.T1.1.1.1.m1.1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.1.1.1.m1.1.1.2.cmml" xref="S6.T1.1.1.1.m1.1.1.2">18.4</cn><cn type="float" id="S6.T1.1.1.1.m1.1.1.3.cmml" xref="S6.T1.1.1.1.m1.1.1.3">9.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.1.1.1.m1.1c">18.4\pm 9.6</annotation></semantics></math></td>
<td id="S6.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.25pt 10.0pt;"><math id="S6.T1.2.2.2.m1.1" class="ltx_Math" alttext="12.3\pm 4.6" display="inline"><semantics id="S6.T1.2.2.2.m1.1a"><mrow id="S6.T1.2.2.2.m1.1.1" xref="S6.T1.2.2.2.m1.1.1.cmml"><mn id="S6.T1.2.2.2.m1.1.1.2" xref="S6.T1.2.2.2.m1.1.1.2.cmml">12.3</mn><mo id="S6.T1.2.2.2.m1.1.1.1" xref="S6.T1.2.2.2.m1.1.1.1.cmml">±</mo><mn id="S6.T1.2.2.2.m1.1.1.3" xref="S6.T1.2.2.2.m1.1.1.3.cmml">4.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.2.2.2.m1.1b"><apply id="S6.T1.2.2.2.m1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1"><csymbol cd="latexml" id="S6.T1.2.2.2.m1.1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.2.2.2.m1.1.1.2.cmml" xref="S6.T1.2.2.2.m1.1.1.2">12.3</cn><cn type="float" id="S6.T1.2.2.2.m1.1.1.3.cmml" xref="S6.T1.2.2.2.m1.1.1.3">4.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.2.2.m1.1c">12.3\pm 4.6</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.4.4" class="ltx_tr">
<th id="S6.T1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.25pt 10.0pt;">50</th>
<td id="S6.T1.3.3.1" class="ltx_td ltx_align_left" style="padding:1.25pt 10.0pt;"><math id="S6.T1.3.3.1.m1.1" class="ltx_Math" alttext="17.9\pm 11.5" display="inline"><semantics id="S6.T1.3.3.1.m1.1a"><mrow id="S6.T1.3.3.1.m1.1.1" xref="S6.T1.3.3.1.m1.1.1.cmml"><mn id="S6.T1.3.3.1.m1.1.1.2" xref="S6.T1.3.3.1.m1.1.1.2.cmml">17.9</mn><mo id="S6.T1.3.3.1.m1.1.1.1" xref="S6.T1.3.3.1.m1.1.1.1.cmml">±</mo><mn id="S6.T1.3.3.1.m1.1.1.3" xref="S6.T1.3.3.1.m1.1.1.3.cmml">11.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.3.3.1.m1.1b"><apply id="S6.T1.3.3.1.m1.1.1.cmml" xref="S6.T1.3.3.1.m1.1.1"><csymbol cd="latexml" id="S6.T1.3.3.1.m1.1.1.1.cmml" xref="S6.T1.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.3.3.1.m1.1.1.2.cmml" xref="S6.T1.3.3.1.m1.1.1.2">17.9</cn><cn type="float" id="S6.T1.3.3.1.m1.1.1.3.cmml" xref="S6.T1.3.3.1.m1.1.1.3">11.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.3.1.m1.1c">17.9\pm 11.5</annotation></semantics></math></td>
<td id="S6.T1.4.4.2" class="ltx_td ltx_align_left" style="padding:1.25pt 10.0pt;"><math id="S6.T1.4.4.2.m1.1" class="ltx_Math" alttext="12.8\pm 4.2" display="inline"><semantics id="S6.T1.4.4.2.m1.1a"><mrow id="S6.T1.4.4.2.m1.1.1" xref="S6.T1.4.4.2.m1.1.1.cmml"><mn id="S6.T1.4.4.2.m1.1.1.2" xref="S6.T1.4.4.2.m1.1.1.2.cmml">12.8</mn><mo id="S6.T1.4.4.2.m1.1.1.1" xref="S6.T1.4.4.2.m1.1.1.1.cmml">±</mo><mn id="S6.T1.4.4.2.m1.1.1.3" xref="S6.T1.4.4.2.m1.1.1.3.cmml">4.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.4.4.2.m1.1b"><apply id="S6.T1.4.4.2.m1.1.1.cmml" xref="S6.T1.4.4.2.m1.1.1"><csymbol cd="latexml" id="S6.T1.4.4.2.m1.1.1.1.cmml" xref="S6.T1.4.4.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.4.4.2.m1.1.1.2.cmml" xref="S6.T1.4.4.2.m1.1.1.2">12.8</cn><cn type="float" id="S6.T1.4.4.2.m1.1.1.3.cmml" xref="S6.T1.4.4.2.m1.1.1.3">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.4.2.m1.1c">12.8\pm 4.2</annotation></semantics></math></td>
</tr>
<tr id="S6.T1.6.6" class="ltx_tr">
<th id="S6.T1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:1.25pt 10.0pt;">75</th>
<td id="S6.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.25pt 10.0pt;"><math id="S6.T1.5.5.1.m1.1" class="ltx_Math" alttext="19.0\pm 12.2" display="inline"><semantics id="S6.T1.5.5.1.m1.1a"><mrow id="S6.T1.5.5.1.m1.1.1" xref="S6.T1.5.5.1.m1.1.1.cmml"><mn id="S6.T1.5.5.1.m1.1.1.2" xref="S6.T1.5.5.1.m1.1.1.2.cmml">19.0</mn><mo id="S6.T1.5.5.1.m1.1.1.1" xref="S6.T1.5.5.1.m1.1.1.1.cmml">±</mo><mn id="S6.T1.5.5.1.m1.1.1.3" xref="S6.T1.5.5.1.m1.1.1.3.cmml">12.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.5.5.1.m1.1b"><apply id="S6.T1.5.5.1.m1.1.1.cmml" xref="S6.T1.5.5.1.m1.1.1"><csymbol cd="latexml" id="S6.T1.5.5.1.m1.1.1.1.cmml" xref="S6.T1.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.5.5.1.m1.1.1.2.cmml" xref="S6.T1.5.5.1.m1.1.1.2">19.0</cn><cn type="float" id="S6.T1.5.5.1.m1.1.1.3.cmml" xref="S6.T1.5.5.1.m1.1.1.3">12.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.5.5.1.m1.1c">19.0\pm 12.2</annotation></semantics></math></td>
<td id="S6.T1.6.6.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.25pt 10.0pt;"><math id="S6.T1.6.6.2.m1.1" class="ltx_Math" alttext="14.0\pm 3.8" display="inline"><semantics id="S6.T1.6.6.2.m1.1a"><mrow id="S6.T1.6.6.2.m1.1.1" xref="S6.T1.6.6.2.m1.1.1.cmml"><mn id="S6.T1.6.6.2.m1.1.1.2" xref="S6.T1.6.6.2.m1.1.1.2.cmml">14.0</mn><mo id="S6.T1.6.6.2.m1.1.1.1" xref="S6.T1.6.6.2.m1.1.1.1.cmml">±</mo><mn id="S6.T1.6.6.2.m1.1.1.3" xref="S6.T1.6.6.2.m1.1.1.3.cmml">3.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T1.6.6.2.m1.1b"><apply id="S6.T1.6.6.2.m1.1.1.cmml" xref="S6.T1.6.6.2.m1.1.1"><csymbol cd="latexml" id="S6.T1.6.6.2.m1.1.1.1.cmml" xref="S6.T1.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S6.T1.6.6.2.m1.1.1.2.cmml" xref="S6.T1.6.6.2.m1.1.1.2">14.0</cn><cn type="float" id="S6.T1.6.6.2.m1.1.1.3.cmml" xref="S6.T1.6.6.2.m1.1.1.3">3.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.6.6.2.m1.1c">14.0\pm 3.8</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T1.8.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S6.T1.9.2" class="ltx_text" style="font-size:90%;">FL Round durations of FedKeeper and FedLess for varying numbers of clients.
FedKeeper clients in these experiments already made use of our caching mechanism.
</span></figcaption>
</figure>
<figure id="S6.F11" class="ltx_figure"><img src="/html/2111.03396/assets/x12.png" id="S6.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S6.F11.3.2" class="ltx_text" style="font-size:90%;">
Client execution times for FedKeeper and FedLess on MNIST without cold-starts.
</span></figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.5.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.6.2" class="ltx_text ltx_font_italic">Comparison with Fedkeeper</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To see how well <em id="S6.SS3.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em> performs compared to <em id="S6.SS3.p1.1.2" class="ltx_emph ltx_font_italic">FedKeeper</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we re-implemented <em id="S6.SS3.p1.1.3" class="ltx_emph ltx_font_italic">FedKeeper</em> with our <em id="S6.SS3.p1.1.4" class="ltx_emph ltx_font_italic">FedLess</em> components.
Our implementation works almost identical to the original implementation. However, it uses the modular components we implemented for <em id="S6.SS3.p1.1.5" class="ltx_emph ltx_font_italic">FedLess</em>, and we introduced performance improvements like caching.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">We deploy 100 client functions with 2048MB memory for both systems on GCF.
We only use one cloud provider to minimize the impact of performance variations between FaaS platforms. We use an on-premise OpenWhisk cluster for the aggregation for both systems and invoker functions for <em id="S6.SS3.p2.1.1" class="ltx_emph ltx_font_italic">FedKeeper</em> (§<a href="#S5.SS3" title="V-C Serverless client functions configuration and distribution ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>). While in <em id="S6.SS3.p2.1.2" class="ltx_emph ltx_font_italic">FedLess</em> the FL client functions are directly invoked by the client invoker in the <em id="S6.SS3.p2.1.3" class="ltx_emph ltx_font_italic">FedLess</em> controller (§<a href="#S4.SS1" title="IV-A System Design ‣ IV Fedless ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>), <em id="S6.SS3.p2.1.4" class="ltx_emph ltx_font_italic">Fedkeeper</em> uses a separate component called invoker functions which are responsible for invoking the FL clients via an HTTP request. The invoker functions are FaaS functions configured with a memory limit of 256MB. We compare both systems using the MNIST dataset and model (§<a href="#S5.SS1" title="V-A Datasets ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>, §<a href="#S5.SS2" title="V-B Model Architectures and Hyperparameters ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>) for 25, 50, and 75 clients per round.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Table <a href="#S6.T1" title="TABLE I ‣ VI-B Local Differential Privacy ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the round duration for both the systems. We observe that <em id="S6.SS3.p3.1.1" class="ltx_emph ltx_font_italic">FedLess</em> is always significantly faster than <em id="S6.SS3.p3.1.2" class="ltx_emph ltx_font_italic">FedKeeper</em> with less variability in the round times. The large variance in the round times of <em id="S6.SS3.p3.1.3" class="ltx_emph ltx_font_italic">FedKeeper</em> can be attributed to the cold starts of the invoker functions and their varying runtimes. We re-ran these experiments and exchanged one client function of <em id="S6.SS3.p3.1.4" class="ltx_emph ltx_font_italic">FedKeeper</em> with the original, unoptimized version without caching. The different client execution times are shown in Figure <a href="#S6.F11" title="Figure 11 ‣ VI-B Local Differential Privacy ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. Caching the model architecture and local dataset in-memory of the function instance reduces the median client execution time from 4.46 seconds to 3.83 seconds, a speed-up of roughly 14%. Only the optimized client functions of <em id="S6.SS3.p3.1.5" class="ltx_emph ltx_font_italic">FedKeeper</em> are slightly faster than the <em id="S6.SS3.p3.1.6" class="ltx_emph ltx_font_italic">FedLess</em> clients, whose median execution time is 4.21 seconds. The additional time in the <em id="S6.SS3.p3.1.7" class="ltx_emph ltx_font_italic">FedLess</em> clients is spent on communicating with the parameter server directly instead of through the invoker functions, which is slightly faster. These results clearly show that <em id="S6.SS3.p3.1.8" class="ltx_emph ltx_font_italic">FedLess</em> is faster and imposes much fewer hardware requirements due to eliminating invoker functions.</p>
</div>
<figure id="S6.F12" class="ltx_figure"><img src="/html/2111.03396/assets/x13.png" id="S6.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S6.F12.3.2" class="ltx_text" style="font-size:90%;">
Total client costs of FaaS and IaaS to reach a target accuracy on MNIST.
Boxplots visualize rough upper and lower bounds depending on the IaaS and FaaS provider’s compute capabilities.
</span></figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.5.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.6.2" class="ltx_text ltx_font_italic">Comparing IaaS and FaaS for FL</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">To compare how our system performs compared to a traditional, non-FaaS based FL system, we implemented a baseline system using the open-source FL framework <em id="S6.SS4.p1.1.1" class="ltx_emph ltx_font_italic">Flower</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. We chose <em id="S6.SS4.p1.1.2" class="ltx_emph ltx_font_italic">Flower</em> since it was easy to modify, extend, and integrate into our existing experimentation pipeline. Furthermore, it provides baseline implementations for various FL strategies. For our experiments, we use the default <span id="S6.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">FedAvg</span> implementation. For the communication between the central server and the clients <em id="S6.SS4.p1.1.4" class="ltx_emph ltx_font_italic">Flower</em> uses <span id="S6.SS4.p1.1.5" class="ltx_text ltx_font_typewriter">gRPC</span>. To support invoking a large number of clients in parallel, we had to modify the current <em id="S6.SS4.p1.1.6" class="ltx_emph ltx_font_italic">Flower</em> implementation. For our IaaS based experiments, we use <em id="S6.SS4.p1.1.7" class="ltx_emph ltx_font_italic">cgroups</em> to simulate compute instances similar to general-purpose instances of public cloud providers (e.g. <em id="S6.SS4.p1.1.8" class="ltx_emph ltx_font_italic">m4.large</em> on AWS EC2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, <em id="S6.SS4.p1.1.9" class="ltx_emph ltx_font_italic">n2-standard-2</em> on Google Cloud Compute Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>) on the LRZ compute cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. This is also similar to the setup used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> (mostly 2 CPUs, 8GB RAM), and similar to some workloads in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We deploy all Flower clients in separate Docker containers that can each use two vCPUs and 8GB memory. Like in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> our central server for Flower has CPUs and RAM similar to a c5.4xlarge instance on AWS EC2, i.e., 16 vCPU cores and 32 GB RAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. For a fair comparison, we deploy the client functions for <em id="S6.SS4.p1.1.10" class="ltx_emph ltx_font_italic">FedLess</em> on the same hardware using OpenFaaS. OpenFaaS allows us to limit each client function to 2 vCPU cores and 2GB of memory. As a result, the <em id="S6.SS4.p1.1.11" class="ltx_emph ltx_font_italic">FedLess</em> client functions have the same processing power as the Flower clients. We compare both systems using the MNIST and FEMNIST datasets with 100 clients using the same models and hyperparameters as described in §<a href="#S5.SS2" title="V-B Model Architectures and Hyperparameters ‣ V Experimental Setup ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.2" class="ltx_p">Since both systems use <span id="S6.SS4.p2.2.1" class="ltx_text ltx_font_typewriter">FedAvg</span> and the same hyperparameters, we are primarily interested in timing and pricing differences. Due to the ephemeral, stateless nature of FaaS based systems, we observe that Flower is faster than <em id="S6.SS4.p2.2.2" class="ltx_emph ltx_font_italic">FedLess</em> in all experiments. The relative difference depends on the dataset used. To assess this difference, we use the geometric mean to calculate the mean training round times for <em id="S6.SS4.p2.2.3" class="ltx_emph ltx_font_italic">FedLess</em> and Flower, as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. The mean training round times for <em id="S6.SS4.p2.2.4" class="ltx_emph ltx_font_italic">FedLess</em> are on average <math id="S6.SS4.p2.1.m1.1" class="ltx_math_unparsed" alttext="1.77\times" display="inline"><semantics id="S6.SS4.p2.1.m1.1a"><mrow id="S6.SS4.p2.1.m1.1b"><mn id="S6.SS4.p2.1.m1.1.1">1.77</mn><mo lspace="0.222em" id="S6.SS4.p2.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p2.1.m1.1c">1.77\times</annotation></semantics></math> as long for FEMNIST compared to Flower, whereas they take <math id="S6.SS4.p2.2.m2.1" class="ltx_math_unparsed" alttext="1.6\times" display="inline"><semantics id="S6.SS4.p2.2.m2.1a"><mrow id="S6.SS4.p2.2.m2.1b"><mn id="S6.SS4.p2.2.m2.1.1">1.6</mn><mo lspace="0.222em" id="S6.SS4.p2.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p2.2.m2.1c">1.6\times</annotation></semantics></math> as long for MNIST. However, a key advantage of our system is that client functions that are already finished in a round do not cost anything while waiting and can be scaled to zero. In traditional FL systems, like the one we implemented with Flower, clients simply turn idle while waiting for further requests, accumulating cost, or blocking hardware resources.</p>
</div>
<figure id="S6.F13" class="ltx_figure"><img src="/html/2111.03396/assets/x14.png" id="S6.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S6.F13.3.2" class="ltx_text" style="font-size:90%;">
Total client costs of FaaS and IaaS to reach a target accuracy on FEMNIST.
</span></figcaption>
</figure>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">To understand the differences between FaaS/IaaS for FL in terms of cost, we created pricing estimates for MNIST and FEMNIST, based on the results of our previous experiments. We base these calculations on the assumption that all clients run on Google Cloud, both for IaaS and FaaS-based training. The compute resources we used for the Flower clients are similar to n2-standard-2 VM instances in both vCPUs and memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. For the <em id="S6.SS4.p3.1.1" class="ltx_emph ltx_font_italic">FedLess</em> clients, we use the execution time presented in experiments from §<a href="#S6.SS3" title="VI-C Comparison with Fedkeeper ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a> (MNIST), §<a href="#S6.SS1" title="VI-A Analyzing FedLess Performance ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a> (FEMNIST). Since the <em id="S6.SS4.p3.1.2" class="ltx_emph ltx_font_italic">Fedless</em> clients use the same parameter server, hyperparameters, and local datasets, the comparison is fair. For <em id="S6.SS4.p3.1.3" class="ltx_emph ltx_font_italic">FedLess</em> and Flower, we calculate the mean estimated cost of the training runs for both datasets. To not get results skewed by implementation differences, we assume <em id="S6.SS4.p3.1.4" class="ltx_emph ltx_font_italic">FedLess</em> and Flower take an equal number of rounds to reach a particular target accuracy. In addition, for both FaaS functions and IaaS VMs, we ignore additional costs like disk space, costs for cloud buckets, costs to set up and deploy clients, and free-tiers. All prices are calculated for the <span id="S6.SS4.p3.1.5" class="ltx_text ltx_font_typewriter">us-central1</span> region. For our experiments, we only calculate direct costs. For Google Cloud Functions, these are GB-seconds and GHz-seconds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, invocations and networking costs, and for VMs, they are instance runtime and networking costs.</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p id="S6.SS4.p4.3" class="ltx_p">Since we can only provide estimates, we do not calculate point estimates for the pricing but compute broader bounds. Towards this, we do not only take individual client functions execution time and total training time from the <em id="S6.SS4.p4.3.1" class="ltx_emph ltx_font_italic">FedLess</em>/Flower experiments but additionally include in our calculations how costs would change if the <em id="S6.SS4.p4.3.2" class="ltx_emph ltx_font_italic">FedLess</em> client functions took <math id="S6.SS4.p4.1.m1.1" class="ltx_math_unparsed" alttext="2\times" display="inline"><semantics id="S6.SS4.p4.1.m1.1a"><mrow id="S6.SS4.p4.1.m1.1b"><mn id="S6.SS4.p4.1.m1.1.1">2</mn><mo lspace="0.222em" id="S6.SS4.p4.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p4.1.m1.1c">2\times</annotation></semantics></math> and <math id="S6.SS4.p4.2.m2.1" class="ltx_math_unparsed" alttext="3\times" display="inline"><semantics id="S6.SS4.p4.2.m2.1a"><mrow id="S6.SS4.p4.2.m2.1b"><mn id="S6.SS4.p4.2.m2.1.1">3</mn><mo lspace="0.222em" id="S6.SS4.p4.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p4.2.m2.1c">3\times</annotation></semantics></math> as long, and the Flower clients only took <math id="S6.SS4.p4.3.m3.1" class="ltx_math_unparsed" alttext="0.5\times" display="inline"><semantics id="S6.SS4.p4.3.m3.1a"><mrow id="S6.SS4.p4.3.m3.1b"><mn id="S6.SS4.p4.3.m3.1.1">0.5</mn><mo lspace="0.222em" id="S6.SS4.p4.3.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS4.p4.3.m3.1c">0.5\times</annotation></semantics></math> as long. For the MNIST dataset, the total training costs for both systems until certain target accuracies are reached for different numbers of clients per round is shown in Figure <a href="#S6.F12" title="Figure 12 ‣ VI-C Comparison with Fedkeeper ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We see that overall, the clients’ costs are smaller with our FaaS-based approach for the shown settings. Especially further into the training, pricing differences are amplified. However, the cost benefits of using <em id="S6.SS4.p4.3.3" class="ltx_emph ltx_font_italic">FedLess</em> are larger for a smaller number of clients sampled per round. This is intuitive since the more clients are used per round, the less compute time is wasted in the IaaS based approach. Comparing the results for 25 clients per round for MNIST and FEMNIST in Figures <a href="#S6.F12" title="Figure 12 ‣ VI-C Comparison with Fedkeeper ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and <a href="#S6.F13" title="Figure 13 ‣ VI-D Comparing IaaS and FaaS for FL ‣ VI Experimental Results ‣ FedLess: Secure and Scalable Federated Learning Using Serverless Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, we observe that although <em id="S6.SS4.p4.3.4" class="ltx_emph ltx_font_italic">FedLess</em> is still less expensive, the relative difference becomes smaller for larger model sizes. The networking cost due to uploading the updated model is the same for FaaS and IaaS and is responsible for a significant portion of the overall cost. Based on our experiments, we gather first evidence that a FaaS-based FL approach is more cost-efficient for settings where only a fraction of active clients participate in a round. Investigating these trade-offs in detail is part of our future work.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we presented a novel system and framework for serverless FL called <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">FedLess</em>. <em id="S7.p1.1.2" class="ltx_emph ltx_font_italic">FedLess</em> enables FL across a large fabric of heterogeneous FaaS providers while providing important features such as authentication, authorization, and differential privacy. We demonstrated with comprehensive experiments the features and scalability of our system. In comparison with a traditional IaaS based FL system, we demonstrated the practical viability of our system and showed that, albeit being slower, it can be cheaper and more resource-efficient.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In the future, we plan to investigate the benefits of switching from our current hybrid system design to a fully event-driven system using message queues and publish-subscribe systems such as Rabbitmq. It also remains to be seen how well our FaaS-based approach works for even larger datasets and models. Although the conventional client heterogeneity in FL is less of a problem in FaaS as compared to traditional IaaS based approaches since idle clients do not cost anything and can scale to zero, we plan to investigate efficient FL using FaaS, which accounts for clients’ heterogeneous compute requirements or dataset sizes.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgement and Reproducibility</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This work was supported by the funding of the German Federal Ministry of Education and Research (BMBF) in the scope of the Software Campus program. Google Cloud credits in this work were provided by the <span id="S8.p1.1.1" class="ltx_text ltx_font_italic">Google Cloud Research Credits</span> program with the award number NH93G06K20KDXH9U.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">All code artifacts related to this work are available at<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/andreas-grafberger/fedless</span></span></span>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Chiang and T. Zhang, “Fog and iot: An overview of research opportunities,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of things journal</em>, vol. 3, no. 6, pp. 854–864, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Nvidia Clara, “NVIDIA Clara — NVIDIA Developer,” 2020. [Online].
Available: <a target="_blank" href="https://developer.nvidia.com/blog/federated-learning-clara/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/blog/federated-learning-clara/</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">nature</em>, vol.
521, no. 7553, pp. 436–444, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
2018 reform of EU data protection rules. [Online]. Available:
<a target="_blank" href="https://eur-lex.europa.eu/eli/reg/2016/679/oj" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eur-lex.europa.eu/eli/reg/2016/679/oj</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, R. G. L. D’Oliveira, S. E.
Rouayheb, D. Evans, J. Gardner, Z. Garrett, A. Gascón, B. Ghazi, P. B.
Gibbons, M. Gruteser, Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson,
J. Hsu, M. Jaggi, T. Javidi, G. Joshi, M. Khodak, J. Konečný,
A. Korolova, F. Koushanfar, S. Koyejo, T. Lepoint, Y. Liu, P. Mittal,
M. Mohri, R. Nock, A. Özgür, R. Pagh, M. Raykova, H. Qi,
D. Ramage, R. Raskar, D. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh,
F. Tramèr, P. Vepakomma, J. Wang, L. Xiong, Z. Xu, Q. Yang, F. X. Yu,
H. Yu, and S. Zhao, “Advances and Open Problems in Federated Learning,”
p. 16, dec 2019. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1912.04977" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.04977</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Allen, B. Browning, L. Calcote, A. Chaudhry, D. Davis, L. Fourie, A. Gulli,
Y. Haviv, D. Krook, O. Nissan-Messing, C. Munns, K. Owens, M. Peek, C. Zhang,
and C. A., “CNCF WG-Serverless Whitepaper v1.0,” CNCF, Tech. Rep., 2018.
[Online]. Available:
<a target="_blank" href="https://github.com/cncf/wg-serverless/blob/master/whitepapers/serverless-overview/cncf_serverless_whitepaper_v1.0.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/cncf/wg-serverless/blob/master/whitepapers/serverless-overview/cncf_serverless_whitepaper_v1.0.pdf</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
The Apache Foundation, “Apache OpenWhisk is a serverless, open source cloud
platform,” 2018. [Online]. Available: <a target="_blank" href="https://openwhisk.apache.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openwhisk.apache.org/</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
OpenFaaS, “OpenFaaS - Serverless Functions Made Simple,” 2019. [Online].
Available: <a target="_blank" href="https://www.openfaas.com/https://docs.openfaas.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.openfaas.com/https://docs.openfaas.com/</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Aws, “AWS Lambda – Serverless Compute - Amazon Web Services,” 2020.
[Online]. Available: <a target="_blank" href="https://aws.amazon.com/lambda/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/lambda/</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Google Cloud, “Cloud Functions — Google Cloud,” 2020. [Online].
Available: <a target="_blank" href="https://cloud.google.com/functions/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/functions/</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Chadha, A. Jindal, and M. Gerndt, “Towards Federated Learning Using FaaS
Fabric,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Sixth International Workshop on
Serverless Computing</em>, ser. WoSC’20.   New York, NY, USA: Association for Computing Machinery, 2020, pp. 49–54.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1145/3429880.3430100" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3429880.3430100</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. Konečný, S. Mazzocchi, H. B. McMahan, T. V. Overveldt,
D. Petrou, D. Ramage, and J. Roselander, “Towards federated learning at
scale: System design,” 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: A system for large-scale
machine learning,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th USENIX Symposium on
Operating Systems Design and Implementation, OSDI 2016</em>, vol. 10, no. July,
2016, pp. 265–283. [Online]. Available:
<a target="_blank" href="https://www.usenix.org/system/files/conference/osdi16/osdi16-liu.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/system/files/conference/osdi16/osdi16-liu.pdf</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Azure, “Azure Functions Serverless Compute — Microsoft Azure,” 2021.
[Online]. Available:
<a target="_blank" href="https://azure.microsoft.com/en-us/services/functions/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://azure.microsoft.com/en-us/services/functions/</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
“IBM Cloud Functions — IBM.” [Online]. Available:
<a target="_blank" href="https://www.ibm.com/cloud/functions" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ibm.com/cloud/functions</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, N. D. Lane, P. P. B.
de Gusmão, and N. D. Lane, “Flower: A Friendly Federated Learning
Research Framework,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, pp. 1–22, jul 2020. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/2007.14390" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2007.14390</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V. Shankar, K. Krauth, K. Vodrahalli, Q. Pu, B. Recht, I. Stoica,
J. Ragan-Kelley, E. Jonas, and S. Venkataraman, “Serverless linear
algebra,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th ACM Symposium on Cloud
Computing</em>, ser. SoCC ’20.   New York,
NY, USA: Association for Computing Machinery, 2020, p. 281–295. [Online].
Available: <a target="_blank" href="https://doi-org.eaccess.ub.tum.de/10.1145/3419111.3421287" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi-org.eaccess.ub.tum.de/10.1145/3419111.3421287</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Jindal, M. Gerndt, M. Chadha, V. Podolskiy, and P. Chen, “Function delivery
network: Extending serverless computing for heterogeneous platforms,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Software: Practice and Experience</em>, vol. 51, no. 9, pp. 1936–1963,
2021. [Online]. Available:
<a target="_blank" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2966" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2966</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Jindal, M. Chadha, M. Gerndt, J. Frielinghaus, V. Podolskiy, and P. Chen,
“Poster: Function delivery network: Extending serverless to heterogeneous
computing,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 41st International Conference on Distributed
Computing Systems (ICDCS)</em>, 2021, pp. 1128–1129.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Jindal, J. Frielinghaus, M. Chadha, and M. Gerndt, “Courier: Delivering
serverless functions within heterogeneous faas deployments,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2021
IEEE/ACM 14th International Conference on Utility and Cloud Computing
(UCC’21)</em>, ser. UCC ’21.   New York, NY,
USA: Association for Computing Machinery, 2021. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1145/3468737.3494097" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3468737.3494097</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L. Wang, M. Li, Y. Zhang, T. Ristenpart, and M. Swift, “Peeking behind the
curtains of serverless platforms,” in <em id="bib.bib22.6.6" class="ltx_emph ltx_font_italic">2018 <math id="bib.bib22.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib22.1.1.m1.1a"><mo stretchy="false" id="bib.bib22.1.1.m1.1.1" xref="bib.bib22.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.1.1.m1.1b"><ci id="bib.bib22.1.1.m1.1.1.cmml" xref="bib.bib22.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib22.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib22.2.2.m2.1a"><mo stretchy="false" id="bib.bib22.2.2.m2.1.1" xref="bib.bib22.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.2.2.m2.1b"><ci id="bib.bib22.2.2.m2.1.1.cmml" xref="bib.bib22.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.2.m2.1c">\}</annotation></semantics></math> Annual
Technical Conference (<math id="bib.bib22.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib22.3.3.m3.1a"><mo stretchy="false" id="bib.bib22.3.3.m3.1.1" xref="bib.bib22.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.3.3.m3.1b"><ci id="bib.bib22.3.3.m3.1.1.cmml" xref="bib.bib22.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib22.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib22.4.4.m4.1a"><mo stretchy="false" id="bib.bib22.4.4.m4.1.1" xref="bib.bib22.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.4.4.m4.1b"><ci id="bib.bib22.4.4.m4.1.1.cmml" xref="bib.bib22.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.4.4.m4.1c">\}</annotation></semantics></math><math id="bib.bib22.5.5.m5.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib22.5.5.m5.1a"><mo stretchy="false" id="bib.bib22.5.5.m5.1.1" xref="bib.bib22.5.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.5.5.m5.1b"><ci id="bib.bib22.5.5.m5.1.1.cmml" xref="bib.bib22.5.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.5.5.m5.1c">\{</annotation></semantics></math>ATC<math id="bib.bib22.6.6.m6.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib22.6.6.m6.1a"><mo stretchy="false" id="bib.bib22.6.6.m6.1.1" xref="bib.bib22.6.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.6.6.m6.1b"><ci id="bib.bib22.6.6.m6.1.1.cmml" xref="bib.bib22.6.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.6.6.m6.1c">\}</annotation></semantics></math> 18)</em>, 2018, pp. 133–146.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Jindal, M. Chadha, S. Benedict, and M. Gerndt, “Estimating the capacities
of function-as-a-service functions,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th
IEEE/ACM International Conference on Utility and Cloud Computing Companion</em>,
ser. UCC ’21 Companion.   New York, NY,
USA: Association for Computing Machinery, 2021. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1145/3492323.3495628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3492323.3495628</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Chadha, A. Jindal, and M. Gerndt, “Architecture-specific performance
optimization of compute-intensive faas functions,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2107.10008</em>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Kiener, M. Chadha, and M. Gerndt, “Towards demystifying intra-function
parallelism in serverless computing,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2110.12090</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Mohan, H. Sane, K. Doshi, S. Edupuganti, N. Nayak, and V. Sukhomlinov,
“Agile cold starts for scalable serverless,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">11th USENIX
Workshop on Hot Topics in Cloud Computing (HotCloud 19)</em>.   Renton, WA: USENIX Association, Jul. 2019.
[Online]. Available:
<a target="_blank" href="https://www.usenix.org/conference/hotcloud19/presentation/mohan" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/hotcloud19/presentation/mohan</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
G. C. Fox, V. Ishakian, V. Muthusamy, and A. Slominski, “Status of Serverless
Computing and Function-as-a-Service(FaaS) in Industry and Research,” aug
2017. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1708.08028" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1708.08028</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Wang, D. Niu, and B. Li, “Distributed Machine Learning with a Serverless
Architecture,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings - IEEE INFOCOM</em>, vol. 2019-April.   Institute of Electrical and Electronics
Engineers Inc., apr 2019, pp. 1288–1296. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1109/INFOCOM.2019.8737391" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/INFOCOM.2019.8737391</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Carreira, P. Fonseca, A. Tumanov, A. Zhang, and R. Katz, “Cirrus: A
Serverless Framework for End-To-end ML Workflows,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">SoCC 2019 -
Proceedings of the ACM Symposium on Cloud Computing</em>.   Association for Computing Machinery, nov 2019, pp. 13–24.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Jiang, S. Gan, Y. Liu, F. Wang, G. Alonso, A. Klimovic, A. Singla, W. Wu,
and C. Zhang, “Towards Demystifying Serverless Machine Learning
Training,” vol. 15, no. 21, may 2021. [Online]. Available:
<a target="_blank" href="http://dx.doi.org/10.1145/3448016.3459240" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1145/3448016.3459240</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard <em id="bib.bib31.5.5" class="ltx_emph ltx_font_italic">et al.</em>, “Tensorflow: A system for
large-scale machine learning,” in <em id="bib.bib31.4.4" class="ltx_emph ltx_font_italic">12th <math id="bib.bib31.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib31.1.1.m1.1a"><mo stretchy="false" id="bib.bib31.1.1.m1.1.1" xref="bib.bib31.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib31.1.1.m1.1b"><ci id="bib.bib31.1.1.m1.1.1.cmml" xref="bib.bib31.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib31.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib31.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib31.2.2.m2.1a"><mo stretchy="false" id="bib.bib31.2.2.m2.1.1" xref="bib.bib31.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib31.2.2.m2.1b"><ci id="bib.bib31.2.2.m2.1.1.cmml" xref="bib.bib31.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib31.2.2.m2.1c">\}</annotation></semantics></math> symposium on
operating systems design and implementation (<math id="bib.bib31.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib31.3.3.m3.1a"><mo stretchy="false" id="bib.bib31.3.3.m3.1.1" xref="bib.bib31.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib31.3.3.m3.1b"><ci id="bib.bib31.3.3.m3.1.1.cmml" xref="bib.bib31.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib31.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib31.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib31.4.4.m4.1a"><mo stretchy="false" id="bib.bib31.4.4.m4.1.1" xref="bib.bib31.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib31.4.4.m4.1b"><ci id="bib.bib31.4.4.m4.1.1.cmml" xref="bib.bib31.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib31.4.4.m4.1c">\}</annotation></semantics></math> 16)</em>, 2016, pp.
265–283.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and
Z. Zhang, “Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems,” 2015.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
“OpenMined/PySyft: A library for answering questions using data you cannot
see.” [Online]. Available: <a target="_blank" href="https://github.com/OpenMined/PySyft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenMined/PySyft</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. K. Konečny, S. Mazzocchi, H. B. Mcmahan, T. V. Overveldt,
D. Petrou, D. Ramage, and J. Roselander, “TensorFlow Federated,” 2020.
[Online]. Available: <a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang, P. Vepakomma,
A. Singh, H. Qiu, X. Zhu, J. Wang, L. Shen, P. Zhao, Y. Kang, Y. Liu,
R. Raskar, Q. Yang, M. Annavaram, and S. Avestimehr, “FedML: A Research
Library and Benchmark for Federated Machine Learning,” 2020. [Online].
Available: <a target="_blank" href="https://fedml.ai." title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fedml.ai.</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
“PaddlePaddle/PaddleFL: Federated Deep Learning in PaddlePaddle.” [Online].
Available: <a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/PaddleFL</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
“Fate.” [Online]. Available: <a target="_blank" href="https://fate.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fate.fedai.org/</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
“faasd - OpenFaaS.” [Online]. Available:
<a target="_blank" href="https://docs.openfaas.com/deployment/faasd/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.openfaas.com/deployment/faasd/</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
MongoDB Inc., “The most popular database for modern apps — MongoDB,”
2021. [Online]. Available: <a target="_blank" href="https://www.mongodb.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mongodb.com/</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
F. Chollet and Others, “Keras: the Python deep learning API,” 2020.
[Online]. Available: <a target="_blank" href="https://keras.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keras.io/</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
“pydantic.” [Online]. Available: <a target="_blank" href="https://pydantic-docs.helpmanual.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pydantic-docs.helpmanual.io/</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Z. Xu, F. Yu, J. Xiong, and X. Chen, “Helios: Heterogeneity-Aware Federated
Learning with Dynamically Balanced Collaboration,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, dec 2019.
[Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1912.01684" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.01684</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
D. Hardt, “The OAuth 2.0 Authorization Framework,” Internet Requests for
Comments, RFC Editor, RFC 6749, oct 2012. [Online]. Available:
<a target="_blank" href="http://www.rfc-editor.org/rfc/rfc6749.txt" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.rfc-editor.org/rfc/rfc6749.txt</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
S. Cantor, J. Kemp, R. Philpott, E. Maler, and E. Goodman, “Assertions and
Protocol for the OASIS Security Assertion Markup Language (SAML),” OASIS
Open, Tech. Rep. November, 2005. [Online]. Available:
<a target="_blank" href="http://www.oasis-open.org/committees/documents.php?wg_abbrev=security" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.oasis-open.org/committees/documents.php?wg_abbrev=security</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
AWS, “Amazon Cognito - Simple and Secure User Sign Up &amp; Sign In — Amazon
Web Services (AWS),” 2020. [Online]. Available:
<a target="_blank" href="https://aws.amazon.com/cognito/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/cognito/</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
M. Jones, J. Bradley, and N. Sakimura, “JSON Web Token (JWT),” Internet
Requests for Comments, RFC Editor, RFC 7519, may 2015. [Online]. Available:
<a target="_blank" href="http://www.rfc-editor.org/rfc/rfc7519.txt" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.rfc-editor.org/rfc/rfc7519.txt</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
“OWASP Top Ten Web Application Security Risks — OWASP.” [Online].
Available: <a target="_blank" href="https://owasp.org/www-project-top-ten/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://owasp.org/www-project-top-ten/</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Michener, “Security issues with functions as a service,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IT
Professional</em>, vol. 22, no. 5, pp. 24–31, sep 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
L. Lyu, H. Yu, J. Zhao, and Q. Yang, “Threats to Federated Learning: A
Survey,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Federated Learning - Privacy and Incentive</em>, ser. Lecture
Notes in Computer Science, Q. Yang, L. Fan, and H. Yu, Eds.   Springer, Cham, 2020, vol. 12500, pp. 3–16.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1007/978-3-030-63076-8_1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-63076-8_1</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
D. Enthoven and Z. Al-Ars, <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">An Overview of Federated Deep Learning Privacy
Attacks and Defensive Strategies</em>.   Cham: Springer International Publishing, 2021, pp. 173–196. [Online].
Available: <a target="_blank" href="https://doi.org/10.1007/978-3-030-70604-3_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-70604-3_8</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. Truex, T. Steinke, N. Baracaldo, H. Ludwig, Y. Zhou, A. Anwar, and R. Zhang,
“A hybrid approach to privacy-preserving federated learning,” in
<em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Conference on Computer and Communications
Security</em>.   Association for Computing
Machinery, dec 2019, pp. 1–11. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/1812.03224v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1812.03224v2</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacy-Preserving
Deep Learning via Additively Homomorphic Encryption,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Information Forensics and Security</em>, vol. 13, no. 5, pp.
1333–1345, may 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
C. Dwork, “Differential Privacy: A Survey of Results,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Theory and
Applications of Models of Computation</em>.   Springer Berlin Heidelberg, apr 2008, pp. 1–19. [Online].
Available:
<a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-540-79228-4_1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/chapter/10.1007/978-3-540-79228-4_1</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
C. Dwork and A. Roth, “The algorithmic foundations of differential
privacy,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends in Theoretical Computer Science</em>,
vol. 9, no. 3-4, pp. 211–407, 2014.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
C. Zhang, S. Li, J. Xia, W. Wang, F. Yan, and Y. Liu, “BatchCrypt: Efficient
homomorphic encryption for cross-silo federated learning,” in
<em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020</em>,
2020, pp. 493–506. [Online]. Available:
<a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/zhang-chengliang" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc20/presentation/zhang-chengliang</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
H. Zhu, R. Wang, Y. Jin, K. Liang, and J. Ning, “Distributed Additive
Encryption and Quantization for Privacy Preserving Federated Deep
Learning,” nov 2020. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/2011.12623" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2011.12623</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for
privacy-preserving machine learning,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM
Conference on Computer and Communications Security</em>.   New York, NY, USA: Association for Computing Machinery, oct
2017, pp. 1175–1191. [Online]. Available:
<a target="_blank" href="https://dl.acm.org/doi/10.1145/3133956.3133982" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.acm.org/doi/10.1145/3133956.3133982</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, feb 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
M. Kim, O. Günlü, and R. F. Schaefer, “Federated Learning with
Local Differential Privacy: Trade-offs between Privacy, Utility, and
Communication,” feb 2021. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/2102.04737" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2102.04737</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin, T. Q. Quek, and
H. Vincent Poor, “Federated Learning with Differential Privacy:
Algorithms and Performance Analysis,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Information Forensics and Security</em>, vol. 15, pp. 3454–3469, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting
unintended feature leakage in collaborative learning,” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings
- IEEE Symposium on Security and Privacy</em>, vol. 2019-May.   Institute of Electrical and Electronics Engineers
Inc., may 2019, pp. 691–706. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1805.04049" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1805.04049</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
H. B. McMahan, G. Andrew, U. Erlingsson, S. Chien, I. Mironov, N. Papernot, and
P. Kairouz, “A General Approach to Adding Differential Privacy to Iterative
Training Procedures,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, dec 2018. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1812.06210" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.06210</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
T. community, “tensorflow/privacy: Library for training machine learning
models with privacy for training data,” 2020. [Online]. Available:
<a target="_blank" href="https://github.com/tensorflow/privacy" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tensorflow/privacy</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konečný, H. B. McMahan,
V. Smith, and A. Talwalkar, “LEAF: A Benchmark for Federated Settings,”
in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Workshop on Federated Learning for Data Privacy and Confidentiality,
NeurIPS</em>, 2018, pp. 1–9. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.01097</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Y. LeCun and C. Cortes, “MNIST handwritten digit database,” 2010. [Online].
Available: <a target="_blank" href="http://yann.lecun.com/exdb/mnist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist/</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konečný, H. B. McMahan,
V. Smith, and A. Talwalkar, “LEAF: A Benchmark for Federated Settings,”
in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Workshop on Federated Learning for Data Privacy and Confidentiality,
NeurIPS</em>, 2018, pp. 1–9. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.01097</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
“The Complete Works of William Shakespeare by William Shakespeare - Free
Ebook.” [Online]. Available: <a target="_blank" href="http://www.gutenberg.org/ebooks/100" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.gutenberg.org/ebooks/100</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
G. Cohen, S. Afshar, J. Tapson, and A. van Schaik, “EMNIST: an extension of
MNIST to handwritten letters,” feb 2017. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1702.05373" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1702.05373</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
“NGINX — High Performance Load Balancer, Web Server, &amp; Reverse Proxy.”
[Online]. Available: <a target="_blank" href="https://www.nginx.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nginx.com/</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Neural
computation</em>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
LRZ Compute Cloud, <a target="_blank" href="https://doku.lrz.de/display/PUBLIC/Compute+Cloud" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doku.lrz.de/display/PUBLIC/Compute+Cloud</a>,
accessed on 09/24/2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
A. V. Papadopoulos, L. Versluis, A. Bauer, N. Herbst, J. v. Kistowski,
A. Ali-Eldin, C. L. Abad, J. N. Amaral, P. Tůma, and A. Iosup,
“Methodological principles for reproducible performance evaluation in cloud
computing,” <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, vol. 47,
no. 8, pp. 1528–1543, 2021.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Y. Chen, Y. Ning, M. Slawski, and H. Rangwala, “Asynchronous online federated
learning for edge devices with non-iid data,” in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Big Data (Big Data)</em>.   IEEE, 2020, pp. 15–24.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
“Amazon EC2 - Amazon Web Services.” [Online]. Available:
<a target="_blank" href="http://aws.amazon.com/ec2/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://aws.amazon.com/ec2/</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Google, “Machine types — Compute Engine Documentation — Google Cloud,”
2019. [Online]. Available:
<a target="_blank" href="https://cloud.google.com/compute/docs/machine-types" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/compute/docs/machine-types</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
“Compute Cloud - Leibniz-Rechenzentrum (LRZ) Dokumentation.” [Online].
Available: <a target="_blank" href="https://doku.lrz.de/display/PUBLIC/Compute+Cloud" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doku.lrz.de/display/PUBLIC/Compute+Cloud</a>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
T. Hoefler and R. Belli, “Scientific benchmarking of parallel computing
systems: Twelve ways to tell the masses when reporting performance results,”
in <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis</em>, ser. SC ’15.   New York, NY, USA: Association for Computing
Machinery, 2015. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1145/2807591.2807644" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2807591.2807644</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
“VM instances pricing — Compute Engine: Virtual Machines (VMs).” [Online].
Available: <a target="_blank" href="https://cloud.google.com/compute/vm-instance-pricing" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/compute/vm-instance-pricing</a>

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Google Cloud Functions Pricing,
<a target="_blank" href="https://cloud.google.com/functions/pricing" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.google.com/functions/pricing</a>, accessed 09/24/2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.03395" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.03396" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.03396">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.03396" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.03397" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 20:30:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
