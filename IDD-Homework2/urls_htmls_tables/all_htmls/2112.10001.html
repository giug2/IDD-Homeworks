<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.10001] Cross-Domain Federated Learning in Medical Imaging</title><meta property="og:description" content="Federated learning is increasingly being explored in the field of medical imaging to train deep learning models on large scale datasets distributed across different data centers while preserving privacy by avoiding the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cross-Domain Federated Learning in Medical Imaging">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cross-Domain Federated Learning in Medical Imaging">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.10001">

<!--Generated on Fri Mar  8 19:11:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.11" class="ltx_ERROR undefined">\jmlrvolume</span>
<p id="p1.10" class="ltx_p">– Under Review
<span id="p1.10.1" class="ltx_ERROR undefined">\jmlryear</span>2022
<span id="p1.10.2" class="ltx_ERROR undefined">\jmlrworkshop</span>Full Paper – MIDL 2022 submission


<span id="p1.10.3" class="ltx_ERROR undefined">\midlauthor</span><span id="p1.10.4" class="ltx_ERROR undefined">\Name</span>Vishwa S Parekh <span id="p1.10.5" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.6" class="ltx_sup"><span id="p1.10.6.1" class="ltx_text ltx_font_italic">1,2</span></sup> <span id="p1.10.7" class="ltx_ERROR undefined">\Email</span>vishwaparekh@jhu.edu
<br class="ltx_break"><span id="p1.10.8" class="ltx_ERROR undefined">\Name</span>Shuhao Lai <span id="p1.10.9" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.10" class="ltx_sup"><span id="p1.10.10.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.10.11" class="ltx_ERROR undefined">\Email</span>slai16@jhu.edu
<br class="ltx_break"><span id="p1.10.12" class="ltx_ERROR undefined">\Name</span>Vladimir Braverman <span id="p1.10.13" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.14" class="ltx_sup"><span id="p1.10.14.1" class="ltx_text ltx_font_italic">1</span></sup> <span id="p1.10.15" class="ltx_ERROR undefined">\Email</span>vova@cs.jhu.edu
<br class="ltx_break"><span id="p1.10.16" class="ltx_ERROR undefined">\Name</span>Jeff Leal <span id="p1.10.17" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.18" class="ltx_sup"><span id="p1.10.18.1" class="ltx_text ltx_font_italic">2</span></sup> <span id="p1.10.19" class="ltx_ERROR undefined">\Email</span>jleal1@jhmi.edu
<br class="ltx_break"><span id="p1.10.20" class="ltx_ERROR undefined">\Name</span>Steven Rowe <span id="p1.10.21" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.22" class="ltx_sup"><span id="p1.10.22.1" class="ltx_text ltx_font_italic">2</span></sup> <span id="p1.10.23" class="ltx_ERROR undefined">\Email</span>srowe8@jhmi.edu
<br class="ltx_break"><span id="p1.10.24" class="ltx_ERROR undefined">\Name</span>Jay J. Pillai <span id="p1.10.25" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.26" class="ltx_sup"><span id="p1.10.26.1" class="ltx_text ltx_font_italic">2</span></sup> <span id="p1.10.27" class="ltx_ERROR undefined">\Email</span>jpillai1@jhmi.edu
<br class="ltx_break"><span id="p1.10.28" class="ltx_ERROR undefined">\Name</span>Michael A Jacobs <span id="p1.10.29" class="ltx_ERROR undefined">\nametag</span><sup id="p1.10.30" class="ltx_sup"><span id="p1.10.30.1" class="ltx_text ltx_font_italic">2,3</span></sup> <span id="p1.10.31" class="ltx_ERROR undefined">\Email</span>mikej@mri.jhu.edu
<br class="ltx_break"><span id="p1.10.32" class="ltx_ERROR undefined">\addr</span><sup id="p1.10.33" class="ltx_sup"><span id="p1.10.33.1" class="ltx_text ltx_font_italic">1</span></sup> Department of Computer Science, The Johns Hopkins University, Baltimore, MD 21218 
<br class="ltx_break"><span id="p1.10.34" class="ltx_ERROR undefined">\addr</span><sup id="p1.10.35" class="ltx_sup"><span id="p1.10.35.1" class="ltx_text ltx_font_italic">2</span></sup> The Russell H. Morgan Department of Radiology and Radiological Sciences, The Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 
<br class="ltx_break"><span id="p1.10.36" class="ltx_ERROR undefined">\addr</span><sup id="p1.10.37" class="ltx_sup"><span id="p1.10.37.1" class="ltx_text ltx_font_italic">3</span></sup> Sidney Kimmel Comprehensive Center, The Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA

<span id="p1.10.38" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Cross-Domain Federated Learning in Medical Imaging</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" lang="en">Federated learning is increasingly being explored in the field of medical imaging to train deep learning models on large scale datasets distributed across different data centers while preserving privacy by avoiding the need to transfer sensitive patient information. In this manuscript, we explore federated learning in a multi-domain, multi-task setting wherein different participating nodes may contain datasets sourced from different domains and are trained to solve different tasks. We evaluated cross-domain federated learning for the tasks of object detection and segmentation across two different experimental settings: multi-modal and multi-organ. The result from our experiments on cross-domain federated learning framework were very encouraging with an overlap similarity of 0.79 for organ localization and 0.65 for lesion segmentation. Our results demonstrate the potential of federated learning in developing multi-domain, multi-task deep learning models without sharing data from different domains.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
<span id="id2.id1" class="ltx_text" lang="en">
deep learning, federated learning, multi-task, multi-domain, cross-domain, segmentation, radiological imaging
</span>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_editors"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">editors: </span>Under Review for MIDL 2022</span></span></span>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning techniques are being developed and applied across various applications in the field of Radiology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Suzuki(2017)</a>, <a href="#bib.bibx4" title="" class="ltx_ref">McBee et al.(2018)McBee, Awan, Colucci, Ghobadi, Kadom, Kansagra,
Tridandapani, and Auffermann</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Parekh and Jacobs(2019)</a>]</cite>. There are various intrinsic similarities across different deep learning models being evaluated in radiological applications. For example, segmentation models developed for multiparametric (mp) MRI would learn a similar tissue signature for segmenting different lesions irrespective of the underlying application being evaluated (e.g. brain , breast , or prostate MRI). Similarly, an object detection model built for localizing organs in a whole-body (WB) MRI would would share architectural (anatomical) similarities with object detection models built for localizing organs in a different modality like WB-PET or WB-CT. As a result, deep learning models being built for different tasks across diverse domains would significantly benefit from sharing knowledge or collaborative training.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The applicability of deep learning models in a cross-domain setting for medical imaging has previously been explore across many applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Samala et al.(2018)Samala, Chan, Hadjiiski, Helvie, Richter, and
Cha</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Hadad et al.(2017)Hadad, Bakalo, Ben-Ari, Hashoul, and
Amit</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Tomczak et al.(2020)Tomczak, Ilic, Marquardt, Engel, Forster, Navab,
and Albarqouni</a>, <a href="#bib.bibx10" title="" class="ltx_ref">Parekh et al.(2020)Parekh, Braverman, Jacobs,
et al.</a>]</cite>. However, current applications in training cross-domain deep learning models require centralized datasets and have several challenges. For example, a medical imaging center may not be equipped with PET scanners and sourcing PET data from a different medical imaging center could be difficult owing to patient privacy concerns. Similarly, different medical imaging centers could be developing deep learning models for related tasks; suppose one center is training a breast tumor segmentation model from mpMRI while another center is training a normal tissue segmentation model in breast mpMRI for breast density estimation. Cross-domain collaborative, decentralized learning in these scenarios could have numerous benefits. First, different centers lacking specialized equipment (e.g. imaging equipment) could train models to learn PET scan signals without having to source PET scans from other institutions. Second, annotating datasets is expensive and time consuming. Different institutions could potentially benefit from each others’ annotations without explicitly sharing them. The third benefit could be observed in terms of computational and space efficiency of training and storing comparatively fewer deep learning models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Collaborative, decentralized training could be achieved using federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">McMahan et al.(2017)McMahan, Moore, Ramage, Hampson, and
y Arcas</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Chang et al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen,
Rubin, and Kalpathy-Cramer</a>]</cite>. Federated learning is a machine learning paradigm that aims to train a machine learning algorithm (e.g. deep neural network) on multiple datasets stored across decentralized nodes. The local datasets stored on different nodes are not shared, therefore, preserving privacy. The global machine learning model is trained by training several local machine learning models across different nodes that exchange model parameters (e.g. weights and biases of a deep neural network) at a certain frequency which are then aggregated and shared back to the local nodes. Federated learning is increasingly being explored in the field of medical imaging to train deep learning models on large-scale datasets distributed across different data centers while preserving privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Chang et al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen,
Rubin, and Kalpathy-Cramer</a>, <a href="#bib.bibx7" title="" class="ltx_ref">Ng et al.(2021)Ng, Lan, Yao, Chan, and Feng</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Sarma et al.(2021)Sarma, Harmon, Sanford, Roth, Xu, Tetreault, Xu,
Flores, Raman, Kulkarni, et al.</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Shen et al.(2021)Shen, Wang, Roth, Yang, Xu, Oda, Wang, Fuh, Chen,
Liu, Liao, and Mori</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Qayyum et al.(2021)Qayyum, Ahmad, Ahsan, Al-Fuqaha, and
Qadir</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we explore the potential of cross-domain federated learning across two different experimental setups. The first experiment involves learning an organ localization model across two imaging modalities, PET and CT. The second experiment involves learning a lesion segmentation model across two organs, brain and breast, using mpMRI. The experiments are detailed in Section 2, followed by results in Section 3 and Discussion in Section 4.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Experiments</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Experiment 1: Multi-modal organ localization</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Clinical Data</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">All studies were performed in accordance with the institutional guidelines for clinical research under a protocol approved by our Institutional Review Board (IRB) and all HIPAA agreements were followed for this retrospective study. The clinical data consisted of fifty patients. Patients were imaged on a Biograph mCT 128-slice PET/CT scanner (Siemens Healthineers). For the 18F-DCFPyL scans, patients were intravenously injected with no more than 333 MBq (9 mCi) of radiotracer approximately 60 min before image acquisition. The field of view was vertex to mid thigh for 18F-DCFPyL. The WB PET-CT images were evaluated in this study for localization of kidneys.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Deep learning model</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The deep learning model used for localization of kidneys was a U-Net model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Ronneberger et al.(2015)Ronneberger, Fischer, and
Brox</a>]</cite>. The encoder, decoder, and the bridge sections of the U-Net consisted of five, one, and five convolutional blocks, respectively. Each covolutional block comprised of two convolutional layers with ReLU activation, followed by batch normalization. A max pooling layer (window size=2x2) preceded the encoder convolutional blocks, while the decoder convolutional blocks were preceded by an unpooling layer, followed by concatenation with the layer at the same level in the encoder section, as shown in Figure 1.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S2.F1.2" class="ltx_ERROR ltx_figure_panel undefined">\floatconts</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F1.1" class="ltx_p ltx_figure_panel">fig:example1

<img src="/html/2112.10001/assets/UNET.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="310" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the U-Net model used for segmentation and localization of different regions of interest across all the experiments</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Experimental Setup</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">The federated learning set up for this experiment consists of a central server and two client nodes as shown in Figure 2. The fifty patients were randomly split into train-test subsets with 35 patients used for training and 15 patients used for testing. The training set was divided across two nodes, with the first node comprising of CT images and the second node comprising of PET images.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p">The global U-Net model for localization of kidneys across PET and CT modalities was trained by the central server using the following steps:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Step 1: The central server initializes the global U-Net model.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Step 2: Repeat Steps 3 to 6 N times, where N corresponds to the number of communication iterations.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Step 3: The central server communicates the global U-Net model to each node</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Step 4: Each node locally trains the model on their respective local datasets for one epoch.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">Step 5: The updated model parameters are communicated back to the central server from each node.</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i6.p1" class="ltx_para">
<p id="S2.I1.i6.p1.1" class="ltx_p">Step 6: The central server updates the global U-Net model by aggregating the updated model parameters from each node using the Federated Averaging (FedAvg) algorithm.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S2.F2.2" class="ltx_ERROR ltx_figure_panel undefined">\floatconts</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F2.1" class="ltx_p ltx_figure_panel">fig:example2

<img src="/html/2112.10001/assets/FedExp1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="475" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the U-Net model used for segmentation and localization of different regions of interest across all the experiments</figcaption>
</figure>
<div id="S2.SS1.SSS3.p3" class="ltx_para">
<p id="S2.SS1.SSS3.p3.1" class="ltx_p">The local U-Net models were trained with a batch size of four. The output activation function was set to Sigmoid activation. The loss function was set to binary cross-entropy and the Adam optimizer with a learning rate of 0.0002 was used for this experiment. The performance of the global U-Net model on the test dataset was evaluated using a modified overlap similarity metric. The metric was computed by dividing the intersection between the organ boundary and the bounding box by the organ boundary to determine if the bounding box was able to bound the organ entirely or not.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Experiment 2: Multi-organ tumor segmentation</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Clinical Data</h4>

<section id="S2.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Breast mpMRI</h5>

<div id="S2.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.Px1.p1.1" class="ltx_p">Fifty patients (25 malignant and 25 benign lesions were included in this experiment. Patients were scanned on a 3T MRI system (3T Achieva, Philips Medical Systems, Best, The Netherlands) using a bilateral, dedicated phased array breast coil (InVivo, Orlando, FL) with the patient in the prone position. The complete acquisition details have been described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Parekh and Jacobs(2017)</a>]</cite>. The following imaging sequences were acquired for each patient.</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">T1-weighted MRI</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Fat-suppressed T2-weighted MRI</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">Diffusion Weighted Images (DWI)/Apparent Diffusion Coefficient (ADC) map acquired at b values of b0 and b600</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p">High temporal resolution dynamic contrast enhanced (DCE) MRI (One pre- and fourteen post-contrast images).</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p">High spatial resolution DCE-MRI (One pre- and one post-contrast image).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Brain mpMRI</h5>

<div id="S2.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS1.Px2.p1.1" class="ltx_p">The brain mpMRI comprised of 50 patients randomly selected from the brain tumor segmentation (BRATS 2017) challenge dataset. Of the 50 patients, 25 patients had High Grade Glioma and 25 patients had Low Grade Glioma. The mpMRI consisted of pre and post 3D T1-weighted, 2D T2-weighted, and T2-weighted FLAIR images. Complete MRI acquisition parameters can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Menze et al.(2014)Menze, Jakab, Bauer, Kalpathy-Cramer, Farahani,
Kirby, Burren, Porz, Slotboom, Wiest, et al.</a>]</cite></p>
</div>
</section>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Deep learning model</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The U-Net architecture, shown in Figure 1 used for the localization of kidneys was also used for segmentation of brain and breast lesions.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Experimental Setup</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The federated learning set up for this experiment consists of a central server and two client nodes similar to the one shown in Figure 2. The fifty patients from each group were randomly split into train-test subsets with 35 patients used for training and 15 patients used for testing. The training set was divided across two nodes, with the first node comprising of breast mpMRI images and the second node comprising of brain mpMRI images.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">The imaging sequences of T1-weighted pre- and post-contrast enhanced MRI, and T2-weighted images were used to train the U-Net model for tumor segmentation. The global U-Net model for segmentation of both breast and brain lesions were trained using the steps outlined in Section 2.1.3</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p">The local U-Net models were trained with a batch size of four. The loss function was set to binary cross-entropy and the Adam optimizer with a learning rate of 0.0002 was used for this experiment. The performance of the global U-Net model on the test dataset was evaluated using the dice-similarity metric.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The cross-domain federated learning demonstrated encouraging results across both the experiments as shown in figures 3 and 4. The test set for kidney localization consisted of a total of thirty cases with fifteen cases each for PET and CT scans. The average overlap similarity for kidney localization was 0.79 across both modalities. For the lesion segmentation experiment, the test set consisted of fifteen brain mpMRI and fifteen breast mpMRI dataset. The average dice similarity for lesion segmentation across brain and breast mpMRI was 0.65 and the model failed to segment the lesion on three cases (one brain and two breast cases).</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F3.2" class="ltx_ERROR ltx_figure_panel undefined">\floatconts</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.1" class="ltx_p ltx_figure_panel">fig:example3

<img src="/html/2112.10001/assets/x1.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="252" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the performance of the U-Net model for localization of kidneys across PET and CT</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F4.2" class="ltx_ERROR ltx_figure_panel undefined">\floatconts</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F4.1" class="ltx_p ltx_figure_panel">fig:example4

<img src="/html/2112.10001/assets/x2.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="252" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of the performance of the U-Net model for lesion segmentation across brain and breast</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We demonstrated the feasibility of training cross-domain federated learning models with encouraging results. Our results assert the possibility of training a general AI framework that can leverage the unique domain and tasks provided by participating institutions. Federated learning has previously been successfully applied for single medical imaging segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Li et al.(2019)Li, Milletarì, Xu, Rieke, Hancox, Zhu, Baust,
Cheng, Ourselin, Cardoso, et al.</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Chang et al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen,
Rubin, and Kalpathy-Cramer</a>, <a href="#bib.bibx7" title="" class="ltx_ref">Ng et al.(2021)Ng, Lan, Yao, Chan, and Feng</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Sarma et al.(2021)Sarma, Harmon, Sanford, Roth, Xu, Tetreault, Xu,
Flores, Raman, Kulkarni, et al.</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Shen et al.(2021)Shen, Wang, Roth, Yang, Xu, Oda, Wang, Fuh, Chen,
Liu, Liao, and Mori</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Qayyum et al.(2021)Qayyum, Ahmad, Ahsan, Al-Fuqaha, and
Qadir</a>]</cite>. However, our work broadens the possibilities for application of federated learning in the field of medical imaging, especially when participating institutions lack specialized equipment (e.g. PET/CT scanners).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Deep learning techniques are increasingly being used in radiological applications. However, an increase in the number of applications where deep learning could be applied to radiology makes the maintenance and deployment of different deep learning models to radiology computationally very expensive. Our approach offers a solution by collaboratively building multi-task, multi-domain deep learning models.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Our study has certain limitations. This is a preliminary study with experiments performed on small datasets to establish a proof of concept. The use of small datasets is also observed in our results, which are encouraging but not comparable to state-of-art established in the literature. The federated learning model was able to identify the correct region of interest on most of the patients across both datasets, with an exception of three cases. However, the model failed to localize the entire region of interest in many cases resulting in low dice similarity metrics. In the future, we plan to optimize our models and expand our experiments on large-scale datasets. In conclusion, this work initiates a first step towards a cross-domain federated learning setup and explores its potential in building a collaborative multi-domain network of participants sharing heterogeneous datasets.</p>
</div>
<div id="S4.p4" class="ltx_para">
<span id="S4.p4.1" class="ltx_ERROR undefined">\midlacknowledgments</span>
<p id="S4.p4.2" class="ltx_p">National Institutes of Health (NIH) grant numbers: 5P30CA006973 (Imaging Response Assessment Team-IRAT), U01CA140204. Defense Advanced Research Projects Agency DARPA-PA-20-02-11-ShELL-FP-007.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chang et al.(2018)Chang, Balachandar, Lam, Yi, Brown, Beers, Rosen,
Rubin, and Kalpathy-Cramer]</span>
<span class="ltx_bibblock">
Ken Chang, Niranjan Balachandar, Carson Lam, Darvin Yi, James Brown, Andrew
Beers, Bruce Rosen, Daniel Rubin, and Jayashree Kalpathy-Cramer.

</span>
<span class="ltx_bibblock">Distributed deep learning networks among institutions for medical
imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association :
JAMIA</em>, 25, 03 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1093/jamia/ocy017" title="" class="ltx_ref">10.1093/jamia/ocy017</a>.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hadad et al.(2017)Hadad, Bakalo, Ben-Ari, Hashoul, and
Amit]</span>
<span class="ltx_bibblock">
Omer Hadad, Ran Bakalo, Rami Ben-Ari, Sharbell Hashoul, and Guy Amit.

</span>
<span class="ltx_bibblock">Classification of breast lesions using cross-modal deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 14th International Symposium on Biomedical Imaging
(ISBI 2017)</em>, pages 109–112. IEEE, 2017.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Li et al.(2019)Li, Milletarì, Xu, Rieke, Hancox, Zhu, Baust,
Cheng, Ourselin, Cardoso, et al.]</span>
<span class="ltx_bibblock">
Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao
Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso,
et al.

</span>
<span class="ltx_bibblock">Privacy-preserving federated brain tumour segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">International workshop on machine learning in medical
imaging</em>, pages 133–141. Springer, 2019.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[McBee et al.(2018)McBee, Awan, Colucci, Ghobadi, Kadom, Kansagra,
Tridandapani, and Auffermann]</span>
<span class="ltx_bibblock">
Morgan P McBee, Omer A Awan, Andrew T Colucci, Comeron W Ghobadi, Nadja Kadom,
Akash P Kansagra, Srini Tridandapani, and William F Auffermann.

</span>
<span class="ltx_bibblock">Deep learning in radiology.

</span>
<span class="ltx_bibblock"><em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">Academic radiology</em>, 25(11):1472–1480,
2018.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[McMahan et al.(2017)McMahan, Moore, Ramage, Hampson, and
y Arcas]</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data, 2017.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Menze et al.(2014)Menze, Jakab, Bauer, Kalpathy-Cramer, Farahani,
Kirby, Burren, Porz, Slotboom, Wiest, et al.]</span>
<span class="ltx_bibblock">
Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan
Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland
Wiest, et al.

</span>
<span class="ltx_bibblock">The multimodal brain tumor image segmentation benchmark (brats).

</span>
<span class="ltx_bibblock"><em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on medical imaging</em>, 34(10):1993–2024, 2014.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ng et al.(2021)Ng, Lan, Yao, Chan, and Feng]</span>
<span class="ltx_bibblock">
Dianwen Ng, Xiang Lan, Melissa Min-Szu Yao, Wing P Chan, and Mengling Feng.

</span>
<span class="ltx_bibblock">Federated learning: a collaborative effort to achieve better medical
imaging models for individual sites that have small labelled datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Quantitative Imaging in Medicine and Surgery</em>, 11(2):852, 2021.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Parekh and Jacobs(2017)]</span>
<span class="ltx_bibblock">
Vishwa S Parekh and Michael A Jacobs.

</span>
<span class="ltx_bibblock">Integrated radiomic framework for breast cancer and tumor biology
using advanced machine learning and multiparametric mri.

</span>
<span class="ltx_bibblock"><em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">NPJ breast cancer</em>, 3(1):1–9, 2017.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Parekh and Jacobs(2019)]</span>
<span class="ltx_bibblock">
Vishwa S Parekh and Michael A Jacobs.

</span>
<span class="ltx_bibblock">Deep learning and radiomics in precision medicine.

</span>
<span class="ltx_bibblock"><em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Expert review of precision medicine and drug development</em>,
4(2):59–72, 2019.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Parekh et al.(2020)Parekh, Braverman, Jacobs,
et al.]</span>
<span class="ltx_bibblock">
Vishwa S Parekh, Vladimir Braverman, Michael A Jacobs, et al.

</span>
<span class="ltx_bibblock">Multitask radiological modality invariant landmark localization using
deep reinforcement learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Medical Imaging with Deep Learning</em>, pages 588–600. PMLR,
2020.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Qayyum et al.(2021)Qayyum, Ahmad, Ahsan, Al-Fuqaha, and
Qadir]</span>
<span class="ltx_bibblock">
Adnan Qayyum, Kashif Ahmad, Muhammad Ahtazaz Ahsan, Ala Al-Fuqaha, and Junaid
Qadir.

</span>
<span class="ltx_bibblock">Collaborative federated learning for healthcare: Multi-modal covid-19
diagnosis at the edge.

</span>
<span class="ltx_bibblock"><em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.07511</em>, 2021.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ronneberger et al.(2015)Ronneberger, Fischer, and
Brox]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical image computing and
computer-assisted intervention</em>, pages 234–241. Springer, 2015.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Samala et al.(2018)Samala, Chan, Hadjiiski, Helvie, Richter, and
Cha]</span>
<span class="ltx_bibblock">
Ravi K Samala, Heang-Ping Chan, Lubomir Hadjiiski, Mark A Helvie, Caleb
Richter, and Kenny Cha.

</span>
<span class="ltx_bibblock">Cross-domain and multi-task transfer learning of deep convolutional
neural network for breast cancer diagnosis in digital breast tomosynthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Medical Imaging 2018: Computer-Aided Diagnosis</em>, volume
10575, page 105750Q. International Society for Optics and Photonics, 2018.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sarma et al.(2021)Sarma, Harmon, Sanford, Roth, Xu, Tetreault, Xu,
Flores, Raman, Kulkarni, et al.]</span>
<span class="ltx_bibblock">
Karthik V Sarma, Stephanie Harmon, Thomas Sanford, Holger R Roth, Ziyue Xu,
Jesse Tetreault, Daguang Xu, Mona G Flores, Alex G Raman, Rushikesh Kulkarni,
et al.

</span>
<span class="ltx_bibblock">Federated learning improves site performance in multicenter deep
learning without data sharing.

</span>
<span class="ltx_bibblock"><em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association</em>,
28(6):1259–1264, 2021.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Shen et al.(2021)Shen, Wang, Roth, Yang, Xu, Oda, Wang, Fuh, Chen,
Liu, Liao, and Mori]</span>
<span class="ltx_bibblock">
Chen Shen, Pochuan Wang, Holger R. Roth, Dong Yang, Daguang Xu, Masahiro Oda,
Weichung Wang, Chiou-Shann Fuh, Po-Ting Chen, Kao-Lang Liu, Wei-Chih Liao,
and Kensaku Mori.

</span>
<span class="ltx_bibblock">Multi-task federated learning for heterogeneous pancreas
segmentation, 2021.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Suzuki(2017)]</span>
<span class="ltx_bibblock">
Kenji Suzuki.

</span>
<span class="ltx_bibblock">Overview of deep learning in medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Radiological physics and technology</em>, 10(3):257–273, 2017.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tomczak et al.(2020)Tomczak, Ilic, Marquardt, Engel, Forster, Navab,
and Albarqouni]</span>
<span class="ltx_bibblock">
Agnieszka Tomczak, Slobodan Ilic, Gaby Marquardt, Thomas Engel, Frank Forster,
Nassir Navab, and Shadi Albarqouni.

</span>
<span class="ltx_bibblock">Multi-task multi-domain learning for digital staining and
classification of leukocytes.

</span>
<span class="ltx_bibblock"><em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.10000" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.10001" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.10001">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.10001" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.10002" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 19:11:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
