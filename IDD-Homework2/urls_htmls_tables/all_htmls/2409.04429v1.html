<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.04429] VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</title><meta property="og:description" content="VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.04429">

<!--Generated on Sun Oct  6 00:19:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yecheng Wu<sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Zhuoyang Zhang<sup id="id19.19.id2" class="ltx_sup"><span id="id19.19.id2.1" class="ltx_text ltx_font_italic">2,3</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>  Junyu Chen<sup id="id20.20.id3" class="ltx_sup"><span id="id20.20.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Haotian Tang<sup id="id21.21.id4" class="ltx_sup"><span id="id21.21.id4.1" class="ltx_text ltx_font_italic">2,3</span></sup>  
<br class="ltx_break"><span id="id5.5.1" class="ltx_text ltx_font_bold">Dacheng Li<sup id="id5.5.1.1" class="ltx_sup"><span id="id5.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3,4</span></sup></span>  <span id="id6.6.2" class="ltx_text ltx_font_bold">Yunhao Fang<sup id="id6.6.2.1" class="ltx_sup"><span id="id6.6.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3,5</span></sup></span>  <span id="id7.7.3" class="ltx_text ltx_font_bold">Ligeng Zhu<sup id="id7.7.3.1" class="ltx_sup"><span id="id7.7.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>  <span id="id8.8.4" class="ltx_text ltx_font_bold">Enze Xie<sup id="id8.8.4.1" class="ltx_sup"><span id="id8.8.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>  
<br class="ltx_break"><span id="id9.9.5" class="ltx_text ltx_font_bold">Hongxu Yin<sup id="id9.9.5.1" class="ltx_sup"><span id="id9.9.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>  <span id="id10.10.6" class="ltx_text ltx_font_bold">Li Yi<sup id="id10.10.6.1" class="ltx_sup"><span id="id10.10.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>  <span id="id11.11.7" class="ltx_text ltx_font_bold">Song Han<sup id="id11.11.7.1" class="ltx_sup"><span id="id11.11.7.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup></span>  <span id="id12.12.8" class="ltx_text ltx_font_bold">Yao Lu<sup id="id12.12.8.1" class="ltx_sup"><span id="id12.12.8.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span> 
<br class="ltx_break">Tsinghua University<sup id="id22.22.id5" class="ltx_sup"><span id="id22.22.id5.1" class="ltx_text ltx_font_italic">1</span></sup> MIT<sup id="id23.23.id6" class="ltx_sup"><span id="id23.23.id6.1" class="ltx_text ltx_font_italic">2</span></sup> NVIDIA<sup id="id24.24.id7" class="ltx_sup"><span id="id24.24.id7.1" class="ltx_text ltx_font_italic">3</span></sup> 
<br class="ltx_break">UC Berkeley<sup id="id25.25.id8" class="ltx_sup"><span id="id25.25.id8.1" class="ltx_text ltx_font_italic">4</span></sup> UC San Diego<sup id="id26.26.id9" class="ltx_sup"><span id="id26.26.id9.1" class="ltx_text ltx_font_italic">5</span></sup>
</span><span class="ltx_author_notes"> equal contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id27.id1" class="ltx_p"><span id="id27.id1.1" class="ltx_text ltx_font_bold">VILA-U</span> is a <span id="id27.id1.2" class="ltx_text ltx_font_bold">U</span>nified foundation model that integrates <span id="id27.id1.3" class="ltx_text ltx_font_bold">V</span>ideo, <span id="id27.id1.4" class="ltx_text ltx_font_bold">I</span>mage, <span id="id27.id1.5" class="ltx_text ltx_font_bold">La</span>nguage understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, large language models (LLMs) have demonstrated superior capabilities in various language tasks. Their appealing properties like instruction following, zero-shot generalization, and few-shot in-context learning motivate researchers to combine them with vision models to build visual language models (VLMs) for multi-modal tasks. Many efforts <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">instructblip</span> </a>; <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">llava</span> </a>; <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lin2023vila</span> </a></cite> have been put into this field, achieving remarkable performance on visual language understanding benchmarks. In these works, visual inputs are projected onto LLMs’ semantic space through a vision foundation model like CLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">clip</span> </a></cite> to bridge two modalities by including text-image alignment training objectives.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In addition to visual understanding, another essential research direction in combining visual and language modalities is visual generation. There are two popular approaches for text-guided image generation. One approach employs diffusion models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ldm</span> </a></cite>, a powerful tool for various generation tasks. The other line of work converts visual content into discrete tokens through vector quantization (VQ) and then leveraging autoregressive transformers for high-quality and diverse generation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">VQGAN</span> </a>; <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">vit-vqgan</span> </a>; <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Witnessing the rapid advancements in both visual understanding and generation, an emerging trend is to unify these techniques into a single multi-modal framework. There are two main approaches to achieving such unification. Many VLMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">jin2023unified</span> </a>; <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">li2024mgm</span> </a>; <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu</span> </a>; <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu2</span> </a></cite> maintain an understanding-oriented framework and offload the generation task to an external diffusion model. This disjoint approach adds complexity to infrastructure design. Available large-scale foundation model training pipelines and deployment systems have already been highly optimized for language modeling with next-token prediction. Designing a new stack to support diffusion models would incur significant engineering costs. To circumvent such costs, it is desirable to design a single <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">end-to-end autoregressive</span> framework for both image understanding and generation. There is a trend in VLMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a>; <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">CM3Leon</span> </a></cite> that adopt VQ encoders to convert visual inputs into discrete tokens and treat them in the same next-token prediction manner as language data. However, replacing continuous tokens with VQ tokens in VLMs usually results in a severe performance drop in downstream visual perception tasks. Other works <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Unified-io2</span> </a>; <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chameleon</span> </a></cite> have to make various architectural modifications and conduct multi-modal training from scratch, which is computationally expensive.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we present <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">VILA-U</span>, an <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">end-to-end autoregressive</span> framework with a unified next-token prediction objective for both visual and text inputs that can achieve competitive performance on both visual language understanding and generation tasks, without the help of external components like diffusion models.
We identify two critical principles to unify vision and language modalities effectively and efficiently. (1) Existing end-to-end autoregressive VLMs cannot achieve competitive visual understanding performance because the discrete VQ tokens are trained solely on image reconstruction loss and are not aligned with textual inputs. Therefore, it is crucial to introduce text alignment during VQ vision tower pretraining to enhance perception capabilities. (2) Autoregressive image generation can attain similar quality as diffusion models if trained on a high-quality data corpus with sufficient size. Guided by these insights, VILA-U features a unified foundation vision tower that converts visual inputs into discrete tokens through vector quantization and aligns these tokens with textual inputs using contrastive learning. The multi-modal training of VILA-U takes advantage of a unified next-token prediction objective for both visual and textual tokens on a small-size high-quality image-text corpus.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We evaluate VILA-U on common visual language tasks, including image-language understanding, video-language understanding, image generation and video generation. VILA-U significantly narrows the gap in visual understanding performance between end-to-end autoregressive models and continuous-token VLMs, while introducing competitive <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">native</span> visual generation capabilities.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Large Language Models (LLMs).</span> LLMs based on pre-trained large-scale transformers <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">NIPS2017Transformer</span> </a></cite> has drastically revolutionized natural language processing field. Featuring gigantic model size and pre-training data corpus, LLM has achieved remarkable performance on various linguistic tasks. The development of open-source LLMs such as LLaMA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite>, Mixtral <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">mixtral</span> </a></cite> and Vicuna <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">vicuna</span> </a></cite> has furthered nourished research on how to adopt LLM for complex language tasks. Besides excellent zero-shot generalizability to diverse domains, LLM is commonly finetuned on custom datasets for better performance on specific tasks. Instruction tuning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ChatGPT</span> </a>; <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chung2024scaling</span> </a>; <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ouyang2022training</span> </a></cite> also stands as a key step for better outputs in applying LLMs. In this work, we adopt the LLaMA-2-7B<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">llama</span> </a></cite> model as our basic LLM.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Visual Language Models (VLMs).</span> Combining computer vision and natural language processing gives rise to VLM in this LLM era. In VLMs, researchers leverage vision foundation models such as CLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">clip</span> </a></cite>, BLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">blip</span> </a></cite> and CoCa <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">CoCa</span> </a></cite> to extract visual features, align with texts, and feed them into LLM to achieve the cross-modality understanding between texts and visual content. Building upon such progress, many VLMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">alayrac2022flamingo</span> </a>; <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">blip2</span> </a>; <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">llava</span> </a>; <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lin2023vila</span> </a></cite> have been designed and trained on extensive vision-language data to achieve remarkable performance on visual understanding and reasoning tasks. VLMs are divided into two types. (1) <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">BLIP-style</span> VLMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">openflamingo</span> </a>; <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">alayrac2022flamingo</span> </a>; <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">li2022blip</span> </a>; <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">li2023blip</span> </a>; <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Dai2023InstructBLIP</span> </a>; <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">hong2023cogagent</span> </a></cite> utilizes cross attention mechanism to fuse language and visual information and optionally apply perceivers <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">jaegle2021perceiver</span> </a></cite> to downsample visual tokens. (2) <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">LLaVA-style</span> VLMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">liu2023llava</span> </a>; <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">driess2023palm</span> </a>; <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chen2023pali</span> </a>; <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">fuyu</span> </a>; <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">zhu2023minigpt</span> </a>; <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ye2023mplug</span> </a>; <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">qwen</span> </a>; <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">aiello2023jointly</span> </a>; <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chen2023internvl</span> </a>; <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">liu2023improved</span> </a>; <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lin2023vila</span> </a>; <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">zhang2023internlm</span> </a></cite> converts visual inputs to tokens (patches) and pass them through ViTs. The output of ViTs undergoes MLP layers and gets aligned to the language space.
In this work, we aim to develop a VLM with visual understanding capacities comparable to prior works, while also possessing the new capacity of visual generation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Unified Visual Language Models.</span> Numerous efforts have been made to develop unified visual language models capable of generating both text and visual content, including images and videos. There are two mainstream methods to generate visual content in VLMs. Many works <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu</span> </a>; <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu2</span> </a>; <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">jin2023unified</span> </a>; <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-lavit</span> </a>; <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">li2024mgm</span> </a></cite> combine VLMs with diffusion models like Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ldm</span> </a></cite> for high-quality image generation. Other works <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a>; <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">CM3Leon</span> </a>; <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Unified-io2</span> </a>; <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chameleon</span> </a>; <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xie2024show</span> </a></cite> adopt VQGAN-based vision encoders to convert visual inputs into discrete tokens and make LLMs learn to predict them.
In this work, we design our framework based on the autoregressive next-token prediction method for visual generation and make our VLM learn to generate visual content effectively and efficiently.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.04429/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">An overview of our framework’s multi-modal training and inference process.<span id="S3.F1.4.2.1" class="ltx_text ltx_font_medium"> Visual inputs are tokenized into discrete tokens and concatenated with textual tokens to form a multi-modal token sequence. All tokens are involved in our next-token prediction process, enabling a unified training objective. During inference, the output tokens are decoded by our text detokenizer or vision tower decoder to yield multi-modal content. </span></span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This work proposes a multi-modal framework that aims to unify visual and language modalities efficiently and effectively. The key components enabling such unification are a unified foundation vision tower that converts visual inputs into discrete tokens aligned with text, and a unified multi-modal generative training procedure. An overview of the main multi-modal training and inference process within our framework is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methods ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Unified Foundation Vision Tower</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To support diverse visual understanding and generation tasks, we first build a unified foundation vision tower to provide appropriate visual features. We propose to include text-image contrastive loss and VQ-based image reconstruction loss in our vision tower training, empowering the text alignment and discrete tokenization abilities for our vision tower. As depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ Unified Training Recipe. ‣ 3.1 Unified Foundation Vision Tower ‣ 3 Methods ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the features extracted from images are primarily discretized through residual quantization. Then in one route, the discrete visual features are fed into a decoder to reconstruct the image and compute the reconstruction loss; on the other route, we compute the image-text contrastive loss between the discrete visual features and the textual features provided by a text encoder. With this training procedure, the vision tower learns to extract discrete features suitable for both understanding and generation in our VLM.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Unified Training Recipe.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">A straightforward combination of contrastive and reconstruction loss cannot converge. This is because alignment and reconstruction tasks require high-level semantic and low-level appearance features, respectively. Training the entire vision tower from scratch with both objectives could induce conflicting goals. In practice, we observe that training the vector-quantized vision tower from scratch with both image reconstruction and contrastive loss results in a mere 5% Top-1 accuracy for zero-shot image classification on ImageNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">deng2009imagenet</span> </a></cite> after several epochs of training.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">To address this issue, we experiment with different training recipes and find the following solution to be most effective. Instead of learning both objectives simultaneously, our training recipe suggests first equipping the model with text-image alignment ability and then learning reconstruction while maintaining alignment ability. We initialize the vision encoder and text encoder with pretrained weights from the CLIP model to ensure good text-image alignment. Next, we freeze the text encoder and keep all vision components trainable using both contrastive and reconstruction loss. The contrastive loss maintains alignment ability, while the reconstruction loss develops reconstruction ability. This training approach converges quickly and yields strong performance. The pre-trained CLIP weights contain learned high-level priors, which are difficult and computationally expensive to learn from scratch. Initializing with these weights enables the binding of low-level and high-level features much faster and more tractably for the vision encoder. With this training recipe, we can efficiently train a vision tower that exhibits both good text alignment and image reconstruction abilities. We use weighted sum to combine the text-image contrastive loss and VQ-based image reconstruction loss:</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{total}=w_{contra}\mathcal{L}_{contra}+w_{recon}\mathcal{L}_{recon}" display="inline"><semantics id="S3.E1X.2.1.1.m1.1a"><mrow id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml"><msub id="S3.E1X.2.1.1.m1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.1.1.2.2" xref="S3.E1X.2.1.1.m1.1.1.2.2.cmml">ℒ</mi><mrow id="S3.E1X.2.1.1.m1.1.1.2.3" xref="S3.E1X.2.1.1.m1.1.1.2.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.2.3.2" xref="S3.E1X.2.1.1.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.2.3.1" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.3" xref="S3.E1X.2.1.1.m1.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.2.3.1a" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.4" xref="S3.E1X.2.1.1.m1.1.1.2.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.2.3.1b" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.5" xref="S3.E1X.2.1.1.m1.1.1.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.2.3.1c" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.6" xref="S3.E1X.2.1.1.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S3.E1X.2.1.1.m1.1.1.1" xref="S3.E1X.2.1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.1.1.3" xref="S3.E1X.2.1.1.m1.1.1.3.cmml"><mrow id="S3.E1X.2.1.1.m1.1.1.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.cmml"><msub id="S3.E1X.2.1.1.m1.1.1.3.2.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.2.cmml">w</mi><mrow id="S3.E1X.2.1.1.m1.1.1.3.2.2.3" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1a" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.4" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1b" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.5" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1c" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.6" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1d" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.7" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.7.cmml">a</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.1" xref="S3.E1X.2.1.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S3.E1X.2.1.1.m1.1.1.3.2.3" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.1.1.3.2.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.2.cmml">ℒ</mi><mrow id="S3.E1X.2.1.1.m1.1.1.3.2.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1a" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.4" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1b" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.5" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1c" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.6" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1d" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.7" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.7.cmml">a</mi></mrow></msub></mrow><mo id="S3.E1X.2.1.1.m1.1.1.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1X.2.1.1.m1.1.1.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.cmml"><msub id="S3.E1X.2.1.1.m1.1.1.3.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.2" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.2.cmml">w</mi><mrow id="S3.E1X.2.1.1.m1.1.1.3.3.2.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1a" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.4" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1b" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.5" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1c" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.6" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.6.cmml">n</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E1X.2.1.1.m1.1.1.3.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.1.1.3.3.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="S3.E1X.2.1.1.m1.1.1.3.3.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1a" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.4" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1b" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.5" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1c" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.6" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.6.cmml">n</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.1b"><apply id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1"><eq id="S3.E1X.2.1.1.m1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1"></eq><apply id="S3.E1X.2.1.1.m1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.2">ℒ</ci><apply id="S3.E1X.2.1.1.m1.1.1.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3"><times id="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.2">𝑡</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.3">𝑜</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.4">𝑡</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.5">𝑎</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.6">𝑙</ci></apply></apply><apply id="S3.E1X.2.1.1.m1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3"><plus id="S3.E1X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.1"></plus><apply id="S3.E1X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2"><times id="S3.E1X.2.1.1.m1.1.1.3.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.1"></times><apply id="S3.E1X.2.1.1.m1.1.1.3.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.3.2.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.2">𝑤</ci><apply id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3"><times id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.2">𝑐</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.3">𝑜</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.4">𝑛</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.5">𝑡</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.6">𝑟</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.2.3.7.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.2.3.7">𝑎</ci></apply></apply><apply id="S3.E1X.2.1.1.m1.1.1.3.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.3.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.2">ℒ</ci><apply id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3"><times id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.2">𝑐</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.3">𝑜</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.4">𝑛</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.5">𝑡</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.6">𝑟</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.2.3.3.7.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2.3.3.7">𝑎</ci></apply></apply></apply><apply id="S3.E1X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3"><times id="S3.E1X.2.1.1.m1.1.1.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.1"></times><apply id="S3.E1X.2.1.1.m1.1.1.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.3.3.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.2">𝑤</ci><apply id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3"><times id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.2">𝑟</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.3">𝑒</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.4">𝑐</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.5">𝑜</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.2.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.2.3.6">𝑛</ci></apply></apply><apply id="S3.E1X.2.1.1.m1.1.1.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.3.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.2">ℒ</ci><apply id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3"><times id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.2">𝑟</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.3">𝑒</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.4">𝑐</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.5">𝑜</ci><ci id="S3.E1X.2.1.1.m1.1.1.3.3.3.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3.3.3.6">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{total}=w_{contra}\mathcal{L}_{contra}+w_{recon}\mathcal{L}_{recon}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px1.p4" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p4.2" class="ltx_p">In our experiments, we pick <math id="S3.SS1.SSS0.Px1.p4.1.m1.1" class="ltx_Math" alttext="w_{contra}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p4.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p4.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.2.cmml">w</mi><mrow id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1a" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.4" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1b" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.5" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1c" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.6" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1d" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.7" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.7.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p4.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.2">𝑤</ci><apply id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.4">𝑛</ci><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.5">𝑡</ci><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.6.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.6">𝑟</ci><ci id="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.7.cmml" xref="S3.SS1.SSS0.Px1.p4.1.m1.1.1.3.7">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p4.1.m1.1c">w_{contra}</annotation></semantics></math> = 1 and <math id="S3.SS1.SSS0.Px1.p4.2.m2.1" class="ltx_Math" alttext="w_{recon}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p4.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p4.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.2.cmml">w</mi><mrow id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.2" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.3" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1a" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.4" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1b" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.5" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1c" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.6" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p4.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.2">𝑤</ci><apply id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3"><times id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.2">𝑟</ci><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.4">𝑐</ci><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.5.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.5">𝑜</ci><ci id="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.6.cmml" xref="S3.SS1.SSS0.Px1.p4.2.m2.1.1.3.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p4.2.m2.1c">w_{recon}</annotation></semantics></math> = 1.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.04429/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">
<span id="S3.F2.4.2.1" class="ltx_text ltx_font_bold">Overview of our unified foundation vision tower.</span> Given input images the features extracted by the vision encoder are discretized using residual quantization. Then the discrete vision features are meanwhile put into the vision decoder to reconstruct images and used to perform the text-image alignment. During this process, the reconstruction loss and contrastive loss are computed to update the vision tower, endowing it to produce discrete visual features with text alignment.</span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Discussion: Failed Training Recipes.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">We experiment with numerous training recipes and find none to be as effective as our final approach. We list four alternative recipes and discuss their shortcomings compared to our final recipe: (1) Load pre-trained CLIP weights into the text encoder only; (2) Load pre-trained RQ-VAE weights for the vision encoder and decoder while training other parts from scratch; (3) Freeze the vision encoder; (4) Make the text encoder trainable.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">Recipes 1) and 2) fail due to the lack of pre-trained CLIP weights for the vision encoder. Training a CLIP model from scratch typically requires numerous GPU days with a large global batch size (e.g., 32k). However, VQ-based reconstruction training necessitates a relatively small global batch size (e.g., 512) for steady improvement. With such a small batch size, training a text-aligned vision tower from scratch would be prohibitively time-consuming and resource-intensive.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p3.1" class="ltx_p">Recipe 3) fails because freezing the vision encoder prevents it from learning the low-level features essential for reconstruction. In this case, the burden of reconstruction falls entirely on the vision decoder, but it is impossible to reconstruct images well using only semantic features.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p4.1" class="ltx_p">Recipe 4) fails because the quantized features are chaotic during the initial training steps, and the contrastive loss disrupts the text encoder weights, slowing down the entire training process.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p5" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p5.1" class="ltx_p">In contrast, our final training recipe leverages pre-trained CLIP weights for the vision encoder, enabling it to maintain learned semantic features rather than grasping them from scratch. This allows us to train with a small batch size while keeping the vision encoder trainable, facilitating the learning of low-level features for reconstruction during training.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Residual Vector Quantization.</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.2" class="ltx_p">Our visual features are discretely quantized, so their representation ability heavily depends on the code size used in our quantizer. Since we hope they contain both high-level and low-level features, we need more capacities in their vector feature space, making a larger code size necessary for good performance in downstream tasks. However, too many codes for each image will result in too many tokens for LLM to produce in the visual generation process, incurring much latency.
So in an attempt to increase the vector feature capacity and meanwhile maintain a reasonable number of tokens for LLM, we adopt a residual vector quantization method following RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite> to discretize a vector <math id="S3.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">\mathbf{z}</annotation></semantics></math> as <math id="S3.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">D</annotation></semantics></math> discrete codes:</p>
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2X.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{R}\mathcal{Q}(\mathbf{z};\mathcal{C},D)" display="inline"><semantics id="S3.E2X.2.1.1.m1.3a"><mrow id="S3.E2X.2.1.1.m1.3.4" xref="S3.E2X.2.1.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.3.4.2" xref="S3.E2X.2.1.1.m1.3.4.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.4.1" xref="S3.E2X.2.1.1.m1.3.4.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.3.4.3" xref="S3.E2X.2.1.1.m1.3.4.3.cmml">𝒬</mi><mo lspace="0em" rspace="0em" id="S3.E2X.2.1.1.m1.3.4.1a" xref="S3.E2X.2.1.1.m1.3.4.1.cmml">​</mo><mrow id="S3.E2X.2.1.1.m1.3.4.4.2" xref="S3.E2X.2.1.1.m1.3.4.4.1.cmml"><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.4.4.2.1" xref="S3.E2X.2.1.1.m1.3.4.4.1.cmml">(</mo><mi id="S3.E2X.2.1.1.m1.1.1" xref="S3.E2X.2.1.1.m1.1.1.cmml">𝐳</mi><mo id="S3.E2X.2.1.1.m1.3.4.4.2.2" xref="S3.E2X.2.1.1.m1.3.4.4.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.2.2" xref="S3.E2X.2.1.1.m1.2.2.cmml">𝒞</mi><mo id="S3.E2X.2.1.1.m1.3.4.4.2.3" xref="S3.E2X.2.1.1.m1.3.4.4.1.cmml">,</mo><mi id="S3.E2X.2.1.1.m1.3.3" xref="S3.E2X.2.1.1.m1.3.3.cmml">D</mi><mo stretchy="false" id="S3.E2X.2.1.1.m1.3.4.4.2.4" xref="S3.E2X.2.1.1.m1.3.4.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.3b"><apply id="S3.E2X.2.1.1.m1.3.4.cmml" xref="S3.E2X.2.1.1.m1.3.4"><times id="S3.E2X.2.1.1.m1.3.4.1.cmml" xref="S3.E2X.2.1.1.m1.3.4.1"></times><ci id="S3.E2X.2.1.1.m1.3.4.2.cmml" xref="S3.E2X.2.1.1.m1.3.4.2">ℛ</ci><ci id="S3.E2X.2.1.1.m1.3.4.3.cmml" xref="S3.E2X.2.1.1.m1.3.4.3">𝒬</ci><list id="S3.E2X.2.1.1.m1.3.4.4.1.cmml" xref="S3.E2X.2.1.1.m1.3.4.4.2"><ci id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">𝐳</ci><ci id="S3.E2X.2.1.1.m1.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2">𝒞</ci><ci id="S3.E2X.2.1.1.m1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3">𝐷</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.3c">\displaystyle\mathcal{R}\mathcal{Q}(\mathbf{z};\mathcal{C},D)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2X.3.2.2.m1.3" class="ltx_Math" alttext="\displaystyle=\left(k_{1},\cdots,k_{D}\right)\in[K]^{D}," display="inline"><semantics id="S3.E2X.3.2.2.m1.3a"><mrow id="S3.E2X.3.2.2.m1.3.3.1" xref="S3.E2X.3.2.2.m1.3.3.1.1.cmml"><mrow id="S3.E2X.3.2.2.m1.3.3.1.1" xref="S3.E2X.3.2.2.m1.3.3.1.1.cmml"><mi id="S3.E2X.3.2.2.m1.3.3.1.1.4" xref="S3.E2X.3.2.2.m1.3.3.1.1.4.cmml"></mi><mo id="S3.E2X.3.2.2.m1.3.3.1.1.5" xref="S3.E2X.3.2.2.m1.3.3.1.1.5.cmml">=</mo><mrow id="S3.E2X.3.2.2.m1.3.3.1.1.2.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml"><mo id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.3" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml">(</mo><msub id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.2.cmml">k</mi><mn id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.3" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.4" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E2X.3.2.2.m1.1.1" xref="S3.E2X.3.2.2.m1.1.1.cmml">⋯</mi><mo id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.5" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml">,</mo><msub id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.cmml"><mi id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.2.cmml">k</mi><mi id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.3" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.3.cmml">D</mi></msub><mo id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.6" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml">)</mo></mrow><mo id="S3.E2X.3.2.2.m1.3.3.1.1.6" xref="S3.E2X.3.2.2.m1.3.3.1.1.6.cmml">∈</mo><msup id="S3.E2X.3.2.2.m1.3.3.1.1.7" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.cmml"><mrow id="S3.E2X.3.2.2.m1.3.3.1.1.7.2.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.2.1.cmml"><mo stretchy="false" id="S3.E2X.3.2.2.m1.3.3.1.1.7.2.2.1" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.2.1.1.cmml">[</mo><mi id="S3.E2X.3.2.2.m1.2.2" xref="S3.E2X.3.2.2.m1.2.2.cmml">K</mi><mo stretchy="false" id="S3.E2X.3.2.2.m1.3.3.1.1.7.2.2.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.2.1.1.cmml">]</mo></mrow><mi id="S3.E2X.3.2.2.m1.3.3.1.1.7.3" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.3.cmml">D</mi></msup></mrow><mo id="S3.E2X.3.2.2.m1.3.3.1.2" xref="S3.E2X.3.2.2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.3.2.2.m1.3b"><apply id="S3.E2X.3.2.2.m1.3.3.1.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1"><and id="S3.E2X.3.2.2.m1.3.3.1.1a.cmml" xref="S3.E2X.3.2.2.m1.3.3.1"></and><apply id="S3.E2X.3.2.2.m1.3.3.1.1b.cmml" xref="S3.E2X.3.2.2.m1.3.3.1"><eq id="S3.E2X.3.2.2.m1.3.3.1.1.5.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.5"></eq><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.3.3.1.1.4.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.4">absent</csymbol><vector id="S3.E2X.3.2.2.m1.3.3.1.1.2.3.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2"><apply id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.2">𝑘</ci><cn type="integer" id="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.1.1.1.3">1</cn></apply><ci id="S3.E2X.3.2.2.m1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1">⋯</ci><apply id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.2">𝑘</ci><ci id="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.2.2.2.3">𝐷</ci></apply></vector></apply><apply id="S3.E2X.3.2.2.m1.3.3.1.1c.cmml" xref="S3.E2X.3.2.2.m1.3.3.1"><in id="S3.E2X.3.2.2.m1.3.3.1.1.6.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.6"></in><share href="#S3.E2X.3.2.2.m1.3.3.1.1.2.cmml" id="S3.E2X.3.2.2.m1.3.3.1.1d.cmml" xref="S3.E2X.3.2.2.m1.3.3.1"></share><apply id="S3.E2X.3.2.2.m1.3.3.1.1.7.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.7"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.3.3.1.1.7.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.7">superscript</csymbol><apply id="S3.E2X.3.2.2.m1.3.3.1.1.7.2.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.2.2"><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.3.3.1.1.7.2.1.1.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.2.2.1">delimited-[]</csymbol><ci id="S3.E2X.3.2.2.m1.2.2.cmml" xref="S3.E2X.3.2.2.m1.2.2">𝐾</ci></apply><ci id="S3.E2X.3.2.2.m1.3.3.1.1.7.3.cmml" xref="S3.E2X.3.2.2.m1.3.3.1.1.7.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.3.2.2.m1.3c">\displaystyle=\left(k_{1},\cdots,k_{D}\right)\in[K]^{D},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.SSS0.Px3.p1.8" class="ltx_p">where <math id="S3.SS1.SSS0.Px3.p1.3.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px3.p1.3.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m1.1b"><ci id="S3.SS1.SSS0.Px3.p1.3.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m1.1c">\mathcal{C}</annotation></semantics></math> is the codebook, <math id="S3.SS1.SSS0.Px3.p1.4.m2.1" class="ltx_Math" alttext="K=|\mathcal{C}|" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.4.m2.1a"><mrow id="S3.SS1.SSS0.Px3.p1.4.m2.1.2" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.2" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.2.cmml">K</mi><mo id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.1" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.2" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.2.1" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px3.p1.4.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.1.cmml">𝒞</mi><mo stretchy="false" id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.2.2" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.4.m2.1b"><apply id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2"><eq id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.2">𝐾</ci><apply id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.2"><abs id="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.2.3.2.1"></abs><ci id="S3.SS1.SSS0.Px3.p1.4.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m2.1.1">𝒞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.4.m2.1c">K=|\mathcal{C}|</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px3.p1.5.m3.1" class="ltx_Math" alttext="k_{d}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.5.m3.1a"><msub id="S3.SS1.SSS0.Px3.p1.5.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.2" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1.2.cmml">k</mi><mi id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.3" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.5.m3.1b"><apply id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1.2">𝑘</ci><ci id="S3.SS1.SSS0.Px3.p1.5.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.5.m3.1c">k_{d}</annotation></semantics></math> is the code of <math id="S3.SS1.SSS0.Px3.p1.6.m4.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.6.m4.1a"><mi id="S3.SS1.SSS0.Px3.p1.6.m4.1.1" xref="S3.SS1.SSS0.Px3.p1.6.m4.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.6.m4.1b"><ci id="S3.SS1.SSS0.Px3.p1.6.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m4.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.6.m4.1c">\mathbf{z}</annotation></semantics></math> at depth <math id="S3.SS1.SSS0.Px3.p1.7.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.7.m5.1a"><mi id="S3.SS1.SSS0.Px3.p1.7.m5.1.1" xref="S3.SS1.SSS0.Px3.p1.7.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.7.m5.1b"><ci id="S3.SS1.SSS0.Px3.p1.7.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.7.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.7.m5.1c">d</annotation></semantics></math>. Starting with <math id="S3.SS1.SSS0.Px3.p1.8.m6.1" class="ltx_Math" alttext="\mathbf{r}_{0}=\mathbf{z}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.8.m6.1a"><mrow id="S3.SS1.SSS0.Px3.p1.8.m6.1.1" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.cmml"><msub id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.2" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.2.cmml">𝐫</mi><mn id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.3" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.1" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.1.cmml">=</mo><mi id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.3" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.3.cmml">𝐳</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.8.m6.1b"><apply id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1"><eq id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.1"></eq><apply id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.2">𝐫</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.2.3">0</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.8.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m6.1.1.3">𝐳</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.8.m6.1c">\mathbf{r}_{0}=\mathbf{z}</annotation></semantics></math>, we recursively perform vector quantization by</p>
<table id="S3.E3" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E3X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle k_{d}" display="inline"><semantics id="S3.E3X.2.1.1.m1.1a"><msub id="S3.E3X.2.1.1.m1.1.1" xref="S3.E3X.2.1.1.m1.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.2.cmml">k</mi><mi id="S3.E3X.2.1.1.m1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.1b"><apply id="S3.E3X.2.1.1.m1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.2">𝑘</ci><ci id="S3.E3X.2.1.1.m1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.1c">\displaystyle k_{d}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3X.3.2.2.m1.2" class="ltx_Math" alttext="\displaystyle=\mathcal{Q}\left(\mathbf{r}_{d-1},\mathcal{C}\right)," display="inline"><semantics id="S3.E3X.3.2.2.m1.2a"><mrow id="S3.E3X.3.2.2.m1.2.2.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.cmml"><mrow id="S3.E3X.3.2.2.m1.2.2.1.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.2.2.1.1.3" xref="S3.E3X.3.2.2.m1.2.2.1.1.3.cmml"></mi><mo id="S3.E3X.3.2.2.m1.2.2.1.1.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3X.3.2.2.m1.2.2.1.1.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.3.2.2.m1.2.2.1.1.1.3" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.3.cmml">𝒬</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.2.2.1.1.1.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">𝐫</mi><mrow id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.2.cmml">d</mi><mo id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.1" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3X.3.2.2.m1.1.1" xref="S3.E3X.3.2.2.m1.1.1.cmml">𝒞</mi><mo id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.4" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3X.3.2.2.m1.2.2.1.2" xref="S3.E3X.3.2.2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.3.2.2.m1.2b"><apply id="S3.E3X.3.2.2.m1.2.2.1.1.cmml" xref="S3.E3X.3.2.2.m1.2.2.1"><eq id="S3.E3X.3.2.2.m1.2.2.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E3X.3.2.2.m1.2.2.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.3">absent</csymbol><apply id="S3.E3X.3.2.2.m1.2.2.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1"><times id="S3.E3X.3.2.2.m1.2.2.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.2"></times><ci id="S3.E3X.3.2.2.m1.2.2.1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.3">𝒬</ci><interval closure="open" id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1"><apply id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.2">𝐫</ci><apply id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3"><minus id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.1"></minus><ci id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.2">𝑑</ci><cn type="integer" id="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E3X.3.2.2.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E3X.3.2.2.m1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1">𝒞</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.3.2.2.m1.2c">\displaystyle=\mathcal{Q}\left(\mathbf{r}_{d-1},\mathcal{C}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr id="S3.E3Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{r}_{d}" display="inline"><semantics id="S3.E3Xa.2.1.1.m1.1a"><msub id="S3.E3Xa.2.1.1.m1.1.1" xref="S3.E3Xa.2.1.1.m1.1.1.cmml"><mi id="S3.E3Xa.2.1.1.m1.1.1.2" xref="S3.E3Xa.2.1.1.m1.1.1.2.cmml">𝐫</mi><mi id="S3.E3Xa.2.1.1.m1.1.1.3" xref="S3.E3Xa.2.1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3Xa.2.1.1.m1.1b"><apply id="S3.E3Xa.2.1.1.m1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E3Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.2">𝐫</ci><ci id="S3.E3Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.2.1.1.m1.1c">\displaystyle\mathbf{r}_{d}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3Xa.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\mathbf{r}_{d-1}-\mathbf{e}\left(k_{d}\right)," display="inline"><semantics id="S3.E3Xa.3.2.2.m1.1a"><mrow id="S3.E3Xa.3.2.2.m1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.cmml"><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.3.cmml"></mi><mo id="S3.E3Xa.3.2.2.m1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.cmml"><msub id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.2.cmml">𝐫</mi><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.2.cmml">d</mi><mo id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml">𝐞</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3Xa.3.2.2.m1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3Xa.3.2.2.m1.1b"><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1"><eq id="S3.E3Xa.3.2.2.m1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E3Xa.3.2.2.m1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.3">absent</csymbol><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1"><minus id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.2"></minus><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.2">𝐫</ci><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3"><minus id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.1"></minus><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.2">𝑑</ci><cn type="integer" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1"><times id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3">𝐞</ci><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.3.2.2.m1.1c">\displaystyle=\mathbf{r}_{d-1}-\mathbf{e}\left(k_{d}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.SSS0.Px3.p1.11" class="ltx_p">for each depth <math id="S3.SS1.SSS0.Px3.p1.9.m1.4" class="ltx_Math" alttext="d=1,2,\cdots,D" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.9.m1.4a"><mrow id="S3.SS1.SSS0.Px3.p1.9.m1.4.5" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.cmml"><mi id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.2" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.2.cmml">d</mi><mo id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.1" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.2" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.9.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.9.m1.1.1.cmml">1</mn><mo id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.2.1" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.1.cmml">,</mo><mn id="S3.SS1.SSS0.Px3.p1.9.m1.2.2" xref="S3.SS1.SSS0.Px3.p1.9.m1.2.2.cmml">2</mn><mo id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.2.2" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS0.Px3.p1.9.m1.3.3" xref="S3.SS1.SSS0.Px3.p1.9.m1.3.3.cmml">⋯</mi><mo id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.2.3" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.9.m1.4.4" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.4.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.9.m1.4b"><apply id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5"><eq id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.2">𝑑</ci><list id="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.5.3.2"><cn type="integer" id="S3.SS1.SSS0.Px3.p1.9.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.1.1">1</cn><cn type="integer" id="S3.SS1.SSS0.Px3.p1.9.m1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.2.2">2</cn><ci id="S3.SS1.SSS0.Px3.p1.9.m1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.3.3">⋯</ci><ci id="S3.SS1.SSS0.Px3.p1.9.m1.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m1.4.4">𝐷</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.9.m1.4c">d=1,2,\cdots,D</annotation></semantics></math>, where <math id="S3.SS1.SSS0.Px3.p1.10.m2.1" class="ltx_Math" alttext="\mathbf{e}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.10.m2.1a"><mi id="S3.SS1.SSS0.Px3.p1.10.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.10.m2.1.1.cmml">𝐞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.10.m2.1b"><ci id="S3.SS1.SSS0.Px3.p1.10.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.10.m2.1.1">𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.10.m2.1c">\mathbf{e}</annotation></semantics></math> is the codebook embedding table and <math id="S3.SS1.SSS0.Px3.p1.11.m3.1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.11.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px3.p1.11.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.11.m3.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.11.m3.1b"><ci id="S3.SS1.SSS0.Px3.p1.11.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m3.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.11.m3.1c">\mathcal{Q}</annotation></semantics></math> is the standard vector quantization:</p>
<table id="S3.E4" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E4X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4X.2.1.1.m1.5" class="ltx_Math" alttext="\displaystyle\mathcal{Q}(\mathbf{z};\mathcal{C})=\underset{k\in[K]}{\arg\min}\|\mathbf{z}-\mathbf{e}(k)\|_{2}^{2}." display="inline"><semantics id="S3.E4X.2.1.1.m1.5a"><mrow id="S3.E4X.2.1.1.m1.5.5.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4X.2.1.1.m1.5.5.1.1.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.2.cmml">𝒬</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.3.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.1.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.1.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.2.2" xref="S3.E4X.2.1.1.m1.2.2.cmml">𝐳</mi><mo id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S3.E4X.2.1.1.m1.3.3" xref="S3.E4X.2.1.1.m1.3.3.cmml">𝒞</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.2.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.5.5.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.cmml"><munder accentunder="true" id="S3.E4X.2.1.1.m1.1.1" xref="S3.E4X.2.1.1.m1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.2.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.2.1" xref="S3.E4X.2.1.1.m1.1.1.2.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E4X.2.1.1.m1.1.1.2a" xref="S3.E4X.2.1.1.m1.1.1.2.cmml">⁡</mo><mi id="S3.E4X.2.1.1.m1.1.1.2.2" xref="S3.E4X.2.1.1.m1.1.1.2.2.cmml">min</mi></mrow><mrow id="S3.E4X.2.1.1.m1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.1.1.1.3" xref="S3.E4X.2.1.1.m1.1.1.1.3.cmml">k</mi><mo id="S3.E4X.2.1.1.m1.1.1.1.2" xref="S3.E4X.2.1.1.m1.1.1.1.2.cmml">∈</mo><mrow id="S3.E4X.2.1.1.m1.1.1.1.4.2" xref="S3.E4X.2.1.1.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.1.1.1.4.2.1" xref="S3.E4X.2.1.1.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S3.E4X.2.1.1.m1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.cmml">K</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.1.1.1.4.2.2" xref="S3.E4X.2.1.1.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></munder><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.2.cmml">​</mo><msubsup id="S3.E4X.2.1.1.m1.5.5.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">𝐳</mi><mo id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml">𝐞</mi><mo lspace="0em" rspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.1" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.4.4" xref="S3.E4X.2.1.1.m1.4.4.cmml">k</mi><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo lspace="0em" id="S3.E4X.2.1.1.m1.5.5.1.2" xref="S3.E4X.2.1.1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4X.2.1.1.m1.5b"><apply id="S3.E4X.2.1.1.m1.5.5.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1"><eq id="S3.E4X.2.1.1.m1.5.5.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.2"></eq><apply id="S3.E4X.2.1.1.m1.5.5.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3"><times id="S3.E4X.2.1.1.m1.5.5.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.1"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.2">𝒬</ci><list id="S3.E4X.2.1.1.m1.5.5.1.1.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.3.3.2"><ci id="S3.E4X.2.1.1.m1.2.2.cmml" xref="S3.E4X.2.1.1.m1.2.2">𝐳</ci><ci id="S3.E4X.2.1.1.m1.3.3.cmml" xref="S3.E4X.2.1.1.m1.3.3">𝒞</ci></list></apply><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.2"></times><apply id="S3.E4X.2.1.1.m1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1"><apply id="S3.E4X.2.1.1.m1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1"><in id="S3.E4X.2.1.1.m1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.2"></in><ci id="S3.E4X.2.1.1.m1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.3">𝑘</ci><apply id="S3.E4X.2.1.1.m1.1.1.1.4.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.4.2"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.1.1.1.4.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S3.E4X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1">𝐾</ci></apply></apply><apply id="S3.E4X.2.1.1.m1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.2"><arg id="S3.E4X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.2.1"></arg><min id="S3.E4X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E4X.2.1.1.m1.1.1.2.2"></min></apply></apply><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1">superscript</csymbol><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1">subscript</csymbol><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1"><minus id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.2">𝐳</ci><apply id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3"><times id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.1.1.1.3.2">𝐞</ci><ci id="S3.E4X.2.1.1.m1.4.4.cmml" xref="S3.E4X.2.1.1.m1.4.4">𝑘</ci></apply></apply></apply><cn type="integer" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E4X.2.1.1.m1.5.5.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.5.5.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4X.2.1.1.m1.5c">\displaystyle\mathcal{Q}(\mathbf{z};\mathcal{C})=\underset{k\in[K]}{\arg\min}\|\mathbf{z}-\mathbf{e}(k)\|_{2}^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.SSS0.Px3.p1.14" class="ltx_p">The quantized vector for <math id="S3.SS1.SSS0.Px3.p1.12.m1.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.12.m1.1a"><mi id="S3.SS1.SSS0.Px3.p1.12.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.12.m1.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.12.m1.1b"><ci id="S3.SS1.SSS0.Px3.p1.12.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m1.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.12.m1.1c">\mathbf{z}</annotation></semantics></math> is the sum over the depth dim: <math id="S3.SS1.SSS0.Px3.p1.13.m2.1" class="ltx_Math" alttext="\widehat{\mathbf{z}}=\sum_{i=1}^{D}\mathbf{e}\left(k_{i}\right)" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.13.m2.1a"><mrow id="S3.SS1.SSS0.Px3.p1.13.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.cmml"><mover accent="true" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.2.cmml">𝐳</mi><mo id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.1.cmml">^</mo></mover><mo rspace="0.111em" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.2.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.cmml"><msubsup id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.cmml"><mo id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.3.cmml">D</mi></msubsup><mrow id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.3.cmml">𝐞</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.2.cmml">k</mi><mi id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.13.m2.1b"><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1"><eq id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.2"></eq><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3"><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.1">^</ci><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.3.2">𝐳</ci></apply><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1"><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2">superscript</csymbol><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2">subscript</csymbol><sum id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.2"></sum><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3"><eq id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.2.3">𝐷</ci></apply><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1"><times id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.2"></times><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.3">𝐞</ci><apply id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m2.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.13.m2.1c">\widehat{\mathbf{z}}=\sum_{i=1}^{D}\mathbf{e}\left(k_{i}\right)</annotation></semantics></math>. Intuitively, in each depth we choose a code to reduce the quantization error. So compared to the standard vector quantization methods, we have <math id="S3.SS1.SSS0.Px3.p1.14.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.14.m3.1a"><mi id="S3.SS1.SSS0.Px3.p1.14.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.14.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.14.m3.1b"><ci id="S3.SS1.SSS0.Px3.p1.14.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.14.m3.1c">D</annotation></semantics></math> codes to quantize one vector, allowing for finer approximation and larger feature space. During multi-modal training and inference, LLM only needs to predict the code embedding, with codes in different depth sequentially produced by a depth transformer taking the code embedding as the initial input, as we will introduce in Section <a href="#S3.SS2" title="3.2 Unified Multi-modal Generative Pre-training ‣ 3 Methods ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. So with this residual quantization, we can enhance the representation capability of our vision tower while incurring little latency.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Unified Multi-modal Generative Pre-training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methods ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an overview of our unified multi-modal pre-training process. Our vision tower encoder processes visual inputs sequentially, generating a 1D token sequence. This sequence is then concatenated with text tokens to form a multi-modal sequence. To distinguish between modalities and enable visual content generation, we insert special tokens: <span id="S3.SS2.p1.4.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;image_start&gt;</span> and <span id="S3.SS2.p1.4.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;image_end&gt;</span> at the start and end of image tokens, and <span id="S3.SS2.p1.4.3" class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;video_start&gt;</span> and <span id="S3.SS2.p1.4.4" class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;video_end&gt;</span> at the start and end of video tokens. Video tokens are the direct concatenation of multi-frame image tokens.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Pre-training data form.</span> In terms of unified pre-training data, we leverage different concatenation forms between text and visual tokens to facilitate both understanding and generation. We use <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">[image, text]</span>, <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">[text, image]</span>, and <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">[text, video]</span> forms, with supervision loss added only on the latter modality in each pair to avoid unconditional content generation and promote modality alignment. We also employ an interleaved text and image concatenation form for enhanced understanding, with supervision loss applied solely to the text. Notably, we exclude the <span id="S3.SS2.p2.1.5" class="ltx_text ltx_font_typewriter">[video, text]</span> form during pre-training for efficiency reasons, as we find incorporating it during supervised fine-tuning effectively yields excellent video understanding ability.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Training Objective.</span> Since both visual tokens and text tokens are discrete, we can train our LLM with the general language modeling next-token prediction objective. However, due to the use of residual quantization for visual tokens, the training objectives for text and visual tokens differ slightly. For text tokens, the negative log-likelihood loss is calculated as</p>
<table id="S3.E5" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E5X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{text}}=-\sum_{i=1}^{T}\log P_{\theta}\left(y_{i}|y_{&lt;i}\right)," display="inline"><semantics id="S3.E5X.2.1.1.m1.1a"><mrow id="S3.E5X.2.1.1.m1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S3.E5X.2.1.1.m1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.cmml"><msub id="S3.E5X.2.1.1.m1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5X.2.1.1.m1.1.1.1.1.3.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E5X.2.1.1.m1.1.1.1.1.3.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.3a.cmml">text</mtext></msub><mo id="S3.E5X.2.1.1.m1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml"><mo id="S3.E5X.2.1.1.m1.1.1.1.1.1a" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><munderover id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2a" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml">T</mi></munderover></mstyle><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3a" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E5X.2.1.1.m1.1.1.1.2" xref="S3.E5X.2.1.1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5X.2.1.1.m1.1b"><apply id="S3.E5X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1"><eq id="S3.E5X.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.2"></eq><apply id="S3.E5X.2.1.1.m1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.2">ℒ</ci><ci id="S3.E5X.2.1.1.m1.1.1.1.1.3.3a.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E5X.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.3.3">text</mtext></ci></apply><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1"><minus id="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1"></minus><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1"><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1"><times id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.2"></times><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3"><log id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.1"></log><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.2">𝑃</ci><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝑦</ci><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{\text{text}}=-\sum_{i=1}^{T}\log P_{\theta}\left(y_{i}|y_{&lt;i}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(5)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.11" class="ltx_p">where <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">T</annotation></semantics></math> is the length of the multi-modal sequence and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">i</annotation></semantics></math> only counts when the text token appears at position <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">i</annotation></semantics></math>.
For visual tokens, residual quantization introduces a depth-stacked structure of codes at each visual position <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">j</annotation></semantics></math>. To address this, we leverage the depth transformer introduced in RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>. Specifically, given the code embedding <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="h_{j}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">h</mi><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ℎ</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">h_{j}</annotation></semantics></math> generated by the LLM for visual tokens at position <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">j</annotation></semantics></math>, the depth transformer autoregressively predicts D residual tokens (<math id="S3.SS2.p4.7.m7.1" class="ltx_Math" alttext="k_{j1}" display="inline"><semantics id="S3.SS2.p4.7.m7.1a"><msub id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml">k</mi><mrow id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml"><mi id="S3.SS2.p4.7.m7.1.1.3.2" xref="S3.SS2.p4.7.m7.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.7.m7.1.1.3.1" xref="S3.SS2.p4.7.m7.1.1.3.1.cmml">​</mo><mn id="S3.SS2.p4.7.m7.1.1.3.3" xref="S3.SS2.p4.7.m7.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2">𝑘</ci><apply id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3"><times id="S3.SS2.p4.7.m7.1.1.3.1.cmml" xref="S3.SS2.p4.7.m7.1.1.3.1"></times><ci id="S3.SS2.p4.7.m7.1.1.3.2.cmml" xref="S3.SS2.p4.7.m7.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS2.p4.7.m7.1.1.3.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">k_{j1}</annotation></semantics></math>, …, <math id="S3.SS2.p4.8.m8.1" class="ltx_Math" alttext="k_{jD}" display="inline"><semantics id="S3.SS2.p4.8.m8.1a"><msub id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml"><mi id="S3.SS2.p4.8.m8.1.1.2" xref="S3.SS2.p4.8.m8.1.1.2.cmml">k</mi><mrow id="S3.SS2.p4.8.m8.1.1.3" xref="S3.SS2.p4.8.m8.1.1.3.cmml"><mi id="S3.SS2.p4.8.m8.1.1.3.2" xref="S3.SS2.p4.8.m8.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.8.m8.1.1.3.1" xref="S3.SS2.p4.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.8.m8.1.1.3.3" xref="S3.SS2.p4.8.m8.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m8.1.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p4.8.m8.1.1.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2">𝑘</ci><apply id="S3.SS2.p4.8.m8.1.1.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3"><times id="S3.SS2.p4.8.m8.1.1.3.1.cmml" xref="S3.SS2.p4.8.m8.1.1.3.1"></times><ci id="S3.SS2.p4.8.m8.1.1.3.2.cmml" xref="S3.SS2.p4.8.m8.1.1.3.2">𝑗</ci><ci id="S3.SS2.p4.8.m8.1.1.3.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">k_{jD}</annotation></semantics></math>). During training, the input of the depth transformer <math id="S3.SS2.p4.9.m9.1" class="ltx_Math" alttext="v_{jd}" display="inline"><semantics id="S3.SS2.p4.9.m9.1a"><msub id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml">v</mi><mrow id="S3.SS2.p4.9.m9.1.1.3" xref="S3.SS2.p4.9.m9.1.1.3.cmml"><mi id="S3.SS2.p4.9.m9.1.1.3.2" xref="S3.SS2.p4.9.m9.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.9.m9.1.1.3.1" xref="S3.SS2.p4.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.9.m9.1.1.3.3" xref="S3.SS2.p4.9.m9.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2">𝑣</ci><apply id="S3.SS2.p4.9.m9.1.1.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3"><times id="S3.SS2.p4.9.m9.1.1.3.1.cmml" xref="S3.SS2.p4.9.m9.1.1.3.1"></times><ci id="S3.SS2.p4.9.m9.1.1.3.2.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2">𝑗</ci><ci id="S3.SS2.p4.9.m9.1.1.3.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">v_{jd}</annotation></semantics></math> at depth d is defined as the sum of the code embeddings of up to depth <math id="S3.SS2.p4.10.m10.1" class="ltx_Math" alttext="d-1" display="inline"><semantics id="S3.SS2.p4.10.m10.1a"><mrow id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml"><mi id="S3.SS2.p4.10.m10.1.1.2" xref="S3.SS2.p4.10.m10.1.1.2.cmml">d</mi><mo id="S3.SS2.p4.10.m10.1.1.1" xref="S3.SS2.p4.10.m10.1.1.1.cmml">−</mo><mn id="S3.SS2.p4.10.m10.1.1.3" xref="S3.SS2.p4.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><apply id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1"><minus id="S3.SS2.p4.10.m10.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1.1"></minus><ci id="S3.SS2.p4.10.m10.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.2">𝑑</ci><cn type="integer" id="S3.SS2.p4.10.m10.1.1.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">d-1</annotation></semantics></math> for <math id="S3.SS2.p4.11.m11.1" class="ltx_Math" alttext="d&gt;1" display="inline"><semantics id="S3.SS2.p4.11.m11.1a"><mrow id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml"><mi id="S3.SS2.p4.11.m11.1.1.2" xref="S3.SS2.p4.11.m11.1.1.2.cmml">d</mi><mo id="S3.SS2.p4.11.m11.1.1.1" xref="S3.SS2.p4.11.m11.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p4.11.m11.1.1.3" xref="S3.SS2.p4.11.m11.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.1b"><apply id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1"><gt id="S3.SS2.p4.11.m11.1.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1.1"></gt><ci id="S3.SS2.p4.11.m11.1.1.2.cmml" xref="S3.SS2.p4.11.m11.1.1.2">𝑑</ci><cn type="integer" id="S3.SS2.p4.11.m11.1.1.3.cmml" xref="S3.SS2.p4.11.m11.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.1c">d&gt;1</annotation></semantics></math> such that</p>
<table id="S3.E6" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E6X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle v_{jd}=\sum_{d^{\prime}=1}^{d-1}\mathbf{e}(k_{jd^{\prime}})," display="inline"><semantics id="S3.E6X.2.1.1.m1.1a"><mrow id="S3.E6X.2.1.1.m1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S3.E6X.2.1.1.m1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml"><msub id="S3.E6X.2.1.1.m1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2.cmml">v</mi><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.3.cmml">d</mi></mrow></msub><mo id="S3.E6X.2.1.1.m1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.cmml"><munderover id="S3.E6X.2.1.1.m1.1.1.1.1.1.2a" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.cmml"><msup id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.2.cmml">d</mi><mo id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.3.cmml">′</mo></msup><mo id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.2.cmml">d</mi><mo id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></munderover></mstyle><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.3.cmml">𝐞</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">k</mi><mrow id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><msup id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">d</mi><mo id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">′</mo></msup></mrow></msub><mo stretchy="false" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E6X.2.1.1.m1.1.1.1.2" xref="S3.E6X.2.1.1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6X.2.1.1.m1.1b"><apply id="S3.E6X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1"><eq id="S3.E6X.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.2"></eq><apply id="S3.E6X.2.1.1.m1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.2">𝑣</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3"><times id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.1"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.2">𝑗</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.3.3.3">𝑑</ci></apply></apply><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1"><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3"><eq id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.1"></eq><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2">superscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.2">𝑑</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.2.3">′</ci></apply><cn type="integer" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3"><minus id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.2">𝑑</ci><cn type="integer" id="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.2"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.3">𝐞</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2">𝑘</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2">𝑗</ci><apply id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.2">𝑑</ci><ci id="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E6X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.3">′</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6X.2.1.1.m1.1c">\displaystyle v_{jd}=\sum_{d^{\prime}=1}^{d-1}\mathbf{e}(k_{jd^{\prime}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(6)</span></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.p4.14" class="ltx_p">and <math id="S3.SS2.p4.12.m1.1" class="ltx_Math" alttext="v_{j1}=h_{j}" display="inline"><semantics id="S3.SS2.p4.12.m1.1a"><mrow id="S3.SS2.p4.12.m1.1.1" xref="S3.SS2.p4.12.m1.1.1.cmml"><msub id="S3.SS2.p4.12.m1.1.1.2" xref="S3.SS2.p4.12.m1.1.1.2.cmml"><mi id="S3.SS2.p4.12.m1.1.1.2.2" xref="S3.SS2.p4.12.m1.1.1.2.2.cmml">v</mi><mrow id="S3.SS2.p4.12.m1.1.1.2.3" xref="S3.SS2.p4.12.m1.1.1.2.3.cmml"><mi id="S3.SS2.p4.12.m1.1.1.2.3.2" xref="S3.SS2.p4.12.m1.1.1.2.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.12.m1.1.1.2.3.1" xref="S3.SS2.p4.12.m1.1.1.2.3.1.cmml">​</mo><mn id="S3.SS2.p4.12.m1.1.1.2.3.3" xref="S3.SS2.p4.12.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p4.12.m1.1.1.1" xref="S3.SS2.p4.12.m1.1.1.1.cmml">=</mo><msub id="S3.SS2.p4.12.m1.1.1.3" xref="S3.SS2.p4.12.m1.1.1.3.cmml"><mi id="S3.SS2.p4.12.m1.1.1.3.2" xref="S3.SS2.p4.12.m1.1.1.3.2.cmml">h</mi><mi id="S3.SS2.p4.12.m1.1.1.3.3" xref="S3.SS2.p4.12.m1.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m1.1b"><apply id="S3.SS2.p4.12.m1.1.1.cmml" xref="S3.SS2.p4.12.m1.1.1"><eq id="S3.SS2.p4.12.m1.1.1.1.cmml" xref="S3.SS2.p4.12.m1.1.1.1"></eq><apply id="S3.SS2.p4.12.m1.1.1.2.cmml" xref="S3.SS2.p4.12.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m1.1.1.2.1.cmml" xref="S3.SS2.p4.12.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.12.m1.1.1.2.2.cmml" xref="S3.SS2.p4.12.m1.1.1.2.2">𝑣</ci><apply id="S3.SS2.p4.12.m1.1.1.2.3.cmml" xref="S3.SS2.p4.12.m1.1.1.2.3"><times id="S3.SS2.p4.12.m1.1.1.2.3.1.cmml" xref="S3.SS2.p4.12.m1.1.1.2.3.1"></times><ci id="S3.SS2.p4.12.m1.1.1.2.3.2.cmml" xref="S3.SS2.p4.12.m1.1.1.2.3.2">𝑗</ci><cn type="integer" id="S3.SS2.p4.12.m1.1.1.2.3.3.cmml" xref="S3.SS2.p4.12.m1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.SS2.p4.12.m1.1.1.3.cmml" xref="S3.SS2.p4.12.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.12.m1.1.1.3.1.cmml" xref="S3.SS2.p4.12.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.12.m1.1.1.3.2.cmml" xref="S3.SS2.p4.12.m1.1.1.3.2">ℎ</ci><ci id="S3.SS2.p4.12.m1.1.1.3.3.cmml" xref="S3.SS2.p4.12.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m1.1c">v_{j1}=h_{j}</annotation></semantics></math>. Thus, the depth transformer predicts the next code for a finer estimation of the feature <math id="S3.SS2.p4.13.m2.1" class="ltx_Math" alttext="\boldsymbol{\hat{z}}_{j}" display="inline"><semantics id="S3.SS2.p4.13.m2.1a"><msub id="S3.SS2.p4.13.m2.1.1" xref="S3.SS2.p4.13.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p4.13.m2.1.1.2" xref="S3.SS2.p4.13.m2.1.1.2.cmml"><mi id="S3.SS2.p4.13.m2.1.1.2.2" xref="S3.SS2.p4.13.m2.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p4.13.m2.1.1.2.1" xref="S3.SS2.p4.13.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.13.m2.1.1.3" xref="S3.SS2.p4.13.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.13.m2.1b"><apply id="S3.SS2.p4.13.m2.1.1.cmml" xref="S3.SS2.p4.13.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.13.m2.1.1.1.cmml" xref="S3.SS2.p4.13.m2.1.1">subscript</csymbol><apply id="S3.SS2.p4.13.m2.1.1.2.cmml" xref="S3.SS2.p4.13.m2.1.1.2"><ci id="S3.SS2.p4.13.m2.1.1.2.1.cmml" xref="S3.SS2.p4.13.m2.1.1.2.1">bold-^</ci><ci id="S3.SS2.p4.13.m2.1.1.2.2.cmml" xref="S3.SS2.p4.13.m2.1.1.2.2">𝒛</ci></apply><ci id="S3.SS2.p4.13.m2.1.1.3.cmml" xref="S3.SS2.p4.13.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.13.m2.1c">\boldsymbol{\hat{z}}_{j}</annotation></semantics></math> based on the previous estimations up to <math id="S3.SS2.p4.14.m3.1" class="ltx_Math" alttext="d-1" display="inline"><semantics id="S3.SS2.p4.14.m3.1a"><mrow id="S3.SS2.p4.14.m3.1.1" xref="S3.SS2.p4.14.m3.1.1.cmml"><mi id="S3.SS2.p4.14.m3.1.1.2" xref="S3.SS2.p4.14.m3.1.1.2.cmml">d</mi><mo id="S3.SS2.p4.14.m3.1.1.1" xref="S3.SS2.p4.14.m3.1.1.1.cmml">−</mo><mn id="S3.SS2.p4.14.m3.1.1.3" xref="S3.SS2.p4.14.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.14.m3.1b"><apply id="S3.SS2.p4.14.m3.1.1.cmml" xref="S3.SS2.p4.14.m3.1.1"><minus id="S3.SS2.p4.14.m3.1.1.1.cmml" xref="S3.SS2.p4.14.m3.1.1.1"></minus><ci id="S3.SS2.p4.14.m3.1.1.2.cmml" xref="S3.SS2.p4.14.m3.1.1.2">𝑑</ci><cn type="integer" id="S3.SS2.p4.14.m3.1.1.3.cmml" xref="S3.SS2.p4.14.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.14.m3.1c">d-1</annotation></semantics></math>. Then the negative log-likelihood loss for visual tokens is</p>
<table id="S3.E7" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E7X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E7X.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{visual}}=-\sum_{j=1}^{T}\sum_{d=1}^{D}\log P_{\delta}\left(k_{jd}|k_{j,&lt;d}\right)," display="inline"><semantics id="S3.E7X.2.1.1.m1.3a"><mrow id="S3.E7X.2.1.1.m1.3.3.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S3.E7X.2.1.1.m1.3.3.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.cmml"><msub id="S3.E7X.2.1.1.m1.3.3.1.1.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7X.2.1.1.m1.3.3.1.1.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E7X.2.1.1.m1.3.3.1.1.3.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.3a.cmml">visual</mtext></msub><mo id="S3.E7X.2.1.1.m1.3.3.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.cmml"><mo id="S3.E7X.2.1.1.m1.3.3.1.1.1a" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.cmml">−</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.cmml"><munderover id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2a" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.3.cmml">T</mi></munderover></mstyle><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.cmml"><munderover id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2a" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.2.cmml">d</mi><mo id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.3.cmml">D</mi></munderover></mstyle><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3a" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.3.cmml">δ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">k</mi><mrow id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml">d</mi></mrow></msub><mo fence="false" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">k</mi><mrow id="S3.E7X.2.1.1.m1.2.2.2.2" xref="S3.E7X.2.1.1.m1.2.2.2.3.cmml"><mi id="S3.E7X.2.1.1.m1.1.1.1.1" xref="S3.E7X.2.1.1.m1.1.1.1.1.cmml">j</mi><mo id="S3.E7X.2.1.1.m1.2.2.2.2.2" xref="S3.E7X.2.1.1.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E7X.2.1.1.m1.2.2.2.2.1" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.cmml"><mi id="S3.E7X.2.1.1.m1.2.2.2.2.1.2" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.2.cmml"></mi><mo id="S3.E7X.2.1.1.m1.2.2.2.2.1.1" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.1.cmml">&lt;</mo><mi id="S3.E7X.2.1.1.m1.2.2.2.2.1.3" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.3.cmml">d</mi></mrow></mrow></msub></mrow><mo id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E7X.2.1.1.m1.3.3.1.2" xref="S3.E7X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7X.2.1.1.m1.3b"><apply id="S3.E7X.2.1.1.m1.3.3.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1"><eq id="S3.E7X.2.1.1.m1.3.3.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.2"></eq><apply id="S3.E7X.2.1.1.m1.3.3.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.3.3.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.2">ℒ</ci><ci id="S3.E7X.2.1.1.m1.3.3.1.1.3.3a.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.3"><mtext mathsize="70%" id="S3.E7X.2.1.1.m1.3.3.1.1.3.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.3.3">visual</mtext></ci></apply><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1"><minus id="S3.E7X.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1"></minus><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1"><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3"><eq id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.2">𝑗</ci><cn type="integer" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1"><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.2"></sum><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3"><eq id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.2">𝑑</ci><cn type="integer" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.2.3">𝐷</ci></apply><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1"><times id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3"><log id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.1"></log><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.2">𝑃</ci><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.3.2.3">𝛿</ci></apply></apply><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">𝑘</ci><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3"><times id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.2">𝑗</ci><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.3">𝑑</ci></apply></apply><apply id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝑘</ci><list id="S3.E7X.2.1.1.m1.2.2.2.3.cmml" xref="S3.E7X.2.1.1.m1.2.2.2.2"><ci id="S3.E7X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E7X.2.1.1.m1.1.1.1.1">𝑗</ci><apply id="S3.E7X.2.1.1.m1.2.2.2.2.1.cmml" xref="S3.E7X.2.1.1.m1.2.2.2.2.1"><lt id="S3.E7X.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.1"></lt><csymbol cd="latexml" id="S3.E7X.2.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.2">absent</csymbol><ci id="S3.E7X.2.1.1.m1.2.2.2.2.1.3.cmml" xref="S3.E7X.2.1.1.m1.2.2.2.2.1.3">𝑑</ci></apply></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7X.2.1.1.m1.3c">\displaystyle\mathcal{L}_{\text{visual}}=-\sum_{j=1}^{T}\sum_{d=1}^{D}\log P_{\delta}\left(k_{jd}|k_{j,&lt;d}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.3" class="ltx_p">where <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">T</annotation></semantics></math> is the length of the multi-modal sequence and <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">j</annotation></semantics></math> only counts when a visual token appears at position <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">j</annotation></semantics></math>. During the multi-modal pre-training, the weights of the depth transformer are randomly initialized and updated together with the LLM.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we introduce comprehensive experiments to evaluate the performance of our method on various visual understanding and generation tasks. Firstly, we outline our experimental setup, including the model architecture, training datasets, and evaluation benchmarks. Subsequently, we evaluate the performance of our unified foundation vision tower. Then, we compare our method with other popular VLMs on various visual understanding and generation benchmarks. Finally, we give some qualitative results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">In our experiments, we employ LLaMA-2-7B <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">touvron2023llama</span> </a></cite> as our base language model. For the vision tower, we choose SigLIP-Large-patch16-256 / SigLIP-SO400M-patch14-384 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">zhai2023sigmoid</span> </a></cite> as our vision encoder architecture, and adopt the residual quantizer, depth transformer as well as the decoder architecture from RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>. The quantizer codebook size is 16384. All images and videos are resized to a resolution of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">256\times 256</annotation></semantics></math> / <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="384\times 384" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">384</cn><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">384\times 384</annotation></semantics></math>, with each image or video frame converted into a <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="16\times 16\times 4" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mn id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.3.m3.1.1.1a" xref="S4.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.3.m3.1.1.4" xref="S4.SS1.p1.3.m3.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><times id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">16</cn><cn type="integer" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">16</cn><cn type="integer" id="S4.SS1.p1.3.m3.1.1.4.cmml" xref="S4.SS1.p1.3.m3.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">16\times 16\times 4</annotation></semantics></math> / <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="27\times 27\times 16" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mn id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">27</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">27</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.4.m4.1.1.1a" xref="S4.SS1.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.4.m4.1.1.4" xref="S4.SS1.p1.4.m4.1.1.4.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">27</cn><cn type="integer" id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">27</cn><cn type="integer" id="S4.SS1.p1.4.m4.1.1.4.cmml" xref="S4.SS1.p1.4.m4.1.1.4">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">27\times 27\times 16</annotation></semantics></math> code with the residual depth <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="D=4" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">D</mi><mo id="S4.SS1.p1.5.m5.1.1.1" xref="S4.SS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><eq id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1.1"></eq><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">D=4</annotation></semantics></math> / <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="D=16" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mrow id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">D</mi><mo id="S4.SS1.p1.6.m6.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><eq id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1"></eq><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">D=16</annotation></semantics></math>. We train our vision tower on COYO-700M <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">coyo-700m</span> </a></cite> and evaluate it for zero-shot classification and reconstruction performance on ImageNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">imagenet</span> </a></cite>. For visual understanding, we leverage 1M <span id="S4.SS1.p1.6.1" class="ltx_text ltx_font_typewriter">[image, text]</span> data from ShareGPT4V <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">sharegpt4v</span> </a></cite>, 6M interleaved text and image data from MMC4 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">mmc4</span> </a></cite>. For visual generation, we incorporate 15M high-quality <span id="S4.SS1.p1.6.2" class="ltx_text ltx_font_typewriter">[text, image]</span> data curated from our internal dataset and 1M <span id="S4.SS1.p1.6.3" class="ltx_text ltx_font_typewriter">[text, video]</span> data from OpenVid <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">nan2024openvid</span> </a></cite> datasets. Classifier-free guidance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">cfg</span> </a></cite> is employed for visual generation with a CFG value of 3.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For examining visual understanding ability, we evaluate our model on the widely adopted zero-shot image-based visual-language benchmarks including VQA-v2 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">vqa_v2</span> </a></cite>, GQA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">gqa</span> </a></cite>, TextVQA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">textvqa</span> </a></cite>, POPE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">POPE</span> </a></cite>, MME <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">mme</span> </a></cite>, SEED <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">seed</span> </a></cite>, MM-Vet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">mm-vet</span> </a></cite> and video-based visual-language benchmarks including ActivityNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">activitynet</span> </a></cite>, MSVD <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chen2011collecting</span> </a></cite>, MSRVTT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">MSVD</span> </a></cite>, TGIF <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">tgif</span> </a></cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To evaluate the visual generation capability, we use MJHQ-30K <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">playgroundv2.5</span> </a></cite> and GenAI-Bench <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">GenAI-Bench</span> </a></cite> as our benchmarks. The former adopts the FID between generated images and 30K high-quality images to reflect the overall capability of image generation. The latter is a challenging image-to-text generation benchmark that reflects the comprehensive generative abilities of visual generation models. This benchmark is divided into two categories of prompts: <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">basic</span> skills, which include attribute, scene, and relation understanding in text inputs, and <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">advanced</span> skills, which encompass counting, differentiation, comparison, and logical relation understanding in text inputs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Unified Foundation Vision Tower</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We present the commonly used metrics reconstruction FID (rFID) and Top-1 accuracy for zero-shot image classification on ImageNet to measure the reconstruction and text alignment capabilities of the unified foundation vision tower in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Unified Foundation Vision Tower ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our model achieves significantly better reconstruction results than VQ-GAN. Our rFID is slightly inferior to that of RQ-VAE when using the same code shape. This is expected as the introduction of contrastive loss during training, aimed at enhancing image understanding, led to a decrease in reconstruction quality. For the text alignment capability, our unified vision tower achieves a Top-1 accuracy of 73.3 / 78.0 under 256 / 384 resolution. This demonstrates the exceptional text alignment capability of our unified vision tower. However, it is worth noting that both the rFID and Top-1 accuracy of the vision tower only serves as a medium indicator instead of directly linear correlated to the final performance of our whole multi-modal framework. The performance on visual understanding and generation tasks presented in the following sections is more important.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.18.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.19.2" class="ltx_text" style="font-size:90%;">The reconstruction FID (rFID) and Top-1 accuracy for zero-shot image classification of our unified vision tower on ImageNet.</span></figcaption>
<div id="S4.T1.16" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:586.8pt;height:98.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.6pt,5.4pt) scale(0.9,0.9) ;">
<table id="S4.T1.16.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.2.2.2.3.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.2.2.2.4.1" class="ltx_text ltx_font_bold">Pretrained Weights</span></td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.2.2.2.5.1" class="ltx_text ltx_font_bold">Resolution</span></td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.2.2.2.6.1" class="ltx_text ltx_font_bold">Shape of Code</span></td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">rFID<math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.2.2.2.2.1" class="ltx_text ltx_font_bold">Top-1 Accuracy<math id="S4.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">VQ-GAN <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">esser2021taming</span> </a></cite>
</th>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">256 <math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><times id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\times</annotation></semantics></math> 256</td>
<td id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">16 <math id="S4.T1.4.4.4.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.4.4.4.2.m1.1a"><mo id="S4.T1.4.4.4.2.m1.1.1" xref="S4.T1.4.4.4.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.2.m1.1b"><times id="S4.T1.4.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.2.m1.1c">\times</annotation></semantics></math> 16</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">4.98</td>
<td id="S4.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T1.7.7.7" class="ltx_tr">
<th id="S4.T1.7.7.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>
</th>
<td id="S4.T1.7.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T1.5.5.5.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">256 <math id="S4.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.5.5.5.1.m1.1a"><mo id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><times id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">\times</annotation></semantics></math> 256</td>
<td id="S4.T1.7.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">8 <math id="S4.T1.6.6.6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.6.6.6.2.m1.1a"><mo id="S4.T1.6.6.6.2.m1.1.1" xref="S4.T1.6.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.2.m1.1b"><times id="S4.T1.6.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.2.m1.1c">\times</annotation></semantics></math> 8 <math id="S4.T1.7.7.7.3.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.7.7.7.3.m2.1a"><mo id="S4.T1.7.7.7.3.m2.1.1" xref="S4.T1.7.7.7.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.3.m2.1b"><times id="S4.T1.7.7.7.3.m2.1.1.cmml" xref="S4.T1.7.7.7.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.3.m2.1c">\times</annotation></semantics></math> 4</td>
<td id="S4.T1.7.7.7.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">3.20</td>
<td id="S4.T1.7.7.7.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T1.10.10.10" class="ltx_tr">
<th id="S4.T1.10.10.10.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>
</th>
<td id="S4.T1.10.10.10.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T1.8.8.8.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">256 <math id="S4.T1.8.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.8.8.8.1.m1.1a"><mo id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.1b"><times id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.1c">\times</annotation></semantics></math> 256</td>
<td id="S4.T1.10.10.10.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">16 <math id="S4.T1.9.9.9.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.9.9.9.2.m1.1a"><mo id="S4.T1.9.9.9.2.m1.1.1" xref="S4.T1.9.9.9.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.2.m1.1b"><times id="S4.T1.9.9.9.2.m1.1.1.cmml" xref="S4.T1.9.9.9.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.2.m1.1c">\times</annotation></semantics></math> 16 <math id="S4.T1.10.10.10.3.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.10.10.10.3.m2.1a"><mo id="S4.T1.10.10.10.3.m2.1.1" xref="S4.T1.10.10.10.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.3.m2.1b"><times id="S4.T1.10.10.10.3.m2.1.1.cmml" xref="S4.T1.10.10.10.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.3.m2.1c">\times</annotation></semantics></math> 4</td>
<td id="S4.T1.10.10.10.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.30</td>
<td id="S4.T1.10.10.10.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T1.13.13.13" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.13.13.13.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.13.13.13.4.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<td id="S4.T1.13.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.13.13.13.5.1" class="ltx_text" style="background-color:#E6E6E6;">SigLIP-Large</span></td>
<td id="S4.T1.11.11.11.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.11.11.11.1.1" class="ltx_text" style="background-color:#E6E6E6;">256 <math id="S4.T1.11.11.11.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.11.11.11.1.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.11.11.11.1.1.m1.1.1" xref="S4.T1.11.11.11.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.1.m1.1b"><times id="S4.T1.11.11.11.1.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.1.m1.1c">\times</annotation></semantics></math> 256</span></td>
<td id="S4.T1.13.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.13.13.13.3.2" class="ltx_text" style="background-color:#E6E6E6;">16 <math id="S4.T1.12.12.12.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.12.12.12.2.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.12.12.12.2.1.m1.1.1" xref="S4.T1.12.12.12.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.2.1.m1.1b"><times id="S4.T1.12.12.12.2.1.m1.1.1.cmml" xref="S4.T1.12.12.12.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.2.1.m1.1c">\times</annotation></semantics></math> 16 <math id="S4.T1.13.13.13.3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.13.13.13.3.2.m2.1a"><mo mathbackground="#E6E6E6" id="S4.T1.13.13.13.3.2.m2.1.1" xref="S4.T1.13.13.13.3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.3.2.m2.1b"><times id="S4.T1.13.13.13.3.2.m2.1.1.cmml" xref="S4.T1.13.13.13.3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.3.2.m2.1c">\times</annotation></semantics></math> 4</span></td>
<td id="S4.T1.13.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.13.13.13.6.1" class="ltx_text" style="background-color:#E6E6E6;">1.80</span></td>
<td id="S4.T1.13.13.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.13.13.13.7.1" class="ltx_text" style="background-color:#E6E6E6;">73.3</span></td>
</tr>
<tr id="S4.T1.16.16.16" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T1.16.16.16.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.16.16.16.4.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<td id="S4.T1.16.16.16.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.16.16.16.5.1" class="ltx_text" style="background-color:#E6E6E6;">SigLIP-SO400M</span></td>
<td id="S4.T1.14.14.14.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.14.14.14.1.1" class="ltx_text" style="background-color:#E6E6E6;">384 <math id="S4.T1.14.14.14.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.14.14.14.1.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.14.14.14.1.1.m1.1.1" xref="S4.T1.14.14.14.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.1.1.m1.1b"><times id="S4.T1.14.14.14.1.1.m1.1.1.cmml" xref="S4.T1.14.14.14.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.1.1.m1.1c">\times</annotation></semantics></math> 384</span></td>
<td id="S4.T1.16.16.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.16.16.16.3.2" class="ltx_text" style="background-color:#E6E6E6;">27 <math id="S4.T1.15.15.15.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.15.15.15.2.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.15.15.15.2.1.m1.1.1" xref="S4.T1.15.15.15.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.15.2.1.m1.1b"><times id="S4.T1.15.15.15.2.1.m1.1.1.cmml" xref="S4.T1.15.15.15.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.15.2.1.m1.1c">\times</annotation></semantics></math> 27 <math id="S4.T1.16.16.16.3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.16.16.16.3.2.m2.1a"><mo mathbackground="#E6E6E6" id="S4.T1.16.16.16.3.2.m2.1.1" xref="S4.T1.16.16.16.3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.16.3.2.m2.1b"><times id="S4.T1.16.16.16.3.2.m2.1.1.cmml" xref="S4.T1.16.16.16.3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.16.3.2.m2.1c">\times</annotation></semantics></math> 16</span></td>
<td id="S4.T1.16.16.16.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.16.16.16.6.1" class="ltx_text" style="background-color:#E6E6E6;">1.25</span></td>
<td id="S4.T1.16.16.16.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T1.16.16.16.7.1" class="ltx_text" style="background-color:#E6E6E6;">78.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Quantitative Evaluation</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.16.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.17.2" class="ltx_text" style="font-size:90%;">Comparison with leading methods on image-based visual language benchmarks. Our performance is close to leading VLMs, surpassing many methods by a large margin under the same LLM size, even with a discrete visual token type. * indicates that images in the training split of these datasets are observed during VLM training.</span></figcaption>
<div id="S4.T2.14" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:547.0pt;height:195.1pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-106.4pt,37.8pt) scale(0.72,0.72) ;">
<table id="S4.T2.14.14" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.14.14.15.1" class="ltx_tr">
<th id="S4.T2.14.14.15.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T2.14.14.15.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.2.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T2.14.14.15.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.3.1" class="ltx_text ltx_font_bold">Visual Token</span></th>
<th id="S4.T2.14.14.15.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.4.1" class="ltx_text ltx_font_bold">Res.</span></th>
<td id="S4.T2.14.14.15.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.5.1" class="ltx_text ltx_font_bold">VQAv2</span></td>
<td id="S4.T2.14.14.15.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.6.1" class="ltx_text ltx_font_bold">GQA</span></td>
<td id="S4.T2.14.14.15.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.7.1" class="ltx_text ltx_font_bold">TextVQA</span></td>
<td id="S4.T2.14.14.15.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.8.1" class="ltx_text ltx_font_bold">POPE</span></td>
<td id="S4.T2.14.14.15.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.9.1" class="ltx_text ltx_font_bold">MME</span></td>
<td id="S4.T2.14.14.15.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.10.1" class="ltx_text ltx_font_bold">SEED</span></td>
<td id="S4.T2.14.14.15.1.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.15.1.11.1" class="ltx_text ltx_font_bold">MM-Vet</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">llava</span> </a></cite>
</th>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Vicuna-1.5-7B</th>
<th id="S4.T2.2.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.2.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">336</th>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">78.5<sup id="S4.T2.1.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">62.0<sup id="S4.T2.2.2.2.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">58.2</td>
<td id="S4.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">85.9</td>
<td id="S4.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">1510.7</td>
<td id="S4.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">58.6</td>
<td id="S4.T2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">30.5</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<th id="S4.T2.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">VILA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lin2023vila</span> </a></cite>
</th>
<th id="S4.T2.4.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T2.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.4.4.4.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">336</th>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">79.9<sup id="S4.T2.3.3.3.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">62.3<sup id="S4.T2.4.4.4.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">64.4</td>
<td id="S4.T2.4.4.4.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">85.5</td>
<td id="S4.T2.4.4.4.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1533.0</td>
<td id="S4.T2.4.4.4.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">61.1</td>
<td id="S4.T2.4.4.4.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">34.9</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<th id="S4.T2.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Unified-IO 2 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Unified-io2</span> </a></cite>
</th>
<th id="S4.T2.5.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">6.8B from scratch</th>
<th id="S4.T2.5.5.5.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.5.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">384</th>
<td id="S4.T2.5.5.5.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">79.4<sup id="S4.T2.5.5.5.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.5.5.5.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.5.5.5.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">87.7</td>
<td id="S4.T2.5.5.5.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.5.5.5.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">61.8</td>
<td id="S4.T2.5.5.5.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.14.14.16.2" class="ltx_tr">
<th id="S4.T2.14.14.16.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">instructblip</span> </a></cite>
</th>
<th id="S4.T2.14.14.16.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Vicuna-7B</th>
<th id="S4.T2.14.14.16.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.14.14.16.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.14.14.16.2.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.16.2.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">49.2</td>
<td id="S4.T2.14.14.16.2.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">50.1</td>
<td id="S4.T2.14.14.16.2.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.16.2.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.16.2.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">53.4</td>
<td id="S4.T2.14.14.16.2.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">26.2</td>
</tr>
<tr id="S4.T2.14.14.17.3" class="ltx_tr">
<th id="S4.T2.14.14.17.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">IDEFICS-9B <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">idefics</span> </a></cite>
</th>
<th id="S4.T2.14.14.17.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-7B</th>
<th id="S4.T2.14.14.17.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.14.14.17.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.14.14.17.3.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">50.9</td>
<td id="S4.T2.14.14.17.3.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">38.4</td>
<td id="S4.T2.14.14.17.3.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">25.9</td>
<td id="S4.T2.14.14.17.3.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.17.3.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.17.3.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.17.3.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.14.14.18.4" class="ltx_tr">
<th id="S4.T2.14.14.18.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Emu <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu</span> </a></cite>
</th>
<th id="S4.T2.14.14.18.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-13B</th>
<th id="S4.T2.14.14.18.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.14.14.18.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.14.14.18.4.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">52.0</td>
<td id="S4.T2.14.14.18.4.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.18.4.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.18.4.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.18.4.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.18.4.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.18.4.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.14.14.19.5" class="ltx_tr">
<th id="S4.T2.14.14.19.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LaVIT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">jin2023unified</span> </a></cite>
</th>
<th id="S4.T2.14.14.19.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-7B</th>
<th id="S4.T2.14.14.19.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.14.14.19.5.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.14.14.19.5.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">66.0</td>
<td id="S4.T2.14.14.19.5.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">46.8</td>
<td id="S4.T2.14.14.19.5.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.19.5.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.19.5.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.19.5.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.19.5.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<th id="S4.T2.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">DreamLLM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">dreamllm</span> </a></cite>
</th>
<th id="S4.T2.6.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Vicuna-7B</th>
<th id="S4.T2.6.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.6.6.6.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.6.6.6.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">72.9<sup id="S4.T2.6.6.6.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.6.6.6.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">41.8</td>
<td id="S4.T2.6.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.6.6.6.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.6.6.6.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.6.6.6.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">36.6</td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<th id="S4.T2.8.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Video-LaVIT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-lavit</span> </a></cite>
</th>
<th id="S4.T2.8.8.8.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T2.8.8.8.5" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T2.8.8.8.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">80.2<sup id="S4.T2.7.7.7.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.8.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">63.6<sup id="S4.T2.8.8.8.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.8.8.8.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.8.8.8.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.8.8.8.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1581.5</td>
<td id="S4.T2.8.8.8.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">64.4</td>
<td id="S4.T2.8.8.8.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">35.0</td>
</tr>
<tr id="S4.T2.14.14.20.6" class="ltx_tr">
<th id="S4.T2.14.14.20.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">CM3Leon-7B <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">CM3Leon</span> </a></cite>
</th>
<th id="S4.T2.14.14.20.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">7B from scratch</th>
<th id="S4.T2.14.14.20.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Discrete</th>
<th id="S4.T2.14.14.20.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">256</th>
<td id="S4.T2.14.14.20.6.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">47.6</td>
<td id="S4.T2.14.14.20.6.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.20.6.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.20.6.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.20.6.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.20.6.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.20.6.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.14.14.21.7" class="ltx_tr">
<th id="S4.T2.14.14.21.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LWM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a></cite>
</th>
<th id="S4.T2.14.14.21.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T2.14.14.21.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Discrete</th>
<th id="S4.T2.14.14.21.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">256</th>
<td id="S4.T2.14.14.21.7.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">55.8</td>
<td id="S4.T2.14.14.21.7.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">44.8</td>
<td id="S4.T2.14.14.21.7.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">18.8</td>
<td id="S4.T2.14.14.21.7.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">75.2</td>
<td id="S4.T2.14.14.21.7.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.21.7.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.14.14.21.7.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">9.6</td>
</tr>
<tr id="S4.T2.10.10.10" class="ltx_tr">
<th id="S4.T2.10.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Show-o <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xie2024show</span> </a></cite>
</th>
<th id="S4.T2.10.10.10.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Phi-1.5B</th>
<th id="S4.T2.10.10.10.5" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Discrete</th>
<th id="S4.T2.10.10.10.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">256</th>
<td id="S4.T2.9.9.9.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">59.3<sup id="S4.T2.9.9.9.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.10.10.10.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">48.7<sup id="S4.T2.10.10.10.2.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T2.10.10.10.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.10.10.10.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">73.8</td>
<td id="S4.T2.10.10.10.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">948.4</td>
<td id="S4.T2.10.10.10.10" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T2.10.10.10.11" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.12.12.12.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.3.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<th id="S4.T2.12.12.12.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.4.1" class="ltx_text" style="background-color:#E6E6E6;">LLaMA-2-7B</span></th>
<th id="S4.T2.12.12.12.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.5.1" class="ltx_text" style="background-color:#E6E6E6;">Discrete</span></th>
<th id="S4.T2.12.12.12.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.6.1" class="ltx_text" style="background-color:#E6E6E6;">256</span></th>
<td id="S4.T2.11.11.11.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.11.11.11.1.1" class="ltx_text" style="background-color:#E6E6E6;">75.3<sup id="S4.T2.11.11.11.1.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S4.T2.12.12.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.2.1" class="ltx_text" style="background-color:#E6E6E6;">58.3<sup id="S4.T2.12.12.12.2.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S4.T2.12.12.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.7.1" class="ltx_text" style="background-color:#E6E6E6;">48.3</span></td>
<td id="S4.T2.12.12.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.8.1" class="ltx_text" style="background-color:#E6E6E6;">83.9</span></td>
<td id="S4.T2.12.12.12.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.9.1" class="ltx_text" style="background-color:#E6E6E6;">1336.2</span></td>
<td id="S4.T2.12.12.12.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.10.1" class="ltx_text" style="background-color:#E6E6E6;">56.3</span></td>
<td id="S4.T2.12.12.12.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.12.12.12.11.1" class="ltx_text" style="background-color:#E6E6E6;">27.7</span></td>
</tr>
<tr id="S4.T2.14.14.14" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.14.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.3.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<th id="S4.T2.14.14.14.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.4.1" class="ltx_text" style="background-color:#E6E6E6;">LLaMA-2-7B</span></th>
<th id="S4.T2.14.14.14.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.5.1" class="ltx_text" style="background-color:#E6E6E6;">Discrete</span></th>
<th id="S4.T2.14.14.14.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.6.1" class="ltx_text" style="background-color:#E6E6E6;">384</span></th>
<td id="S4.T2.13.13.13.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.13.13.13.1.1" class="ltx_text" style="background-color:#E6E6E6;">79.4<sup id="S4.T2.13.13.13.1.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S4.T2.14.14.14.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.2.1" class="ltx_text" style="background-color:#E6E6E6;">60.8<sup id="S4.T2.14.14.14.2.1.1" class="ltx_sup">∗</sup></span></td>
<td id="S4.T2.14.14.14.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.7.1" class="ltx_text" style="background-color:#E6E6E6;">60.8</span></td>
<td id="S4.T2.14.14.14.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.8.1" class="ltx_text" style="background-color:#E6E6E6;">85.8</span></td>
<td id="S4.T2.14.14.14.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.9.1" class="ltx_text" style="background-color:#E6E6E6;">1401.8</span></td>
<td id="S4.T2.14.14.14.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.10.1" class="ltx_text" style="background-color:#E6E6E6;">59.0</span></td>
<td id="S4.T2.14.14.14.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T2.14.14.14.11.1" class="ltx_text" style="background-color:#E6E6E6;">33.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Comparison with leading methods on video-based visual language benchmarks. The performance of our method is close to state-of-the-art VLMs, surpassing many methods under the same LLM size, even with a discrete visual token type.</span></figcaption>
<div id="S4.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:546.0pt;height:143.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-106.2pt,27.7pt) scale(0.72,0.72) ;">
<table id="S4.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T3.4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.2.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T3.4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Visual Token</span></th>
<th id="S4.T3.4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Res.</span></th>
<td id="S4.T3.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.5.1" class="ltx_text ltx_font_bold">MSVD-QA</span></td>
<td id="S4.T3.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.6.1" class="ltx_text ltx_font_bold">MSRVTT-QA</span></td>
<td id="S4.T3.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.7.1" class="ltx_text ltx_font_bold">TGIF-QA</span></td>
<td id="S4.T3.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.1.1.8.1" class="ltx_text ltx_font_bold">Activity Net-QA</span></td>
</tr>
<tr id="S4.T3.4.1.2.2" class="ltx_tr">
<th id="S4.T3.4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Unified-IO 2 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Unified-io2</span> </a></cite>
</th>
<th id="S4.T3.4.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">6.8B from scratch</th>
<th id="S4.T3.4.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">384</th>
<td id="S4.T3.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">52.1</td>
<td id="S4.T3.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">42.5</td>
<td id="S4.T3.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T3.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T3.4.1.3.3" class="ltx_tr">
<th id="S4.T3.4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Emu <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">Emu</span> </a></cite>
</th>
<th id="S4.T3.4.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-13B</th>
<th id="S4.T3.4.1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T3.4.1.3.3.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">18.8</td>
<td id="S4.T3.4.1.3.3.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">8.3</td>
<td id="S4.T3.4.1.3.3.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T3.4.1.4.4" class="ltx_tr">
<th id="S4.T3.4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">VideoChat <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">videochat</span> </a></cite>
</th>
<th id="S4.T3.4.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Vicuna-7B</th>
<th id="S4.T3.4.1.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">56.3</td>
<td id="S4.T3.4.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">45</td>
<td id="S4.T3.4.1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">34.4</td>
<td id="S4.T3.4.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T3.4.1.5.5" class="ltx_tr">
<th id="S4.T3.4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Video-LLaMA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-llama</span> </a></cite>
</th>
<th id="S4.T3.4.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T3.4.1.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.5.5.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">51.6</td>
<td id="S4.T3.4.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">29.6</td>
<td id="S4.T3.4.1.5.5.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T3.4.1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T3.4.1.6.6" class="ltx_tr">
<th id="S4.T3.4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Video-ChatGPT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-chatgpt</span> </a></cite>
</th>
<th id="S4.T3.4.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T3.4.1.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">64.9</td>
<td id="S4.T3.4.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">49.3</td>
<td id="S4.T3.4.1.6.6.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">51.4</td>
<td id="S4.T3.4.1.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">35.2</td>
</tr>
<tr id="S4.T3.4.1.7.7" class="ltx_tr">
<th id="S4.T3.4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Video-LLava <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-llava</span> </a></cite>
</th>
<th id="S4.T3.4.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Vicuna-7B</th>
<th id="S4.T3.4.1.7.7.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.7.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70.7</td>
<td id="S4.T3.4.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">59.2</td>
<td id="S4.T3.4.1.7.7.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70.0</td>
<td id="S4.T3.4.1.7.7.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">45.3</td>
</tr>
<tr id="S4.T3.4.1.8.8" class="ltx_tr">
<th id="S4.T3.4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Video-LaVIT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">video-lavit</span> </a></cite>
</th>
<th id="S4.T3.4.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T3.4.1.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Continuous</th>
<th id="S4.T3.4.1.8.8.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">224</th>
<td id="S4.T3.4.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">73.5</td>
<td id="S4.T3.4.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">59.5</td>
<td id="S4.T3.4.1.8.8.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
<td id="S4.T3.4.1.8.8.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">50.2</td>
</tr>
<tr id="S4.T3.4.1.9.9" class="ltx_tr">
<th id="S4.T3.4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LWM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a></cite>
</th>
<th id="S4.T3.4.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">LLaMA-2-7B</th>
<th id="S4.T3.4.1.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">Discrete</th>
<th id="S4.T3.4.1.9.9.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">256</th>
<td id="S4.T3.4.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">55.9</td>
<td id="S4.T3.4.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">44.1</td>
<td id="S4.T3.4.1.9.9.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">40.9</td>
<td id="S4.T3.4.1.9.9.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">–</td>
</tr>
<tr id="S4.T3.4.1.10.10" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T3.4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.1.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<th id="S4.T3.4.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.2.1" class="ltx_text" style="background-color:#E6E6E6;">LLaMA-2-7B</span></th>
<th id="S4.T3.4.1.10.10.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.3.1" class="ltx_text" style="background-color:#E6E6E6;">Discrete</span></th>
<th id="S4.T3.4.1.10.10.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.4.1" class="ltx_text" style="background-color:#E6E6E6;">256</span></th>
<td id="S4.T3.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.5.1" class="ltx_text" style="background-color:#E6E6E6;">73.4</span></td>
<td id="S4.T3.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.6.1" class="ltx_text" style="background-color:#E6E6E6;">58.9</span></td>
<td id="S4.T3.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.7.1" class="ltx_text" style="background-color:#E6E6E6;">51.3</span></td>
<td id="S4.T3.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.10.10.8.1" class="ltx_text" style="background-color:#E6E6E6;">51.6</span></td>
</tr>
<tr id="S4.T3.4.1.11.11" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T3.4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.1.1" class="ltx_text" style="background-color:#E6E6E6;">Ours</span></th>
<th id="S4.T3.4.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.2.1" class="ltx_text" style="background-color:#E6E6E6;">LLaMA-2-7B</span></th>
<th id="S4.T3.4.1.11.11.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.3.1" class="ltx_text" style="background-color:#E6E6E6;">Discrete</span></th>
<th id="S4.T3.4.1.11.11.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.4.1" class="ltx_text" style="background-color:#E6E6E6;">384</span></th>
<td id="S4.T3.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.5.1" class="ltx_text" style="background-color:#E6E6E6;">75.3</span></td>
<td id="S4.T3.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.6.1" class="ltx_text" style="background-color:#E6E6E6;">60.0</span></td>
<td id="S4.T3.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.7.1" class="ltx_text" style="background-color:#E6E6E6;">51.9</span></td>
<td id="S4.T3.4.1.11.11.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S4.T3.4.1.11.11.8.1" class="ltx_text" style="background-color:#E6E6E6;">52.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Visual Understanding Tasks.</span> Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarize the comparison between our method and other leading VLMs on the image-language and video-language benchmarks respectively. Compared to the mainstream choice of continuous visual tokens produced by foundation models like CLIP, the VQGAN-based discrete visual tokens have less alignment with text, thus harming VLMs’ performance on visual understanding tasks. With our unified foundation vision tower, our model can have a performance close to leading VLMs even with discrete visual tokens.</p>
</div>
<figure id="S4.T4" class="ltx_table ltx_align_floatright">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:127.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.9pt,8.9pt) scale(0.876824164236353,0.876824164236353) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">#Images</span></td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">FID<math id="S4.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SD-XL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">sdxl</span> </a></cite>
</th>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Diffusion</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">2000M</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">9.55</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PixArt <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">chen2023pixart</span> </a></cite>
</th>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center">Diffusion</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center">25M</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center">6.14</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<th id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Playground v2.5 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">playgroundv2.5</span> </a></cite>
</th>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center">Diffusion</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center">4.48</td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<th id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LWM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a></cite>
</th>
<td id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_center">Autoregressive</td>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center">–</td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center">17.77</td>
</tr>
<tr id="S4.T4.1.1.6.5" class="ltx_tr">
<th id="S4.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Show-o <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xie2024show</span> </a></cite>
</th>
<td id="S4.T4.1.1.6.5.2" class="ltx_td ltx_align_center">Autoregressive</td>
<td id="S4.T4.1.1.6.5.3" class="ltx_td ltx_align_center">36M</td>
<td id="S4.T4.1.1.6.5.4" class="ltx_td ltx_align_center">15.18</td>
</tr>
<tr id="S4.T4.1.1.7.6" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T4.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.1.7.6.1.1" class="ltx_text" style="background-color:#E6E6E6;">Ours (256)</span></th>
<td id="S4.T4.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.2.1" class="ltx_text" style="background-color:#E6E6E6;">Autoregressive</span></td>
<td id="S4.T4.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.3.1" class="ltx_text" style="background-color:#E6E6E6;">15M</span></td>
<td id="S4.T4.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.7.6.4.1" class="ltx_text" style="background-color:#E6E6E6;">12.81</span></td>
</tr>
<tr id="S4.T4.1.1.8.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.1.8.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">Ours (384)</span></th>
<td id="S4.T4.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.8.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">Autoregressive</span></td>
<td id="S4.T4.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.8.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">15M</span></td>
<td id="S4.T4.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.1.8.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">7.69</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.5.2" class="ltx_text" style="font-size:90%;">Comparison with other visual generation methods on MJHQ-30K evaluation benchmark.</span></figcaption>
</figure>
<figure id="S4.SS3.9" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.SS3.9.11.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.SS3.9.12.2" class="ltx_text" style="font-size:90%;">Comparison with other visual generation methods on GenAI-Bench <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">GenAI-Bench</span> </a></cite>. The results show that our method outperforms previous autoregressive visual generation methods. For <span id="S4.SS3.9.12.2.1" class="ltx_text ltx_font_italic">advanced</span> prompts that require better text following ability to generate, our method can have a relatively small performance gap with diffusion-based methods, even with much less training data and time.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.SS3.4.4" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:349.5pt;height:134.4pt;vertical-align:-128.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.7pt,0.8pt) scale(0.8,0.8) ;"><span id="S4.SS3.4.4.5" class="ltx_ERROR undefined">{NiceTabular}</span>
<p id="S4.SS3.4.4.4" class="ltx_p">lccccccc|c
<span id="S4.SS3.4.4.4.5" class="ltx_ERROR undefined">\CodeBefore</span><span id="S4.SS3.4.4.4.6" class="ltx_ERROR undefined">\Body</span>
<span id="S4.SS3.4.4.4.7" class="ltx_text ltx_font_bold">Method</span>  <span id="S4.SS3.4.4.4.8" class="ltx_text ltx_font_bold">Type</span>  <span id="S4.SS3.4.4.4.9" class="ltx_text">#<span id="S4.SS3.4.4.4.9.1" class="ltx_text ltx_font_bold">Training Images</span></span>  <span id="S4.SS3.1.1.1.1" class="ltx_text ltx_font_bold">Attribute<math id="S4.SS3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.SS3.1.1.1.1.1.m1.1.1" xref="S4.SS3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.1.1.1.1.1.m1.1b"><ci id="S4.SS3.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>  <span id="S4.SS3.2.2.2.2" class="ltx_text ltx_font_bold">Scene<math id="S4.SS3.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.SS3.2.2.2.2.1.m1.1.1" xref="S4.SS3.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.2.2.2.2.1.m1.1b"><ci id="S4.SS3.2.2.2.2.1.m1.1.1.cmml" xref="S4.SS3.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span>   <span id="S4.SS3.4.4.4.4" class="ltx_text ltx_font_bold">Relation<math id="S4.SS3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.SS3.3.3.3.3.m1.1.1" xref="S4.SS3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.3.3.3.3.m1.1b"><ci id="S4.SS3.3.3.3.3.m1.1.1.cmml" xref="S4.SS3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>   <span id="S4.SS3.4.4.4.4.1" class="ltx_text">Overall<math id="S4.SS3.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.SS3.4.4.4.4.1.m1.1.1" xref="S4.SS3.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.4.4.4.4.1.m1.1b"><ci id="S4.SS3.4.4.4.4.1.m1.1.1.cmml" xref="S4.SS3.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span> 
<br class="ltx_break">
     Spatial  Action  Part 
<br class="ltx_break">SD v2.1 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ldm</span> </a></cite>  Diffusion  2000M  0.80  0.79  0.76  0.77  0.80  0.78 
<br class="ltx_break">SD-XL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">sdxl</span> </a></cite>  Diffusion  2000M  0.84  0.84  0.82  0.83  0.89  0.83
<br class="ltx_break">Midjourney v6 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">midjourney</span> </a></cite>  Diffusion  –  0.88  0.87  0.87  0.87  0.91  0.87 
<br class="ltx_break">DALL-E 3 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">dalle-3</span> </a></cite>  Diffusion  –  0.91  0.90  0.92  0.89  0.91  0.90 
<br class="ltx_break">LWM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a></cite>  Autoregressive  –  0.63  0.62  0.65  0.63  0.70  0.63 
<br class="ltx_break">Show-o <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xie2024show</span> </a></cite>  Autoregressive  36M  0.72  0.72  0.70  0.70  0.75  0.70 
<br class="ltx_break"><span id="S4.SS3.4.4.4.4.2" class="ltx_text" style="background-color:#E6E6E6;">Ours (256)  Autoregressive  15M  0.78  0.78  0.77  0.78  0.79  0.76 
<br class="ltx_break">Ours (384)  Autoregressive  15M  0.75  0.76  0.75  0.73  0.75  0.73 
<br class="ltx_break"></span></span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.SS3.9.9" class="ltx_p ltx_figure_panel"><span id="S4.SS3.9.9.5" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">(a) VQAScores on <span id="S4.SS3.9.9.5.6" class="ltx_text ltx_font_medium ltx_font_italic">basic</span> prompts of GenAI-Bench 
<br class="ltx_break">

<span id="S4.SS3.9.9.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:332.1pt;height:145.9pt;vertical-align:-140.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,0.9pt) scale(0.76,0.76) ;"><span id="S4.SS3.9.9.5.5.6" class="ltx_ERROR undefined">{NiceTabular}</span>
<span id="S4.SS3.9.9.5.5.5" class="ltx_p">lccccccc|c
<span id="S4.SS3.9.9.5.5.5.5" class="ltx_ERROR undefined">\CodeBefore</span><span id="S4.SS3.9.9.5.5.5.6" class="ltx_ERROR undefined">\Body</span>
<span id="S4.SS3.9.9.5.5.5.7" class="ltx_text">Method</span>  <span id="S4.SS3.9.9.5.5.5.8" class="ltx_text">Type</span> 
<span id="S4.SS3.9.9.5.5.5.9" class="ltx_text">#Training Images</span> 
<span id="S4.SS3.5.5.1.1.1.1" class="ltx_text">Count<math id="S4.SS3.5.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.5.5.1.1.1.1.m1.1a"><mo mathbackground="#E6E6E6" stretchy="false" id="S4.SS3.5.5.1.1.1.1.m1.1.1" xref="S4.SS3.5.5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.5.5.1.1.1.1.m1.1b"><ci id="S4.SS3.5.5.1.1.1.1.m1.1.1.cmml" xref="S4.SS3.5.5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.5.5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>  <span id="S4.SS3.6.6.2.2.2.2" class="ltx_text">Differ<math id="S4.SS3.6.6.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.6.6.2.2.2.2.m1.1a"><mo mathbackground="#E6E6E6" stretchy="false" id="S4.SS3.6.6.2.2.2.2.m1.1.1" xref="S4.SS3.6.6.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.6.6.2.2.2.2.m1.1b"><ci id="S4.SS3.6.6.2.2.2.2.m1.1.1.cmml" xref="S4.SS3.6.6.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.6.6.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math></span>  <span id="S4.SS3.7.7.3.3.3.3" class="ltx_text">Compare<math id="S4.SS3.7.7.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.7.7.3.3.3.3.m1.1a"><mo mathbackground="#E6E6E6" stretchy="false" id="S4.SS3.7.7.3.3.3.3.m1.1.1" xref="S4.SS3.7.7.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.7.7.3.3.3.3.m1.1b"><ci id="S4.SS3.7.7.3.3.3.3.m1.1.1.cmml" xref="S4.SS3.7.7.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.7.7.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math></span>   Logical<math id="S4.SS3.8.8.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.8.8.4.4.4.m1.1a"><mo mathbackground="#E6E6E6" stretchy="false" id="S4.SS3.8.8.4.4.4.m1.1.1" xref="S4.SS3.8.8.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.8.8.4.4.4.m1.1b"><ci id="S4.SS3.8.8.4.4.4.m1.1.1.cmml" xref="S4.SS3.8.8.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.8.8.4.4.4.m1.1c">\uparrow</annotation></semantics></math>   <span id="S4.SS3.9.9.5.5.5.4" class="ltx_text">Overall<math id="S4.SS3.9.9.5.5.5.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.SS3.9.9.5.5.5.4.m1.1a"><mo mathbackground="#E6E6E6" stretchy="false" id="S4.SS3.9.9.5.5.5.4.m1.1.1" xref="S4.SS3.9.9.5.5.5.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.9.9.5.5.5.4.m1.1b"><ci id="S4.SS3.9.9.5.5.5.4.m1.1.1.cmml" xref="S4.SS3.9.9.5.5.5.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.9.9.5.5.5.4.m1.1c">\uparrow</annotation></semantics></math></span> 
<br class="ltx_break">
      Negate  Universal 
<br class="ltx_break">SD v2.1 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">ldm</span> </a></cite>  Diffusion  2000M  0.68  0.70  0.68  0.54  0.64  0.62 
<br class="ltx_break">SD-XL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">sdxl</span> </a></cite>  Diffusion  2000M  0.71  0.73  0.69  0.50  0.66  0.63 
<br class="ltx_break">Midjourney v6 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">midjourney</span> </a></cite>  Diffusion  –  0.78  0.78  0.79  0.50  0.76  0.69 
<br class="ltx_break">DALL-E 3 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">dalle-3</span> </a></cite>  Diffusion  –
 0.82  0.78  0.82  0.48  0.80  0.70 
<br class="ltx_break">LWM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lwm</span> </a></cite>  Autoregressive  –
0.59  0.58  0.54  0.49  0.52  0.53 
<br class="ltx_break">Show-o <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xie2024show</span> </a></cite>  Autoregressive  36M  0.70  0.62  0.71  0.51  0.65  0.60 
<br class="ltx_break">Ours (256)  Autoregressive  15M  0.70  0.71  0.74  0.53  0.66  0.64 
<br class="ltx_break">Ours (384)  Autoregressive  15M  0.68  0.67  0.71  0.51  0.64  0.61 
<br class="ltx_break"></span>
</span></span> 
<br class="ltx_break">
(b) VQAScores on <span id="S4.SS3.9.9.5.7" class="ltx_text ltx_font_medium ltx_font_italic">advanced</span> prompts of GenAI-Bench


</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.SS3.9.13" class="ltx_p ltx_figure_panel"><span id="S4.SS3.9.13.1" class="ltx_text ltx_font_bold">Visual Generation Tasks.</span> As shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, VILA-U can achieve a better FID than other autoregressive methods and have comparable performance with some diffusion based methods. This result shows the feasibility of our method for visual generation. Table <a href="#S4.SS3" title="4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> summarizes the quantitative results of our method and other visual generation methods on GenAI-Bench. Although Our method is inferior to diffusion-based visual generation methods that have been trained on billions-level image-text pairs, our method has comparable performance with SD v2.1 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">rombach2022high</span> </a></cite> and SD-XL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">sdxl</span> </a></cite> on <span id="S4.SS3.9.13.2" class="ltx_text ltx_font_italic">advanced</span> prompts even trained with magnitude-level less data. This further shows that VILA-U can learn the correlation among visual and textual modalities effectively and efficiently with our unified training framework.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS4" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Evaluation</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.04429/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">VILA-U can correctly caption videos and cover all the details, thanks to the text alignment of our vision encoder.</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Visual Understanding.</span> To validate the effectiveness of VILA-U in comprehensive visual understanding tasks, we apply it in several understanding and reasoning tasks, as some examples shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. From the results, we can see the versatility of VILA-U in various tasks including visual captioning and visual question answering. Besides, our model has inherited some important capabilities from VILA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lin2023vila</span> </a></cite> including multi-image understanding, in-context learning, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:203.8pt;"><img src="/html/2409.04429/assets/x4.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="193" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F5.1.2.2" class="ltx_text" style="font-size:90%;">VILA-U has good visual question answering capability. The images and questions are from the test split of VQAv2 dataset.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:221.1pt;"><img src="/html/2409.04429/assets/x5.png" id="S4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="213" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.2.2.2" class="ltx_text" style="font-size:90%;">VILA-U has good in-context learning capability. We feed two image-text pairs and a third image as the context to prompt the VLM.</span></figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.04429/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">VILA-U can correctly reason over multiple images.</span></figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Visual Generation.</span> We present some examples of the visual generation results with VILA-U in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We can see that our model can be employed in both image generation and video generation, even trained with a relatively small data corpus. In the given examples, our method can generate nice-looking images and continuous videos adhering to users’ input.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2409.04429/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">VILA-U can generate high-quality images and videos given text input.</span></figcaption>
</figure>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Ablation Study</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we delve into some key design components of our framework and design ablation experiments to show how they will influence the performance.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Impact of Contrastive Loss to Visual Understanding</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We include contrastive loss in vision tower training, which endows it with the text alignment ability. During our multi-modal training, such text alignment ability is crucial in enhancing modality fusion and performance on downstream visual language tasks. We validate the importance of this alignment by training the vision tower with and without the contrastive loss, evaluating its impact on visual language understanding performance. For this ablation study, we randomly sample 25M data from COYO-700M to train the vision tower. For multi-modal training, we use ShareGPT4V and MMC4 without text-image and text-video data. The results of the first two lines in Table <a href="#S5.T6" title="Table 6 ‣ 5.1 Impact of Contrastive Loss to Visual Understanding ‣ 5 Ablation Study ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrate the crucial role of text alignment in achieving strong visual language understanding performance.
Scaling the dataset size from 25M to 700M further enhances performance, highlighting the importance of learning text alignment on a large-scale dataset.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S5.T6.3.2" class="ltx_text" style="font-size:90%;">Impact of contrastive loss to visual understanding.</span></figcaption>
<div id="S5.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:420.6pt;height:58.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.6pt,7.2pt) scale(0.8,0.8) ;">
<table id="S5.T6.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.4.1.1.1" class="ltx_tr">
<th id="S5.T6.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Pretrained Weights</span></th>
<th id="S5.T6.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.2.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Data size</span></th>
<th id="S5.T6.4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Loss Type</span></th>
<th id="S5.T6.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Top-1 Accuracy</span></th>
<th id="S5.T6.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">VQAv2</span></th>
<th id="S5.T6.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.6.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">POPE</span></th>
<th id="S5.T6.4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.7.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">MME</span></th>
<th id="S5.T6.4.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.8.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">SEED</span></th>
<th id="S5.T6.4.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.T6.4.1.1.1.9.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">MM-Vet</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.4.1.2.1" class="ltx_tr">
<th id="S5.T6.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">SigLIP-Large</th>
<th id="S5.T6.4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">25M</th>
<th id="S5.T6.4.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Recon.</th>
<th id="S5.T6.4.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">–</th>
<td id="S5.T6.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">57.7</td>
<td id="S5.T6.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">75.1</td>
<td id="S5.T6.4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">937.7</td>
<td id="S5.T6.4.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">38.7</td>
<td id="S5.T6.4.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">15.3</td>
</tr>
<tr id="S5.T6.4.1.3.2" class="ltx_tr">
<th id="S5.T6.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">SigLIP-Large</th>
<th id="S5.T6.4.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.5pt;padding-right:5.5pt;">25M</th>
<th id="S5.T6.4.1.3.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">Recon. + Contra.</th>
<th id="S5.T6.4.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">62.9</th>
<td id="S5.T6.4.1.3.2.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">68.0</td>
<td id="S5.T6.4.1.3.2.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">83.7</td>
<td id="S5.T6.4.1.3.2.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1219</td>
<td id="S5.T6.4.1.3.2.8" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">50.4</td>
<td id="S5.T6.4.1.3.2.9" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">20.8</td>
</tr>
<tr id="S5.T6.4.1.4.3" class="ltx_tr">
<th id="S5.T6.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">SigLIP-Large</th>
<th id="S5.T6.4.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">700M</th>
<th id="S5.T6.4.1.4.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">Recon. + Contra.</th>
<th id="S5.T6.4.1.4.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">73.3</th>
<td id="S5.T6.4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">75.3</td>
<td id="S5.T6.4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">83.9</td>
<td id="S5.T6.4.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">1336.2</td>
<td id="S5.T6.4.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">56.3</td>
<td id="S5.T6.4.1.4.3.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">27.7</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Impact of Contrastive Loss to Visual Generation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We conduct two experiments to demonstrate the influence of contrastive loss to generation performance. For efficiency, we conduct only text-to-image pretraining and utilize Sheared-LLaMA-1.3B <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">xia2023sheared</span> </a></cite> instead of LLaMA-2-7B as the LLM. In the first experiment, we use the RQ-VAE as the vision tower, which has an rFID of 1.30. In the second experiment, we employ our unified vision tower. Results are shown in Table <a href="#S5.SS2" title="5.2 Impact of Contrastive Loss to Visual Generation ‣ 5 Ablation Study ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Our Unified Vision Tower yielded slightly worse FID results than the RQ-VAE on MJHQ-30K, possibly due to its inferior rFID resulting from the introduction of contrastive loss.</p>
</div>
<div id="S5.SS2.5" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:281.9pt;">
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S5.T7.3.2" class="ltx_text" style="font-size:90%;">Impact of contrastive loss to visual generation.</span></figcaption>
</figure>
<div id="S5.SS2.5.p1" class="ltx_para">
<div id="S5.SS2.4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:434.0pt;height:44pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.3pt,5.4pt) scale(0.8,0.8) ;">
<table id="S5.SS2.4.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.SS2.2.2.2.2" class="ltx_tr">
<th id="S5.SS2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.SS2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">Vision Tower</span></th>
<th id="S5.SS2.2.2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.SS2.2.2.2.2.4.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S5.SS2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.SS2.2.2.2.2.5.1" class="ltx_text ltx_font_bold">Resolution</span></th>
<th id="S5.SS2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S5.SS2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">rFID</span> <math id="S5.SS2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.SS2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.SS2.1.1.1.1.1.m1.1.1" xref="S5.SS2.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.1.1.1.1.1.m1.1b"><ci id="S5.SS2.1.1.1.1.1.m1.1.1.cmml" xref="S5.SS2.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.SS2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S5.SS2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">FID</span> <math id="S5.SS2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.SS2.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.SS2.2.2.2.2.2.m1.1.1" xref="S5.SS2.2.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.2.2.2.2.2.m1.1b"><ci id="S5.SS2.2.2.2.2.2.m1.1.1.cmml" xref="S5.SS2.2.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.2.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.SS2.3.3.3.3" class="ltx_tr">
<th id="S5.SS2.3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">RQ-VAE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">lee2022autoregressive</span> </a></cite>
</th>
<th id="S5.SS2.3.3.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">Sheared-LLaMA-1.3B</th>
<td id="S5.SS2.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">256 <math id="S5.SS2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.3.3.3.3.1.m1.1a"><mo id="S5.SS2.3.3.3.3.1.m1.1.1" xref="S5.SS2.3.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.3.3.3.3.1.m1.1b"><times id="S5.SS2.3.3.3.3.1.m1.1.1.cmml" xref="S5.SS2.3.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.3.3.3.3.1.m1.1c">\times</annotation></semantics></math> 256</td>
<td id="S5.SS2.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">1.30</td>
<td id="S5.SS2.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">12.0</td>
</tr>
<tr id="S5.SS2.4.4.4.4" class="ltx_tr">
<th id="S5.SS2.4.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">Ours</th>
<th id="S5.SS2.4.4.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">Sheared-LLaMA-1.3B</th>
<td id="S5.SS2.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">256 <math id="S5.SS2.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.4.4.4.4.1.m1.1a"><mo id="S5.SS2.4.4.4.4.1.m1.1.1" xref="S5.SS2.4.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.4.4.4.4.1.m1.1b"><times id="S5.SS2.4.4.4.4.1.m1.1.1.cmml" xref="S5.SS2.4.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.4.4.4.4.1.m1.1c">\times</annotation></semantics></math> 256</td>
<td id="S5.SS2.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">1.80</td>
<td id="S5.SS2.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">13.2</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<div id="S5.SS2.7" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:130.1pt;">
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S5.T8.3.2" class="ltx_text" style="font-size:90%;">Impact of CFG.</span></figcaption>
</figure>
<div id="S5.SS2.7.p1" class="ltx_para">
<div id="S5.SS2.6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:83.1pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.4pt,9.0pt) scale(0.8,0.8) ;">
<table id="S5.SS2.6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.SS2.6.1.1.1" class="ltx_tr">
<th id="S5.SS2.6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S5.SS2.6.1.1.1.2.1" class="ltx_text ltx_font_bold">CFG Value</span></th>
<th id="S5.SS2.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S5.SS2.6.1.1.1.1.1" class="ltx_text ltx_font_bold">FID</span> <math id="S5.SS2.6.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.SS2.6.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.SS2.6.1.1.1.1.m1.1.1" xref="S5.SS2.6.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.6.1.1.1.1.m1.1b"><ci id="S5.SS2.6.1.1.1.1.m1.1.1.cmml" xref="S5.SS2.6.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.6.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.SS2.6.1.1.2.1" class="ltx_tr">
<td id="S5.SS2.6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">1.0</td>
<td id="S5.SS2.6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">14.1</td>
</tr>
<tr id="S5.SS2.6.1.1.3.2" class="ltx_tr">
<td id="S5.SS2.6.1.1.3.2.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">2.0</td>
<td id="S5.SS2.6.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">13.0</td>
</tr>
<tr id="S5.SS2.6.1.1.4.3" class="ltx_tr">
<td id="S5.SS2.6.1.1.4.3.1" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">3.0</td>
<td id="S5.SS2.6.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">12.8</td>
</tr>
<tr id="S5.SS2.6.1.1.5.4" class="ltx_tr">
<td id="S5.SS2.6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">5.0</td>
<td id="S5.SS2.6.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">13.2</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Impact of Classifier-free Guidance</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We adopt classifier-free guidance during the visual content generation. We investigate the impact of the CFG value on our 256-resolution model. Results presented in Table <a href="#S5.SS2" title="5.2 Impact of Contrastive Loss to Visual Generation ‣ 5 Ablation Study ‣ 4.4 Qualitative Evaluation ‣ 4.3 Quantitative Evaluation ‣ 4 Experiments ‣ VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> indicate that a CFG value of 3.0 yields the best FID score.</p>
</div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We present VILA-U, a novel and unified visual language model that integrates video, image and language understanding and generation tasks into one autoregressive next-token prediction framework. Our method is not only more concise than most VLMs that leverage additional components like diffusion models for unifying visual generation and understanding, but also demonstrates that autoregressive methods can achieve comparable performance to state-of-the-art VLMs. Our success is due to both a unified foundation vision tower that aligns discrete visual features with texts during pre-training and a high-quality dataset suitable for visual understanding and generation training. We believe VILA-U can serve as a general-purpose framework for diverse visual language tasks.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Limitations.</span> There is still a performance gap in the visual understanding ability between VILA-U and state-of-the-art VLMs leveraging continuous visual feature. Besides, the visual generation quality is relatively low compared to state-of-the-art diffusion models. In future work, we will be committed to overcoming these limitations to build an advanced VLM that can achieve state-of-the-art performance in all kinds of visual language tasks.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
ADEPT AI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">Fuyu-8B: A multimodal architecture for AI agents.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.adept.ai/blog/fuyu-8b" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.adept.ai/blog/fuyu-8b</a><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Jointly training large autoregressive multimodal models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.15564</span><span id="bib.bib2.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib3.7.2" class="ltx_text" style="font-size:90%;">, 35:23716–23736, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Openflamingo, March 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">Technical report, Alibaba Group, 2023.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/2303.08774</a><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">Coyo-700m: Image-text pair dataset.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kakaobrain/coyo-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/kakaobrain/coyo-dataset</a><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Activitynet: A large-scale video benchmark for human activity understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the ieee conference on computer vision and pattern recognition</span><span id="bib.bib7.8.3" class="ltx_text" style="font-size:90%;">, pages 961–970, 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
David Chen and William B Dolan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">Collecting highly parallel data for paraphrase evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</span><span id="bib.bib8.8.3" class="ltx_text" style="font-size:90%;">, pages 190–200, 2011.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.3.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">Pixart-</span><math id="bib.bib9.1.m1.1" class="ltx_Math" alttext="alpha" display="inline"><semantics id="bib.bib9.1.m1.1a"><mrow id="bib.bib9.1.m1.1.1" xref="bib.bib9.1.m1.1.1.cmml"><mi mathsize="90%" id="bib.bib9.1.m1.1.1.2" xref="bib.bib9.1.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="bib.bib9.1.m1.1.1.1" xref="bib.bib9.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="bib.bib9.1.m1.1.1.3" xref="bib.bib9.1.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="bib.bib9.1.m1.1.1.1a" xref="bib.bib9.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="bib.bib9.1.m1.1.1.4" xref="bib.bib9.1.m1.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="bib.bib9.1.m1.1.1.1b" xref="bib.bib9.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="bib.bib9.1.m1.1.1.5" xref="bib.bib9.1.m1.1.1.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="bib.bib9.1.m1.1.1.1c" xref="bib.bib9.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="bib.bib9.1.m1.1.1.6" xref="bib.bib9.1.m1.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="bib.bib9.1.m1.1b"><apply id="bib.bib9.1.m1.1.1.cmml" xref="bib.bib9.1.m1.1.1"><times id="bib.bib9.1.m1.1.1.1.cmml" xref="bib.bib9.1.m1.1.1.1"></times><ci id="bib.bib9.1.m1.1.1.2.cmml" xref="bib.bib9.1.m1.1.1.2">𝑎</ci><ci id="bib.bib9.1.m1.1.1.3.cmml" xref="bib.bib9.1.m1.1.1.3">𝑙</ci><ci id="bib.bib9.1.m1.1.1.4.cmml" xref="bib.bib9.1.m1.1.1.4">𝑝</ci><ci id="bib.bib9.1.m1.1.1.5.cmml" xref="bib.bib9.1.m1.1.1.5">ℎ</ci><ci id="bib.bib9.1.m1.1.1.6.cmml" xref="bib.bib9.1.m1.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">alpha</annotation></semantics></math><span id="bib.bib9.7.2" class="ltx_text" style="font-size:90%;">: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.00426</span><span id="bib.bib9.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Sharegpt4v: Improving large multi-modal models with better captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.12793</span><span id="bib.bib10.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Pali-x: On scaling up a multilingual vision and language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.18565</span><span id="bib.bib11.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.14238</span><span id="bib.bib12.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">See https://vicuna. lmsys. org (accessed 14 April 2023)</span><span id="bib.bib13.7.2" class="ltx_text" style="font-size:90%;">, 2(3):6, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Scaling instruction-finetuned language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</span><span id="bib.bib14.7.2" class="ltx_text" style="font-size:90%;">, 25(70):1–53, 2024.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib15.7.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib16.7.2" class="ltx_text" style="font-size:90%;">, abs/2305.06500, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern recognition</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern recognition</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Dreamllm: Synergistic multimodal comprehension and creation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.11499</span><span id="bib.bib19.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Palm-e: An embodied multimodal language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.03378</span><span id="bib.bib20.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Patrick Esser, Robin Rombach, and Bjorn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Taming transformers for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib21.8.3" class="ltx_text" style="font-size:90%;">, pages 12873–12883, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Patrick Esser, Robin Rombach, and Bjorn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Taming transformers for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, pages 12873–12883, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib24.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho and Tim Salimans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Classifier-free diffusion guidance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.12598</span><span id="bib.bib25.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">Cogagent: A visual language model for gui agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.08914</span><span id="bib.bib26.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">, pages 6700–6709, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Perceiver: General perception with iterative attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">, pages 4651–4664. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Mixtral of experts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2401.04088</span><span id="bib.bib29.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.03161</span><span id="bib.bib30.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, CHEN Bin, Chengru Song, Di ZHANG, Wenwu Ou, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Unified language-vision pretraining in llm with dynamic discrete visual tokenization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Hugo Laurençon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">URL https://huggingface. co/blog/idefics. Accessed</span><span id="bib.bib32.7.2" class="ltx_text" style="font-size:90%;">, pages 09–18, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Autoregressive image generation using residual quantization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib33.8.3" class="ltx_text" style="font-size:90%;">, pages 11523–11532, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Seed-bench: Benchmarking multimodal llms with generative comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.16125</span><span id="bib.bib34.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.17245</span><span id="bib.bib35.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib36.8.3" class="ltx_text" style="font-size:90%;">, pages 19730–19742. PMLR, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</span><span id="bib.bib37.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib38.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib39.8.3" class="ltx_text" style="font-size:90%;">, pages 12888–12900. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Videochat: Chat-centric video understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.06355</span><span id="bib.bib40.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">Mini-gemini: Mining the potential of multi-modality vision language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2403.18814</span><span id="bib.bib41.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Evaluating object hallucination in large vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.10355</span><span id="bib.bib42.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Tgif: A new dataset and benchmark on animated gif description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib43.8.3" class="ltx_text" style="font-size:90%;">, pages 4641–4650, 2016.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">Video-llava: Learning united visual representation by alignment before projection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.10122</span><span id="bib.bib44.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Vila: On pre-training for visual language models, 2023.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">Evaluating text-to-visual generation with image-to-text generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2404.01291</span><span id="bib.bib46.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Evaluating text-to-visual generation with image-to-text generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2404.01291</span><span id="bib.bib47.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">World model on million-length video and language with ringattention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.08268</span><span id="bib.bib48.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">Improved baselines with visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.03744</span><span id="bib.bib49.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib50.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib51.7.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.17172</span><span id="bib.bib52.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Video-chatgpt: Towards detailed video understanding via large vision and language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.05424</span><span id="bib.bib53.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Openvid-1m: A large-scale high-quality dataset for text-to-video generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2407.02371</span><span id="bib.bib54.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text" style="font-size:90%;">Chatgpt.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openai.com/blog/chatgpt/</a><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib56.7.2" class="ltx_text" style="font-size:90%;">, 35:27730–27744, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">Sdxl: Improving latent diffusion models for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.01952</span><span id="bib.bib57.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.5.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib58.8.3" class="ltx_text" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
Ar Mohesh Radhakrishnan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.5.1" class="ltx_text" style="font-size:90%;">Is midjourney-ai the new anti-hero of architectural imagery &amp; creativity?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">GSJ</span><span id="bib.bib59.7.2" class="ltx_text" style="font-size:90%;">, 11(1):94–104, 2023.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.5.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib60.8.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.5.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib61.8.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.5.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib62.8.3" class="ltx_text" style="font-size:90%;">, pages 8317–8326, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="font-size:90%;">
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.5.1" class="ltx_text" style="font-size:90%;">Generative multimodal models are in-context learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.13286</span><span id="bib.bib63.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.2.2.1" class="ltx_text" style="font-size:90%;">[64]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.4.1" class="ltx_text" style="font-size:90%;">
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.5.1" class="ltx_text" style="font-size:90%;">Generative pretraining in multimodality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2307.05222</span><span id="bib.bib64.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.2.2.1" class="ltx_text" style="font-size:90%;">[65]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.4.1" class="ltx_text" style="font-size:90%;">
Chameleon Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.5.1" class="ltx_text" style="font-size:90%;">Chameleon: Mixed-modal early-fusion foundation models, 2024.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.2.2.1" class="ltx_text" style="font-size:90%;">[66]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.4.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.5.1" class="ltx_text" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.13971</span><span id="bib.bib66.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.2.2.1" class="ltx_text" style="font-size:90%;">[67]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.4.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.5.1" class="ltx_text" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv:2302.13971</span><span id="bib.bib67.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.2.2.1" class="ltx_text" style="font-size:90%;">[68]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.4.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.5.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.6.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib68.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib68.8.3" class="ltx_text" style="font-size:90%;">, volume 30. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.2.2.1" class="ltx_text" style="font-size:90%;">[69]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.4.1" class="ltx_text" style="font-size:90%;">
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.5.1" class="ltx_text" style="font-size:90%;">Sheared llama: Accelerating language model pre-training via structured pruning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.06694</span><span id="bib.bib69.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.2.2.1" class="ltx_text" style="font-size:90%;">[70]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.4.1" class="ltx_text" style="font-size:90%;">
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.5.1" class="ltx_text" style="font-size:90%;">Show-o: One single transformer to unify multimodal understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2408.12528</span><span id="bib.bib70.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.2.2.1" class="ltx_text" style="font-size:90%;">[71]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.4.1" class="ltx_text" style="font-size:90%;">
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.5.1" class="ltx_text" style="font-size:90%;">Video question answering via gradually refined attention over appearance and motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 25th ACM international conference on Multimedia</span><span id="bib.bib71.8.3" class="ltx_text" style="font-size:90%;">, pages 1645–1653, 2017.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.2.2.1" class="ltx_text" style="font-size:90%;">[72]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.4.1" class="ltx_text" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.5.1" class="ltx_text" style="font-size:90%;">mplug-owl: Modularization empowers large language models with multimodality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.14178</span><span id="bib.bib72.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.2.2.1" class="ltx_text" style="font-size:90%;">[73]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.4.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.5.1" class="ltx_text" style="font-size:90%;">Vector-quantized image modeling with improved vqgan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.04627</span><span id="bib.bib73.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.2.2.1" class="ltx_text" style="font-size:90%;">[74]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.4.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.5.1" class="ltx_text" style="font-size:90%;">Coca: Contrastive captioners are image-text foundation models, 2022.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.2.2.1" class="ltx_text" style="font-size:90%;">[75]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.4.1" class="ltx_text" style="font-size:90%;">
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.5.1" class="ltx_text" style="font-size:90%;">Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.02591</span><span id="bib.bib75.7.2" class="ltx_text" style="font-size:90%;">, 2(3), 2023.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.2.2.1" class="ltx_text" style="font-size:90%;">[76]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.4.1" class="ltx_text" style="font-size:90%;">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.5.1" class="ltx_text" style="font-size:90%;">Mm-vet: Evaluating large multimodal models for integrated capabilities.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.02490</span><span id="bib.bib76.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.2.2.1" class="ltx_text" style="font-size:90%;">[77]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.4.1" class="ltx_text" style="font-size:90%;">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.5.1" class="ltx_text" style="font-size:90%;">Sigmoid loss for language image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib77.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span id="bib.bib77.8.3" class="ltx_text" style="font-size:90%;">, pages 11975–11986, 2023.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.2.2.1" class="ltx_text" style="font-size:90%;">[78]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.4.1" class="ltx_text" style="font-size:90%;">
Hang Zhang, Xin Li, and Lidong Bing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.5.1" class="ltx_text" style="font-size:90%;">Video-llama: An instruction-tuned audio-visual language model for video understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.02858</span><span id="bib.bib78.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.2.2.1" class="ltx_text" style="font-size:90%;">[79]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.4.1" class="ltx_text" style="font-size:90%;">
Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.5.1" class="ltx_text" style="font-size:90%;">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.15112</span><span id="bib.bib79.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.2.2.1" class="ltx_text" style="font-size:90%;">[80]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.4.1" class="ltx_text" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.5.1" class="ltx_text" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.10592</span><span id="bib.bib80.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.2.2.1" class="ltx_text" style="font-size:90%;">[81]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.4.1" class="ltx_text" style="font-size:90%;">
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.5.1" class="ltx_text" style="font-size:90%;">Multimodal c4: An open, billion-scale corpus of images interleaved with text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib81.7.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
</ul>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.04428" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.04429" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.04429">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.04429" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.04430" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:19:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
