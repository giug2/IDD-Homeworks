<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.05214] Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data</title><meta property="og:description" content="In this paper, we propose a method for improving the angular accuracy and photo-reality of gaze and head redirection in full-face images.
The problem with current models is that they cannot handle redirection at large â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.05214">

<!--Generated on Wed Feb 28 06:34:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Gaze and head redirection,  Data augmentation,  3D Face Reconstruction,  and Identity Preservation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiawei Qin
</span><span class="ltx_author_notes">This work was conducted during the first authorâ€™s internship at CyberAgent, Inc.
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">The University of Tokyo
<br class="ltx_break"></span>Tokyo, Japan 
<br class="ltx_break">jqin@iis.u-tokyo.ac.jp
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xueting Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">CyberAgent Inc.
<br class="ltx_break"></span>Tokyo, Japan 
<br class="ltx_break">wang_xueting@cyberagent.co.jp
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">In this paper, we propose a method for improving the angular accuracy and photo-reality of gaze and head redirection in full-face images.
The problem with current models is that they cannot handle redirection at large angles, and this limitation mainly comes from the lack of training data.
To resolve this problem, we create data augmentation by monocular 3D face reconstruction to extend the head pose and gaze range of the real data, which allows the model to handle a wider redirection range.
In addition to the main focus on data augmentation, we also propose a framework with better image quality and identity preservation of unseen subjects even training with synthetic data.
Experiments show that our method significantly improves redirection performance in terms of redirection angular accuracy while maintaining high image quality, especially when redirecting to large angles.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Gaze and head redirection, Data augmentation, 3D Face Reconstruction, and Identity Preservation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, gaze-related research has gained widespread interest due to its significant role in many applications such as human-computer interaction and human behavior analysis.
For example, gaze estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> makes great progress with the development of deep learning.
Redirecting the gaze and head of a given image becomes an important topic due to the growing demand for semi-supervised or unsupervised training for gaze estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and it is also can be applied to diverse digital face synthesis.
There are many works focused on redirecting gaze based on image-warpingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, graphical modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, or generative adversarial network (GAN)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
While these methods did not consider the full-face input including the head pose factor.
Recently, another learning-based face synthesis, ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, has shown the simultaneous redirection of gaze and head for full-face images.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2309.05214/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="447" height="312" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the angle range limitation of current learning-based gaze and head redirection.
For datasets with a large range (top row) and a small range (bottom row), the head pose and gaze distributions along with examples of the redirected images by these training data are shown from left to right.
When training with a smaller angle range, the prior work ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> fails to redirect to a large target angle (bottom), as compared to the wider training data (top).
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Learning-based gaze and head redirection requires gaze and head pose labels during training, and collecting comprehensive training data is a large challenge in gaze redirection.
Specifically, while head pose labels are relatively easy to get, sophisticated devices are required to obtain accurate gaze labels.
As illustrated in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we experimentally showed that the current state-of-the-art model, ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, cannot precisely redirect to a large angle that is out of the range of the training dataset.
Previous in-the-wild datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> have successfully captured diverse environments with large subject scales as some examples are shown in the upper part of Fig.Â <a href="#S3.F2" title="Figure 2 â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, but the gaze range is naturally limited by the devices used for collecting data.
On the other hand, large-angle data can be collected in lab-controlled settingsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, but the high cost makes it difficult to reach a large subject scale.
In addition, the identity similarity and photo-reality of the generated image also drop due to the lack of high-quality gaze datasets with a large number of subject scales.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the above challenges, we propose a redirection framework including creating synthetic training data with extended angle range and a corresponding redirection modelÂ ST-AEDÂ , which learns better using synthetic data with image quality improvement.
For data augmentation, we apply 3D face reconstruction that can preserve the accurate original gaze feature and can be rotated in 3D space without estimation error.
In the redirection modelÂ ST-AEDÂ , to reduce the side effect of synthetic data with relatively more noise, we propose to adopt a higher-level loss instead of the pixel loss used in prior worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Besides, we leverage a state-of-the-art face recognition model as the identity encoder with an identity loss to further constrain the model to have a better appearance feature extraction.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our contribution can be summarized as</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We create augmentation data with a much larger angle range than real data to improve learning-based redirection by 3D face reconstruction.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a gaze and head redirection framework to improve image quality and identity preservation on unseen subjects by a pre-trained identity encoder with a designed identity loss.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experimental results proved that the proposal can redirect the gaze and face to a larger target angle range with improved image quality compared with the SOTA redirection work.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Face Editing by GANs</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">GANs have been widely used in face editing to generate highly realistic imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
With the help of high-quality training datasets such as FFHQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and CelebA-HQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, GAN models can be used to edit facial expressions, age, glasses, pose, and other features.
Many previous works have focused on training GANs to change the pose of a given imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and most of these methods manipulate the head pose in the latent code space to generate an image with a different head pose.
However, these GAN-based methods are not directly applicable to redirecting head and gaze, as they are not trained on datasets with gaze labels.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Data Augmentation by 3D Face Reconstruction</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The rapid development of monocular 3D face reconstruction has led to improvements in other face-related tasks such as face recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and gaze estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
By rotating the 3D reconstructed face, researchers have been able to create more diverse training data for face recognition and extend the range of poses for gaze estimation.
The pixel RGB values of the input image are used as the 3D face texture, preserving the original appearance.
The projective-matching processÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> allows for the 3D face to be rotated and translated within 3D physical space, allowing for arbitrary manipulation and rendering without the need for additional training.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Gaze and Head Redirection</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Gaze redirection was originally for eye-only images by deep warpingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> method or GANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Recently, full-face gaze and head redirectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> becomes a more significant topic.
ParkÂ <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al</span>.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed an encoder-decoder model that disentangles the appearance, head, and gaze features and transforms the source embeddings to target by rotation.
During training, the paired images are used such that the model learns the transformation supervised by the head pose and gaze label.
ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> focused on improving the image quality performance and took into consideration extraneous factors such as illumination and hue by introducing an unsupervised self-learning pipeline.
Consequently, the model enables redirecting a source image to a target image or any target angle.
Learning-based approaches generally require head pose and gaze labels for training and may depend much on the training data.
Although ST-ED is a state-of-the-art model in gaze and head redirection, its ability to redirect to a very large angle with satisfying image quality is not verified.
Collecting training data with a large subject scale and angle range is not a trivial task, but 3D face reconstruction can create an arbitrary amount of face-rotated images with accurate angles without extra training.
Therefore, we consider 3D face reconstruction for data augmentation synthesis to extend the limited angle range of real data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Data Augmentation</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.05214/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="257" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The distribution and samples of original GazeCapture (up) and the augmentation dataset Aug-60H (bottom). </figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Real Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">GazeCapture</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is collected via crowdsourcing when subjects are using tablets, resulting in a large subject scale with more than 1,400 subjects.
It totally contains more than 1.6 million images.
However, the angle range is limited as shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a).
<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">ETH-XGaze</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> put 18 synchronous cameras under different view angles to take pictures, which achieves a very large angle range.
However, it contains only 110 subjects.
Each subject has around 600 frames, and thus more than 1 million images in total.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We follow the split of ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to create GazeCapture Train and GazeCapture Test.
If not specifically mentioned, we abbreviate GazeCapture Train as GazeCapture.
GazeCapture contains 1,177 subjects with 1,379,083 images, and GazeCapture Test contains 139 subjects with 191,842 images.
We also split the public 80 subjects of the official ETH-XGaze into 70 for training and 10 for testing, denoted as <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">XGaze</span> and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">XGaze Test</span>, respectively.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2309.05214/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="119" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The proposedÂ ST-AEDÂ uses a pre-trained AdaFace network as the identity encoder (left).
A mixed reconstruction loss <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="\mathcal{L}_{rec}" display="inline"><semantics id="S3.F3.3.m1.1b"><msub id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F3.3.m1.1.1.2" xref="S3.F3.3.m1.1.1.2.cmml">â„’</mi><mrow id="S3.F3.3.m1.1.1.3" xref="S3.F3.3.m1.1.1.3.cmml"><mi id="S3.F3.3.m1.1.1.3.2" xref="S3.F3.3.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.F3.3.m1.1.1.3.1" xref="S3.F3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.F3.3.m1.1.1.3.3" xref="S3.F3.3.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F3.3.m1.1.1.3.1b" xref="S3.F3.3.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.F3.3.m1.1.1.3.4" xref="S3.F3.3.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><apply id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.3.m1.1.1.1.cmml" xref="S3.F3.3.m1.1.1">subscript</csymbol><ci id="S3.F3.3.m1.1.1.2.cmml" xref="S3.F3.3.m1.1.1.2">â„’</ci><apply id="S3.F3.3.m1.1.1.3.cmml" xref="S3.F3.3.m1.1.1.3"><times id="S3.F3.3.m1.1.1.3.1.cmml" xref="S3.F3.3.m1.1.1.3.1"></times><ci id="S3.F3.3.m1.1.1.3.2.cmml" xref="S3.F3.3.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.F3.3.m1.1.1.3.3.cmml" xref="S3.F3.3.m1.1.1.3.3">ğ‘’</ci><ci id="S3.F3.3.m1.1.1.3.4.cmml" xref="S3.F3.3.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">\mathcal{L}_{rec}</annotation></semantics></math> and identity consistency loss <math id="S3.F3.4.m2.1" class="ltx_Math" alttext="\mathcal{L}_{id}" display="inline"><semantics id="S3.F3.4.m2.1b"><msub id="S3.F3.4.m2.1.1" xref="S3.F3.4.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F3.4.m2.1.1.2" xref="S3.F3.4.m2.1.1.2.cmml">â„’</mi><mrow id="S3.F3.4.m2.1.1.3" xref="S3.F3.4.m2.1.1.3.cmml"><mi id="S3.F3.4.m2.1.1.3.2" xref="S3.F3.4.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.F3.4.m2.1.1.3.1" xref="S3.F3.4.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.F3.4.m2.1.1.3.3" xref="S3.F3.4.m2.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F3.4.m2.1c"><apply id="S3.F3.4.m2.1.1.cmml" xref="S3.F3.4.m2.1.1"><csymbol cd="ambiguous" id="S3.F3.4.m2.1.1.1.cmml" xref="S3.F3.4.m2.1.1">subscript</csymbol><ci id="S3.F3.4.m2.1.1.2.cmml" xref="S3.F3.4.m2.1.1.2">â„’</ci><apply id="S3.F3.4.m2.1.1.3.cmml" xref="S3.F3.4.m2.1.1.3"><times id="S3.F3.4.m2.1.1.3.1.cmml" xref="S3.F3.4.m2.1.1.3.1"></times><ci id="S3.F3.4.m2.1.1.3.2.cmml" xref="S3.F3.4.m2.1.1.3.2">ğ‘–</ci><ci id="S3.F3.4.m2.1.1.3.3.cmml" xref="S3.F3.4.m2.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.m2.1d">\mathcal{L}_{id}</annotation></semantics></math> are computed between the target image and the redirected image (right).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Data Augmentation Creation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We create data augmentation to address the limitation of the above real datasets.
We follow an off-the-shelf 3D face reconstruction pipeline 3DDFAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to do reconstruction and apply the projective-matchingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to finally obtain a 3D face.
Given a target head direction or gaze direction, we can compute a rotation matrix, rotate the face, and render a new image.
And we follow the original settingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to use random images and random colors as the background for the rendered images.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For augmentation data, we filter out subjects with less than 30 samples in the original GazeCapture, and we randomly sample 30 images from the remained 861 subjects.
Each source image will be augmented to 10 new images, and some augmentation data examples are shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
As a result, the augmentation dataset contains 257,470 samples.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">In detail, we sample the target head poses from a circle-shaped uniform distribution with a radius of 60Â°.
We compute a rotation matrix based on the source head pose and the sampled target head pose (<span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">head-based sampling</span>).
Then we rotate the 3D face by this rotation matrix and render new images.
Consequently, the distributions of the original GazeCapture and its augmentation dataset denoted as Aug-60H with â€˜Hâ€™ representing <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_italic">head-based</span>, are shown in Fig.Â <a href="#S3.F2" title="Figure 2 â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Redirection Model ST-AED</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.6" class="ltx_p">Through experiment, we found that the generated image by the state-of-the-art baseline ST-ED still cannot realize the faithful identity for unseen subjects, which also results in the dropping of image quality.
Therefore, we proposeÂ ST-AED, which tackles these problems by extracting better face identity features for enhancing generated image quality.
The framework of our proposed modelÂ ST-AEDÂ is shown in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ III-A Real Datasets â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Following ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we use DenseNet to encode the personal-varying factors including head pose, gaze, and other extraneous factors such as lighting and hue.
Each factor is encoded to a pseudo-condition <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\tilde{c}" display="inline"><semantics id="S4.p1.1.m1.1a"><mover accent="true" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">c</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><ci id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1">~</ci><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\tilde{c}</annotation></semantics></math> and embedding <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">z</annotation></semantics></math>, as illustrated in the left side of Fig.Â <a href="#S3.F3" title="Figure 3 â€£ III-A Real Datasets â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
For head and gaze with the ground-truth label, the transformation of embedding <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">z</annotation></semantics></math> is computed based on the label and the pseudo-condition <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="\tilde{c}" display="inline"><semantics id="S4.p1.4.m4.1a"><mover accent="true" id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">c</mi><mo id="S4.p1.4.m4.1.1.1" xref="S4.p1.4.m4.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><ci id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1.1">~</ci><ci id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">\tilde{c}</annotation></semantics></math>.
The redirected image is formulated as <math id="S4.p1.5.m5.2" class="ltx_Math" alttext="\hat{X}_{t}=D(z^{id}_{s},\hat{z}^{f}_{t})" display="inline"><semantics id="S4.p1.5.m5.2a"><mrow id="S4.p1.5.m5.2.2" xref="S4.p1.5.m5.2.2.cmml"><msub id="S4.p1.5.m5.2.2.4" xref="S4.p1.5.m5.2.2.4.cmml"><mover accent="true" id="S4.p1.5.m5.2.2.4.2" xref="S4.p1.5.m5.2.2.4.2.cmml"><mi id="S4.p1.5.m5.2.2.4.2.2" xref="S4.p1.5.m5.2.2.4.2.2.cmml">X</mi><mo id="S4.p1.5.m5.2.2.4.2.1" xref="S4.p1.5.m5.2.2.4.2.1.cmml">^</mo></mover><mi id="S4.p1.5.m5.2.2.4.3" xref="S4.p1.5.m5.2.2.4.3.cmml">t</mi></msub><mo id="S4.p1.5.m5.2.2.3" xref="S4.p1.5.m5.2.2.3.cmml">=</mo><mrow id="S4.p1.5.m5.2.2.2" xref="S4.p1.5.m5.2.2.2.cmml"><mi id="S4.p1.5.m5.2.2.2.4" xref="S4.p1.5.m5.2.2.2.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.2.2.2.3" xref="S4.p1.5.m5.2.2.2.3.cmml">â€‹</mo><mrow id="S4.p1.5.m5.2.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.p1.5.m5.2.2.2.2.2.3" xref="S4.p1.5.m5.2.2.2.2.3.cmml">(</mo><msubsup id="S4.p1.5.m5.1.1.1.1.1.1" xref="S4.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S4.p1.5.m5.1.1.1.1.1.1.2.2" xref="S4.p1.5.m5.1.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.p1.5.m5.1.1.1.1.1.1.3" xref="S4.p1.5.m5.1.1.1.1.1.1.3.cmml">s</mi><mrow id="S4.p1.5.m5.1.1.1.1.1.1.2.3" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.cmml"><mi id="S4.p1.5.m5.1.1.1.1.1.1.2.3.2" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.1.1.1.1.2.3.1" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.1.1.1.1.2.3.3" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.3.cmml">d</mi></mrow></msubsup><mo id="S4.p1.5.m5.2.2.2.2.2.4" xref="S4.p1.5.m5.2.2.2.2.3.cmml">,</mo><msubsup id="S4.p1.5.m5.2.2.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.2.2.cmml"><mover accent="true" id="S4.p1.5.m5.2.2.2.2.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2.cmml"><mi id="S4.p1.5.m5.2.2.2.2.2.2.2.2.2" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2.2.cmml">z</mi><mo id="S4.p1.5.m5.2.2.2.2.2.2.2.2.1" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.p1.5.m5.2.2.2.2.2.2.3" xref="S4.p1.5.m5.2.2.2.2.2.2.3.cmml">t</mi><mi id="S4.p1.5.m5.2.2.2.2.2.2.2.3" xref="S4.p1.5.m5.2.2.2.2.2.2.2.3.cmml">f</mi></msubsup><mo stretchy="false" id="S4.p1.5.m5.2.2.2.2.2.5" xref="S4.p1.5.m5.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.2b"><apply id="S4.p1.5.m5.2.2.cmml" xref="S4.p1.5.m5.2.2"><eq id="S4.p1.5.m5.2.2.3.cmml" xref="S4.p1.5.m5.2.2.3"></eq><apply id="S4.p1.5.m5.2.2.4.cmml" xref="S4.p1.5.m5.2.2.4"><csymbol cd="ambiguous" id="S4.p1.5.m5.2.2.4.1.cmml" xref="S4.p1.5.m5.2.2.4">subscript</csymbol><apply id="S4.p1.5.m5.2.2.4.2.cmml" xref="S4.p1.5.m5.2.2.4.2"><ci id="S4.p1.5.m5.2.2.4.2.1.cmml" xref="S4.p1.5.m5.2.2.4.2.1">^</ci><ci id="S4.p1.5.m5.2.2.4.2.2.cmml" xref="S4.p1.5.m5.2.2.4.2.2">ğ‘‹</ci></apply><ci id="S4.p1.5.m5.2.2.4.3.cmml" xref="S4.p1.5.m5.2.2.4.3">ğ‘¡</ci></apply><apply id="S4.p1.5.m5.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2"><times id="S4.p1.5.m5.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.3"></times><ci id="S4.p1.5.m5.2.2.2.4.cmml" xref="S4.p1.5.m5.2.2.2.4">ğ·</ci><interval closure="open" id="S4.p1.5.m5.2.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.2.2"><apply id="S4.p1.5.m5.1.1.1.1.1.1.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><apply id="S4.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.1.1.1.1.2.1.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1">superscript</csymbol><ci id="S4.p1.5.m5.1.1.1.1.1.1.2.2.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.2.2">ğ‘§</ci><apply id="S4.p1.5.m5.1.1.1.1.1.1.2.3.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3"><times id="S4.p1.5.m5.1.1.1.1.1.1.2.3.1.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.1"></times><ci id="S4.p1.5.m5.1.1.1.1.1.1.2.3.2.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.2">ğ‘–</ci><ci id="S4.p1.5.m5.1.1.1.1.1.1.2.3.3.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.2.3.3">ğ‘‘</ci></apply></apply><ci id="S4.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S4.p1.5.m5.1.1.1.1.1.1.3">ğ‘ </ci></apply><apply id="S4.p1.5.m5.2.2.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><apply id="S4.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.5.m5.2.2.2.2.2.2.2.1.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2">superscript</csymbol><apply id="S4.p1.5.m5.2.2.2.2.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2"><ci id="S4.p1.5.m5.2.2.2.2.2.2.2.2.1.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2.1">^</ci><ci id="S4.p1.5.m5.2.2.2.2.2.2.2.2.2.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2.2.2.2">ğ‘§</ci></apply><ci id="S4.p1.5.m5.2.2.2.2.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2.2.3">ğ‘“</ci></apply><ci id="S4.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S4.p1.5.m5.2.2.2.2.2.2.3">ğ‘¡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.2c">\hat{X}_{t}=D(z^{id}_{s},\hat{z}^{f}_{t})</annotation></semantics></math>, where <math id="S4.p1.6.m6.3" class="ltx_Math" alttext="\hat{z}^{f}_{t}=T(z^{f}_{s},\hat{c}^{f}_{s},\hat{c}^{f}_{t})" display="inline"><semantics id="S4.p1.6.m6.3a"><mrow id="S4.p1.6.m6.3.3" xref="S4.p1.6.m6.3.3.cmml"><msubsup id="S4.p1.6.m6.3.3.5" xref="S4.p1.6.m6.3.3.5.cmml"><mover accent="true" id="S4.p1.6.m6.3.3.5.2.2" xref="S4.p1.6.m6.3.3.5.2.2.cmml"><mi id="S4.p1.6.m6.3.3.5.2.2.2" xref="S4.p1.6.m6.3.3.5.2.2.2.cmml">z</mi><mo id="S4.p1.6.m6.3.3.5.2.2.1" xref="S4.p1.6.m6.3.3.5.2.2.1.cmml">^</mo></mover><mi id="S4.p1.6.m6.3.3.5.3" xref="S4.p1.6.m6.3.3.5.3.cmml">t</mi><mi id="S4.p1.6.m6.3.3.5.2.3" xref="S4.p1.6.m6.3.3.5.2.3.cmml">f</mi></msubsup><mo id="S4.p1.6.m6.3.3.4" xref="S4.p1.6.m6.3.3.4.cmml">=</mo><mrow id="S4.p1.6.m6.3.3.3" xref="S4.p1.6.m6.3.3.3.cmml"><mi id="S4.p1.6.m6.3.3.3.5" xref="S4.p1.6.m6.3.3.3.5.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.3.3.3.4" xref="S4.p1.6.m6.3.3.3.4.cmml">â€‹</mo><mrow id="S4.p1.6.m6.3.3.3.3.3" xref="S4.p1.6.m6.3.3.3.3.4.cmml"><mo stretchy="false" id="S4.p1.6.m6.3.3.3.3.3.4" xref="S4.p1.6.m6.3.3.3.3.4.cmml">(</mo><msubsup id="S4.p1.6.m6.1.1.1.1.1.1" xref="S4.p1.6.m6.1.1.1.1.1.1.cmml"><mi id="S4.p1.6.m6.1.1.1.1.1.1.2.2" xref="S4.p1.6.m6.1.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.p1.6.m6.1.1.1.1.1.1.3" xref="S4.p1.6.m6.1.1.1.1.1.1.3.cmml">s</mi><mi id="S4.p1.6.m6.1.1.1.1.1.1.2.3" xref="S4.p1.6.m6.1.1.1.1.1.1.2.3.cmml">f</mi></msubsup><mo id="S4.p1.6.m6.3.3.3.3.3.5" xref="S4.p1.6.m6.3.3.3.3.4.cmml">,</mo><msubsup id="S4.p1.6.m6.2.2.2.2.2.2" xref="S4.p1.6.m6.2.2.2.2.2.2.cmml"><mover accent="true" id="S4.p1.6.m6.2.2.2.2.2.2.2.2" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2.cmml"><mi id="S4.p1.6.m6.2.2.2.2.2.2.2.2.2" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2.2.cmml">c</mi><mo id="S4.p1.6.m6.2.2.2.2.2.2.2.2.1" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.p1.6.m6.2.2.2.2.2.2.3" xref="S4.p1.6.m6.2.2.2.2.2.2.3.cmml">s</mi><mi id="S4.p1.6.m6.2.2.2.2.2.2.2.3" xref="S4.p1.6.m6.2.2.2.2.2.2.2.3.cmml">f</mi></msubsup><mo id="S4.p1.6.m6.3.3.3.3.3.6" xref="S4.p1.6.m6.3.3.3.3.4.cmml">,</mo><msubsup id="S4.p1.6.m6.3.3.3.3.3.3" xref="S4.p1.6.m6.3.3.3.3.3.3.cmml"><mover accent="true" id="S4.p1.6.m6.3.3.3.3.3.3.2.2" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2.cmml"><mi id="S4.p1.6.m6.3.3.3.3.3.3.2.2.2" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2.2.cmml">c</mi><mo id="S4.p1.6.m6.3.3.3.3.3.3.2.2.1" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2.1.cmml">^</mo></mover><mi id="S4.p1.6.m6.3.3.3.3.3.3.3" xref="S4.p1.6.m6.3.3.3.3.3.3.3.cmml">t</mi><mi id="S4.p1.6.m6.3.3.3.3.3.3.2.3" xref="S4.p1.6.m6.3.3.3.3.3.3.2.3.cmml">f</mi></msubsup><mo stretchy="false" id="S4.p1.6.m6.3.3.3.3.3.7" xref="S4.p1.6.m6.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.3b"><apply id="S4.p1.6.m6.3.3.cmml" xref="S4.p1.6.m6.3.3"><eq id="S4.p1.6.m6.3.3.4.cmml" xref="S4.p1.6.m6.3.3.4"></eq><apply id="S4.p1.6.m6.3.3.5.cmml" xref="S4.p1.6.m6.3.3.5"><csymbol cd="ambiguous" id="S4.p1.6.m6.3.3.5.1.cmml" xref="S4.p1.6.m6.3.3.5">subscript</csymbol><apply id="S4.p1.6.m6.3.3.5.2.cmml" xref="S4.p1.6.m6.3.3.5"><csymbol cd="ambiguous" id="S4.p1.6.m6.3.3.5.2.1.cmml" xref="S4.p1.6.m6.3.3.5">superscript</csymbol><apply id="S4.p1.6.m6.3.3.5.2.2.cmml" xref="S4.p1.6.m6.3.3.5.2.2"><ci id="S4.p1.6.m6.3.3.5.2.2.1.cmml" xref="S4.p1.6.m6.3.3.5.2.2.1">^</ci><ci id="S4.p1.6.m6.3.3.5.2.2.2.cmml" xref="S4.p1.6.m6.3.3.5.2.2.2">ğ‘§</ci></apply><ci id="S4.p1.6.m6.3.3.5.2.3.cmml" xref="S4.p1.6.m6.3.3.5.2.3">ğ‘“</ci></apply><ci id="S4.p1.6.m6.3.3.5.3.cmml" xref="S4.p1.6.m6.3.3.5.3">ğ‘¡</ci></apply><apply id="S4.p1.6.m6.3.3.3.cmml" xref="S4.p1.6.m6.3.3.3"><times id="S4.p1.6.m6.3.3.3.4.cmml" xref="S4.p1.6.m6.3.3.3.4"></times><ci id="S4.p1.6.m6.3.3.3.5.cmml" xref="S4.p1.6.m6.3.3.3.5">ğ‘‡</ci><vector id="S4.p1.6.m6.3.3.3.3.4.cmml" xref="S4.p1.6.m6.3.3.3.3.3"><apply id="S4.p1.6.m6.1.1.1.1.1.1.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1">subscript</csymbol><apply id="S4.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.1.1.1.1.2.1.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1">superscript</csymbol><ci id="S4.p1.6.m6.1.1.1.1.1.1.2.2.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1.2.2">ğ‘§</ci><ci id="S4.p1.6.m6.1.1.1.1.1.1.2.3.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1.2.3">ğ‘“</ci></apply><ci id="S4.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="S4.p1.6.m6.1.1.1.1.1.1.3">ğ‘ </ci></apply><apply id="S4.p1.6.m6.2.2.2.2.2.2.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.6.m6.2.2.2.2.2.2.1.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2">subscript</csymbol><apply id="S4.p1.6.m6.2.2.2.2.2.2.2.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.6.m6.2.2.2.2.2.2.2.1.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2">superscript</csymbol><apply id="S4.p1.6.m6.2.2.2.2.2.2.2.2.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2"><ci id="S4.p1.6.m6.2.2.2.2.2.2.2.2.1.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2.1">^</ci><ci id="S4.p1.6.m6.2.2.2.2.2.2.2.2.2.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2.2.2.2">ğ‘</ci></apply><ci id="S4.p1.6.m6.2.2.2.2.2.2.2.3.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2.2.3">ğ‘“</ci></apply><ci id="S4.p1.6.m6.2.2.2.2.2.2.3.cmml" xref="S4.p1.6.m6.2.2.2.2.2.2.3">ğ‘ </ci></apply><apply id="S4.p1.6.m6.3.3.3.3.3.3.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.p1.6.m6.3.3.3.3.3.3.1.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3">subscript</csymbol><apply id="S4.p1.6.m6.3.3.3.3.3.3.2.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.p1.6.m6.3.3.3.3.3.3.2.1.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3">superscript</csymbol><apply id="S4.p1.6.m6.3.3.3.3.3.3.2.2.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2"><ci id="S4.p1.6.m6.3.3.3.3.3.3.2.2.1.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2.1">^</ci><ci id="S4.p1.6.m6.3.3.3.3.3.3.2.2.2.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3.2.2.2">ğ‘</ci></apply><ci id="S4.p1.6.m6.3.3.3.3.3.3.2.3.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3.2.3">ğ‘“</ci></apply><ci id="S4.p1.6.m6.3.3.3.3.3.3.3.cmml" xref="S4.p1.6.m6.3.3.3.3.3.3.3">ğ‘¡</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.3c">\hat{z}^{f}_{t}=T(z^{f}_{s},\hat{c}^{f}_{s},\hat{c}^{f}_{t})</annotation></semantics></math>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.3" class="ltx_p">In order to have better generalizability on unseen subjects, we adopt the MS1MV2-pre-trainedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> AdaFaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> model for encoding identity features, instead of the DenseNet trained from scratch in the baseline ST-ED model.
Besides, since the augmentation data contains relatively more noise such as reconstruction artifacts and random background, we use a mixed loss of MS-SSIMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\ell 1" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">â„“</ci><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\ell 1</annotation></semantics></math> loss to avoid over-focusing on the pixel difference inspired fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Formally, given the target image <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="X_{t}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">X</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ‘‹</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">X_{t}</annotation></semantics></math> and the generated image <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\tilde{X_{t}}" display="inline"><semantics id="S4.p2.3.m3.1a"><mover accent="true" id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><msub id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml"><mi id="S4.p2.3.m3.1.1.2.2" xref="S4.p2.3.m3.1.1.2.2.cmml">X</mi><mi id="S4.p2.3.m3.1.1.2.3" xref="S4.p2.3.m3.1.1.2.3.cmml">t</mi></msub><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><ci id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1">~</ci><apply id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.2.1.cmml" xref="S4.p2.3.m3.1.1.2">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.2.cmml" xref="S4.p2.3.m3.1.1.2.2">ğ‘‹</ci><ci id="S4.p2.3.m3.1.1.2.3.cmml" xref="S4.p2.3.m3.1.1.2.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\tilde{X_{t}}</annotation></semantics></math>, the reconstruction loss is formulated as</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{rec}=\alpha(1-\textrm{MS-SSIM}(\tilde{X_{t}},X_{t}))+(1-\alpha)|\tilde{X_{t}}-X_{t}|_{1}." display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.5" xref="S4.E1.m1.2.2.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.1.1.5.2" xref="S4.E1.m1.2.2.1.1.5.2.cmml">â„’</mi><mrow id="S4.E1.m1.2.2.1.1.5.3" xref="S4.E1.m1.2.2.1.1.5.3.cmml"><mi id="S4.E1.m1.2.2.1.1.5.3.2" xref="S4.E1.m1.2.2.1.1.5.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.5.3.1" xref="S4.E1.m1.2.2.1.1.5.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.2.2.1.1.5.3.3" xref="S4.E1.m1.2.2.1.1.5.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.5.3.1a" xref="S4.E1.m1.2.2.1.1.5.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.2.2.1.1.5.3.4" xref="S4.E1.m1.2.2.1.1.5.3.4.cmml">c</mi></mrow></msub><mo id="S4.E1.m1.2.2.1.1.4" xref="S4.E1.m1.2.2.1.1.4.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.3.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">1</mn><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3a.cmml">MS-SSIM</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mover accent="true" id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">X</mi><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">~</mo></mover><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.3.4" xref="S4.E1.m1.2.2.1.1.3.4.cmml">+</mo><mrow id="S4.E1.m1.2.2.1.1.3.3" xref="S4.E1.m1.2.2.1.1.3.3.cmml"><mrow id="S4.E1.m1.2.2.1.1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.cmml"><mn id="S4.E1.m1.2.2.1.1.2.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.2.2.1.1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.1.cmml">âˆ’</mo><mi id="S4.E1.m1.2.2.1.1.2.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.3.cmml">Î±</mi></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.3.3.3" xref="S4.E1.m1.2.2.1.1.3.3.3.cmml">â€‹</mo><msub id="S4.E1.m1.2.2.1.1.3.3.2" xref="S4.E1.m1.2.2.1.1.3.3.2.cmml"><mrow id="S4.E1.m1.2.2.1.1.3.3.2.1.1" xref="S4.E1.m1.2.2.1.1.3.3.2.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.3.3.2.1.1.2" xref="S4.E1.m1.2.2.1.1.3.3.2.1.2.1.cmml">|</mo><mrow id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.cmml"><msub id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.2.cmml">X</mi><mi id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.3.cmml">t</mi></msub><mo id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.1" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.1.cmml">~</mo></mover><mo id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.2.cmml">X</mi><mi id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.3" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.3.3.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.3.2.1.2.1.cmml">|</mo></mrow><mn id="S4.E1.m1.2.2.1.1.3.3.2.3" xref="S4.E1.m1.2.2.1.1.3.3.2.3.cmml">1</mn></msub></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.4"></eq><apply id="S4.E1.m1.2.2.1.1.5.cmml" xref="S4.E1.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.5.1.cmml" xref="S4.E1.m1.2.2.1.1.5">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.5.2.cmml" xref="S4.E1.m1.2.2.1.1.5.2">â„’</ci><apply id="S4.E1.m1.2.2.1.1.5.3.cmml" xref="S4.E1.m1.2.2.1.1.5.3"><times id="S4.E1.m1.2.2.1.1.5.3.1.cmml" xref="S4.E1.m1.2.2.1.1.5.3.1"></times><ci id="S4.E1.m1.2.2.1.1.5.3.2.cmml" xref="S4.E1.m1.2.2.1.1.5.3.2">ğ‘Ÿ</ci><ci id="S4.E1.m1.2.2.1.1.5.3.3.cmml" xref="S4.E1.m1.2.2.1.1.5.3.3">ğ‘’</ci><ci id="S4.E1.m1.2.2.1.1.5.3.4.cmml" xref="S4.E1.m1.2.2.1.1.5.3.4">ğ‘</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3"><plus id="S4.E1.m1.2.2.1.1.3.4.cmml" xref="S4.E1.m1.2.2.1.1.3.4"></plus><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"></times><ci id="S4.E1.m1.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.3">ğ›¼</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.2"></minus><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.3">1</cn><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3"><mtext id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3">MS-SSIM</mtext></ci><interval closure="open" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><ci id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1">~</ci><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">ğ‘‹</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">ğ‘¡</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply></interval></apply></apply></apply><apply id="S4.E1.m1.2.2.1.1.3.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3"><times id="S4.E1.m1.2.2.1.1.3.3.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3.3"></times><apply id="S4.E1.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.1"><minus id="S4.E1.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.1"></minus><cn type="integer" id="S4.E1.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.2">1</cn><ci id="S4.E1.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.2.2.1.1.1.3">ğ›¼</ci></apply><apply id="S4.E1.m1.2.2.1.1.3.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2">subscript</csymbol><apply id="S4.E1.m1.2.2.1.1.3.3.2.1.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1"><abs id="S4.E1.m1.2.2.1.1.3.3.2.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.2"></abs><apply id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1"><minus id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.1"></minus><apply id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2"><ci id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.1">~</ci><apply id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.2">ğ‘‹</ci><ci id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.2.2.3">ğ‘¡</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.2">ğ‘‹</ci><ci id="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.1.1.1.3.3">ğ‘¡</ci></apply></apply></apply><cn type="integer" id="S4.E1.m1.2.2.1.1.3.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.3.2.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\mathcal{L}_{rec}=\alpha(1-\textrm{MS-SSIM}(\tilde{X_{t}},X_{t}))+(1-\alpha)|\tilde{X_{t}}-X_{t}|_{1}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.4" class="ltx_p">While the pre-trained identity encoder is expected to improve the generalization to unseen subjects,
we further add an identity loss to force the decoded image to have a close identity to the target image,
<math id="S4.p2.4.m1.2" class="ltx_Math" alttext="\mathcal{L}_{id}=1-\textrm{Sim}(\tilde{X_{t}},X_{t})," display="inline"><semantics id="S4.p2.4.m1.2a"><mrow id="S4.p2.4.m1.2.2.1" xref="S4.p2.4.m1.2.2.1.1.cmml"><mrow id="S4.p2.4.m1.2.2.1.1" xref="S4.p2.4.m1.2.2.1.1.cmml"><msub id="S4.p2.4.m1.2.2.1.1.3" xref="S4.p2.4.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.4.m1.2.2.1.1.3.2" xref="S4.p2.4.m1.2.2.1.1.3.2.cmml">â„’</mi><mrow id="S4.p2.4.m1.2.2.1.1.3.3" xref="S4.p2.4.m1.2.2.1.1.3.3.cmml"><mi id="S4.p2.4.m1.2.2.1.1.3.3.2" xref="S4.p2.4.m1.2.2.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m1.2.2.1.1.3.3.1" xref="S4.p2.4.m1.2.2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p2.4.m1.2.2.1.1.3.3.3" xref="S4.p2.4.m1.2.2.1.1.3.3.3.cmml">d</mi></mrow></msub><mo id="S4.p2.4.m1.2.2.1.1.2" xref="S4.p2.4.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.p2.4.m1.2.2.1.1.1" xref="S4.p2.4.m1.2.2.1.1.1.cmml"><mn id="S4.p2.4.m1.2.2.1.1.1.3" xref="S4.p2.4.m1.2.2.1.1.1.3.cmml">1</mn><mo id="S4.p2.4.m1.2.2.1.1.1.2" xref="S4.p2.4.m1.2.2.1.1.1.2.cmml">âˆ’</mo><mrow id="S4.p2.4.m1.2.2.1.1.1.1" xref="S4.p2.4.m1.2.2.1.1.1.1.cmml"><mtext id="S4.p2.4.m1.2.2.1.1.1.1.3" xref="S4.p2.4.m1.2.2.1.1.1.1.3a.cmml">Sim</mtext><mo lspace="0em" rspace="0em" id="S4.p2.4.m1.2.2.1.1.1.1.2" xref="S4.p2.4.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.p2.4.m1.2.2.1.1.1.1.1.1" xref="S4.p2.4.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p2.4.m1.2.2.1.1.1.1.1.1.2" xref="S4.p2.4.m1.2.2.1.1.1.1.1.2.cmml">(</mo><mover accent="true" id="S4.p2.4.m1.1.1" xref="S4.p2.4.m1.1.1.cmml"><msub id="S4.p2.4.m1.1.1.2" xref="S4.p2.4.m1.1.1.2.cmml"><mi id="S4.p2.4.m1.1.1.2.2" xref="S4.p2.4.m1.1.1.2.2.cmml">X</mi><mi id="S4.p2.4.m1.1.1.2.3" xref="S4.p2.4.m1.1.1.2.3.cmml">t</mi></msub><mo id="S4.p2.4.m1.1.1.1" xref="S4.p2.4.m1.1.1.1.cmml">~</mo></mover><mo id="S4.p2.4.m1.2.2.1.1.1.1.1.1.3" xref="S4.p2.4.m1.2.2.1.1.1.1.1.2.cmml">,</mo><msub id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S4.p2.4.m1.2.2.1.1.1.1.1.1.4" xref="S4.p2.4.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.p2.4.m1.2.2.1.2" xref="S4.p2.4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m1.2b"><apply id="S4.p2.4.m1.2.2.1.1.cmml" xref="S4.p2.4.m1.2.2.1"><eq id="S4.p2.4.m1.2.2.1.1.2.cmml" xref="S4.p2.4.m1.2.2.1.1.2"></eq><apply id="S4.p2.4.m1.2.2.1.1.3.cmml" xref="S4.p2.4.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.p2.4.m1.2.2.1.1.3.1.cmml" xref="S4.p2.4.m1.2.2.1.1.3">subscript</csymbol><ci id="S4.p2.4.m1.2.2.1.1.3.2.cmml" xref="S4.p2.4.m1.2.2.1.1.3.2">â„’</ci><apply id="S4.p2.4.m1.2.2.1.1.3.3.cmml" xref="S4.p2.4.m1.2.2.1.1.3.3"><times id="S4.p2.4.m1.2.2.1.1.3.3.1.cmml" xref="S4.p2.4.m1.2.2.1.1.3.3.1"></times><ci id="S4.p2.4.m1.2.2.1.1.3.3.2.cmml" xref="S4.p2.4.m1.2.2.1.1.3.3.2">ğ‘–</ci><ci id="S4.p2.4.m1.2.2.1.1.3.3.3.cmml" xref="S4.p2.4.m1.2.2.1.1.3.3.3">ğ‘‘</ci></apply></apply><apply id="S4.p2.4.m1.2.2.1.1.1.cmml" xref="S4.p2.4.m1.2.2.1.1.1"><minus id="S4.p2.4.m1.2.2.1.1.1.2.cmml" xref="S4.p2.4.m1.2.2.1.1.1.2"></minus><cn type="integer" id="S4.p2.4.m1.2.2.1.1.1.3.cmml" xref="S4.p2.4.m1.2.2.1.1.1.3">1</cn><apply id="S4.p2.4.m1.2.2.1.1.1.1.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1"><times id="S4.p2.4.m1.2.2.1.1.1.1.2.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.2"></times><ci id="S4.p2.4.m1.2.2.1.1.1.1.3a.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.3"><mtext id="S4.p2.4.m1.2.2.1.1.1.1.3.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.3">Sim</mtext></ci><interval closure="open" id="S4.p2.4.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1"><apply id="S4.p2.4.m1.1.1.cmml" xref="S4.p2.4.m1.1.1"><ci id="S4.p2.4.m1.1.1.1.cmml" xref="S4.p2.4.m1.1.1.1">~</ci><apply id="S4.p2.4.m1.1.1.2.cmml" xref="S4.p2.4.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p2.4.m1.1.1.2.1.cmml" xref="S4.p2.4.m1.1.1.2">subscript</csymbol><ci id="S4.p2.4.m1.1.1.2.2.cmml" xref="S4.p2.4.m1.1.1.2.2">ğ‘‹</ci><ci id="S4.p2.4.m1.1.1.2.3.cmml" xref="S4.p2.4.m1.1.1.2.3">ğ‘¡</ci></apply></apply><apply id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.p2.4.m1.2.2.1.1.1.1.1.1.1.3">ğ‘¡</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m1.2c">\mathcal{L}_{id}=1-\textrm{Sim}(\tilde{X_{t}},X_{t}),</annotation></semantics></math>
where the identity similarity is computed using another pre-trained AdaFace face recognition network.
The final loss is formulated as</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\textrm{total}}=\mathcal{L}_{\textrm{ST-ED}}+\lambda_{id}\mathcal{L}_{id}+\lambda_{rec}\mathcal{L}_{rec}," display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">â„’</mi><mtext id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3a.cmml">total</mtext></msub><mo id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><msub id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml">â„’</mi><mtext id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3a.cmml">ST-ED</mtext></msub><mo id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml"><msub id="S4.E2.m1.1.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.2.2" xref="S4.E2.m1.1.1.1.1.3.3.2.2.cmml">Î»</mi><mrow id="S4.E2.m1.1.1.1.1.3.3.2.3" xref="S4.E2.m1.1.1.1.1.3.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.2.3.2" xref="S4.E2.m1.1.1.1.1.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.3.2.3.1" xref="S4.E2.m1.1.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.3.2.3.3" xref="S4.E2.m1.1.1.1.1.3.3.2.3.3.cmml">d</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.3.1" xref="S4.E2.m1.1.1.1.1.3.3.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.3.2.cmml">â„’</mi><mrow id="S4.E2.m1.1.1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.3.3.3.1" xref="S4.E2.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.3.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.3.3.cmml">d</mi></mrow></msub></mrow><mo id="S4.E2.m1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.4" xref="S4.E2.m1.1.1.1.1.3.4.cmml"><msub id="S4.E2.m1.1.1.1.1.3.4.2" xref="S4.E2.m1.1.1.1.1.3.4.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.4.2.2" xref="S4.E2.m1.1.1.1.1.3.4.2.2.cmml">Î»</mi><mrow id="S4.E2.m1.1.1.1.1.3.4.2.3" xref="S4.E2.m1.1.1.1.1.3.4.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.4.2.3.2" xref="S4.E2.m1.1.1.1.1.3.4.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.2.3.1" xref="S4.E2.m1.1.1.1.1.3.4.2.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.4.2.3.3" xref="S4.E2.m1.1.1.1.1.3.4.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.2.3.1a" xref="S4.E2.m1.1.1.1.1.3.4.2.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.4.2.3.4" xref="S4.E2.m1.1.1.1.1.3.4.2.3.4.cmml">c</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.1" xref="S4.E2.m1.1.1.1.1.3.4.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.1.1.3.4.3" xref="S4.E2.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.4.3.2" xref="S4.E2.m1.1.1.1.1.3.4.3.2.cmml">â„’</mi><mrow id="S4.E2.m1.1.1.1.1.3.4.3.3" xref="S4.E2.m1.1.1.1.1.3.4.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.2" xref="S4.E2.m1.1.1.1.1.3.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.3.3.1" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.3" xref="S4.E2.m1.1.1.1.1.3.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.3.3.1a" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.4" xref="S4.E2.m1.1.1.1.1.3.4.3.3.4.cmml">c</mi></mrow></msub></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"></eq><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2">â„’</ci><ci id="S4.E2.m1.1.1.1.1.2.3a.cmml" xref="S4.E2.m1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3">total</mtext></ci></apply><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><plus id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></plus><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2">â„’</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3a.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3">ST-ED</mtext></ci></apply><apply id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3"><times id="S4.E2.m1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.1"></times><apply id="S4.E2.m1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2.2">ğœ†</ci><apply id="S4.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2.3"><times id="S4.E2.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2.3.2">ğ‘–</ci><ci id="S4.E2.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2.3.3">ğ‘‘</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.2">â„’</ci><apply id="S4.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.3"><times id="S4.E2.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><ci id="S4.E2.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.3.3">ğ‘‘</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4"><times id="S4.E2.m1.1.1.1.1.3.4.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.1"></times><apply id="S4.E2.m1.1.1.1.1.3.4.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.4.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.4.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.2">ğœ†</ci><apply id="S4.E2.m1.1.1.1.1.3.4.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.3"><times id="S4.E2.m1.1.1.1.1.3.4.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.4.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.3.2">ğ‘Ÿ</ci><ci id="S4.E2.m1.1.1.1.1.3.4.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.3.3">ğ‘’</ci><ci id="S4.E2.m1.1.1.1.1.3.4.2.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2.3.4">ğ‘</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.4.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.4.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.4.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.2">â„’</ci><apply id="S4.E2.m1.1.1.1.1.3.4.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3"><times id="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.2">ğ‘Ÿ</ci><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.3">ğ‘’</ci><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.4">ğ‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\mathcal{L}_{\textrm{total}}=\mathcal{L}_{\textrm{ST-ED}}+\lambda_{id}\mathcal{L}_{id}+\lambda_{rec}\mathcal{L}_{rec},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.6" class="ltx_p">where <math id="S4.p2.5.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\textrm{ST-ED}}" display="inline"><semantics id="S4.p2.5.m1.1a"><msub id="S4.p2.5.m1.1.1" xref="S4.p2.5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.5.m1.1.1.2" xref="S4.p2.5.m1.1.1.2.cmml">â„’</mi><mtext id="S4.p2.5.m1.1.1.3" xref="S4.p2.5.m1.1.1.3a.cmml">ST-ED</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p2.5.m1.1b"><apply id="S4.p2.5.m1.1.1.cmml" xref="S4.p2.5.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m1.1.1.1.cmml" xref="S4.p2.5.m1.1.1">subscript</csymbol><ci id="S4.p2.5.m1.1.1.2.cmml" xref="S4.p2.5.m1.1.1.2">â„’</ci><ci id="S4.p2.5.m1.1.1.3a.cmml" xref="S4.p2.5.m1.1.1.3"><mtext mathsize="70%" id="S4.p2.5.m1.1.1.3.cmml" xref="S4.p2.5.m1.1.1.3">ST-ED</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m1.1c">\mathcal{L}_{\textrm{ST-ED}}</annotation></semantics></math> represents the losses used in ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> excluding the original <math id="S4.p2.6.m2.1" class="ltx_Math" alttext="\ell 1" display="inline"><semantics id="S4.p2.6.m2.1a"><mrow id="S4.p2.6.m2.1.1" xref="S4.p2.6.m2.1.1.cmml"><mi mathvariant="normal" id="S4.p2.6.m2.1.1.2" xref="S4.p2.6.m2.1.1.2.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S4.p2.6.m2.1.1.1" xref="S4.p2.6.m2.1.1.1.cmml">â€‹</mo><mn id="S4.p2.6.m2.1.1.3" xref="S4.p2.6.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m2.1b"><apply id="S4.p2.6.m2.1.1.cmml" xref="S4.p2.6.m2.1.1"><times id="S4.p2.6.m2.1.1.1.cmml" xref="S4.p2.6.m2.1.1.1"></times><ci id="S4.p2.6.m2.1.1.2.cmml" xref="S4.p2.6.m2.1.1.2">â„“</ci><cn type="integer" id="S4.p2.6.m2.1.1.3.cmml" xref="S4.p2.6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m2.1c">\ell 1</annotation></semantics></math> loss.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p"><span id="S4.p3.3.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
We follow the same setting as the original ST-EDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for the normalization settings and training hyperparameters including the learning rate.
The weight for the mixed reconstruction loss is <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\alpha=0.84" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">Î±</mi><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">0.84</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></eq><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">ğ›¼</ci><cn type="float" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">0.84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\alpha=0.84</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
We emperically set <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\lambda_{id}=2" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><msub id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml"><mi id="S4.p3.2.m2.1.1.2.2" xref="S4.p3.2.m2.1.1.2.2.cmml">Î»</mi><mrow id="S4.p3.2.m2.1.1.2.3" xref="S4.p3.2.m2.1.1.2.3.cmml"><mi id="S4.p3.2.m2.1.1.2.3.2" xref="S4.p3.2.m2.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.2.3.1" xref="S4.p3.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p3.2.m2.1.1.2.3.3" xref="S4.p3.2.m2.1.1.2.3.3.cmml">d</mi></mrow></msub><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><eq id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></eq><apply id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.2.1.cmml" xref="S4.p3.2.m2.1.1.2">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.2.cmml" xref="S4.p3.2.m2.1.1.2.2">ğœ†</ci><apply id="S4.p3.2.m2.1.1.2.3.cmml" xref="S4.p3.2.m2.1.1.2.3"><times id="S4.p3.2.m2.1.1.2.3.1.cmml" xref="S4.p3.2.m2.1.1.2.3.1"></times><ci id="S4.p3.2.m2.1.1.2.3.2.cmml" xref="S4.p3.2.m2.1.1.2.3.2">ğ‘–</ci><ci id="S4.p3.2.m2.1.1.2.3.3.cmml" xref="S4.p3.2.m2.1.1.2.3.3">ğ‘‘</ci></apply></apply><cn type="integer" id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\lambda_{id}=2</annotation></semantics></math> and <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="\lambda_{rec}=200" display="inline"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><msub id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml"><mi id="S4.p3.3.m3.1.1.2.2" xref="S4.p3.3.m3.1.1.2.2.cmml">Î»</mi><mrow id="S4.p3.3.m3.1.1.2.3" xref="S4.p3.3.m3.1.1.2.3.cmml"><mi id="S4.p3.3.m3.1.1.2.3.2" xref="S4.p3.3.m3.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.2.3.1" xref="S4.p3.3.m3.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p3.3.m3.1.1.2.3.3" xref="S4.p3.3.m3.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.2.3.1a" xref="S4.p3.3.m3.1.1.2.3.1.cmml">â€‹</mo><mi id="S4.p3.3.m3.1.1.2.3.4" xref="S4.p3.3.m3.1.1.2.3.4.cmml">c</mi></mrow></msub><mo id="S4.p3.3.m3.1.1.1" xref="S4.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><eq id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1"></eq><apply id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.2.1.cmml" xref="S4.p3.3.m3.1.1.2">subscript</csymbol><ci id="S4.p3.3.m3.1.1.2.2.cmml" xref="S4.p3.3.m3.1.1.2.2">ğœ†</ci><apply id="S4.p3.3.m3.1.1.2.3.cmml" xref="S4.p3.3.m3.1.1.2.3"><times id="S4.p3.3.m3.1.1.2.3.1.cmml" xref="S4.p3.3.m3.1.1.2.3.1"></times><ci id="S4.p3.3.m3.1.1.2.3.2.cmml" xref="S4.p3.3.m3.1.1.2.3.2">ğ‘Ÿ</ci><ci id="S4.p3.3.m3.1.1.2.3.3.cmml" xref="S4.p3.3.m3.1.1.2.3.3">ğ‘’</ci><ci id="S4.p3.3.m3.1.1.2.3.4.cmml" xref="S4.p3.3.m3.1.1.2.3.4">ğ‘</ci></apply></apply><cn type="integer" id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">\lambda_{rec}=200</annotation></semantics></math>.
The AdaFace encoder is a ResNet-50 modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and we intentionally freeze it and only train the other parts DenseNet encoder and decoder to avoid overfitting on the training subjects.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span> Redirection performance on XGaze Test. The first column is the training data and the second column is the redirection model.
<math id="S5.T1.2.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T1.2.m1.1b"><mo id="S5.T1.2.m1.1.1" xref="S5.T1.2.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.m1.1c"><ci id="S5.T1.2.m1.1.1.cmml" xref="S5.T1.2.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.m1.1d">\dagger</annotation></semantics></math>: GC short for GazeCapture</figcaption>
<div id="S5.T1.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:394.6pt;height:138.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(17.9pt,-6.3pt) scale(1.1,1.1) ;">
<table id="S5.T1.8.6" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.7.5.5" class="ltx_tr">
<td id="S5.T1.7.5.5.6" class="ltx_td ltx_align_left ltx_border_t">Training Data</td>
<td id="S5.T1.7.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model</td>
<td id="S5.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Head <math id="S5.T1.3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.3.1.1.1.m1.1a"><mo stretchy="false" id="S5.T1.3.1.1.1.m1.1.1" xref="S5.T1.3.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.1.1.1.m1.1b"><ci id="S5.T1.3.1.1.1.m1.1.1.cmml" xref="S5.T1.3.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Gaze <math id="S5.T1.4.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.4.2.2.2.m1.1a"><mo stretchy="false" id="S5.T1.4.2.2.2.m1.1.1" xref="S5.T1.4.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.2.2.2.m1.1b"><ci id="S5.T1.4.2.2.2.m1.1.1.cmml" xref="S5.T1.4.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T1.5.3.3.3" class="ltx_td ltx_align_center ltx_border_t">LPIPS <math id="S5.T1.5.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.5.3.3.3.m1.1a"><mo stretchy="false" id="S5.T1.5.3.3.3.m1.1.1" xref="S5.T1.5.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.3.3.3.m1.1b"><ci id="S5.T1.5.3.3.3.m1.1.1.cmml" xref="S5.T1.5.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T1.6.4.4.4" class="ltx_td ltx_align_center ltx_border_t">FID <math id="S5.T1.6.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.6.4.4.4.m1.1a"><mo stretchy="false" id="S5.T1.6.4.4.4.m1.1.1" xref="S5.T1.6.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.4.4.4.m1.1b"><ci id="S5.T1.6.4.4.4.m1.1.1.cmml" xref="S5.T1.6.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T1.7.5.5.5" class="ltx_td ltx_align_center ltx_border_t">Sim. (<math id="S5.T1.7.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.7.5.5.5.m1.1a"><mo stretchy="false" id="S5.T1.7.5.5.5.m1.1.1" xref="S5.T1.7.5.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.5.5.5.m1.1b"><ci id="S5.T1.7.5.5.5.m1.1.1.cmml" xref="S5.T1.7.5.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.5.5.5.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr id="S5.T1.8.6.7" class="ltx_tr">
<td id="S5.T1.8.6.7.1" class="ltx_td ltx_align_left ltx_border_t">XGaze-SH</td>
<td id="S5.T1.8.6.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ST-ED</td>
<td id="S5.T1.8.6.7.3" class="ltx_td ltx_align_center ltx_border_t">6.13</td>
<td id="S5.T1.8.6.7.4" class="ltx_td ltx_align_center ltx_border_t">11.56</td>
<td id="S5.T1.8.6.7.5" class="ltx_td ltx_align_center ltx_border_t">0.203</td>
<td id="S5.T1.8.6.7.6" class="ltx_td ltx_align_center ltx_border_t">48.01</td>
<td id="S5.T1.8.6.7.7" class="ltx_td ltx_align_center ltx_border_t">0.267</td>
</tr>
<tr id="S5.T1.8.6.8" class="ltx_tr">
<td id="S5.T1.8.6.8.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S5.T1.8.6.8.1.1" class="ltx_text">XGaze</span></td>
<td id="S5.T1.8.6.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ST-ED</td>
<td id="S5.T1.8.6.8.3" class="ltx_td ltx_align_center ltx_border_t">2.92</td>
<td id="S5.T1.8.6.8.4" class="ltx_td ltx_align_center ltx_border_t">7.93</td>
<td id="S5.T1.8.6.8.5" class="ltx_td ltx_align_center ltx_border_t">0.173</td>
<td id="S5.T1.8.6.8.6" class="ltx_td ltx_align_center ltx_border_t">42.88</td>
<td id="S5.T1.8.6.8.7" class="ltx_td ltx_align_center ltx_border_t">0.331</td>
</tr>
<tr id="S5.T1.8.6.9" class="ltx_tr">
<td id="S5.T1.8.6.9.1" class="ltx_td ltx_align_left ltx_border_r">ST-AED</td>
<td id="S5.T1.8.6.9.2" class="ltx_td ltx_align_center">2.94</td>
<td id="S5.T1.8.6.9.3" class="ltx_td ltx_align_center">7.56</td>
<td id="S5.T1.8.6.9.4" class="ltx_td ltx_align_center">0.163</td>
<td id="S5.T1.8.6.9.5" class="ltx_td ltx_align_center">36.30</td>
<td id="S5.T1.8.6.9.6" class="ltx_td ltx_align_center">0.351</td>
</tr>
<tr id="S5.T1.8.6.6" class="ltx_tr">
<td id="S5.T1.8.6.6.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">Â </span>
GC<math id="S5.T1.8.6.6.1.m1.1" class="ltx_Math" alttext="~{}\dagger" display="inline"><semantics id="S5.T1.8.6.6.1.m1.1a"><mo id="S5.T1.8.6.6.1.m1.1.1" xref="S5.T1.8.6.6.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.6.6.1.m1.1b"><ci id="S5.T1.8.6.6.1.m1.1.1.cmml" xref="S5.T1.8.6.6.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.6.6.1.m1.1c">~{}\dagger</annotation></semantics></math>
</td>
<td id="S5.T1.8.6.6.2" class="ltx_td ltx_align_left ltx_border_r">ST-ED</td>
<td id="S5.T1.8.6.6.3" class="ltx_td ltx_align_center">22.04</td>
<td id="S5.T1.8.6.6.4" class="ltx_td ltx_align_center">33.83</td>
<td id="S5.T1.8.6.6.5" class="ltx_td ltx_align_center">0.380</td>
<td id="S5.T1.8.6.6.6" class="ltx_td ltx_align_center">137.97</td>
<td id="S5.T1.8.6.6.7" class="ltx_td ltx_align_center">0.118</td>
</tr>
<tr id="S5.T1.8.6.10" class="ltx_tr">
<td id="S5.T1.8.6.10.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" rowspan="2"><span id="S5.T1.8.6.10.1.1" class="ltx_text">GC + Aug-60H</span></td>
<td id="S5.T1.8.6.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ST-ED</td>
<td id="S5.T1.8.6.10.3" class="ltx_td ltx_align_center ltx_border_t">9.27</td>
<td id="S5.T1.8.6.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.8.6.10.4.1" class="ltx_text ltx_font_bold">18.89</span></td>
<td id="S5.T1.8.6.10.5" class="ltx_td ltx_align_center ltx_border_t">0.324</td>
<td id="S5.T1.8.6.10.6" class="ltx_td ltx_align_center ltx_border_t">116.10</td>
<td id="S5.T1.8.6.10.7" class="ltx_td ltx_align_center ltx_border_t">0.142</td>
</tr>
<tr id="S5.T1.8.6.11" class="ltx_tr">
<td id="S5.T1.8.6.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">ST-AED</td>
<td id="S5.T1.8.6.11.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.8.6.11.2.1" class="ltx_text ltx_font_bold">8.08</span></td>
<td id="S5.T1.8.6.11.3" class="ltx_td ltx_align_center ltx_border_b">19.03</td>
<td id="S5.T1.8.6.11.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.8.6.11.4.1" class="ltx_text ltx_font_bold">0.290</span></td>
<td id="S5.T1.8.6.11.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.8.6.11.5.1" class="ltx_text ltx_font_bold">109.08</span></td>
<td id="S5.T1.8.6.11.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.8.6.11.6.1" class="ltx_text ltx_font_bold">0.210</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Experimental Settings</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">For evaluation setting, previous work calculates the redirection error of a model when redirecting to a target image (<span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">redirect-to-image</span>) by using ground truth paired imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
However, this evaluation method is limited by the angle range of the test data, that is, it cannot evaluate the redirection error under large target angles if the test dataset has a limited range.
Therefore, we also evaluate the redÃ¥irection error when directly redirecting to a target angle (<span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">redirect-to-angle</span>).
Specifically, we randomly sample 10 new directions from a uniform distribution with a radius of 60Â°.
We compute the rotation matrix based on the source and target head pose and apply the rotation matrix on both the head and gaze embeddings.
We sample 20 samples from each subject of GazeCapture as the source image since GazeCaptureâ€™s images are almost frontal.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To obtain the head pose and gaze direction of the generated image, we use a ResNet-18Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> estimator.
The estimator is trained on the corresponding dataset when evaluating the model on each dataset such that it can predict accurate head pose and gaze.
The average estimation errors of head/gaze for GazeCapture and ETH-XGaze are 0.88/1.37 degrees and 0.61/0.83 degrees, respectively, indicating that the estimator is reliable.
For pre-processing, we adopt the data normalizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which is commonly used in gaze-related tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation Method</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Metrics</span>
We compute the <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">redirection angular error</span> between the ground-truth target direction and the estimated direction of the generated image.
Both the head redirection error and the gaze redirection error are evaluated.
We adopt <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">LPIPS</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to evaluate the general similarity of the image between the generated image and the target image.
Besides the above metrics used in previous works, we further evaluate the image quality of generation by <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_bold">FrÃ©chet Inception Distance</span> (FID)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which has been widely used to evaluate the visual similarity of two groups of images that is close to human perception.
In addition, to evaluate the identity preservation, we also compute the <span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_bold">identity similarity</span> between the target image and generated image using AdaFaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Notice this evaluator is different from the one used for training.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Baseline methods</span>
Besides, since we focus on data creation, we only compare with the SOTA generative model <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_bold">ST-ED</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which hugely outperforms most previous methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and is still more indicative and suggestive than the latest neural-radiance-fields-based (NeRF) redirection methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The average angular error (degree) of redirection to target direction.
<math id="S5.T2.2.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T2.2.m1.1b"><mo id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><ci id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">\dagger</annotation></semantics></math>: GC short for GazeCapture
</figcaption>
<div id="S5.T2.5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:289.6pt;height:75.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.9pt,-1.8pt) scale(1.05,1.05) ;">
<table id="S5.T2.5.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.4.2.2.2" class="ltx_tr">
<td id="S5.T2.4.2.2.2.3" class="ltx_td ltx_align_left ltx_border_t">Training Data</td>
<td id="S5.T2.4.2.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model</td>
<td id="S5.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Head Error <math id="S5.T2.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.3.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.3.1.1.1.1.m1.1.1" xref="S5.T2.3.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.1.1.1.m1.1b"><ci id="S5.T2.3.1.1.1.1.m1.1.1.cmml" xref="S5.T2.3.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T2.4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Gaze Error <math id="S5.T2.4.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.4.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T2.4.2.2.2.2.m1.1.1" xref="S5.T2.4.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.2.2.2.m1.1b"><ci id="S5.T2.4.2.2.2.2.m1.1.1.cmml" xref="S5.T2.4.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T2.5.3.3.3" class="ltx_tr">
<td id="S5.T2.5.3.3.3.1" class="ltx_td ltx_align_left ltx_border_t">GCÂ <math id="S5.T2.5.3.3.3.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S5.T2.5.3.3.3.1.m1.1a"><mo id="S5.T2.5.3.3.3.1.m1.1.1" xref="S5.T2.5.3.3.3.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.3.3.3.1.m1.1b"><ci id="S5.T2.5.3.3.3.1.m1.1.1.cmml" xref="S5.T2.5.3.3.3.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.3.3.3.1.m1.1c">\dagger</annotation></semantics></math>
</td>
<td id="S5.T2.5.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ST-ED</td>
<td id="S5.T2.5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">4.96</td>
<td id="S5.T2.5.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">16.42</td>
</tr>
<tr id="S5.T2.5.3.3.4" class="ltx_tr">
<td id="S5.T2.5.3.3.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" rowspan="2"><span id="S5.T2.5.3.3.4.1.1" class="ltx_text">GC + Aug-60H</span></td>
<td id="S5.T2.5.3.3.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ST-ED</td>
<td id="S5.T2.5.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">3.63</td>
<td id="S5.T2.5.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">12.37</td>
</tr>
<tr id="S5.T2.5.3.3.5" class="ltx_tr">
<td id="S5.T2.5.3.3.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">ST-AED</td>
<td id="S5.T2.5.3.3.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.5.3.3.5.2.1" class="ltx_text ltx_font_bold">3.67</span></td>
<td id="S5.T2.5.3.3.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.5.3.3.5.3.1" class="ltx_text ltx_font_bold">11.09</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2309.05214/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The error reduction (degree) distribution of head pose (left side) and gaze (right side) by different target angle for each setting by the proposed augmentation Aug-60H andÂ ST-AED.
The error reduction of the proposal is more obvious under larger target angles.
</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Experiments Result</span>
</h3>

</section>
<section id="S5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Redirection</h3>

<div id="S5.SSx1.p1" class="ltx_para">
<p id="S5.SSx1.p1.1" class="ltx_p">For the <span id="S5.SSx1.p1.1.1" class="ltx_text ltx_font_italic">redirect-to-image</span> result shown in TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we use XGaze Test as the testing data.
TableÂ <a href="#S5.T2" title="TABLE II â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the result of the <span id="S5.SSx1.p1.1.2" class="ltx_text ltx_font_italic">redirect-to-angle</span> evaluation.
As a reference, we first compare the results of using XGaze and its subset XGaze-SH as training data, the angle distribution and qualitative results of which were previously shown in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The subset XGaze-SH with a small angle range showed worse performance than XGaze on all metrics in TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, showing the significant influence of the limited angle range of the training data.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2309.05214/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="320" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> Examples of redirecting images to the target direction.
The values at the bottom are the redirection error of the head pose (left) and gaze (right), and the arrows are the visualization of the ground truth and the estimated head pose.</figcaption>
</figure>
<div id="S5.SSx1.p2" class="ltx_para">
<p id="S5.SSx1.p2.1" class="ltx_p"><span id="S5.SSx1.p2.1.1" class="ltx_text ltx_font_bold">Angle Extension.</span>
For GazeCapture as training data (GC), as shown in the fourth row of TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>,
it showed very large redirection errors since XGaze Test has a much larger direction range than GazeCapture.
Similar results can also be observed from the first row of TableÂ <a href="#S5.T2" title="TABLE II â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
The proposed augmentation-based training data (GC+Aug-60H), as expected, showed a significant reduction in the head redirection error (by about 13Â°) and gaze redirection error (by about 15Â°) because it extends the angle range of training data to be similar to the XGaze Test.
In TableÂ <a href="#S5.T2" title="TABLE II â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, since the process of creating augmentation data as described in SectionÂ <a href="#S3.SS2" title="III-B Data Augmentation Creation â€£ III Data Augmentation â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> is similar to the <span id="S5.SSx1.p2.1.2" class="ltx_text ltx_font_italic">redirect-to-angle</span> pattern, the data augmentation shows effective reduction on both head and gaze error.
For the further reduction ofÂ ST-AEDÂ in the last row of both TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and TableÂ <a href="#S5.T2" title="TABLE II â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, it is reasonable to speculate that it is partially due to the improved image quality, making the estimator easier to predict the direction.</p>
</div>
<div id="S5.SSx1.p3" class="ltx_para">
<p id="S5.SSx1.p3.1" class="ltx_p">Fig.Â <a href="#S5.F4" title="Figure 4 â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> further shows the redirection error reduction distribution according to its target angle.
The reduction is based on the GazeCapture-only (GC) baseline, which corresponds to the first row in TableÂ <a href="#S5.T2" title="TABLE II â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
The negative values mean that the error is reduced from the baseline.
We can observe that the error reduction of the proposal is more obvious and effective under large target angles.
From the examples of redirected images in Fig.Â <a href="#S5.F5" title="Figure 5 â€£ Redirection â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we can observe that the source image can be redirected to a larger angle through the proposed data augmentation.</p>
</div>
<div id="S5.SSx1.p4" class="ltx_para">
<p id="S5.SSx1.p4.1" class="ltx_p"><span id="S5.SSx1.p4.1.1" class="ltx_text ltx_font_bold">Image Quality.</span>
First, by comparing GC and GC+Aug-60H (ST-ED) in TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the proposed augmentation (Aug-60H) also improved the image quality on metrics of LPIPS, FID, and identity similarity, because it can generate better head redirection.
Moreover, the proposedÂ ST-AEDÂ further improved the identity preservation and image quality, as can be seen from the last row of TableÂ <a href="#S5.T1" title="TABLE I â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
From the qualitative results in Fig.Â <a href="#S5.F4" title="Figure 4 â€£ V-B Evaluation Method â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can observe that the images generated byÂ ST-AEDÂ (the third and sixth rows) show more photo-realistic appearance than those by the ST-ED baseline.</p>
</div>
</section>
<section id="S5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_italic ltx_title_subsection">Ablation Studies</h3>

<figure id="S5.F6" class="ltx_figure"><img src="/html/2309.05214/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The label distribution of GazeCapture and the augmentation datasets.
The top row is the head pose distribution and the bottom row is the gaze distribution.</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2309.05214/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> Examples of <span id="S5.F7.2.1" class="ltx_text ltx_font_italic">redirect both</span>.
The blue and yellow numbers correspond to the redirection error of head and gaze, respectively.
The red and yellow arrows correspond to the ground-truth gaze direction and the estimated gaze direction, respectively.</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2309.05214/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Examples of <span id="S5.F8.2.1" class="ltx_text ltx_font_italic">redirect gaze</span>.
</figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2309.05214/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Examples of <span id="S5.F9.2.1" class="ltx_text ltx_font_italic">redirect head</span>.
</figcaption>
</figure>
<div id="S5.SSx2.p1" class="ltx_para">
<p id="S5.SSx2.p1.1" class="ltx_p"><span id="S5.SSx2.p1.1.1" class="ltx_text ltx_font_bold">Data creation patterns.</span>
Besides the <span id="S5.SSx2.p1.1.2" class="ltx_text ltx_font_italic">head-based</span> sampling, we also create augÃ¥mentation datasets with <span id="S5.SSx2.p1.1.3" class="ltx_text ltx_font_italic">gaze-based</span> sampling, where the rotation matrix is computed based on the source and target gaze direction.
In addition to the 60-degree sampling, to further evaluate the impact of the angle range of the augmentation data, we also sample the target direction from a 40-degree circle-shaped uniform distribution.
Besides augmenting based on the head pose, we also generate samples based on the target gaze.
As a result, weâ€™ve produced four augmentation datasets for comparison, named Aug-40G, Aug-40H, Aug-60G, and Aug-60H, whose distributions are shown in Fig.Â <a href="#S5.F6" title="Figure 6 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, where G and H represent head-pose-based sampling and gaze-based sampling, respectively.</p>
</div>
<div id="S5.SSx2.p2" class="ltx_para">
<p id="S5.SSx2.p2.1" class="ltx_p">We compare these datasets by the <span id="S5.SSx2.p2.1.1" class="ltx_text ltx_font_italic">redirect-to-angle</span> evaluation.
In addition, we consider three redirection patterns.
1) Redirect both: we compute the rotation matrix based on source and target head pose, and apply the rotation matrix on head and gaze embeddings at the same time;
2) Gaze only: we compute the rotation matrix based on source and target gaze, and apply the rotation matrix only on gaze embeddings, which means fixing the original head pose;
3) Head only: we compute the rotation matrix based on the source and target head pose, and apply the rotation matrix only on head embeddings, which means fixing the original gaze direction.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>The redirection error in three patterns for GazeCapture and GazeCapture with different augmentation datasets.</figcaption>
<div id="S5.T3.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:363.8pt;height:138.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(16.5pt,-6.3pt) scale(1.1,1.1) ;">
<table id="S5.T3.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.6.6.7" class="ltx_tr">
<td id="S5.T3.6.6.7.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.6.6.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Both</td>
<td id="S5.T3.6.6.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Gaze Only</td>
<td id="S5.T3.6.6.7.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Head Only</td>
</tr>
<tr id="S5.T3.6.6.6" class="ltx_tr">
<td id="S5.T3.6.6.6.7" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center">Head <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Gaze <math id="S5.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_align_center">Head <math id="S5.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">Gaze <math id="S5.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T3.4.4.4.4.m1.1.1" xref="S5.T3.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.5.5.5.5" class="ltx_td ltx_align_center">Head <math id="S5.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S5.T3.5.5.5.5.m1.1.1" xref="S5.T3.5.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.m1.1b"><ci id="S5.T3.5.5.5.5.m1.1.1.cmml" xref="S5.T3.5.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.6.6.6.6" class="ltx_td ltx_align_center">Gaze <math id="S5.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S5.T3.6.6.6.6.m1.1.1" xref="S5.T3.6.6.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.m1.1b"><ci id="S5.T3.6.6.6.6.m1.1.1.cmml" xref="S5.T3.6.6.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.6.6.8" class="ltx_tr">
<td id="S5.T3.6.6.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GazeCapture</td>
<td id="S5.T3.6.6.8.2" class="ltx_td ltx_align_center ltx_border_t">5.85</td>
<td id="S5.T3.6.6.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.15</td>
<td id="S5.T3.6.6.8.4" class="ltx_td ltx_align_center ltx_border_t">1.62</td>
<td id="S5.T3.6.6.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.59</td>
<td id="S5.T3.6.6.8.6" class="ltx_td ltx_align_center ltx_border_t">5.81</td>
<td id="S5.T3.6.6.8.7" class="ltx_td ltx_align_center ltx_border_t">3.81</td>
</tr>
<tr id="S5.T3.6.6.9" class="ltx_tr">
<td id="S5.T3.6.6.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">+ Aug-40G</td>
<td id="S5.T3.6.6.9.2" class="ltx_td ltx_align_center ltx_border_t">4.62</td>
<td id="S5.T3.6.6.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.67</td>
<td id="S5.T3.6.6.9.4" class="ltx_td ltx_align_center ltx_border_t">1.66</td>
<td id="S5.T3.6.6.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.53</td>
<td id="S5.T3.6.6.9.6" class="ltx_td ltx_align_center ltx_border_t">4.28</td>
<td id="S5.T3.6.6.9.7" class="ltx_td ltx_align_center ltx_border_t">4.09</td>
</tr>
<tr id="S5.T3.6.6.10" class="ltx_tr">
<td id="S5.T3.6.6.10.1" class="ltx_td ltx_align_left ltx_border_r">+ Aug-40H</td>
<td id="S5.T3.6.6.10.2" class="ltx_td ltx_align_center">4.28</td>
<td id="S5.T3.6.6.10.3" class="ltx_td ltx_align_center ltx_border_r">12.52</td>
<td id="S5.T3.6.6.10.4" class="ltx_td ltx_align_center">1.63</td>
<td id="S5.T3.6.6.10.5" class="ltx_td ltx_align_center ltx_border_r">11.22</td>
<td id="S5.T3.6.6.10.6" class="ltx_td ltx_align_center">4.32</td>
<td id="S5.T3.6.6.10.7" class="ltx_td ltx_align_center">4.01</td>
</tr>
<tr id="S5.T3.6.6.11" class="ltx_tr">
<td id="S5.T3.6.6.11.1" class="ltx_td ltx_align_left ltx_border_r">+ Aug-60G</td>
<td id="S5.T3.6.6.11.2" class="ltx_td ltx_align_center">4.17</td>
<td id="S5.T3.6.6.11.3" class="ltx_td ltx_align_center ltx_border_r">12.63</td>
<td id="S5.T3.6.6.11.4" class="ltx_td ltx_align_center"><span id="S5.T3.6.6.11.4.1" class="ltx_text ltx_font_bold">1.46</span></td>
<td id="S5.T3.6.6.11.5" class="ltx_td ltx_align_center ltx_border_r">11.16</td>
<td id="S5.T3.6.6.11.6" class="ltx_td ltx_align_center"><span id="S5.T3.6.6.11.6.1" class="ltx_text ltx_font_bold">4.16</span></td>
<td id="S5.T3.6.6.11.7" class="ltx_td ltx_align_center">3.83</td>
</tr>
<tr id="S5.T3.6.6.12" class="ltx_tr">
<td id="S5.T3.6.6.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">+ Aug-60H</td>
<td id="S5.T3.6.6.12.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.6.6.12.2.1" class="ltx_text ltx_font_bold">3.67</span></td>
<td id="S5.T3.6.6.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.6.6.12.3.1" class="ltx_text ltx_font_bold">11.09</span></td>
<td id="S5.T3.6.6.12.4" class="ltx_td ltx_align_center ltx_border_b">1.57</td>
<td id="S5.T3.6.6.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.6.6.12.5.1" class="ltx_text ltx_font_bold">10.76</span></td>
<td id="S5.T3.6.6.12.6" class="ltx_td ltx_align_center ltx_border_b">4.24</td>
<td id="S5.T3.6.6.12.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.6.6.12.7.1" class="ltx_text ltx_font_bold">3.81</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2309.05214/assets/x10.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span> Examples of redirected images that can show the difference of face identity and image quality.
Each column corresponds to the row in TableÂ <a href="#S5.T4" title="TABLE IV â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and TableÂ <a href="#S5.T5" title="TABLE V â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
The first three rows are from GazeCapture Test, and the last three rows are from XGaze Test.</figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>The ablation study of <span id="S5.T4.9.1" class="ltx_text ltx_font_italic">redirect-to-image</span> evaluation for GazeCapture Test.
The second to fourth columns are the components in the proposedÂ ST-AED.
The last row with all components corresponds to the proposalÂ ST-AED.</figcaption>
<div id="S5.T4.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:414.4pt;height:89.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,0.4pt) scale(0.99,0.99) ;">
<table id="S5.T4.7.7" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.7.7.7" class="ltx_tr">
<td id="S5.T4.7.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training Data</td>
<td id="S5.T4.7.7.7.9" class="ltx_td ltx_align_center ltx_border_t">AdaFace</td>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">mix <math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{rec}" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><msub id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T4.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.m1.1.1.2.cmml">â„’</mi><mrow id="S5.T4.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.m1.1.1.3.cmml"><mi id="S5.T4.1.1.1.1.m1.1.1.3.2" xref="S5.T4.1.1.1.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.m1.1.1.3.1" xref="S5.T4.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T4.1.1.1.1.m1.1.1.3.3" xref="S5.T4.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.T4.1.1.1.1.m1.1.1.3.1a" xref="S5.T4.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T4.1.1.1.1.m1.1.1.3.4" xref="S5.T4.1.1.1.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.m1.1.1.2">â„’</ci><apply id="S5.T4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3"><times id="S5.T4.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3.1"></times><ci id="S5.T4.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S5.T4.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S5.T4.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\mathcal{L}_{rec}</annotation></semantics></math>
</td>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{id}" display="inline"><semantics id="S5.T4.2.2.2.2.m1.1a"><msub id="S5.T4.2.2.2.2.m1.1.1" xref="S5.T4.2.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T4.2.2.2.2.m1.1.1.2" xref="S5.T4.2.2.2.2.m1.1.1.2.cmml">â„’</mi><mrow id="S5.T4.2.2.2.2.m1.1.1.3" xref="S5.T4.2.2.2.2.m1.1.1.3.cmml"><mi id="S5.T4.2.2.2.2.m1.1.1.3.2" xref="S5.T4.2.2.2.2.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.T4.2.2.2.2.m1.1.1.3.1" xref="S5.T4.2.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T4.2.2.2.2.m1.1.1.3.3" xref="S5.T4.2.2.2.2.m1.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><apply id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.2.2.2.2.m1.1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T4.2.2.2.2.m1.1.1.2.cmml" xref="S5.T4.2.2.2.2.m1.1.1.2">â„’</ci><apply id="S5.T4.2.2.2.2.m1.1.1.3.cmml" xref="S5.T4.2.2.2.2.m1.1.1.3"><times id="S5.T4.2.2.2.2.m1.1.1.3.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1.3.1"></times><ci id="S5.T4.2.2.2.2.m1.1.1.3.2.cmml" xref="S5.T4.2.2.2.2.m1.1.1.3.2">ğ‘–</ci><ci id="S5.T4.2.2.2.2.m1.1.1.3.3.cmml" xref="S5.T4.2.2.2.2.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\mathcal{L}_{id}</annotation></semantics></math></td>
<td id="S5.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">Head <math id="S5.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T4.3.3.3.3.m1.1.1" xref="S5.T4.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">Gaze <math id="S5.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T4.4.4.4.4.m1.1.1" xref="S5.T4.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.4.m1.1b"><ci id="S5.T4.4.4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">LPIPS <math id="S5.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.5.5.5.5.m1.1a"><mo stretchy="false" id="S5.T4.5.5.5.5.m1.1.1" xref="S5.T4.5.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.5.m1.1b"><ci id="S5.T4.5.5.5.5.m1.1.1.cmml" xref="S5.T4.5.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t">FID <math id="S5.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.6.6.6.6.m1.1a"><mo stretchy="false" id="S5.T4.6.6.6.6.m1.1.1" xref="S5.T4.6.6.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.6.m1.1b"><ci id="S5.T4.6.6.6.6.m1.1.1.cmml" xref="S5.T4.6.6.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">Sim. <math id="S5.T4.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.7.7.7.7.m1.1a"><mo stretchy="false" id="S5.T4.7.7.7.7.m1.1.1" xref="S5.T4.7.7.7.7.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.7.7.m1.1b"><ci id="S5.T4.7.7.7.7.m1.1.1.cmml" xref="S5.T4.7.7.7.7.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.7.7.8" class="ltx_tr">
<td id="S5.T4.7.7.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T4.7.7.8.1.1" class="ltx_text"><span id="S5.T4.7.7.8.1.1.1" class="ltx_text"></span> <span id="S5.T4.7.7.8.1.1.2" class="ltx_text">
<span id="S5.T4.7.7.8.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.7.7.8.1.1.2.1.1" class="ltx_tr">
<span id="S5.T4.7.7.8.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GC</span></span>
<span id="S5.T4.7.7.8.1.1.2.1.2" class="ltx_tr">
<span id="S5.T4.7.7.8.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+ Aug-60H</span></span>
</span></span> <span id="S5.T4.7.7.8.1.1.3" class="ltx_text"></span></span></td>
<td id="S5.T4.7.7.8.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T4.7.7.8.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T4.7.7.8.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T4.7.7.8.5" class="ltx_td ltx_align_center ltx_border_t">1.26</td>
<td id="S5.T4.7.7.8.6" class="ltx_td ltx_align_center ltx_border_t">3.86</td>
<td id="S5.T4.7.7.8.7" class="ltx_td ltx_align_center ltx_border_t">0.191</td>
<td id="S5.T4.7.7.8.8" class="ltx_td ltx_align_center ltx_border_t">41.76</td>
<td id="S5.T4.7.7.8.9" class="ltx_td ltx_align_center ltx_border_t">0.472</td>
</tr>
<tr id="S5.T4.7.7.9" class="ltx_tr">
<td id="S5.T4.7.7.9.1" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T4.7.7.9.2" class="ltx_td"></td>
<td id="S5.T4.7.7.9.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T4.7.7.9.4" class="ltx_td ltx_align_center">1.14</td>
<td id="S5.T4.7.7.9.5" class="ltx_td ltx_align_center">3.78</td>
<td id="S5.T4.7.7.9.6" class="ltx_td ltx_align_center">0.176</td>
<td id="S5.T4.7.7.9.7" class="ltx_td ltx_align_center"><span id="S5.T4.7.7.9.7.1" class="ltx_text ltx_font_bold">36.30</span></td>
<td id="S5.T4.7.7.9.8" class="ltx_td ltx_align_center">0.511</td>
</tr>
<tr id="S5.T4.7.7.10" class="ltx_tr">
<td id="S5.T4.7.7.10.1" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T4.7.7.10.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T4.7.7.10.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T4.7.7.10.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.7.10.4.1" class="ltx_text ltx_font_bold">1.13</span></td>
<td id="S5.T4.7.7.10.5" class="ltx_td ltx_align_center">3.77</td>
<td id="S5.T4.7.7.10.6" class="ltx_td ltx_align_center">0.175</td>
<td id="S5.T4.7.7.10.7" class="ltx_td ltx_align_center">38.91</td>
<td id="S5.T4.7.7.10.8" class="ltx_td ltx_align_center">0.534</td>
</tr>
<tr id="S5.T4.7.7.11" class="ltx_tr">
<td id="S5.T4.7.7.11.1" class="ltx_td ltx_align_center ltx_border_b">âœ“</td>
<td id="S5.T4.7.7.11.2" class="ltx_td ltx_align_center ltx_border_b">âœ“</td>
<td id="S5.T4.7.7.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">âœ“</td>
<td id="S5.T4.7.7.11.4" class="ltx_td ltx_align_center ltx_border_b">1.19</td>
<td id="S5.T4.7.7.11.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.7.7.11.5.1" class="ltx_text ltx_font_bold">3.76</span></td>
<td id="S5.T4.7.7.11.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.7.7.11.6.1" class="ltx_text ltx_font_bold">0.175</span></td>
<td id="S5.T4.7.7.11.7" class="ltx_td ltx_align_center ltx_border_b">39.24</td>
<td id="S5.T4.7.7.11.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T4.7.7.11.8.1" class="ltx_text ltx_font_bold">0.535</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>The ablation study of <span id="S5.T5.9.1" class="ltx_text ltx_font_italic">redirect-to-image</span> evaluation for XGaze Test.
The second to fourth columns are the components in the proposedÂ ST-AED.
The last row with all components corresponds to the proposalÂ ST-AED.</figcaption>
<div id="S5.T5.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:416.0pt;height:89.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,0.4pt) scale(0.99,0.99) ;">
<table id="S5.T5.7.7" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.7.7.7" class="ltx_tr">
<td id="S5.T5.7.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Training Data</td>
<td id="S5.T5.7.7.7.9" class="ltx_td ltx_align_center ltx_border_t">AdaFace</td>
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">mix <math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{rec}" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><msub id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.1.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.1.m1.1.1.2.cmml">â„’</mi><mrow id="S5.T5.1.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.1.m1.1.1.3.cmml"><mi id="S5.T5.1.1.1.1.m1.1.1.3.2" xref="S5.T5.1.1.1.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.T5.1.1.1.1.m1.1.1.3.1" xref="S5.T5.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T5.1.1.1.1.m1.1.1.3.3" xref="S5.T5.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.T5.1.1.1.1.m1.1.1.3.1a" xref="S5.T5.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T5.1.1.1.1.m1.1.1.3.4" xref="S5.T5.1.1.1.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.1.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T5.1.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.1.m1.1.1.2">â„’</ci><apply id="S5.T5.1.1.1.1.m1.1.1.3.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3"><times id="S5.T5.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3.1"></times><ci id="S5.T5.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S5.T5.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S5.T5.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T5.1.1.1.1.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\mathcal{L}_{rec}</annotation></semantics></math>
</td>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{id}" display="inline"><semantics id="S5.T5.2.2.2.2.m1.1a"><msub id="S5.T5.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.2.2.2.2.m1.1.1.2" xref="S5.T5.2.2.2.2.m1.1.1.2.cmml">â„’</mi><mrow id="S5.T5.2.2.2.2.m1.1.1.3" xref="S5.T5.2.2.2.2.m1.1.1.3.cmml"><mi id="S5.T5.2.2.2.2.m1.1.1.3.2" xref="S5.T5.2.2.2.2.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.T5.2.2.2.2.m1.1.1.3.1" xref="S5.T5.2.2.2.2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.T5.2.2.2.2.m1.1.1.3.3" xref="S5.T5.2.2.2.2.m1.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.m1.1b"><apply id="S5.T5.2.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.2.2.2.2.m1.1.1.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T5.2.2.2.2.m1.1.1.2.cmml" xref="S5.T5.2.2.2.2.m1.1.1.2">â„’</ci><apply id="S5.T5.2.2.2.2.m1.1.1.3.cmml" xref="S5.T5.2.2.2.2.m1.1.1.3"><times id="S5.T5.2.2.2.2.m1.1.1.3.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1.3.1"></times><ci id="S5.T5.2.2.2.2.m1.1.1.3.2.cmml" xref="S5.T5.2.2.2.2.m1.1.1.3.2">ğ‘–</ci><ci id="S5.T5.2.2.2.2.m1.1.1.3.3.cmml" xref="S5.T5.2.2.2.2.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.m1.1c">\mathcal{L}_{id}</annotation></semantics></math></td>
<td id="S5.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">Head <math id="S5.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T5.3.3.3.3.m1.1.1" xref="S5.T5.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.3.m1.1b"><ci id="S5.T5.3.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">Gaze <math id="S5.T5.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T5.4.4.4.4.m1.1.1" xref="S5.T5.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.4.m1.1b"><ci id="S5.T5.4.4.4.4.m1.1.1.cmml" xref="S5.T5.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">LPIPS <math id="S5.T5.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.5.5.5.5.m1.1a"><mo stretchy="false" id="S5.T5.5.5.5.5.m1.1.1" xref="S5.T5.5.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.5.m1.1b"><ci id="S5.T5.5.5.5.5.m1.1.1.cmml" xref="S5.T5.5.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t">FID <math id="S5.T5.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.6.6.6.6.m1.1a"><mo stretchy="false" id="S5.T5.6.6.6.6.m1.1.1" xref="S5.T5.6.6.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.6.m1.1b"><ci id="S5.T5.6.6.6.6.m1.1.1.cmml" xref="S5.T5.6.6.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">Sim. <math id="S5.T5.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.7.7.7.7.m1.1a"><mo stretchy="false" id="S5.T5.7.7.7.7.m1.1.1" xref="S5.T5.7.7.7.7.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.7.m1.1b"><ci id="S5.T5.7.7.7.7.m1.1.1.cmml" xref="S5.T5.7.7.7.7.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T5.7.7.8" class="ltx_tr">
<td id="S5.T5.7.7.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="S5.T5.7.7.8.1.1" class="ltx_text"><span id="S5.T5.7.7.8.1.1.1" class="ltx_text"></span> <span id="S5.T5.7.7.8.1.1.2" class="ltx_text">
<span id="S5.T5.7.7.8.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T5.7.7.8.1.1.2.1.1" class="ltx_tr">
<span id="S5.T5.7.7.8.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GC</span></span>
<span id="S5.T5.7.7.8.1.1.2.1.2" class="ltx_tr">
<span id="S5.T5.7.7.8.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+ Aug-60H</span></span>
</span></span> <span id="S5.T5.7.7.8.1.1.3" class="ltx_text"></span></span></td>
<td id="S5.T5.7.7.8.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.7.7.8.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T5.7.7.8.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T5.7.7.8.5" class="ltx_td ltx_align_center ltx_border_t">9.27</td>
<td id="S5.T5.7.7.8.6" class="ltx_td ltx_align_center ltx_border_t">18.89</td>
<td id="S5.T5.7.7.8.7" class="ltx_td ltx_align_center ltx_border_t">0.324</td>
<td id="S5.T5.7.7.8.8" class="ltx_td ltx_align_center ltx_border_t">116.10</td>
<td id="S5.T5.7.7.8.9" class="ltx_td ltx_align_center ltx_border_t">0.142</td>
</tr>
<tr id="S5.T5.7.7.9" class="ltx_tr">
<td id="S5.T5.7.7.9.1" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T5.7.7.9.2" class="ltx_td"></td>
<td id="S5.T5.7.7.9.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T5.7.7.9.4" class="ltx_td ltx_align_center">9.76</td>
<td id="S5.T5.7.7.9.5" class="ltx_td ltx_align_center">20.63</td>
<td id="S5.T5.7.7.9.6" class="ltx_td ltx_align_center">0.317</td>
<td id="S5.T5.7.7.9.7" class="ltx_td ltx_align_center">117.02</td>
<td id="S5.T5.7.7.9.8" class="ltx_td ltx_align_center">0.160</td>
</tr>
<tr id="S5.T5.7.7.10" class="ltx_tr">
<td id="S5.T5.7.7.10.1" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T5.7.7.10.2" class="ltx_td ltx_align_center">âœ“</td>
<td id="S5.T5.7.7.10.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T5.7.7.10.4" class="ltx_td ltx_align_center">8.18</td>
<td id="S5.T5.7.7.10.5" class="ltx_td ltx_align_center"><span id="S5.T5.7.7.10.5.1" class="ltx_text ltx_font_bold">18.64</span></td>
<td id="S5.T5.7.7.10.6" class="ltx_td ltx_align_center">0.290</td>
<td id="S5.T5.7.7.10.7" class="ltx_td ltx_align_center"><span id="S5.T5.7.7.10.7.1" class="ltx_text ltx_font_bold">105.11</span></td>
<td id="S5.T5.7.7.10.8" class="ltx_td ltx_align_center">0.196</td>
</tr>
<tr id="S5.T5.7.7.11" class="ltx_tr">
<td id="S5.T5.7.7.11.1" class="ltx_td ltx_align_center ltx_border_b">âœ“</td>
<td id="S5.T5.7.7.11.2" class="ltx_td ltx_align_center ltx_border_b">âœ“</td>
<td id="S5.T5.7.7.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">âœ“</td>
<td id="S5.T5.7.7.11.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.7.7.11.4.1" class="ltx_text ltx_font_bold">8.08</span></td>
<td id="S5.T5.7.7.11.5" class="ltx_td ltx_align_center ltx_border_b">19.03</td>
<td id="S5.T5.7.7.11.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.7.7.11.6.1" class="ltx_text ltx_font_bold">0.290</span></td>
<td id="S5.T5.7.7.11.7" class="ltx_td ltx_align_center ltx_border_b">109.08</td>
<td id="S5.T5.7.7.11.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T5.7.7.11.8.1" class="ltx_text ltx_font_bold">0.210</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S5.SSx2.p3" class="ltx_para">
<p id="S5.SSx2.p3.1" class="ltx_p">The average redirection errors of the three patterns are shown in TableÂ <a href="#S5.T3" title="TABLE III â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
While all augmentation datasets showed a reduction in both head error and gaze error, 60-degree augmentation shows a better performance, which is intuitively reasonable.
However, this range should not be overly enlarged since the facial region may be totally invisible.
Since the process of creating Aug-60H is similar to the <span id="S5.SSx2.p3.1.1" class="ltx_text ltx_font_italic">redirect both</span> pattern, it shows the best performance as in the second to third columns of TableÂ <a href="#S5.T3" title="TABLE III â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
For <span id="S5.SSx2.p3.1.2" class="ltx_text ltx_font_italic">gaze only</span> and <span id="S5.SSx2.p3.1.3" class="ltx_text ltx_font_italic">head only</span>, the Aug-60G showed the best head error while the Aug-60H showed the best gaze error.
We speculate the reason to be that the Aug-60G has a larger head pose range while the Aug-60H has a larger gaze range, as can be observed in Fig.Â <a href="#S5.F6" title="Figure 6 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
Finally, we show qualitative results of the three patterns in FigÂ <a href="#S5.F7" title="Figure 7 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, FigÂ <a href="#S5.F8" title="Figure 8 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, and FigÂ <a href="#S5.F9" title="Figure 9 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, respectively.
Further video samples are also included in the supplementary materials.</p>
</div>
<div id="S5.SSx2.p4" class="ltx_para">
<p id="S5.SSx2.p4.1" class="ltx_p"><span id="S5.SSx2.p4.1.1" class="ltx_text ltx_font_bold">ST-AEDÂ Components.</span>
We conduct an ablation study using the <span id="S5.SSx2.p4.1.2" class="ltx_text ltx_font_italic">redirect-to-image</span> evaluation to compare the performance of individual components of the model.
We use the proposed augmentation-based training data (GC+Aug-60H) as the training data for all settings.
We use two datasets, GazeCapture Test and XGaze Test, to evaluate the modelâ€™s effectiveness in terms of image quality and identity preservation.
From the results shown in TableÂ <a href="#S5.T4" title="TABLE IV â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> andÂ <a href="#S5.T5" title="TABLE V â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>,
the proposed AdaEncoder, mixed reconstruction loss, and identity loss all contribute to improving the performance, especially identity similarity.
Qualitative results are also shown in Fig.Â <a href="#S5.F10" title="Figure 10 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
In the GazeCapture Test, theÂ ST-AEDÂ effectively generates detailed and more target-like images.
In the XGaze Test, we observe that simply training the baseline ST-ED model with synthetic data results in low-quality images, as shown in the second column of Fig.Â <a href="#S5.F10" title="Figure 10 â€£ Ablation Studies â€£ V Experiments â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.
While the proposedÂ ST-AEDÂ showed a tendency to generate images with less blurred eye regions and better facial shapes under large angles.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text ltx_font_bold">Conclusion.</span> For learning-based gaze and head redirection, we proposed using 3D face reconstruction-based synthetic data to tackle the limited angle range of existing training datasets.
Experiments showed effectiveness in terms of redirection accuracy.
To better train with the synthetic images, we further proposed a corresponding gaze and head redirection frameworkÂ ST-AED, which can generate face images with satisfying quality even under large target angles.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Future Work.</span>
Although the main contribution of this work is to create effective augmentation data for extending the redirection angle range and improving the redirection accuracy, we acknowledge that there is still room for improvement in the image quality.
This issue is largely tied to the quality of the original training data, with datasets such as GazeCapture providing lower quality than the ETH-XGaze dataset (see Fig.Â <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Angle Range and Identity Similarity Enhanced Gaze and Head Redirection based on Synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Therefore, the image quality and corresponding identity problem is indeed a challenging issue in future work, with one potential solution being to further investigate transfer learning among datasets without compromising the redirection accuracy.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S.Â Ghosh, A.Â Dhall, M.Â Hayat, J.Â Knibbe, and Q.Â Ji, â€œAutomatic gaze analysis:
A survey of deep learning based approaches,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2108.05479</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.Â Zheng, S.Â Park, X.Â Zhang, S.Â D. Mello, and O.Â Hilliges, â€œSelf-learning
transformations for improving gaze and head redirection,â€ in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc.
NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y.Â Ganin, D.Â Kononenko, D.Â Sungatullina, and V.Â Lempitsky, â€œDeepwarp:
Photorealistic image resynthesis for gaze manipulation,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc.
ECCV</em>.Â Â Â Springer, 2016, pp. 311â€“326.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E.Â Wood, T.Â BaltruÅ¡aitis, L.-P. Morency, P.Â Robinson, and A.Â Bulling,
â€œGazedirector: Fully articulated eye gaze redirection in video,â€ in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Graphics Forum</em>, vol.Â 37, no.Â 2.Â Â Â Wiley Online Library, 2018, pp. 217â€“225.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z.Â He, A.Â Spurr, X.Â Zhang, and O.Â Hilliges, â€œPhoto-realistic monocular gaze
redirection using generative adversarial networks,â€ in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. ICCV</em>,
2019, pp. 6932â€“6941.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X.Â Zhang, Y.Â Sugano, M.Â Fritz, and A.Â Bulling, â€œItâ€™s written all over your
face: Full-face appearance-based gaze estimation,â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. CVPRW</em>,
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K.Â Krafka, A.Â Khosla, P.Â Kellnhofer, H.Â Kannan, S.Â Bhandarkar, W.Â Matusik, and
A.Â Torralba, â€œEye tracking for everyone,â€ in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X.Â Zhang, S.Â Park, T.Â Beeler, D.Â Bradley, S.Â Tang, and O.Â Hilliges,
â€œEth-xgaze: A large scale dataset for gaze estimation under extreme head
pose and gaze variation,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.Â Park, S.Â D. Mello, P.Â Molchanov, U.Â Iqbal, O.Â Hilliges, and J.Â Kautz,
â€œFew-shot adaptive gaze estimation,â€ in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.Â Kammoun, R.Â Slama, H.Â Tabia, T.Â Ouni, and M.Â Abid, â€œGenerative adversarial
networks for face generation: A survey,â€ <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys
(CSUR)</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T.Â Karras, T.Â Aila, S.Â Laine, and J.Â Lehtinen, â€œProgressive growing of GANs
for improved quality, stability, and variation,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2018.
[Online]. Available: https://openreview.net/forum?id=Hk99zCeAb

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T.Â Karras, S.Â Laine, and T.Â Aila, â€œA style-based generator architecture for
generative adversarial networks,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, June 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y.Â Shen, J.Â Gu, X.Â Tang, and B.Â Zhou, â€œInterpreting the latent space of gans
for semantic face editing,â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yuxiao Hu, Dalong Jiang, Shuicheng Yan, Lei Zhang, and Hongjiang
zhang, â€œAutomatic 3d reconstruction for face recognition,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc.
FG</em>, 2004.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
I.Â Masi, T.Â Hassner, A.Â T. Tran, and G.Â Medioni, â€œRapid synthesis of massive
face sets for improved face recognition,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. FG</em>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H.Â Zhou, J.Â Liu, Z.Â Liu, Y.Â Liu, and X.Â Wang, â€œRotate-and-render: Unsupervised
photorealistic face rotation from single-view images,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>,
2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.Â Qin, T.Â Shimoyama, and Y.Â Sugano, â€œLearning-by-novel-view-synthesis for
full-face appearance-based 3d gaze estimation,â€ in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. CVPRW</em>, June
2022, pp. 4981â€“4991.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.Â Jindal and X.Â E. Wang, â€œCuda-ghr: Controllable unsupervised domain
adaptation for gaze and head redirection,â€ <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2106.10852</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.Â Guo, X.Â Zhu, and Z.Â Lei, â€œ3ddfa,â€
https://github.com/cleardusk/3DDFA, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.Â Guo, X.Â Zhu, Y.Â Yang, F.Â Yang, Z.Â Lei, and S.Â Z. Li, â€œTowards fast,
accurate and stable 3d dense face alignment,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
X.Â Zhu, X.Â Liu, Z.Â Lei, and S.Â Z. Li, â€œFace alignment in full pose range: A 3d
total solution,â€ <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">PAMI</em>, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â Deng, J.Â Guo, N.Â Xue, and S.Â Zafeiriou, â€œArcface: Additive angular margin
loss for deep face recognition,â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2019, pp. 4690â€“4699.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.Â Kim, A.Â K. Jain, and X.Â Liu, â€œAdaface: Quality adaptive margin for face
recognition,â€ in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z.Â Wang, E.Â P. Simoncelli, and A.Â C. Bovik, â€œMultiscale structural similarity
for image quality assessment,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">The Thrity-Seventh ACSSC, 2003</em>,
vol.Â 2.Â Â Â Ieee, 2003, pp. 1398â€“1402.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y.Â Nitzan, A.Â Bermano, Y.Â Li, and D.Â Cohen-Or, â€œFace identity disentanglement
via latent space mapping,â€ <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">TOG</em>, vol.Â 39, pp. 1 â€“ 14, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H.Â Zhao, O.Â Gallo, I.Â Frosio, and J.Â Kautz, â€œLoss functions for image
restoration with neural networks,â€ <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE TCI</em>, vol.Â 3, no.Â 1, pp.
47â€“57, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
recognition,â€ in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X.Â Zhang, Y.Â Sugano, and A.Â Bulling, â€œRevisiting data normalization for
appearance-based gaze estimation,â€ in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. ETRA</em>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
R.Â Zhang, P.Â Isola, A.Â A. Efros, E.Â Shechtman, and O.Â Wang, â€œThe unreasonable
effectiveness of deep features as a perceptual metric,â€ in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc.
CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M.Â Heusel, H.Â Ramsauer, T.Â Unterthiner, B.Â Nessler, and S.Â Hochreiter, â€œGans
trained by a two time-scale update rule converge to a local nash
equilibrium,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.Â Seitzer, â€œpytorch-fid: FID Score for PyTorch,â€
https://github.com/mseitzer/pytorch-fid, August 2020, version 0.2.1.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y.Â Choi, M.Â Choi, M.Â Kim, J.-W. Ha, S.Â Kim, and J.Â Choo, â€œStargan: Unified
generative adversarial networks for multi-domain image-to-image
translation,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2018, pp. 8789â€“8797.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P.Â Yin, J.Â Dai, J.Â Wang, D.Â Xie, and S.Â Pu, â€œNerf-gaze: A head-eye redirection
parametric model for gaze estimation,â€ <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2212.14710</em>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A.Â Ruzzi, X.Â Shi, X.Â Wang, G.Â Li, S.Â DeÂ Mello, H.Â J. Chang, X.Â Zhang, and
O.Â Hilliges, â€œGazenerf: 3d-aware gaze redirection with neural radiance
fields,â€ <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.04823</em>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.05213" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.05214" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.05214">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.05214" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.05215" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 06:34:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
