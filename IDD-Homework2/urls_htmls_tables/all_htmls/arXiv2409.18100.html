<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation</title>
<!--Generated on Thu Sep 26 16:49:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Self-supervised learning Image segmentation Cardiovascular magnetic resonance Deep learning." lang="en" name="keywords"/>
<base href="/html/2409.18100v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S1" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS1" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS2" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Baseline model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS3" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Self-supervised pretraining methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS4" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Pretraining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS5" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS6" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Generalization evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.SS7" title="In 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.7 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S4" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S4.SS0.SSS1" title="In 4 Discussion ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span><span class="ltx_ERROR undefined">\discintname</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#Pt0.A1" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Training details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#Pt0.A2" title="In Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Test DSC per class</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department of Biomedical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands

<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>r.a.j.d.mooij@tue.nl</span></span></span>
<br class="ltx_break"/></span></span></span>
<h1 class="ltx_title ltx_title_document">Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rob A.J. de Mooij<sup class="ltx_sup" id="id1.1.id1">(✉)</sup><span class="ltx_ERROR undefined" id="id2.2.id2">\orcidlink</span>0009-0000-1014-3580
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Josien P.W. Pluim
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cian M. Scannell<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0001-9240-793X
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Self-supervised pretraining (SSP) has shown promising results in learning from large unlabeled datasets and, thus, could be useful for automated cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However, inconsistent reports of the benefits of SSP for segmentation have made it difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP methods for CMR cine segmentation.</p>
<p class="ltx_p" id="id5.id2">To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were used for unlabeled pretraining with four SSP methods; SimCLR, positional contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying numbers of subjects were used for supervised fine-tuning of 2D models for each SSP method, as well as to train a 2D baseline model from scratch. The fine-tuned models were compared to the baseline using the 3D Dice similarity coefficient (DSC) in a test dataset of 140 subjects.</p>
<p class="ltx_p" id="id6.id3">The SSP methods showed no performance gains with the largest supervised fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects (231 2D slices) are available for supervised training, SSP using MIM (DSC = 0.86) improves over training from scratch (DSC = 0.82).</p>
<p class="ltx_p" id="id7.id4">This study found that SSP is valuable for CMR cine segmentation when labeled training data is scarce, but does not aid state-of-the-art deep learning methods when ample labeled data is available. Moreover, the choice of SSP method is important. The code is publicly available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/q-cardIA/ssp-cmr-cine-segmentation" title="">https://github.com/q-cardIA/ssp-cmr-cine-segmentation</a></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Self-supervised learning Image segmentation Cardiovascular magnetic resonance Deep learning.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Self-supervised learning allows deep learning models to learn useful information from unlabeled data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib1" title="">1</a>]</cite>. Self-supervised and traditional supervised learning are often combined for specific tasks, where a model first trains on unlabeled data with self-supervised pretraining (SSP), before supervised fine-tuning on labeled data for a specific downstream task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib17" title="">17</a>]</cite>. SSP could be especially important in medical imaging, due to the lack of high quality labeled datasets.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Cardiovascular magnetic resonance (CMR) short-axis cine segmentation is an interesting problem on which to investigate SSP, both from a methodological and clinical perspective. CMR short-axis cine data consists of 4D (3D + time) images, allowing SSP methods that incorporate spatial and temporal information into the pretraining task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib15" title="">15</a>]</cite>. Additionally, labeled short-axis cine datasets consist of many unlabeled images as only certain cardiac phases are annotated. Therefore, models can be pretrained with SSP on all available images and fine-tuned on the labeled cardiac phases. Finally, improving the accuracy of CMR segmentation, potentially with SSP, could improve the evaluation of cardiac structure and function in the clinic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, SSP has yielded conflicting results in its application in medical imaging. Many SSP methods have been proposed for medical imaging, but many results have been difficult to generalize to different problems and data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib16" title="">16</a>]</cite>. Medical classification problems have seen multiple promising SSP methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib9" title="">9</a>]</cite>. However, benefits of SSP in segmentation tasks often hold only with very limited labeled training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib16" title="">16</a>]</cite>. Additionally, it is difficult to disentangle the effect of SSP from that of novel architectures such as vision transformers (ViT), and results depend on data and training hyperparameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib16" title="">16</a>]</cite>. These inconsistent findings make it challenging to choose the most effective SSP method in medical imaging problems, especially for segmentation tasks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This study investigates SSP methods for CMR cine segmentation with convolutional neural networks (CNNs), and the effectiveness of SSP methods with varying amounts of labeled data available during supervised fine-tuning. In particular, we will: (1) develop a strong CMR cine segmentation model to serve as baseline, (2) optimize four promising SSP methods for unlabeled pretraining of the baseline model, (3) compare the fine-tuned models, after SSP, to the baseline in downstream segmentation performance with varying amounts of labeled fine-tuning data, and (4) investigate the effects of data augmentation and SSP on model generalizability.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Short-axis cine data of the M&amp;Ms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib2" title="">2</a>]</cite> and M&amp;Ms-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib12" title="">12</a>]</cite> challenges were used for a total of 510 subjects. For each subject, a 4D cine image was available with a varying number of slices (median 11) and time frames (median 25). Each cine image had labels for the end diastole (ED) and end systole (ES) time frames, delineating the left ventricle (LV), myocardium (MYO), and right ventricle (RV). All 345 publicly available subjects of the M&amp;Ms challenge data were included. Only 165 subjects of the M&amp;Ms-2 challenge data were included to avoid possible overlapping subjects.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Data were split on a subject level into 296 training, 74 validation, and 140 test subjects. First, 60 test subjects were selected from the M&amp;Ms data, using random sampling stratified by MR scanner vendor. The 80 test subjects for the M&amp;Ms-2 challenge data were obtained from the original challenge test set, after removing the subjects with possible overlap. The remaining 370 subjects were then randomly divided into training and validation using an 80/20% split.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The training set consisted of 90618 2D slices, of which 6738 were labeled. To simulate the effect of smaller labeled datasets, three subsets of 50, 25, 15 and 10 random training subjects were used, consisting of on average 1151, 582, 349, and 231 slices respectively, depending on the subject subset. The labeled validation dataset was used for hyperparameter optimization, while the labeled test dataset was only used for the final evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Baseline model</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">A 2D fully convolutional U-Net baseline model was developed based on the nnU-Net method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib10" title="">10</a>]</cite>. All hyperparameters were set based on the automatic configuration and CMR data experiments of the original nnU-Net work, including deep supervision, with more extreme data augmentation implemented to improve robustness and stability. The baseline was trained from scratch separately on the fully labeled training dataset and its subsets, which was repeated with three training seeds. This model setup served as a realistic baseline with a competitive performance in 2D cine segmentation against which to compare the SSP methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Self-supervised pretraining methods</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Four SSP methods were compared; <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">a simple framework for contrastive learning of visual representations</span> (SimCLR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib4" title="">4</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">positional contrastive learning</span> (PCL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib15" title="">15</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.3">self-<span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.3.1">di</span>stillation with <span class="ltx_text ltx_font_bold" id="S2.SS3.p1.1.3.2">no</span> labels</span> (DINO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib3" title="">3</a>]</cite>, and <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.4">masked image modeling</span> (MIM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib8" title="">8</a>]</cite>. These methods are illustrated in figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S2.F1" title="Figure 1 ‣ 2.3 Self-supervised pretraining methods ‣ 2 Methods ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. SimCLR, PCL, and DINO were used to pretrain the encoder of the 2D U-Net, while MIM was used to pretrain both the encoder and decoder of the 2D U-Net.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><svg class="ltx_picture ltx_figure_panel" height="236.77" id="S2.F1.pic1" overflow="visible" version="1.1" width="339.91"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,236.77) matrix(1 0 0 -1 0 0) translate(-194.96,0) translate(0,-22.95)"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 195.24 259.45 L 195.24 23.23" style="fill:none"></path></g><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 389.22 259.45 L 389.22 23.23" style="fill:none"></path></g><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 534.6 259.45 L 534.6 23.23" style="fill:none"></path></g></g></svg></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1326" id="S2.F1.sf1.g1" src="x1.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F1.sf1.3.2" style="font-size:90%;">SimCLR</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1326" id="S2.F1.sf2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F1.sf2.3.2" style="font-size:90%;">PCL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S2.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1810" id="S2.F1.sf3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F1.sf3.3.2" style="font-size:90%;">DINO</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S2.F1.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="3963" id="S2.F1.sf4.g1" src="x4.png" width="826"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S2.F1.sf4.3.2" style="font-size:90%;">MIM</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Visualizations of the four SSP methods that were used.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Contrastive learning methods like SimCLR and PCL aim to train the model to extract useful features from the images by contrasting positive and negative image pairs. It aims for a feature space where positive image pairs are close together, while negative image pairs are further apart <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib16" title="">16</a>]</cite>. For SimCLR and PCL, positive and negative image pairs are obtained from a training batch of augmented images, containing two instances of the same image with different data augmentations. For SimCLR, only the two augmented versions of the same image are considered a positive pair, while every other image pair is a negative pair <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib4" title="">4</a>]</cite>. In PCL, each 2D image is assigned a relative position in the range [0, 1], indicating its relative position in the 3D volume it originated from. An image pair is considered positive when the difference between their relative positions is below a certain threshold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">With DINO, a student network learns from a teacher network how to predict global image features from local image patches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib3" title="">3</a>]</cite>. For each image, eight augmented copies are used; two global crops, and six local crops. The teacher network receives the two global crops, while the student network receives both the global and local crops. The student is trained to output the same as the teacher, while the teacher is only updated with the exponential moving average of the student network.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">MIM trains a model to restore an image that has been partially masked. An augmented image is first divided into equally sized patches, after which some of these patches are masked out. Patches are masked out by setting their pixel intensities to a predetermined value <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Pretraining</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">All methods were pretrained, without labels, on the full training dataset of 90618 slices. Pretraining hyperparameters were based on the hyperparameters from the original implementation. Hyperparameters were further fine-tuned for each method based on pretraining stability and cine segmentation performance after fine-tuning. Choices that worked well for all pretraining methods included: (1) 100 epochs, (2) stochastic gradient descent (SGD) optimizer, (3) extensive data augmentation, and (4) cosine learning rate scheduler. Details for all model trainings can be found in supplementary material <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#Pt0.A1" title="Appendix 0.A Training details ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">0.A</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">SimCLR had no method-specific hyperparameters, while the relative position threshold for positive pairs in PCL was chosen as 0.1. For DINO, the original hyperparameters from the CNN experiments were used for reference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib3" title="">3</a>]</cite>. Global and local patch sizes were increased to 256x256 and 128x128 respectively due to the deep CNN architecture requiring the image size to be divisible by 64. Finally, the DINO head output dimensionality was reduced to 8192.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">The MIM hyperparameters were adapted based on multiple similar implementations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib8" title="">8</a>]</cite>. A patch size of 32x32 was used, with a mask ratio of 0.75, and a constant masking value of 0.0, applied after image standardization and data augmentation. The architecture was the same as that of the 2D baseline model, except for the removal of the additional outputs, as deep supervision was not used during MIM pretraining. Transferring the weights of the encoder and decoder, but without the output convolution at the end, resulted in the best downstream performance. A mean squared error loss was used, applied only on the masked patches.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Fine-tuning</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">After SSP with all available images, models were fine-tuned separately on all labeled time frames and subsets of that. Fine-tuning was repeated three times for each pretrained model, changing the training seed that determined data loading, data augmentation, and model weight initialization for weights that had not been pretrained. The training seed was also used to select the subjects in the subset experiments. Fine-tuning used the hyperparameters of the fully-supervised training of the baseline from scratch with the learning rate lowered to 0.005.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Generalization evaluation</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">The best performing SSP method was further investigated to give insights into its behaviour. Two experiments investigated the generalizability of fine-tuned models after SSP on all data. For each experiment, the best performing SSP method was used to pretrain on all available data, while fine-tuning and test data were varied. Data augmentation was investigated separately, for a total of three experiments:</p>
</div>
<div class="ltx_para" id="S2.SS6.p2">
<p class="ltx_p" id="S2.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS6.p2.1.1">Generalization to unseen cardiac phases.</span> To evaluate the benefit of SSP in generalizing to data unseen during fine-tuning, we limited the fine-tuning data to a single cardiac phase, allowing for out-of-domain evaluation. A baseline model was trained from scratch on a single time frame per subject for reference. Then, the model pretrained on all data was fine-tuned on a single time frame per subject. Each experiment was performed separately for ED and ES time frames.</p>
</div>
<div class="ltx_para" id="S2.SS6.p3">
<p class="ltx_p" id="S2.SS6.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS6.p3.1.1">Generalization to unseen vendors.</span> A similar experiment was performed to investigate the generalizability to vendors unseen during fine-tuning. The four vendors of the M&amp;Ms challenge datasets were divided into two groups: Siemens and Philips (A+B), and General Electric and Canon (C+D). Baseline training and SSP fine-tuning were trained on both vendor groups separately. The in-domain and out-of-domain performance of the resulting models was compared separately on the test data.</p>
</div>
<div class="ltx_para" id="S2.SS6.p4">
<p class="ltx_p" id="S2.SS6.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS6.p4.1.1">Generalization from data augmentation.</span> So far, all models were trained with data augmentation, making it difficult to know to what extent model performance and robustness was due to SSP, and to what extent to data augmentation. Therefore, this final experiment trained versions of the baseline and best performing SSP method without data augmentation during training from scratch and fine-tuning respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.7 </span>Evaluation</h3>
<div class="ltx_para" id="S2.SS7.p1">
<p class="ltx_p" id="S2.SS7.p1.1">Segmentation performance was evaluated with the Dice similarity coefficient (DSC) in 3D, for both the ED and ES time frames. The metric was calculated separately for the LV, MYO and RV classes. For each fine-tuned model, the mean DSC across the three classes and for all test subjects was calculated. The SSP methods were compared with the baseline model for the three random training seeds. For the generalization experiments, test subsets for different cardiac phases and vendors were also evaluated separately.</p>
</div>
<div class="ltx_para" id="S2.SS7.p2">
<p class="ltx_p" id="S2.SS7.p2.1">All pretraining and fine-tuning used custom implementations that are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/q-cardIA/ssp-cmr-cine-segmentation" title="">https://github.com/q-cardIA/ssp-cmr-cine-segmentation</a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The mean test DSCs across all foreground classes and varying numbers of fine-tuning subjects can be seen in table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T1" title="Table 1 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. All models show similar performances when trained or fine-tuned on all available labeled training data. This also holds when looking at individual classes. All models showed mean 3D test DSCs of 0.93, 0.85, and 0.90 for the LV, MYO, and RV classes respectively. More details are shown in supplementary material <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#Pt0.A2" title="Appendix 0.B Test DSC per class ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">0.B</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">Mean test DSC across all 140 test subjects and foreground classes for the baseline and fine-tuned SSP models, for varying numbers of labeled subjects ± sample standard deviation across different training seeds. For each number of labeled subjects, the average number of slices across all training seeds is shown. The biggest performance increase compared to the baseline is indicated in bold.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T1.4.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.4.1.1.1.1">
<tr class="ltx_tr" id="S3.T1.4.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.4.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.1.1.1.1.1">#subjects</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.4.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.1.1.2.1.1">(#slices)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.2.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.3.1">SimCLR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.4.1">PCL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.5.1">DINO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.6.1">MIM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.2.1.1">296 (6738)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1.2">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1.3">0.89 ± 0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1.4">0.89 ± 0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1.5">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.2.1.6">0.89 ± 0.001</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.3.2.1">50 (1151)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.2.2">0.87 ± 0.007</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.2.3">0.87 ± 0.011</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.2.4">0.88 ± 0.005</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.2.5">0.87 ± 0.006</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.2.6">0.88 ± 0.006</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.4.3.1">25 (582)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.3.2">0.86 ± 0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.3.3">0.86 ± 0.014</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.3.4">0.85 ± 0.014</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.3.5">0.84 ± 0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.3.6">0.87 ± 0.007</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.5.4.1">15 (349)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.4.2">0.84 ± 0.007</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.4.3">0.83 ± 0.024</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.4.4">0.85 ± 0.009</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.4.5">0.79 ± 0.009</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.4.6">0.86 ± 0.015</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.6.5.1">10 (231)</th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.5.2">0.82 ± 0.009</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.5.3">0.80 ± 0.024</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.5.4">0.84 ± 0.009</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.5.5">0.74 ± 0.022</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.5.6"><span class="ltx_text ltx_font_bold" id="S3.T1.4.6.5.6.1">0.86 ± 0.007</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">As shown in table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T1" title="Table 1 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, PCL and MIM pretraining outperform the baseline performance for smaller labeled fine-tuning subsets. The biggest increase compared to the baseline can be seen for MIM pretraining and fine-tuning on the smallest labeled subset. SimCLR and DINO pretraining, on the other hand, show a performance decrease for fine-tuning on smaller labeled subsets, as well as a higher sample standard deviation.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Since MIM pretraining consistently showed the best results, MIM was used for the generalization evaluation. Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T2" title="Table 2 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T3" title="Table 3 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> show the results for the first two generalization experiments, investigating fine-tuned model generalization to unseen (during fine-tuning) cardiac phases and vendors. Both experiments showed similar overall, in-domain, and out-of-domain baseline performances. For labeled ED time frames fine-tuning, both overall and out-of-domain performances increased slightly with SSP. Contrastingly, labeled ES time frames fine-tuning only showed a slight in-domain performance increase with SSP. SSP did not result in changes in performance when fine-tuning on labeled vendors A and B. A slight increase in out-of-domain performance can be seen after fine-tuning on labeled vendors C and D.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">Mean test DSC ± sample standard deviation, comparing fine-tuned model generalization to unseen cardiac phases. Improvements after SSP are indicated in bold.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.1.1">Pretraining</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.2.1">Fine-tuning</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.4.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.4.1.1.3.1">
<tr class="ltx_tr" id="S3.T2.4.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.3.1.1.1.1">Both time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.3.1.2.1.1">frames</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.4.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.4.1.1.4.1">
<tr class="ltx_tr" id="S3.T2.4.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.4.1.1.1.1">ED time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.4.1.2.1.1">frames</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.4.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.4.1.1.5.1">
<tr class="ltx_tr" id="S3.T2.4.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.5.1.1.1.1">ES time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.4.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.5.1.2.1.1">frames</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.2.1.1">None</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.2.1.2">ED time frames</th>
<td class="ltx_td ltx_align_center" id="S3.T2.4.2.1.3">0.88 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.2.1.4">0.90 ± 0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.2.1.5">0.86 ± 0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.3.2.1">All data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.3.2.2">ED time frames</th>
<td class="ltx_td ltx_align_center" id="S3.T2.4.3.2.3"><span class="ltx_text ltx_font_bold" id="S3.T2.4.3.2.3.1">0.89 ± 0.001</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.3.2.4">0.90 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.3.2.5"><span class="ltx_text ltx_font_bold" id="S3.T2.4.3.2.5.1">0.87 ± 0.001</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.4.3.1">None</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.4.3.2">ES time frames</th>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.3.3">0.88 ± 0.007</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.3.4">0.88 ± 0.012</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.3.5">0.88 ± 0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.5.4.1">All data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.4.5.4.2">ES time frames</th>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.4.3">0.88 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.4.4">0.88 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.4.5"><span class="ltx_text ltx_font_bold" id="S3.T2.4.5.4.5.1">0.89 ± 0.000</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.3.2" style="font-size:90%;">Mean test DSC ± sample standard deviation, comparing fine-tuned model generalization to unseen vendors. Improvements after SSP are indicated in bold.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.1.1">Pretraining</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.2.1">Fine-tuning</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.3.1">All vendors</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.4.1">Vendors A+B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.5.1">Vendors C+D</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.2.2.1">None</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.2.2.2">Vendors A+B</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.2.3">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.2.4">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.2.2.5">0.88 ± 0.002</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.3.3.1">All data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.3.3.2">Vendors A+B</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.3">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.4">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.5">0.88 ± 0.000</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.4.4.1">None</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.4.4.2">Vendors C+D</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.3">0.88 ± 0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.4">0.87 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.5">0.89 ± 0.004</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.5.5.1">All data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.5.5.2">Vendors C+D</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.3">0.88 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.4"><span class="ltx_text ltx_font_bold" id="S3.T3.4.5.5.4.1">0.88 ± 0.002</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.5">0.89 ± 0.002</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The baseline mean test DSC (± sample standard deviation across training seeds) of 0.89 ± 0.001 decreased to 0.69 ± 0.039 when trained without data augmentation. With MIM pretraining, the mean test DSC of 0.89 ± 0.001 lowered to 0.78 ± 0.040 without data augmentation, a smaller decrease than for the baseline model.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The DSCs for varying cardiac phase and vendor subsets for baseline models trained on all labeled data with and without data augmentation can be seen in table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T4" title="Table 4 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>. This shows that without data augmentation, there is a large difference in DSC between ED and ES time frames, as well as between both vendor groups. With data augmentation the performance gap between times frames is mostly bridged, while the performance gap between vendors is bridged completely.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.3.2" style="font-size:90%;">Mean test DSC ± standard deviation, for varying cardiac phases and vendor subsets, for baseline models with and without data augmentation.</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S3.T4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.1.1">Data augmentation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T4.4.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.4.1.1.2.1">
<tr class="ltx_tr" id="S3.T4.4.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.2.1.1.1.1">ED time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.2.1.2.1.1">frames</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T4.4.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.4.1.1.3.1">
<tr class="ltx_tr" id="S3.T4.4.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.3.1.1.1.1">ES time</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.3.1.2.1.1">frames</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T4.4.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.4.1.1.4.1">
<tr class="ltx_tr" id="S3.T4.4.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.4.1.1.1.1">Vendors</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.4.1.2.1.1">A+B</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T4.4.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.4.1.1.5.1">
<tr class="ltx_tr" id="S3.T4.4.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.5.1.1.1.1">Vendors</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.4.1.1.5.1.2.1.1">C+D</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.4.2.1.1">Yes</th>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.1.2">0.90 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.1.3">0.88 ± 0.003</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.1.4">0.89 ± 0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.2.1.5">0.89 ± 0.006</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.4.3.2.1">No</th>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.2.2">0.71 ± 0.042</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.2.3">0.66 ± 0.036</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.2.4">0.72 ± 0.036</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.3.2.5">0.61 ± 0.049</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This study aimed to investigate SSP methods for CMR cine segmentation with CNNs, and the effectiveness of SSP methods with varying amounts of fine-tuning data. More specifically, four SSP methods were compared to a baseline for varying numbers of labeled subjects for fine-tuning. Additionally, further experiments investigated generalizability of fine-tuned models for unseen cardiac phases and MR scanner vendors after SSP, and generalizability due to data augmentation.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T1" title="Table 1 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that SSP with unlabeled data only yields an improvement in CMR cine segmentation when very limited amounts of labeled data are available for fine-tuning. Moreover, choice of SSP method is important. PCL and MIM showed performance increases for small labeled fine-tuning datasets, indicating that these methods contribute useful information for the problem. MIM may be the most effective because it pretrains both the encoder and decoder of the U-Net architecture.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">SimCLR and DINO show decreased performances for limited amounts of labeled data, with a generally higher standard deviation. DINO pretraining shows the largest performance decrease, a potential explanation is that it is ill-suited for CNN architectures, as it is designed for ViTs. Additionally, both SimCLR and DINO generally require large batch sizes during training which was limited by our available hardware <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">The cardiac phase generalization experiment shows a slight benefit in SSP in generalizing to unseen (during fine-tuning) cardiac phases. Out-of-domain performances can slightly increase with SSP, while in-domain performances did not decrease. This indicates that there may be benefit in SSP in this situation. However, the baseline models already show the ability to generalize to unseen cardiac phases, leaving little room for improvement as a result of SSP. This could be explained by the data augmentation. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T4" title="Table 4 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> shows that there is a performance gap between cardiac phases when training on all labeled data without data augmentation. However, adding data augmentation largely closes this gap, indicating that data augmentation accounts for most of the generalizability shown in the unseen cardiac phase experiment.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Similar results can be seen for the unseen vendor experiment, showing a small out-of-domain performance increase. For the two vendor groups, table <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#S3.T4" title="Table 4 ‣ 3 Results ‣ Self-supervised pretraining for cardiovascular magnetic resonance cine segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> also shows a large performance gap without data augmentation, which is closed with data augmentation. While these models were trained on all labeled data, these results do indicate the importance of data augmentation in general model performance and generalizability to unseen data. The results of the data augmentation experiment in combination with MIM pretraining further show the importance of data augmentation, even when using SSP. While SSP shows a clear benefit when not using data augmentation in fine-tuning and training from scratch, data augmentation is still necessary to achieve the best performance. This indicates that data augmentation can better cover the data distribution compared to our SSP methods with larger unlabeled datasets. These results support claims that data augmentation can mostly meet or exceed the benefits of SSP, when appropriately selected for the downstream task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib16" title="">16</a>]</cite>. However, our results also indicate that data augmentation is a crucial step in enabling the possible benefits of SSP.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">Future research should further investigate whether these findings hold when using other model architectures, including ViTs. SSP methods such as DINO and MIM are generally developed for and perform better on ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18100v1#bib.bib5" title="">5</a>]</cite>. However, this study intentionally focused on CNN architectures, to separate the effects of SSP from new model architectures.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">In conclusion, SSP can be beneficial for CMR cine segmentation with limited amounts of labeled data, but its effectiveness depends on the SSP method. Additionally, SSP with large unlabeled datasets can provide slight benefits in generalizing to unseen domains for which labeled data is not available but unlabeled data is available for SSP.</p>
</div>
<div class="ltx_para" id="S4.p8">
<span class="ltx_ERROR undefined" id="S4.p8.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span><span class="ltx_ERROR undefined" id="S4.SS0.SSS1.1.1">\discintname</span>
</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">The authors have no competing interests to declare that are relevant to the content of this article.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian, Y., Schwarzschild, A., Wilson, A.G., Geiping, J., Garrido, Q., Fernandez, P., Bar, A., Pirsiavash, H., LeCun, Y., Goldblum, M.: A Cookbook of Self-Supervised Learning (Jun 2023), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.12210" title="">http://arxiv.org/abs/2304.12210</a>, arXiv:2304.12210 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Campello, V.M., Gkontra, P., Izquierdo, C., Martin-Isla, C., Sojoudi, A., Full, P.M., Maier-Hein, K., Zhang, Y., He, Z., Ma, J., Parreno, M., Albiol, A., Kong, F., Shadden, S.C., Acero, J.C., Sundaresan, V., Saber, M., Elattar, M., Li, H., Menze, B., Khader, F., Haarburger, C., Scannell, C.M., Veta, M., Carscadden, A., Punithakumar, K., Liu, X., Tsaftaris, S.A., Huang, X., Yang, X., Li, L., Zhuang, X., Vilades, D., Descalzo, M.L., Guala, A., Mura, L.L., Friedrich, M.G., Garg, R., Lebel, J., Henriques, F., Karakas, M., Cavus, E., Petersen, S.E., Escalera, S., Segui, S., Rodriguez-Palomares, J.F., Lekadir, K.: Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation: The M&amp;Ms Challenge. IEEE Transactions on Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">40</span>(12), 3543–3554 (Dec 2021). https://doi.org/10.1109/TMI.2021.3090082

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging Properties in Self-Supervised Vision Transformers. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 9630–9640. IEEE, Montreal, QC, Canada (Oct 2021). https://doi.org/10.1109/ICCV48922.2021.00951

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A Simple Framework for Contrastive Learning of Visual Representations (Jun 2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2002.05709" title="">http://arxiv.org/abs/2002.05709</a>, arXiv:2002.05709 [cs, stat]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, Z., Agarwal, D., Aggarwal, K., Safta, W., Balan, M.M., Brown, K.: Masked Image Modeling Advances 3D Medical Image Analysis. In: 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 1969–1979. IEEE, Waikoloa, HI, USA (Jan 2023). https://doi.org/10.1109/WACV56688.2023.00201

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Davies, R.H., Augusto, J.B., Bhuva, A., Xue, H., Treibel, T.A., Ye, Y., Hughes, R.K., Bai, W., Lau, C., Shiwani, H., Fontana, M., Kozor, R., Herrey, A., Lopes, L.R., Maestrini, V., Rosmini, S., Petersen, S.E., Kellman, P., Rueckert, D., Greenwood, J.P., Captur, G., Manisty, C., Schelbert, E., Moon, J.C.: Precision measurement of cardiac structure and function in cardiovascular magnetic resonance using machine learning. Journal of Cardiovascular Magnetic Resonance <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">24</span>(1),  16 (Jan 2022). https://doi.org/10.1186/s12968-022-00846-4

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Dominic, J., Bhaskhar, N., Desai, A.D., Schmidt, A., Rubin, E., Gunel, B., Gold, G.E., Hargreaves, B.A., Lenchik, L., Boutin, R., Chaudhari, A.S.: Improving Data-Efficiency and Robustness of Medical Imaging Segmentation Using Inpainting-Based Self-Supervised Learning. Bioengineering <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">10</span>(2),  207 (Feb 2023). https://doi.org/10.3390/bioengineering10020207

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked Autoencoders Are Scalable Vision Learners (Dec 2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2111.06377" title="">http://arxiv.org/abs/2111.06377</a>, arXiv:2111.06377 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Huang, S.C., Pareek, A., Jensen, M., Lungren, M.P., Yeung, S., Chaudhari, A.S.: Self-supervised learning for medical image classification: a systematic review and implementation guidelines. npj Digital Medicine <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">6</span>(1),  74 (Apr 2023). https://doi.org/10.1038/s41746-023-00811-0

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H.: nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">18</span>(2), 203–211 (Feb 2021). https://doi.org/10.1038/s41592-020-01008-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kalapos, A., Gyires-Tóth, B.: Self-Supervised Pretraining for 2D Medical Image Segmentation (2023), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2209.00314" title="">http://arxiv.org/abs/2209.00314</a>, arXiv:2209.00314 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Martín-Isla, C., Campello, V.M., Izquierdo, C., Kushibar, K., Sendra-Balcells, C., Gkontra, P., Sojoudi, A., Fulton, M.J., Arega, T.W., Punithakumar, K., Li, L., Sun, X., Al Khalil, Y., Liu, D., Jabbar, S., Queirós, S., Galati, F., Mazher, M., Gao, Z., Beetz, M., Tautz, L., Galazis, C., Varela, M., Hüllebrand, M., Grau, V., Zhuang, X., Puig, D., Zuluaga, M.A., Mohy-ud Din, H., Metaxas, D., Breeuwer, M., Van Der Geest, R.J., Noga, M., Bricq, S., Rentschler, M.E., Guala, A., Petersen, S.E., Escalera, S., Palomares, J.F.R., Lekadir, K.: Deep Learning Segmentation of the Right Ventricle in Cardiac MRI: The M&amp;Ms Challenge. IEEE Journal of Biomedical and Health Informatics <span class="ltx_text ltx_font_bold" id="bib.bib12.1.1">27</span>(7), 3302–3313 (Jul 2023). https://doi.org/10.1109/JBHI.2023.3267857

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sirajuddin, A., Mirmomen, S.M., Kligerman, S.J., Groves, D.W., Burke, A.P., Kureshi, F., White, C.S., Arai, A.E.: Ischemic Heart Disease: Noninvasive Imaging Techniques and Findings. RadioGraphics <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">41</span>(4), E990–E1021 (Jul 2021). https://doi.org/10.1148/rg.2021200125

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
VanBerlo, B., Hoey, J., Wong, A.: A survey of the impact of self-supervised pretraining for diagnostic tasks in medical X-ray, CT, MRI, and ultrasound. BMC Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">24</span>(1),  79 (Apr 2024). https://doi.org/10.1186/s12880-024-01253-0

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zeng, D., Wu, Y., Hu, X., Xu, X., Yuan, H., Huang, M., Zhuang, J., Hu, J., Shi, Y.: Positional Contrastive Learning for Volumetric Medical Image Segmentation (Sep 2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.09157" title="">http://arxiv.org/abs/2106.09157</a>, arXiv:2106.09157 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zhang, C., Zheng, H., Gu, Y.: Dive into the details of self-supervised learning for medical image analysis. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib16.1.1">89</span>, 102879 (Oct 2023). https://doi.org/10.1016/j.media.2023.102879

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Zoph, B., Ghiasi, G., Lin, T.Y., Cui, Y., Liu, H., Cubuk, E.D., Le, Q.V.: Rethinking Pre-training and Self-training. CoRR <span class="ltx_text ltx_font_bold" id="bib.bib17.1.1">abs/2006.06882</span> (2020)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p ltx_align_center" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:144%;">Supplementary materials</span><span class="ltx_text" id="p1.1.2" style="font-size:144%;"></span></p>
</div>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Training details</h2>
<div class="ltx_para" id="Pt0.A1.p1">
<p class="ltx_p" id="Pt0.A1.p1.1">Hyperparameters for each model training. The DINO learning rate is reported after applying the linear scaling rule based on batch size.</p>
</div>
<figure class="ltx_table" id="Pt0.A1.tab1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Pt0.A1.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.tab1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.2.1">Baseline</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.3.1">SimCLR</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.4.1">PCL</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.5.1">DINO</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.6.1">MIM</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="Pt0.A1.tab1.1.1.1.7.1">Fine-tuning</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.2.2.1">Epochs</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.2">1000</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.3">100</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.4">100</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.5">100</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.6">100</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.2.2.7">1000</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.3.3.1">Learning rate</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.2">0.01</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.3">0.1</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.4">0.1</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.5">0.0075</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.6">0.01</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.3.3.7">0.005</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.4.4.1">Scheduler</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.2">Polynomial</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.3">Cosine</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.4">Cosine</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.5">Cosine</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.6">Cosine</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.4.4.7">Polynomial</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.5.5.1">Optimizer</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.2">SGD</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.3">SGD</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.4">SGD</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.5">SGD</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.6">SGD</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.5.5.7">SGD</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.6.6.1">Nesterov momentum</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.2">yes</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.3">no</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.4">no</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.5">no</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.6">yes</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.6.6.7">yes</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.7.7.1">Momentum</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.2">0.99</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.3">0.9</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.4">0.9</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.5">0.9</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.6">0.99</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.7.7.7">0.99</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.8.8.1">Weight decay</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.2">3.0e-05</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.3">1.0e-04</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.4">1.0e-05</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.5">1.0e-04</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.6">3.0e-05</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.8.8.7">3.0e-05</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.9.9.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.2">32</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.3">224</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.4">64</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.5">64</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.6">128</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.9.9.7">32</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.tab1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.tab1.1.10.10.1">Mixed precision</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.2">no</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.3">yes</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.4">no</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.5">yes</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.6">yes</td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.tab1.1.10.10.7">no</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Test DSC per class</h2>
<div class="ltx_para" id="Pt0.A2.p1">
<p class="ltx_p" id="Pt0.A2.p1.1">Mean test DSC for the baseline and fine-tuned SSP models, for separate foreground classes ± mean sample standard deviation across all 140 test subjects.</p>
</div>
<figure class="ltx_table" id="Pt0.A2.tab1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Pt0.A2.tab1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Pt0.A2.tab1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="Pt0.A2.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.1.1">Class</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Pt0.A2.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.2.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Pt0.A2.tab1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.3.1">SimCLR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Pt0.A2.tab1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.4.1">PCL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Pt0.A2.tab1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.5.1">DINO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="Pt0.A2.tab1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="Pt0.A2.tab1.1.1.1.6.1">MIM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A2.tab1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.tab1.1.2.1.1">LV</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.2.1.2">0.93 ± 0.047</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.2.1.3">0.93 ± 0.047</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.2.1.4">0.93 ± 0.048</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.2.1.5">0.93 ± 0.048</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.2.1.6">0.93 ± 0.048</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.tab1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.tab1.1.3.2.1">MYO</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.3.2.2">0.85 ± 0.055</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.3.2.3">0.85 ± 0.048</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.3.2.4">0.85 ± 0.052</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.3.2.5">0.85 ± 0.052</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.3.2.6">0.85 ± 0.053</td>
</tr>
<tr class="ltx_tr" id="Pt0.A2.tab1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A2.tab1.1.4.3.1">RV</th>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.4.3.2">0.90 ± 0.062</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.4.3.3">0.90 ± 0.064</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.4.3.4">0.90 ± 0.061</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.4.3.5">0.90 ± 0.068</td>
<td class="ltx_td ltx_align_center" id="Pt0.A2.tab1.1.4.3.6">0.90 ± 0.068</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 26 16:49:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
