<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1907.09693] A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection</title><meta property="og:description" content="Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1907.09693">

<!--Generated on Tue Mar 19 11:46:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Qinbin Li<sup id="id13.2.id1" class="ltx_sup"><span id="id13.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zeyi Wen<sup id="id14.2.id1" class="ltx_sup">2</sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhaomin Wu<sup id="id15.2.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sixu Hu<sup id="id16.2.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Naibo Wang<sup id="id17.2.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuan Li<sup id="id18.2.id1" class="ltx_sup"><span id="id18.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xu Liu<sup id="id19.2.id1" class="ltx_sup"><span id="id19.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Bingsheng He<sup id="id20.5.id1" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id21.6.id2" class="ltx_sup"><span id="id21.6.id2.1" class="ltx_text ltx_font_italic">1</span></sup>National University of Singapore
<br class="ltx_break"><sup id="id22.7.id3" class="ltx_sup">2</sup>The University of Western Australia
<br class="ltx_break"><sup id="id23.8.id4" class="ltx_sup">1</sup>{qinbin
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> zhaomin
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> sixuhu
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> naibowang
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> liyuan
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> liuxu
</span><span class="ltx_author_notes"></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> hebs}@comp.nus.edu.sg
<br class="ltx_break"><sup id="id24.2.id1" class="ltx_sup">2</sup>zeyi.wen@uwa.edu.au
<br class="ltx_break">
</span><span class="ltx_author_notes"></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id25.id1" class="ltx_p">Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To achieve smooth flow and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many machine learning algorithms are data hungry, and in reality, data are dispersed over different organizations under the protection of privacy restrictions. Due to these factors, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> has become a hot research topic in machine learning. For example, data of different hospitals are isolated and become “data islands”. Since each data island has limitations in size and approximating real distributions, a single hospital may not be able to train a high-quality model that has a good predictive accuracy for a specific task. Ideally, hospitals can benefit more if they can collaboratively train a machine learning model on the union of their data. However, the data cannot simply be shared among the hospitals due to various policies and regulations. Such phenomena on “data islands” are commonly seen in many areas such as finance, government, and supply chains. Policies such as General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> stipulate rules on data sharing among different organizations. Thus, it is challenging to develop a federated learning system which has a good predictive accuracy while obeying policies and regulations to protect privacy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="color:#000000;">Many efforts have recently been devoted to implementing federated learning algorithms to support effective machine learning models. Specifically, researchers try to support more machine learning models with different privacy-preserving approaches, including deep neural networks (NNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib213" title="" class="ltx_ref">213</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, gradient boosted decision trees (GBDTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, logistics regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and support vector machines (SVMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>.</span> For instance, <cite class="ltx_cite ltx_citemacro_citet">Nikolaenko et al. [<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> propose approaches to conduct FL based on linear regression. Since GBDTs have become very successful in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite>, the corresponding Federated Learning Systems (FLSs) have also been proposed by <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Cheng et al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>. Moreover, there are many FLSs supporting the training of NNs. Google proposes a scalable production system which enables tens of millions of devices to train a deep neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="color:#000000;">As there are common methods and building blocks (e.g., privacy mechanisms such as differential privacy) for building FL algorithms, it makes sense to develop systems and infrastructures to ease the development of various FL algorithms. Systems and infrastructures allow algorithm developers to reuse the common building blocks, and avoid building algorithms every time from scratch. Similar to deep learning systems such as PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> and TensorFlow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> that boost the development of deep learning algorithms, FLSs are equivalently important for the success of FL. However, building a successful FLS is challenging, which needs to consider multiple aspects such as effectiveness, efficiency, privacy, and autonomy.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we take a survey on the existing FLSs from a system view. First, we show the definition of FLSs, and compare it with conventional federated systems. Second, we analyze the system components of FLSs, including the parties, the manager, and the computation-communication framework. Third, we categorize FLSs based on six different aspects: data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation, and motivation of federation. These aspects can direct the design of an FLS as common building blocks and system abstractions. Fourth, based on these aspects, we systematically summarize the existing studies, which can be used to direct the design of FLSs. Last, to make FL more practical and powerful, we present future research directions to work on. <span id="S1.p4.1.1" class="ltx_text" style="color:#000000;">We believe that systems and infrastructures are essential for the success of FL. More work has to be carried out to address the system research issues in effectiveness, efficiency, privacy, and autonomy.</span></p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Surveys</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p"><span id="S1.SS1.p1.1.1" class="ltx_text" style="color:#000000;">There have been several surveys on FL. A seminal survey written by Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite> introduces the basics and concepts in FL, and further proposes a comprehensive secure FL framework. The paper mainly target at a relatively small number of parties which are typically enterprise data owners. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> summarize challenges and future directions of FL in massive networks of mobile and edge devices. Recently, <cite class="ltx_cite ltx_citemacro_citet">Kairouz et al. [<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> have a comprehensive description about the characteristics and challenges on FL from different research topics. However, they mainly focus on cross-device FL, where the participants are a very large number of mobile or IoT devices. More recently, another survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> summarizes the platforms, protocols and applications of federated learning. Some surveys only focus on an aspect of federated learning. For example, <cite class="ltx_cite ltx_citemacro_citet">Lim et al. [<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> conduct a survey of FL specific to mobile edge computing, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> focuses on the threats to federated learning.</span></p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Our Contribution</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">To the best of our knowledge, there lacks a survey on reviewing existing systems and infrastructure of FLSs and on boosting the attention of creating systems for FL (Similar to prosperous system research in deep learning). <span id="S1.SS2.p1.1.1" class="ltx_text" style="color:#000000;">In comparison with the previous surveys,</span> the main contributions of this paper are as follows. <span id="S1.SS2.p1.1.2" class="ltx_text" style="color:#000000;">(1) Our survey is the first one to provide a comprehensive analysis on FL from a system’s point of view, including system components, taxonomy, summary, design, and vision.</span> (2) We provide a comprehensive taxonomy against FLSs on six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation, and motivation of federation, which can be used as common building blocks and system abstractions of FLSs. (3) We summarize existing typical and state-of-the-art studies according to their domains, which is convenient for researchers and developers to refer to. (4) We present the design factors for a successful FLS and comprehensively review solutions for each scenario. (5) We propose interesting research directions and challenges for future generations of FLSs.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">The rest of the paper is organized as follows. In Section <a href="#S2" title="2 An Overview of Federated Learning Systems ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we introduce the concept and the system components of FLSs. In Section <a href="#S3" title="3 Taxonomy ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we propose six aspects to classify FLSs. In Section <a href="#S4" title="4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we summary existing studies and systems on FL. We then present the design factors and solutions for an FLS in Section <a href="#S5" title="5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Last, we propose possible future directions on FL in Section <a href="#S7" title="7 Vision ‣ 6.3 Finance ‣ 6 Case Study ‣ 5.6 Evaluation ‣ 5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and conclude our paper in Section <a href="#S8" title="8 Conclusion ‣ 7.3 FL in Domains ‣ 7 Vision ‣ 6.3 Finance ‣ 6 Case Study ‣ 5.6 Evaluation ‣ 5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>An Overview of Federated Learning Systems</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As data breach becomes a major concern, more and more governments establish regulations to protect users’ data, such as GDPR in European Union <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite>, PDPA in Singapore <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and CCPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in the US. The cost of breaching these policies is pretty high for companies. In a breach of 600,000 drivers’ personal information in 2016, Uber had to pay $148 million to settle the investigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. SingHealth was fined $750,000 by the Singapore government for a breach of PDPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Google was fined $57 million for a breach of GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which is the largest penalty as of March 18, 2020 under the European Union privacy law.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Under the above circumstances, federated learning, a collaborative learning without exchanging users’ original data, has drawn increasingly attention nowadays. While machine learning, especially deep learning, has attracted many attentions again recently, the combination of federation and machine learning is emerging as a new and hot research topic.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Definition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FL enables multiple parties jointly train a machine learning model without exchanging the local data. It covers the techniques from multiple research areas such as distributed system, machine learning, and privacy. <span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Inspired by the definition of FL given by other studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>, here we give a definition of FLSs.</span></p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text" style="color:#000000;">In a federated learning system, multiple parties collaboratively train machine learning models without exchanging their raw data. The output of the system is a machine learning model for each party (which can be same or different). A practical federated learning system has the following constraint: given an evaluation metric such as test accuracy, the performance of the model learned by federated learning should be better than the model learned by local training with the same model architecture.</span></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Compare with Conventional Federated Systems</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The concept of federation can be found with its counterparts in the real world such as business and sports.  The main characteristic of federation is cooperation. Federation not only commonly appears in society, but also plays an important role in computing. In computer science, federated computing systems have been an attractive area of research under different contexts.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Around 1990, there were many studies on federated database systems (FDBSs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>. An FDBS is a collection of autonomous databases cooperating for mutual benefits. As pointed out in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, three important components of an FDBS are autonomy, heterogeneity, and distribution.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Autonomy</em>. A database system (DBS) that participates in an FDBS is autonomous, which means it is under separate and independent control. The parties can still manage the data without the FDBS.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Heterogeneity</em>. The database management systems can be different inside an FDBS. For example, the difference can lie in the data structures, query languages, system software requirements, and communication capabilities.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><em id="S2.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Distribution</em>. Due to the existence of multiple DBSs before an FDBS is built, the data distribution may differ in different DBSs. A data record can be horizontally or vertically partitioned into different DBSs, and can also be duplicated in multiple DBSs to increase the reliability.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">More recently, with the development of cloud computing, many studies have been done for federated cloud computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. A federated cloud (FC) is the deployment and management of multiple external and internal cloud computing services. The concept of cloud federation enables further reduction of costs due to partial outsourcing to more cost-efficient regions. Resource migration and resource redundancy are two basic features of federated clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. First, resources may be transferred from one cloud provider to another. Migration enables the relocation of resources. Second, redundancy allows concurrent usage of similar service features in different domains. For example, the data can be partitioned and processed at different providers following the same computation logic. Overall, the scheduling of different resources is a key factor in the design of a federated cloud system.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">There are some similarities and differences between FLSs and conventional federated systems. First, the concept of federation still applies. The common and basic idea is about the cooperation of multiple independent parties. Therefore, the perspective of considering heterogeneity and autonomy among the parties can still be applied to FLSs. Second, some factors in the design of distributed systems are still important for FLSs. For example, how the data are shared between the parties can influence the efficiency of the systems. For the differences, these federated systems have different emphasis on collaboration and constraints. While FDBSs focus on the management of distributed data and FCs focus on the scheduling of the resources, FLSs care more about the secure computation among multiple parties. FLSs induce new challenges such as the algorithm designs of the distributed training and the data protection under the privacy restrictions.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Compare with Conventional Federated Systems ‣ 2 An Overview of Federated Learning Systems ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of papers in each year for these three research areas. Here we count the papers by searching keywords “federated database”, “federated cloud”, and “federated learning” in Google Scholar<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://scholar.google.com/</span></span></span></span>. Although federated database was proposed 30 years ago, there are still about 400 papers that mentioned it in recent years. The popularity of federated cloud grows more quickly than federated database at the beginning, while it appears to decrease in recent years probably because cloud computing becomes more mature and the incentives of federation diminish. For FL, the number of related papers is increasing rapidly and has achieved about 4,400 last year. Nowadays, the “data island” phenomena are common and have increasingly become an important issue in machine learning. Also, there is a increasing privacy concern and social awareness from the general public. Thus, we expect the popularity of FL will keep increasing for at least five years until there may be mature FLSs.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1907.09693/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The number of related papers on “federated database”, “federated cloud”, and “federated learning”</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>System Components</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">There are three major components in an FLS: parties (e.g., clients), the manager (e.g., server), and the communication-computation framework to train the machine learning model.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Parties</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">In FLSs, the parties are the data owners and the beneficiaries of FL. They can be organizations or mobile devices, named cross-silo or cross-device settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, respectively. We consider the following properties of the parties that affect the design of FLSs.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.p2.1" class="ltx_p">First, what is the hardware capacity of the parties? The hardware capacity includes the computation power and storage. If the parties are mobile phones, the capacity is weak and the parties cannot perform much computation and train a large model. For example, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> consider a resource constrained setting in FL. They design an objective to include the resource budget and proposed an algorithm to determine the rounds of local updates.</p>
</div>
<div id="S2.SS4.SSS1.p3" class="ltx_para">
<p id="S2.SS4.SSS1.p3.1" class="ltx_p">Second, what is the scale and stability of the parties? For organizations, the scale is relative small compared with the mobile devices. Also, the stability of the cross-silo setting is better than the cross-device setting. Thus, in the cross-silo setting, we can expect that every party can continuously conduct computation and communication tasks in the entire federated process, which is a common setting in many studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>. If the parties are mobile devices, the system has to handle possible issues such as connection lost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Moreover, since the number of devices can be very large (e.g., millions), it is unpractical to assume all the devices to participate every round in FL. The widely used setting is to choose a fraction of devices to perform computation in each round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS1.p4" class="ltx_para">
<p id="S2.SS4.SSS1.p4.1" class="ltx_p">Last, what are the data distributions among the parties? Usually, no matter cross-device or cross-silo setting, the non-IID (identically and independently distributed) data distribution is considered a practical and challenging setting in federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, which is evaluated in the experiments of recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib213" title="" class="ltx_ref">213</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>. Such non-IID data distribution may be more obvious among the organizations. For example, a bank and an insurance company can conduct FL to improve their predictions (e.g., whether a person can repay the loan and whether the person will buy the insurance products), while even the features can vary a lot in these organizations. Techniques in transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>, meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite> may be useful to combine the knowledge of various kinds of parties.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Manager</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">In the cross-device setting, the manager is usually a powerful central server. It conducts the training of the global machine learning model and manages the communication between the parties and the server. The stability and reliability of the server are quite important. Once the server fails to provide the accurate computation results, the FLS may produce a bad model. To address these potential issues, blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> may be a possible technique to offer a decentralized solution in order to increase the system reliability. For example, <cite class="ltx_cite ltx_citemacro_citet">Kim et al. [<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> leverage the blockchain in lieu of the central server in their system, where the blockchain enables exchanging the devices’ updates and providing rewards to them.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.p2.1" class="ltx_p">In the cross-silo setting, since the organizations are expected to have powerful machines, the manager can also be one of the organizations who dominates the FL process. This is particularly used in the vertical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>, which we will introduce in Section <a href="#S3.SS1" title="3.1 Data Partitioning ‣ 3 Taxonomy ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> in detail. In a vertical FL setting by <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>, the features of data are vertically partitioned across the parties and only one party has the labels. The party that owns the labels is naturally considered as the FL manager.</p>
</div>
<div id="S2.SS4.SSS2.p3" class="ltx_para">
<p id="S2.SS4.SSS2.p3.1" class="ltx_p">One challenge can be that it is hard to find a trusted server or party as the manager, especially in the cross-silo setting. Then, a fully-decentralized setting can be a good choice, where the parties communicate with each other directly and almost equally contribute to the global machine learning model training. These parties jointly set a FL task and deploy the FLS. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> propose a federated gradient boosting decision trees framework, where each party trains decision trees sequentially and the final model is the combination of all trees. It is challenging to design a fully-decentralized FLS with reasonable communication overhead.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>Communication-Computation Framework</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">In FLSs, the computation happens on the parties and the manager, while the communication happens between the parties and the manager. Usually, the aim of the computation is for the model training and the aim of the communication is for exchanging the model parameters.</p>
</div>
<div id="S2.SS4.SSS3.p2" class="ltx_para">
<p id="S2.SS4.SSS3.p2.1" class="ltx_p">A basic and widely used framework is Federated Averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> proposed in 2016, as shown in Figure <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.4.3 Communication-Computation Framework ‣ 2.4 System Components ‣ 2 An Overview of Federated Learning Systems ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>. In each iteration, the server first sends the current global model to the selected parties. Then, the selected parties update the global model with their local data. Next, the updated models are sent back to the server. Last, the server averages all the received local models to get a new global model. FedAvg repeats the above process until reaching the specified number of iterations. The global model of the server is the final output.</p>
</div>
<div id="S2.SS4.SSS3.p3" class="ltx_para">
<p id="S2.SS4.SSS3.p3.1" class="ltx_p">While FedAvg is a centralized FL framework, SimFL, proposed by <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, represents a decentralized FL framework. In SimFL, no trusted server is needed. In each iteration, the parties first update the gradients of their local data. Then, the gradients are sent to a selected party. Next, the selected party use its local data and the gradients to update the model. Last, the model is sent to all the other parties. To ensure fairness and utilize the data from different parties, every party is selected for updating the model for about the same number of rounds. SimFL repeats a specified number of iterations and outputs the final model.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1907.09693/assets/x2.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="207" height="193" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>FedAvg</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1907.09693/assets/x3.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="207" height="193" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>SimFL</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated learning frameworks</figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Taxonomy</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Considering the common system abstractions and building blocks for different FLSs, we classify FLSs by six aspects: data partitioning, machine learning model, privacy mechanism, communication architecture, scale of federation, and motivation of federation. These aspects include common factors (e.g., data partitioning, communication architecture) in previous FLSs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and unique consideration (e.g., machine learning model and privacy mechanism) for FLSs. Furthermore, these aspects can be used to guide the design of FLSs. Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Taxonomy ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the summary of the taxonomy of FLSs.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text" style="color:#000000;">In Table 1 of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, they consider different characteristics to distinguish distributed learning, cross-device federated learning, and cross-silo federated learning, including setting, data distribution, communication, etc. Our taxonomy is used to distinguish different federated learning systems from a deployment view, and aspects like machine learning models and motivation of federation are not considered in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.</span></p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1907.09693/assets/x4.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Taxonomy of federated learning systems</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Partitioning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Based on how data are distributed over the sample and feature spaces, FLSs can be typically categorized in horizontal, vertical, and hybrid FLSs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In horizontal FL, the datasets of different parties have the same feature space but little intersection on the sample space. This is a natural data partitioning especially for the cross-device setting, where different users try to improve their model performance on the same task using FL. Also, the majority of FL studies adopt horizontal partitioning. Since the local data are in the same feature space, the parties can train the local models using their local data with the same model architecture. The global model can simply be updated by averaging all the local models. A basic and popular framework of horizontal federated learning is FedAvg, as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.4.3 Communication-Computation Framework ‣ 2.4 System Components ‣ 2 An Overview of Federated Learning Systems ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Wake-word recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, such as ‘Hey Siri’ and ‘OK Google’, is a typical application of horizontal partition because each user speaks the same sentence with a different voice.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In vertical FL, the datasets of different parties have the same or similar sample space but differ in the feature space. For the vertical FLS, it usually adopts <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">entity alignment</span> techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to collect the overlapped samples of the parties. Then the overlapped data are used to train the machine learning model using encryption methods. <cite class="ltx_cite ltx_citemacro_citet">Cheng et al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> propose a lossless vertical FLS to enable parties to collaboratively train gradient boosting decision trees. They use privacy-preserving entity alignment to find common users among two parties, whose gradients are used to jointly train the decision trees. Cooperation among different companies usually can be treated as a situation of vertical partition.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In many other applications, while existing FLSs mostly focus on one kind of partition, the partition of data among the parties may be a hybrid of horizontal partition and vertical partition. Let us take cancer diagnosis system as an example. A group of hospitals wants to build an FLS for cancer diagnosis but each hospital has different patients as well as different kinds of medical examination results. Transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite> is a possible solution for such scenarios. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> propose a secure federated transfer learning system which can learn a representation among the features of parties using common instances.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Machine Learning Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Since FL is used to solve machine learning problems, the parties usually want to train a state-of-the-art machine learning model on a specified task. There have been many efforts in developing new models or reinventing current models to the federated setting. Here, we consider the widely-used models nowadays. The most popular machine learning model now is neural network (NN), which achieves state-of-the-art results in many tasks such as image classification and word prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>. There are many federated learning studies based on stochastic gradient descent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which can be used to train NNs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Another widely used model is decision tree, which is highly efficient to train and easy to interpret compared with NNs. A tree-based FLS is designed for the federated training of single or multiple decision trees (e.g., gradient boosting decision trees (GBDTs) and random forests). GBDTs are especially popular recently and it has a very good performance in many classification and regression tasks. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Cheng et al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> propose FLSs for GBDTs on horizontally and vertically partitioned data, respectively.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Besides NNs and trees, linear models (e.g., linear regression, logistic regression, SVM) are classic and easy-to-use models. There are some well developed systems for linear regression and logistic regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. These linear models are easy to learn compared with other complex models (e.g., NNs).</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text" style="color:#000000;">While a single machine learning model may be weak, ensemble methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> such as stacking and voting can be applied in the federated setting. Each party trains a local model and sends it to the server, which aggregates all the models as an ensemble. The ensemble can directly be used for prediction by max voting or be used to train a meta-model by stacking. A benefit of federated ensemble learning is that each party can train heterogeneous models as there is no averaging of model parameters. As shown in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, federated ensemble learning can also achieve a good accuracy in a single communication round.</span></p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Currently, many FL frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib192" title="" class="ltx_ref">192</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> are proposed based on stochastic gradient descent, which is a typical optimization algorithm for many models including neural networks and logistic regression. However, to increase the effectiveness of FL, we may have to exploit the model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>. Since the research of FL is still at an early stage, there is still a gap for FLSs to better support the state-of-the-art models.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Privacy Mechanisms</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Although the local data are not exposed in FL, the exchanged model parameters may still leak sensitive information about the data. There have been many attacks against machine learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>, such as model inversion attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and membership inference attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, which can potentially infer the raw data by accessing to the model. Moreover, there are many privacy mechanisms such as differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">k</annotation></semantics></math>-anonymity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, which provide different privacy guarantees. The characteristics of existing privacy mechanisms are summarized in the survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>. Here we introduce two major approaches that are adopted in the current FLSs for data protection: cryptographic methods and differential privacy.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Cryptographic methods such as homomorphic encryption <span id="S3.SS3.p2.1.1" class="ltx_text" style="color:#000000;"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite></span>, and secure multi-party computation (SMC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> are widely used in privacy-preserving machine learning algorithms. Basically, the parties have to encrypt their messages before sending, operate on the encrypted messages, and decrypt the encrypted output to get the final result. Applying the above methods, the user privacy of FLSs can usually be well protected <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>. <span id="S3.SS3.p2.1.2" class="ltx_text" style="color:#000000;">For example, SMC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> guarantees that all the parties cannot learn anything except the output, which can be used to securely aggregate the transferred gradients. However, SMC does not provide privacy guarantees for the final model, which is still vulnerable to the inference attacks and model inversion attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</span> Also, due to the additional encryption and decryption operations, such systems suffer from the extremely high computation overhead.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> guarantees that one single record does not influence much on the output of a function. Many studies adopt differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>, <a href="#bib.bib220" title="" class="ltx_ref">220</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> for data privacy protection to ensure the parties cannot know whether an individual record participates in the learning or not. By injecting random noises to the data or model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>, differential privacy provides statistical privacy guarantees for individual records and protection against the inference attack on the model. Due to the noises in the learning process, such systems tend to produce less accurate models.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text" style="color:#000000;">Note that the above methods are independent of each other, and an FLS can adopt multiple methods to enhance the privacy guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib205" title="" class="ltx_ref">205</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>.</span> There are also other approaches to protect the user privacy. An interesting hardware-based approach is to use trusted execution environment (TEE) such as Intel SGX processors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>, which can guarantee that the code and data loaded inside are protected. Such environment can be used inside the central server to increase its credibility.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Related to privacy level, the threat models also vary in FLSs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>. The attacks can come from any stage of the process of FL, including inputs, the learning process, and the learnt model.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Inputs</em> The malicious parties can conduct data poisoning attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> on FL. For example, the parties can modify the labels of training samples with a specific class, so that the final model performs badly on this class.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Learning process</em> During the learning process, the parties can perform model poisoning attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> to upload designed model parameters. Like data poisoning attacks, the global model can have a very low accuracy due to the poisoned local updating. Besides model poisoning attacks, the Byzantine fault <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> is also a common issue in distributed learning, where the parties may behave arbitrarily badly and upload random updates.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">The learnt model</em>. If the learnt model is published, the inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> can be conducted on it. The server can infer sensitive information about the training data from the exchanged model parameters. For example, membership inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> can infer whether a specific data record is used in the training. Note that the inference attacks may also be conducted in the learning process by the FL manager, who has access to the local updates of the parties.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Communication Architecture</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">There are two major ways of communication in FLSs: centralized design and decentralized design. In the centralized design, the data flow is often asymmetric, which means the manager aggregates the information (e.g., local models) from parties and sends back training results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The parameter updates on the global model are always done in this manager. The communication between the manager and the local parties can be synchronous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> or asynchronous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>, <a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>. In a decentralized design, the communications are performed among the parties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> and every party is able to update the global parameters directly.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Google Keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> is a case of centralized architecture. The server collects local model updates from users’ devices and trains a global model, which is sent back to the users for inference, as shown in Figure <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.4.3 Communication-Computation Framework ‣ 2.4 System Components ‣ 2 An Overview of Federated Learning Systems ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>. The scalability and stability are two important factors in the system design of the centralized FL.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">While the centralized design is widely used in existing studies, the decentralized design is preferred at some aspects since concentrating information on one server may bring potential risks or unfairness. <span id="S3.SS4.p3.1.1" class="ltx_text" style="color:#000000;">However, the design of the decentralized communication architecture is challenging, which should take fairness and communication overhead into consideration. There are currently three different decentralized designs: P2P <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite>, graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> or blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>, <a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite>. In a P2P design, the parties are equally privileged and treated during federated learning. An example is SimFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, where each party trains a tree sequentially and sends the tree to all the other parties. The communication architecture can also be modeled as a graph with the additional constrains such as latency and computation time. <cite class="ltx_cite ltx_citemacro_citet">Marfoq et al. [<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> propose an algorithm to find a throughput-optimal topology design. </span>Recently, blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> is a popular decentralized platform for consideration. It can be used to store the information of parties in federated learning and ensure the transparency of federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Scale of Federation</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The FLSs can be categorized into two typical types by the scale of federation: cross-silo FLSs and cross-device FLSs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. The differences between them lie on the number of parties and the amount of data stored in each party.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">In cross-silo FLS, the parties are organizations or data centers. There are usually a relatively small number of parties and each of them has a relatively large amount of data as well as computational power. For example, Amazon wants to recommend items for users by training the shopping data collected from hundreds of data centers around the world. Each data center possesses a huge amount of data as well as sufficient computational resources. <span id="S3.SS5.p2.1.1" class="ltx_text" style="color:#000000;">Another example is that federated learning can be used among medical institutions. Different hospitals can use federated learning to train a CNN for chest radiography classification while keeping their chest X-ray images locally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. With federated learning, the accuracy of the model can be significantly improved.</span> One challenge that such FLS faces is how to efficiently distribute computation to data centers under the constraint of privacy models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">In cross-device FLS, on the contrary, the number of parties is relatively large and each party has a relatively small amount of data as well as computational power. The parties are usually mobile devices. Google Keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> is an example of cross-device FLSs. The query suggestions of Google Keyboard can be improved with the help of FL. Due to the energy consumption concern, the devices cannot be asked to conduct complex training tasks. Under this occasion, the system should be powerful enough to manage a large number of parties and deal with possible issues such as the unstable connection between the device and the server.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Motivation of Federation</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">In real-world applications of FL, individual parties need the motivation to get involved in the FLS. The motivation can be regulations or incentives. FL inside a company or an organization is usually motivated by regulations (e.g., FL across different departments of a company). <span id="S3.SS6.p1.1.1" class="ltx_text" style="color:#000000;">For example, the department which has the transaction records of users can help another department to predict user credit by federated learning.</span> In many cases, parties cannot be forced to provide their data by regulations. <span id="S3.SS6.p1.1.2" class="ltx_text" style="color:#000000;">However, parties that choose to participate in federated learning can benefit from it, e.g., higher model accuracy. For example, hospitals can conduct federated learning to train a machine learning model for chest radiography classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> or COVID-19 detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>. Then, the hospitals can get a good model which has a higher accuracy than human experts and the model trained locally without federation.</span>
Another example is Google Keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>. While users have the choice to prevent Google Keyboard from utilizing their data, those who agree to upload input data may enjoy a higher accuracy of word prediction. Users may be willing to participate in federated learning for their convenience.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">A challenging problem is how to design a fair incentive mechanism, such that the party that contributes more can also benefit more from federated learning.
<span id="S3.SS6.p2.1.1" class="ltx_text" style="color:#000000;">There have been some successful cases for incentive designs in blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. The parties inside the system can be collaborators as well as competitors. Other incentive designs like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> are proposed to attract participants with high-quality data for FL. We expect game theory models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> and their equilibrium designs should be revisited under the FLSs. Even in the case of Google Keyboard, the users need to be motivated to participate this collaborative learning process. </span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Summary of Existing Studies</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Last updated on . We will periodically update this section to include the state-of-the-art and valuable FL studies. Please check out our latest version at this URL: https://arxiv.org/abs/1907.09693. Also, if you have any reference that you want to add into this survey, kindly drop Dr. Bingsheng He an email (hebs@comp.nus.edu.sg).</span></span></span>, we summarize and compare the existing studies on FLSs according to the aspects considered in Section <a href="#S3" title="3 Taxonomy ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Methodology</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To discover the existing studies on FL, we search keyword “Federated Learning” in Google Scholar. Here we only consider the published studies in the computer science community.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Since the scale of federation and the motivation of federation are problem dependent, we do not compare the existing studies by these two aspects. For ease of presentation, we use “NN”, “DT” and “LM” to denote neural networks, decision trees and linear models, respectively. Moreover, we use “CM” and “DP” to denote cryptographic methods and differential privacy, respectively. Note that the algorithms (e.g., federated stochastic gradient descent) in some studies can be used to learn many machine learning models (e.g., logistic regression and neural networks). Thus, in the “model implementation” column, we present the models implemented in the experiments of corresponding papers. Moreover, in the “main area” column, we indicate the major area that the papers study on.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Individual Studies</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We summarize existing popular and state-of-the-art research work, as shown in Table <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. From Table <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we have the following four key findings.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">First, most of the existing studies consider a horizontal data partitioning. We conjecture a part of the reason is that the experimental studies and benchmarks in horizontal data partitioning are relatively ready than vertical data partitioning. However, vertical FL is also common in real world, especially between different organizations. Vertical FL can enable more collaboration between diverse parties. Thus, more efforts should be paid to vertical FL to fill the gap.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Second, most studies consider exchanging the raw model parameters without any privacy guarantees. This may not be right if more powerful attacks on machine learning models are discovered in the future. Currently, the mainstream methods to provide privacy guarantees are differential privacy and cryptographic methods (e.g., secure multi-party computation and homomorphic encryption). Differential privacy may influence the final model quality a lot. Moreover, the cryptographic methods bring much computation and communication overhead and may be the bottleneck of FLSs. We look forward to a cheap way with reasonable privacy guarantees to satisfy the regulations.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Third, the centralized design is the mainstream of current implementations. A trusted server is needed in their settings. However, it may be hard to find a trusted server especially in the cross-silo setting. One naive approach to remove the central server is that the parties share the model parameters with all the other parties and each party also maintains the same global model locally. This method bring more communication and computation cost compared with the centralized setting. More studies should be done for practical FL with the decentralized architecture.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Last, the main research directions (also the main challenge) of FL are to improve the effectiveness, efficiency, and privacy, which are also three important metrics to evaluate an FLS. Meanwhile, there are many other research topics on FL such as fairness and incentive mechanisms. Since FL is related to many research areas, we believe that FL will attract more researchers and we can see more interesting studies in the near future.</p>
</div>
<figure id="S4.SS2.12" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison among existing published studies. LM denotes Linear Models. DM denotes Decision Trees. NN denotes Neural Networks. CM denotes Cryptographic Methods. DP denotes Differential Privacy.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.SS2.12.12" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:486.8pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-369.5pt,414.4pt) scale(0.369798162361679,0.369798162361679) ;">
<table id="S4.SS2.12.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.1.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">FL</td>
</tr>
<tr id="S4.SS2.12.12.12.13.1.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Studies</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.2.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">main</td>
</tr>
<tr id="S4.SS2.12.12.12.13.2.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">area</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.3.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">data</td>
</tr>
<tr id="S4.SS2.12.12.12.13.3.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">partitioning</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.4.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">model</td>
</tr>
<tr id="S4.SS2.12.12.12.13.4.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">implementation</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.5.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">privacy</td>
</tr>
<tr id="S4.SS2.12.12.12.13.5.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mechanism</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.6.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">communication</td>
</tr>
<tr id="S4.SS2.12.12.12.13.6.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">architecture</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.13.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.13.7.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.13.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">remark</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.SS2.1.1.1.1" class="ltx_tr">
<td id="S4.SS2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>
</td>
<td id="S4.SS2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="27"><span id="S4.SS2.1.1.1.1.3.1" class="ltx_text">
<span id="S4.SS2.1.1.1.1.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.1.1.1.1.3.1.1.1" class="ltx_tr">
<span id="S4.SS2.1.1.1.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Effective</span></span>
<span id="S4.SS2.1.1.1.1.3.1.1.2" class="ltx_tr">
<span id="S4.SS2.1.1.1.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Algorithms</span></span>
</span></span></td>
<td id="S4.SS2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="12"><span id="S4.SS2.1.1.1.1.4.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="17"><span id="S4.SS2.1.1.1.1.1.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.1.1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.1.1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.1.1.1.1.1.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.1.1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.1.1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.1.1.1.1.1.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="16"><span id="S4.SS2.1.1.1.1.6.1" class="ltx_text">centralized</span></td>
<td id="S4.SS2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="13"><span id="S4.SS2.1.1.1.1.7.1" class="ltx_text">
<span id="S4.SS2.1.1.1.1.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.1.1.1.1.7.1.1.1" class="ltx_tr">
<span id="S4.SS2.1.1.1.1.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SGD-based</span></span>
</span></span></td>
</tr>
<tr id="S4.SS2.12.12.12.14" class="ltx_tr">
<td id="S4.SS2.12.12.12.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedSVRG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM</td>
</tr>
<tr id="S4.SS2.12.12.12.15" class="ltx_tr">
<td id="S4.SS2.12.12.12.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.16" class="ltx_tr">
<td id="S4.SS2.12.12.12.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.17" class="ltx_tr">
<td id="S4.SS2.12.12.12.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
</tr>
<tr id="S4.SS2.12.12.12.18" class="ltx_tr">
<td id="S4.SS2.12.12.12.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Per-FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
</tr>
<tr id="S4.SS2.12.12.12.19" class="ltx_tr">
<td id="S4.SS2.12.12.12.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">pFedMe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.20" class="ltx_tr">
<td id="S4.SS2.12.12.12.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">IAPGD, AL2SGD+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM</td>
</tr>
<tr id="S4.SS2.12.12.12.21" class="ltx_tr">
<td id="S4.SS2.12.12.12.21.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">IFCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.22" class="ltx_tr">
<td id="S4.SS2.12.12.12.22.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Agnostic FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.23" class="ltx_tr">
<td id="S4.SS2.12.12.12.23.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedRobust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.23.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
</tr>
<tr id="S4.SS2.12.12.12.24" class="ltx_tr">
<td id="S4.SS2.12.12.12.24.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
</tr>
<tr id="S4.SS2.12.12.12.25" class="ltx_tr">
<td id="S4.SS2.12.12.12.25.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7">
<span id="S4.SS2.12.12.12.25.1.1" class="ltx_ERROR undefined">\cdashline</span>2-2                                                         
FedBCD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite></td>
<td id="S4.SS2.12.12.12.25.2" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.25.3" class="ltx_td ltx_nopad_r ltx_align_center">vertical</td>
<td id="S4.SS2.12.12.12.25.4" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="4"><span id="S4.SS2.12.12.12.25.4.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.12.12.12.25.5" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.25.6" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.25.7" class="ltx_td ltx_nopad_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.26" class="ltx_tr">
<td id="S4.SS2.12.12.12.26.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7">
<span id="S4.SS2.12.12.12.26.1.1" class="ltx_ERROR undefined">\cdashline</span>4-6
PNFM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite></td>
<td id="S4.SS2.12.12.12.26.2" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.26.3" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="2"><span id="S4.SS2.12.12.12.26.3.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.12.12.12.26.4" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.26.5" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.26.6" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="3"><span id="S4.SS2.12.12.12.26.6.1" class="ltx_text">
<span id="S4.SS2.12.12.12.26.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.12.12.12.26.6.1.1.1" class="ltx_tr">
<span id="S4.SS2.12.12.12.26.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">NN-specialized</span></span>
</span></span></td>
</tr>
<tr id="S4.SS2.12.12.12.27" class="ltx_tr">
<td id="S4.SS2.12.12.12.27.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.27.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.27.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.27.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.28" class="ltx_tr">
<td id="S4.SS2.12.12.12.28.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SplitNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.28.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vertical</td>
<td id="S4.SS2.12.12.12.28.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.28.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.29" class="ltx_tr">
<td id="S4.SS2.12.12.12.29.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7">
<span id="S4.SS2.12.12.12.29.1.1" class="ltx_ERROR undefined">\cdashline</span>2-2                                                         
Tree-based FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite></td>
<td id="S4.SS2.12.12.12.29.2" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.29.3" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="4"><span id="S4.SS2.12.12.12.29.3.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.12.12.12.29.4" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="5"><span id="S4.SS2.12.12.12.29.4.1" class="ltx_text">DT</span></td>
<td id="S4.SS2.12.12.12.29.5" class="ltx_td ltx_nopad_r ltx_align_center">DP</td>
<td id="S4.SS2.12.12.12.29.6" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="2"><span id="S4.SS2.12.12.12.29.6.1" class="ltx_text">decentralized</span></td>
<td id="S4.SS2.12.12.12.29.7" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="5"><span id="S4.SS2.12.12.12.29.7.1" class="ltx_text">DT-specialized</span></td>
</tr>
<tr id="S4.SS2.12.12.12.30" class="ltx_tr">
<td id="S4.SS2.12.12.12.30.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SimFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.30.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.30.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">hashing</td>
<td id="S4.SS2.12.12.12.30.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.30.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.31" class="ltx_tr">
<td id="S4.SS2.12.12.12.31.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7">
<span id="S4.SS2.12.12.12.31.1.1" class="ltx_ERROR undefined">\cdashline</span>2-4
FedXGB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite></td>
<td id="S4.SS2.12.12.12.31.2" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.31.3" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="7"><span id="S4.SS2.12.12.12.31.3.1" class="ltx_text">CM</span></td>
<td id="S4.SS2.12.12.12.31.4" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="16"><span id="S4.SS2.12.12.12.31.4.1" class="ltx_text">centralized</span></td>
</tr>
<tr id="S4.SS2.12.12.12.32" class="ltx_tr">
<td id="S4.SS2.12.12.12.32.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedForest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.32.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.32.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.32.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.32.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.32.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.33" class="ltx_tr">
<td id="S4.SS2.12.12.12.33.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SecureBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.33.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vertical</td>
<td id="S4.SS2.12.12.12.33.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.33.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.33.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.33.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.34" class="ltx_tr">
<td id="S4.SS2.12.12.12.34.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="7">
<span id="S4.SS2.12.12.12.34.1.1" class="ltx_ERROR undefined">\cdashline</span>2-2<span id="S4.SS2.12.12.12.34.1.2" class="ltx_ERROR undefined">\cdashline</span>5-6                                                         
Ridge Regression FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite></td>
<td id="S4.SS2.12.12.12.34.2" class="ltx_td ltx_nopad_r"></td>
<td id="S4.SS2.12.12.12.34.3" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="2"><span id="S4.SS2.12.12.12.34.3.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.12.12.12.34.4" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="5"><span id="S4.SS2.12.12.12.34.4.1" class="ltx_text">LM</span></td>
<td id="S4.SS2.12.12.12.34.5" class="ltx_td ltx_nopad_r ltx_align_center" rowspan="4"><span id="S4.SS2.12.12.12.34.5.1" class="ltx_text">LM-specialized</span></td>
</tr>
<tr id="S4.SS2.12.12.12.35" class="ltx_tr">
<td id="S4.SS2.12.12.12.35.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">PPRR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.35.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.35.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.35.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.35.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.35.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.36" class="ltx_tr">
<td id="S4.SS2.12.12.12.36.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Linear Regression FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.36.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vertical</td>
<td id="S4.SS2.12.12.12.36.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.36.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.36.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.36.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.37" class="ltx_tr">
<td id="S4.SS2.12.12.12.37.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Logistic Regression FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.37.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="32"><span id="S4.SS2.12.12.12.37.2.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.12.12.12.37.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.37.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.37.5" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.37.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.2.2.2.2" class="ltx_tr">
<td id="S4.SS2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.2.2.2.2.2.1" class="ltx_ERROR undefined">\cdashline</span>2-4<span id="S4.SS2.2.2.2.2.2.2" class="ltx_ERROR undefined">\cdashline</span>6-6
Federated MTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>
</td>
<td id="S4.SS2.2.2.2.2.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="9"><span id="S4.SS2.2.2.2.2.1.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.2.2.2.2.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.2.2.2.2.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.2.2.2.2.1.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.2.2.2.2.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.2.2.2.2.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.2.2.2.2.1.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.2.2.2.2.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">multi-task learning</td>
</tr>
<tr id="S4.SS2.12.12.12.38" class="ltx_tr">
<td id="S4.SS2.12.12.12.38.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.38.1.1" class="ltx_ERROR undefined">\cdashline</span>2-3<span id="S4.SS2.12.12.12.38.1.2" class="ltx_ERROR undefined">\cdashline</span>5-6
Federated Meta-Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.38.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.SS2.12.12.12.38.2.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.12.12.12.38.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.38.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.12.12.12.38.4.1" class="ltx_text">meta-learning</span></td>
</tr>
<tr id="S4.SS2.12.12.12.39" class="ltx_tr">
<td id="S4.SS2.12.12.12.39.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Personalized FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.39.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.39.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.40" class="ltx_tr">
<td id="S4.SS2.12.12.12.40.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.40.1.1" class="ltx_ERROR undefined">\cdashline</span>2-6
LFRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.40.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.40.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.40.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">reinforcement learning</td>
</tr>
<tr id="S4.SS2.12.12.12.41" class="ltx_tr">
<td id="S4.SS2.12.12.12.41.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FBO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.41.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.41.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM</td>
<td id="S4.SS2.12.12.12.41.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.41.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Bayesian optimization</td>
</tr>
<tr id="S4.SS2.12.12.12.42" class="ltx_tr">
<td id="S4.SS2.12.12.12.42.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.42.1.1" class="ltx_ERROR undefined">\cdashline</span>3-3<span id="S4.SS2.12.12.12.42.1.2" class="ltx_ERROR undefined">\cdashline</span>5-6
Structure Updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.42.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="19"><span id="S4.SS2.12.12.12.42.2.1" class="ltx_text">
<span id="S4.SS2.12.12.12.42.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.12.12.12.42.2.1.1.1" class="ltx_tr">
<span id="S4.SS2.12.12.12.42.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Practicality</span></span>
<span id="S4.SS2.12.12.12.42.2.1.1.2" class="ltx_tr">
<span id="S4.SS2.12.12.12.42.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Enhancement</span></span>
</span></span></td>
<td id="S4.SS2.12.12.12.42.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="7"><span id="S4.SS2.12.12.12.42.3.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.12.12.12.42.4" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.42.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S4.SS2.12.12.12.42.5.1" class="ltx_text">
<span id="S4.SS2.12.12.12.42.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.12.12.12.42.5.1.1.1" class="ltx_tr">
<span id="S4.SS2.12.12.12.42.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">efficiency</span></span>
<span id="S4.SS2.12.12.12.42.5.1.1.2" class="ltx_tr">
<span id="S4.SS2.12.12.12.42.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">improvement</span></span>
</span></span></td>
</tr>
<tr id="S4.SS2.12.12.12.43" class="ltx_tr">
<td id="S4.SS2.12.12.12.43.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Multi-Objective FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.43.2" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.44" class="ltx_tr">
<td id="S4.SS2.12.12.12.44.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">On-Device ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.44.2" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.45" class="ltx_tr">
<td id="S4.SS2.12.12.12.45.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Sparse Ternary Compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.45.2" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.SS2.12.12.12.46" class="ltx_tr">
<td id="S4.SS2.12.12.12.46.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.46.1.1" class="ltx_ERROR undefined">\cdashline</span>2-5
DPASGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.46.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.46.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">decentralized</td>
</tr>
<tr id="S4.SS2.12.12.12.47" class="ltx_tr">
<td id="S4.SS2.12.12.12.47.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.47.1.1" class="ltx_ERROR undefined">\cdashline</span>2-4
Client-Level DP FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.47.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.SS2.12.12.12.47.2.1" class="ltx_text">DP</span></td>
<td id="S4.SS2.12.12.12.47.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="23"><span id="S4.SS2.12.12.12.47.3.1" class="ltx_text">centralized</span></td>
<td id="S4.SS2.12.12.12.47.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S4.SS2.12.12.12.47.4.1" class="ltx_text">
<span id="S4.SS2.12.12.12.47.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.SS2.12.12.12.47.4.1.1.1" class="ltx_tr">
<span id="S4.SS2.12.12.12.47.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">privacy</span></span>
<span id="S4.SS2.12.12.12.47.4.1.1.2" class="ltx_tr">
<span id="S4.SS2.12.12.12.47.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">guarantees</span></span>
</span></span></td>
</tr>
<tr id="S4.SS2.12.12.12.48" class="ltx_tr">
<td id="S4.SS2.12.12.12.48.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FL-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.49" class="ltx_tr">
<td id="S4.SS2.12.12.12.49.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Local DP FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.49.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.50" class="ltx_tr">
<td id="S4.SS2.12.12.12.50.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Secure Aggregation FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.50.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.12.12.12.50.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CM</td>
</tr>
<tr id="S4.SS2.12.12.12.51" class="ltx_tr">
<td id="S4.SS2.12.12.12.51.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Hybrid FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.51.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, DT, NN</td>
<td id="S4.SS2.12.12.12.51.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CM, DP</td>
</tr>
<tr id="S4.SS2.3.3.3.3" class="ltx_tr">
<td id="S4.SS2.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.3.3.3.3.2.1" class="ltx_ERROR undefined">\cdashline</span>2-3<span id="S4.SS2.3.3.3.3.2.2" class="ltx_ERROR undefined">\cdashline</span>6-6
Backdoor FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib174" title="" class="ltx_ref">174</a>, <a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>
</td>
<td id="S4.SS2.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.SS2.3.3.3.3.3.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="13"><span id="S4.SS2.3.3.3.3.1.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.3.3.3.3.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.3.3.3.3.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.3.3.3.3.1.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.3.3.3.3.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.3.3.3.3.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.3.3.3.3.1.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="6"><span id="S4.SS2.3.3.3.3.4.1" class="ltx_text">robustness and attacks</span></td>
</tr>
<tr id="S4.SS2.12.12.12.52" class="ltx_tr">
<td id="S4.SS2.12.12.12.52.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Adversarial Lens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.53" class="ltx_tr">
<td id="S4.SS2.12.12.12.53.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Distributed Backdoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.54" class="ltx_tr">
<td id="S4.SS2.12.12.12.54.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Image Reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.55" class="ltx_tr">
<td id="S4.SS2.12.12.12.55.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.55.1.1" class="ltx_ERROR undefined">\cdashline</span>2-3
RSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.55.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM</td>
</tr>
<tr id="S4.SS2.12.12.12.56" class="ltx_tr">
<td id="S4.SS2.12.12.12.56.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model Poison <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.56.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.4.4.4.4" class="ltx_tr">
<td id="S4.SS2.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.4.4.4.4.1.1" class="ltx_ERROR undefined">\cdashline</span>5-6<span id="S4.SS2.4.4.4.4.1.2" class="ltx_ERROR undefined">\cdashline</span>2-3
<math id="S4.SS2.4.4.4.4.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.4.4.4.4.1.m1.1a"><mi id="S4.SS2.4.4.4.4.1.m1.1.1" xref="S4.SS2.4.4.4.4.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.4.4.4.4.1.m1.1b"><ci id="S4.SS2.4.4.4.4.1.m1.1.1.cmml" xref="S4.SS2.4.4.4.4.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.4.4.4.4.1.m1.1c">q</annotation></semantics></math>-FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>
</td>
<td id="S4.SS2.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
<td id="S4.SS2.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fairness</td>
</tr>
<tr id="S4.SS2.12.12.12.57" class="ltx_tr">
<td id="S4.SS2.12.12.12.57.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">BlockFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.57.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.12.12.12.57.2.1" class="ltx_text">LM</span></td>
<td id="S4.SS2.12.12.12.57.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.12.12.12.57.3.1" class="ltx_text">incentives</span></td>
</tr>
<tr id="S4.SS2.12.12.12.58" class="ltx_tr">
<td id="S4.SS2.12.12.12.58.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Reputation FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.59" class="ltx_tr">
<td id="S4.SS2.12.12.12.59.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.59.1.1" class="ltx_ERROR undefined">\cdashline</span>3-3<span id="S4.SS2.12.12.12.59.1.2" class="ltx_ERROR undefined">\cdashline</span>5-6
FedCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.59.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="9"><span id="S4.SS2.12.12.12.59.2.1" class="ltx_text">Applications</span></td>
<td id="S4.SS2.12.12.12.59.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.12.12.12.59.3.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.12.12.12.59.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.SS2.12.12.12.59.4.1" class="ltx_text">edge computing</span></td>
</tr>
<tr id="S4.SS2.12.12.12.60" class="ltx_tr">
<td id="S4.SS2.12.12.12.60.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DRL-MEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.61" class="ltx_tr">
<td id="S4.SS2.12.12.12.61.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Resource-Constrained MEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.61.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
</tr>
<tr id="S4.SS2.12.12.12.62" class="ltx_tr">
<td id="S4.SS2.12.12.12.62.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedGKT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.62.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
</tr>
<tr id="S4.SS2.12.12.12.63" class="ltx_tr">
<td id="S4.SS2.12.12.12.63.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.63.1.1" class="ltx_ERROR undefined">\cdashline</span>2-3<span id="S4.SS2.12.12.12.63.1.2" class="ltx_ERROR undefined">\cdashline</span>5-6
FedCF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.63.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.12.12.12.63.2.1" class="ltx_text">LM</span></td>
<td id="S4.SS2.12.12.12.63.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.63.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">collaborative filter</td>
</tr>
<tr id="S4.SS2.12.12.12.64" class="ltx_tr">
<td id="S4.SS2.12.12.12.64.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedMF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.64.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.64.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">matrix factorization</td>
</tr>
<tr id="S4.SS2.12.12.12.65" class="ltx_tr">
<td id="S4.SS2.12.12.12.65.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.65.1.1" class="ltx_ERROR undefined">\cdashline</span>2-3<span id="S4.SS2.12.12.12.65.1.2" class="ltx_ERROR undefined">\cdashline</span>6-6
FedRecSys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.65.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
<td id="S4.SS2.12.12.12.65.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CM</td>
<td id="S4.SS2.12.12.12.65.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">recommender system</td>
</tr>
<tr id="S4.SS2.5.5.5.5" class="ltx_tr">
<td id="S4.SS2.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FL Keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
<td id="S4.SS2.5.5.5.5.3" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.SS2.5.5.5.5.1.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.5.5.5.5.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.5.5.5.5.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.5.5.5.5.1.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.5.5.5.5.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.5.5.5.5.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.5.5.5.5.1.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">natural language processing</td>
</tr>
<tr id="S4.SS2.12.12.12.66" class="ltx_tr">
<td id="S4.SS2.12.12.12.66.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Fraud detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.66.2" class="ltx_td ltx_border_r"></td>
<td id="S4.SS2.12.12.12.66.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.12.12.12.66.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">credit card transaction</td>
</tr>
<tr id="S4.SS2.12.12.12.67" class="ltx_tr">
<td id="S4.SS2.12.12.12.67.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.67.1.1" class="ltx_ERROR undefined">\cdashline</span>5-5
FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.67.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="10"><span id="S4.SS2.12.12.12.67.2.1" class="ltx_text">Benchmarks</span></td>
<td id="S4.SS2.12.12.12.67.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.67.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.67.3.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.67.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">horizontal</td>
</tr>
<tr id="S4.SS2.12.12.12.67.3.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.67.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">&amp;vertical</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.67.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM, NN</td>
<td id="S4.SS2.12.12.12.67.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.SS2.12.12.12.67.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.SS2.12.12.12.67.5.1.1" class="ltx_tr">
<td id="S4.SS2.12.12.12.67.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">centralized</td>
</tr>
<tr id="S4.SS2.12.12.12.67.5.1.2" class="ltx_tr">
<td id="S4.SS2.12.12.12.67.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">&amp;decentralized</td>
</tr>
</table>
</td>
<td id="S4.SS2.12.12.12.67.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.SS2.12.12.12.67.6.1" class="ltx_text">general purpose benchmarks</span></td>
</tr>
<tr id="S4.SS2.12.12.12.68" class="ltx_tr">
<td id="S4.SS2.12.12.12.68.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedEval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.68.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="9"><span id="S4.SS2.12.12.12.68.2.1" class="ltx_text">horizontal</span></td>
<td id="S4.SS2.12.12.12.68.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.12.12.12.68.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">centralized</td>
</tr>
<tr id="S4.SS2.12.12.12.69" class="ltx_tr">
<td id="S4.SS2.12.12.12.69.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">OARF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.69.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.SS2.12.12.12.69.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CM,DP</td>
<td id="S4.SS2.12.12.12.69.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">centralized</td>
</tr>
<tr id="S4.SS2.8.8.8.8" class="ltx_tr">
<td id="S4.SS2.8.8.8.8.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Edge AIBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S4.SS2.6.6.6.6.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_t"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.6.6.6.6.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.6.6.6.6.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.6.6.6.6.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.6.6.6.6.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.6.6.6.6.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.6.6.6.6.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S4.SS2.7.7.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="7"><span id="S4.SS2.7.7.7.7.2.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.7.7.7.7.2.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.7.7.7.7.2.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.7.7.7.7.2.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.7.7.7.7.2.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.7.7.7.7.2.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.7.7.7.7.2.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.8.8.8.8.3" class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_t"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.8.8.8.8.3.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.8.8.8.8.3.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.8.8.8.8.3.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.8.8.8.8.3.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.8.8.8.8.3.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.8.8.8.8.3.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></td>
</tr>
<tr id="S4.SS2.12.12.12.70" class="ltx_tr">
<td id="S4.SS2.12.12.12.70.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.12.12.12.70.1.1" class="ltx_ERROR undefined">\cdashline</span>2-3<span id="S4.SS2.12.12.12.70.1.2" class="ltx_ERROR undefined">\cdashline</span>5-5
PerfEval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.70.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.SS2.12.12.12.70.2.1" class="ltx_text">NN</span></td>
<td id="S4.SS2.12.12.12.70.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.SS2.12.12.12.70.3.1" class="ltx_text">centralized</span></td>
<td id="S4.SS2.12.12.12.70.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.SS2.12.12.12.70.4.1" class="ltx_text">targeted benchmarks</span></td>
</tr>
<tr id="S4.SS2.12.12.12.71" class="ltx_tr">
<td id="S4.SS2.12.12.12.71.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.12.12.12.72" class="ltx_tr">
<td id="S4.SS2.12.12.12.72.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">semi-supervised benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>
</td>
</tr>
<tr id="S4.SS2.10.10.10.10" class="ltx_tr">
<td id="S4.SS2.10.10.10.10.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">non-IID benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>
</td>
<td id="S4.SS2.9.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS2.9.9.9.9.1.1" class="ltx_text ltx_nopad"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.9.9.9.9.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.9.9.9.9.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.9.9.9.9.1.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.9.9.9.9.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.9.9.9.9.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.9.9.9.9.1.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></span></td>
<td id="S4.SS2.10.10.10.10.2" class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_t"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.10.10.10.10.2.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.10.10.10.10.2.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.10.10.10.10.2.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.10.10.10.10.2.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.10.10.10.10.2.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.10.10.10.10.2.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></td>
</tr>
<tr id="S4.SS2.11.11.11.11" class="ltx_tr">
<td id="S4.SS2.11.11.11.11.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS2.11.11.11.11.2.1" class="ltx_ERROR undefined">\cdashline</span>2-5
LEAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S4.SS2.11.11.11.11.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_r"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.11.11.11.11.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.11.11.11.11.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.11.11.11.11.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.11.11.11.11.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.11.11.11.11.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.11.11.11.11.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S4.SS2.11.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">centralized</td>
<td id="S4.SS2.11.11.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S4.SS2.11.11.11.11.4.1" class="ltx_text">datasets</span></td>
</tr>
<tr id="S4.SS2.12.12.12.12" class="ltx_tr">
<td id="S4.SS2.12.12.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Street Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>
</td>
<td id="S4.SS2.12.12.12.12.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S4.SS2.12.12.12.12.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><svg version="1.1" height="0" width="0" overflow="visible"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.12.12.12.12.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS2.12.12.12.12.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS2.12.12.12.12.1.pic1.1.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible">
<span id="S4.SS2.12.12.12.12.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS2.12.12.12.12.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS2.12.12.12.12.1.pic1.2.1.1.1" class="ltx_p"></span>
</span>
</span></foreignObject></g></g></g></svg></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS2.SSS1" class="ltx_subsubsection ltx_figure_panel">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Effectiveness Improvement</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">While some algorithms are based on SGD, the other algorithms are specially designed for one or several kinds of model architectures. Thus, we classify them into SGD-based algorithms and model specialized algorithms accordingly.</p>
</div>
<section id="S4.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">SGD-Based</h4>

<div id="S4.SS2.SSSx1.p1" class="ltx_para">
<p id="S4.SS2.SSSx1.p1.1" class="ltx_p">If we consider the local data on a party as a single batch, SGD can be easily implemented in a federated setting by performing a single batch gradient calculation each round (i.e., FedSGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>). However, such method may require a large number of communication rounds to converge. To reduce the number of communication rounds, FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, as introduced in Section 2.3.3 and Figure 1a of the main paper, is now a typical and practical FL framework based on SGD. In FedAvg, each party conducts multiple training rounds with SGD on its local model. Then, the weights of the global model are updated as the mean of weights of the local models. The global model is sent back to the parties to finish a global iteration. By averaging the weights, the local parties can take multiple steps of gradient descent on their local models, so that the number of communication rounds can be reduced compared with FedSGD.</p>
</div>
<div id="S4.SS2.SSSx1.p2" class="ltx_para">
<p id="S4.SS2.SSSx1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Konečnỳ et al. [<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> propose federated SVRG (FSVRG). The major difference between federated SVRG and federated averaging is the way to update parameters of the local model and global model (i.e., step 2 and step 4). The formulas to update the model weights are based on stochastic variance reduced gradient (SVRG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> and distributed approximate newton algorithm (DANE) in federated SVRG. They compare their algorithm with the other baselines like CoCoA+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> and simple distributed gradient descent. Their method can achieve better accuracy with the same communication rounds for the logistic regression model. There is no comparison between federated averaging and federated SVRG.</p>
</div>
<div id="S4.SS2.SSSx1.p3" class="ltx_para">
<p id="S4.SS2.SSSx1.p3.1" class="ltx_p"><span id="S4.SS2.SSSx1.p3.1.1" class="ltx_text" style="color:#000000;">A key challenge in federated learning is the heterogeneity of local data (i.e., non-IID data) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>, which can degrade the performance of federated learning a lot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>. Since the local models are updated towards their local optima, which are far from each other due to non-IID data, the averaged global model may also far from the global optima. To address the challenge, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> propose FedProx. Since too many local updates may lead the averaged model far from the global optima, FedProx introduces an additional proximal term in the local objective to limit the amount of local changes. Instead of directly limiting the size of local updates, SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> applies the variance reduction technique to correct the local updates. While FedProx and SCAFFOLD improve the local training stage of FedAvg, FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> improves the aggregation stage of FedAvg. It takes the heterogeneous local updates of each party into consideration and normalizes the local models according to the local updates before averaging.</span></p>
</div>
<div id="S4.SS2.SSSx1.p4" class="ltx_para">
<p id="S4.SS2.SSSx1.p4.1" class="ltx_p"><span id="S4.SS2.SSSx1.p4.1.1" class="ltx_text" style="color:#000000;">The above studies’ objective is to minimize the loss on the whole training dataset under the non-IID data setting. Another solution is to design personalized federated learning algorithms, where the aim is that each party learns a personalized model which can perform well on its local data. Per-FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> applies the idea of model-agnostic meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> framework in FedAvg. pFedMe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> uses Moreau envelope to help decompose the personalized model optimization. <cite class="ltx_cite ltx_citemacro_citet">Hanzely et al. [<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> establish the lower bound for the communication complexity and local oracle complexity of the personalized federated learning optimization. Moreover, they apply accelerated proximal gradient descent (APGD) and accelerated L2SGD+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, which can achieve optimal complexity bound. IFCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> assumes that the parties are partitioned into clusters by the local objectives. The idea is to alternatively minimize the loss functions while estimating the cluster identities.</span></p>
</div>
<div id="S4.SS2.SSSx1.p5" class="ltx_para">
<p id="S4.SS2.SSSx1.p5.1" class="ltx_p"><span id="S4.SS2.SSSx1.p5.1.1" class="ltx_text" style="color:#000000;">Another research direction related to the non-IID data setting is to design robust federated learning against possible combinations of the local distributions. <cite class="ltx_cite ltx_citemacro_citet">Mohri et al. [<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> propose a new framework named agnostic FL. Instead of minimizing the loss with respect to the average distribution among the data distributions from local clients, they try to train a centralized model optimized for any possible target distribution formed by a mixture of the client distributions. FedRobust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite> considers a structured affine distribution shifts. It proposes gradient descent ascent method to solve the distributed minimax optimization problem.</span></p>
</div>
<div id="S4.SS2.SSSx1.p6" class="ltx_para">
<p id="S4.SS2.SSSx1.p6.1" class="ltx_p"><span id="S4.SS2.SSSx1.p6.1.1" class="ltx_text" style="color:#000000;">While the above studies consider the heterogeneity of data, the heterogeneity of the local models may also exist in federated learning. The parties can train models with different architectures. FedDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> utilizes knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> to aggregate the local models. It assumes a public dataset exists on the server-side, which can be used to extract the knowledge of the local models and update the global model.</span></p>
</div>
<div id="S4.SS2.SSSx1.p7" class="ltx_para">
<p id="S4.SS2.SSSx1.p7.1" class="ltx_p">There are few studies on SGD-based vertical federated learning. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> propose the Federated Stochastic Block Coordinate Descent (FedBCD) for vertical FL. By applying coordinate descent, each party updates its local parameter for multiple rounds before communicating the intermediate results. They also provide convergence analysis for FedBCD. <cite class="ltx_cite ltx_citemacro_citet">Hu et al. [<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> propose FDML for vertical FL assuming all parties have the labels. Instead of exchanging the intermediate results, it aggregates the local prediction from each of the participated party.</p>
</div>
<section id="S4.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Neural Networks</h4>

<div id="S4.SS2.SSSx2.p1" class="ltx_para">
<p id="S4.SS2.SSSx2.p1.1" class="ltx_p">Although neural networks can be trained using the SGD optimizer, we can potentially increase the model utility if the model architecture can also be exploited. <cite class="ltx_cite ltx_citemacro_citet">Yurochkin et al. [<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> develop probabilistic federated neural matching (PFNM) for multilayer perceptrons by applying Bayesian nonparametric machinery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. They use an Beta-Bernoulli process informed matching procedure to combine the local models into a federated global model. The experiments show that their approach can outperform FedAvg on both IID and non-IID data partitioning.</p>
</div>
<div id="S4.SS2.SSSx2.p2" class="ltx_para">
<p id="S4.SS2.SSSx2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite> show how to apply PFNM to CNNs (convolutional neural networks) and LSTMs (long short-term memory networks). Moreover, they propose Federated Matched Averaging (FedMA) with a layer-wise matching scheme by exploting the model architecture. Specifically, they use matched averaging to update a layer of the global model each time, which also reduces the communication size. The experiments show that FedMA performs better than FedAvg and FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> on CNNs and LSTMs.</p>
</div>
<div id="S4.SS2.SSSx2.p3" class="ltx_para">
<p id="S4.SS2.SSSx2.p3.1" class="ltx_p"><span id="S4.SS2.SSSx2.p3.1.1" class="ltx_text" style="color:#000000;">Another study for vertical federated learning on neural networks is split learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Vepakomma et al. [<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> propose a novel paradigm named SplitNN, where a neural network is divided into two parts. Each participated party just need to train a few layers of the network, then the output at the cut layer are transmitted to the party who has label and completes the rest of the training.</span></p>
</div>
<section id="S4.SS2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Trees</h4>

<div id="S4.SS2.SSSx3.p1" class="ltx_para">
<p id="S4.SS2.SSSx3.p1.1" class="ltx_p">Besides neural networks, decision trees are also widely used in the academic and industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>. Compared with NNs, the training and inference of trees are highly efficient. However, the tree parameters cannot be directly optimized by SGD, which means that SGD-based FL frameworks are not applicable to learn trees. We need specialized frameworks for trees. Among the tree models, the Gradient Boosting Decision Tree (GBDT) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> is quite popular. There are several studies on federated GBDT.</p>
</div>
<div id="S4.SS2.SSSx3.p2" class="ltx_para">
<p id="S4.SS2.SSSx3.p2.1" class="ltx_p">There are some studies on horizontal federated GBDTs. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. [<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite> propose the first FLS for GBDTs. In their framework, each decision tree is trained locally without the communications between parties. The trees trained in a party are sent to the next party to continuous train a number of trees. Differential privacy is used to protect the decision trees.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> exploit similarity information in the building of federated GBDTs by using locality-sensitive hashing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. They utilize the data distribution of local parties by aggregating gradients of similar instances. Within a weaker privacy model compared with secure multi-party computation, their approach is effective and efficient.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> propose a federated extreme boosting learning framework for mobile crowdsensing. They adopted secret sharing to achieve privacy-preserving learning of GBDTs.</p>
</div>
<div id="S4.SS2.SSSx3.p3" class="ltx_para">
<p id="S4.SS2.SSSx3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> propose Federated Forest, which enables training random forests in the vertical FL setting. In the building of each node, the party with the corresponding split feature is responsible for splitting the samples and sharing the results. They encrypt the communicated data to protect privacy. Their approach is as accurate as the non-federated version.</p>
</div>
<div id="S4.SS2.SSSx3.p4" class="ltx_para">
<p id="S4.SS2.SSSx3.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Cheng et al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> propose SecureBoost, a framework for GBDTs in the vertical FL setting. In their assumption, only one party has the label information. They used the entity alignment technique to get the common data and then build the decision trees. Additively homomorphic encryption is used to protect the gradients.</p>
</div>
<section id="S4.SS2.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Linear/Logistic Regression</h4>

<div id="S4.SS2.SSSx4.p1" class="ltx_para">
<p id="S4.SS2.SSSx4.p1.1" class="ltx_p">Linear/logistic regression can be achieved using SGD. Here we show the studies that are not SGD-based and specially designed for linear/logistic regression.</p>
</div>
<div id="S4.SS2.SSSx4.p2" class="ltx_para">
<p id="S4.SS2.SSSx4.p2.1" class="ltx_p">In the horizontal FL setting, <cite class="ltx_cite ltx_citemacro_citet">Nikolaenko et al. [<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> propose a system for privacy-preserving ridge regression. Their approaches combine both homomorphic encryption and Yao’s garbled circuit to achieve privacy requirements. An extra evaluator is needed to run the algorithm.
<cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> propose a system for privacy-preserving ridge regression. Their approaches combine both secure summation and homomorphic encryption to achieve privacy requirements. They provided a complete communication and computation overhead comparison among their approach and the previous state-of-the-art approaches.</p>
</div>
<div id="S4.SS2.SSSx4.p3" class="ltx_para">
<p id="S4.SS2.SSSx4.p3.1" class="ltx_p">In the vertical FL setting, <cite class="ltx_cite ltx_citemacro_citet">Sanil et al. [<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> present a secure regression model. They focus on the linear regression model and secret sharing is applied to ensure privacy in their solution.
<cite class="ltx_cite ltx_citemacro_citet">Hardy et al. [<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> present a solution for two-party vertical federated logistic regression. They apply entity resolution and additively homomorphic encryption.</p>
</div>
<section id="S4.SS2.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Others</h4>

<div id="S4.SS2.SSSx5.p1" class="ltx_para">
<p id="S4.SS2.SSSx5.p1.1" class="ltx_p">There are many studies that combine FL with other machine learning techniques such as multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>, meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, and transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSSx5.p2" class="ltx_para">
<p id="S4.SS2.SSSx5.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Smith et al. [<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> combine FL with multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite>. Their method considers the issues of high communication cost, stragglers, and fault tolerance for MTL in the federated environment.
<cite class="ltx_cite ltx_citemacro_citet">Corinzia and Buhmann [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> propose a federated MTL method with non-convex models. They treated the central server and the local parties as a Bayesian network and the inference is performed using variational methods.</p>
</div>
<div id="S4.SS2.SSSx5.p3" class="ltx_para">
<p id="S4.SS2.SSSx5.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. [<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> adopt meta-learning in the learning process of FedAvg. Instead of training the local NNs and exchanging the model parameters, the parties adopt the Model-Agnostic Meta-Learning (MAML) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> algorithm in the local training and exchange the gradients of MAML.
<cite class="ltx_cite ltx_citemacro_citet">Jiang et al. [<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> interpret FedAvg in the light of existing MAML algorithms. Furthermore, they apply Reptile algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite> to fine-tune the global model trained by FedAvg. Their experiments show that the meta-learning algorithm can improve the effectiveness of the global model.</p>
</div>
<div id="S4.SS2.SSSx5.p4" class="ltx_para">
<p id="S4.SS2.SSSx5.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> propose a lifelong federated reinforcement learning framework. Adopting transfer learning techniques, a global model is trained to effectively remember what the robots have learned in reinforcement learning.</p>
</div>
<div id="S4.SS2.SSSx5.p5" class="ltx_para">
<p id="S4.SS2.SSSx5.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Dai et al. [<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> considers Bayesian optimization in FL. They propose federated Thompson sampling to address the communication efficiency and heterogeneity of the clients. Their approach can potentially be used in the parameter search in federated learning.</p>
</div>
<div id="S4.SS2.SSSx5.p6" class="ltx_para">
<p id="S4.SS2.SSSx5.p6.1" class="ltx_p">Another issue in FL is the package loss or party disconnection during FL process, which usually happens on mobile devices. When the number of failed messages is small, the server can simply ignore them as they have a small weight on the updating of the global model. If the party failure is significant, the server can restart from the results of the previous round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We look forward to more novel solutions to deal with the disconnection issue for effectiveness improvement.</p>
</div>
<section id="S4.SS2.SSSx6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Summary</h4>

<div id="S4.SS2.SSSx6.p1" class="ltx_para">
<p id="S4.SS2.SSSx6.p1.1" class="ltx_p">We summarize the above studies as follows.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">As the SGD-based framework has been widely studied and used, more studies focus on model specialized FL recently. We expect to achieve better model accuracy by using model specialized methods. Moreover, we encourage researchers to study on federated decision trees models. The tree models have a small model size and are easy to train compared with neural networks, which can result in a low communication and computation overhead in FL.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The study on FL is still on a early stage. Few studies have been done on appling FL to train the state-of-the-art neural networks such as ResNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> and EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite>. How to design an effective and practical algorithm to train a complex machine learning model is still a challenging and on-going research direction.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">While most studies focus on horizontal FL, there is still no well developed algorithm for vertical FL. However, the vertical federated setting is common in real world applications where multiple organizations are involved. We look forward to more studies on this promising area.</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Communication Efficiency</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">While the computation of FL can be accelerated using modern hardware and techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> in high performance computing community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>, the FL studies mainly work on reducing the communication size during the FL process.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Konečnỳ et al. [<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> propose two ways, structured updates and sketched updates, to reduce the communication costs in federated averaging. The first approach restricts the structure of local updates and transforms it to the multiplication of two smaller matrices. Only one small matrix is sent during the learning process. The second approach uses a lossy compression method to compress the updates. Their method can reduce the communication cost by two orders of magnitude with a slight degradation in convergence speed.
<cite class="ltx_cite ltx_citemacro_citet">Zhu and Jin [<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite> design a multi-objective evolutionary algorithm to minimize the communication costs and global model test errors simultaneously. Considering the minimization of the communication cost and the maximization of the global learning accuracy as two objectives, they formulated FL as a bi-objective optimization problem and solve it by the multi-objective evolutionary algorithm.
<cite class="ltx_cite ltx_citemacro_citet">Jeong et al. [<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> propose a FL framework for devices with non-IID local data. They design federated distillation, whose communication size depends on the output dimension but not on the model size. Also, they propose a data augmentation scheme using a generative adversarial network (GAN) to make the training dataset become IID. <span id="S4.SS2.SSS2.p2.1.1" class="ltx_text" style="color:#000000;">Many other studies also design specialize approach for non-IID data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib221" title="" class="ltx_ref">221</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite></span>.
<cite class="ltx_cite ltx_citemacro_citet">Sattler et al. [<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> propose a new compression framework named sparse ternary compression (STC). Specifically, STC compresses the communication using sparsification, ternarization, error accumulation, and optimal Golomb encoding. Their method is robust to non-IID data and large numbers of parties.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p"><span id="S4.SS2.SSS2.p3.1.1" class="ltx_text" style="color:#000000;">Beside the communication size, the communication architecture can also be improved to increase the training efficiency. <cite class="ltx_cite ltx_citemacro_citet">Marfoq et al. [<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> consider the topology design for cross-silo federated learning. They propose an approach to find a throughput-optimal topology, which can significantly reduce the training time.</span></p>
</div>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Privacy, Robustness and Attacks</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Although the original data is not exchanged in FL, the model parameters can also leak sensitive information about the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>. Thus, it is important to provide privacy guarantees for the exchanged local updates.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">Differential privacy is a popular method to provide privacy guarantees. <cite class="ltx_cite ltx_citemacro_citet">Geyer et al. [<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> apply differential privacy in federated averaging from a client-level perspective. They use the Gaussian mechanism to distort the sum of updates of gradients to protect a whole client’s dataset instead of a single data point.
<cite class="ltx_cite ltx_citemacro_citet">McMahan et al. [<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> deploy federated averaging in the training of LSTM. They also use client-level differential privacy to protect the parameters.
<cite class="ltx_cite ltx_citemacro_citet">Bhowmick et al. [<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> apply local differential privacy to protect the parameters in FL. To increase the model quality, they consider a practical threat model that wishes to decode individuals’ data but has little prior information on them. Within this assumption, they can better utilize the privacy budget.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bonawitz et al. [<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> apply secure multi-party computation to protect the local parameters on the basis of federated averaging. Specifically, they present a secure aggregation protocol to securely compute the sum of vectors based on secret sharing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>. They also discuss how to combine differential privacy with secure aggregation.</p>
</div>
<div id="S4.SS2.SSS3.p4" class="ltx_para">
<p id="S4.SS2.SSS3.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Truex et al. [<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> combine both secure multiparty computation and differential privacy for privacy-preserving FL. They use differential privacy to inject noises to the local updates. Then the noisy updates will be encrypted using the Paillier cryptosystem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> before sent to the central server.</p>
</div>
<div id="S4.SS2.SSS3.p5" class="ltx_para">
<p id="S4.SS2.SSS3.p5.1" class="ltx_p">For the attacks on FL, one kind of popular attack is backdoor attack, which aims to achieve a bad global model by exchanging malicious local updates.</p>
</div>
<div id="S4.SS2.SSS3.p6" class="ltx_para">
<p id="S4.SS2.SSS3.p6.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bagdasaryan et al. [<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> conduct model poisoning attack on FL. The malicious parties commit the attack models to the server so that the global model may overfit with the poisoned data. The secure multi-party computation cannot prevent such attack since it aims to protect the confidentiality of the model parameters.
<cite class="ltx_cite ltx_citemacro_citet">Bhagoji et al. [<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> also study the model poisoning attack on FL. Since the averaging step will reduce the effect of the malicious model, it adopts an explicit boosting way to increase the committed weight update.
<cite class="ltx_cite ltx_citemacro_citet">Sun et al. <span id="S4.SS2.SSS3.p6.1.1.1.1.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib174" title="" class="ltx_ref">174</a><span id="S4.SS2.SSS3.p6.1.2.2.2.1" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS2.SSS3.p6.1.3" class="ltx_text" style="color:#000000;"> conduct experiments to evaluate backdoor attacks and defenses for federated learning on federated EMNIST dataset to see what factors can affect the performance of adversary. They find that in the absence of defenses, the performance of the attack largely depends on the fraction of adversaries presented and the ”complexity” of the targeted task. The more backdoor tasks we have, the harder it is to backdoor a ﬁxed-capacity model while maintaining its performance on the main task.</span> <cite class="ltx_cite ltx_citemacro_citet">Wang et al. <span id="S4.SS2.SSS3.p6.1.4.1.1.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib188" title="" class="ltx_ref">188</a><span id="S4.SS2.SSS3.p6.1.5.2.2.1" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS2.SSS3.p6.1.6" class="ltx_text" style="color:#000000;"> discuss the backdoor attack from a theoretical view and prove that it is feasible in FL. They also propose a new class of backdoor attacks named edge-case backdoors, which are resistant to the current defending methods.</span> <cite class="ltx_cite ltx_citemacro_citet">Xie et al. [<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> propose a distributed backdoor attack on FL. They decompose the global trigger pattern into local patterns. Each adversarial party only employs one local pattern. The experiments show that their distributed backdoor attack outperforms the central backdoor attack.</p>
</div>
<div id="S4.SS2.SSS3.p7" class="ltx_para">
<p id="S4.SS2.SSS3.p7.1" class="ltx_p"><span id="S4.SS2.SSS3.p7.1.1" class="ltx_text" style="color:#000000;">Another kind of attack is the <span id="S4.SS2.SSS3.p7.1.1.1" class="ltx_text ltx_font_italic">Byzantine</span>-attacks, where adversaries fully control some authenticated devices and behave arbitrarily to disrupt the network. There have been some existing robust aggregation rules in distributed learning such as <math id="S4.SS2.SSS3.p7.1.1.m1.1" class="ltx_Math" alttext="Krum" display="inline"><semantics id="S4.SS2.SSS3.p7.1.1.m1.1a"><mrow id="S4.SS2.SSS3.p7.1.1.m1.1.1" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S4.SS2.SSS3.p7.1.1.m1.1.1.2" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.p7.1.1.m1.1.1.1" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS2.SSS3.p7.1.1.m1.1.1.3" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.p7.1.1.m1.1.1.1a" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS2.SSS3.p7.1.1.m1.1.1.4" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.p7.1.1.m1.1.1.1b" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS2.SSS3.p7.1.1.m1.1.1.5" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.5.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p7.1.1.m1.1b"><apply id="S4.SS2.SSS3.p7.1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1"><times id="S4.SS2.SSS3.p7.1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS3.p7.1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.2">𝐾</ci><ci id="S4.SS2.SSS3.p7.1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.3">𝑟</ci><ci id="S4.SS2.SSS3.p7.1.1.m1.1.1.4.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.4">𝑢</ci><ci id="S4.SS2.SSS3.p7.1.1.m1.1.1.5.cmml" xref="S4.SS2.SSS3.p7.1.1.m1.1.1.5">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p7.1.1.m1.1c">Krum</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and Bulyan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>. These rules can be directly applied in federated learning. However, since each party conduct multiple local update steps in federated learning, it is interesting to investigate the Byzantine attacks and defenses in federated learning. <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> propose RSA, a Byzantine-robust stochastic aggregation method for federated learning on non-IID data setting. <cite class="ltx_cite ltx_citemacro_citet">Fang et al. [<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> propose model poison attacks for byzantine-robust federated learning approaches. The goal of their approach is to modify the local models such that the global model deviates the most towards the inverse of the correct update direction. </span></p>
</div>
<div id="S4.SS2.SSS3.p8" class="ltx_para">
<p id="S4.SS2.SSS3.p8.1" class="ltx_p"><span id="S4.SS2.SSS3.p8.1.1" class="ltx_text" style="color:#000000;">Another line of study about FL attack are the inference attacks. There are existing studies for the inferences attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> on the machine learning model trained in a centralized setting. For the federated setting, <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. [<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> show that it is possible to reconstruct the training images from the knowledge of the exchanged gradients.</span></p>
</div>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Fairness and Incentive Mechanisms</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.4" class="ltx_p">By taking fairness into consideration based on FedAvg, <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> propose <math id="S4.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mi id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><ci id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">q</annotation></semantics></math>-FedAvg. Specifically, they define the fairness according to the variance of the performance of the model on the parties. If such variance is smaller, then the model is more fair. Thus, they design a new objective inspired by <math id="S4.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS2.SSS4.p1.2.m2.1a"><mi id="S4.SS2.SSS4.p1.2.m2.1.1" xref="S4.SS2.SSS4.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.2.m2.1b"><ci id="S4.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.2.m2.1c">\alpha</annotation></semantics></math>-fairness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Based on federated averaging, they propose <math id="S4.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.SSS4.p1.3.m3.1a"><mi id="S4.SS2.SSS4.p1.3.m3.1.1" xref="S4.SS2.SSS4.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.3.m3.1b"><ci id="S4.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.3.m3.1c">q</annotation></semantics></math>-FedAvg to solve their new objective. The major difference between <math id="S4.SS2.SSS4.p1.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS2.SSS4.p1.4.m4.1a"><mi id="S4.SS2.SSS4.p1.4.m4.1.1" xref="S4.SS2.SSS4.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.4.m4.1b"><ci id="S4.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS4.p1.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.4.m4.1c">q</annotation></semantics></math>-FedAvg with FedAvg is in the formulas to update model parameters.</p>
</div>
<div id="S4.SS2.SSS4.p2" class="ltx_para">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Kim et al. [<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> combine blockchain architecture with FL. On the basis of federated averaging, they use a blockchain network to exchange the devices’ local model updates, which is more stable than a central server and can provide the rewards for the devices. <cite class="ltx_cite ltx_citemacro_citet">Kang et al. [<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> designed a reputation-based worker selection scheme for reliable FL by using a multi-weight subjective logic model. They also leverage the blockchain to achieve secure reputation management for workers with non-repudiation and tamper-resistance properties in a decentralized manner.</p>
</div>
<section id="S4.SS2.SSSx7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Summary</h4>

<div id="S4.SS2.SSSx7.p1" class="ltx_para">
<p id="S4.SS2.SSSx7.p1.1" class="ltx_p">According to the review above, we summarize the studies in Section <a href="#S4.SS2.SSS2" title="4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a> to Section <a href="#S4.SS2.SSS4" title="4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.4</span></a> as follows.</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Besides effectiveness, efficiency and privacy are the other two important factors of an FLS. Compared with these three areas, there are fewer studies on fairness and incentive mechanisms. We look forward to more studies on fairness and incentive mechanisms, which can encourage the usage of FL in the real world.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">For the efficiency improvement of FLSs, the communication overhead is still the main challenge. Most studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> try to reduce the communication size of each iteration. How to reasonably set the number of communication rounds is also promising <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>. The trade-off between the computation and communication still needs to be further investigated.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">For the privacy guarantees, differential privacy and secure multi-party computation are two popular techniques. However, differential privacy may impact the model quality significantly and secure multi-party computation may be very time-consuming. It is still challenging to design a practical FLS with strong privacy guarantees. Also, the effective robust algorithms against poisoning attacks are not widely adopted yet.</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Applications</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">One related area with FL is edge computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite>, where the parties are edge devices. Many studies try to integrate FL with the mobile edge systems. FL also shows promising results in recommender system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite>, natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> <span id="S4.SS2.SSS5.p1.1.1" class="ltx_text" style="color:#000000;">and transaction fraud detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>.</span></p>
</div>
<section id="S4.SS2.SSSx8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Edge Computing</h4>

<div id="S4.SS2.SSSx8.p1" class="ltx_para">
<p id="S4.SS2.SSSx8.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Nishio and Yonetani [<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> implement federated averaging in practical mobile edge computing (MEC) frameworks. They use an operator of MEC framworks to manage the resources of heterogeneous clients. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> adopt both distributed deep reinforcement learning (DRL) and federatd learning in mobile edge computing system. The usage of DRL and FL can effectively optimize the mobile edge computing, caching, and communication. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. [<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> perform FL on resource-constrained MEC systems. They address the problem of how to efficiently utilize the limited computation and communication resources at the edge. Using federated averaging, they implement many machine learning algorithms including linear regression, SVM, and CNN. <cite class="ltx_cite ltx_citemacro_citet">He et al. [<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> also consider the limited computing resources in the edge devices. They propose FedGKT, where each device only trains a small part of a whole ResNet to reduce the computation overhead.</p>
</div>
<section id="S4.SS2.SSSx9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Recommender System</h4>

<div id="S4.SS2.SSSx9.p1" class="ltx_para">
<p id="S4.SS2.SSSx9.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Ammad-ud din et al. [<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> formulate the first federated collaborative filter method. Based on a stochastic gradient approach, the item-factor matrix is trained in a global server by aggregating the local updates. They empirically show that the federated method has almost no accuracy loss compared with the centralized method. <cite class="ltx_cite ltx_citemacro_citet">Chai et al. [<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> design a federated matrix factorization framework. They use federated SGD to learn the matrices. Moreover, they adopt homomorphic encryption to protect the communicated gradients. <cite class="ltx_cite ltx_citemacro_citet">Tan et al. <span id="S4.SS2.SSSx9.p1.1.1.1.1.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib177" title="" class="ltx_ref">177</a><span id="S4.SS2.SSSx9.p1.1.2.2.2.1" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS2.SSSx9.p1.1.3" class="ltx_text" style="color:#000000;"> build a federated recommender system (FedRecSys) based on FATE. FedRecSys has implemented popular recommendation algorithms with SMC protocols. The algorithms include matrix factorization, singular value decomposition, factorization machine, and deep learning.</span></p>
</div>
<section id="S4.SS2.SSSx10" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Natural Language Processing</h4>

<div id="S4.SS2.SSSx10.p1" class="ltx_para">
<p id="S4.SS2.SSSx10.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Hard et al. [<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> apply FL in mobile keyboard next-word prediction. They adopt the federated averaging method to learn a variant of LSTM called Coupled Input and Forget Gate (CIFG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. The FL method can achieve better precision recall than the server-based training with log data.</p>
</div>
<section id="S4.SS2.SSSx11" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">Transaction Fraud Detection</h4>

<div id="S4.SS2.SSSx11.p1" class="ltx_para">
<p id="S4.SS2.SSSx11.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Zheng et al. <span id="S4.SS2.SSSx11.p1.1.1.1.1.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib222" title="" class="ltx_ref">222</a><span id="S4.SS2.SSSx11.p1.1.2.2.2.1" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.SS2.SSSx11.p1.1.3" class="ltx_text" style="color:#000000;"> introduce FL into the field of fraud detection on credit card transaction. They design a novel meta-learning based federated learning framework, named deep K-tuplet network, which not only guarantees data privacy but also achieves a significantly higher performance compared with the existing approaches.</span><span id="S4.SS2.SSSx11.p1.1.4" class="ltx_text"></span></p>
</div>
<section id="S4.SS2.SSSx12" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Summary</h4>

<div id="S4.SS2.SSSx12.p1" class="ltx_para">
<p id="S4.SS2.SSSx12.p1.1" class="ltx_p">According to the above studies, we have the following summaries.</p>
</div>
<div id="S4.SS2.SSSx12.p2" class="ltx_para">
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Edge computing naturally fits the cross-device federated setting. A nontrivial issue of applying FL to edge computing is how to effectively utilize and manage the edge resources. The usage of FL can bring benefits to users, especially for improving mobile device services.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">FL can solve many traditional machine learning tasks such as image classification and work prediction. Due to the regulations and “data islands”, the federated setting may be a common setting in the next years. With the fast development of FL, we believe that there will be more applications in computer vision, natural language processing, and healthcare.</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6 </span>Benchmark</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para">
<p id="S4.SS2.SSS6.p1.1" class="ltx_p">Benchmark is important for directing the development of FLSs. Multiple benchmark-related works have been conducted recently, and several benchmark frameworks are available online. We categorize them into three types: 1) <em id="S4.SS2.SSS6.p1.1.1" class="ltx_emph ltx_font_italic">General purpose benchmark systems</em> aim at comprehensively evaluate FLSs and give a detailed characterization of different aspects of FLSs; 2) <em id="S4.SS2.SSS6.p1.1.2" class="ltx_emph ltx_font_italic">Targeted benchmarks</em> aim at one or more aspects that concentrated in a small domain and tries to optimize the performance of the system in that domain; 3) <em id="S4.SS2.SSS6.p1.1.3" class="ltx_emph ltx_font_italic">Dataset benchmarks</em> aim at providing dedicated datasets for federated learning.</p>
</div>
<section id="S4.SS2.SSSx13" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">General Purpose Benchmark Systems</h4>

<div id="S4.SS2.SSSx13.p1" class="ltx_para">
<p id="S4.SS2.SSSx13.p1.1" class="ltx_p">FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> is a research library that provides both frameworks for federated learning and benchmark functionalities. As a benchmark, it provides comprehensive baseline implementations for multiple ML models and FL algorithms, including FedAvg, FedNAS, Vertical FL, and split learning. Moreover, it supports three computing paradigms, namely distributed training, mobile on-device training, and standalone simulation. Although some of its experiment results are currently still at a preliminary stage, it is one of the most comprehensive benchmark frameworks concerning its functionalities.</p>
</div>
<div id="S4.SS2.SSSx13.p2" class="ltx_para">
<p id="S4.SS2.SSSx13.p2.1" class="ltx_p">FedEval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is another evaluation model for federated learning. It features the “ACTPR” model, i.e., using accuracy, communication, time consumption, privacy and robustness as its evaluation targets. It utilizes Docker containers to provide an isolated evaluation environment to work around the hardware resource limitation problem, and simulated up to 100 clients in the implementation. Currently, two horizontal algorithms are supported: FedSGD and FedAvg, and the models including MLP and LeNet are tested.</p>
</div>
<div id="S4.SS2.SSSx13.p3" class="ltx_para">
<p id="S4.SS2.SSSx13.p3.1" class="ltx_p">OARF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> provides a set of utilities and reference implementations for FL benchmarks. It features the measurement of different components in FLSs, including FL algorithms, encryption mechanisms, privacy mechanisms, and communication methods. In addition, it also features realistic partitioning of datasets, which utilizes public datasets collected from different sources to reflect real-world data distributions. Both horizontal vertical algorithms are tested.</p>
</div>
<div id="S4.SS2.SSSx13.p4" class="ltx_para">
<p id="S4.SS2.SSSx13.p4.1" class="ltx_p">Edge AIBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> provides a testbed for federated learning applications, and models four application scenarios as reference implementations: ICU patients monitor, surveillance camera, smart home, and autonomous vehicles. The implementation is open sourced, but no experiment result has been reported currently.</p>
</div>
<section id="S4.SS2.SSSx14" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Targeted Benchmarks</h4>

<div id="S4.SS2.SSSx14.p1" class="ltx_para">
<p id="S4.SS2.SSSx14.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Nilsson et al. [<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> propose a method utilizing correlated t-test to compare between different types of federated learning algorithms while bypassing the influence of data distributions. Three FL algorithms, FedAvg, FedSVRG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> and CO-OP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> are compared in both IID and non-IID setup in their work, and the result shows that FedAvg achieves the highest accuracy among the three algorithms regardless of how data is partitioned.</p>
</div>
<div id="S4.SS2.SSSx14.p2" class="ltx_para">
<p id="S4.SS2.SSSx14.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Zhuang et al. [<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite> utilize benchmark analysis to improve the performance of federated person re-identification. The benchmark part uses 9 different datasets to simulate real-world situations and uses federated partial averaging, an algorithm that allows the aggregation of partially different models, as the reference implementations.</p>
</div>
<div id="S4.SS2.SSSx14.p3" class="ltx_para">
<p id="S4.SS2.SSSx14.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite> present a benchmark targeted at semi-supervised federated learning setting, where users only have unlabelled data, and the server only has a small amount of labelled data, and explore the relation between final model accuracy and multiple metrics, including the distribution of the data, the algorithm and communication settings, and the number of clients. Utilizing the experiment results, their semi-supervised learning improved method achieves better generalization performance.</p>
</div>
<div id="S4.SS2.SSSx14.p4" class="ltx_para">
<p id="S4.SS2.SSSx14.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> focus on the non-IID problem, where datasets are distributed unevenly across the participating parties. Their work explores methods for quantitatively describing the skewness of the data distribution, and propose several non-IID dataset generation approaches.</p>
</div>
<section id="S4.SS2.SSSx15" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Datasets</h4>

<div id="S4.SS2.SSSx15.p1" class="ltx_para">
<p id="S4.SS2.SSSx15.p1.1" class="ltx_p">LEAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is one of the earliest dataset proposals for federated learning. It contains six datasets covering different domains, including image classification, sentiment analysis, and next-character prediction. A set of utilities is provided to divide datasets into different parties in an IID or non-IID way. For each dataset, a reference implementation is also provided to demonstrate the usage of that dataset in the training process.</p>
</div>
<div id="S4.SS2.SSSx15.p2" class="ltx_para">
<p id="S4.SS2.SSSx15.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Luo et al. [<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> present real-world image datasets which are collected from 26 different street cameras. Images in that dataset contain objects of 7 different categories and are suitable for the object detection task. Implementations with federated averaging running YOLOv3 model and Faster R-CNN model are provided as references.</p>
</div>
<section id="S4.SS2.SSSx16" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Summary</h4>

<div id="S4.SS2.SSSx16.p1" class="ltx_para">
<p id="S4.SS2.SSSx16.p1.1" class="ltx_p">Summarizing the studies above, we have the following discoveries</p>
<ul id="S4.I4" class="ltx_itemize">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.1" class="ltx_p">Benchmarks serve an important role in the development of federated learning. Through different types of benchmarks, we can quantitatively characterize the different components and aspects of federated learning. Benchmarks regarding the security and privacy issues in federated learning are still at an early stage and require further development.</p>
</div>
</li>
<li id="S4.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i2.p1" class="ltx_para">
<p id="S4.I4.i2.p1.1" class="ltx_p">Currently no comprehensive enough benchmark system has been implemented to cover all the algorithms or application types in FLSs. Even the most comprehensive benchmark systems lack supports for certain algorithms and evaluation metrics for each level of the system. Further development of comprehensive benchmark systems requires the support of extensive FL frameworks.</p>
</div>
</li>
<li id="S4.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i3.p1" class="ltx_para">
<p id="S4.I4.i3.p1.1" class="ltx_p">Most benchmark researches are using datasets which are split from a single dataset, and there is no consensus on what type of splitting method should be used. Similarly, regarding the non-IID problem, there is no consensus on the metric of non-IID-ness. Using realistic partitioning method, as proposed in FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> and OARF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> may mitigate this issue, but for federated learning at a large-scale, realistic partitioning is not suitable due to the difficulty of collecting data from different sources.</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Open Source Systems</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we introduce five open source FLSs: Federated AI Technology Enabler (FATE)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/FederatedAI/FATE</span></span></span></span>, Google TensorFlow Federated (TFF)<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/tensorflow/federated</span></span></span></span>, OpenMined PySyft<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/OpenMined/PySyft</span></span></span></span>, Baidu PaddleFL<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/PaddlePaddle/PaddleFL</span></span></span></span>, and
FedML<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/FedML-AI/FedML</span></span></span></span>.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>FATE</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p"><span id="S4.SS3.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">FATE is an industrial level FL framework developed by WeBank, which aims to provide FL services between different organizations. FATE is based on Python and can be installed on Linux or Mac. It has attracted about 3.2k stars and 900 forks on GitHub.</span> The overall structure of FATE is shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.1 FATE ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. It has six major modules: EggRoll, FederatedML, FATE-Flow, FATE-Serving, FATE-Board, and KubeFATE. EggRoll manages the distributed computing and storage. It provides computing and storage AIPs for the other modules. FederatedML includes the federated algorithms and secure protocols. Currently, it supports training many kinds of machine learning models under both horizontal and vertical federated setting, including NNs, GBDTs, and logistic regression. <span id="S4.SS3.SSS1.p1.1.2" class="ltx_text" style="color:#000000;">FATE assumes that the parties are honest-but-curious. Thus, it uses secure multi-party computation and homomorphic encryption to protect the communicated messages. However, it does not support differential privacy to protect the final model.</span> FATE-Flow is a platform for the users to define their pipeline of the FL process. The pipeline can include the data preprocessing, federated training, federated evaluation, model management, and model publishing. FATE-Serving provides inference services for the users. It supports loading the FL models and conducting online inference on them. FATE-Board is a visualization tool for FATE. It provides a visual way to track the job execution and model performance. Last, KubeFATE helps deploy FATE on clusters by using Docker or Kubernetes. It provides customized deployment and cluster management services. In general, FATE is a powerful and easy-to-use FLS. Users can simply set the parameters to run a FL algorithm. Moreover, FATE provides detailed documents on its deployment and usage. However, since FATE provides algorithm-level interfaces, practitioners have to modify the source code of FATE to implement their own federated algorithms. This is not easy for non-expert users.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1907.09693/assets/x5.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The FATE system structure</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>TFF</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p"><span id="S4.SS3.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">TFF, developed by Google, provides the building blocks for FL based on TensorFlow. It has attracted about 1.5k stars and 380 forks on GitHub. TFF provides a Python package which can be easily installed and imported.</span> As shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3.2 TFF ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, it provides two APIs of different layers: FL API and Federated Core (FC) API. FL API offers high-level interfaces. It includes three key parts, which are models, federated computation builders, and datasets. FL API allows users to define the models or simply load the Keras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> model. The federated computation builders include the typical federated averaging algorithm. Also, FL API provides simulated federated datasets and functions to access and enumerate the local datasets for FL. Besides high-level interfaces, FC API also includes lower-level interfaces as the foundation of the FL process. Developers can implement their functions and interfaces inside the federated core.
Finally, FC provides the building blocks for FL. It support multiple federated operators such as federated sum, federated reduce, and federated broadcast. Developers can define their own operators to implement the FL algorithm. Overall, TFF is a lightweight system for developers to design and implement new FL algorithms. Currently, <span id="S4.SS3.SSS2.p1.1.2" class="ltx_text" style="color:#000000;">TFF does not consider consider any adversaries during FL training. It does not provide privacy mechanisms.</span> TFF can only deploy on a single machine now, where the federated setting is implemented by simulation.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1907.09693/assets/x6.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The TFF system structure</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>PySyft</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p"><span id="S4.SS3.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">PySyft, first proposed by <cite class="ltx_cite ltx_citemacro_citet">Ryffel et al. [<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> and developed by OpenMined, is a python library that provides interfaces for developers to implement their training algorithm. It has attracted about 7.3k stars and 1.7k forks on GitHub. While TFF is based on TensorFlow, PySyft can work well with both PyTorch and TensorFlow. PySyft provides multiple optional privacy mechanisms including secure multi-party computation and differential privacy. Thus, it can support running on honest-but-curious parties.</span> Moreover, it can be deployed on a single machine or multiple machines, where the communication between different clients is through the websocket API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>. However, while PySyft provides a set of tutorials, there is no detailed document on its interfaces and system architecture.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>PaddleFL</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p"><span id="S4.SS3.SSS4.p1.1.1" class="ltx_text" style="color:#000000;">PaddleFL is a FLS based on PaddlePaddle<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note"><span id="footnote8.1.1.1" class="ltx_text" style="color:#000000;">8</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://github.com/PaddlePaddle/Paddle</span></span></span></span>, which is a deep learning platform developed by Baidu. It is implemented on C++ and Python. It has attracted about 260 stars and 60 forks on GitHub. Like PySyft, PaddleFL supports both differential privacy and secure multi-party computation and can work on honest-but-curious parties.</span> The system structure of PaddleFL is shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.3.4 PaddleFL ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In the compile time, there are four components including FL strategies, user defined models and algorithms, distributed training configuration, and FL job generator. The FL strategies include the horizontal FL algorithms such as FedAvg. Vertical FL algorithms will be integrated in the future. Besides the provided FL strategies, users can also define their own models and training algorithms. The distributed training configuration defines the training node information in the distributed setting. FL job generator generates the jobs for federated server and workers. In the run time, there are three components including FL server, FL worker, and FL scheduler. The server and worker are the manager and parties in FL, respectively. The scheduler selects the workers that participate in the training in each round. Currently, the development of PaddleFL is still in a early stage and the documents and examples are not clear enough.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1907.09693/assets/x7.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The PaddleFL system structure</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>FedML</h4>

<div id="S4.SS3.SSS5.p1" class="ltx_para">
<p id="S4.SS3.SSS5.p1.1" class="ltx_p"><span id="S4.SS3.SSS5.p1.1.1" class="ltx_text" style="color:#000000;">FedML provides both a framework for federated learning and a platform for FL benchmark. It is developed by a team from University of Southern California <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> based on PyTorch. FedML has attracted about 660 stars and 180 forks on GitHub. As an FL framework, It’s core structure is divided into two levels, as shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.3.5 FedML ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. In the low-level FedML-core, training engine and distributed communication infrastructures are implemented.
The high-level FedML-API is built on top of them and provides training models, datasets, and FL algorithms. Reference application/benchmark implementations are further built on top of the FedML-API. While most algorithms implemented on FedML does not consider any adversaries, it supports applying differential privacy when aggregating the messages from the parties. FedML supports three computing paradigms, namely standalone simulation, distributed computing and on-device training, which provides a simulation environment for a broad spectrum of hardware requirements. Reference implementations for all supported FL algorithms are provided. Although there are still gaps between some of the experiment results and the optimal results, they provide useful information for further development.</span></p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/1907.09693/assets/x8.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The FedML system structure</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.6 </span>Others</h4>

<div id="S4.SS3.SSS6.p1" class="ltx_para">
<p id="S4.SS3.SSS6.p1.1" class="ltx_p">There are other closed source federated learning systems. NVIDIA Clara <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://developer.nvidia.com/clara</span></span></span></span> has enabled FL. It adopts a centralized architecture and encrypted communication channel. The targeted users of Clara FL is hospitals and medical institutions. Ping An Technology aims to build a federated learning system named Hive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which targets at the financial industries. While Clara FL provides APIs and documents, we cannot find the official documents of Hive.</p>
</div>
</section>
<section id="S4.SS3.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.7 </span>Summary</h4>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The comparison among some existing FLSs. The notations used in this table are the same as Table <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. The cell is left empty if the system does not support the corresponding feature. There is no release version for FedML.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:282.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.0pt,12.3pt) scale(0.919496359010275,0.919496359010275) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Supported features</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FATE 1.5.0</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TFF 0.17.0</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PySyft 0.3.0</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PaddleFL 1.1.0</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedML</td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T2.1.1.2.1.1" class="ltx_text">Operation systems</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mac</td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Linux</td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Windows</td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iOS</td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android</td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.7.1.1" class="ltx_text">Data partitioning</span></td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">horizontal</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.8" class="ltx_tr">
<td id="S4.T2.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vertical</td>
<td id="S4.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.8.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.8.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.9" class="ltx_tr">
<td id="S4.T2.1.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T2.1.1.9.1.1" class="ltx_text">Models</span></td>
<td id="S4.T2.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NN</td>
<td id="S4.T2.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.10" class="ltx_tr">
<td id="S4.T2.1.1.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DT</td>
<td id="S4.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.10.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.10.6" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.1.11" class="ltx_tr">
<td id="S4.T2.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LM</td>
<td id="S4.T2.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.12" class="ltx_tr">
<td id="S4.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.12.1.1" class="ltx_text">Privacy Mechanisms</span></td>
<td id="S4.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DP</td>
<td id="S4.T2.1.1.12.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.13" class="ltx_tr">
<td id="S4.T2.1.1.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CM</td>
<td id="S4.T2.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.13.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.13.6" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.1.14" class="ltx_tr">
<td id="S4.T2.1.1.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.14.1.1" class="ltx_text">Communication</span></td>
<td id="S4.T2.1.1.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">simulated</td>
<td id="S4.T2.1.1.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.15" class="ltx_tr">
<td id="S4.T2.1.1.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">distributed</td>
<td id="S4.T2.1.1.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.15.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.16" class="ltx_tr">
<td id="S4.T2.1.1.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.16.1.1" class="ltx_text">Hardwares</span></td>
<td id="S4.T2.1.1.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPUs</td>
<td id="S4.T2.1.1.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S4.T2.1.1.17" class="ltx_tr">
<td id="S4.T2.1.1.17.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">GPUs</td>
<td id="S4.T2.1.1.17.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.17.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.17.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.1.17.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T2.1.1.17.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS3.SSS7.p1" class="ltx_para">
<p id="S4.SS3.SSS7.p1.1" class="ltx_p">Overall, FATE, PaddleFL, and FedML try to provide algorithm-level APIs for users to use directly, while TFF and PySyft try to provide more detailed building blocks so that the developers can easily implement their FL process. Table <a href="#S4.T2" title="Table 2 ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the comparison between the open-source systems. In the algorithm level, FATE is the most comprehensive system that supports many machine learning models under both horizontal and vertical settings. TFF and PySyft only implement FedAvg, which is a basic framework in FL as shown in Section <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. PaddleFL supports several horizontal FL algorithms currently on NNs and logistic regression. FedML integrates several state-of-the-art FL algorithms such as FedOpt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite> and FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>. Compared with FATE, TFF, and FedML, PySyft and PaddleFL provide more privacy mechanisms. PySyft covers all the listed features that TFF supports, while TFF is based on TensorFlow and PySyft works better on PyTorch. <span id="S4.SS3.SSS7.p1.1.1" class="ltx_text" style="color:#000000;">Based on the popularity on GitHub, PySyft is currently the most impactful federated learning system in the machine learning community.</span></p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>System Design</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Figure <a href="#S5.F8" title="Figure 8 ‣ 5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the factors that need to be considered in the design of an FLS. <span id="S5.p1.1.1" class="ltx_text" style="color:#000000;">Here effectiveness, efficiency, and privacy are three important metrics of FLSs, which are also main research directions of federated learning. Inspired by federated database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, we also consider autonomy, which is necessary to make FLSs practical.</span> Next, we explain these factors in detail.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/1907.09693/assets/x9.png" id="S5.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The design factors of FLSs</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Effectiveness</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The core of an FLS is an (multiple) effective algorithm (algorithms). To determine the algorithm to be implemented from lots of existing studies as shown in Table <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we should first check the data partitioning of the parties. If the parties have the same features but different samples, one can use FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> for NNs and SimFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> for trees. If the parties have the same sample space but different features, one can use FedBCD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> for NNs and SecureBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> for trees.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Privacy</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">An important requirement of FLSs is to protect the user privacy. Here we analyze the reliability of the manager. If the manager is honest and not curious, then we do not need to adopt any additional technique, since the FL framework ensures that the raw data is not exchanged. If the manager is honest but curious, then we have to take possible inference attacks into consideration. The model parameters may also expose sensitive information about the training data. One can adopt differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> to inject random noises into the parameters or use SMC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to exchanged encrypted parameters. If the manager cannot be trusted at all, then we can use trusted execution environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to execute the code in the manager. Blockchain is also an option to play the role as a manager <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Efficiency</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Efficiency is an important factor in the success of many existing systems such as XGBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and ThunderSVM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>. Since federated learning involves multi-rounds training and communication, the computation and communication costs may be large, which increases the threshold of usage of FLSs.</span> To increase the efficiency, the most effective way is to deal with the bottleneck. If the bottleneck lies in the computation, we can use powerful hardware such as GPUs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and TPUs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. If the bottleneck lies in the communication, the compression techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> can be applied to reduce the communication size.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Autonomy</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Like federated databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, a practical FLS has to consider the autonomy of the parties. The parties may drop out (e.g., network failure) during the FL process, especially in the cross-device setting where the scale is large and the parties are unreliable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. <span id="S5.SS4.p1.1.1" class="ltx_text" style="color:#000000;">Thus, the FLS should be robust and stable, which can tolerate the failure of parties or reduce the number of failure cases. Google has developed a practical FLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In their system, they monitor devices’ health statistics to avoid wasting devices’ battery or bandwidth. Also, the system will complete the current round or restart from the results of the previously committed round if there are failures. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> propose a blockchain-based approach to detect the device disconnection. Robust secure aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is applicable to protect the communicated message in case of party drop out.</span> Besides the disconnection issues, the parties may be selfish and are not willing to share the model with good quality. Incentive mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> can encourage the participation of the parties and improve the final model quality.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>The Design Reference</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text" style="color:#000000;">Based on our taxonomy shown in Section <a href="#S3" title="3 Taxonomy ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and the design factors shown in Figure <a href="#S5.F8" title="Figure 8 ‣ 5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we derive a simple design reference for developing an FLS.</span></p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text" style="color:#000000;">The first step is to identity the participated entities and the task, which significantly influence the system design. The participated entities determines the communication architecture, the data partitioning and the scale of federation. The task determines the suitable machine learning models to train. Then, we can choose or design a suitable FL algorithm according to the above attributes and Table <a href="#S4.SS2" title="4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. After fixing the FL algorithm, to satisfy the privacy requirements, we may determine the privacy mechanisms to protect the communicated messages. DP is preferred if efficiency is more important than model performance compared with SMC. Last, incentive mechanism can be considered to enhance the system. Existing systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> usually do not support incentive mechanisms. However, incentive mechanisms can encourage the parties to participate and contribute in the system and make the system more attractive. Shapley value <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> is a fair approach that can be considered.</span></p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">For real-world applications of federated learning systems, please refer to Section 4 of the supplementary material.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span><span id="S5.SS6.1.1" class="ltx_text" style="color:#000000;">Evaluation</span>
</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p"><span id="S5.SS6.p1.1.1" class="ltx_text" style="color:#000000;">The evaluation of FLSs is very challenging. According to our studied system factors, it has to cover the following aspects: (1) model performance, (2) system security, (3) system efficiency, and (4) system robustness.</span></p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p"><span id="S5.SS6.p2.1.1" class="ltx_text" style="color:#000000;">For the evaluation of the model, there are two different settings. One is to evaluate the performance (e.g., prediction accuracy) of the final global model on a global dataset. The other one is to evaluate the performance of the final local models on the corresponding non-IID local datasets. The evaluation setting depends on the objective of FL, i.e., learn a global model or learn personalized local models.</span></p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p"><span id="S5.SS6.p3.1.1" class="ltx_text" style="color:#000000;">While theoretical security/privacy guarantee is a good evaluation metric for system security, another way is to conduct membership inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite> or model inversion attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> to test the system security. These attacks can be conducted in two ways: (1) white-box attack: the attacker has access to all the exchanged models during the FL process. (2) black-box attack: the attacker only has access to the final output model. The attack success ratio can be an evaluation metric for the system security.
</span></p>
</div>
<div id="S5.SS6.p4" class="ltx_para">
<p id="S5.SS6.p4.1" class="ltx_p"><span id="S5.SS6.p4.1.1" class="ltx_text" style="color:#000000;">The efficiency of the system includes two parts: computation efficiency and communication efficiency. An intuitive metric is the training time, including the computation and communication time. Note that FL is usually a multi-round process. Thus, for a fair comparison, one approach is to use time per round as a metric. Another approach is to record the time or round to achieve the same target performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.
</span></p>
</div>
<div id="S5.SS6.p5" class="ltx_para">
<p id="S5.SS6.p5.1" class="ltx_p"><span id="S5.SS6.p5.1.1" class="ltx_text" style="color:#000000;">It is challenging to quantifying the robustness of an FLS. A possible solution is to use a similar metric as robust secure aggregation, i.e., the maximum number of disconnected parties that can tolerate during the FL process.
</span></p>
</div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Case Study</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we present several real-world applications of FL according to our taxonomy, as summarized in Table <a href="#S6.T3" title="Table 3 ‣ 6 Case Study ‣ 5.6 Evaluation ‣ 5 System Design ‣ 4.3.7 Summary ‣ 4.3 Open Source Systems ‣ Summary ‣ Datasets ‣ Targeted Benchmarks ‣ General Purpose Benchmark Systems ‣ 4.2.6 Benchmark ‣ Summary ‣ Transaction Fraud Detection ‣ Natural Language Processing ‣ Recommender System ‣ Edge Computing ‣ 4.2.5 Applications ‣ Summary ‣ 4.2.4 Fairness and Incentive Mechanisms ‣ 4.2.3 Privacy, Robustness and Attacks ‣ 4.2.2 Communication Efficiency ‣ Summary ‣ Others ‣ Linear/Logistic Regression ‣ Trees ‣ Neural Networks ‣ SGD-Based ‣ 4.2.1 Effectiveness Improvement ‣ 4.2 Individual Studies ‣ 4 Summary of Existing Studies ‣ A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Requirements of the real-world federated systems</figcaption>
<table id="S6.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T3.3.1" class="ltx_tr">
<td id="S6.T3.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.1.1.1" class="ltx_text" style="font-size:90%;">System Aspect</span></td>
<td id="S6.T3.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.1.2.1" class="ltx_text" style="font-size:90%;">Mobile Service</span></td>
<td id="S6.T3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.1.3.1" class="ltx_text" style="font-size:90%;">Healthcare</span></td>
<td id="S6.T3.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.1.4.1" class="ltx_text" style="font-size:90%;">Financial</span></td>
</tr>
<tr id="S6.T3.3.2" class="ltx_tr">
<td id="S6.T3.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.2.1.1" class="ltx_text" style="font-size:90%;">Data Partitioning</span></td>
<td id="S6.T3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.2.2.1" class="ltx_text" style="font-size:90%;">Horizontal Partitioning</span></td>
<td id="S6.T3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.2.3.1" class="ltx_text" style="font-size:90%;">Hybrid Partitioning</span></td>
<td id="S6.T3.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.2.4.1" class="ltx_text" style="font-size:90%;">Vertical Partitioning</span></td>
</tr>
<tr id="S6.T3.3.3" class="ltx_tr">
<td id="S6.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.3.1.1" class="ltx_text" style="font-size:90%;">Machine Learning Model</span></td>
<td id="S6.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.3.2.1" class="ltx_text" style="font-size:90%;">No specific Models</span></td>
<td id="S6.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.3.3.1" class="ltx_text" style="font-size:90%;">No specific Models</span></td>
<td id="S6.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.3.4.1" class="ltx_text" style="font-size:90%;">No specific Models</span></td>
</tr>
<tr id="S6.T3.3.4" class="ltx_tr">
<td id="S6.T3.3.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.4.1.1" class="ltx_text" style="font-size:90%;">Scale of Federations</span></td>
<td id="S6.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.4.2.1" class="ltx_text" style="font-size:90%;">Cross-device</span></td>
<td id="S6.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.4.3.1" class="ltx_text" style="font-size:90%;">Cross-silo</span></td>
<td id="S6.T3.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.4.4.1" class="ltx_text" style="font-size:90%;">Cross-silo</span></td>
</tr>
<tr id="S6.T3.3.5" class="ltx_tr">
<td id="S6.T3.3.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.5.1.1" class="ltx_text" style="font-size:90%;">Communication Architecture</span></td>
<td id="S6.T3.3.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.5.2.1" class="ltx_text" style="font-size:90%;">Centralized</span></td>
<td id="S6.T3.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.5.3.1" class="ltx_text" style="font-size:90%;">Distributed</span></td>
<td id="S6.T3.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.5.4.1" class="ltx_text" style="font-size:90%;">Distributed</span></td>
</tr>
<tr id="S6.T3.3.6" class="ltx_tr">
<td id="S6.T3.3.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.6.1.1" class="ltx_text" style="font-size:90%;">Privacy Mechanism</span></td>
<td id="S6.T3.3.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.6.2.1" class="ltx_text" style="font-size:90%;">DP</span></td>
<td id="S6.T3.3.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.6.3.1" class="ltx_text" style="font-size:90%;">DP/SMC</span></td>
<td id="S6.T3.3.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.3.6.4.1" class="ltx_text" style="font-size:90%;">DP/SMC</span></td>
</tr>
<tr id="S6.T3.3.7" class="ltx_tr">
<td id="S6.T3.3.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.3.7.1.1" class="ltx_text" style="font-size:90%;">Motivation of Federation</span></td>
<td id="S6.T3.3.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T3.3.7.2.1" class="ltx_text" style="font-size:90%;">Incentive Motivated</span></td>
<td id="S6.T3.3.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T3.3.7.3.1" class="ltx_text" style="font-size:90%;">Policy Motivated</span></td>
<td id="S6.T3.3.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T3.3.7.4.1" class="ltx_text" style="font-size:90%;">Interest Motivated</span></td>
</tr>
</table>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Mobile Service</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">There are many corporations providing predicting service to their mobile users, such as Google Keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>, Apple’s emoji suggestion and QuickType <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>. These services bring much convenience to the users. However, the training data come from users’ edge devices, like smartphones. If the company collects data from all users and trains a global model, it might potentially cause privacy leakage. On the other hand, the data of each single user are insufficient to train an accurate prediction model. FL enables these companies to train an accuracy prediction model without accessing users’ original data, which means protecting users’ privacy. In the framework of FLSs, the users calculate and send their local models instead of their original data. That means a Google Keyboard user can enjoy an accurate prediction for the next word while not sharing his/her input history. If FLS can be widely applied to such prediction services, there will be much less data leakage since data are always stored in the edge.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">In such a scenario, data are usually horizontally split into millions of devices. Hence, the limitation of single device computational resource and the bandwidth are two major problems. Besides, the robustness of the system should also be considered since a user could join or leave the system at anytime. In other words, a centralized, cross-device FLS on horizontal data should be designed for such prediction services.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Although the basic framework of an FLS can have somehow protected individuals’ privacy, it may not be secure against inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>. Some additional privacy mechanisms like differential privacy should be leveraged to ensure the indistinguishability of individuals. Here secure multi-party computation may not be appropriate since each device has a weak computation capacity and cannot afford expensive encryption operations. Apart from guaranteeing users’ privacy, some incentive mechanisms should be developed to encourage users to contribute their data. In reality, these incentives could be vouchers or additional service.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Healthcare</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Modern health systems require cooperation among research institutes, hospitals, and federal agencies to improve health care of the nation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Moreover, collaborative research among countries is vital when facing global health emergencies, like COVID-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. These health systems mostly aim to train a model for the diagnosis of a disease. These models for diagnosis should be as accurate as possible. However, the information of patients are not allowed to transfer under some regulations such as GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The privacy of data is even more concerned in international collaboration. Without solving the privacy issue, the collaborative research could be stagnated, threatening the public health. The data privacy in such collaboration is largely based on confidentiality agreement. However, this solution is based on “trust”, which is not reliable. FL makes the cooperation possible because it can ensure the privacy theoretically, which is provable and reliable. In this way, every hospital or institute only has to share local models to get an accurate model for diagnosis.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">In such a scenario, the health care data is partitioned both horizontally and vertically: each party contains health data of residents for a specific purpose (e.g., patient treatment), but the features used in each party are diverse. The number of parties is limited and each party usually has plenty of computational resource. In other words, a private FLS on hybrid partitioned data is required. One of the most challenging problems is how to train the hybrid partitioned data. The design of the FLS could be more complicated than a simple horizontal system. In a federation of healthcare, there is probably no central server. So, another challenging part is the design of a decentralized FLS, which should also be robust against some dishonest or malicious parties. Moreover, the privacy concern can be solved by additional mechanisms like secure multi-party computation and differential privacy. The collaboration is largely motivated by regulations.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Finance</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">A federation of financial consists of banks, insurance companies, etc. They often hope to cooperate in daily financial operations. For example, some ‘bad’ users might pack back a loan in one back with the money borrowed from another bank. All the banks want to avoid such malicious behavior while not revealing other customers’ information. Also, insurance companies also want to learn from the banks about the reputation of customers. However, a leakage of ‘good’ customers’ information may cause loss of interest or some legal issues.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">This kind of cooperation can happen if we have a trusted third party, like the government. However, in many cases, the government is not involved in the federation or the government is not always trusted. So, an FLS with privacy mechanisms can be introduced. In the FLS, the privacy of each bank can be guaranteed by theoretical proved privacy mechanisms.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">In such a scenario, financial data are often vertically partitioned, linked by user ID. Training a classifier in vertically partitioned data is quite challenging. Generally, the training process can be divided into two parts: privacy-preserving record linkage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> and vertical federated training. The first part aims to find links between vertical partitioned data, and it has been well studied. The second part aims to train the linked data without sharing the original data of each party, which still remains a challenge. The cross-silo and decentralized setting are applied in this federation. Also, some privacy mechanisms should be adopted in this scenario and the participant can be motivated by interest.</p>
</div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Vision</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="color:#000000;">In this section, we show interesting directions to work on in the future. Although some directions are already covered in existing studies introduced in Section 4, we believe they are important and provide more insights on them.</span></p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Heterogeneity</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">The heterogeneity of the parties is an important characteristic in FLSs. Basically, the parties can differ in the accessibility, privacy requirements, contribution to the federation, and reliability. Thus, it is important to consider such practical issues in FLSs.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para ltx_noindent">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">Dynamic scheduling</span>
Due to the instability of the parties, the number of parties may not be fixed during the learning process. However, the number of parties is fixed in many existing studies and they do not consider the situations where there are entries of new parties or departures of the current parties. The system should support dynamic scheduling and have the ability to adjust its strategy when there is a change in the number of parties. There are some studies addressing this issue. For example, Google’s system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> can tolerate the drop-out of the devices. Also, the emergence of blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> can be an ideal and transparent platform for multi-party learning. <span id="S7.SS1.p2.1.2" class="ltx_text" style="color:#000000;">However, to the best of our knowledge, there is no work that study a increasing number of parties during FL. In such a case, more attention may be paid to the later parties, as the current global model may have been welled trained on existing parties.</span></p>
</div>
<div id="S7.SS1.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">Diverse privacy restrictions</span>
Little work has considered the privacy heterogeneity of FLSs, where the parties have different privacy requirements. The existing systems adopt techniques to protect the model parameters or gradients for all the parties on the same level. However, the privacy restrictions of the parties usually differ in reality. It would be interesting to design an FLS which treats the parties differently according to their privacy restrictions. The learned model should have a better performance if we can maximize the utilization of data of each party while not violating their privacy restrictions. <span id="S7.SS1.p3.1.2" class="ltx_text" style="color:#000000;">The heterogeneous differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> may be useful in such settings, where users have different privacy attitudes and expectations.</span></p>
</div>
<div id="S7.SS1.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.p4.1" class="ltx_p"><span id="S7.SS1.p4.1.1" class="ltx_text ltx_font_bold">Intelligent benefits</span></p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.1" class="ltx_p">Intuitively, one party should gain more from the FLS if it contributes more information. <span id="S7.SS1.p5.1.1" class="ltx_text" style="color:#000000;">Existing incentive mechanisms are mostly based on Shapley values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>, the computation overhead is a major concern. A computation-efficient and fair incentive mechanism needs to be developed.</span></p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>System Development</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">To boost the development of FLSs, besides the detailed algorithm design, we need to study from a high-level view.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS2.p2.1" class="ltx_p"><span id="S7.SS2.p2.1.1" class="ltx_text ltx_font_bold">System architecture</span>
Like the parameter server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> in deep learning which controls the parameter synchronization, some common system architectures are needed to be investigated for FL. Although FedAvg is a widely used framework, the applicable scenarios are still limited. <span id="S7.SS2.p2.1.2" class="ltx_text" style="color:#000000;">For example, while unsupervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> still adopt model averaging as the model aggregation method, which cannot work if the parties want to train heterogeneous models.</span> We want a general system architecture, which provides many aggregation methods and learning algorithms for different settings.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para ltx_noindent">
<p id="S7.SS2.p3.1" class="ltx_p"><span id="S7.SS2.p3.1.1" class="ltx_text ltx_font_bold">Model market</span>
<span id="S7.SS2.p3.1.2" class="ltx_text" style="color:#000000;">Model market <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> is a promising platform for model storing, sharing, and selling. An interesting idea is to use the model market for federated learning. </span>The party can buy the models to conduct model aggregation locally. Moreover, it can contribute its models to the market with additional information such as the target task. Such a design introduces more flexibility to the federation and is more acceptable for the organizations, since the FL just like several transactions. A well evaluation of the models is important in such systems. The incentive mechanisms may be helpful <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.</p>
</div>
<div id="S7.SS2.p4" class="ltx_para ltx_noindent">
<p id="S7.SS2.p4.1" class="ltx_p"><span id="S7.SS2.p4.1.1" class="ltx_text ltx_font_bold">Benchmark</span>
As more FLSs are being developed, a benchmark with representative data sets and workloads is quite important to evaluate the existing systems and direct future development. Although there have been quite a few benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, no benchmark has been widely used in the experiments of federated learning studies. We need a robust benchmark which has representative datasets and strict privacy evaluation. <span id="S7.SS2.p4.1.2" class="ltx_text" style="color:#000000;">Also, comprehensive evaluation metric including model performance, system efficiency, system security, and system robustness is often ignored in existing benchmarks. The evaluation of model performance on non-IID datasets and system security under data pollution needs more investigation.</span></p>
</div>
<div id="S7.SS2.p5" class="ltx_para ltx_noindent">
<p id="S7.SS2.p5.1" class="ltx_p"><span id="S7.SS2.p5.1.1" class="ltx_text ltx_font_bold">Data life cycles</span>
<span id="S7.SS2.p5.1.2" class="ltx_text" style="color:#000000;">Learning is simply one aspects of a federated system. A data life cycle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> consists of multiple stages including data creation, storage, use, share, archive and destroy. For the data security and privacy of the entire application, we need to invent new data life cycles under FL context. Although data sharing is clearly one of the focused stage, the design of FLSs also affects other stages. For example, data creation may help to prepare the data and features that are suitable for FL.</span></p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>FL in Domains</h3>

<div id="S7.SS3.p1" class="ltx_para ltx_noindent">
<p id="S7.SS3.p1.1" class="ltx_p"><span id="S7.SS3.p1.1.1" class="ltx_text ltx_font_bold">Internet-of-thing</span>
Security and privacy issues have been a hot research area in fog computing and edge computing, due to the increasing deployment of Internet-of-thing applications. For more details, readers can refer to some recent surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>, <a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>. FL can be one potential approach in addressing the data privacy issues, while still offering reasonably good machine learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>. The additional key challenges come from the computation and energy constraints. The mechanisms of privacy and security introduces runtime overhead. For example, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. [<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> apply independent Gaussian random projection to improve the data privacy, and then the training of a deep network can be too costly. The authors need to develop a new resource scheduling algorithm to move the workload to the nodes with more computation power. Similar issues happen in other environments such as vehicle-to-vehicle networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para ltx_noindent">
<p id="S7.SS3.p2.1" class="ltx_p"><span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_bold">Regulations</span>
While FL enables collaborative learning without exposing the raw data, it is still not clear how FL comply with the existing regulations. For example, GDPR proposes limitations on data transfer. Since the model and gradients are actually not safe enough, is such limitation still apply to the model or gradients? Also, the “right to explainability” is hard to execute since the global model is an averaging of the local models. The explainability of the FL models is an open problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>. Moreover, if a user wants to delete its data, should the global model be retrained without the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>? There is still a gap between the FL techniques and the regulations in reality. We may expect the cooperation between the computer science community and the law community.</p>
</div>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Many efforts have been devoted to developing federated learning systems (FLSs). A complete overview and summary for existing FLSs is important and meaningful. Inspired by the previous federated systems, we have shown that heterogeneity and autonomy are two important factors in the design of practical FLSs. Moreover, with six different aspects, we provide a comprehensive categorization for FLSs. Based on these aspects, we also present the comparison on features and designs among existing FLSs. More importantly, we have pointed out a number of opportunities, ranging from more benchmarks to integration of emerging platforms such as blockchain. <span id="S8.p1.1.1" class="ltx_text" style="color:#000000;">FLSs will be an exciting research direction, which calls for the effort from machine learning, system and data privacy communities.</span></p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is supported by a MoE AcRF Tier 1 grant (T1 251RES1824), an SenseTime Young Scholars Research Fund, and a MOE Tier 2 grant (MOE2017-T2-1-122) in Singapore.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
California Consumer Privacy Act Home Page.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.caprivacy.org/</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.intel.com/content/www/us/en/customer-spotlight/stories/ping-an-sgx-customer-story.html</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ube [2018]</span>
<span class="ltx_bibblock">
Uber settles data breach investigation for $148 million, 2018.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nytimes.com/2018/09/26/technology/uber-data-breach.html</span>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">goo [2019]</span>
<span class="ltx_bibblock">
Google is fined $57 million under europe’s data privacy law, 2019.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nytimes.com/2019/01/21/technology/google-europe-gdpr-fine.html</span>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">sin [2019]</span>
<span class="ltx_bibblock">
2019 is a ’fine’ year: Pdpc has fined s’pore firms a record $1.29m for data
breaches, 2019.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://vulcanpost.com/676006/pdpc-data-breach-singapore-2019/</span>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">cov [2020]</span>
<span class="ltx_bibblock">
Rolling updates on coronavirus disease (covid-19), 2020.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-they-happen</span>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. [2016a]</span>
<span class="ltx_bibblock">
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.

</span>
<span class="ltx_bibblock">Tensorflow: A system for large-scale machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.4.4" class="ltx_emph ltx_font_italic">12th <math id="bib.bib7.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib7.1.1.m1.1a"><mo stretchy="false" id="bib.bib7.1.1.m1.1.1" xref="bib.bib7.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.1.1.m1.1b"><ci id="bib.bib7.1.1.m1.1.1.cmml" xref="bib.bib7.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib7.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib7.2.2.m2.1a"><mo stretchy="false" id="bib.bib7.2.2.m2.1.1" xref="bib.bib7.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.2.2.m2.1b"><ci id="bib.bib7.2.2.m2.1.1.cmml" xref="bib.bib7.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.2.2.m2.1c">\}</annotation></semantics></math> Symposium on Operating Systems Design
and Implementation (<math id="bib.bib7.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib7.3.3.m3.1a"><mo stretchy="false" id="bib.bib7.3.3.m3.1.1" xref="bib.bib7.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.3.3.m3.1b"><ci id="bib.bib7.3.3.m3.1.1.cmml" xref="bib.bib7.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib7.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib7.4.4.m4.1a"><mo stretchy="false" id="bib.bib7.4.4.m4.1.1" xref="bib.bib7.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.4.4.m4.1b"><ci id="bib.bib7.4.4.m4.1.1.cmml" xref="bib.bib7.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.4.4.m4.1c">\}</annotation></semantics></math> 16)</em>, pages 265–283, 2016a.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. [2016b]</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security</em>, pages 308–318. ACM, 2016b.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alaggan et al. [2015]</span>
<span class="ltx_bibblock">
Mohammad Alaggan, Sébastien Gambs, and Anne-Marie Kermarrec.

</span>
<span class="ltx_bibblock">Heterogeneous differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.06998</em>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albrecht [2016]</span>
<span class="ltx_bibblock">
Jan Philipp Albrecht.

</span>
<span class="ltx_bibblock">How the gdpr will change the world.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Eur. Data Prot. L. Rev.</em>, 2:287, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aledhari et al. [2020]</span>
<span class="ltx_bibblock">
Mohammed Aledhari, Rehma Razzak, Reza M Parizi, and Fahad Saeed.

</span>
<span class="ltx_bibblock">Federated learning: A survey on enabling technologies, protocols, and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8:140699–140725, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alfeld et al. [2016]</span>
<span class="ltx_bibblock">
Scott Alfeld, Xiaojin Zhu, and Paul Barford.

</span>
<span class="ltx_bibblock">Data poisoning attacks against autoregressive models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Thirtieth AAAI Conference on Artificial Intelligence</em>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altman et al. [2008]</span>
<span class="ltx_bibblock">
Eitan Altman, Konstantin Avrachenkov, and Andrey Garnaev.

</span>
<span class="ltx_bibblock">Generalized <math id="bib.bib13.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib13.1.m1.1a"><mi id="bib.bib13.1.m1.1.1" xref="bib.bib13.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib13.1.m1.1b"><ci id="bib.bib13.1.m1.1.1.cmml" xref="bib.bib13.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.1.m1.1c">\alpha</annotation></semantics></math>-fair resource allocation in wireless networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.2.1" class="ltx_emph ltx_font_italic">2008 47th IEEE Conference on Decision and Control</em>, pages
2414–2419. IEEE, 2008.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammad-ud din et al. [2019]</span>
<span class="ltx_bibblock">
Muhammad Ammad-ud din, Elena Ivannikova, Suleiman A Khan, Were Oyomno, Qiang
Fu, Kuan Eeik Tan, and Adrian Flanagan.

</span>
<span class="ltx_bibblock">Federated collaborative filtering for privacy-preserving personalized
recommendation system.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.09888</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aono et al. [2018]</span>
<span class="ltx_bibblock">
Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning via additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em>,
13(5):1333–1345, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al. [2020]</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</em>, pages 2938–2948. PMLR, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bell et al. [2020]</span>
<span class="ltx_bibblock">
James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède
Lepoint, and Mariana Raykova.

</span>
<span class="ltx_bibblock">Secure single-server aggregation with (poly) logarithmic overhead.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CCS</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bernstein et al. [2018]</span>
<span class="ltx_bibblock">
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.

</span>
<span class="ltx_bibblock">signsgd: Compressed optimisation for non-convex problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.04434</em>, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et al. [2018]</span>
<span class="ltx_bibblock">
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.

</span>
<span class="ltx_bibblock">Analyzing federated learning through an adversarial lens, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhowmick et al. [2018]</span>
<span class="ltx_bibblock">
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan
Rogers.

</span>
<span class="ltx_bibblock">Protection against reconstruction and its applications in private
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00984</em>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al. [2017]</span>
<span class="ltx_bibblock">
Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
119–129, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2016]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for federated learning on user-held
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.04482</em>, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2017]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>, pages 1175–1191. ACM, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2019]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi,
H Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em>, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. [2018]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečnỳ, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caruana [1997]</span>
<span class="ltx_bibblock">
Rich Caruana.

</span>
<span class="ltx_bibblock">Multitask learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Machine learning</em>, 28(1):41–75, 1997.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro et al. [1999]</span>
<span class="ltx_bibblock">
Miguel Castro, Barbara Liskov, et al.

</span>
<span class="ltx_bibblock">Practical byzantine fault tolerance.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">OSDI</em>, volume 99, pages 173–186, 1999.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chabanne et al. [2017]</span>
<span class="ltx_bibblock">
Hervé Chabanne, Amaury de Wargny, Jonathan Milgram, Constance Morel, and
Emmanuel Prouff.

</span>
<span class="ltx_bibblock">Privacy-preserving classification on deep neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IACR Cryptology ePrint Archive</em>, 2017:35, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al. [2019]</span>
<span class="ltx_bibblock">
Di Chai, Leye Wang, Kai Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secure federated matrix factorization.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.05108</em>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al. [2020]</span>
<span class="ltx_bibblock">
Di Chai, Leye Wang, Kai Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Fedeval: A benchmark system with a comprehensive evaluation model for
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.09655</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhuri et al. [2011]</span>
<span class="ltx_bibblock">
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate.

</span>
<span class="ltx_bibblock">Differentially private empirical risk minimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 12(Mar):1069–1109, 2011.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaum [1988]</span>
<span class="ltx_bibblock">
David Chaum.

</span>
<span class="ltx_bibblock">The dining cryptographers problem: Unconditional sender and recipient
untraceability.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Journal of cryptology</em>, 1(1):65–75, 1988.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018a]</span>
<span class="ltx_bibblock">
Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He.

</span>
<span class="ltx_bibblock">Federated meta-learning for recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.07876</em>, 2018a.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Guestrin [2016]</span>
<span class="ltx_bibblock">
Tianqi Chen and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Xgboost: A scalable tree boosting system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">KDD</em>, pages 785–794. ACM, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2017]</span>
<span class="ltx_bibblock">
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.

</span>
<span class="ltx_bibblock">Targeted backdoor attacks on deep learning systems using data
poisoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.05526</em>, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018b]</span>
<span class="ltx_bibblock">
Yi-Ruei Chen, Amir Rezapour, and Wen-Guey Tzeng.

</span>
<span class="ltx_bibblock">Privacy-preserving ridge regression on distributed data.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, 451:34–49,
2018b.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Yu Chen, Fang Luo, Tong Li, Tao Xiang, Zheli Liu, and Jin Li.

</span>
<span class="ltx_bibblock">A training-integrity privacy-preserving federated learning scheme
with trusted execution environment.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, 522:69–79, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2019]</span>
<span class="ltx_bibblock">
Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secureboost: A lossless federated learning framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.08755</em>, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chik [2013]</span>
<span class="ltx_bibblock">
Warren B Chik.

</span>
<span class="ltx_bibblock">The singapore personal data protection act and an assessment of
future trends in data privacy reform.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Computer Law &amp; Security Review</em>, 29(5):554–575, 2013.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhury et al. [2019]</span>
<span class="ltx_bibblock">
Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla,
Yoonyoung Park, Grace Hsu, and Amar Das.

</span>
<span class="ltx_bibblock">Differential privacy-enabled federated learning for sensitive health
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.02578</em>, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christen [2012]</span>
<span class="ltx_bibblock">
Peter Christen.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Data matching: concepts and techniques for record linkage,
entity resolution, and duplicate detection</em>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2012.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook [2012]</span>
<span class="ltx_bibblock">
Shane Cook.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">CUDA programming: a developer’s guide to parallel computing
with GPUs</em>.

</span>
<span class="ltx_bibblock">Newnes, 2012.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corinzia and Buhmann [2019]</span>
<span class="ltx_bibblock">
Luca Corinzia and Joachim M Buhmann.

</span>
<span class="ltx_bibblock">Variational federated multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06268</em>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2020]</span>
<span class="ltx_bibblock">
Zhongxiang Dai, Kian Hsiang Low, and Patrick Jaillet.

</span>
<span class="ltx_bibblock">Federated bayesian optimization via thompson sampling.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Datar et al. [2004]</span>
<span class="ltx_bibblock">
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni.

</span>
<span class="ltx_bibblock">Locality-sensitive hashing scheme based on p-stable distributions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the twentieth annual symposium on
Computational geometry</em>, pages 253–262. ACM, 2004.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinh et al. [2020]</span>
<span class="ltx_bibblock">
Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen.

</span>
<span class="ltx_bibblock">Personalized federated learning with moreau envelopes.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.08848</em>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan [2019]</span>
<span class="ltx_bibblock">
Moming Duan.

</span>
<span class="ltx_bibblock">Astraea: Self-balancing federated learning for improving
classification accuracy of mobile deep learning applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.01132</em>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. [2006]</span>
<span class="ltx_bibblock">
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.

</span>
<span class="ltx_bibblock">Calibrating noise to sensitivity in private data analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Theory of cryptography conference</em>, pages 265–284.
Springer, 2006.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. [2014]</span>
<span class="ltx_bibblock">
Cynthia Dwork, Aaron Roth, et al.

</span>
<span class="ltx_bibblock">The algorithmic foundations of differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Theoretical Computer
Science</em>, 9(3–4):211–407, 2014.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El Emam and Dankar [2008]</span>
<span class="ltx_bibblock">
Khaled El Emam and Fida Kamal Dankar.

</span>
<span class="ltx_bibblock">Protecting privacy using k-anonymity.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Journal of the American Medical Informatics Association</em>,
15(5):627–637, 2008.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyal et al. [2016]</span>
<span class="ltx_bibblock">
Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, and Robbert Van Renesse.

</span>
<span class="ltx_bibblock">Bitcoin-ng: A scalable blockchain protocol.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">13th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 16)</em>, pages 45–59, Santa Clara, CA, March 2016.
USENIX Association.

</span>
<span class="ltx_bibblock">ISBN 978-1-931971-29-4.

</span>
<span class="ltx_bibblock">URL
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/eyal</span>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fallah et al. [2020]</span>
<span class="ltx_bibblock">
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.

</span>
<span class="ltx_bibblock">Personalized federated learning with theoretical guarantees: A
model-agnostic meta-learning approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. [2020]</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to byzantine-robust federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">USENIX</em>, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. [2018]</span>
<span class="ltx_bibblock">
Ji Feng, Yang Yu, and Zhi-Hua Zhou.

</span>
<span class="ltx_bibblock">Multi-layered gradient boosting decision trees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
3551–3561, 2018.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn et al. [2017]</span>
<span class="ltx_bibblock">
Chelsea Finn, Pieter Abbeel, and Sergey Levine.

</span>
<span class="ltx_bibblock">Model-agnostic meta-learning for fast adaptation of deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em>, pages 1126–1135. JMLR. org, 2017.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fredrikson et al. [2015]</span>
<span class="ltx_bibblock">
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.

</span>
<span class="ltx_bibblock">Model inversion attacks that exploit confidence information and basic
countermeasures.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC Conference on Computer
and Communications Security</em>, pages 1322–1333. ACM, 2015.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman et al. [2010]</span>
<span class="ltx_bibblock">
Charles P Friedman, Adam K Wong, and David Blumenthal.

</span>
<span class="ltx_bibblock">Achieving a nationwide learning health system.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Science translational medicine</em>, 2(57):57cm29–57cm29, 2010.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al. [2020]</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.

</span>
<span class="ltx_bibblock">Inverting gradients–how easy is it to break privacy in federated
learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.14053</em>, 2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gershman and Blei [2012]</span>
<span class="ltx_bibblock">
Samuel J Gershman and David M Blei.

</span>
<span class="ltx_bibblock">A tutorial on bayesian nonparametric models.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Journal of Mathematical Psychology</em>, 56(1):1–12, 2012.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer et al. [2017]</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo Klein, and Moin Nabi.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>, 2017.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. [2020]</span>
<span class="ltx_bibblock">
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran.

</span>
<span class="ltx_bibblock">An efficient framework for clustered federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.04088</em>, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginart et al. [2019]</span>
<span class="ltx_bibblock">
Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou.

</span>
<span class="ltx_bibblock">Making ai forget you: Data deletion in machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
3513–3526, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldreich [1998]</span>
<span class="ltx_bibblock">
Oded Goldreich.

</span>
<span class="ltx_bibblock">Secure multi-party computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Manuscript. Preliminary version</em>, 78, 1998.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goryczka and Xiong [2015]</span>
<span class="ltx_bibblock">
Slawomir Goryczka and Li Xiong.

</span>
<span class="ltx_bibblock">A comprehensive comparison of multiparty secure additions with
differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on dependable and secure computing</em>,
14(5):463–477, 2015.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greff et al. [2016]</span>
<span class="ltx_bibblock">
Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and
Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Lstm: A search space odyssey.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>,
28(10):2222–2232, 2016.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulli and Pal [2017]</span>
<span class="ltx_bibblock">
Antonio Gulli and Sujit Pal.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Deep learning with Keras</em>.

</span>
<span class="ltx_bibblock">Packt Publishing Ltd, 2017.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunning [2017]</span>
<span class="ltx_bibblock">
David Gunning.

</span>
<span class="ltx_bibblock">Explainable artificial intelligence (xai).

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Defense Advanced Research Projects Agency (DARPA), nd Web</em>, 2,
2017.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely and Richtárik [2020]</span>
<span class="ltx_bibblock">
Filip Hanzely and Peter Richtárik.

</span>
<span class="ltx_bibblock">Federated learning of a mixture of global and local models.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05516</em>, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely et al. [2020]</span>
<span class="ltx_bibblock">
Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter
Richtárik.

</span>
<span class="ltx_bibblock">Lower bounds and optimal algorithms for personalized federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.02372</em>, 2020.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. [2019]</span>
<span class="ltx_bibblock">
Tianshu Hao, Yunyou Huang, Xu Wen, Wanling Gao, Fan Zhang, Chen Zheng, Lei
Wang, Hainan Ye, Kai Hwang, Zujie Ren, et al.

</span>
<span class="ltx_bibblock">Edge aibench: Towards comprehensive end-to-end edge computing
benchmarking.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.01924</em>, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. [2018]</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al. [2017]</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.10677</em>, 2017.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020a]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Federated learning of large cnns at the
edge.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33,
2020a.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020b]</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020b.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al. [2015]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. [2013]</span>
<span class="ltx_bibblock">
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B
Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing.

</span>
<span class="ltx_bibblock">More effective distributed ml via a stale synchronous parallel
parameter server.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
1223–1231, 2013.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2020]</span>
<span class="ltx_bibblock">
Sixu Hu, Yuan Li, Xu Liu, Qinbin Li, Zhaomin Wu, and Bingsheng He.

</span>
<span class="ltx_bibblock">The oarf benchmark suite: Characterization and implications for
federated learning systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.07856</em>, 2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2019]</span>
<span class="ltx_bibblock">
Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou.

</span>
<span class="ltx_bibblock">Fdml: A collaborative machine learning framework for distributed
features.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</em>, pages 2232–2240, 2019.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al. [2018]</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and
Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">Communication-efficient on-device machine learning: Federated
distillation and augmentation under non-iid private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.11479</em>, 2018.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2019a]</span>
<span class="ltx_bibblock">
Linshan Jiang, Rui Tan, Xin Lou, and Guosheng Lin.

</span>
<span class="ltx_bibblock">On lightweight privacy-preserving collaborative learning for
internet-of-things objects.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Internet of
Things Design and Implementation</em>, IoTDI ’19, pages 70–81, New York, NY,
USA, 2019a. ACM.

</span>
<span class="ltx_bibblock">ISBN 978-1-4503-6283-2.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3302505.3310070</span>.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://doi.acm.org/10.1145/3302505.3310070</span>.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2019b]</span>
<span class="ltx_bibblock">
Yihan Jiang, Jakub Konečnỳ, Keith Rush, and Sreeram Kannan.

</span>
<span class="ltx_bibblock">Improving federated learning personalization via model agnostic meta
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.12488</em>, 2019b.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Zhang [2013]</span>
<span class="ltx_bibblock">
Rie Johnson and Tong Zhang.

</span>
<span class="ltx_bibblock">Accelerating stochastic gradient descent using predictive variance
reduction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
315–323, 2013.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jouppi et al. [2017]</span>
<span class="ltx_bibblock">
Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al.

</span>
<span class="ltx_bibblock">In-datacenter performance analysis of a tensor processing unit.

</span>
<span class="ltx_bibblock">In <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 44th Annual International Symposium on
Computer Architecture</em>, pages 1–12, 2017.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jurca and Faltings [2003]</span>
<span class="ltx_bibblock">
R. Jurca and B. Faltings.

</span>
<span class="ltx_bibblock">An incentive compatible reputation mechanism.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">EEE International Conference on E-Commerce, 2003. CEC
2003.</em>, pages 285–292, June 2003.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/COEC.2003.1210263</span>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. [2019]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>, 2019.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaissis et al. [2021]</span>
<span class="ltx_bibblock">
Georgios Kaissis, Alexander Ziller, Jonathan Passerat-Palmbach, Théo
Ryffel, Dmitrii Usynin, Andrew Trask, Ionésio Lima, Jason Mancuso,
Friederike Jungmann, Marc-Matthias Steinborn, et al.

</span>
<span class="ltx_bibblock">End-to-end privacy preserving deep learning on multi-institutional
medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, pages 1–12, 2021.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. [2019a]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang.

</span>
<span class="ltx_bibblock">Incentive mechanism for reliable federated learning: A joint
optimization approach to combining reputation and contract theory.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2019a.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. [2019b]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Han Yu, Ying-Chang Liang, and Dong In
Kim.

</span>
<span class="ltx_bibblock">Incentive design for efficient federated learning in mobile networks:
A contract theory approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07479</em>, 2019b.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantarcioglu and Clifton [2004]</span>
<span class="ltx_bibblock">
Murat Kantarcioglu and Chris Clifton.

</span>
<span class="ltx_bibblock">Privacy-preserving distributed mining of association rules on
horizontally partitioned data.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge &amp; Data Engineering</em>, (9):1026–1037, 2004.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. [2020]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
5132–5143. PMLR, 2020.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karr et al. [2009]</span>
<span class="ltx_bibblock">
Alan F Karr, Xiaodong Lin, Ashish P Sanil, and Jerome P Reiter.

</span>
<span class="ltx_bibblock">Privacy-preserving analysis of vertically partitioned data using
secure matrix products.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Journal of Official Statistics</em>, 25(1):125,
2009.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2017]</span>
<span class="ltx_bibblock">
Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
Ye, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">Lightgbm: A highly efficient gradient boosting decision tree.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2017.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2018]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">On-device federated learning via blockchain and its latency analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.03949</em>, 2018.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016a]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter
Richtárik.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016a.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016b]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016b.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2012]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
1097–1105, 2012.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurze et al. [2011]</span>
<span class="ltx_bibblock">
Tobias Kurze, Markus Klems, David Bermbach, Alexander Lenk, Stefan Tai, and
Marcel Kunze.

</span>
<span class="ltx_bibblock">Cloud federation.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Cloud Computing</em>, 2011:32–38, 2011.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al. [2019]</span>
<span class="ltx_bibblock">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
Dureau.

</span>
<span class="ltx_bibblock">Federated learning for keyword spotting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6341–6345. IEEE, 2019.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2016]</span>
<span class="ltx_bibblock">
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik.

</span>
<span class="ltx_bibblock">Data poisoning attacks on factorization-based collaborative
filtering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
1885–1893, 2016.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019a]</span>
<span class="ltx_bibblock">
Liping Li, Wei Xu, Tianyi Chen, Georgios B Giannakis, and Qing Ling.

</span>
<span class="ltx_bibblock">Rsa: Byzantine-robust stochastic aggregation methods for distributed
learning from heterogeneous datasets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2019a.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2015]</span>
<span class="ltx_bibblock">
Peilong Li, Yan Luo, Ning Zhang, and Yu Cao.

</span>
<span class="ltx_bibblock">Heterospark: A heterogeneous cpu/gpu spark platform for machine
learning algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Networking,
Architecture and Storage (NAS)</em>, pages 347–348. IEEE, 2015.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019b]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Adaptive kernel value caching for svm training.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>,
2019b.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020a]</span>
<span class="ltx_bibblock">
Qinbin Li, Bingsheng He, and Dawn Song.

</span>
<span class="ltx_bibblock">Model-agnostic round-optimal federated learning via knowledge
transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01017</em>, 2020a.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020b]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Practical federated gradient boosting decision trees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 4642–4649, 2020b.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020c]</span>
<span class="ltx_bibblock">
Qinbin Li, Zhaomin Wu, Zeyi Wen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Privacy-preserving gradient boosting decision trees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 784–791, 2020c.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021a]</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.02079</em>, 2021a.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021b]</span>
<span class="ltx_bibblock">
Qinbin Li, Bingsheng He, and Dawn Song.

</span>
<span class="ltx_bibblock">Model-contrastive federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021b.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em>, 2018.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019c]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions,
2019c.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019d]</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, and Virginia Smith.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10497</em>, 2019d.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019e]</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.

</span>
<span class="ltx_bibblock">On the convergence of fedavg on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.02189</em>, 2019e.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lianto et al. [2020]</span>
<span class="ltx_bibblock">
Hans Albert Lianto, Yang Zhao, and Jun Zhao.

</span>
<span class="ltx_bibblock">Attacks to federated learning: Responsive web user interface to
recover training data from user gradients.

</span>
<span class="ltx_bibblock">In <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">The ACM Asia Conference on Computer and Communications
Security (ASIACCS)</em>, 2020.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al. [2019]</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey,
2019.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2020]</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.07242</em>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019a]</span>
<span class="ltx_bibblock">
Boyi Liu, Lujia Wang, Ming Liu, and Chengzhong Xu.

</span>
<span class="ltx_bibblock">Lifelong federated reinforcement learning: a learning architecture
for navigation in cloud robotic systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.06455</em>, 2019a.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2017]</span>
<span class="ltx_bibblock">
Jian Liu, Mika Juuti, Yao Lu, and Nadarajah Asokan.

</span>
<span class="ltx_bibblock">Oblivious neural network predictions via minionn transformations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>, pages 619–631. ACM, 2017.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2020]</span>
<span class="ltx_bibblock">
Lifeng Liu, Fengda Zhang, Jun Xiao, and Chao Wu.

</span>
<span class="ltx_bibblock">Evaluation framework for large-scale federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.01575</em>, 2020.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019b]</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief.

</span>
<span class="ltx_bibblock">Edge-assisted hierarchical federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.06641</em>, 2019b.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2018]</span>
<span class="ltx_bibblock">
Yang Liu, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secure federated transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.03337</em>, 2018.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019c]</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi
Hong, and Qiang Yang.

</span>
<span class="ltx_bibblock">A communication efficient vertical federated learning framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.11187</em>, 2019c.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019d]</span>
<span class="ltx_bibblock">
Yang Liu, Yingting Liu, Zhijie Liu, Junbo Zhang, Chuishi Meng, and Yu Zheng.

</span>
<span class="ltx_bibblock">Federated forest.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10053</em>, 2019d.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019e]</span>
<span class="ltx_bibblock">
Yang Liu, Zhuo Ma, Ximeng Liu, Siqi Ma, Surya Nepal, and Robert Deng.

</span>
<span class="ltx_bibblock">Boosting privately: Privacy-preserving federated extreme boosting for
mobile crowdsensing.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.10218</em>, 2019e.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopes and Ribeiro [2011]</span>
<span class="ltx_bibblock">
Noel Lopes and Bernardete Ribeiro.

</span>
<span class="ltx_bibblock">Gpumlib: An efficient open-source gpu machine learning library.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Information Systems and
Industrial Management Applications</em>, 3:355–362, 2011.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2019]</span>
<span class="ltx_bibblock">
Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, and
Qiang Yang.

</span>
<span class="ltx_bibblock">Real-world image datasets for federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.11089</em>, 2019.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2020]</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Threats to federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>, 2020.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2017]</span>
<span class="ltx_bibblock">
Chenxin Ma, Jakub Konečnỳ, Martin Jaggi, Virginia Smith, Michael I
Jordan, Peter Richtárik, and Martin Takáč.

</span>
<span class="ltx_bibblock">Distributed optimization with arbitrary local solvers.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">optimization Methods and Software</em>, 32(4):813–848, 2017.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahajan et al. [2018]</span>
<span class="ltx_bibblock">
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten.

</span>
<span class="ltx_bibblock">Exploring the limits of weakly supervised pretraining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>, pages 181–196, 2018.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marfoq et al. [2020]</span>
<span class="ltx_bibblock">
Othmane Marfoq, Chuan Xu, Giovanni Neglia, and Richard Vidal.

</span>
<span class="ltx_bibblock">Throughput-optimal topology design for cross-silo federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.12229</em>, 2020.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2016]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>, 2016.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>, 2017.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al. [2019]</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</em>, pages
691–706. IEEE, 2019.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mhamdi et al. [2018]</span>
<span class="ltx_bibblock">
El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault.

</span>
<span class="ltx_bibblock">The hidden vulnerability of distributed learning in byzantium.

</span>
<span class="ltx_bibblock"><em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.07927</em>, 2018.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mnih et al. [2015]</span>
<span class="ltx_bibblock">
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al.

</span>
<span class="ltx_bibblock">Human-level control through deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 518(7540):529–533, 2015.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al. [2019]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.00146</em>, 2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al. [2017]</span>
<span class="ltx_bibblock">
M. Mukherjee, R. Matam, L. Shu, L. Maglaras, M. A. Ferrag,
N. Choudhury, and V. Kumar.

</span>
<span class="ltx_bibblock">Security and privacy in fog computing: Challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 5:19293–19304, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ACCESS.2017.2749422</span>.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naor et al. [1999]</span>
<span class="ltx_bibblock">
Moni Naor, Benny Pinkas, and Reuban Sumner.

</span>
<span class="ltx_bibblock">Privacy preserving auctions and mechanism design.

</span>
<span class="ltx_bibblock">In <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st ACM Conference on Electronic
Commerce</em>, EC ’99, pages 129–139, New York, NY, USA, 1999. ACM.

</span>
<span class="ltx_bibblock">ISBN 1-58113-176-3.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/336992.337028</span>.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://doi.acm.org/10.1145/336992.337028</span>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasr et al. [2019]</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri, and Amir Houmansadr.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Comprehensive Privacy Analysis of Deep Learning: Passive and
Active White-box Inference Attacks against Centralized and Federated
Learning</em>, page 0. IEEE, 2019.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2018]</span>
<span class="ltx_bibblock">
Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni,
N. Asokan, and Ahmad-Reza Sadeghi.

</span>
<span class="ltx_bibblock">DÏot: A federated self-learning anomaly detection system for iot,
2018.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol and Schulman [2018]</span>
<span class="ltx_bibblock">
Alex Nichol and John Schulman.

</span>
<span class="ltx_bibblock">Reptile: a scalable metalearning algorithm.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.02999</em>, 2:2, 2018.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niknam et al. [2019]</span>
<span class="ltx_bibblock">
Solmaz Niknam, Harpreet S Dhillon, and Jeffery H Reed.

</span>
<span class="ltx_bibblock">Federated learning for wireless communications: Motivation,
opportunities and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.06847</em>, 2019.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolaenko et al. [2013]</span>
<span class="ltx_bibblock">
Valeria Nikolaenko, Udi Weinsberg, Stratis Ioannidis, Marc Joye, Dan Boneh, and
Nina Taft.

</span>
<span class="ltx_bibblock">Privacy-preserving ridge regression on hundreds of millions of
records.

</span>
<span class="ltx_bibblock">In <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">2013 IEEE Symposium on Security and Privacy</em>, pages
334–348. IEEE, 2013.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilsson et al. [2018]</span>
<span class="ltx_bibblock">
Adrian Nilsson, Simon Smith, Gregor Ulm, Emil Gustavsson, and Mats Jirstrand.

</span>
<span class="ltx_bibblock">A performance evaluation of federated learning algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Workshop on Distributed
Infrastructures for Deep Learning</em>, pages 1–8. ACM, 2018.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio and Yonetani [2019]</span>
<span class="ltx_bibblock">
Takayuki Nishio and Ryo Yonetani.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources
in mobile edge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">ICC 2019-2019 IEEE International Conference on
Communications (ICC)</em>, pages 1–7. IEEE, 2019.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nock et al. [2018]</span>
<span class="ltx_bibblock">
Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Entity resolution and federated learning get a federated resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.04035</em>, 2018.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohrimenko et al. [2016]</span>
<span class="ltx_bibblock">
Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Sebastian
Nowozin, Kapil Vaswani, and Manuel Costa.

</span>
<span class="ltx_bibblock">Oblivious multi-party machine learning on trusted processors.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.4.4" class="ltx_emph ltx_font_italic">25th <math id="bib.bib145.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib145.1.1.m1.1a"><mo stretchy="false" id="bib.bib145.1.1.m1.1.1" xref="bib.bib145.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib145.1.1.m1.1b"><ci id="bib.bib145.1.1.m1.1.1.cmml" xref="bib.bib145.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib145.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib145.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib145.2.2.m2.1a"><mo stretchy="false" id="bib.bib145.2.2.m2.1.1" xref="bib.bib145.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib145.2.2.m2.1b"><ci id="bib.bib145.2.2.m2.1.1.cmml" xref="bib.bib145.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib145.2.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib145.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib145.3.3.m3.1a"><mo stretchy="false" id="bib.bib145.3.3.m3.1.1" xref="bib.bib145.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib145.3.3.m3.1b"><ci id="bib.bib145.3.3.m3.1.1.cmml" xref="bib.bib145.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib145.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib145.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib145.4.4.m4.1a"><mo stretchy="false" id="bib.bib145.4.4.m4.1.1" xref="bib.bib145.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib145.4.4.m4.1b"><ci id="bib.bib145.4.4.m4.1.1.cmml" xref="bib.bib145.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib145.4.4.m4.1c">\}</annotation></semantics></math>
Security 16)</em>, pages 619–636, 2016.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paillier [1999]</span>
<span class="ltx_bibblock">
Pascal Paillier.

</span>
<span class="ltx_bibblock">Public-key cryptosystems based on composite degree residuosity
classes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">International Conference on the Theory and Applications of
Cryptographic Techniques</em>, pages 223–238. Springer, 1999.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan and Yang [2010]</span>
<span class="ltx_bibblock">
Sinno Jialin Pan and Qiang Yang.

</span>
<span class="ltx_bibblock">A survey on transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on knowledge and data engineering</em>,
22(10):1345–1359, 2010.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. [2017]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.

</span>
<span class="ltx_bibblock">Automatic differentiation in pytorch.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
8024–8035, 2019.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polikar [2012]</span>
<span class="ltx_bibblock">
Robi Polikar.

</span>
<span class="ltx_bibblock">Ensemble learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">Ensemble machine learning</em>. Springer, 2012.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polyzotis et al. [2018]</span>
<span class="ltx_bibblock">
Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich.

</span>
<span class="ltx_bibblock">Data lifecycle challenges in production machine learning: a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">ACM SIGMOD Record</em>, 47(2):17–28, 2018.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qayyum et al. [2021]</span>
<span class="ltx_bibblock">
Adnan Qayyum, Kashif Ahmad, Muhammad Ahtazaz Ahsan, Ala Al-Fuqaha, and Junaid
Qadir.

</span>
<span class="ltx_bibblock">Collaborative federated learning for healthcare: Multi-modal covid-19
diagnosis at the edge, 2021.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. [2019]</span>
<span class="ltx_bibblock">
Yongfeng Qian, Long Hu, Jing Chen, Xin Guan, Mohammad Mehedi Hassan, and
Abdulhameed Alelaiwi.

</span>
<span class="ltx_bibblock">Privacy-aware service placement for mobile edge computing via
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">Information Sciences</em>, 505:562–570, 2019.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. [2020]</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.00295</em>, 2020.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh et al. [2020]</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie.

</span>
<span class="ltx_bibblock">Robust federated learning: The case of affine distribution shifts.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.08907</em>, 2020.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riazi et al. [2018]</span>
<span class="ltx_bibblock">
M Sadegh Riazi, Christian Weinert, Oleksandr Tkachenko, Ebrahim M Songhori,
Thomas Schneider, and Farinaz Koushanfar.

</span>
<span class="ltx_bibblock">Chameleon: A hybrid secure computation framework for machine learning
applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 on Asia Conference on Computer and
Communications Security</em>, pages 707–721. ACM, 2018.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder [2017]</span>
<span class="ltx_bibblock">
Sebastian Ruder.

</span>
<span class="ltx_bibblock">An overview of multi-task learning in deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05098</em>, 2017.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. [2018]</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>, 2018.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabt et al. [2015]</span>
<span class="ltx_bibblock">
Mohamed Sabt, Mohammed Achemlal, and Abdelmadjid Bouabdallah.

</span>
<span class="ltx_bibblock">Trusted execution environment: what it is, and what it is not.

</span>
<span class="ltx_bibblock">In <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Trustcom/BigDataSE/ISPA</em>, volume 1, pages 57–64.
IEEE, 2015.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samarakoon et al. [2018]</span>
<span class="ltx_bibblock">
Sumudu Samarakoon, Mehdi Bennis, Walid Saad, and Merouane Debbah.

</span>
<span class="ltx_bibblock">Federated learning for ultra-reliable low-latency v2v communications,
2018.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samek et al. [2017]</span>
<span class="ltx_bibblock">
Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller.

</span>
<span class="ltx_bibblock">Explainable artificial intelligence: Understanding, visualizing and
interpreting deep learning models.

</span>
<span class="ltx_bibblock"><em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1708.08296</em>, 2017.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanil et al. [2004]</span>
<span class="ltx_bibblock">
Ashish P Sanil, Alan F Karr, Xiaodong Lin, and Jerome P Reiter.

</span>
<span class="ltx_bibblock">Privacy preserving regression modelling via distributed computation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">Proceedings of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining</em>, pages 677–682. ACM, 2004.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarikaya and Ercetin [2019]</span>
<span class="ltx_bibblock">
Yunus Sarikaya and Ozgur Ercetin.

</span>
<span class="ltx_bibblock">Motivating workers in federated learning: A stackelberg game
perspective, 2019.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al. [2019]</span>
<span class="ltx_bibblock">
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek.

</span>
<span class="ltx_bibblock">Robust and communication-efficient federated learning from non-iid
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.02891</em>, 2019.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shamir [1979]</span>
<span class="ltx_bibblock">
Adi Shamir.

</span>
<span class="ltx_bibblock">How to share a secret.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 22(11):612–613, 1979.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheth and Larson [1990]</span>
<span class="ltx_bibblock">
Amit P Sheth and James A Larson.

</span>
<span class="ltx_bibblock">Federated database systems for managing distributed, heterogeneous,
and autonomous databases.

</span>
<span class="ltx_bibblock"><em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 22(3):183–236, 1990.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri et al. [2017]</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</em>, pages
3–18. IEEE, 2017.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skvorc et al. [2014]</span>
<span class="ltx_bibblock">
Dejan Skvorc, Matija Horvat, and Sinisa Srbljic.

</span>
<span class="ltx_bibblock">Performance evaluation of websocket protocol for implementation of
full-duplex web streams.

</span>
<span class="ltx_bibblock">In <em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">2014 37th International Convention on Information and
Communication Technology, Electronics and Microelectronics (MIPRO)</em>, pages
1003–1008. IEEE, 2014.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. [2017]</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar.

</span>
<span class="ltx_bibblock">Federated multi-task learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
4424–4434, 2017.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2013]</span>
<span class="ltx_bibblock">
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate.

</span>
<span class="ltx_bibblock">Stochastic gradient descent with differentially private updates.

</span>
<span class="ltx_bibblock">In <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">2013 IEEE Global Conference on Signal and Information
Processing</em>, pages 245–248. IEEE, 2013.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sprague et al. [2018]</span>
<span class="ltx_bibblock">
Michael R Sprague, Amir Jalalirad, Marco Scavuzzo, Catalin Capota, Moritz Neun,
Lyman Do, and Michael Kopp.

</span>
<span class="ltx_bibblock">Asynchronous federated learning for geospatial applications.

</span>
<span class="ltx_bibblock">In <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and Knowledge
Discovery in Databases</em>, pages 21–28. Springer, 2018.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stojmenovic et al. [2016]</span>
<span class="ltx_bibblock">
Ivan Stojmenovic, Sheng Wen, Xinyi Huang, and Hao Luan.

</span>
<span class="ltx_bibblock">An overview of fog computing and its security issues.

</span>
<span class="ltx_bibblock"><em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">Concurr. Comput. : Pract. Exper.</em>, 28(10):2991–3005, July 2016.

</span>
<span class="ltx_bibblock">ISSN 1532-0626.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1002/cpe.3485</span>.

</span>
<span class="ltx_bibblock">URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1002/cpe.3485</span>.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su and Xu [2018]</span>
<span class="ltx_bibblock">
Lili Su and Jiaming Xu.

</span>
<span class="ltx_bibblock">Securing distributed machine learning in high dimensions.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.10140</em>, 2018.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2019]</span>
<span class="ltx_bibblock">
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>, 2019.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundermeyer et al. [2012]</span>
<span class="ltx_bibblock">
Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.

</span>
<span class="ltx_bibblock">Lstm neural networks for language modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">Thirteenth annual conference of the international speech
communication association</em>, 2012.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swan [2015]</span>
<span class="ltx_bibblock">
Melanie Swan.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">Blockchain: Blueprint for a new economy</em>.

</span>
<span class="ltx_bibblock">” O’Reilly Media, Inc.”, 2015.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2020]</span>
<span class="ltx_bibblock">
Ben Tan, Bo Liu, Vincent Zheng, and Qiang Yang.

</span>
<span class="ltx_bibblock">A federated recommender system for online services.

</span>
<span class="ltx_bibblock">In <em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">Fourteenth ACM Conference on Recommender Systems</em>, pages
579–581, 2020.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le [2019]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc V Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.11946</em>, 2019.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. [2017]</span>
<span class="ltx_bibblock">
ADP Team et al.

</span>
<span class="ltx_bibblock">Learning with privacy at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">Apple Machine Learning Journal</em>, 1(8), 2017.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakkar et al. [2019]</span>
<span class="ltx_bibblock">
Om Thakkar, Galen Andrew, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Differentially private learning with adaptive clipping.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.03871</em>, 2019.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al. [2019]</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui
Zhang, and Yi Zhou.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop on Artificial
Intelligence and Security</em>, pages 1–11. ACM, 2019.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vartak et al. [2016]</span>
<span class="ltx_bibblock">
Manasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah
Husnoo, Samuel Madden, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Modeldb: a system for machine learning model management.

</span>
<span class="ltx_bibblock">In <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Human-In-the-Loop Data
Analytics</em>, pages 1–3, 2016.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vatsalan et al. [2017]</span>
<span class="ltx_bibblock">
Dinusha Vatsalan, Ziad Sehili, Peter Christen, and Erhard Rahm.

</span>
<span class="ltx_bibblock">Privacy-preserving record linkage for big data: Current approaches
and research challenges.

</span>
<span class="ltx_bibblock">In <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Handbook of Big Data Technologies</em>, pages 851–895.
Springer, 2017.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vepakomma et al. [2018]</span>
<span class="ltx_bibblock">
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Split learning for health: Distributed deep learning without sharing
raw patient data.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00564</em>, 2018.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voigt and Von dem Bussche [2017]</span>
<span class="ltx_bibblock">
Paul Voigt and Axel Von dem Bussche.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</em>, 2017.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner and Eckhoff [2018]</span>
<span class="ltx_bibblock">
Isabel Wagner and David Eckhoff.

</span>
<span class="ltx_bibblock">Technical privacy metrics: a systematic survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 51(3):57,
2018.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019a]</span>
<span class="ltx_bibblock">
Guan Wang, Charlie Xiaoqian Dang, and Ziye Zhou.

</span>
<span class="ltx_bibblock">Measure contribution of participants in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data (Big Data)</em>,
pages 2597–2604. IEEE, 2019a.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020a]</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33,
2020a.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020b]</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.06440</em>, 2020b.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020c]</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous
federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
2020c.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021]</span>
<span class="ltx_bibblock">
Rui Wang, Heju Li, and Erwu Liu.

</span>
<span class="ltx_bibblock">Blockchain-based federated learning in mobile edge networks with
application in internet of vehicles.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.01116</em>, 2021.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019b]</span>
<span class="ltx_bibblock">
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian
Makaya, Ting He, and Kevin Chan.

</span>
<span class="ltx_bibblock">Adaptive federated learning in resource constrained edge computing
systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, 37(6):1205–1221, 2019b.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020d]</span>
<span class="ltx_bibblock">
Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song.

</span>
<span class="ltx_bibblock">A principled approach to data valuation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">Federated Learning</em>, pages 153–167. Springer,
2020d.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019c]</span>
<span class="ltx_bibblock">
Xiaofei Wang, Yiwen Han, Chenyang Wang, Qiyang Zhao, Xu Chen, and Min Chen.

</span>
<span class="ltx_bibblock">In-edge ai: Intelligentizing mobile edge computing, caching and
communication by federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, 2019c.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang [2017]</span>
<span class="ltx_bibblock">
Yushi Wang.

</span>
<span class="ltx_bibblock">Co-op: Cooperative machine learning from mobile devices.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019d]</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2019-IEEE Conference on Computer
Communications</em>, pages 2512–2520. IEEE, 2019d.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2018a]</span>
<span class="ltx_bibblock">
Zeyi Wen, Bingsheng He, Ramamohanarao Kotagiri, Shengliang Lu, and Jiashuai
Shi.

</span>
<span class="ltx_bibblock">Efficient gradient boosted decision tree training on gpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Parallel and Distributed Processing
Symposium (IPDPS)</em>, pages 234–243. IEEE, 2018a.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2018b]</span>
<span class="ltx_bibblock">
Zeyi Wen, Jiashuai Shi, Qinbin Li, Bingsheng He, and Jian Chen.

</span>
<span class="ltx_bibblock">ThunderSVM: A fast SVM library on GPUs and CPUs.

</span>
<span class="ltx_bibblock"><em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 19:797–801,
2018b.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2019a]</span>
<span class="ltx_bibblock">
Zeyi Wen, Jiashuai Shi, Bingsheng He, Jian Chen, Kotagiri Ramamohanarao, and
Qinbin Li.

</span>
<span class="ltx_bibblock">Exploiting gpus for efficient gradient boosting decision tree
training.

</span>
<span class="ltx_bibblock"><em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>,
2019a.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2019b]</span>
<span class="ltx_bibblock">
Zeyi Wen, Jiashuai Shi, Qinbin Li, Bingsheng He, and Jian Chen.

</span>
<span class="ltx_bibblock">Thundergbm: Fast gbdts and random forests on gpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">https://github.com/Xtra-Computing/thundergbm</em>,
2019b.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng et al. [2019]</span>
<span class="ltx_bibblock">
Jiasi Weng, Jian Weng, Jilian Zhang, Ming Li, Yue Zhang, and Weiqi Luo.

</span>
<span class="ltx_bibblock">Deepchain: Auditable and privacy-preserving deep learning with
blockchain-based incentive.

</span>
<span class="ltx_bibblock"><em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Dependable and Secure Computing</em>, 2019.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2010]</span>
<span class="ltx_bibblock">
Xiaokui Xiao, Guozhang Wang, and Johannes Gehrke.

</span>
<span class="ltx_bibblock">Differential privacy via wavelet transforms.

</span>
<span class="ltx_bibblock"><em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on knowledge and data engineering</em>,
23(8):1200–1214, 2010.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2019a]</span>
<span class="ltx_bibblock">
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li.

</span>
<span class="ltx_bibblock">Dba: Distributed backdoor attacks against federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>,
2019a.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2019b]</span>
<span class="ltx_bibblock">
Cong Xie, Sanmi Koyejo, and Indranil Gupta.

</span>
<span class="ltx_bibblock">Asynchronous federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.03934</em>, 2019b.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2019]</span>
<span class="ltx_bibblock">
Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, and Heiko Ludwig.

</span>
<span class="ltx_bibblock">Hybridalpha: An efficient approach for privacy-preserving federated
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop on Artificial
Intelligence and Security</em>, pages 13–23, 2019.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2016]</span>
<span class="ltx_bibblock">
Zhuang Yan, Li Guoliang, and Feng Jianhua.

</span>
<span class="ltx_bibblock">A survey on entity alignment of knowledge base.

</span>
<span class="ltx_bibblock"><em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">Journal of Computer Research and Development</em>, 1:165–192, 2016.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):12, 2019.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2018]</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>, 2018.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. [2015]</span>
<span class="ltx_bibblock">
Shanhe Yi, Zhengrui Qin, and Qun Li.

</span>
<span class="ltx_bibblock">Security and privacy issues of fog computing: A survey.

</span>
<span class="ltx_bibblock">In <em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">WASA</em>, 2015.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshida et al. [2019]</span>
<span class="ltx_bibblock">
Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto, and Ryo
Yonetani.

</span>
<span class="ltx_bibblock">Hybrid-fl: Cooperative learning mechanism using non-iid data in
wireless networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07210</em>, 2019.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2006]</span>
<span class="ltx_bibblock">
Hwanjo Yu, Xiaoqian Jiang, and Jaideep Vaidya.

</span>
<span class="ltx_bibblock">Privacy-preserving svm using nonlinear kernels on horizontally
partitioned data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2006 ACM symposium on Applied computing</em>,
pages 603–610. ACM, 2006.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2018]</span>
<span class="ltx_bibblock">
Zhengxin Yu, Jia Hu, Geyong Min, Haochuan Lu, Zhiwei Zhao, Haozhe Wang, and
Nektarios Georgalas.

</span>
<span class="ltx_bibblock">Federated learning based proactive content caching in edge computing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Global Communications Conference (GLOBECOM)</em>,
pages 1–6. IEEE, 2018.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yurochkin et al. [2019]</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald,
Trong Nghia Hoang, and Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Bayesian nonparametric federated learning of neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib213.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.12022</em>, 2019.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020a]</span>
<span class="ltx_bibblock">
Weishan Zhang, Qinghua Lu, Qiuyu Yu, Zhaotong Li, Yue Liu, Sin Kit Lo, Shiping
Chen, Xiwei Xu, and Liming Zhu.

</span>
<span class="ltx_bibblock">Blockchain-based federated learning for device failure detection in
industrial iot.

</span>
<span class="ltx_bibblock"><em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2020a.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Yang [2017]</span>
<span class="ltx_bibblock">
Yu Zhang and Qiang Yang.

</span>
<span class="ltx_bibblock">A survey on multi-task learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib215.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.08114</em>, 2017.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020b]</span>
<span class="ltx_bibblock">
Zhengming Zhang, Zhewei Yao, Yaoqing Yang, Yujun Yan, Joseph E Gonzalez, and
Michael W Mahoney.

</span>
<span class="ltx_bibblock">Benchmarking semi-supervised federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.11364</em>, 2020b.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2018a]</span>
<span class="ltx_bibblock">
Lingchen Zhao, Lihao Ni, Shengshan Hu, Yaniiao Chen, Pan Zhou, Fu Xiao, and
Libing Wu.

</span>
<span class="ltx_bibblock">Inprivate digging: Enabling tree-based distributed data mining with
differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib217.1.1" class="ltx_emph ltx_font_italic">INFOCOM</em>, pages 2087–2095. IEEE, 2018a.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2019]</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, and Dusit Niyato.

</span>
<span class="ltx_bibblock">Mobile edge computing, blockchain and reputation-based crowdsourcing
iot federated learning: A secure, decentralized and privacy-preserving
system.

</span>
<span class="ltx_bibblock"><em id="bib.bib218.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.10893</em>, 2019.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020a]</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, Dusit Niyato, Zengxiang Li,
Lingjuan Lyu, and Yingbo Liu.

</span>
<span class="ltx_bibblock">Privacy-preserving blockchain-based federated learning for iot
devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib219.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2020a.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020b]</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao, Mengmeng Yang, Teng Wang, Ning Wang, Lingjuan Lyu, Dusit
Niyato, and Kwok Yan Lam.

</span>
<span class="ltx_bibblock">Local differential privacy based federated learning for internet of
things.

</span>
<span class="ltx_bibblock"><em id="bib.bib220.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.08856</em>, 2020b.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2018b]</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib221.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>, 2018b.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2020]</span>
<span class="ltx_bibblock">
Wenbo Zheng, Lan Yan, Chao Gou, and Fei-Yue Wang.

</span>
<span class="ltx_bibblock">Federated meta-learning for fraudulent credit card detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib222.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint
Conference on Artificial Intelligence (IJCAI-20)</em>, 2020.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2018]</span>
<span class="ltx_bibblock">
Zibin Zheng, Shaoan Xie, Hong-Ning Dai, Xiangping Chen, and Huaimin Wang.

</span>
<span class="ltx_bibblock">Blockchain challenges and opportunities: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib223.1.1" class="ltx_emph ltx_font_italic">International Journal of Web and Grid Services</em>, 14(4):352–375, 2018.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2019a]</span>
<span class="ltx_bibblock">
Amelie Chi Zhou, Yao Xiao, Bingsheng He, Jidong Zhai, Rui Mao, et al.

</span>
<span class="ltx_bibblock">Privacy regulation aware process mapping in geo-distributed cloud
data centers.

</span>
<span class="ltx_bibblock"><em id="bib.bib224.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>,
2019a.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2019b]</span>
<span class="ltx_bibblock">
Pan Zhou, Kehao Wang, Linke Guo, Shimin Gong, and Bolong Zheng.

</span>
<span class="ltx_bibblock">A privacy-preserving distributed contextual federated online learning
framework with big data support in social recommender systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib225.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>,
2019b.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Jin [2019]</span>
<span class="ltx_bibblock">
Hangyu Zhu and Yaochu Jin.

</span>
<span class="ltx_bibblock">Multi-objective evolutionary federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib226.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>,
2019.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. [2020]</span>
<span class="ltx_bibblock">
Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin, Dongzhan
Zhou, Shuai Zhang, and Shuai Yi.

</span>
<span class="ltx_bibblock">Performance optimization of federated person re-identification via
benchmark analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib227.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</em>, pages 955–963, 2020.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zyskind et al. [2015]</span>
<span class="ltx_bibblock">
G. Zyskind, O. Nathan, and A. ’. Pentland.

</span>
<span class="ltx_bibblock">Decentralizing privacy: Using blockchain to protect personal data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib228.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Security and Privacy Workshops</em>, pages 180–184,
May 2015.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/SPW.2015.27</span>.

</span>
</li>
</ul>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1907.09692" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1907.09693" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1907.09693">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1907.09693" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1907.09694" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 11:46:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
