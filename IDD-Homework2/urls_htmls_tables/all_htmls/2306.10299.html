<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.10299] Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning</title><meta property="og:description" content="Federated edge learning is envisioned as the bedrock of enabling intelligence in next-generation wireless networks, but the limited spectral resources often constrain its scalability.
In light of this challenge, a line…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.10299">

<!--Generated on Thu Feb 29 00:34:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Edge intelligence,  federated learning,  analog over-the-air computation,  interference.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zihan Chen, <span id="id3.1.id1" class="ltx_text ltx_font_italic">Member, IEEE</span>,
Howard H. Yang, <span id="id4.2.id2" class="ltx_text ltx_font_italic">Member, IEEE</span>,
and Tony Q. S. Quek, <span id="id5.3.id3" class="ltx_text ltx_font_italic">Fellow, IEEE</span>
</span><span class="ltx_author_notes">This paper is supported in part by the National Research Foundation, Singapore and Infocomm Media Development Authority under its Future Communications Research &amp; Development Programme, in part by the National Natural Science Foundation of China under Grant 62271513, in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LGJ22F010001, and in part by the Zhejiang - Singapore Innovation and AI Joint Research Lab. (<span id="id6.4.id1" class="ltx_text ltx_font_italic">Corresponding authors: T. Q. S. Quek, H. H. Yang</span>)Z. Chen and T. Q. S. Quek are with the Singapore University of Technology and Design, Singapore 487372. T. Q. S. Quek is also with the Department of Electronic Engineering, Kyung Hee University, Yongin 17104, South Korea (e-mail: {zihan_chen, tonyquek}@sutd.edu.sg).H. H. Yang is with the Zhejiang University/University of Illinois Urbana-Champaign Institute, Zhejiang University, Haining 314400, China, the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310007, China, and the Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL 61820, USA (email: haoyang@intl.zju.edu.cn).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Federated edge learning is envisioned as the bedrock of enabling intelligence in next-generation wireless networks, but the limited spectral resources often constrain its scalability.
In light of this challenge, a line of recent research suggested integrating analog over-the-air computations into federated edge learning systems, to exploit the superposition property of electromagnetic waves for fast aggregation of intermediate parameters and achieve (almost) unlimited scalability.
Over-the-air computations also benefit the system in other aspects, such as low hardware cost, reduced access latency, and enhanced privacy protection.
Despite these advantages, the interference introduced by wireless communications also influences various aspects of the model training process, while its importance is not well recognized yet. This article provides a comprehensive overview of the positive and negative effects of interference on over-the-air computation-based edge learning systems. The potential open issues and research trends are also discussed.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Edge intelligence, federated learning, analog over-the-air computation, interference.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid development of Internet-of-Things (IoT), artificial intelligence (AI), and edge computing is forging ahead a variety of emerging applications, ranging from autonomous driving and smart healthcare to intelligent network management, that significantly improve the quality of life of humans.
These applications rely on advanced machine learning (ML) algorithms that train statistical models based on the sheer volumes of data collected from various edge nodes, enabling devices to make decisions in accordance with local events.
However, traditional ML approaches require users to gather their data in a central server for model training.
This reveals oppressive privacy and security consequences in which end-users’ sensitive information may be shared with and/or exploited by other entities, hence intensifying the demand for new solutions.
With the advances in communication techniques and the improvements in the computation capabilities of edge devices, distributed learning, especially federated learning (FL), provides a promising solution for widely dispersed edge devices to collaboratively train a global ML model without sharing the private data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Recently, FL over wireless network, which involves deploying ML models at the network edge, i.e., <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">federated edge learning</span>, has gained significant attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A typical federated edge learning system consists of an edge server and multiple user equipments (UEs), in which a global model is shared amongst the entities.
In each iteration, a portion of UEs is selected to conduct on-device model training based on their private dataset.
Then, the server collates the local gradients to improve the global model and broadcasts the updated result back to a new subset of selected UEs for the next round of local computations.
Such interactions between the edge server and UEs would repeat until the model converges.
Generally, it takes hundreds, even thousands, of communication rounds to reach convergence.
In view of the hefty communication overhead incurred by the iterative exchange of intermediate parameters, which becomes particularly cumbersome in the digital communication-based federated edge learning system where every UE needs an orthogonal sub-channel to upload its local update, a new design that adopts <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">analog over-the-air computations</span> for model aggregation is proposed to address such a communication bottleneck <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The key idea of analog over-the-air computations is that UEs modulate their local gradients onto a set of common waveforms and simultaneously send out the analog signals to the edge server, exploiting the superposition property of multi-access channels to realize fast aggregation of the gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Incorporating this technique into the federated edge learning system has the following benefits:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">High spectral efficiency:</span> Compared with the conventional digital communication-based FL that needs to select a portion of the UEs for parameter updating in each communication round, analog over-the-air computing dramatically boosts up the spectral utilization by allowing all the UEs to simultaneously access the spectrum and upload their parameters to the edge server, irrespective of the number of UEs present in the system. As a result, the bandwidth no longer constrains the number of participating UEs in the system, expediting the large-scale deployment of edge learning systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Low communication cost:</span> Analog over-the-air computations can be achieved by elementary communication techniques such as amplitude modulation. Additionally, the UEs are also disburdened with estimating the instantaneous channel state information before each global transmission. Moreover, as opposed to digital communication-based FL, an increase in the number of UEs in the network can improve the system’s energy efficiency, thereby enabling the UEs to reduce their transmit power <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. These salient features make it possible to establish an intelligent edge system over massively distributed UEs with low-cost communication modules for cognitive network management.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Improved privacy protection:</span>
The channel fading and interference noise induced by analog over-the-air computation impose a random perturbation to each UE’s uploaded parameter. As a result, even if there are eavesdroppers in the network, no original information can be recovered from the aggregated gradients. Such an enhancement to privacy protection in network management is particularly relevant in privacy-aware edge learning systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Reduced access latency:</span> By virtue of analog over-the-air computing, the local updates would be automatically aggregated at the edge server’s output. This inherent integration of communication and computation significantly reduces the access, as well as processing latency, since the system no longer needs to go through the encoding (resp. decoding) and modulation (resp. demodulation) processes to obtain the individual gradients of each UE before adding them up <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.10299/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of the analog over-the-air federated edge learning system. Local gradients are uploaded using amplitude modulations. Aggregation is then performed automatically in the air, where a heavy-tailed noise with tail index <math id="S1.F1.2.m1.1" class="ltx_Math" alttext="\alpha=1.5" display="inline"><semantics id="S1.F1.2.m1.1b"><mrow id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml"><mi id="S1.F1.2.m1.1.1.2" xref="S1.F1.2.m1.1.1.2.cmml">α</mi><mo id="S1.F1.2.m1.1.1.1" xref="S1.F1.2.m1.1.1.1.cmml">=</mo><mn id="S1.F1.2.m1.1.1.3" xref="S1.F1.2.m1.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><apply id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1"><eq id="S1.F1.2.m1.1.1.1.cmml" xref="S1.F1.2.m1.1.1.1"></eq><ci id="S1.F1.2.m1.1.1.2.cmml" xref="S1.F1.2.m1.1.1.2">𝛼</ci><cn type="float" id="S1.F1.2.m1.1.1.3.cmml" xref="S1.F1.2.m1.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\alpha=1.5</annotation></semantics></math> is added to the aggregated global gradient signal. Steps of the framework in a typical communication round are numbered accordingly.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the above advantages, the uncoded analog transmissions also expose UEs’ gradients to channel fading and interference noise, which can afflict the received signal quality, leading to a degradation in the system performance. That said, in the context of ML, noise does not merely have a negative effect.
Indeed, through appropriate signal processing techniques, we can turn the interference noise from a setback into an asset for federated edge learning systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
This two-sided effect of interference results in several important operational changes in analog over-the-air-based FL, which is not widely known.
The central goal of the present article is, therefore, to summarize the key research findings related to the interference in analog over-the-air computing and their implications in the design of federated edge learning systems, and more importantly, this article seeks to provide insights for the new design of the analog over-the-air federated edge learning system via taming and repurposing the channel interference.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Analog Over-the-Air Machine Learning</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section gives a brief overview of FL under analog over-the-air computations, also coined as analog over-the-air machine/edge learning.
Note that the main impact of analog over-the-air computing is on alleviating burdens in the uplink transmission, i.e., from UEs to the edge server, which is the well-known “communication bottleneck” of the system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">General Framework</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">The training process of the analog over-the-air edge learning is detailed as follows (see Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for an overview). Let us consider an edge learning system in which <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">N</annotation></semantics></math> UEs collaboratively train a global ML model under the coordination of an edge server.
To be more concrete, let <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="f_{i}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.2" xref="S2.SS1.p1.2.m2.1.2.cmml"><mrow id="S2.SS1.p1.2.m2.1.2.2" xref="S2.SS1.p1.2.m2.1.2.2.cmml"><msub id="S2.SS1.p1.2.m2.1.2.2.2" xref="S2.SS1.p1.2.m2.1.2.2.2.cmml"><mi id="S2.SS1.p1.2.m2.1.2.2.2.2" xref="S2.SS1.p1.2.m2.1.2.2.2.2.cmml">f</mi><mi id="S2.SS1.p1.2.m2.1.2.2.2.3" xref="S2.SS1.p1.2.m2.1.2.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.2.2.1" xref="S2.SS1.p1.2.m2.1.2.2.1.cmml">​</mo><mrow id="S2.SS1.p1.2.m2.1.2.2.3.2" xref="S2.SS1.p1.2.m2.1.2.2.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.1.2.2.3.2.1" xref="S2.SS1.p1.2.m2.1.2.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">⋅</mo><mo rspace="0.278em" stretchy="false" id="S2.SS1.p1.2.m2.1.2.2.3.2.2" xref="S2.SS1.p1.2.m2.1.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S2.SS1.p1.2.m2.1.2.1" xref="S2.SS1.p1.2.m2.1.2.1.cmml">:</mo><mrow id="S2.SS1.p1.2.m2.1.2.3" xref="S2.SS1.p1.2.m2.1.2.3.cmml"><msup id="S2.SS1.p1.2.m2.1.2.3.2" xref="S2.SS1.p1.2.m2.1.2.3.2.cmml"><mi id="S2.SS1.p1.2.m2.1.2.3.2.2" xref="S2.SS1.p1.2.m2.1.2.3.2.2.cmml">ℝ</mi><mi id="S2.SS1.p1.2.m2.1.2.3.2.3" xref="S2.SS1.p1.2.m2.1.2.3.2.3.cmml">d</mi></msup><mo stretchy="false" id="S2.SS1.p1.2.m2.1.2.3.1" xref="S2.SS1.p1.2.m2.1.2.3.1.cmml">→</mo><mi id="S2.SS1.p1.2.m2.1.2.3.3" xref="S2.SS1.p1.2.m2.1.2.3.3.cmml">ℝ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.2.cmml" xref="S2.SS1.p1.2.m2.1.2"><ci id="S2.SS1.p1.2.m2.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.2.1">:</ci><apply id="S2.SS1.p1.2.m2.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.2.2"><times id="S2.SS1.p1.2.m2.1.2.2.1.cmml" xref="S2.SS1.p1.2.m2.1.2.2.1"></times><apply id="S2.SS1.p1.2.m2.1.2.2.2.cmml" xref="S2.SS1.p1.2.m2.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.2.2.2.1.cmml" xref="S2.SS1.p1.2.m2.1.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.2.2.2.2.cmml" xref="S2.SS1.p1.2.m2.1.2.2.2.2">𝑓</ci><ci id="S2.SS1.p1.2.m2.1.2.2.2.3.cmml" xref="S2.SS1.p1.2.m2.1.2.2.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">⋅</ci></apply><apply id="S2.SS1.p1.2.m2.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.2.3"><ci id="S2.SS1.p1.2.m2.1.2.3.1.cmml" xref="S2.SS1.p1.2.m2.1.2.3.1">→</ci><apply id="S2.SS1.p1.2.m2.1.2.3.2.cmml" xref="S2.SS1.p1.2.m2.1.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.2.3.2.1.cmml" xref="S2.SS1.p1.2.m2.1.2.3.2">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.2.3.2.2.cmml" xref="S2.SS1.p1.2.m2.1.2.3.2.2">ℝ</ci><ci id="S2.SS1.p1.2.m2.1.2.3.2.3.cmml" xref="S2.SS1.p1.2.m2.1.2.3.2.3">𝑑</ci></apply><ci id="S2.SS1.p1.2.m2.1.2.3.3.cmml" xref="S2.SS1.p1.2.m2.1.2.3.3">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">f_{i}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}</annotation></semantics></math> be the empirical loss function constructed via the local dataset of UE <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">i</annotation></semantics></math>. The goal of all the entities in the system is to solve the following optimization problem:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\underset{{\boldsymbol{w}\in\mathbb{R}^{d}}}{\mathrm{min}}~{}~{}f(\boldsymbol{w})=\frac{1}{N}\sum_{n=1}^{N}f_{n}(\boldsymbol{w})," display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><munder accentunder="true" id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.2.2.2.cmml">min</mi><mrow id="S2.E1.m1.3.3.1.1.2.2.1" xref="S2.E1.m1.3.3.1.1.2.2.1.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.1.2" xref="S2.E1.m1.3.3.1.1.2.2.1.2.cmml">𝒘</mi><mo id="S2.E1.m1.3.3.1.1.2.2.1.1" xref="S2.E1.m1.3.3.1.1.2.2.1.1.cmml">∈</mo><msup id="S2.E1.m1.3.3.1.1.2.2.1.3" xref="S2.E1.m1.3.3.1.1.2.2.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.1.3.2" xref="S2.E1.m1.3.3.1.1.2.2.1.3.2.cmml">ℝ</mi><mi id="S2.E1.m1.3.3.1.1.2.2.1.3.3" xref="S2.E1.m1.3.3.1.1.2.2.1.3.3.cmml">d</mi></msup></mrow></munder><mo lspace="0.660em" rspace="0em" id="S2.E1.m1.3.3.1.1.2.1" xref="S2.E1.m1.3.3.1.1.2.1.cmml">​</mo><mi id="S2.E1.m1.3.3.1.1.2.3" xref="S2.E1.m1.3.3.1.1.2.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.2.1a" xref="S2.E1.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.2.4.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.4.2.1" xref="S2.E1.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">𝒘</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.4.2.2" xref="S2.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><mfrac id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><mn id="S2.E1.m1.3.3.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.cmml">1</mn><mi id="S2.E1.m1.3.3.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.3.3" xref="S2.E1.m1.3.3.1.1.3.3.cmml"><munderover id="S2.E1.m1.3.3.1.1.3.3.1" xref="S2.E1.m1.3.3.1.1.3.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.3.3.1.2.2" xref="S2.E1.m1.3.3.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.3.3.1.1.3.3.1.2.3" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.3.1.2.3.2" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.2.cmml">n</mi><mo id="S2.E1.m1.3.3.1.1.3.3.1.2.3.1" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.3.3.1.1.3.3.1.2.3.3" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.3.3.1.1.3.3.1.3" xref="S2.E1.m1.3.3.1.1.3.3.1.3.cmml">N</mi></munderover><mrow id="S2.E1.m1.3.3.1.1.3.3.2" xref="S2.E1.m1.3.3.1.1.3.3.2.cmml"><msub id="S2.E1.m1.3.3.1.1.3.3.2.2" xref="S2.E1.m1.3.3.1.1.3.3.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.3.3.2.2.2" xref="S2.E1.m1.3.3.1.1.3.3.2.2.2.cmml">f</mi><mi id="S2.E1.m1.3.3.1.1.3.3.2.2.3" xref="S2.E1.m1.3.3.1.1.3.3.2.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.3.3.2.1" xref="S2.E1.m1.3.3.1.1.3.3.2.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.3.3.2.3.2" xref="S2.E1.m1.3.3.1.1.3.3.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.3.3.2.3.2.1" xref="S2.E1.m1.3.3.1.1.3.3.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">𝒘</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.3.3.2.3.2.2" xref="S2.E1.m1.3.3.1.1.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"></eq><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><times id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.1"></times><apply id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"><apply id="S2.E1.m1.3.3.1.1.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1"><in id="S2.E1.m1.3.3.1.1.2.2.1.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.1"></in><ci id="S2.E1.m1.3.3.1.1.2.2.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.2">𝒘</ci><apply id="S2.E1.m1.3.3.1.1.2.2.1.3.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.2.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.3">superscript</csymbol><ci id="S2.E1.m1.3.3.1.1.2.2.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.3.2">ℝ</ci><ci id="S2.E1.m1.3.3.1.1.2.2.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.2.2.1.3.3">𝑑</ci></apply></apply><ci id="S2.E1.m1.3.3.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.2">min</ci></apply><ci id="S2.E1.m1.3.3.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.2.3">𝑓</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝒘</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><times id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1"></times><apply id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><divide id="S2.E1.m1.3.3.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2"></divide><cn type="integer" id="S2.E1.m1.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2">1</cn><ci id="S2.E1.m1.3.3.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">𝑁</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.3.cmml" xref="S2.E1.m1.3.3.1.1.3.3"><apply id="S2.E1.m1.3.3.1.1.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.3.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.3.3.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.2.2"></sum><apply id="S2.E1.m1.3.3.1.1.3.3.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3"><eq id="S2.E1.m1.3.3.1.1.3.3.1.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.1"></eq><ci id="S2.E1.m1.3.3.1.1.3.3.1.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.2">𝑛</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.3.3.1.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.1.1.3.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3.3.1.3">𝑁</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2"><times id="S2.E1.m1.3.3.1.1.3.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2.1"></times><apply id="S2.E1.m1.3.3.1.1.3.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.3.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.3.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2.2.2">𝑓</ci><ci id="S2.E1.m1.3.3.1.1.3.3.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2.2.3">𝑛</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝒘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\underset{{\boldsymbol{w}\in\mathbb{R}^{d}}}{\mathrm{min}}~{}~{}f(\boldsymbol{w})=\frac{1}{N}\sum_{n=1}^{N}f_{n}(\boldsymbol{w}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.11" class="ltx_p">where <math id="S2.SS1.p1.4.m1.1" class="ltx_Math" alttext="\boldsymbol{w}\in\mathbb{R}^{d}" display="inline"><semantics id="S2.SS1.p1.4.m1.1a"><mrow id="S2.SS1.p1.4.m1.1.1" xref="S2.SS1.p1.4.m1.1.1.cmml"><mi id="S2.SS1.p1.4.m1.1.1.2" xref="S2.SS1.p1.4.m1.1.1.2.cmml">𝒘</mi><mo id="S2.SS1.p1.4.m1.1.1.1" xref="S2.SS1.p1.4.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.4.m1.1.1.3" xref="S2.SS1.p1.4.m1.1.1.3.cmml"><mi id="S2.SS1.p1.4.m1.1.1.3.2" xref="S2.SS1.p1.4.m1.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p1.4.m1.1.1.3.3" xref="S2.SS1.p1.4.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m1.1b"><apply id="S2.SS1.p1.4.m1.1.1.cmml" xref="S2.SS1.p1.4.m1.1.1"><in id="S2.SS1.p1.4.m1.1.1.1.cmml" xref="S2.SS1.p1.4.m1.1.1.1"></in><ci id="S2.SS1.p1.4.m1.1.1.2.cmml" xref="S2.SS1.p1.4.m1.1.1.2">𝒘</ci><apply id="S2.SS1.p1.4.m1.1.1.3.cmml" xref="S2.SS1.p1.4.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m1.1.1.3.1.cmml" xref="S2.SS1.p1.4.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.4.m1.1.1.3.2.cmml" xref="S2.SS1.p1.4.m1.1.1.3.2">ℝ</ci><ci id="S2.SS1.p1.4.m1.1.1.3.3.cmml" xref="S2.SS1.p1.4.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m1.1c">\boldsymbol{w}\in\mathbb{R}^{d}</annotation></semantics></math> denotes the model parameters and <math id="S2.SS1.p1.5.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.5.m2.1a"><mi id="S2.SS1.p1.5.m2.1.1" xref="S2.SS1.p1.5.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m2.1b"><ci id="S2.SS1.p1.5.m2.1.1.cmml" xref="S2.SS1.p1.5.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m2.1c">N</annotation></semantics></math> represents the total number of UEs.
The learning task in Eq. (<a href="#S2.E1" title="In II-A General Framework ‣ II Analog Over-the-Air Machine Learning ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) is accomplished by means of gradient descent (GD) training method and analog over-the-air computing (See illustration in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with steps numbered accordingly).
Specifically, in a typical communication round <math id="S2.SS1.p1.6.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.6.m3.1a"><mi id="S2.SS1.p1.6.m3.1.1" xref="S2.SS1.p1.6.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m3.1b"><ci id="S2.SS1.p1.6.m3.1.1.cmml" xref="S2.SS1.p1.6.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m3.1c">k</annotation></semantics></math>, the model training contains the following steps: (1) The latest global model is distributed by the edge server to each UE <math id="S2.SS1.p1.7.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.7.m4.1a"><mi id="S2.SS1.p1.7.m4.1.1" xref="S2.SS1.p1.7.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m4.1b"><ci id="S2.SS1.p1.7.m4.1.1.cmml" xref="S2.SS1.p1.7.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m4.1c">i</annotation></semantics></math>. (2) Each UE <math id="S2.SS1.p1.8.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.8.m5.1a"><mi id="S2.SS1.p1.8.m5.1.1" xref="S2.SS1.p1.8.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m5.1b"><ci id="S2.SS1.p1.8.m5.1.1.cmml" xref="S2.SS1.p1.8.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m5.1c">i</annotation></semantics></math> computes its local gradients <math id="S2.SS1.p1.9.m6.1" class="ltx_Math" alttext="\nabla f_{i}" display="inline"><semantics id="S2.SS1.p1.9.m6.1a"><mrow id="S2.SS1.p1.9.m6.1.1" xref="S2.SS1.p1.9.m6.1.1.cmml"><mo rspace="0.167em" id="S2.SS1.p1.9.m6.1.1.1" xref="S2.SS1.p1.9.m6.1.1.1.cmml">∇</mo><msub id="S2.SS1.p1.9.m6.1.1.2" xref="S2.SS1.p1.9.m6.1.1.2.cmml"><mi id="S2.SS1.p1.9.m6.1.1.2.2" xref="S2.SS1.p1.9.m6.1.1.2.2.cmml">f</mi><mi id="S2.SS1.p1.9.m6.1.1.2.3" xref="S2.SS1.p1.9.m6.1.1.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m6.1b"><apply id="S2.SS1.p1.9.m6.1.1.cmml" xref="S2.SS1.p1.9.m6.1.1"><ci id="S2.SS1.p1.9.m6.1.1.1.cmml" xref="S2.SS1.p1.9.m6.1.1.1">∇</ci><apply id="S2.SS1.p1.9.m6.1.1.2.cmml" xref="S2.SS1.p1.9.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m6.1.1.2.1.cmml" xref="S2.SS1.p1.9.m6.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.9.m6.1.1.2.2.cmml" xref="S2.SS1.p1.9.m6.1.1.2.2">𝑓</ci><ci id="S2.SS1.p1.9.m6.1.1.2.3.cmml" xref="S2.SS1.p1.9.m6.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m6.1c">\nabla f_{i}</annotation></semantics></math> and then (3) modulates the gradient on a common set of orthonormal waveforms, in which the amplitude of each waveform is tuned in accordance with the value of a distinct element of the gradient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
All UEs transmit their modulated waveforms over the air in a synchronized manner and the signals arrive at the edge server simultaneously <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The precise synchronization can be achieved by appropriately setting the cyclic prefix (CP) in an orthogonal frequency division multiplexing (OFDM) symbol or leveraging other state-of-the-art protocols used in cellular systems.</span></span></span>.
Benefiting from the superposition property of multiple access channels, (4) all the signals would be automatically aggregated in the air.
(5) After passing the received signal to a bank of match filters that are tuned according to the waveforms <math id="S2.SS1.p1.10.m7.1" class="ltx_Math" alttext="s_{i}(T-t)" display="inline"><semantics id="S2.SS1.p1.10.m7.1a"><mrow id="S2.SS1.p1.10.m7.1.1" xref="S2.SS1.p1.10.m7.1.1.cmml"><msub id="S2.SS1.p1.10.m7.1.1.3" xref="S2.SS1.p1.10.m7.1.1.3.cmml"><mi id="S2.SS1.p1.10.m7.1.1.3.2" xref="S2.SS1.p1.10.m7.1.1.3.2.cmml">s</mi><mi id="S2.SS1.p1.10.m7.1.1.3.3" xref="S2.SS1.p1.10.m7.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m7.1.1.2" xref="S2.SS1.p1.10.m7.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.10.m7.1.1.1.1" xref="S2.SS1.p1.10.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m7.1.1.1.1.2" xref="S2.SS1.p1.10.m7.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.10.m7.1.1.1.1.1" xref="S2.SS1.p1.10.m7.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.10.m7.1.1.1.1.1.2" xref="S2.SS1.p1.10.m7.1.1.1.1.1.2.cmml">T</mi><mo id="S2.SS1.p1.10.m7.1.1.1.1.1.1" xref="S2.SS1.p1.10.m7.1.1.1.1.1.1.cmml">−</mo><mi id="S2.SS1.p1.10.m7.1.1.1.1.1.3" xref="S2.SS1.p1.10.m7.1.1.1.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S2.SS1.p1.10.m7.1.1.1.1.3" xref="S2.SS1.p1.10.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m7.1b"><apply id="S2.SS1.p1.10.m7.1.1.cmml" xref="S2.SS1.p1.10.m7.1.1"><times id="S2.SS1.p1.10.m7.1.1.2.cmml" xref="S2.SS1.p1.10.m7.1.1.2"></times><apply id="S2.SS1.p1.10.m7.1.1.3.cmml" xref="S2.SS1.p1.10.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m7.1.1.3.1.cmml" xref="S2.SS1.p1.10.m7.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.10.m7.1.1.3.2.cmml" xref="S2.SS1.p1.10.m7.1.1.3.2">𝑠</ci><ci id="S2.SS1.p1.10.m7.1.1.3.3.cmml" xref="S2.SS1.p1.10.m7.1.1.3.3">𝑖</ci></apply><apply id="S2.SS1.p1.10.m7.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m7.1.1.1.1"><minus id="S2.SS1.p1.10.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m7.1.1.1.1.1.1"></minus><ci id="S2.SS1.p1.10.m7.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.10.m7.1.1.1.1.1.2">𝑇</ci><ci id="S2.SS1.p1.10.m7.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.10.m7.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m7.1c">s_{i}(T-t)</annotation></semantics></math>, the edge server can obtain the aggregated global gradients from the output.
Owing to the effects of the channel fading and interference on analog signals, (6) the aggregated global gradients obtained by the edge server in <math id="S2.SS1.p1.11.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.11.m8.1a"><mi id="S2.SS1.p1.11.m8.1.1" xref="S2.SS1.p1.11.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m8.1b"><ci id="S2.SS1.p1.11.m8.1.1.cmml" xref="S2.SS1.p1.11.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m8.1c">k</annotation></semantics></math>-th communication round can be expressed as follows:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="\boldsymbol{g}_{k}=\frac{1}{N}\sum_{n=1}^{N}h_{n,k}\nabla f_{n}\left(\boldsymbol{w}_{k}\right)+\boldsymbol{\xi}_{k}," display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">𝒈</mi><mi id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml">k</mi></msub><mo id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mfrac id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mn id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">1</mn><mi id="S2.E2.m1.3.3.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.cmml"><munderover id="S2.E2.m1.3.3.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E2.m1.3.3.1.1.1.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.2.cmml">n</mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.3.3.1.1.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml">h</mi><mrow id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">n</mi><mo id="S2.E2.m1.2.2.2.4.1" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">k</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.4" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.cmml"><mo rspace="0.167em" id="S2.E2.m1.3.3.1.1.1.1.1.1.4.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.1.cmml">∇</mo><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.2.cmml">f</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.3.cmml">n</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.1.1.2a" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">𝒘</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.2.cmml">+</mo><msub id="S2.E2.m1.3.3.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.3.2.cmml">𝝃</mi><mi id="S2.E2.m1.3.3.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.3.3.cmml">k</mi></msub></mrow></mrow><mo id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1"><eq id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"></eq><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.2">𝒈</ci><ci id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3">𝑘</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><plus id="S2.E2.m1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"></plus><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><divide id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"></divide><cn type="integer" id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">1</cn><ci id="S2.E2.m1.3.3.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3"><eq id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.2">𝑛</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.3.2">ℎ</ci><list id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.4"><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">𝑛</ci><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">𝑘</ci></list></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4"><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.1">∇</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.2">𝑓</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.4.2.3">𝑛</ci></apply></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝒘</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></apply><apply id="S2.E2.m1.3.3.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.3.2">𝝃</ci><ci id="S2.E2.m1.3.3.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\boldsymbol{g}_{k}=\frac{1}{N}\sum_{n=1}^{N}h_{n,k}\nabla f_{n}\left(\boldsymbol{w}_{k}\right)+\boldsymbol{\xi}_{k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.14" class="ltx_p">where the <math id="S2.SS1.p1.12.m1.2" class="ltx_Math" alttext="h_{n,k}" display="inline"><semantics id="S2.SS1.p1.12.m1.2a"><msub id="S2.SS1.p1.12.m1.2.3" xref="S2.SS1.p1.12.m1.2.3.cmml"><mi id="S2.SS1.p1.12.m1.2.3.2" xref="S2.SS1.p1.12.m1.2.3.2.cmml">h</mi><mrow id="S2.SS1.p1.12.m1.2.2.2.4" xref="S2.SS1.p1.12.m1.2.2.2.3.cmml"><mi id="S2.SS1.p1.12.m1.1.1.1.1" xref="S2.SS1.p1.12.m1.1.1.1.1.cmml">n</mi><mo id="S2.SS1.p1.12.m1.2.2.2.4.1" xref="S2.SS1.p1.12.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS1.p1.12.m1.2.2.2.2" xref="S2.SS1.p1.12.m1.2.2.2.2.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m1.2b"><apply id="S2.SS1.p1.12.m1.2.3.cmml" xref="S2.SS1.p1.12.m1.2.3"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m1.2.3.1.cmml" xref="S2.SS1.p1.12.m1.2.3">subscript</csymbol><ci id="S2.SS1.p1.12.m1.2.3.2.cmml" xref="S2.SS1.p1.12.m1.2.3.2">ℎ</ci><list id="S2.SS1.p1.12.m1.2.2.2.3.cmml" xref="S2.SS1.p1.12.m1.2.2.2.4"><ci id="S2.SS1.p1.12.m1.1.1.1.1.cmml" xref="S2.SS1.p1.12.m1.1.1.1.1">𝑛</ci><ci id="S2.SS1.p1.12.m1.2.2.2.2.cmml" xref="S2.SS1.p1.12.m1.2.2.2.2">𝑘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m1.2c">h_{n,k}</annotation></semantics></math> denotes the channel fading for UE <math id="S2.SS1.p1.13.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.13.m2.1a"><mi id="S2.SS1.p1.13.m2.1.1" xref="S2.SS1.p1.13.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m2.1b"><ci id="S2.SS1.p1.13.m2.1.1.cmml" xref="S2.SS1.p1.13.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m2.1c">n</annotation></semantics></math> and <math id="S2.SS1.p1.14.m3.1" class="ltx_Math" alttext="\boldsymbol{\xi}_{k}" display="inline"><semantics id="S2.SS1.p1.14.m3.1a"><msub id="S2.SS1.p1.14.m3.1.1" xref="S2.SS1.p1.14.m3.1.1.cmml"><mi id="S2.SS1.p1.14.m3.1.1.2" xref="S2.SS1.p1.14.m3.1.1.2.cmml">𝝃</mi><mi id="S2.SS1.p1.14.m3.1.1.3" xref="S2.SS1.p1.14.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.14.m3.1b"><apply id="S2.SS1.p1.14.m3.1.1.cmml" xref="S2.SS1.p1.14.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.14.m3.1.1.1.cmml" xref="S2.SS1.p1.14.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.14.m3.1.1.2.cmml" xref="S2.SS1.p1.14.m3.1.1.2">𝝃</ci><ci id="S2.SS1.p1.14.m3.1.1.3.cmml" xref="S2.SS1.p1.14.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.14.m3.1c">\boldsymbol{\xi}_{k}</annotation></semantics></math> is a vector representing the interference noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Aided with the aggregated (noisy) gradients, (7) the edge server updates the global model by gradient descent, and then broadcasts the new model to all UEs for the next round of computations.
Overall, as a coordinator in the system, the edge server is responsible for aggregating and processing the analog signals, performing the global model update, and broadcasting the new model to the UEs, which plays an important role in spatial-temporal connection during training.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">A Closer Look at Interference </span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">The constructive property of electromagnetic waves often leads to the effects of strong repulsion in the interference noise.
A direct consequence of this effect is the spikes in the edge server’s received signal, as illustrated in Fig. 1.
From the perspective of statistics, such a phenomenon indicates that the interference distribution is heavy-tailed.
And it has been widely accepted that a symmetric <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\alpha</annotation></semantics></math>-stable distribution can appropriately capture the heavy-tailed characteristic of interference and hence is often used for modeling interference in wireless networks.
The parameter <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\alpha</annotation></semantics></math> is commonly known as the tail index and the detailed expression of the characteristic function can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.5" class="ltx_p">Notably, <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\alpha</annotation></semantics></math>-stable distributions generally have no analytical expressions of the probability density function (PDF), except for two special cases, namely, if <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\alpha=1" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mrow id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">α</mi><mo id="S2.SS2.p2.2.m2.1.1.1" xref="S2.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><eq id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1.1"></eq><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">𝛼</ci><cn type="integer" id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\alpha=1</annotation></semantics></math>, the distribution reduces to Cauchy, and when <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="\alpha=2" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mrow id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">α</mi><mo id="S2.SS2.p2.3.m3.1.1.1" xref="S2.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><eq id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1.1"></eq><ci id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2">𝛼</ci><cn type="integer" id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\alpha=2</annotation></semantics></math>, it reduces to Gaussian.
Moreover, a random variable that follows <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">\alpha</annotation></semantics></math>-stable distribution has finite moments only up to the <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">\alpha</annotation></semantics></math> order, beyond which the moment is unbounded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.4" class="ltx_p">In general, an <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\alpha</annotation></semantics></math>-stable random variable with a smaller value of <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\alpha</annotation></semantics></math> ensues a heavier tail in the distribution.
Correspondingly, the interference noise has a higher chance of generating repulsive responses: As illustrated by Fig. <a href="#S3.F2" title="Figure 2 ‣ III The Negative Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, interference noise with a smaller tail index (i.e. <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="\alpha=1.2" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mrow id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">α</mi><mo id="S2.SS2.p3.3.m3.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><eq id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1"></eq><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">𝛼</ci><cn type="float" id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">\alpha=1.2</annotation></semantics></math>) results in more huge spikes in the analog signal.
Note that we shall adopt different values for <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">\alpha</annotation></semantics></math> across this work to demonstrate the effects of interference.
According to (2), these spikes severely deviate from the received signal values, which in turn distorts the aggregated gradients.
To this end, the following discussions are centered around the effects of the interference characteristics on the various aspects of system performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">The Negative Side of Interference</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Interference noise inflicts degradation on wireless transmissions.
Analog over-the-air computing is not immune from that.
In the context of analog over-the-air FL, the main detriments stem from interference are the following:
</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.10299/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="219" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> An illustrative example of the effects of heavy-tailed noise imposed on three different sinusoid signals, in which smaller <math id="S3.F2.3.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F2.3.m1.1b"><mi id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><ci id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">\alpha</annotation></semantics></math> leads to more huge spikes (i.e., higher amplitude) on the analog signal, while a larger <math id="S3.F2.4.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F2.4.m2.1b"><mi id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><ci id="S3.F2.4.m2.1.1.cmml" xref="S3.F2.4.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">\alpha</annotation></semantics></math> exhibit fewer impacts.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Slower Convergence Rate</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The nature of heavy-tailed distribution in electromagnetic interference makes it indispensable to consider the possibility of strong impulsive noise in the aggregated global gradients.
In the GD-based learning process, the unanticipated extreme value in interference power can significantly distort the global gradients, leading to unexpected divergence for the subsequent optimization processes.
As such, the first fundamental question is: <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Does analog over-the-air edge learning algorithms converge with the noisy gradients?</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.7" class="ltx_p">Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Slower Convergence Rate ‣ III The Negative Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives the experimental result that compares the convergence performance under different tail index <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\alpha</annotation></semantics></math>, where <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">100</annotation></semantics></math> UEs jointly train a neural network to accomplish an image classification task on MNIST dataset via the analog over-the-air edge learning system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
This figure plots the training loss as a function of communication rounds and it conveys a two-fold message: (<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">a</annotation></semantics></math>) GD-based model training is resilient to parameter distortion and hence relevant to the edge learning system and (<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">b</annotation></semantics></math>) the tail index <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\alpha</annotation></semantics></math> plays a pivotal role in convergence rate.
Specifically, as the tail index decreases from <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\alpha=1.9" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">α</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">1.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><eq id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1"></eq><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝛼</ci><cn type="float" id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">1.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\alpha=1.9</annotation></semantics></math> (relatively light-tailed) to <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\alpha=1.1" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">α</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">1.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><eq id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></eq><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝛼</ci><cn type="float" id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">1.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\alpha=1.1</annotation></semantics></math> (relatively heavy-tailed), there is an orders-of-magnitude slowdown in the convergence rate.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">In fact, if the global loss function <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="f(\boldsymbol{w})" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.2" xref="S3.SS1.p3.1.m1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.2.2" xref="S3.SS1.p3.1.m1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.1.m1.1.2.1" xref="S3.SS1.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.1.m1.1.2.3.2" xref="S3.SS1.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.1.2.3.2.1" xref="S3.SS1.p3.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">𝒘</mi><mo stretchy="false" id="S3.SS1.p3.1.m1.1.2.3.2.2" xref="S3.SS1.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.2"><times id="S3.SS1.p3.1.m1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2.1"></times><ci id="S3.SS1.p3.1.m1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2.2">𝑓</ci><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝒘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">f(\boldsymbol{w})</annotation></semantics></math> given in (1) possesses good structural properties (i.e., strong convexity and smoothness), it can be shown that the algorithm converges in the order of <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{O}(1/k^{\alpha-1})" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">1</mn><mo id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">/</mo><msup id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.2.cmml">k</mi><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.2.cmml">α</mi><mo id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msup></mrow><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2"></times><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝒪</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><divide id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1"></divide><cn type="integer" id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">1</cn><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.2">𝑘</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3"><minus id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.1"></minus><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.2">𝛼</ci><cn type="integer" id="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathcal{O}(1/k^{\alpha-1})</annotation></semantics></math>, where <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">k</annotation></semantics></math> is the number of global iterations.
This mathematical expression also confirms that the heavy-tailedness of interference distribution is a dominating factor in the convergence rate of analog over-the-air machine learning, whereas the heavier the tail, the slower the algorithm converges.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.10299/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="218" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Simulation results of the training loss of a multi-layer-perceptron (MLP) on the MNIST dataset, with different tail index <math id="S3.F3.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F3.2.m1.1b"><mi id="S3.F3.2.m1.1.1" xref="S3.F3.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><ci id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">\alpha</annotation></semantics></math>. </figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Unstable Training Performance</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The heavy-tailed interference not just decelerates the convergence rate of FL, but more crucially, incurs severe disturbance to the model training process.
Specifically, during the optimization steps of the GD, large discrepancy and divergence would be caused by the aggregated gradients with the possibly unexpected extreme-value noise (see Fig. <a href="#S3.F2" title="Figure 2 ‣ III The Negative Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for an illustrative example of strong impulses in the interference).
Hence, observing relatively large fluctuations during the training process would be inevitable.
Note that this can be readily implemented on the existing wireless infrastructure, e.g., let UEs send the parameters using the orthogonal frequency-division multiplexing (OFDM) modulation in which the (discrete) Fourier basis already provides a set of orthogonal “waveforms”. A similar phenomenon can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where we can clearly see that the algorithm converges in the presence of wireless noise and the noise leads to the spikes in the convergence curve.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C Inadequate Analytical Framework ‣ III The Negative Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides a pictorial demonstration of this phenomenon. Particularly, it considers an edge learning system in which 100 UEs jointly carry out a LASSO task based on distributed ADMM (<span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_italic">i.e.</span>, alternating direction method of multipliers) and analog over-the-air computations.
The figure clearly shows that decreasing the tail index <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\alpha</annotation></semantics></math>, which increases the level of heavy-tailedness, results in more variations in the convergence curve.
Besides, even in the regime of relatively light-tailed interference (namely, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\alpha=1.9" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">α</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">1.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝛼</ci><cn type="float" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">1.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\alpha=1.9</annotation></semantics></math>), there is also unsteadiness associated with the convergence curve.
Part of the reason is that compared to GD-based algorithms, ADMM-based approaches are more sensitive to the variants in the intermediate parameters.
This experiment demonstrates that the interference threatens the system’s stability.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Inadequate Analytical Framework</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">When it comes to the theoretical aspects, the conventional analysis framework relies on the existence of the gradients’ second moments and may not be applicable when the tail index of the interference is small, i.e., <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\alpha&lt;2" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><lt id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></lt><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝛼</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\alpha&lt;2</annotation></semantics></math>, since in this case, the variance of the aggregated gradients given in (2) is infinite.
It thus entails the establishment of a new theoretical framework to quantify the convergence performance in analog over-the-air edge learning.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">In order to develop an analytical framework that is universally applicable to different tail indices of the interference, one shall opt for the <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\alpha</annotation></semantics></math>-norm to quantify the difference in the model parameters upon each global updating step <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. As the expected value of aggregated gradients under this new distance metric can now be bounded, convergence analysis can be carried out via recursively bounding the variations in the global parameters and then conducting telescoping.
It is also noteworthy that when <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\alpha=2" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">α</mi><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><eq id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></eq><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝛼</ci><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\alpha=2</annotation></semantics></math>, the <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\alpha</annotation></semantics></math>-norm reduces to the conventional Euclidean norm, so as the convergence analysis.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Nonetheless, there are only a few initial works on this research topic, where the current results heavily rely on assumptions that the objective function is strongly convex and smooth.
These presumptions have constrained the power of the analytical tools because many popular ML models, e.g., neural networks, are highly non-convex.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In summary, the bad face of the heavy-tailed interference in analog over-the-air computations brings challenges as well as the opportunity for both theoretical analysis and algorithm design. A general framework could be developed to tackle the performance degradation by capturing the intrinsic property of the heavy-tailed interference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.10299/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="218" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Simulation results of the training loss of LASSO problem solved via ADMM over synthetic data, with different tail index <math id="S3.F4.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F4.2.m1.1b"><mi id="S3.F4.2.m1.1.1" xref="S3.F4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F4.2.m1.1c"><ci id="S3.F4.2.m1.1.1.cmml" xref="S3.F4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.m1.1d">\alpha</annotation></semantics></math>. </figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">The Positive Side of Interference</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As we know, nothing interesting is ever completely one-sided, so as interference.
In this section, the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">good face</span> of interference, i.e., its potential benefits on the generalization capability, convergence acceleration, and privacy protection in analog over-the-air FL will be discussed.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To commence with, as an epitome among the diverse ML techniques, deep learning (DL) achieves great success in various areas and applications.
In practice, stochastic gradient descent (SGD) or mini-batch SGD is one of the de facto choices for training DL models.
Concurrently, the gradient noise would be introduced due to randomness in the SGD algorithm.
Recent work reveals that the gradient noise in SGD tends to exhibit a heavy-tailed behavior with highly non-Gaussian characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">On the other hand, both the empirical and theoretical findings demonstrate that the gradient noise in SGD can guide the optimizer to find a flat minimum that possesses a good generalization power.
In consequence, perturbed gradient descent (PGD) methods have been investigated by several works, demonstrating that injecting gradient noise during the training process can effectively improve the generalization performance as well as avoid over-fitting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The reason mainly attributes to that artificial noise can help the model escape local minimum and saddle points.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Following our discussions in the previous sections, since the wireless interference often obeys a heavy-tailed distribution, it could be utilized as a natural perturbation to the locally updated gradients from UEs in such GD-based analog over-the-air edge learning.
Another question then begs: “<span id="S4.p4.1.1" class="ltx_text ltx_font_italic">Can the channel noise play a similar role as the artificial gradient noise to boost the FL performance?</span>”</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Potential Generalization Enhancements</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Taking the induced interference as a natural perturbation to the aggregated gradients in over-the-air edge learning, it is worthwhile to explore the relationship with the generalization capability.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Interestingly, recent work established theoretical bounds to show that it is possible to achieve a better generalization in the presence of heavy-tailed interference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
More precisely, in the analog over-the-air edge learning system, the generalization error would decrease along with the interference tail index.
Even though this finding may be empirically counter-intuitive with regard to the conventional analysis of the interference, it would be a promising application by developing approaches to reap this potential gain.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10299/assets/x5.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="218" height="164" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.10299/assets/x6.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="219" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> A depiction of good face of <math id="S4.F5.5.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.F5.5.m1.1b"><mi id="S4.F5.5.m1.1.1" xref="S4.F5.5.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.F5.5.m1.1c"><ci id="S4.F5.5.m1.1.1.cmml" xref="S4.F5.5.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.m1.1d">\alpha</annotation></semantics></math>-stable interference: (<math id="S4.F5.6.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.F5.6.m2.1b"><mi id="S4.F5.6.m2.1.1" xref="S4.F5.6.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.F5.6.m2.1c"><ci id="S4.F5.6.m2.1.1.cmml" xref="S4.F5.6.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.6.m2.1d">a</annotation></semantics></math>) gives simulation results of the training loss of MLP on the MNIST data set, with different tail index <math id="S4.F5.7.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.F5.7.m3.1b"><mi id="S4.F5.7.m3.1.1" xref="S4.F5.7.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.F5.7.m3.1c"><ci id="S4.F5.7.m3.1.1.cmml" xref="S4.F5.7.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.7.m3.1d">\alpha</annotation></semantics></math> and (<math id="S4.F5.8.m4.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.F5.8.m4.1b"><mi id="S4.F5.8.m4.1.1" xref="S4.F5.8.m4.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.F5.8.m4.1c"><ci id="S4.F5.8.m4.1.1.cmml" xref="S4.F5.8.m4.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.m4.1d">b</annotation></semantics></math>) gives an illustrative example via a loss landscape that interference helps optimizer jump out of local minimum. </figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As shown in Fig. <a href="#S4.F5.sf1" title="In Figure 5 ‣ IV-A Potential Generalization Enhancements ‣ IV The Positive Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, the interference in analog over-the-air computations with a smaller value of tail index <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\alpha</annotation></semantics></math> (i.e., a heavier tail in the distribution) could achieve better generalization performance with respect to the test accuracy of the trained ML model.
This benefit is ascribed to the possible occurrence of the strong impulse of the heavy-tailed interference.
For better illustration, we provide a loss landscape with the task of training a 3-layer neural network on MNIST dataset.
As exemplified by Fig. <a href="#S4.F5.sf2" title="In Figure 5 ‣ IV-A Potential Generalization Enhancements ‣ IV The Positive Side of Interference ‣ Edge Intelligence Over the Air: Two Faces of Interference in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>, when the optimizer is trapped in the local optimal status, such a “big jump” could help the global model escape from the local minima.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">It is also important to stress that this phenomenon occurs with a certain probability because the change of the channel status is independent of the status of model training.
Therefore, it would be another open question as well as the potential research opportunity to develop a method or framework to utilize the good face of the interference and further enhance the performance for analog over-the-air edge learning systems.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Accelerating Model Training with Power Control</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The previous part shows that interference may help the optimizer jump out of the local optimum or escape from the saddle point during the GD-based training process.
However, the mentioned phenomenon of generalization enhancements only occurs with a certain probability due to the randomness of channel interference.
Then, a natural consideration is to maneuver the interference when the optimizer falls in the region of local optimum and/or saddle points, so as to assuredly reap the performance gain in analog over-the-air edge learning.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Motivated by the above issue, a practical approach is proposed to harness the interference in analog over-the-air computations to accelerate training of distributed principle component analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
The acceleration is based on the detection of saddle points. Once a saddle point is detected during training, an online power control mechanism in network management would be applied to amplify the effect of noise to help the GD-based optimizer escape from the saddle points and realize acceleration.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Therefore, via a proper manipulation on the noisy effect based on the real-time training status, the interference could indeed be repurposed to play a positive role in the training process and accelerate the convergence of analog over-the-air edge learning system.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Privacy Preservation and Efficient Sampling</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Differential privacy (DP) is a widely adopted standard to quantify the privacy leakage of sensitive data, which can be generally achieved by randomly perturbing the data.
In the context of analog over-the-air federated edge learning, random interference serves as a natural perturbation to the uncoded signal, thereby enhancing the privacy protection of the end-users’ dataset.
Particularly, considering that channel noise has different influences on the convergence and privacy aspects, power control can be used to actively keep the signal-to-noise ratio (SNR) at an acceptable level while ensuring the DP constraint<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The level of DP typically depends on the degree of perturbation imposed by the randomized mechanism. The DP performance in this case would be affected by the SNR adjusted via power control.</span></span></span>. In fact, with an adequately devised power control mechanism, privacy can be obtained for free under certain DP constraints with guaranteed learning performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">In the context of Bayesian federated learning, especially the gradient-based Markov Chain Monte Carlo (MCMC) method, randomness also exists in the MCMC sampling.
Then, the interference would hold a double role of the randomness for approximation in MCMC and privacy protection in such a scenario.
Both theoretical and empirical results reveal that, the channel interference could be repurposed under a suitable condition to make the system with noisy communications achieve the same performance as with the ideal communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In summary, the inherent channel interference in analog over-the-air computations could also bring benefit to the training and privacy performance of federated edge learning. Considering both the bad face and good face of the interference in analog over-the-air edge learning system, this two-sided effect would provide a new perspective to designing integrated communication and computation framework for robust, communication-efficient, and privacy-preserving edge learning.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Future Trends and Open Issues</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The above discussions have amply demonstrated interference’s effects on federated edge learning: On the positive side, it has the potential of achieving “one stone, many (and many) birds”; on the negative side, it could be “the straw that broke the camel’s back”, whereas a very strong electromagnetic impulse may damage the whole trained results.
To harvest the potential gains and cope with the existing crux, we summarize the following directions for future research pursuits:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">How to conduct pruning and adaptive optimization to improve system efficiency?</span> Most of the current analog over-the-air methods have no requirements or assumptions on the optimizations and training process in both the edge server and local sides. Consequently, some communication-efficient distributed learning approaches (e.g., pruning, quantization, and masking) and federated optimization methods can be further incorporated into the over-the-air-based federated edge learning systems.
Applying the pruning or quantization method at the side of UEs for the model weight in the uplink transmission, better communication efficiency could be achieved in the bandwidth-limited network.
How to mitigate the variance and fluctuations brought by the interference in over-the-air computations could be further investigated in these cases.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">How to stabilize the training process?</span> With the existence of the heavy-tailed interference in the analog over-the-air edge learning system, a better control mechanism at the edge server is of necessity to smooth out the fluctuations and hence improve robustness in the training process. Adaptive optimization and robust training methods are possible candidate techniques for this purpose. For example, federated optimization methods, e.g., FedAvgM and FedOpt with different adaptive optimizers, can be applied on either the (global) edge server-side or (local) UE side to enhance the stability performance. Furthermore, robust training techniques like <span id="S5.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">gradient clipping</span> could also be used during local training to eliminate the excessive deviations in model parameters and thus stabilize the training performance.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">How to re-purpose interference for better generalization?</span> The mentioned potential gain on generalization performance relies on the random noise effect to jump out of the local minimum. It remains an open question about the development of methods to repurpose the interference for generalization capability enhancements. Following experience in the previous trials, one needs to jointly consider the local geometric landscape of the optimizer and the instantaneous variations in the interference. On the other hand, more advanced wireless technologies such as multiple antenna arrays and intelligent reflection surfaces can be employed to not just enhance the received signal quality, but, more importantly, engineer the interference distribution for better performance of the edge learning system.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p"><span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">How about temporal correlation in the interference noise?</span> In practice, interference does not always vary over time in an i.i.d. manner.
Actually, the correlations of interferers’ spatial locations and fading effect among a transmission area would lead to temporal correlation in the interference noise.
And the effect of such a temporally correlated interference on the performance of analog over-the-air edge learning system is another open question.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p"><span id="S5.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">What about data corruption in the end-user equipments?</span> Considering a more realistic edge learning system, the diversity among different UEs’ hardware reliability, bias, and preference should be taken into account in the design of edge learning systems and network management. In such a system, we shall get rid of the ideal assumption that all the local training data and labels are correct. In particular, we may consider heterogeneous label noise exists among UEs, where a subset of data may have incorrect labels. It is necessary to design a robust federated edge learning system for network management in the presence of both channel interference and local label noise. And whether the potential benefits of the interference exist or not for corrupted local data remains as an open question.</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i6.p1" class="ltx_para">
<p id="S5.I1.i6.p1.1" class="ltx_p"><span id="S5.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">How to personalize analog over-the-air FL system?</span> <span id="S5.I1.i6.p1.1.2" class="ltx_text ltx_font_italic">Personalizing</span> the FL model has become a trending approach to tackle the poor generalization issue of conventional FL framework that only focuses on a single generic global model and hence not adapting well to the diverse local data distribution in heterogeneous wireless networks. Recently, it was proposed to utilize personalized model training to realize the enhancements on robustness and fairness of FL systems. As the induced channel interference in analog over-the-air computations
and the distributed real-world noisy training data would possibly harm the training performance, the personalized local training procedures would have the potential to be further explored in the analog over-the-air edge learning system to enhance the robustness and stability to mitigate the performance fluctuations in such scenarios. For instance, each client could maintain its own personalized local training objectives or model architectures.</p>
</div>
</li>
<li id="S5.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i7.p1" class="ltx_para">
<p id="S5.I1.i7.p1.1" class="ltx_p"><span id="S5.I1.i7.p1.1.1" class="ltx_text ltx_font_italic">How to design a secure analog over-the-air computing framework with diverse attacks?</span> Even though the interference exhibited enhanced DP protection to the transmitted data, it remains unclear whether the induced interference could help defend against the gradient-based attacks, especially for neural network-based tasks, such as gradient inversion. Another promising topic is to explore the backdoor attack and the corresponding defense in analog over-the-air computations. It is particularly worthwhile to investigate the possibilities of conducting proactive adversarial perturbation or defense to combat backdoor attacks via harnessing the induced interference.
In addition, how to defend against signal interception remains to be further explored.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Concluding Remarks</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">By exploiting the superposition property of wireless waveforms, analog over-the-air computations provide a high-scalability and concurrently low-cost solution to federated edge learning systems. In this article, the two-sided effect of the interference in the analog over-the-air edge learning system has been investigated. Specifically, while interference impedes the convergence rate and threatens system stability, there are also silver linings of this factor, that it has the potential to enhance end-user privacy, help the training model escape saddle points or local minimum, and attain a better generalization. Such an effect appeals to designing new optimization frameworks as well as control mechanisms in intelligent network management to repurpose the channel interference in analog over-the-air computations to enhance the performance over-the-air edge learning.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. Int. Conf. Artif. Intell. Stat. (AISTATS)</em>, Fort
Lauderdale, USA, Apr. 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network
intelligence at the edge,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em>, vol. 107, no. 11, pp.
2204–2239, Oct. 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, “Scheduling policies for
federated learning in wireless networks,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Commun.</em>,
vol. 68, no. 1, pp. 317–333, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-the-air
computation,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol. 19, no. 3, pp.
2022–2035, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Zhu, J. Xu, K. Huang, and S. Cui, “Over-the-air computing for wireless data
aggregation in massive IoT,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Wireless Commun.</em>, vol. 28, no. 4,
pp. 57–65, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. H. Yang, Z. Chen, T. Q. S. Quek, and H. V. Poor, “Revisiting analog
over-the-air machine learning: The blessing and curse of interference,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Topics Signal Process.</em>, vol. 16, no. 3, pp. 406–419,
2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Guo, Y. Zhu, H. Ma, V. K. Lau, K. Huang, X. Li, H. Nong, and M. Zhou,
“Over-the-air aggregation for federated learning: Waveform superposition and
prototype validation,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">J. of Commun. and Inf. Networks</em>, vol. 6,
no. 4, pp. 429–442, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Elgabli, J. Park, C. B. Issaid, and M. Bennis, “Harnessing wireless
channels for scalable and privacy-preserving federated learning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Commun.</em>, vol. 69, no. 8, pp. 5194–5208, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Liu and O. Simeone, “Privacy for free: Wireless federated learning via
uncoded transmission with adaptive power control,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas
Commun.</em>, vol. 39, no. 1, pp. 170–185, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
G. Zhu, Y. Wang, and K. Huang, “Broadband analog aggregation for low-latency
federated edge learning,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol. 19,
no. 1, pp. 491–506, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Zhang, G. Zhu, R. Wang, V. K. Lau, and K. Huang, “Turning channel noise
into an accelerator for over-the-air principal component analysis,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Shi, Y. Zhou, and Y. Shi, “Over-the-air decentralized federated learning,”
in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Int. Symp. on Inform. Theory (ISIT)</em>.   IEEE, 2021, pp. 455–460.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
U. Simsekli, L. Sagun, and M. Gurbuzbalaban, “A tail-index analysis of
stochastic gradient noise in deep neural networks,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on
Machine Learn. (ICML)</em>.   PMLR, 2019,
pp. 5827–5837.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Orvieto, H. Kersting, F. Proske, F. Bach, and A. Lucchi, “Anticorrelated
noise injection for improved generalization,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Available as
ArXiv:2202.02831</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Liu and O. Simeone, “Wireless federated langevin monte carlo: Repurposing
channel noise for bayesian sampling and privacy,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless
Commun.</em>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.10298" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.10299" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.10299">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.10299" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.10301" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 00:34:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
