<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.08072] Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model</title><meta property="og:description" content="This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems.
For training, fine-tuning GPT models is a common practice in resource-rich languages like English, howev…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.08072">

<!--Generated on Tue Feb 27 23:07:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kosuke Takahashi, Takahiro Omi, Kosuke Arima
<br class="ltx_break">Stockmark 
<br class="ltx_break">kosuke.takahashi, takahiro.omi, kosuke.arima@stockmark.co.jp <span id="id1.1.id1" class="ltx_ERROR undefined">\AND</span>Tatsuya Ishigaki 
<br class="ltx_break">National Institute of Advanced Industrial Science and Technology 
<br class="ltx_break">ishigaki.tatsuya@aist.go.jp
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems.
For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs.
Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses.
In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner.
We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model.
The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Fine-tuning large language models (LLMs) has been proven effective for enhancing question-answering systems <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>.
However, extending this approach to languages other than English presents challenges due to the scarcity of adequate QA pairs for training.
In this study, we specifically target Japanese as a representative non-English language.
We propose a straightforward approach that synthesizes Japanese QA pairs using an instruct-tuned model.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our experiments utilize OpenAI’s ChatAPI with the <span id="footnote1.1" class="ltx_text ltx_font_italic">gpt-3.5-turbo-0613</span> model.</span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Question-answering tasks can be categorized into two main settings: questions with context and without context <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.
In this study, we focus on the context-based setting as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In this setting, the system takes a question along with the accompanying context as input.
The model generates an answer by utilizing the information provided within the context.
On the other hand, the setting without context involves the system processing only the question as input.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.08072/assets/fig/task.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The task of the generative context-aware QA.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We present a straightforward yet cost-effective method for generating synthetic question-answer (QA) pairs.
Existing QA systems are trained on either human-authored datasets or automatically generated QA pairs  <cite class="ltx_cite ltx_citemacro_cite">Sachan and Xing (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>); Tang et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>, both leading to high labor costs.
By contrast, this paper investigates utilizing an instruct-tuned model inspired by their reasonable ability to produce synthetic dataset <cite class="ltx_cite ltx_citemacro_cite">Gilardi et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.
We use a context as input and generate both the corresponding question and its answer.
The instruct-tuned model allows us to produce QA pairs in a zero-shot or few-shot manner, eliminating the need for manual curation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our experiments compare question-answering systems fine-tuned on synthetic data generated through various strategies.
Specifically, we explore different sources of contexts, the number of shots fed into the instruct-tuned model, and the quantity of QA pairs generated.
The evaluation on JSQuAD’s evaluation dataset <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> provides three findings.
Firstly, employing contexts extracted from a corpus with similar characteristics to the evaluation dataset yields improved performance.
Secondly, the one-shot strategy outperforms the zero-shot approach.
Lastly, generating three QA pairs for each context is more effective than generating a lower number of QA pairs.
The top-performing model fine-tuned on our synthetic data exhibits comparable performance to models trained on manually curated data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Existing QA focus on two major settings: "closedQA" with context and "commonsens-QA" without context <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.
For the former, which we target, the QA systems receive a question along with a context, such as a Wikipedia article, and generate an answer.
On the other hand, in the latter setting, the systems only receive a question as input.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">There are two types of QA systems: extractive and generative.
Extractive methods extract an answer as it is from the context by models like BERT <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar et al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite>, while generative methods often use the expressions that are not in the context by models like T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> or GPT <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>.
Our focus is on the latter.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">While several manually created datasets exist in English, such as SQuAD <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar et al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> and QuALITY <cite class="ltx_cite ltx_citemacro_cite">Pang et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, these resources do not directly apply to the Japanese language.
For Japanese, JSQuAD <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> and JAQKET<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.nlp.ecei.tohoku.ac.jp/projects/jaqket/#Reference" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nlp.ecei.tohoku.ac.jp/projects/jaqket/#Reference</a></span></span></span> are available.
We use JSQuAD<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Strictly, JSQuAD is not for evaluating generative QA, but the span extraction-based setting. We use this data because there is no common evaluation data in Japanese for generative QA. Our models generate answers not extract spans, thus, we also conduct human evaluations.</span></span></span> because the evaluation data of JAQKET is not public.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Existing studies synthesize QA pairs by two main approaches: supervised <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Sachan and Xing (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>); Tang et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> and unsupervised <cite class="ltx_cite ltx_citemacro_cite">Puri et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>.
The supervised approaches train question-answer generators using manually created datasets.
Our approach generates QA pairs from contexts in a zero-shot or few-shot manner, eliminating the need to train generators.
In the unsupervised approach, <cite class="ltx_cite ltx_citemacro_citet">Puri et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> uses a named entity recognizer (NER) for answer candidate extraction while our approach uses only an instruct-tuned model in end-to-end and does not require NER.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthesizing QA Pairs</h2>

<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text">
<span id="S3.F2.1.1.1" class="ltx_inline-block ltx_align_bottom ltx_framed ltx_framed_rectangle">
<span id="S3.F2.1.1.1.1" class="ltx_block ltx_align_bottom">
<span id="S3.F2.1.1.1.1.1" class="ltx_p">Based on the given texts, please make a pair of answerable question and answer.</span>
<span id="S3.F2.1.1.1.1.2" class="ltx_p">Please make the answer in Japanese polite language.</span>
<span id="S3.F2.1.1.1.1.3" class="ltx_p">Please respond in the JSON format.</span>
<span id="S3.F2.1.1.1.1.4" class="ltx_p"><code id="S3.F2.1.1.1.1.4.1" class="ltx_verbatim ltx_font_typewriter">## example</code></span>
<span id="S3.F2.1.1.1.1.5" class="ltx_p">texts:"texts to extract the pair of question and answer"</span>
<span id="S3.F2.1.1.1.1.6" class="ltx_p">output:<code id="S3.F2.1.1.1.1.6.1" class="ltx_verbatim ltx_font_typewriter">{</code>"Question":"the question that can be answered from the texts", "Answer":"the answer to the question"}</span>
<span id="S3.F2.1.1.1.1.7" class="ltx_p"><code id="S3.F2.1.1.1.1.7.1" class="ltx_verbatim ltx_font_typewriter">## input</code></span>
<span id="S3.F2.1.1.1.1.8" class="ltx_p">texts:<code id="S3.F2.1.1.1.1.8.1" class="ltx_verbatim ltx_font_typewriter">{QA context}</code></span>
<span id="S3.F2.1.1.1.1.9" class="ltx_p">output:</span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of zero-shot prompt to generate a pair of QA.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text">
<span id="S3.F3.1.1.1" class="ltx_inline-block ltx_align_bottom ltx_framed ltx_framed_rectangle">
<span id="S3.F3.1.1.1.1" class="ltx_block ltx_align_bottom">
<span id="S3.F3.1.1.1.1.1" class="ltx_p">texts:"Resolving technical debt is difficult; we look at JAL’s challenge…(omitted)…JAL’s watchword is Go To Cloud…(omitted),</span>
<span id="S3.F3.1.1.1.1.2" class="ltx_p">output:<code id="S3.F3.1.1.1.1.2.1" class="ltx_verbatim ltx_font_typewriter">{</code>"Question":"What watchwords does Japan Airlines stand for?", "Answer":"JAL’s watchword is Go To Cloud."}</span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An translated sample of the “## example” part in one-shot prompt. Note that the original is in Japanese.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We describe our approach in this section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Source Contexts and Filtering</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">We generate <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation></semantics></math> question-answer pairs from each context.
<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation></semantics></math> is set to one or three in our experiments.
We compare three specific sources of contexts: 1) a random sample of 6,000 Japanese Wikipedia articles (<span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_typewriter">wiki</span>), 2) a random sample of 6,000 news articles (<span id="S3.SS1.p1.2.2" class="ltx_text ltx_font_typewriter">news</span>), and 3) contexts in JSQuAD’s training dataset (<span id="S3.SS1.p1.2.3" class="ltx_text ltx_font_typewriter">JSQuAD</span>).
To collect the news articles, we gathered the most accessed articles from a search engine <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The URL of the engine/dataset is hidden to preserve the anonymity of authors, and will be shown after acceptance</span></span></span> during the period from May 2022 to May 2023.
We limit each context to the first 300 characters before generating QA pairs by the instruct-tuned model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Prompts for Generating QA Pairs</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">We provide examples of zero-shot and one-shot prompts with the setting <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">N=1</annotation></semantics></math> in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Synthesizing QA Pairs ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Synthesizing QA Pairs ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, respectively.
These prompts aim to generate QA pairs from a context.
In the zero-shot prompt, we first present the task instructions, followed by an explanation of the structure oh how an input text is represented, and their desired output JSON structure as shown in the “## example” section.
For the setting <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="N&gt;1" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><gt id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></gt><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">N&gt;1</annotation></semantics></math>, we modify the example of the JSON structure to include more QA pairs.
Then, we write an input text in the “## input” section.
In the zero-shot prompt setting, we only write the format of input and output structures, without including actual texts or the expected question-answer pairs corresponding to the context.
On the other hand, in the one-shot prompt, we replace the “## example” section in <a href="#S3.F2" title="Figure 2 ‣ 3 Synthesizing QA Pairs ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> with the prompt shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Synthesizing QA Pairs ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Unlike the zero-shot prompt, the one-shot prompt includes actual example contexts and their corresponding expected QA pairs.
To better understand the effects of prompt engineering, we compare these two prompts in our experiments.
The tuples of a context and generated QA pairs are used to fine-tune a GPT by the prompt shown in Figure <a href="#S4.F4" title="Figure 4 ‣ Fine-tuning ‣ 4 Experiments ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Evaluation Dataset and Compared Models: </span>
We use the JSQuAD <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> for evaluation.
This evaluation data contains 4,470 human-authored QA pairs given Wikipedia articles as contexts.
We use whole evaluation data for the automatic evaluation while randomly sampled 500 instances are used for manual evaluation.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">We conduct a comprehensive comparison by exploring various combinations of contexts, the number of generated QA pairs denoted as <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">N</annotation></semantics></math> and prompts.
Regarding contexts, we consider three options: <span id="S4.p2.4.1" class="ltx_text ltx_font_typewriter">wiki</span>, <span id="S4.p2.4.2" class="ltx_text ltx_font_typewriter">news</span>, <span id="S4.p2.4.3" class="ltx_text ltx_font_typewriter">JSQuAD</span>, and, as detailed in Sec. <a href="#S3.SS1" title="3.1 Source Contexts and Filtering ‣ 3 Synthesizing QA Pairs ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
For <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">N</annotation></semantics></math>, we compare <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">N</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><eq id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></eq><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">N=1</annotation></semantics></math> and <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">N</mi><mo id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><eq id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></eq><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">𝑁</ci><cn type="integer" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">N=3</annotation></semantics></math>.
We compare zero-shot and one-shot prompts <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We are constrained to one-shot due to the input length limit of ChatGPT.</span></span></span>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Our proposed models are compared with two models: 1) a plain GPT model without fine-tuning and 2) a model fine-tuned on QA pairs from the JSQuAD training dataset (<span id="S4.p3.1.1" class="ltx_text ltx_font_typewriter">Human</span>), where these QA pairs are human-authored while our proposed QA pairs are not human-authored.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We use the synthesized QA pairs to fine-tune the Japanese version of GPT-NeoX <cite class="ltx_cite ltx_citemacro_cite">Black et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite><span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://huggingface.co/cyberagent/open-calm-7b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/cyberagent/open-calm-7b</a></span></span></span>.
To achieve improved speed, we employ LoRA fine-tuning <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>.
In generating answers, we use a prompt in the zero-shot setting (Figure <a href="#S4.F4" title="Figure 4 ‣ Fine-tuning ‣ 4 Experiments ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1" class="ltx_text">
<span id="S4.F4.1.1.1" class="ltx_inline-block ltx_align_bottom ltx_framed ltx_framed_rectangle">
<span id="S4.F4.1.1.1.1" class="ltx_p ltx_align_bottom">## Instruction

<br class="ltx_break">{QUESTION}

<br class="ltx_break">
<br class="ltx_break">## Context

<br class="ltx_break">{CONTEXT}

<br class="ltx_break">
<br class="ltx_break">## Response</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The prompt to generate answers with the fine-tuned GPT-NeoX.</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p"><span id="S4.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">Metrics: </span>
For automatic evaluation, we employ BERTScore <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib9" title="" class="ltx_ref">2002</a>)</cite>.
BERTScore is implemented on our own with a Japanese BERT model.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://huggingface.co/cl-tohoku/bert-base-japanese-v3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/cl-tohoku/bert-base-japanese-v3</a></span></span></span>
As for BLEU, SacreBLEU library <cite class="ltx_cite ltx_citemacro_cite">Post (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> is used.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p3.1" class="ltx_p">These automatic metrics may not directly capture the correctness of an answer to a given question.
To address this, we also conduct manual evaluations by human judges.
We ask four judges, who are experts in natural language processing or linguistics, to assess whether the generated answer is correct or not.
We showed tuples of questions, answers, and contexts to the judges.
We report the accuracy obtained from the manual evaluation.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p4.2" class="ltx_p"><span id="S4.SS0.SSS0.Px1.p4.2.1" class="ltx_text ltx_font_bold">Parameters</span>
We conducted a grid search for tuning parameters: batch size, learning rate, the number of epochs, as well as LoRA’s hyperparameters (specifically <math id="S4.SS0.SSS0.Px1.p4.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS0.SSS0.Px1.p4.1.m1.1a"><mi id="S4.SS0.SSS0.Px1.p4.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p4.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p4.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p4.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px1.p4.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.SS0.SSS0.Px1.p4.2.m2.1a"><mi id="S4.SS0.SSS0.Px1.p4.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p4.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p4.2.m2.1b"><ci id="S4.SS0.SSS0.Px1.p4.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p4.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p4.2.m2.1c">r</annotation></semantics></math>).
The range of values explored during this search is provided in Table <a href="#S4.T1" title="Table 1 ‣ Fine-tuning ‣ 4 Experiments ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Subsequently, the model that attained the highest BERTScore was chosen for evaluation.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<td id="S4.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_border_t">Batch Size: {4, 8},</td>
</tr>
<tr id="S4.T1.2.4.2" class="ltx_tr">
<td id="S4.T1.2.4.2.1" class="ltx_td ltx_align_left">Learning Rate: {0.00001, 0.00005, 0.000001},</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_b">Epoch: {3, 4, 5,}, <math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mi id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">r</annotation></semantics></math>: {4, 8, 16, 64, 128},
<math id="S4.T1.2.2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T1.2.2.2.m2.1a"><mi id="S4.T1.2.2.2.m2.1.1" xref="S4.T1.2.2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m2.1b"><ci id="S4.T1.2.2.2.m2.1.1.cmml" xref="S4.T1.2.2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m2.1c">\alpha</annotation></semantics></math>: {1, 4, 16}</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The search range values in LoRA fine-tuning.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present the results on JSQuAD.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Automatic Evaluation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our primary interest lies in examining the impact of each strategy for synthesizing QA pairs on the performance of the downstream question answering task.
Specifically, we focus on comparisons involving different contexts, prompts, and the quantities of automatically generated QA pairs.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Automatic Evaluation ‣ 5 Results ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the scores of BERTScore and BLEU obtained by varying the contexts while keeping other settings, i.e., <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">N</annotation></semantics></math> and prompts are fixed.
The table is divided into five sections.
Starting from the top, the first section displays scores for QA models trained on human-authored QA pairs (<span id="S5.SS1.p2.3.1" class="ltx_text ltx_font_typewriter">Human</span>) from the JSQuAD training dataset, along with the plain GPT model (<span id="S5.SS1.p2.3.2" class="ltx_text ltx_font_typewriter">GPT</span>) without fine-tuning.
The second and third sections showcase scores obtained when <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">N</annotation></semantics></math> is fixed to one, but we vary the prompts to zero-shot and one-shot.
The fourth and fifth sections represent scores when we use <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">N</mi><mo id="S5.SS1.p2.3.m3.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><eq id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1"></eq><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">N=3</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.3" class="ltx_p"><span id="S5.SS1.p3.3.1" class="ltx_text ltx_font_bold">Impact of Context on Performance: </span>
We observe that using contexts extracted from the <span id="S5.SS1.p3.3.2" class="ltx_text ltx_font_typewriter">news</span> dataset yields relatively low scores, e.g., 0.713 and 0.747 in terms of BERTScore for zero-shot and one-shot settings with <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">N</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><eq id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></eq><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">N=3</annotation></semantics></math>, respectively.
The <span id="S5.SS1.p3.3.3" class="ltx_text ltx_font_typewriter">wiki</span> context performs better (0.706 and 0.838) than <span id="S5.SS1.p3.3.4" class="ltx_text ltx_font_typewriter">news</span> (0.713 and 0.747) for the same settings.
Notably, the <span id="S5.SS1.p3.3.5" class="ltx_text ltx_font_typewriter">JSQuAD</span> context achieves the highest BERTScore of 0.863 and 0.889 with <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">N</mi><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><eq id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></eq><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">N=1</annotation></semantics></math> and <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">N</mi><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><eq id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1"></eq><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">N=3</annotation></semantics></math>, respectively.
The results suggest that using Wikipedia as context provides an advantage, likely because the JSQuAD evaluation data is also derived from Wikipedia.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.2" class="ltx_p"><span id="S5.SS1.p4.2.1" class="ltx_text ltx_font_bold">Impact of Prompts on Performance: </span>
The one-shot prompt is more effective.
As shown in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Automatic Evaluation ‣ 5 Results ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the model fine-tuned on the zero-shot QA pairs (<math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mi id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">N</mi><mo id="S5.SS1.p4.1.m1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><eq id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1"></eq><ci id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">N=1</annotation></semantics></math>) generated from the contexts in <span id="S5.SS1.p4.2.2" class="ltx_text ltx_font_typewriter">JSQuAD</span> training dataset achieves a BERTScore of 0.724.
However, the one-shot prompts with <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mrow id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml"><mi id="S5.SS1.p4.2.m2.1.1.2" xref="S5.SS1.p4.2.m2.1.1.2.cmml">N</mi><mo id="S5.SS1.p4.2.m2.1.1.1" xref="S5.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.2.m2.1.1.3" xref="S5.SS1.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><apply id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1"><eq id="S5.SS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1.1"></eq><ci id="S5.SS1.p4.2.m2.1.1.2.cmml" xref="S5.SS1.p4.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S5.SS1.p4.2.m2.1.1.3.cmml" xref="S5.SS1.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">N=1</annotation></semantics></math> exhibit a significant performance gain, reaching a BERTScore of 0.863.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Effect of the Number of Generated QA Pairs on Performance: </span>
As we increase the number of QA pairs for context, there is a gain of 2.6 points in BERTScore (from 0.863 to 0.889).
Remarkably, the achieved BERTScore of 0.889 is comparable to that of a model trained on human-authored QA pairs (0.899), despite our approach not utilizing any human-authored QA pairs.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1" class="ltx_tr">
<th id="S5.T2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">context</th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">N</annotation></semantics></math></th>
<th id="S5.T2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">prompt</th>
<th id="S5.T2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">BERTscore</th>
<th id="S5.T2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T2.1.2.1.1.1" class="ltx_text ltx_font_typewriter">Human</span></th>
<th id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">-</th>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.899</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">5.64</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GPT</th>
<th id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.601</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_center">0.00</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">news</th>
<th id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">1</th>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">zero</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.697</td>
<td id="S5.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_tt">0.02</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">wiki</th>
<th id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1</th>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">zero</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.713</td>
<td id="S5.T2.1.5.4.5" class="ltx_td ltx_align_center">0.03</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<th id="S5.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">JSQuAD</th>
<th id="S5.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1</th>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">zero</td>
<td id="S5.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.724</td>
<td id="S5.T2.1.6.5.5" class="ltx_td ltx_align_center">1.55</td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<th id="S5.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">news</th>
<th id="S5.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S5.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">one</td>
<td id="S5.T2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.738</td>
<td id="S5.T2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<th id="S5.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">wiki</th>
<th id="S5.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1</th>
<td id="S5.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">one</td>
<td id="S5.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">0.775</td>
<td id="S5.T2.1.8.7.5" class="ltx_td ltx_align_center">0.09</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<th id="S5.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">JSQuAD</th>
<th id="S5.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1</th>
<td id="S5.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">one</td>
<td id="S5.T2.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">0.863</td>
<td id="S5.T2.1.9.8.5" class="ltx_td ltx_align_center">4.83</td>
</tr>
<tr id="S5.T2.1.10.9" class="ltx_tr">
<th id="S5.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">news</th>
<th id="S5.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S5.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">zero</td>
<td id="S5.T2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.713</td>
<td id="S5.T2.1.10.9.5" class="ltx_td ltx_align_center ltx_border_t">0.38</td>
</tr>
<tr id="S5.T2.1.11.10" class="ltx_tr">
<th id="S5.T2.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">wiki</th>
<th id="S5.T2.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S5.T2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r">zero</td>
<td id="S5.T2.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r">0.706</td>
<td id="S5.T2.1.11.10.5" class="ltx_td ltx_align_center">0.23</td>
</tr>
<tr id="S5.T2.1.12.11" class="ltx_tr">
<th id="S5.T2.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">JSQuAD</th>
<th id="S5.T2.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S5.T2.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r">zero</td>
<td id="S5.T2.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r">0.740</td>
<td id="S5.T2.1.12.11.5" class="ltx_td ltx_align_center">1.85</td>
</tr>
<tr id="S5.T2.1.13.12" class="ltx_tr">
<th id="S5.T2.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">news</th>
<th id="S5.T2.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3</th>
<td id="S5.T2.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">one</td>
<td id="S5.T2.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.747</td>
<td id="S5.T2.1.13.12.5" class="ltx_td ltx_align_center ltx_border_t">1.25</td>
</tr>
<tr id="S5.T2.1.14.13" class="ltx_tr">
<th id="S5.T2.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">wiki</th>
<th id="S5.T2.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">3</th>
<td id="S5.T2.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r">one</td>
<td id="S5.T2.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r">0.838</td>
<td id="S5.T2.1.14.13.5" class="ltx_td ltx_align_center">1.66</td>
</tr>
<tr id="S5.T2.1.15.14" class="ltx_tr">
<th id="S5.T2.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">JSQuAD</th>
<th id="S5.T2.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">3</th>
<td id="S5.T2.1.15.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">one</td>
<td id="S5.T2.1.15.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T2.1.15.14.4.1" class="ltx_text ltx_font_bold">0.889</span></td>
<td id="S5.T2.1.15.14.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.1.15.14.5.1" class="ltx_text ltx_font_bold">6.77</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performances on different contexts and numbers of generated QA pairs.</figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">QA Pairs</th>
<th id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_typewriter">JSQuAD (<math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.m1.1.1.2.cmml">N</mi><mo id="S5.T3.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.T3.1.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1"><eq id="S5.T3.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1.1"></eq><ci id="S5.T3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S5.T3.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">N=3</annotation></semantics></math>, one-shot prompt)</span></td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">45.4</span></td>
</tr>
<tr id="S5.T3.1.3.1" class="ltx_tr">
<td id="S5.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.1.1.1" class="ltx_text ltx_font_typewriter">Human</span></td>
<td id="S5.T3.1.3.1.2" class="ltx_td ltx_align_center">38.4</td>
</tr>
<tr id="S5.T3.1.4.2" class="ltx_tr">
<td id="S5.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Gold</td>
<td id="S5.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">90.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy calculated as the number of correct question-context-answer tuples divided by the total 500 evaluation instances.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation by Human Judges: </h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We present the results of the manual evaluation.
Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Automatic Evaluation ‣ 5 Results ‣ Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the comparisons between three outputs: answers generated by 1) our best performing model (<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">JSQuAD</span> (<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="N=3" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">N</mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><eq id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></eq><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">N=3</annotation></semantics></math>), and one-shot prompt) and 2) a model that is fine-tuned on human-authored QA pairs from the JSQuAD training dataset, and 3) gold answers in JSQuAD evaluation dataset.
Remarkably, despite our approach does not use any human-authored QA pairs, the achieved accuracy is 45.4% while the model fine-tuned on human-authored QA pairs achieves only 38.4% in terms of accuracy.
<cite class="ltx_cite ltx_citemacro_citet">Gilardi et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> mention that automatic annotation with an instructor-tuning model has higher quality than annotations by crowd-workers, and our results are consistent with their claim.
Note that the performance of both fine-tuned models falls significantly behind the Gold standard (90.4%), indicating ample room for improvement.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper proposed to use an instruction-tuned model for synthesizing QA pairs.
Our experimental results demonstrate that the models trained on automatically generated QA pairs achieve comparable or even superior performance compared to the fine-tuned model trained on human-authored QA pairs.
In future studies, we plan to explore the relationship between the diversity of automatically generated QA pairs and their impact on the performance of downstream QA tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al. (2022)</span>
<span class="ltx_bibblock">
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao,
Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds,
Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.bigscience-1.9" title="" class="ltx_ref ltx_href">GPT-NeoX-20B: An open-source autoregressive language model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of BigScience Episode #5 – Workshop on
Challenges &amp; Perspectives in Creating Large Language Models</em>, pages
95–136, virtual+Dublin. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2019)</span>
<span class="ltx_bibblock">
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao,
Ming Zhou, and Hsiao-Wuen Hon. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf" title="" class="ltx_ref ltx_href">Unified language model pre-training for natural language understanding and
generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 32. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al. (2023)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and MaÃ«l Kubli. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1073/pnas.2305016120" title="" class="ltx_ref ltx_href">Chatgpt outperforms
crowd workers for text-annotation tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>,
120(30):e2305016120.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank
adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurihara et al. (2022)</span>
<span class="ltx_bibblock">
Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.lrec-1.317" title="" class="ltx_ref ltx_href">JGLUE: Japanese
general language understanding evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirteenth Language Resources and
Evaluation Conference</em>, pages 2957–2966, Marseille, France. European
Language Resources Association.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2020)</span>
<span class="ltx_bibblock">
Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, and Sung Ju Hwang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.20" title="" class="ltx_ref ltx_href">Generating
diverse and consistent QA pairs from contexts with information-maximizing
hierarchical conditional VAEs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 208–224, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al. (2022)</span>
<span class="ltx_bibblock">
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang,
Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and
Samuel Bowman. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.391" title="" class="ltx_ref ltx_href">QuALITY:
Question answering with long input texts, yes!</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 5336–5358, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W18-6319" title="" class="ltx_ref ltx_href">A call for clarity in
reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186–191, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puri et al. (2020)</span>
<span class="ltx_bibblock">
Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, and Bryan Catanzaro.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.468" title="" class="ltx_ref ltx_href">Training
question answering models from synthetic data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 5811–5826, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the limits
of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1264" title="" class="ltx_ref ltx_href">SQuAD: 100,000+
questions for machine comprehension of text</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2383–2392, Austin, Texas. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan and Xing (2018)</span>
<span class="ltx_bibblock">
Mrinmaya Sachan and Eric Xing. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-1058" title="" class="ltx_ref ltx_href">Self-training for
jointly learning to ask and answer questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 629–640, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2018)</span>
<span class="ltx_bibblock">
Duyu Tang, Nan Duan, Zhao Yan, Zhirui Zhang, Yibo Sun, Shujie Liu, Yuanhua Lv,
and Ming Zhou. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-1141" title="" class="ltx_ref ltx_href">Learning to collaborate
for question answering and asking</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1564–1574, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
2020.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.08071" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.08072" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.08072">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.08072" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.08073" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 23:07:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
