<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.07491] HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning</title><meta property="og:description" content="Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concern‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.07491">

<!--Generated on Wed Mar 13 19:18:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\onlineid</span>
<p id="p1.2" class="ltx_p">1338
<span id="p1.2.1" class="ltx_ERROR undefined">\vgtccategory</span>Research
<span id="p1.2.2" class="ltx_ERROR undefined">\vgtcpapertype</span>Applications



<span id="p1.2.3" class="ltx_ERROR undefined">\authorfooter</span>
Xumeng Wang is with TMCC, CS, Nankai University. E-mail: wangxumeng@nankai.edu.cn.
Wei Chen, Zhen Wen, and Rongchen Zhu are with State Key Lab of CAD&amp;CG, Zhejiang University. E-mail: {chenvis, whenzhen, zrcccrz}@zju.edu.cn.
Wei Chen is also with Laboratory of Art and Archaeology Image (Zhejiang University), Ministry of Education, China.
Jiazhi Xia is with School of Computer Science and Engineering, Central South University. E-mail: xiajiazhi@csu.edu.cn.
Tobias Schreck is with Graz University of Technology, Austria. E-mail: tobias.schreck@cgv.tugraz.at.
Wei Chen and Jiazhi Xia are the corresponding authors.

</p>
</div>
<h1 class="ltx_title ltx_title_document">HetVis: A Visual Analysis Approach for Identifying
<br class="ltx_break">Data Heterogeneity in Horizontal Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xumeng Wang
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Wei Chen
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jiazhi Xia
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zhen Wen
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Rongchen Zhu
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tobias Schreck
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="1.1" class="ltx_p">Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Federated learning; data heterogeneity; cluster analysis, visual analysis.
</div>
<div id="p2" class="ltx_para">
<span id="p2.2" class="ltx_ERROR undefined">\CCScatlist</span><span id="p2.3" class="ltx_ERROR undefined">\CCScat</span>
<p id="p2.1" class="ltx_p">K.6.1Management of Computing and Information SystemsProject and People ManagementLife Cycle;
<span id="p2.1.1" class="ltx_ERROR undefined">\CCScat</span>K.7.mThe Computing ProfessionMiscellaneousEthics

<span id="p2.1.2" class="ltx_ERROR undefined">\teaser</span>
<img src="/html/2208.07491/assets/x1.png" id="p2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
<span id="p2.1.3" class="ltx_text ltx_caption ltx_align_center">The interface of <span id="p2.1.3.1" class="ltx_text ltx_font_italic">HetVis</span> for analyzing the heterogeneity issues among Federated Learning cooperation from the perspective of a participating client. (a) The module of federated learning process observation consists of an information panel introducing the federated model and the local client, the parameter projection view depicting the evolution of the disagreements between the local client and the integrated model, and a performance monitor view recording model performance and users‚Äô annotations. (b) The module of model output comparison identifies and clusters inconsistent records between the output of the federated learning model and the stand-alone training model. (c) The module of heterogeneity examination allows users to analyze an inconsistent cluster by observing the distribution and inspecting instances. Findings can be annotated to the cluster in the control panel.</span>


<span id="p2.1.4" class="ltx_ERROR undefined">\vgtcinsertpkg</span>
Introduction</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">In the big data era, data is often distributed among many isolated data owners. It is an urgent and challenging need for the data owners to benefit from data integration, e.g., to train high-quality models, while also respecting privacy concerns. To satisfy this requirement, federated learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> (FL) keeps data locally in clients when training a shared model. Only encrypted parameters are shared with other clients. Especially, horizontal federated learning (HFL), which integrates data from the same feature space but distributed at different clients, has been widely used in privacy-aware applications, like healthcare and mobile service¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">A key and common challenge of HFL is the <em id="p4.1.1" class="ltx_emph ltx_font_italic">heterogeneity</em> of data distribution. The efficiency of current HFL techniques depends on the assumption that data distribution in different clients is independent and identically distributed (IID). However, the IID assumption usually does not hold. Non-IID data is widely used in model training to increase sample size and leads to difficulties in convergence¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Figuring out their existence is necessary for applications of HFL. Specifically, data heterogeneity can exert both negative impacts and positive impacts on the accuracy of trained models. <span id="p4.1.2" class="ltx_text ltx_font_italic">Statistical heterogeneity</span>, meaning that distributed data cover the same classes but different proportions, can be positive. For example, one client has insufficient samples in one class while the other clients have adequate samples in the same class. In this case, multiple clients are facilitated in the collaboration. <span id="p4.1.3" class="ltx_text ltx_font_italic">Label heterogeneity</span> means that similar instances are labeled differently in different clients. The divergence in label settings usually affects model training negatively. Therefore, it is critical for participating clients to diagnose the data heterogeneity in the training process and fine-tune the model and data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. We identify two analysis goals in heterogeneity: (1) detecting the existence of data heterogeneity and (2) knowing the impacts of data heterogeneity. For example, hospitals collect patient data for disease analysis, and search engines record user logs for advertising recommendations.</p>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">However, analyzing the data heterogeneity in HFL is highly challenging. Traditional approaches identify heterogeneity through comparing data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Due to the privacy issue, a client is prevented from accessing data owned by other clients directly. Direct comparison between local data and global data is infeasible.
The HFL model is the only intelligence fed back to the client by the cooperation.
As it is trained with the global model with a HFL framework, the global data is learned by the HFL model.
There are two challenges in analyzing data heterogeneity based on the HFL model.
First, although a lot of works have been dedicated to understanding deep learning models by input and output data, the inverse workflow of understanding the unavailable training data by the model is still an open problem.
Second, the behavior of the HFL model depends on multiple factors, including the data and model architectures. How to distinguish the effect of training data from other factors is also challenging.
An encompassing analysis workflow is still missing for HFL analysis.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">To fill the above research gap, we present a visual analytics tool for client analysts to detect and understand data heterogeneity in HFL under the privacy limitation. We leverage a contrastive analysis approach to locate heterogeneous records and examine them. To overcome the privacy limitation, we propose to train a stand-alone model based on local data and compare it with the HFL model. Because data is learned by a model in the training process, the comparison between the two models discloses the heterogeneity between the local data and global data. We further propose a cluster analysis method based on rank-based distance measurement to distinguish the impacts from different group of heterogeneous records. We developed an interactive interface, called <span id="p6.1.1" class="ltx_text ltx_font_italic">HetVis</span>, in conjunction with our analysis approach to monitor, explore, and understand the models and instances. A comparative study is implemented to show that the proposed cluster analysis method is more helpful for analyzing heterogeneity than hierarchical clustering. Three case studies were designed to demonstrate the effectiveness of our interface when analyzing different kinds of data heterogeneity. We also collected informative reviews from three experts in FL, confirming the effectiveness of <span id="p6.1.2" class="ltx_text ltx_font_italic">HetVis</span>.</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">In summary, the contributions of this work are:</p>
<ul id="S0.I1" class="ltx_itemize">
<li id="S0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i1.p1" class="ltx_para">
<p id="S0.I1.i1.p1.1" class="ltx_p">A novel visual analysis workflow that assists to analyze the heterogeneity in HFL models.</p>
</div>
</li>
<li id="S0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i2.p1" class="ltx_para">
<p id="S0.I1.i2.p1.1" class="ltx_p">A context-aware clustering approach that hierarchically generates clusters of data which are inconsistent with others.</p>
</div>
</li>
<li id="S0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i3.p1" class="ltx_para">
<p id="S0.I1.i3.p1.1" class="ltx_p">A visual system that integrates a suite of novel designs.</p>
</div>
</li>
</ul>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Related Work</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In this section, related works are summarized from three aspects: heterogeneity in distributed learning, visualization for model diagnosis, and visualization for model comparison.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Heterogeneity in Distributed Learning</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The problem of data heterogeneity has put forward challenges for integrating distributed data into joint or public knowledge. Related scenarios mainly include distributed learning and statistical analysis.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">In the first scenario, heterogeneity issues, including state heterogeneity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, hardware heterogeneity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, statistical heterogeneity, and label heterogeneity, lead to convergence problems of the global model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In this paper, we focus on identifying data heterogeneity issues (i.e., statistical heterogeneity and label heterogeneity). Related studies leverage automatic solutions to improve model robustness. FedProx¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> modifies the weight updating algorithm to limit the impact of local updates on the global model. This approach, however, can hardly address the label heterogeneity problem, because the trained model can be meaningless if the definition of labels varies for different clients. Another solution is to train separated models instead of a global one. Separated models can learn from each other based on state-of-the-art frameworks, such as meta-learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and multi-task learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. However, these automatic solutions lack the ability to analyze heterogeneity and therefore miss the chance to optimize models by means, like managing distributed data.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">In the second scenario, meta-analysis¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is proposed as a methodology to discriminate, combine and summarize multiple statistical analysis results. In the discrimination stage, statistical tests are leveraged to assess the significance of heterogeneity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. If the heterogeneity is significant, the statistical analysis results cannot be integrated directly¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. A feasible solution is to locate and exclude the variables or moderators that result in the heterogeneity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. However, these approaches are not practical for FL due to the privacy limitation.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">In this paper, we leverage limited available information (the parameters and the output of HFL models) to facilitate heterogeneity detection and examination, so that users can better understand the significance of heterogeneity issues.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Visualization for Model Diagnosis</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Approaches for model diagnosis mainly fall into three categories:
monitoring the model performance fluctuation, inspecting model configuration, and leveraging instance-level analysis.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Visualization techniques aim to provide an overview of the model evolution. In existing applications, the performance of the model (e.g., loss) is frequently recorded and represented as time series¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. We also show the performance dynamics in our system to assist users in judging the convergence and performance of the federated model.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">To reason why the performance fluctuates, users need to further inspect the model configuration. DeepEyes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> allows users to check details of detected stable layers in deep neural networks through three linked views, which depict activations, instances, and filters to judge if a layer is oversized or unnecessary. GANViz¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> compares image features from the dimension of time. The comparison results reflect the impact of a feature during the model training process. DGMTracker¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> applies a credit assignment algorithm to locate the neurons that contribute to the failure¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, various models with distinctive structures can be trained based on the federated architecture. We focus on the HFL process, namely the exchange of parameters between the local client and the server, from which the disagreements between the local client and the others can be studied.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">Testing output requires a lower learning cost than model interpretation, which is more acceptable for domain experts. To diagnose and improve the model, failed cases should be emphasized. The What-If-Tool¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> allows users to customize the input of models and learn the mechanics of models by comparing related outputs. RetainVis¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> allows users to modify the input of an RNN model and figure out why a record is classified incorrectly. Krause et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> provide instance-level explanations to verify the effectiveness of features based on a single instance. However, none of the above studies consider data heterogeneity due to the difference between application scenarios.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Visualization for Model Comparison</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Existing studies on model comparison mostly aim at selecting the best model, which requires to compare model performance. Instance-level analysis can be leveraged to facilitate the understanding of model behaviors. DeepCompare¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> groups instances by the combination of classification results of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) model. The group information indicates where the two models disagree with each other. Users can start exploration from the instances that are misclassified by one or two models. The neuron activation pattern of the user-selected instance can be compared through a heatmap. However, models can achieve the same label with a different confidence¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Manifold¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> employs scatterplots with color encodings to show the overview of instance-level details, which are the confidence of the model pair and the ground-truth label.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">Model comparison can also offer an in-depth understanding of the data. On the one hand, the output of the model reflects the underlying characteristics of the data. Employing multiple models allows users to learn data from multiple perspectives. Alexander and Gleicher¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> compare the results of two topic models to illustrate the consistency of documents. They introduce two visual encodings to represent the outputs of the two models, respectively. PK-clustering¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> compares the results of multiple clustering algorithms to reduce uncertainty in prior knowledge. The trend patterns can highlight inconsistencies. ConceptExplorer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
compares the performance fluctuation of the online learning models trained with different time-series datasets. Users can judge if those data have experienced consistent evolution based on the proposed drift-level index. Similarly, we attempt to comprehend the heterogeneity between the local data and the data distributed on the other clients by comparing the stand-alone training model with the HFL model.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approach Designs</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first introduce the background of horizontal federated learning and describe the design requirements.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Horizontal Federated Learning Architecture</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The typical architecture of horizontal federated learning (HFL) consists of a set of <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">clients</span> who own local data and a <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">server</span> which hosts the exchange of parameters among clients. At the beginning of the training, the model settings are unified for initialization. After that, cooperated training performs iteratively. Each communication round includes four steps. First, each client receives centralized parameters from the server. Second, each client updates the parameter by training and testing with local data. Then, each local update is submitted to the server. For privacy concerns, parameters can be encrypted¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> or processed by differential privacy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Third, the server integrates the parameters from the clients by aggregation algorithms. (We employ FedAvg¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as default aggregation algorithms in this work.) Finally, the server sends the integrated results back, based on which each client updates the parameter of their own model and gets ready for the next round. In conclusion, each client can only access the parameters of the HFL model in the entire learning process.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Requirements Analysis</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Because clients cannot exchange the raw data, the primary issue of designing a visual diagnosis tool for data heterogeneity is that a direct comparison among data from different clients is infeasible. We interviewed two FL experts (E1 and E2) to define requirements. Both of them are senior machine learning researchers who have studied FL for more than 3 years and published related papers. They said that they have no effective solutions to deal with heterogeneity issues caused by heterogeneous data. Although a visualization system¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is proposed to inspect the training process of the HFL, a tool for heterogeneity exploration is still missing. Without the understanding of data heterogeneity, it is challenging to remove heterogeneity issues thoroughly. Therefore, the experts agree that an interactive visualization tool to examine the data heterogeneity is critical for real applications.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To identify the major requirements, questions are asked to two FL experts: Without the prior knowledge of data from other clients, how can you be aware of the existence of data heterogeneity? What kind of information is helpful in identifying and understanding the data heterogeneity?</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Based on the interviews and literature review, we identify three requirements.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">R1. Knowing the existence of data heterogeneity.</span>
Both experts pointed out that it is critical to know the occurrence of data heterogeneity during the model building, so that the investigation of data heterogeneity can be performed at appropriate rounds.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">R1.1 Tracking the dynamic performance of the federated model.</span></p>
</div>
<div id="S2.I1.i1.p2" class="ltx_para">
<p id="S2.I1.i1.p2.1" class="ltx_p">Data heterogeneity will hinder the convergence of the federated model and harm its performance.
The fluctuation of performance metrics, such as loss, is a good indicator of the training process¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Tracking them helps to know the dynamics of the training process and further locate the rounds when heterogeneity issues occur and affect the federated model.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">R1.2 Inspecting the parameter conflicts between the client model and federated model.</span>
The federated model iteratively integrates model parameters from client models. The data heterogeneity will lead to differences in model parameters submitted by different clients. Therefore, the significant parameter conflicts are also a signal of possible data heterogeneity. Due to the number of model parameters, users need an efficient analysis manner to inspect the differences.</p>
</div>
</li>
</ul>
<p id="S2.SS2.p4.2" class="ltx_p"><span id="S2.SS2.p4.2.1" class="ltx_text ltx_font_bold">R2. Comparing data in terms of model behaviors.</span>
Both experts agreed that the most critical barrier before investigating the data heterogeneity is the data isolation in FL. The only feedback information from other clients is the federated model. Instead of comparing data directly, E1 suggested that we can compare prediction behaviors of the models trained with the data. Because a model has learned from training data in the training process, based on which the model can make predictions for other inputs.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2208.07491/assets/x2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="917" height="371" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The three-stage workflow for analyzing heterogeneity issues in a federated cooperation from the perspective of a client.</figcaption>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">R2.1 Training a local model with the local data.</span>
The HFL model learns from global data during the training process. However, the client model learns more than the local data because the client model is updated with centralized parameters (see Sec¬†<a href="#S2.SS1" title="2.1 Horizontal Federated Learning Architecture ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). We need a local model whose training process employs local data only. Besides, to exclude impacts of model designs, the local model should be trained in the same architecture (e.g., CNN models in the same network structure) as the HFL model.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">R2.2 Testing model output with various inputs.</span>
Comparing model output is a common practice to learn the differences in prediction behaviors of two models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. For a comprehensive comparison, we should provide inputs that can cover the corresponding data space.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p"><span id="S2.SS2.p6.1.1" class="ltx_text ltx_font_bold">R3. Supporting visual examination of heterogeneity issues.</span>
Visual analysis approaches can facilitate users in understanding heterogeneity issues and reasoning their impacts.</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text ltx_font_italic">R3.1 Identify heterogeneity issues based on inconsistent outputs.</span>
E2 said that clustering records with inconsistent model outputs can facilitate in identification of heterogeneity issues. According to the theories in meta-analysis¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, researchers need to deal with heterogeneity issues based on their significance. After clustering records, the significance of heterogeneity can be observed from the size of each cluster. If the ground-truth label is available, we can further calculate the prediction accuracy of the HFL model for the records in each cluster and assist client analysts to judge whether the impact of the corresponding heterogeneity issue is positive or negative.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text ltx_font_italic">R3.2: Reason the impact of heterogeneity issues by individual records.</span>
Overview of a group can facilitate analysts to come up with hypotheses. A hypothesis could be ‚Äútwo models strongly disagree in the classification of records with certain features.‚Äù Analysts can realize significant data heterogeneity and infer that the federated model may yield wrong judgments for records with these features, according to the verified hypothesis. To make verification, the experts would like an instance-level visualization to access the details, such as the images and labels.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p"><span id="S2.I3.i3.p1.1.1" class="ltx_text ltx_font_italic">R3.3 Tracking the identified issues during the training process.</span> During the training process, the classification results for certain records may update. An intermediate result of a certain communication round may be randomly influenced (e.g., the training data included outliers accidentally), even after the model has converged. To draw a firm conclusion, analysts need to collect suspected issues and track them in the following communication rounds. Besides, analysts can observe what problems HFL faces and whether it can solve them by tracking the training process. It is significant to evaluate the effectiveness and robustness of the HFL model.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Workflow</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To support users, i.e., client analysts, analyzing heterogeneity issues, we propose a three-stage workflow (see Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>): 1) monitoring the HFL process, 2) comparing model output, and 3) examining heterogeneity.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Monitoring the Learning Process</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the first stage, users need to learn the model configurations of the global HFL model and observe the training process. Model configurations include the choice of aggregation algorithms, the number of clients, and the description of the local data. The training process of HFL can be described by performance fluctuation (<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">R1.1</span>) and the exchanges of model parameters between the local client and the server (<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">R1.2</span>, Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b)). Dramatic performance fluctuations or continuous conflicts of model parameters may indicate unsatisfying global data, which can motivate in-depth analysis for heterogeneity issues. The following analysis needs to be implemented based on a static model. To specify the intermediate HFL model, users need to select a communication round according to the observation results of the training process (Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c)).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Comparing Model Output</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the second stage, users need to indirectly compare global data and the local data by comparing prediction behaviors of the two models trained respectively. We employ local data to train a stand-along training model in the same model architecture with the global HFL model (<span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">R2.1</span>). The stand-along training model is used to compare with the HFL model, which is trained with global data. To collect various outputs, we provide three sets of data records as input (<span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">R2.2</span>, Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d)), which are local data and two automatically generated datasets (please refer to Section¬†<a href="#S4.SS1" title="4.1 Input Generation from the Data Space ‚Ä£ 4 Models ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> for details). The records with inconsistent outputs (records classified as different labels by the two models), or we say in the following, <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">inconsistent records</span>, are then identified (Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(e)). Users can leverage a cluster analysis method (<span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">R3.1</span>, Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(f), Section¬†<a href="#S4.SS2" title="4.2 Context-aware Clustering Approach ‚Ä£ 4 Models ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) to generate <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">inconsistency clusters</span> from inconsistent records.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Examining Heterogeneity</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the third stage, users examine an inconsistency cluster to study the corresponding heterogeneity issue and understand the impacts of the heterogeneity issue. As shown in Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(g), users can check data space characteristics of the records in the selected inconsistency cluster. Related findings inspire users to come up with hypotheses on suspicious heterogeneity. To make tentative verification, users can select certain records in the inconsistency cluster and browse record details (Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(h)). Records with ground-truth labels can facilitate users to judge whether the heterogeneity issue leads to a higher accuracy than the stand-alone training model (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">R3.2</span>).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">As shown in Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Requirements Analysis ‚Ä£ 2 Approach Designs ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(i), users can annotate the records with suspicious heterogeneity issues and track these records in the following training process (<span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">R3.3</span>). If the negative impacts of these records is weakened or even disappears, users can consider that the HFL models can overcome such heterogeneity. If not, this indicates users should pay attention to possible heterogeneity issues, and reassess the cooperation of FL.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Models</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we introduce the two models employed in our system.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Input Generation from the Data Space</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We generate a representative input that distributes all over the data space for a comprehensive test of output comparison. Because the original data space is often of too high dimensionality to perform an efficient sampling, we assume that the data is distributed in a low-dimensional subspace. Therefore, we perform PCA on the data to get the low-dimensional space. After that, we employ stratified sampling, which is more efficient and effective than random sampling, to sample inputs in the low-dimensional space.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The low-dimensional samples are then projected back to high-dimensional space to satisfy the input format. To avoid illegal input, range interception is implemented according to the definition of each dimension. For instance, in a handwritten digit dataset, the grayscale of a pixel is regarded as a dimension whose threshold is from 0 to 255. If a dimension is restored as 260, we correct it as 255 for legality and validity.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Context-aware Clustering Approach</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">It is time-consuming to browse each inconsistent record from the high-dimensional space to compile a summary on heterogeneity issues. We, therefore, leverage a cluster analysis to organize inconsistent records and analyze heterogeneity. Observing clustering results with different settings of the cluster number can facilitate users to generalize heterogeneity issues from different levels. To support flexible adjustment of the cluster number, we employ hierarchical clustering to group inconsistent records.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.12" class="ltx_p">However, inconsistent clusters may be mixed up with consistent records, which hardly contribute to extraction of heterogeneity issues. To exclude consistent records from inconsistency clusters, we have to consider the context of consistent records surrounding inconsistent records when constructing hierarchical clustering. Therefore, we change the distance measurement in the hierarchical clustering from Euclidean measurement to a rank-based measurement. We denote the set of all records as <math id="S4.SS2.p2.1.m1.5" class="ltx_Math" alttext="\{s_{i}|i=1,\dots,n\}" display="inline"><semantics id="S4.SS2.p2.1.m1.5a"><mrow id="S4.SS2.p2.1.m1.5.5.2" xref="S4.SS2.p2.1.m1.5.5.3.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.5.5.2.3" xref="S4.SS2.p2.1.m1.5.5.3.1.cmml">{</mo><msub id="S4.SS2.p2.1.m1.4.4.1.1" xref="S4.SS2.p2.1.m1.4.4.1.1.cmml"><mi id="S4.SS2.p2.1.m1.4.4.1.1.2" xref="S4.SS2.p2.1.m1.4.4.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.1.m1.4.4.1.1.3" xref="S4.SS2.p2.1.m1.4.4.1.1.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.2.4" xref="S4.SS2.p2.1.m1.5.5.3.1.cmml">|</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.cmml"><mi id="S4.SS2.p2.1.m1.5.5.2.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.2.cmml">i</mi><mo id="S4.SS2.p2.1.m1.5.5.2.2.1" xref="S4.SS2.p2.1.m1.5.5.2.2.1.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.5.5.2.2.3.2" xref="S4.SS2.p2.1.m1.5.5.2.2.3.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">1</mn><mo id="S4.SS2.p2.1.m1.5.5.2.2.3.2.1" xref="S4.SS2.p2.1.m1.5.5.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">‚Ä¶</mi><mo id="S4.SS2.p2.1.m1.5.5.2.2.3.2.2" xref="S4.SS2.p2.1.m1.5.5.2.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.3.3" xref="S4.SS2.p2.1.m1.3.3.cmml">n</mi></mrow></mrow><mo stretchy="false" id="S4.SS2.p2.1.m1.5.5.2.5" xref="S4.SS2.p2.1.m1.5.5.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.5b"><apply id="S4.SS2.p2.1.m1.5.5.3.cmml" xref="S4.SS2.p2.1.m1.5.5.2"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.5.5.3.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.3">conditional-set</csymbol><apply id="S4.SS2.p2.1.m1.4.4.1.1.cmml" xref="S4.SS2.p2.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.1.1.1.cmml" xref="S4.SS2.p2.1.m1.4.4.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.1.1.2.cmml" xref="S4.SS2.p2.1.m1.4.4.1.1.2">ùë†</ci><ci id="S4.SS2.p2.1.m1.4.4.1.1.3.cmml" xref="S4.SS2.p2.1.m1.4.4.1.1.3">ùëñ</ci></apply><apply id="S4.SS2.p2.1.m1.5.5.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2"><eq id="S4.SS2.p2.1.m1.5.5.2.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.1"></eq><ci id="S4.SS2.p2.1.m1.5.5.2.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.2">ùëñ</ci><list id="S4.SS2.p2.1.m1.5.5.2.2.3.1.cmml" xref="S4.SS2.p2.1.m1.5.5.2.2.3.2"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">1</cn><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">‚Ä¶</ci><ci id="S4.SS2.p2.1.m1.3.3.cmml" xref="S4.SS2.p2.1.m1.3.3">ùëõ</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.5c">\{s_{i}|i=1,\dots,n\}</annotation></semantics></math>. Among them, there exist <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">m</annotation></semantics></math> inconsistent records, which are <math id="S4.SS2.p2.3.m3.5" class="ltx_Math" alttext="\{s_{i}|i=1,\dots,m\}" display="inline"><semantics id="S4.SS2.p2.3.m3.5a"><mrow id="S4.SS2.p2.3.m3.5.5.2" xref="S4.SS2.p2.3.m3.5.5.3.cmml"><mo stretchy="false" id="S4.SS2.p2.3.m3.5.5.2.3" xref="S4.SS2.p2.3.m3.5.5.3.1.cmml">{</mo><msub id="S4.SS2.p2.3.m3.4.4.1.1" xref="S4.SS2.p2.3.m3.4.4.1.1.cmml"><mi id="S4.SS2.p2.3.m3.4.4.1.1.2" xref="S4.SS2.p2.3.m3.4.4.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.3.m3.4.4.1.1.3" xref="S4.SS2.p2.3.m3.4.4.1.1.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.5.5.2.4" xref="S4.SS2.p2.3.m3.5.5.3.1.cmml">|</mo><mrow id="S4.SS2.p2.3.m3.5.5.2.2" xref="S4.SS2.p2.3.m3.5.5.2.2.cmml"><mi id="S4.SS2.p2.3.m3.5.5.2.2.2" xref="S4.SS2.p2.3.m3.5.5.2.2.2.cmml">i</mi><mo id="S4.SS2.p2.3.m3.5.5.2.2.1" xref="S4.SS2.p2.3.m3.5.5.2.2.1.cmml">=</mo><mrow id="S4.SS2.p2.3.m3.5.5.2.2.3.2" xref="S4.SS2.p2.3.m3.5.5.2.2.3.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">1</mn><mo id="S4.SS2.p2.3.m3.5.5.2.2.3.2.1" xref="S4.SS2.p2.3.m3.5.5.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">‚Ä¶</mi><mo id="S4.SS2.p2.3.m3.5.5.2.2.3.2.2" xref="S4.SS2.p2.3.m3.5.5.2.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.3.m3.3.3" xref="S4.SS2.p2.3.m3.3.3.cmml">m</mi></mrow></mrow><mo stretchy="false" id="S4.SS2.p2.3.m3.5.5.2.5" xref="S4.SS2.p2.3.m3.5.5.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.5b"><apply id="S4.SS2.p2.3.m3.5.5.3.cmml" xref="S4.SS2.p2.3.m3.5.5.2"><csymbol cd="latexml" id="S4.SS2.p2.3.m3.5.5.3.1.cmml" xref="S4.SS2.p2.3.m3.5.5.2.3">conditional-set</csymbol><apply id="S4.SS2.p2.3.m3.4.4.1.1.cmml" xref="S4.SS2.p2.3.m3.4.4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.4.4.1.1.1.cmml" xref="S4.SS2.p2.3.m3.4.4.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.4.4.1.1.2.cmml" xref="S4.SS2.p2.3.m3.4.4.1.1.2">ùë†</ci><ci id="S4.SS2.p2.3.m3.4.4.1.1.3.cmml" xref="S4.SS2.p2.3.m3.4.4.1.1.3">ùëñ</ci></apply><apply id="S4.SS2.p2.3.m3.5.5.2.2.cmml" xref="S4.SS2.p2.3.m3.5.5.2.2"><eq id="S4.SS2.p2.3.m3.5.5.2.2.1.cmml" xref="S4.SS2.p2.3.m3.5.5.2.2.1"></eq><ci id="S4.SS2.p2.3.m3.5.5.2.2.2.cmml" xref="S4.SS2.p2.3.m3.5.5.2.2.2">ùëñ</ci><list id="S4.SS2.p2.3.m3.5.5.2.2.3.1.cmml" xref="S4.SS2.p2.3.m3.5.5.2.2.3.2"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">1</cn><ci id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">‚Ä¶</ci><ci id="S4.SS2.p2.3.m3.3.3.cmml" xref="S4.SS2.p2.3.m3.3.3">ùëö</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.5c">\{s_{i}|i=1,\dots,m\}</annotation></semantics></math> (<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="m\leq n" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">m</mi><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">‚â§</mo><mi id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><leq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></leq><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">ùëö</ci><ci id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">m\leq n</annotation></semantics></math>). The Euclidean distance from <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="s_{j}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><msub id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">ùë†</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">s_{j}</annotation></semantics></math> to <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="s_{k}" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><msub id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">ùë†</ci><ci id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">s_{k}</annotation></semantics></math> is represented by <math id="S4.SS2.p2.7.m7.2" class="ltx_Math" alttext="d_{E}(j,k)" display="inline"><semantics id="S4.SS2.p2.7.m7.2a"><mrow id="S4.SS2.p2.7.m7.2.3" xref="S4.SS2.p2.7.m7.2.3.cmml"><msub id="S4.SS2.p2.7.m7.2.3.2" xref="S4.SS2.p2.7.m7.2.3.2.cmml"><mi id="S4.SS2.p2.7.m7.2.3.2.2" xref="S4.SS2.p2.7.m7.2.3.2.2.cmml">d</mi><mi id="S4.SS2.p2.7.m7.2.3.2.3" xref="S4.SS2.p2.7.m7.2.3.2.3.cmml">E</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.7.m7.2.3.1" xref="S4.SS2.p2.7.m7.2.3.1.cmml">‚Äã</mo><mrow id="S4.SS2.p2.7.m7.2.3.3.2" xref="S4.SS2.p2.7.m7.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS2.p2.7.m7.2.3.3.2.1" xref="S4.SS2.p2.7.m7.2.3.3.1.cmml">(</mo><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">j</mi><mo id="S4.SS2.p2.7.m7.2.3.3.2.2" xref="S4.SS2.p2.7.m7.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.7.m7.2.2" xref="S4.SS2.p2.7.m7.2.2.cmml">k</mi><mo stretchy="false" id="S4.SS2.p2.7.m7.2.3.3.2.3" xref="S4.SS2.p2.7.m7.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.2b"><apply id="S4.SS2.p2.7.m7.2.3.cmml" xref="S4.SS2.p2.7.m7.2.3"><times id="S4.SS2.p2.7.m7.2.3.1.cmml" xref="S4.SS2.p2.7.m7.2.3.1"></times><apply id="S4.SS2.p2.7.m7.2.3.2.cmml" xref="S4.SS2.p2.7.m7.2.3.2"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m7.2.3.2.1.cmml" xref="S4.SS2.p2.7.m7.2.3.2">subscript</csymbol><ci id="S4.SS2.p2.7.m7.2.3.2.2.cmml" xref="S4.SS2.p2.7.m7.2.3.2.2">ùëë</ci><ci id="S4.SS2.p2.7.m7.2.3.2.3.cmml" xref="S4.SS2.p2.7.m7.2.3.2.3">ùê∏</ci></apply><interval closure="open" id="S4.SS2.p2.7.m7.2.3.3.1.cmml" xref="S4.SS2.p2.7.m7.2.3.3.2"><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">ùëó</ci><ci id="S4.SS2.p2.7.m7.2.2.cmml" xref="S4.SS2.p2.7.m7.2.2">ùëò</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.2c">d_{E}(j,k)</annotation></semantics></math>. The rank-based measure calculates the distance between a pair of inconsistent records <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="s_{j}" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><msub id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml"><mi id="S4.SS2.p2.8.m8.1.1.2" xref="S4.SS2.p2.8.m8.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.8.m8.1.1.3" xref="S4.SS2.p2.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m8.1.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p2.8.m8.1.1.2.cmml" xref="S4.SS2.p2.8.m8.1.1.2">ùë†</ci><ci id="S4.SS2.p2.8.m8.1.1.3.cmml" xref="S4.SS2.p2.8.m8.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">s_{j}</annotation></semantics></math> and <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="s_{k}" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><msub id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml"><mi id="S4.SS2.p2.9.m9.1.1.2" xref="S4.SS2.p2.9.m9.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.9.m9.1.1.3" xref="S4.SS2.p2.9.m9.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><apply id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.9.m9.1.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p2.9.m9.1.1.2.cmml" xref="S4.SS2.p2.9.m9.1.1.2">ùë†</ci><ci id="S4.SS2.p2.9.m9.1.1.3.cmml" xref="S4.SS2.p2.9.m9.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">s_{k}</annotation></semantics></math> (<math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="1\leq j\leq m" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mrow id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mn id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml">1</mn><mo id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml">‚â§</mo><mi id="S4.SS2.p2.10.m10.1.1.4" xref="S4.SS2.p2.10.m10.1.1.4.cmml">j</mi><mo id="S4.SS2.p2.10.m10.1.1.5" xref="S4.SS2.p2.10.m10.1.1.5.cmml">‚â§</mo><mi id="S4.SS2.p2.10.m10.1.1.6" xref="S4.SS2.p2.10.m10.1.1.6.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><and id="S4.SS2.p2.10.m10.1.1a.cmml" xref="S4.SS2.p2.10.m10.1.1"></and><apply id="S4.SS2.p2.10.m10.1.1b.cmml" xref="S4.SS2.p2.10.m10.1.1"><leq id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3"></leq><cn type="integer" id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2">1</cn><ci id="S4.SS2.p2.10.m10.1.1.4.cmml" xref="S4.SS2.p2.10.m10.1.1.4">ùëó</ci></apply><apply id="S4.SS2.p2.10.m10.1.1c.cmml" xref="S4.SS2.p2.10.m10.1.1"><leq id="S4.SS2.p2.10.m10.1.1.5.cmml" xref="S4.SS2.p2.10.m10.1.1.5"></leq><share href="#S4.SS2.p2.10.m10.1.1.4.cmml" id="S4.SS2.p2.10.m10.1.1d.cmml" xref="S4.SS2.p2.10.m10.1.1"></share><ci id="S4.SS2.p2.10.m10.1.1.6.cmml" xref="S4.SS2.p2.10.m10.1.1.6">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">1\leq j\leq m</annotation></semantics></math>, <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="1\leq k\leq m" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><mrow id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mn id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">1</mn><mo id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml">‚â§</mo><mi id="S4.SS2.p2.11.m11.1.1.4" xref="S4.SS2.p2.11.m11.1.1.4.cmml">k</mi><mo id="S4.SS2.p2.11.m11.1.1.5" xref="S4.SS2.p2.11.m11.1.1.5.cmml">‚â§</mo><mi id="S4.SS2.p2.11.m11.1.1.6" xref="S4.SS2.p2.11.m11.1.1.6.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><and id="S4.SS2.p2.11.m11.1.1a.cmml" xref="S4.SS2.p2.11.m11.1.1"></and><apply id="S4.SS2.p2.11.m11.1.1b.cmml" xref="S4.SS2.p2.11.m11.1.1"><leq id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3"></leq><cn type="integer" id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">1</cn><ci id="S4.SS2.p2.11.m11.1.1.4.cmml" xref="S4.SS2.p2.11.m11.1.1.4">ùëò</ci></apply><apply id="S4.SS2.p2.11.m11.1.1c.cmml" xref="S4.SS2.p2.11.m11.1.1"><leq id="S4.SS2.p2.11.m11.1.1.5.cmml" xref="S4.SS2.p2.11.m11.1.1.5"></leq><share href="#S4.SS2.p2.11.m11.1.1.4.cmml" id="S4.SS2.p2.11.m11.1.1d.cmml" xref="S4.SS2.p2.11.m11.1.1"></share><ci id="S4.SS2.p2.11.m11.1.1.6.cmml" xref="S4.SS2.p2.11.m11.1.1.6">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">1\leq k\leq m</annotation></semantics></math>, and <math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="j\neq k" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><mrow id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml"><mi id="S4.SS2.p2.12.m12.1.1.2" xref="S4.SS2.p2.12.m12.1.1.2.cmml">j</mi><mo id="S4.SS2.p2.12.m12.1.1.1" xref="S4.SS2.p2.12.m12.1.1.1.cmml">‚â†</mo><mi id="S4.SS2.p2.12.m12.1.1.3" xref="S4.SS2.p2.12.m12.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><apply id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1"><neq id="S4.SS2.p2.12.m12.1.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1.1"></neq><ci id="S4.SS2.p2.12.m12.1.1.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2">ùëó</ci><ci id="S4.SS2.p2.12.m12.1.1.3.cmml" xref="S4.SS2.p2.12.m12.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">j\neq k</annotation></semantics></math>) as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.5" class="ltx_Math" alttext="d_{R}(j,k)=r_{j}(k)\times r_{k}(j)," display="block"><semantics id="S4.E1.m1.5a"><mrow id="S4.E1.m1.5.5.1" xref="S4.E1.m1.5.5.1.1.cmml"><mrow id="S4.E1.m1.5.5.1.1" xref="S4.E1.m1.5.5.1.1.cmml"><mrow id="S4.E1.m1.5.5.1.1.2" xref="S4.E1.m1.5.5.1.1.2.cmml"><msub id="S4.E1.m1.5.5.1.1.2.2" xref="S4.E1.m1.5.5.1.1.2.2.cmml"><mi id="S4.E1.m1.5.5.1.1.2.2.2" xref="S4.E1.m1.5.5.1.1.2.2.2.cmml">d</mi><mi id="S4.E1.m1.5.5.1.1.2.2.3" xref="S4.E1.m1.5.5.1.1.2.2.3.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.1.1.2.1" xref="S4.E1.m1.5.5.1.1.2.1.cmml">‚Äã</mo><mrow id="S4.E1.m1.5.5.1.1.2.3.2" xref="S4.E1.m1.5.5.1.1.2.3.1.cmml"><mo stretchy="false" id="S4.E1.m1.5.5.1.1.2.3.2.1" xref="S4.E1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">j</mi><mo id="S4.E1.m1.5.5.1.1.2.3.2.2" xref="S4.E1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">k</mi><mo stretchy="false" id="S4.E1.m1.5.5.1.1.2.3.2.3" xref="S4.E1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.5.5.1.1.1" xref="S4.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.5.5.1.1.3" xref="S4.E1.m1.5.5.1.1.3.cmml"><mrow id="S4.E1.m1.5.5.1.1.3.2" xref="S4.E1.m1.5.5.1.1.3.2.cmml"><mrow id="S4.E1.m1.5.5.1.1.3.2.2" xref="S4.E1.m1.5.5.1.1.3.2.2.cmml"><msub id="S4.E1.m1.5.5.1.1.3.2.2.2" xref="S4.E1.m1.5.5.1.1.3.2.2.2.cmml"><mi id="S4.E1.m1.5.5.1.1.3.2.2.2.2" xref="S4.E1.m1.5.5.1.1.3.2.2.2.2.cmml">r</mi><mi id="S4.E1.m1.5.5.1.1.3.2.2.2.3" xref="S4.E1.m1.5.5.1.1.3.2.2.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.1.1.3.2.2.1" xref="S4.E1.m1.5.5.1.1.3.2.2.1.cmml">‚Äã</mo><mrow id="S4.E1.m1.5.5.1.1.3.2.2.3.2" xref="S4.E1.m1.5.5.1.1.3.2.2.cmml"><mo stretchy="false" id="S4.E1.m1.5.5.1.1.3.2.2.3.2.1" xref="S4.E1.m1.5.5.1.1.3.2.2.cmml">(</mo><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">k</mi><mo rspace="0.055em" stretchy="false" id="S4.E1.m1.5.5.1.1.3.2.2.3.2.2" xref="S4.E1.m1.5.5.1.1.3.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S4.E1.m1.5.5.1.1.3.2.1" xref="S4.E1.m1.5.5.1.1.3.2.1.cmml">√ó</mo><msub id="S4.E1.m1.5.5.1.1.3.2.3" xref="S4.E1.m1.5.5.1.1.3.2.3.cmml"><mi id="S4.E1.m1.5.5.1.1.3.2.3.2" xref="S4.E1.m1.5.5.1.1.3.2.3.2.cmml">r</mi><mi id="S4.E1.m1.5.5.1.1.3.2.3.3" xref="S4.E1.m1.5.5.1.1.3.2.3.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.5.5.1.1.3.1" xref="S4.E1.m1.5.5.1.1.3.1.cmml">‚Äã</mo><mrow id="S4.E1.m1.5.5.1.1.3.3.2" xref="S4.E1.m1.5.5.1.1.3.cmml"><mo stretchy="false" id="S4.E1.m1.5.5.1.1.3.3.2.1" xref="S4.E1.m1.5.5.1.1.3.cmml">(</mo><mi id="S4.E1.m1.4.4" xref="S4.E1.m1.4.4.cmml">j</mi><mo stretchy="false" id="S4.E1.m1.5.5.1.1.3.3.2.2" xref="S4.E1.m1.5.5.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.5.5.1.2" xref="S4.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.5b"><apply id="S4.E1.m1.5.5.1.1.cmml" xref="S4.E1.m1.5.5.1"><eq id="S4.E1.m1.5.5.1.1.1.cmml" xref="S4.E1.m1.5.5.1.1.1"></eq><apply id="S4.E1.m1.5.5.1.1.2.cmml" xref="S4.E1.m1.5.5.1.1.2"><times id="S4.E1.m1.5.5.1.1.2.1.cmml" xref="S4.E1.m1.5.5.1.1.2.1"></times><apply id="S4.E1.m1.5.5.1.1.2.2.cmml" xref="S4.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.1.1.2.2.1.cmml" xref="S4.E1.m1.5.5.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.5.5.1.1.2.2.2.cmml" xref="S4.E1.m1.5.5.1.1.2.2.2">ùëë</ci><ci id="S4.E1.m1.5.5.1.1.2.2.3.cmml" xref="S4.E1.m1.5.5.1.1.2.2.3">ùëÖ</ci></apply><interval closure="open" id="S4.E1.m1.5.5.1.1.2.3.1.cmml" xref="S4.E1.m1.5.5.1.1.2.3.2"><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">ùëó</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">ùëò</ci></interval></apply><apply id="S4.E1.m1.5.5.1.1.3.cmml" xref="S4.E1.m1.5.5.1.1.3"><times id="S4.E1.m1.5.5.1.1.3.1.cmml" xref="S4.E1.m1.5.5.1.1.3.1"></times><apply id="S4.E1.m1.5.5.1.1.3.2.cmml" xref="S4.E1.m1.5.5.1.1.3.2"><times id="S4.E1.m1.5.5.1.1.3.2.1.cmml" xref="S4.E1.m1.5.5.1.1.3.2.1"></times><apply id="S4.E1.m1.5.5.1.1.3.2.2.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2"><times id="S4.E1.m1.5.5.1.1.3.2.2.1.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2.1"></times><apply id="S4.E1.m1.5.5.1.1.3.2.2.2.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.1.1.3.2.2.2.1.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E1.m1.5.5.1.1.3.2.2.2.2.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2.2.2">ùëü</ci><ci id="S4.E1.m1.5.5.1.1.3.2.2.2.3.cmml" xref="S4.E1.m1.5.5.1.1.3.2.2.2.3">ùëó</ci></apply><ci id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">ùëò</ci></apply><apply id="S4.E1.m1.5.5.1.1.3.2.3.cmml" xref="S4.E1.m1.5.5.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.5.5.1.1.3.2.3.1.cmml" xref="S4.E1.m1.5.5.1.1.3.2.3">subscript</csymbol><ci id="S4.E1.m1.5.5.1.1.3.2.3.2.cmml" xref="S4.E1.m1.5.5.1.1.3.2.3.2">ùëü</ci><ci id="S4.E1.m1.5.5.1.1.3.2.3.3.cmml" xref="S4.E1.m1.5.5.1.1.3.2.3.3">ùëò</ci></apply></apply><ci id="S4.E1.m1.4.4.cmml" xref="S4.E1.m1.4.4">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.5c">d_{R}(j,k)=r_{j}(k)\times r_{k}(j),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.18" class="ltx_p">where <math id="S4.SS2.p2.13.m1.1" class="ltx_Math" alttext="r_{j}(k)" display="inline"><semantics id="S4.SS2.p2.13.m1.1a"><mrow id="S4.SS2.p2.13.m1.1.2" xref="S4.SS2.p2.13.m1.1.2.cmml"><msub id="S4.SS2.p2.13.m1.1.2.2" xref="S4.SS2.p2.13.m1.1.2.2.cmml"><mi id="S4.SS2.p2.13.m1.1.2.2.2" xref="S4.SS2.p2.13.m1.1.2.2.2.cmml">r</mi><mi id="S4.SS2.p2.13.m1.1.2.2.3" xref="S4.SS2.p2.13.m1.1.2.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.13.m1.1.2.1" xref="S4.SS2.p2.13.m1.1.2.1.cmml">‚Äã</mo><mrow id="S4.SS2.p2.13.m1.1.2.3.2" xref="S4.SS2.p2.13.m1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.13.m1.1.2.3.2.1" xref="S4.SS2.p2.13.m1.1.2.cmml">(</mo><mi id="S4.SS2.p2.13.m1.1.1" xref="S4.SS2.p2.13.m1.1.1.cmml">k</mi><mo stretchy="false" id="S4.SS2.p2.13.m1.1.2.3.2.2" xref="S4.SS2.p2.13.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.13.m1.1b"><apply id="S4.SS2.p2.13.m1.1.2.cmml" xref="S4.SS2.p2.13.m1.1.2"><times id="S4.SS2.p2.13.m1.1.2.1.cmml" xref="S4.SS2.p2.13.m1.1.2.1"></times><apply id="S4.SS2.p2.13.m1.1.2.2.cmml" xref="S4.SS2.p2.13.m1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.13.m1.1.2.2.1.cmml" xref="S4.SS2.p2.13.m1.1.2.2">subscript</csymbol><ci id="S4.SS2.p2.13.m1.1.2.2.2.cmml" xref="S4.SS2.p2.13.m1.1.2.2.2">ùëü</ci><ci id="S4.SS2.p2.13.m1.1.2.2.3.cmml" xref="S4.SS2.p2.13.m1.1.2.2.3">ùëó</ci></apply><ci id="S4.SS2.p2.13.m1.1.1.cmml" xref="S4.SS2.p2.13.m1.1.1">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.13.m1.1c">r_{j}(k)</annotation></semantics></math> is the ranking of <math id="S4.SS2.p2.14.m2.2" class="ltx_Math" alttext="d_{E}(j,k)" display="inline"><semantics id="S4.SS2.p2.14.m2.2a"><mrow id="S4.SS2.p2.14.m2.2.3" xref="S4.SS2.p2.14.m2.2.3.cmml"><msub id="S4.SS2.p2.14.m2.2.3.2" xref="S4.SS2.p2.14.m2.2.3.2.cmml"><mi id="S4.SS2.p2.14.m2.2.3.2.2" xref="S4.SS2.p2.14.m2.2.3.2.2.cmml">d</mi><mi id="S4.SS2.p2.14.m2.2.3.2.3" xref="S4.SS2.p2.14.m2.2.3.2.3.cmml">E</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.14.m2.2.3.1" xref="S4.SS2.p2.14.m2.2.3.1.cmml">‚Äã</mo><mrow id="S4.SS2.p2.14.m2.2.3.3.2" xref="S4.SS2.p2.14.m2.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS2.p2.14.m2.2.3.3.2.1" xref="S4.SS2.p2.14.m2.2.3.3.1.cmml">(</mo><mi id="S4.SS2.p2.14.m2.1.1" xref="S4.SS2.p2.14.m2.1.1.cmml">j</mi><mo id="S4.SS2.p2.14.m2.2.3.3.2.2" xref="S4.SS2.p2.14.m2.2.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.14.m2.2.2" xref="S4.SS2.p2.14.m2.2.2.cmml">k</mi><mo stretchy="false" id="S4.SS2.p2.14.m2.2.3.3.2.3" xref="S4.SS2.p2.14.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.14.m2.2b"><apply id="S4.SS2.p2.14.m2.2.3.cmml" xref="S4.SS2.p2.14.m2.2.3"><times id="S4.SS2.p2.14.m2.2.3.1.cmml" xref="S4.SS2.p2.14.m2.2.3.1"></times><apply id="S4.SS2.p2.14.m2.2.3.2.cmml" xref="S4.SS2.p2.14.m2.2.3.2"><csymbol cd="ambiguous" id="S4.SS2.p2.14.m2.2.3.2.1.cmml" xref="S4.SS2.p2.14.m2.2.3.2">subscript</csymbol><ci id="S4.SS2.p2.14.m2.2.3.2.2.cmml" xref="S4.SS2.p2.14.m2.2.3.2.2">ùëë</ci><ci id="S4.SS2.p2.14.m2.2.3.2.3.cmml" xref="S4.SS2.p2.14.m2.2.3.2.3">ùê∏</ci></apply><interval closure="open" id="S4.SS2.p2.14.m2.2.3.3.1.cmml" xref="S4.SS2.p2.14.m2.2.3.3.2"><ci id="S4.SS2.p2.14.m2.1.1.cmml" xref="S4.SS2.p2.14.m2.1.1">ùëó</ci><ci id="S4.SS2.p2.14.m2.2.2.cmml" xref="S4.SS2.p2.14.m2.2.2">ùëò</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.14.m2.2c">d_{E}(j,k)</annotation></semantics></math> in <math id="S4.SS2.p2.15.m3.7" class="ltx_Math" alttext="\{d_{E}(j,i)|i=1,\dots,n\}" display="inline"><semantics id="S4.SS2.p2.15.m3.7a"><mrow id="S4.SS2.p2.15.m3.7.7.2" xref="S4.SS2.p2.15.m3.7.7.3.cmml"><mo stretchy="false" id="S4.SS2.p2.15.m3.7.7.2.3" xref="S4.SS2.p2.15.m3.7.7.3.1.cmml">{</mo><mrow id="S4.SS2.p2.15.m3.6.6.1.1" xref="S4.SS2.p2.15.m3.6.6.1.1.cmml"><msub id="S4.SS2.p2.15.m3.6.6.1.1.2" xref="S4.SS2.p2.15.m3.6.6.1.1.2.cmml"><mi id="S4.SS2.p2.15.m3.6.6.1.1.2.2" xref="S4.SS2.p2.15.m3.6.6.1.1.2.2.cmml">d</mi><mi id="S4.SS2.p2.15.m3.6.6.1.1.2.3" xref="S4.SS2.p2.15.m3.6.6.1.1.2.3.cmml">E</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.15.m3.6.6.1.1.1" xref="S4.SS2.p2.15.m3.6.6.1.1.1.cmml">‚Äã</mo><mrow id="S4.SS2.p2.15.m3.6.6.1.1.3.2" xref="S4.SS2.p2.15.m3.6.6.1.1.3.1.cmml"><mo stretchy="false" id="S4.SS2.p2.15.m3.6.6.1.1.3.2.1" xref="S4.SS2.p2.15.m3.6.6.1.1.3.1.cmml">(</mo><mi id="S4.SS2.p2.15.m3.1.1" xref="S4.SS2.p2.15.m3.1.1.cmml">j</mi><mo id="S4.SS2.p2.15.m3.6.6.1.1.3.2.2" xref="S4.SS2.p2.15.m3.6.6.1.1.3.1.cmml">,</mo><mi id="S4.SS2.p2.15.m3.2.2" xref="S4.SS2.p2.15.m3.2.2.cmml">i</mi><mo stretchy="false" id="S4.SS2.p2.15.m3.6.6.1.1.3.2.3" xref="S4.SS2.p2.15.m3.6.6.1.1.3.1.cmml">)</mo></mrow></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.p2.15.m3.7.7.2.4" xref="S4.SS2.p2.15.m3.7.7.3.1.cmml">|</mo><mrow id="S4.SS2.p2.15.m3.7.7.2.2" xref="S4.SS2.p2.15.m3.7.7.2.2.cmml"><mi id="S4.SS2.p2.15.m3.7.7.2.2.2" xref="S4.SS2.p2.15.m3.7.7.2.2.2.cmml">i</mi><mo id="S4.SS2.p2.15.m3.7.7.2.2.1" xref="S4.SS2.p2.15.m3.7.7.2.2.1.cmml">=</mo><mrow id="S4.SS2.p2.15.m3.7.7.2.2.3.2" xref="S4.SS2.p2.15.m3.7.7.2.2.3.1.cmml"><mn id="S4.SS2.p2.15.m3.3.3" xref="S4.SS2.p2.15.m3.3.3.cmml">1</mn><mo id="S4.SS2.p2.15.m3.7.7.2.2.3.2.1" xref="S4.SS2.p2.15.m3.7.7.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.15.m3.4.4" xref="S4.SS2.p2.15.m3.4.4.cmml">‚Ä¶</mi><mo id="S4.SS2.p2.15.m3.7.7.2.2.3.2.2" xref="S4.SS2.p2.15.m3.7.7.2.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.15.m3.5.5" xref="S4.SS2.p2.15.m3.5.5.cmml">n</mi></mrow></mrow><mo stretchy="false" id="S4.SS2.p2.15.m3.7.7.2.5" xref="S4.SS2.p2.15.m3.7.7.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.15.m3.7b"><apply id="S4.SS2.p2.15.m3.7.7.3.cmml" xref="S4.SS2.p2.15.m3.7.7.2"><csymbol cd="latexml" id="S4.SS2.p2.15.m3.7.7.3.1.cmml" xref="S4.SS2.p2.15.m3.7.7.2.3">conditional-set</csymbol><apply id="S4.SS2.p2.15.m3.6.6.1.1.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1"><times id="S4.SS2.p2.15.m3.6.6.1.1.1.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.1"></times><apply id="S4.SS2.p2.15.m3.6.6.1.1.2.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.15.m3.6.6.1.1.2.1.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.15.m3.6.6.1.1.2.2.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.2.2">ùëë</ci><ci id="S4.SS2.p2.15.m3.6.6.1.1.2.3.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.2.3">ùê∏</ci></apply><interval closure="open" id="S4.SS2.p2.15.m3.6.6.1.1.3.1.cmml" xref="S4.SS2.p2.15.m3.6.6.1.1.3.2"><ci id="S4.SS2.p2.15.m3.1.1.cmml" xref="S4.SS2.p2.15.m3.1.1">ùëó</ci><ci id="S4.SS2.p2.15.m3.2.2.cmml" xref="S4.SS2.p2.15.m3.2.2">ùëñ</ci></interval></apply><apply id="S4.SS2.p2.15.m3.7.7.2.2.cmml" xref="S4.SS2.p2.15.m3.7.7.2.2"><eq id="S4.SS2.p2.15.m3.7.7.2.2.1.cmml" xref="S4.SS2.p2.15.m3.7.7.2.2.1"></eq><ci id="S4.SS2.p2.15.m3.7.7.2.2.2.cmml" xref="S4.SS2.p2.15.m3.7.7.2.2.2">ùëñ</ci><list id="S4.SS2.p2.15.m3.7.7.2.2.3.1.cmml" xref="S4.SS2.p2.15.m3.7.7.2.2.3.2"><cn type="integer" id="S4.SS2.p2.15.m3.3.3.cmml" xref="S4.SS2.p2.15.m3.3.3">1</cn><ci id="S4.SS2.p2.15.m3.4.4.cmml" xref="S4.SS2.p2.15.m3.4.4">‚Ä¶</ci><ci id="S4.SS2.p2.15.m3.5.5.cmml" xref="S4.SS2.p2.15.m3.5.5">ùëõ</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.15.m3.7c">\{d_{E}(j,i)|i=1,\dots,n\}</annotation></semantics></math>. Namely, <math id="S4.SS2.p2.16.m4.1" class="ltx_Math" alttext="s_{k}" display="inline"><semantics id="S4.SS2.p2.16.m4.1a"><msub id="S4.SS2.p2.16.m4.1.1" xref="S4.SS2.p2.16.m4.1.1.cmml"><mi id="S4.SS2.p2.16.m4.1.1.2" xref="S4.SS2.p2.16.m4.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.16.m4.1.1.3" xref="S4.SS2.p2.16.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.16.m4.1b"><apply id="S4.SS2.p2.16.m4.1.1.cmml" xref="S4.SS2.p2.16.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.16.m4.1.1.1.cmml" xref="S4.SS2.p2.16.m4.1.1">subscript</csymbol><ci id="S4.SS2.p2.16.m4.1.1.2.cmml" xref="S4.SS2.p2.16.m4.1.1.2">ùë†</ci><ci id="S4.SS2.p2.16.m4.1.1.3.cmml" xref="S4.SS2.p2.16.m4.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.16.m4.1c">s_{k}</annotation></semantics></math> is the <math id="S4.SS2.p2.17.m5.1" class="ltx_Math" alttext="r_{j}(k)" display="inline"><semantics id="S4.SS2.p2.17.m5.1a"><mrow id="S4.SS2.p2.17.m5.1.2" xref="S4.SS2.p2.17.m5.1.2.cmml"><msub id="S4.SS2.p2.17.m5.1.2.2" xref="S4.SS2.p2.17.m5.1.2.2.cmml"><mi id="S4.SS2.p2.17.m5.1.2.2.2" xref="S4.SS2.p2.17.m5.1.2.2.2.cmml">r</mi><mi id="S4.SS2.p2.17.m5.1.2.2.3" xref="S4.SS2.p2.17.m5.1.2.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.17.m5.1.2.1" xref="S4.SS2.p2.17.m5.1.2.1.cmml">‚Äã</mo><mrow id="S4.SS2.p2.17.m5.1.2.3.2" xref="S4.SS2.p2.17.m5.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.17.m5.1.2.3.2.1" xref="S4.SS2.p2.17.m5.1.2.cmml">(</mo><mi id="S4.SS2.p2.17.m5.1.1" xref="S4.SS2.p2.17.m5.1.1.cmml">k</mi><mo stretchy="false" id="S4.SS2.p2.17.m5.1.2.3.2.2" xref="S4.SS2.p2.17.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.17.m5.1b"><apply id="S4.SS2.p2.17.m5.1.2.cmml" xref="S4.SS2.p2.17.m5.1.2"><times id="S4.SS2.p2.17.m5.1.2.1.cmml" xref="S4.SS2.p2.17.m5.1.2.1"></times><apply id="S4.SS2.p2.17.m5.1.2.2.cmml" xref="S4.SS2.p2.17.m5.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.17.m5.1.2.2.1.cmml" xref="S4.SS2.p2.17.m5.1.2.2">subscript</csymbol><ci id="S4.SS2.p2.17.m5.1.2.2.2.cmml" xref="S4.SS2.p2.17.m5.1.2.2.2">ùëü</ci><ci id="S4.SS2.p2.17.m5.1.2.2.3.cmml" xref="S4.SS2.p2.17.m5.1.2.2.3">ùëó</ci></apply><ci id="S4.SS2.p2.17.m5.1.1.cmml" xref="S4.SS2.p2.17.m5.1.1">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.17.m5.1c">r_{j}(k)</annotation></semantics></math>-th closest record to <math id="S4.SS2.p2.18.m6.1" class="ltx_Math" alttext="s_{j}" display="inline"><semantics id="S4.SS2.p2.18.m6.1a"><msub id="S4.SS2.p2.18.m6.1.1" xref="S4.SS2.p2.18.m6.1.1.cmml"><mi id="S4.SS2.p2.18.m6.1.1.2" xref="S4.SS2.p2.18.m6.1.1.2.cmml">s</mi><mi id="S4.SS2.p2.18.m6.1.1.3" xref="S4.SS2.p2.18.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.18.m6.1b"><apply id="S4.SS2.p2.18.m6.1.1.cmml" xref="S4.SS2.p2.18.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.18.m6.1.1.1.cmml" xref="S4.SS2.p2.18.m6.1.1">subscript</csymbol><ci id="S4.SS2.p2.18.m6.1.1.2.cmml" xref="S4.SS2.p2.18.m6.1.1.2">ùë†</ci><ci id="S4.SS2.p2.18.m6.1.1.3.cmml" xref="S4.SS2.p2.18.m6.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.18.m6.1c">s_{j}</annotation></semantics></math> among all records, including consistent records.
Based on the rank-based measurement, inconsistent records with less surrounding consistent records will be preferentially aggregated together, even if the inconsistent records are far apart.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Visual Analysis Interface</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Corresponding to the workflow, the interface of <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">HetVis</span> consists of three modules, which are learning process monitor (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a)), output comparison (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b)), and heterogeneity examination (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span> (c)).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Learning Process Monitor</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The module for monitoring the HFL process (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a)) consists of an information panel, a parameter projection view (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a1)), and a performance view (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a2)). The information panel, which introduces the configuration information of the HFL model and the description of the analyzed client, are listed at the top of the module. The descriptions will be updated with the progress of the model training (i.e., a new communication round is finished).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Parameter projection view: </span>
As shown in Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a1), the parameter projection view summarizes exchanges of model parameters between the local client and the server. Because model parameters updated in a single communication round could be affected by accident outliers, it is necessary to provide an overview of parameter updates during the entire training process. In each communication round, model parameters can be considered as a high-dimensional vector. To observe parameters from a low-dimensional perspective, we generate a 2D projection for the parameter vectors of the HFL model in all communication rounds. To reflect parameter exchanges, parameters submitted by the local client are transformed into vectors of the same size and projected onto the same plane. The employed projection approach is accelerated by a probabilistic algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Considering that different parameters contribute in different ways, we allow users to specify a group of parameters and check them individually. For example, users can specify a layer in the neural network and check the projection of the model parameters on this layer.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.4" class="ltx_p">In the parameter projection view, the points that project model parameters of the HFL model in each communication round are connected by a gray polyline. The grayscale encodes the time sequence. A brown arrow is drawn from the point projecting federated parameters in round <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="i-1" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">i</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">‚àí</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><minus id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></minus><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">ùëñ</ci><cn type="integer" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">i-1</annotation></semantics></math> to the local client parameters in round <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">i</annotation></semantics></math>. The federated parameters in round <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">i</annotation></semantics></math> are different from the local parameters because the federated parameters have integrated parameters from other clients. The size of the angle between the arrow and the polyline implies the disagreement of parameters at round <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mi id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><ci id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">i</annotation></semantics></math>. The cosine of each angle in the high-dimensional space is calculated and encoded by the darkness of the corresponding arrow to avoid misunderstanding caused by projection distortion. Disagreements and compromises between the local client and the server can be observed by comparing historical parameter updates with the actual parameter evolution of the HFL model.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Performance view:</span> At the bottom of the interface, the performance view (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a2)) monitors the dynamics of the performance testified by the local data. Performance indicators consist of the training loss, the accuracy for the test set, and the total accuracy for local data. Users are allowed to switch indicators to apply a comprehensive evaluation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Output Comparison</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The second module (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b)) consists of an output comparison view and a summary of inconsistency clusters.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The result of output comparison is displayed in the output comparison view (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b1)). To contrast inconsistent records (in brown) with others (in gray), all records are projected through ccPCA. As mentioned in Section¬†<a href="#S3.SS2" title="3.2 Comparing Model Output ‚Ä£ 3 Workflow ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the projection could hardly split all inconsistent records from the rest simultaneously. Users are allowed to check the overlaps by switching the top layer between inconsistency and consistency.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We list inconsistency clusters in the order of size (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b2)) to motivate heterogeneity examination. Each inconsistency cluster is represented by a glyph. In each glyph, the convex hull of the inconsistent records is superimposed on a density heatmap of all records in the output comparison view. The cluster size and the accuracy of the HFL model on the cluster are listed below the corresponding glyph. A cluster with an extremely low accuracy may suggest a significant impact from the corresponding heterogeneity issue. Clusters with insufficient records can be regarded as outliers.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Heterogeneity Examination</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Users examine an inconsistency cluster in the third module of the interface (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c)), which includes two views for distribution exploration, an instance verification panel (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c4)), and an annotations panel (Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c5)). The distribution exploration views are introduced as follows.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Dimension exploration view:</span>
Users need to extract heterogeneity issues by identifying commons shared by inconsistent records but not shared by others. However, it is exhausting to inspect each dimension, considering the records are high dimensional. To improve analysis efficiency, we provide users with two entrances for dimension selections. The first entrance navigates users from the perspective of data space. This entrance shows how important each dimension is to distinguish the selected cluster from others based on the first two cPCs of ccPCA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The second entrance navigates users from the perspective of model behaviors, which only works for HFL based on CNN models. Gradient class activation maps (Grad-CAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> can measure how important each dimension is for a CNN model to classify a record. The average Grad-CAM of all records in the cluster can identify significant dimensions to a model. To analyze inconsistent model behaviors, we generate a pair of average Grad-CAMs for the stand-alone training model and the HFL model, respectively. Comparing the two average Grad-CAMs can help users identify the differences in model judgments. However, such differences in model judgment can hardly be demonstrated by Grad-CAMs in rare cases (e.g., models with cascading randomization)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. It is necessary to leverage both of the entrances.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">We employ a pair of pixel maps to represent the user-specified dimension entrance. Note that both entrances generate a pair of importance values. When the entrance based on ccPCA is activated, two pixel maps represent the quantified importance of each dimension measured by the first two cPCs, respectively. When the entrance based on Grad-CAM is used, the two pixel maps represent the quantified importance evaluated by the two models, respectively. In a pixel map, each pixel corresponds to a dimension. If records are in the form of pictures, the relative position of a pixel is consistent with its position in pictures. If the pictures have more than one channel (e.g., RGB channels), users can select a channel and check a channel at a time. The color of a pixel encodes the dimension importance.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Users can hover on a pixel to find the pixel corresponding to the same dimension in the other pixel map. Users can also click a pixel and check the corresponding dimension distribution of all records or the records in the cluster (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c2)). Percentage distributions of overall records, inconsistent records, and consistent records are displayed respectively. To adapt to different distribution patterns, the scale on the y-axis (percentage) can be switched from linear to logarithmic.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Label exploration view: </span>
A matrix design is leveraged to compare the ground-truth labels with the output of the HFL model (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c3)). Each cell of the matrix corresponds to a pair of labels (see Figure¬†<a href="#S5.F2" title="Figure 2 ‚Ä£ 5.3 Heterogeneity Examination ‚Ä£ 5 Visual Analysis Interface ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)). The horizontal position is specified by the ground-truth label, and the vertical position is specified by the output from the HFL model. The records in the non-diagonal cells indicate that the HFL model generates incorrect output. If extra labels (i.e., the labels not included in the ground-truth labels) exist in the output of the HFL model, there will be extra rows listed after the labels included in the ground-truth label. Users can scroll down to check the cells corresponding to extra labels.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2208.07491/assets/x3.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="282" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The visual design of the label exploration view.</figcaption>
</figure>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">The records, which meet the pair of the ground-truth label and the output of the HFL model, are projected in the corresponding cell as a scatter plot by ccPCA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (see Figure¬†<a href="#S5.F2" title="Figure 2 ‚Ä£ 5.3 Heterogeneity Examination ‚Ä£ 5 Visual Analysis Interface ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b)). For each cell, we count the inconsistent records in the selected cluster and the local data, respectively. The numbers are displayed in the upper-left corner (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c3)). The inconsistent records and the consistent records are with the same color encoding as those in the output comparison view. Also, the convex hull of the selected cluster is drawn in each cell.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p">A grid-based heatmap is shown as a background in each cell (see Figure¬†<a href="#S5.F2" title="Figure 2 ‚Ä£ 5.3 Heterogeneity Examination ‚Ä£ 5 Visual Analysis Interface ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b)). The background color encodes the density of the corresponding ground-truth label. Through comparing the background with scatters or convex hulls, users can come up with conjectures, like ‚Äúthere exist label heterogeneity.‚Äù Therefore, the cells in the same column are with the same background. The grid size can be adjusted to observe from different levels of granularity. Users are also allowed to hide the scatters and focus on the label distribution in cells. The contents in all cells can be zoomed in together.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>User Interactions</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Our system supports the following interactions.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Check an intermediate result of the HFL model.</span> In the performance view, users can drag a handle to select a communication round and analyze the HFL model updated at this round. If the round is included in the user-specified range for the updates projection view, a circle appears in the projection view to highlight the corresponding round.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Request for recommended parameters.</span> In consideration of efficiency, the initial contrastive parameters for ccPCA and the number of inconsistency clusters are default as 10 and 8. To seek better effectiveness, users can request recommended parameters by clicking the button on the right of the input box. The recommended contrastive parameter for ccPCA is provided by the original approach¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The cluster number is recommended through the maximum difference¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p"><span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_bold">Examine an inconsistency cluster.</span> After browsing the list of inconsistency clusters, users can click on a glyph in the column of the model output comparison to examine it in the third module. The selected glyph will be highlighted by a thick stroke.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">Apply instance verification.</span> Users can select a record in the label exploration view by clicking. The record description, including dimension details, the ground-truth label, and the output of the two models, can be found in the instance verification view (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c4)).</p>
</div>
<div id="S5.SS4.p6" class="ltx_para">
<p id="S5.SS4.p6.1" class="ltx_p"><span id="S5.SS4.p6.1.1" class="ltx_text ltx_font_bold">Manage records with suspicious heterogeneity issues.</span> Users are allowed to select all records in the currently analyzed cluster as the object in the control panel (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c5)). Note that local data records are selected by the convex hull if the data records in the cluster are sampled data. If necessary, the intersection set or the joint set of the current cluster and annotated records can be selected.</p>
</div>
<div id="S5.SS4.p7" class="ltx_para">
<p id="S5.SS4.p7.1" class="ltx_p"><span id="S5.SS4.p7.1.1" class="ltx_text ltx_font_bold">Leverage the analysis provenance.</span> In the column of heterogeneity examination, users can annotate their findings and record the analyzed inconsistency cluster to the analysis provenance in the control panel (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(c5)). Each annotation generates a message icon in the performance monitor view. Users can click a message icon to review the details of previous annotations. Also, the records in the annotated set will be highlighted in both the model output overview and the label exploration view. If the annotated cluster has an overlap with an inconsistency cluster, the cluster glyph will be highlighted, and the size of the overlap will be shown below the glyph. Each annotation can be deleted by right-clicking on the icon.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Case Studies</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Handwritten Digits Recognition</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The MNIST dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is employed in the first case to train a Multilayer Perceptron (MLP) for classification. The original dataset was distributed to 10 clients. Each client has 6,000 records of pictures, with two consecutive digital labels, e.g., digit-1 and digit-2. <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mn id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">10\%</annotation></semantics></math> records are employed in the test set. The analyzed client owns records with labels consisting of digit-0 and digit-1.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">As shown in Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(a2), the loss became stable at the 40th round (<span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">R1.1</span>). We check the convergence process by selecting this round. We employed local data as input to compare outputs from the stand-alone model and the HFL model (<span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_italic">R2.2</span>). 300 inconsistent records were identified from the comparison. As shown in Figure¬†<a href="#S6.F3" title="Figure 3 ‚Ä£ 6.1 Handwritten Digits Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the numbers of inconsistent records in the label exploration matrix reflect that most inconsistent records locate in non-diagonal cells, where ground-truth labels are different from the output of the HFL model. The HFL model has a lower accuracy than the stand-alone training model. The HFL model classified partial records as other labels.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2208.07491/assets/x4.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Partial matrices in the label exploration view. A majority of inconsistent records distributed in the cells whose output from the HFL model includes other labels, like digit-5 and digit-6.</figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">We generated a ccPCA projection based on the recommended contrastive parameter to have an overview of all records. The projection result indicated that inconsistent records are not gathered closely and heavily overlapped with consistent ones. To distinguish them, we captured the features of inconsistent records by inspecting their clusters (<span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_italic">R3.1</span>).
We paid attention to the four clusters with the most records. The second, the third and the fourth clusters correspond to a rare handwritten style, respectively (see Figure¬†<a href="#S6.F4" title="Figure 4 ‚Ä£ 6.1 Handwritten Digits Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a)).</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2208.07491/assets/x5.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="456" height="441" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Inconsistency clusters of inconsistent records identified from the 40th round. (a) The top four clusters with the recommended cluster number. (b) Two clusters split from the 1st cluster in (a) after cluster number reaches 50.</figcaption>
</figure>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">We noticed that records with various handwritten styles are mixed up in the first cluster. To categorize styles and refine heterogeneity issues, we further split the first cluster up by setting a larger cluster number (see Figure¬†<a href="#S6.F4" title="Figure 4 ‚Ä£ 6.1 Handwritten Digits Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(b)). One of them, consisting of 47 records, can not be further split up with a minor increase in the cluster number. The label exploration view indicates that all of the records are with a ground-truth label of digit-1. To explore this cluster, we checked the cluster distributions of several dimensions with a high weight assigned by ccPCA. As shown in Figure¬†<a href="#S6.F5" title="Figure 5 ‚Ä£ 6.1 Handwritten Digits Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, certain dimensions are distributed in corners of the pictures. Verified by several records, we found that they were digit-1s in typography font (<span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_italic">R3.2</span>). The HFL model is inclined to
misidentify this rare handwritten style as other digit labels, like digit-7.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2208.07491/assets/x6.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="456" height="427" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The distributions of three dimensions selected from the dimension weight chart. The high dimension values demonstrate that the corresponding pixel is passed by strokes.</figcaption>
</figure>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">We annotated the 47 records in the cluster and inspected these records in the final round (i.e., the 200th round) (<span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_italic">R3.3</span>). Partial records of digit-1 with an extra dot or giant digit-0 are corrected, while the rest are not (see Figure¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b2) and (c4) to inspect the cluster of digit-1s in typography font). Although the number of inconsistent records decreases to 208, the accuracy of the HFL model is still not satisfying as the stand-lone training model. Therefore, we should suggest other clients collect more pictures of digit-0 and digit-1 to optimize the HFL model.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Face Mask Recognition</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In the second case, we trained a federated CNN model to recognize if the person in a color picture wears a mask. Two clients participated in the FL cooperation. Our client used the face mask image dataset provided by Jangra¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The other client employed the face mask detection dataset provided by Gurav¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. All records are unified to RGB images in the size of 28*28 pixels. The local training set has 6,000 records and the local test set has 1,792 records.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">We first observed the learning process from the parameter projection view (<span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">R1.2</span>). To compare high-level features extracted by CNN models, we selected the last two convolutional layers, respectively. Both the projected polylines of the parameters become relatively stable at the 90th round. We grouped the 547 inconsistent records identified at this round into 80 clusters (<span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">R3.1</span>). Three clusters with extra-low federated accuracy and a relatively large size caught our attention.
All of the records in these clusters are misclassified as with no mask by the HFL model. We then checked these records by clicking the highlighted circles in the label exploration view. As shown in Figure¬†<a href="#S6.F6" title="Figure 6 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a1), the first cluster with 7 pictures corresponds to a person who puts a banana on the mouth to disguise a mask. These records are labeled incorrectly. In the other two clusters, the masks with special patterns are neglected by the HFL model (see Figure¬†<a href="#S6.F6" title="Figure 6 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a2-a3)). We found more masks with special patterns after we set the cluster number as the recommendation (i.e., 125). As shown in Figure¬†<a href="#S6.F6" title="Figure 6 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a4), a new cluster with 15 records is identified. Similarly, all records are misclassified by the HFL model.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">To investigate the reasons, we compare discriminative regions of the HFL model with the stand-alone training model. Both Grad-CAMs highlight the same region (see Figure¬†<a href="#S6.F6" title="Figure 6 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(b)), which implies that the two models consider the same area as the basis of discrimination. However, the HFL model makes wrong judgments. We had a concern that the performance of the HFL model was affected because the data distributed in the other client is lacking our patterns.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2208.07491/assets/x7.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="456" height="514" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Cluster analysis of the inconsistency identified at the 90th round. (a) Inconsistency clusters with an accuracy of 0%. (b) The Grad-CAMs of the 4th cluster in (a).</figcaption>
</figure>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">To investigate the banana issue and the pattern issue, we annotated corresponding clusters and track them in the HFL process (<span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_italic">R3.3</span>). In the 200th round, the total of inconsistent records has decreased to 347. We reviewed the annotated records. Most pictures of masks with patterns are classified correctly by the HFL model (see Figure¬†<a href="#S6.F7" title="Figure 7 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). It turned out that the HFL model is capable of handling such statistical heterogeneity issues. However, a record with a ‚Äúbanana mask‚Äù was classified as with a mask. To optimize performance, we have to correct the labels of ‚Äúbanana masks‚Äù and notify the other client of this accident. We further compared model output by sample input (<span id="S6.SS2.p4.1.2" class="ltx_text ltx_font_italic">R2.2</span>). The convex hulls of the most inconsistency clusters have no overlap with local data records. To judge whether corresponding data heterogeneity exerts positive impact on our classification task, we need to collect corresponding records from real world and verify the output with ground-truth labels.</p>
</div>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2208.07491/assets/x8.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The label exploration view highlights the annotated records of colorful masks. Most of the records are consistent with the output of the stand-alone learning model, as well as the ground-truth labels.</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Vehicle Recognition</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In the third case, four clients seek accurate vehicle recognition by federated cooperation. Each client collected an equal number of pictures in three of four categories (i.e., plane, car, ship, and truck) from CIFAR-10 dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. No picture is shared by two clients. The analyzed client owns 5,400 pictures: 1,800 pictures of plane, car, and truck, respectively. 900 pictures are included in the test set.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">According to the pattern of parameter fluctuation (<span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">R1.1</span>), we split the carried-out training process into three stages at the 25th round and the 110th round. As shown in the parameter projection view (Figure¬†<a href="#S6.F8" title="Figure 8 ‚Ä£ 6.3 Vehicle Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(a)), parameters of the HFL model change significantly in the first 25 rounds, which indicates a fast learning process (<span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_italic">R1.2</span>). Then, the accuracy has approached its maximum around the 25th round (see Figure¬†<a href="#S6.F8" title="Figure 8 ‚Ä£ 6.3 Vehicle Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(b)). At the second stage, divergence could be observed between local updates and the HFL model from the directions of the arrows. At the end of the second stage (i.e., the 110th round), the loss has reached its minimum (see Figure¬†<a href="#S6.F8" title="Figure 8 ‚Ä£ 6.3 Vehicle Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(b)). In the last stage, the polyline in the parameter projection view fluctuates dramatically within a small range.</p>
</div>
<figure id="S6.F8" class="ltx_figure"><img src="/html/2208.07491/assets/x9.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="584" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The training process of the HFL model. (a) The updates of the parameters in the last convolutional layer become unstable with the increase of the communication rounds. (b) The accuracy and the loss fluctuation during the training process.</figcaption>
</figure>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">We checked the end of each stage to investigate the evolution of heterogeneity issues. Among 955 inconsistent records at the 25th round, 90% of them are classified incorrectly by the HFL model. The statistical results in the label exploration view demonstrate that the HFL model misclassified 425 records as ships (<span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_italic">R3.2</span>). Besides, the HFL model encountered difficulties in distinguishing between cars and trucks, which leads to other 225 incorrect records.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">In the 200th round, 162 records were still classified as a ship (<span id="S6.SS3.p4.1.1" class="ltx_text ltx_font_italic">R3.3</span>). To focus on the label distribution, we hid the scatters. As shown in Figure¬†<a href="#S6.F9" title="Figure 9 ‚Ä£ 6.3 Vehicle Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(a), most planes were projected inside or beside the convex hull of inconsistent records, because planes have similar features to ships (e.g., with blue background). Meanwhile, records with labels of car or truck are distributed far from the convex hulls (see the green grids in Figures¬†<a href="#S6.F9" title="Figure 9 ‚Ä£ 6.3 Vehicle Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(b) and (c)). The HFL model is inclined to classify a plane as a ship.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.1" class="ltx_p">Nevertheless, the car-and-track issue was drastically alleviated since the 110th round. To inspect the confusion among ground-truth labels at the final round, we checked the first three rows of the label exploration matrix, which corresponds to the records classified as plane, car, and truck by the HFL model. There are 36 inconsistent records in the non-diagonal cells (misclassified by the HFL model) and 157 ones in the diagonal cells (misclassified by the stand-alone training model). Therefore, the HFL model is superior to the stand-alone training model in the task of distinguishing local labels. In summary, we could benefit from the HFL cooperation. To fix the plane-ship issue, we should invite more partners to train the HFL model.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2208.07491/assets/x10.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The distributions of the three labels.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we discuss 1) expert reviews on <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">HetVis</span>, and 2) the result of a comparative study for the proposed context-aware clustering approach.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Expert Reviews</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">We interviewed three researchers (E3, E4, and E5) who had worked on FL for two years. For each interview, we first introduced our approach and then presented a demo of our system. After that, experts were allowed to freely explore data heterogeneity of the above three cases in our system. At the end of the interview, we collected their feedback on the following four aspects. Each interview lasted about an hour.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">Effectiveness.</span> Due to data isolation, existing approaches to heterogeneity analysis are mainly by observing local data and identifying skewed distributions. All experts agreed that our system could help them better to formulate reasonable hypotheses of heterogeneity than their original approaches. E3 said that the heterogeneity issues could be solved by adjusting records in the local client (e.g., expanding records) and findings in our system indicated which kind of adjustments could improve the performance of the HFL. E5 also noted that cluster analysis could efficiently locate heterogeneous issues and guide batch corrections.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">Usage experience.</span> The usability of our system received positive feedback from the experts. Both E3 and E4 appreciated the annotation functionality and commented that annotation allowed them to track certain sets of records. For visual designs, E3 and E5 were impressed by the intuitive representations in the parameter projection view. E3 commented that ‚ÄúIt can clearly reflect the conflicts between local updates and the HFL parameters.‚Äù Particularly, a disagreement occurred with the dimension exploration. E4 was inclined to navigate by ccPCA because it could summarize data characteristics. However, E5 preferred the model-driven Grad-CAM to understand model behaviors.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p"><span id="S7.SS1.p4.1.1" class="ltx_text ltx_font_bold">Findings.</span>
When the accuracy of either model was satisfying, E4 found that the inconsistent records could depict the boundary of two classes in the ccPCA projection. Based on this observation, E4 drew the conclusion that the HFL model and the stand-alone training model would fail to reach an agreement especially when the target records were hard to classify, i.e., the records were distributed along the classification boundary. E4 also found that ccPCA separated inconsistent records from the consistent context, which in the meantime split different classes (see Figures¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">HetVis: A Visual Analysis Approach for Identifying<span class="ltx_text"> </span>Data Heterogeneity in Horizontal Federated Learning</span></span>(b1) and <a href="#S6.F7" title="Figure 7 ‚Ä£ 6.2 Face Mask Recognition ‚Ä£ 6 Case Studies ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This indicated that the projection results could also help users to assess model performance.</p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.1" class="ltx_p"><span id="S7.SS1.p5.1.1" class="ltx_text ltx_font_bold">Advice.</span>
Despite the effectiveness and usefulness of our system, the experts offered two suggestions. First, we should take into account the architecture of the neural network when analyzing parameter exchanges (E3, E5). For example, users might focus on parameters in certain layers in the neural network when evaluating a deep learning model. Therefore, we improved the parameter projection view by grouping parameters based on layers. Users are allowed to select a layer and check parameters in the corresponding layer. Second, considering that dozens of labels might be yielded from the HFL model, E3 pointed out that the label exploration view needed an overview to facilitate object location. We plan to add it in the future version of our system.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Comparative Study</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">As mentioned in Section¬†<a href="#S4.SS2" title="4.2 Context-aware Clustering Approach ‚Ä£ 4 Models ‚Ä£ HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we proposed a context-aware clustering approach to extract heterogeneity issues. To prove the effectiveness of our clustering approach, we compared our approach with the distance-based clustering approach based on the local data in the first case to seek instance-level verification.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">Two clustering approaches were applied to cluster the 208 inconsistent records identified in the 200th round. The maximum difference¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> was employed to recommend appropriate cluster numbers for both clustering approaches. Our clustering approach generated 59 clusters, among which the largest cluster consisted of 45 records of typographic digit-1s while the distance-based approach yielded 118 clusters, none of which contained more than 6 records. Although the records in the same cluster were similar to each other, similar records (e.g., typographic digit-1s) were split into different clusters.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">To eliminate the effects of the recommendation algorithm, we set the cluster number to 100 for both approaches in the second experiment. The distance-based approach showed little change‚Äîthe largest cluster only consisted of 7 records. While our proposed approach still clustered 40 records of typographic digit-1s, which is much better than the other. The clustering results can be found in the supplemental material files.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Limitations and Future Work</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">We discuss two limitations of HetVis and summarize our future work.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p"><span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_bold">Scalability.</span> The PCA-based algorithms employed in HetVis can hardly support the analysis of data with thousands of dimensions. We need to integrate high-performance dimensionality reduction approaches and dimension recommendation approaches. As mentioned by E3, the design of the label exploration view also has difficulty in adapting to a large number of labels. We plan to optimize this view by providing an overview and recommending significant labels in the future.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p"><span id="S7.SS3.p3.1.1" class="ltx_text ltx_font_bold">Extensibility.</span> HetVis supports vector data, such as image data and tabular data. But text data and other data modalities are not supported by the current system due to different requirements of federated learning settings. Extending to new data modalities and machine learning tasks is an interesting future work.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we propose <span id="S8.p1.1.1" class="ltx_text ltx_font_italic">HetVis</span>, a visual analysis approach to assist identification and examination of data heterogeneity under the privacy limit of HFL. Instead of directly comparing local data and global data, we compare the output of the HFL model with a stand-alone training model.
A contrastive clustering analysis approach is leveraged to extract heterogeneity issues from the inconsistent records identified from the output comparison. In the future, we would like to extend our system to support online tuning of HFL model. The code of our system is available at the following link: <a target="_blank" href="https://github.com/EmmaammE/HetVis" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/EmmaammE/HetVis</a>.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the National Natural Science Foundation of China (No. 62132017, 61972389). The work of Tobias Schreck has been supported by the Austrian FFG-COMET-K1 Center Pro<sup id="S8.1.1" class="ltx_sup">2</sup>Future (Products and Production Systems of the Future), Contract No.¬†881844.



</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.¬†Adebayo, J.¬†Gilmer, M.¬†Muelly, I.¬†Goodfellow, M.¬†Hardt, and B.¬†Kim.

</span>
<span class="ltx_bibblock">Sanity checks for saliency maps.

</span>
<span class="ltx_bibblock">In S.¬†Bengio, H.¬†Wallach, H.¬†Larochelle, K.¬†Grauman, N.¬†Cesa-Bianchi,
and R.¬†Garnett, eds., <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of Advances in Neural Information
Processing Systems</span>, vol.¬†31. Curran Associates, Inc., 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E.¬†Alexander and M.¬†Gleicher.

</span>
<span class="ltx_bibblock">Task-driven comparison of topic models.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
22(1):320‚Äì329, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.¬†Cashman, G.¬†Patterson, A.¬†Mosca, N.¬†Watts, S.¬†Robinson, and R.¬†Chang.

</span>
<span class="ltx_bibblock">RNNbow: Visualizing learning via backpropagation gradients in RNNs.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Computer Graphics and Applications</span>, 38(6):39‚Äì50, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T.¬†Fujiwara, O.-H. Kwon, and K.-L. Ma.

</span>
<span class="ltx_bibblock">Supporting analysis of dimensionality reduction results with
contrastive learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
26(1):45‚Äì55, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S.¬†A. Guler, K.¬†Ellison, M.¬†Algamdi, H.¬†R. Collard, and C.¬†J. Ryerson.

</span>
<span class="ltx_bibblock">Heterogeneity in unclassifiable interstitial lung disease. a
systematic review and meta-analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Annals of the American Thoracic Society</span>, 15(7):854‚Äì863, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
O.¬†Gurav.

</span>
<span class="ltx_bibblock">Face mask detection dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/omkargurav/face-mask-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/omkargurav/face-mask-dataset</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N.¬†Halko, P.-G. Martinsson, and J.¬†A. Tropp.

</span>
<span class="ltx_bibblock">Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">SIAM Review</span>, 53(2):217‚Äì288, 2011.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J.¬†P. Higgins and S.¬†G. Thompson.

</span>
<span class="ltx_bibblock">Quantifying heterogeneity in a meta-analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Statistics in Medicine</span>, 21(11):1539‚Äì1558, 2002.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.¬†Jangra.

</span>
<span class="ltx_bibblock">Face mask <math id="bib.bib9.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="bib.bib9.1.m1.1a"><mo id="bib.bib9.1.m1.1.1" xref="bib.bib9.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.1.m1.1b"><csymbol cd="latexml" id="bib.bib9.1.m1.1.1.cmml" xref="bib.bib9.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">\sim</annotation></semantics></math>12k images dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.¬†Jaunet, R.¬†Vuillemot, and C.¬†Wolf.

</span>
<span class="ltx_bibblock">DRLViz: Understanding decisions and memory in deep reinforcement
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Computer Graphics Forum</span>, 39(3):49‚Äì61, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M.¬†Khodak, M.-F.¬†F. Balcan, and A.¬†S. Talwalkar.

</span>
<span class="ltx_bibblock">Adaptive gradient-based meta-learning methods.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pp.
5917‚Äì5928, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P.-J. Kindermans, S.¬†Hooker, J.¬†Adebayo, M.¬†Alber, K.¬†T. Sch√ºtt,
S.¬†D√§hne, D.¬†Erhan, and B.¬†Kim.

</span>
<span class="ltx_bibblock">The (un) reliability of saliency methods.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Explainable AI: Interpreting, Explaining and Visualizing Deep
Learning</span>, pp. 267‚Äì280. Springer, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.¬†Krause, A.¬†Dasgupta, J.¬†Swartz, Y.¬†Aphinyanaphongs, and E.¬†Bertini.

</span>
<span class="ltx_bibblock">A workflow for visual diagnostics of binary classifiers using
instance-level explanations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Visual Analytics Science and
Technology</span>, pp. 162‚Äì172.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A.¬†Krizhevsky, G.¬†Hinton, et¬†al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B.¬†C. Kwon, M.-J. Choi, J.¬†T. Kim, E.¬†Choi, Y.¬†B. Kim, S.¬†Kwon, J.¬†Sun, and
J.¬†Choo.

</span>
<span class="ltx_bibblock">RetainVis: Visual analytics with interpretable and interactive
recurrent neural networks on electronic medical records.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
25(1):299‚Äì309, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D.¬†Langan, J.¬†P. Higgins, D.¬†Jackson, J.¬†Bowden, A.¬†A. Veroniki,
E.¬†Kontopantelis, W.¬†Viechtbauer, and M.¬†Simmonds.

</span>
<span class="ltx_bibblock">A comparison of heterogeneity variance estimators in simulated
random-effects meta-analyses.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Research synthesis methods</span>, 10(1):83‚Äì98, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.¬†Lecun, C.¬†Cortes, and C.¬†J.C.¬†Burges.

</span>
<span class="ltx_bibblock">The MNIST database of handwritten digits.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://yann.lecun.com/exdb/mnist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist/</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L.¬†Li, Y.¬†Fan, M.¬†Tse, and K.-Y. Lin.

</span>
<span class="ltx_bibblock">A review of applications in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Computers &amp; Industrial Engineering</span>, p. 106854, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Q.¬†Li, X.¬†Wei, H.¬†Lin, Y.¬†Liu, T.¬†Chen, and X.¬†Ma.

</span>
<span class="ltx_bibblock">Inspecting the running process of horizontal federated learning via
visual analytics.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T.¬†Li, A.¬†K. Sahu, A.¬†Talwalkar, and V.¬†Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50‚Äì60, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T.¬†Li, A.¬†K. Sahu, M.¬†Zaheer, M.¬†Sanjabi, A.¬†Talwalkar, and V.¬†Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.06127</span>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M.¬†Liu, J.¬†Shi, K.¬†Cao, J.¬†Zhu, and S.¬†Liu.

</span>
<span class="ltx_bibblock">Analyzing the training processes of deep generative models.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
24(1):77‚Äì87, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
B.¬†McMahan, E.¬†Moore, D.¬†Ramage, S.¬†Hampson, and B.¬†A. y¬†Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pp. 1273‚Äì1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†Murugesan, S.¬†Malik, F.¬†Du, E.¬†Koh, and T.¬†M. Lai.

</span>
<span class="ltx_bibblock">Deepcompare: Visual and interactive comparison of deep learning model
performance.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Computer Graphics and Applications</span>, 39(5):47‚Äì59, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
N.¬†Pezzotti, T.¬†H√∂llt, J.¬†Van¬†Gemert, B.¬†P. Lelieveldt, E.¬†Eisemann, and
A.¬†Vilanova.

</span>
<span class="ltx_bibblock">DeepEyes: Progressive visual analytics for designing deep neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
24(1):98‚Äì108, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A.¬†Pister, P.¬†Buono, J.-D. Fekete, C.¬†Plaisant, and P.¬†Valdivia.

</span>
<span class="ltx_bibblock">Integrating prior knowledge in mixed initiative social network
clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.02972</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D.¬†Ren, S.¬†Amershi, B.¬†Lee, J.¬†Suh, and J.¬†D. Williams.

</span>
<span class="ltx_bibblock">Squares: Supporting interactive performance analysis for multiclass
classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
23(1):61‚Äì70, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R.¬†Rosenthal and M.¬†R. DiMatteo.

</span>
<span class="ltx_bibblock">Meta-analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Stevens‚Äô handbook of experimental psychology</span>, 2002.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
R.¬†R. Selvaraju, M.¬†Cogswell, A.¬†Das, R.¬†Vedantam, D.¬†Parikh, and D.¬†Batra.

</span>
<span class="ltx_bibblock">Grad-CAM: Visual explanations from deep networks via gradient-based
localization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pp. 618‚Äì626, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
V.¬†Smith, C.-K. Chiang, M.¬†Sanjabi, and A.¬†S. Talwalkar.

</span>
<span class="ltx_bibblock">Federated multi-task learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pp.
4424‚Äì4434, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R.¬†L. Thorndike.

</span>
<span class="ltx_bibblock">Who belongs in the family?

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Psychometrika</span>, 18(4):267‚Äì276, 1953.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J.¬†Wang, L.¬†Gou, H.¬†Yang, and H.-W. Shen.

</span>
<span class="ltx_bibblock">GANViz: A visual analytics approach to understand the adversarial
game.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
24(6):1905‚Äì1917, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X.¬†Wang, W.¬†Chen, J.¬†Xia, Z.¬†Chen, D.¬†Xu, X.¬†Wu, M.¬†Xu, and T.¬†Schreck.

</span>
<span class="ltx_bibblock">ConceptExplorer: Visual analysis of concept drifts in multi-source
time-series data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2020 IEEE Conference on Visual Analytics Science and
Technology</span>, pp. 1‚Äì11.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K.¬†Wei, J.¬†Li, M.¬†Ding, C.¬†Ma, H.¬†H. Yang, F.¬†Farokhi, S.¬†Jin, T.¬†Q. Quek, and
H.¬†V. Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454‚Äì3469, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J.¬†Wexler, M.¬†Pushkarna, T.¬†Bolukbasi, M.¬†Wattenberg, F.¬†Vi√©gas, and
J.¬†Wilson.

</span>
<span class="ltx_bibblock">The What-If Tool: Interactive probing of machine learning models.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
26(1):56‚Äì65, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J.¬†Xu, B.¬†S. Glicksberg, C.¬†Su, P.¬†Walker, J.¬†Bian, and F.¬†Wang.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Journal of Healthcare Informatics Research</span>, pp. 1‚Äì19, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z.¬†Xu, F.¬†Yu, J.¬†Xiong, and X.¬†Chen.

</span>
<span class="ltx_bibblock">Helios: heterogeneity-aware federated learning with dynamically
balanced collaboration.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of 2021 58th ACM/IEEE Design Automation
Conference</span>, pp. 997‚Äì1002, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
C.¬†Yang, Q.¬†Wang, M.¬†Xu, Z.¬†Chen, K.¬†Bian, Y.¬†Liu, and X.¬†Liu.

</span>
<span class="ltx_bibblock">Characterizing impacts of heterogeneity in federated learning upon
large-scale smartphone data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the Web Conference 2021</span>, pp. 935‚Äì946, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C.¬†Yang, Q.¬†Wang, M.¬†Xu, S.¬†Wang, K.¬†Bian, and X.¬†Liu.

</span>
<span class="ltx_bibblock">Heterogeneity-aware federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.06983</span>, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Q.¬†Yang, Y.¬†Liu, T.¬†Chen, and Y.¬†Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology</span>,
10(2):1‚Äì19, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J.¬†Yuan, C.¬†Chen, W.¬†Yang, M.¬†Liu, J.¬†Xia, and S.¬†Liu.

</span>
<span class="ltx_bibblock">A survey of visual analytics techniques for machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Computational Visual Media</span>, 7(1):3‚Äì36, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
C.¬†Zhang, S.¬†Li, J.¬†Xia, W.¬†Wang, F.¬†Yan, and Y.¬†Liu.

</span>
<span class="ltx_bibblock">BatchCrypt: Efficient homomorphic encryption for cross-silo
federated learning.

</span>
<span class="ltx_bibblock">In <math id="bib.bib42.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib42.1.m1.1a"><mo stretchy="false" id="bib.bib42.1.m1.1.1" xref="bib.bib42.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib42.1.m1.1b"><ci id="bib.bib42.1.m1.1.1.cmml" xref="bib.bib42.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib42.1.m1.1c">\{</annotation></semantics></math><span id="bib.bib42.2.1" class="ltx_text ltx_font_italic">USENIX<math id="bib.bib42.2.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib42.2.1.m1.1a"><mo stretchy="false" id="bib.bib42.2.1.m1.1.1" xref="bib.bib42.2.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib42.2.1.m1.1b"><ci id="bib.bib42.2.1.m1.1.1.cmml" xref="bib.bib42.2.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib42.2.1.m1.1c">\}</annotation></semantics></math> Annual Technical Conference</span>, pp. 493‚Äì506,
2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J.¬†Zhang, Y.¬†Wang, P.¬†Molino, L.¬†Li, and D.¬†S. Ebert.

</span>
<span class="ltx_bibblock">Manifold: A model-agnostic framework for interpretation and diagnosis
of machine learning models.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
25(1):364‚Äì373, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.07490" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.07491" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.07491">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.07491" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.07492" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 19:18:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
