<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments</title>
<!--Generated on Sat Oct  5 18:09:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04250v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S1" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S2" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S2.SS1" title="In 2 Related Work ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Semantic &amp; Panoptic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S2.SS2" title="In 2 Related Work ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Motion Planning with Semantic Understanding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS0.SSSx1" title="In 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">System Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS1" title="In 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Panoptic Segmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS1.SSSx1" title="In 3.1 Panoptic Segmentation ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Training Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS1.SSSx2" title="In 3.1 Panoptic Segmentation ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Dataset Preparation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS2" title="In 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dynamic Mapping and Tracking</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS2.SSSx1" title="In 3.2 Dynamic Mapping and Tracking ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Dynamic <abbr class="ltx_glossaryref" title=""></abbr>-based Mapping in 3D</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS2.SSSx2" title="In 3.2 Dynamic Mapping and Tracking ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Semantic Map Creation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS3" title="In 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Motion Planning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS1" title="In 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Panoptic Segmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS1.SSSx1" title="In 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Model Configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS1.SSSx2" title="In 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Performance Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS1.SSSx3" title="In 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title">Dataset Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS2" title="In 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Motion Planning with Semantic Understanding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S5" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions &amp; Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="id7.7.7" style="width:505.9pt;">
<span class="ltx_p" id="id4.4.4.4">Lorenzo Terenzi<sup class="ltx_sup" id="id4.4.4.4.1">†</sup>, Julian Nubert<sup class="ltx_sup" id="id4.4.4.4.2"><span class="ltx_text ltx_font_italic" id="id4.4.4.4.2.1">†,‡</span></sup>, Pol Eyschen<sup class="ltx_sup" id="id4.4.4.4.3">†</sup>, Pascal Roth<sup class="ltx_sup" id="id4.4.4.4.4">†</sup>,</span>
<span class="ltx_p ltx_align_center" id="id7.7.7.7">Simin Fei<sup class="ltx_sup" id="id7.7.7.7.1">†</sup>, Edo Jelavic<sup class="ltx_sup" id="id7.7.7.7.2">†</sup>, and Marco Hutter<sup class="ltx_sup" id="id7.7.7.7.3">†</sup></span>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id1">†</sup> Robotic Systems Lab, ETH Zürich, <sup class="ltx_sup" id="id11.11.id2">‡</sup> MPI for Intelligent Systems, Stuttgart, Germany 
<br class="ltx_break"/>Corresponding Author: Lorenzo Terenzi, <a class="ltx_ref ltx_href" href="mailto:lterenzi@ethz.ch" title="">lterenzi@ethz.ch</a>
</span><span class="ltx_author_notes">This work is supported by the NCCR digital fabrication &amp; robotics, the SNF project No. 188596, and the Max Planck ETH Center for Learning Systems.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection.
We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system’s application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios.
The dataset<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/" title="">https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/</a></span></span></span> and code<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/leggedrobotics/rsl_panoptic_mapping" title="">https://github.com/leggedrobotics/rsl_panoptic_mapping</a></span></span></span> for training and deployment are publicly available to support future research.</p>
</div>
<figure class="ltx_figure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_missing ltx_missing_image" id="S0.F1.g1" src=""/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="253" id="S0.F1.g2" src="x2.jpg" width="1196"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Autonomous navigation with the <span class="ltx_text ltx_font_italic" id="S0.F1.2.1">M545</span> excavator. The top image illustrates the model’s 2D panoptic segmentation prediction, while the bottom image depicts the panoptic map and the planned path of the navigation planner in black.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The construction industry is experiencing a growing need for advanced perception systems to tackle the challenges of dynamic, cluttered, and unstructured environments. As autonomous machinery becomes a focal point for improving safety, efficiency, and productivity in construction, robust scene understanding systems are crucial. Panoptic segmentation, which combines semantic and instance segmentation, offers a promising solution by providing pixel-level semantic labels and object-level instance boundaries. When integrated with 3D  <span class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_long">Light Detection and Ranging</span></span> (<abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>) data, panoptic segmentation enables more detailed and accurate 3D environmental models, essential for autonomous decision-making in construction sites.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, developing effective panoptic segmentation models for construction environments is challenging. Real-world construction sites present unique obstacles, such as varying terrain, dynamic actors (e.g., workers and machinery), and limited availability of labeled data to train perception networks. Previous efforts, like those by Guan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib10" title="">10</a>]</cite>, have shown that specialized construction datasets can improve segmentation performance, but noise and limited class diversity remain. Furthermore, integrating <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> data - while enhancing geometric understanding - adds complexity to training and deployment due to the difficulty of annotating 3D data in such environments.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This work addresses these challenges by developing a comprehensive panoptic segmentation system tailored explicitly for dynamic construction sites. Our approach fine-tunes pre-trained panoptic segmentation models using a custom small construction dataset to improve generalization with limited training data. We also propose a dynamic panoptic mapping technique that fuses 2D image-based segmentation with 3D <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> data to create detailed, real-time maps of construction environments. These maps are enriched with semantic and geometric information, providing a robust foundation for downstream applications, such as navigation.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As a case study, we demonstrate the application of our panoptic segmentation system for autonomous navigation in dynamic construction sites, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S0.F1" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Leveraging the generated dynamic panoptic maps, we integrate them into an online RRT* planner, enabling reactive path planning in unstructured environments. The top image in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S0.F1" title="In ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> illustrates the model’s panoptic segmentation prediction of the scene. In contrast, the bottom image depicts the corresponding panoptic map and the resulting planned path in black.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions include:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a structured approach for fine-tuning large, pre-trained panoptic segmentation models on specialized datasets. We provide recommendations on training regimes, model sizes, and dataset sizes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a method for creating dynamic panoptic maps in environments with scarce labeled data. This method combines image panoptic segmentation and <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-based object detection and tracking to enable robust and safe navigation planning.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We introduce a dataset of 502 images with hand-labeled panoptic annotations from multiple locations covering 35 actively selected semantic categories.<sup class="ltx_sup" id="S1.I1.i3.p1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#footnote1" title="Footnote 1 ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">1</span></a></sup></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We release the code for model fine-tuning and semantic mapping to foster future robotic research.<sup class="ltx_sup" id="S1.I1.i4.p1.1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#footnote2" title="Footnote 2 ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">2</span></a></sup></p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The following sections summarize recent advancements in semantic scene understanding, focusing on transformer-based panoptic segmentation and its implications for motion planning.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic &amp; Panoptic Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The segmentation field has witnessed significant progress with the advent of semantic and instance segmentation techniques, particularly with the introduction of transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib31" title="">31</a>]</cite> architectures for panoptic segmentation. Semantic segmentation assigns a class to each pixel, such as <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">’sky’</span> or <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">’road’</span>, whereas instance segmentation identifies and delineates each object instance. Pioneering methods such as Fully Convolutional Networks (FCN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib20" title="">20</a>]</cite> and DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib3" title="">3</a>]</cite> laid the groundwork for contemporary segmentation models. Following this, instance segmentation was revolutionized by frameworks like Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Panoptic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib16" title="">16</a>]</cite> introduced a unified approach to segmentation, merging semantic and instance segmentation tasks. This led to the creation of models such as Panoptic FPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib15" title="">15</a>]</cite> and EfficientPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib23" title="">23</a>]</cite>, incorporating features from both segmentation domains.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">A notable leap in segmentation quality was achieved through transformer-based decoders, with models like DETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib2" title="">2</a>]</cite> setting new benchmarks at the time. Employing pre-trained transformer encoders, such as Swin and ViT, trained in a supervised manner on large-scale datasets like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib6" title="">6</a>]</cite>, has substantially enhanced panoptic segmentation results over the past few years.
More recent models, including MaskFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib5" title="">5</a>]</cite>, Mask2Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib4" title="">4</a>]</cite>, and PanopticSegformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib17" title="">17</a>]</cite> exemplify this advancement, offering significant improvements in model output quality.
Motivated by this, we investigate the usage and fine-tuning of DETR and Mask2Former in this work and highlight that Mask2Former can significantly outperform older architectures like DETR.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Motion Planning with Semantic Understanding</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Panoptic segmentation enhances motion planning in autonomous driving by allowing a more fine-grained understanding of the surrounding environment.
In 3D object detection, <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short-plural">LiDARs</span></abbr> and visible light cameras are the most common sensors. Among the <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-based detection state-of-the-art systems are pillar-based networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib18" title="">18</a>]</cite>, sparse 3D voxel transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib32" title="">32</a>]</cite>, and PointNet-like transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib33" title="">33</a>]</cite>. Conversely, camera-only approaches, such as those converting 2D images to bird’s-eye view maps for planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib27" title="">27</a>]</cite>, offer cheaper alternative solutions but generally lag in geometric precision when compared to <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-based methods. These camera-only approaches typically project camera images into frustum-shaped features or employ dense transform layers to generate semantic occupancy maps from multi-scale image features.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Hybrid methods that integrate both <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> and camera data often propose end-to-end trained  <span class="ltx_glossaryref" title="Neural Network"><span class="ltx_text ltx_glossary_long">Neural Network</span></span> (<abbr class="ltx_glossaryref" title="Neural Network"><span class="ltx_text ltx_glossary_short">NN</span></abbr>) solutions that rely on extensive datasets, such as Nuscenes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib1" title="">1</a>]</cite> and Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib22" title="">22</a>]</cite>, which include labeled point cloud and image data.
Examples are <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib9" title="">9</a>]</cite>, producing  <span class="ltx_glossaryref" title="Bird’s-Eye View"><span class="ltx_text ltx_glossary_long">Bird’s-Eye View</span></span> (<abbr class="ltx_glossaryref" title="Bird’s-Eye View"><span class="ltx_text ltx_glossary_short">BEV</span></abbr>) representations for planning and demonstrating slightly superior performance in 3D object detection compared to <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-only solutions. Moreover, using imperative learning, Roth et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib28" title="">28</a>]</cite> learn a reactive local end-to-end navigation solution for urban navigation from depth-camera data and semantic images without explicitly creating a <abbr class="ltx_glossaryref" title="Bird’s-Eye View"><span class="ltx_text ltx_glossary_short">BEV</span></abbr> representation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Challenges arise in domains lacking extensive labeled data, where neither autonomous driving datasets nor conventional methods generalize well. For <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-only applications, such as off-road navigation, sparse 3D Convolutional Neural Networks have been explored for classifying <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> inputs into traversability classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib29" title="">29</a>]</cite>. However, these methods struggle with a broad array of classes and terrains. Camera-only and hybrid sensor approaches face limitations in accurately identifying and classifying diverse objects and terrain types due to the inherent restrictions of range sensors and the complexity of integrating sensor data.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">To address these challenges, recent efforts have focused on generating high-definition maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib13" title="">13</a>]</cite> from combined 2D images and 3D point clouds. This leads to coherent semantic maps projected onto occupancy grids, as discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib25" title="">25</a>]</cite>. Further, integrating semantic and geometric data for off-road navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib10" title="">10</a>]</cite>, utilizing fused <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> and camera information to create semantic 2D maps. These maps inform motion planning with predefined traversability costs but lack explicit handling of dynamic obstacles, occlusions, and entities outside the camera’s field of view.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">In this work, we also deploy an explicit mapping-based approach, where 2D panoptic image estimates are lifted to 3D through explicit lidar-based geometric projection. Moreover, we track objects through time and dynamically update the underlying map representation without forgetting, allowing for reliable operation in dynamic and complex environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1107" id="S3.F2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the integrated camera and <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> data processing and semantic mapping pipeline for autonomous navigation in unstructured environments.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section outlines our method for combining camera and <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> data to enable excavator navigation in unstructured settings.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">System Overview</h4>
<div class="ltx_para" id="S3.SS0.SSSx1.p1">
<p class="ltx_p" id="S3.SS0.SSSx1.p1.1">We use a panoptic segmentation network (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS1" title="3.1 Panoptic Segmentation ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) to process the camera feed, identify objects, and integrate this information into a 3D point cloud map via our mapping and tracking system (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS2" title="3.2 Dynamic Mapping and Tracking ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). This allows for real-time map updates and object tracking. Utilizing this map, our navigation system creates a 2D traversability and cost map (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS2.SSSx2" title="Semantic Map Creation ‣ 3.2 Dynamic Mapping and Tracking ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). This is an ideal representation for real-world downstream tasks such as navigation (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.SS3" title="3.3 Motion Planning ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>). An overview of the system is presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.F2" title="In 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="714" id="S3.F3.1.g1" src="extracted/5904143/figures/approach/fig3/00038.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="714" id="S3.F3.2.g1" src="extracted/5904143/figures/approach/fig3/00149.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="714" id="S3.F3.3.g1" src="extracted/5904143/figures/approach/fig3/00246.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="714" id="S3.F3.4.g1" src="extracted/5904143/figures/approach/fig3/00354.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="714" id="S3.F3.5.g1" src="extracted/5904143/figures/approach/fig3/00372.jpg" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Exemplary samples from the generated and hand-labeled dataset. The 502 images published in this work are recorded in various environments, including real-world construction sites, road navigation, and natural environments.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Panoptic Segmentation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This study investigates two transformer-based segmentation models: Mask2Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib4" title="">4</a>]</cite> and DETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib2" title="">2</a>]</cite>. Mask2Former is an encoder-decoder transformer that can use Swin Transformer (available in <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">tiny</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">big</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">large</span> sizes) as a backbone encoder. On the other hand, DETR takes a different path by using attention mechanisms only during its decoding stage while still using a ResNet-50 as an encoder. We kept the original design of both models but tweaked the final layers to match our specific needs for labels.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Training Procedure</h4>
<div class="ltx_para" id="S3.SS1.SSSx1.p1">
<p class="ltx_p" id="S3.SS1.SSSx1.p1.1">We started training with pre-existing weights from the Imagenet 21k/1k for the encoders (backbone). Moreover, we used weights from the COCO dataset training for the segmentation heads/decoder, as COCO covers a wide range of objects found in various scenes, such as roads and indoors.
Since our work focuses on construction sites, we simplified our labels by removing unrelated ones (like food and animals). We combined similar ones into broader categories (e.g., <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p1.1.1">building-other-merged</span>). We also added new labels important for our work, such as <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p1.1.2">bucket</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p1.1.3">gripper</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p1.1.4">self-arm</span>, refining our label list to <span class="ltx_text ltx_font_bold" id="S3.SS1.SSSx1.p1.1.5">34</span> categories.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx1.p2">
<p class="ltx_p" id="S3.SS1.SSSx1.p2.1">For DETR, we followed a three-step training approach: <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p2.1.1">i)</span> first, we fine-tuned the model with our data, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p2.1.2">ii)</span> then fixed the main part of the model while training the segmentation parts and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx1.p2.1.3">iii)</span> finally trained the whole model together for up to 200 epochs. We kept the Mask2Former training close to the original settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib4" title="">4</a>]</cite> but made some minor adjustments to fit our training on a standard GPU and to adjust the final layer for our dataset’s categories. All implementational (training) details can be found in our public repository.<sup class="ltx_sup" id="S3.SS1.SSSx1.p2.1.4"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#footnote2" title="Footnote 2 ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">2</span></a></sup></p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Dataset Preparation</h4>
<div class="ltx_para" id="S3.SS1.SSSx2.p1">
<p class="ltx_p" id="S3.SS1.SSSx2.p1.1">We curated a specialized dataset for training and testing our panoptic segmentation approach, consisting of 502 images from various construction sites, with detailed hand-labeled segmentation masks created using <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSSx2.p1.1.1">Segments.ai</em>. This includes specific construction labels like <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx2.p1.1.2">container</span>, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx2.p1.1.3">stone</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSSx2.p1.1.4">gravel-pile</span>.
An overview of the different sites present in the dataset can be seen in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.F3" title="In System Overview ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
Since some labels (e.g., people and cars) are underrepresented in our dataset, we also incorporated COCO images - containing at least five target classes - during model fine-tuning.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.p2">
<p class="ltx_p" id="S3.SS1.SSSx2.p2.1">As a result, the dataset is split into a training set with 1290 COCO images and 427 construction site images, while the validation set contains 75 construction site images. The dataset is open-sourced and publicly available.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://segments.ai/leggedrobotics/construction_site/" title="">https://segments.ai/leggedrobotics/construction_site/</a></span></span></span>
The corresponding preprocessing code is publicly accessible with the released codebase.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/leggedrobotics/rsl_panoptic/tree/main/panoptic_models/panoptic_models/data" title="">https://github.com/leggedrobotics/rsl_panoptic/tree/main/panoptic_models/panoptic_models/data</a></span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dynamic Mapping and Tracking</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Dynamic <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-based Mapping in 3D</h4>
<div class="ltx_para" id="S3.SS2.SSSx1.p1">
<p class="ltx_p" id="S3.SS2.SSSx1.p1.1">Our dynamic mapping and tracking system processes raw <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> scans, separating them into two labeled point clouds: one representing the static environment (”stuff”) and the other the potentially dynamic objects (”things”). Each dynamic object is tagged with a unique tracking ID. We don’t register the scans actively on the map (e.g., done in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib13" title="">13</a>]</cite>), but we feed them directly to the 2D semantic mapping module described in the next subsubsection based on the current state estimation output. We use the latest version of Graph-MSF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib24" title="">24</a>]</cite> for state and motion estimation, fusing IMU, GNSS, and LiDAR measurements.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx1.p2">
<p class="ltx_p" id="S3.SS2.SSSx1.p2.1">First, all <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> points are projected onto the camera’s image plane, labeled based on the closest pixel in the image segmentation mask. The points outside the camera’s view are assigned an ”unknown” label. We then use DBSCAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib7" title="">7</a>]</cite> to detect clusters of points with the same label. The ground plane of the <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr> scan is removed for this step to get disjoint clusters. A majority vote over the constituting points’ labels determines each cluster’s overall label. The majority label must cross a defined threshold to minimize false detections. The cluster is assigned the ”unknown” label if the label cannot be conclusively determined. The bounding boxes’ position and velocity of the identified clusters are then tracked by a Kalman Filter, assuming constant velocity of the point cluster, even when leaving the field of view by retaining the classes of previous camera-based observations. All points within bounding boxes belonging to either ”things” or unknown labels are added to the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx1.p2.1.1">dynamic</span> cloud.
All the remaining points, including the ones within bounding boxes belonging to ”stuff” labels, are assigned the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx1.p2.1.2">static</span> label.
An exemplary static and dynamic point cloud is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S3.F4" title="In Dynamic -based Mapping in 3D ‣ 3.2 Dynamic Mapping and Tracking ‣ 3 Proposed Approach ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S3.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Segmentation of point cloud data into dynamic (red) and static (blue) elements, highlighting dynamic object tracking (e.g., people, unidentified objects) as bounding boxes.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Semantic Map Creation</h4>
<div class="ltx_para" id="S3.SS2.SSSx2.p1">
<p class="ltx_p" id="S3.SS2.SSSx2.p1.1">A semantic 2D map developed with the GridMap library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib8" title="">8</a>]</cite> is at the core of our mapping and tracking method. This map is divided into two primary layers, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p1.1.1">i)</span> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p1.1.2">static</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p1.1.3">ii)</span> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p1.1.4">dynamic</span>, to differentiate between the non-movable surroundings (”stuff”) and the movable object detections (”things”) from before. Two grid-map layers are necessary to remember the surroundings, even in the presence of dynamic objects, instead of being overwritten.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.p2">
<p class="ltx_p" id="S3.SS2.SSSx2.p2.1">The static layers use the static point cloud generated by the tracking system, while the movable one uses the tracked dynamic point cloud. Fixed parts of the environment, such as walls, terrain, and permanent structures, are categorized as non-movable, and their information in the static layer is retained, even for parts no longer visible to the mapping system. Conversely, movable objects are dynamically updated and removed as new information becomes available from the dynamic cloud.
In each update cycle, the layers merge into a single semantic layer, with <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p2.1.1">dynamic</span> objects superseding <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p2.1.2">static</span> map entries. For instance, if a person walks over gravel, the gravel cell’s classification changes to ”person” before returning to ”gravel” if the person continues.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.p3">
<p class="ltx_p" id="S3.SS2.SSSx2.p3.1">We then produce two new maps from this merged layer: an <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.1">occupancy map</span> based on class metadata and a <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.2">cost map</span>.
Semantic terrain classes like <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.3">road</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.4">gravel</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.5">grass</span> are marked as traversable, while classes like <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.6">person</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSSx2.p3.1.7">fence</span> are non-traversable. The occupancy map aids the sampling planner in eliminating unfeasible routes, and the cost map helps to calculate trajectory costs, factoring in terrain difficulties such as mud or water, which are challenging for construction machinery. The traversability costs are manually assigned and stored in the class metadata.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSSx2.p4">
<p class="ltx_p" id="S3.SS2.SSSx2.p4.1">Moreover, an existing site’s geometric or a priori traversability map can be integrated with the occupancy map derived from the semantic map. This combination, executed with a ”union” operation, is helpful for pre-known site dimensions or geofencing the machinery.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Motion Planning</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our approach for path finding employs online RRT* implemented using the Open Motion Planning Library (OMPL). The planner checks the path’s validity through an occupancy map, considering a path valid only if the occupancy values under the excavator’s footprint are zero, indicating an absence of obstacles. The total trajectory cost, <math alttext="C_{\text{total}}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">C</mi><mtext id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3a.cmml">total</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝐶</ci><ci id="S3.SS3.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p1.1.m1.1.1.3">total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">C_{\text{total}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_C start_POSTSUBSCRIPT total end_POSTSUBSCRIPT</annotation></semantics></math>, is calculated as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="C_{\text{total}}=\lambda_{1}C_{\text{length}}+\lambda_{2}C_{\text{semantic}}," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">C</mi><mtext id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3a.cmml">total</mtext></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.cmml">λ</mi><mn id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml">1</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2.cmml">C</mi><mtext id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3a.cmml">length</mtext></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mn id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2.cmml">C</mi><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml">semantic</mtext></msub></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><mtext id="S3.E1.m1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.2.3">total</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><times id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1"></times><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2">𝜆</ci><cn id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.2.2.3">1</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.2.3.3">length</mtext></ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">𝜆</ci><cn id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.3.2.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.3.3.3">semantic</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">C_{\text{total}}=\lambda_{1}C_{\text{length}}+\lambda_{2}C_{\text{semantic}},</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_C start_POSTSUBSCRIPT total end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT length end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT semantic end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.5">where <math alttext="C_{\text{length}}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m1.1"><semantics id="S3.SS3.p1.2.m1.1a"><msub id="S3.SS3.p1.2.m1.1.1" xref="S3.SS3.p1.2.m1.1.1.cmml"><mi id="S3.SS3.p1.2.m1.1.1.2" xref="S3.SS3.p1.2.m1.1.1.2.cmml">C</mi><mtext id="S3.SS3.p1.2.m1.1.1.3" xref="S3.SS3.p1.2.m1.1.1.3a.cmml">length</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m1.1b"><apply id="S3.SS3.p1.2.m1.1.1.cmml" xref="S3.SS3.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m1.1.1.1.cmml" xref="S3.SS3.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m1.1.1.2.cmml" xref="S3.SS3.p1.2.m1.1.1.2">𝐶</ci><ci id="S3.SS3.p1.2.m1.1.1.3a.cmml" xref="S3.SS3.p1.2.m1.1.1.3"><mtext id="S3.SS3.p1.2.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p1.2.m1.1.1.3">length</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m1.1c">C_{\text{length}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m1.1d">italic_C start_POSTSUBSCRIPT length end_POSTSUBSCRIPT</annotation></semantics></math> is the path’s total length in meters, and <math alttext="C_{\text{semantic}}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m2.1"><semantics id="S3.SS3.p1.3.m2.1a"><msub id="S3.SS3.p1.3.m2.1.1" xref="S3.SS3.p1.3.m2.1.1.cmml"><mi id="S3.SS3.p1.3.m2.1.1.2" xref="S3.SS3.p1.3.m2.1.1.2.cmml">C</mi><mtext id="S3.SS3.p1.3.m2.1.1.3" xref="S3.SS3.p1.3.m2.1.1.3a.cmml">semantic</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m2.1b"><apply id="S3.SS3.p1.3.m2.1.1.cmml" xref="S3.SS3.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m2.1.1.1.cmml" xref="S3.SS3.p1.3.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m2.1.1.2.cmml" xref="S3.SS3.p1.3.m2.1.1.2">𝐶</ci><ci id="S3.SS3.p1.3.m2.1.1.3a.cmml" xref="S3.SS3.p1.3.m2.1.1.3"><mtext id="S3.SS3.p1.3.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p1.3.m2.1.1.3">semantic</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m2.1c">C_{\text{semantic}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m2.1d">italic_C start_POSTSUBSCRIPT semantic end_POSTSUBSCRIPT</annotation></semantics></math> represents the aggregated semantic cost from the cost map. The coefficients <math alttext="\lambda_{1}=1" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m3.1"><semantics id="S3.SS3.p1.4.m3.1a"><mrow id="S3.SS3.p1.4.m3.1.1" xref="S3.SS3.p1.4.m3.1.1.cmml"><msub id="S3.SS3.p1.4.m3.1.1.2" xref="S3.SS3.p1.4.m3.1.1.2.cmml"><mi id="S3.SS3.p1.4.m3.1.1.2.2" xref="S3.SS3.p1.4.m3.1.1.2.2.cmml">λ</mi><mn id="S3.SS3.p1.4.m3.1.1.2.3" xref="S3.SS3.p1.4.m3.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p1.4.m3.1.1.1" xref="S3.SS3.p1.4.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.4.m3.1.1.3" xref="S3.SS3.p1.4.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m3.1b"><apply id="S3.SS3.p1.4.m3.1.1.cmml" xref="S3.SS3.p1.4.m3.1.1"><eq id="S3.SS3.p1.4.m3.1.1.1.cmml" xref="S3.SS3.p1.4.m3.1.1.1"></eq><apply id="S3.SS3.p1.4.m3.1.1.2.cmml" xref="S3.SS3.p1.4.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m3.1.1.2.1.cmml" xref="S3.SS3.p1.4.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.4.m3.1.1.2.2.cmml" xref="S3.SS3.p1.4.m3.1.1.2.2">𝜆</ci><cn id="S3.SS3.p1.4.m3.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.4.m3.1.1.2.3">1</cn></apply><cn id="S3.SS3.p1.4.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p1.4.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m3.1c">\lambda_{1}=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m3.1d">italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1</annotation></semantics></math> and <math alttext="\lambda_{2}=0.1" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m4.1"><semantics id="S3.SS3.p1.5.m4.1a"><mrow id="S3.SS3.p1.5.m4.1.1" xref="S3.SS3.p1.5.m4.1.1.cmml"><msub id="S3.SS3.p1.5.m4.1.1.2" xref="S3.SS3.p1.5.m4.1.1.2.cmml"><mi id="S3.SS3.p1.5.m4.1.1.2.2" xref="S3.SS3.p1.5.m4.1.1.2.2.cmml">λ</mi><mn id="S3.SS3.p1.5.m4.1.1.2.3" xref="S3.SS3.p1.5.m4.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.5.m4.1.1.1" xref="S3.SS3.p1.5.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.5.m4.1.1.3" xref="S3.SS3.p1.5.m4.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m4.1b"><apply id="S3.SS3.p1.5.m4.1.1.cmml" xref="S3.SS3.p1.5.m4.1.1"><eq id="S3.SS3.p1.5.m4.1.1.1.cmml" xref="S3.SS3.p1.5.m4.1.1.1"></eq><apply id="S3.SS3.p1.5.m4.1.1.2.cmml" xref="S3.SS3.p1.5.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m4.1.1.2.1.cmml" xref="S3.SS3.p1.5.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.5.m4.1.1.2.2.cmml" xref="S3.SS3.p1.5.m4.1.1.2.2">𝜆</ci><cn id="S3.SS3.p1.5.m4.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.5.m4.1.1.2.3">2</cn></apply><cn id="S3.SS3.p1.5.m4.1.1.3.cmml" type="float" xref="S3.SS3.p1.5.m4.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m4.1c">\lambda_{2}=0.1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m4.1d">italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.1</annotation></semantics></math> are chosen to weigh the path length and semantic costs, respectively. The semantic cost for each path segment is computed by averaging the traversability costs within the excavator’s footprint, summing these averages for the total path semantic cost.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The path planner functions in real-time, consistently operating at 1 Hz with a maximum planning time of 0.95 seconds. This allows the robot to adjust its route dynamically, always seeking the most efficient path. Trajectories are calculated based on a constant velocity model from the robot’s projected future position at the end of the planning cycle. If a new trajectory offers a lower cost than the current path’s remaining cost, the system switches to the latest trajectory. Trajectories that pose a collision risk are assigned an infinite cost, causing the robot to stop if a potential collision is detected within a 3-meter radius until a safe path can be found.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate our system through both offline and online experiments. <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">i)</span> Offline, we perform segmentation evaluations on our dataset (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS1" title="4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>). <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">ii)</span> Online, we conduct system tests—including segmentation, tracking, and planning—by deploying the system on a <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">Menzi Muck M545</span> excavator (<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.SS2" title="4.2 Motion Planning with Semantic Understanding ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). This machine is equipped with a vertically mounted 4K <span class="ltx_text ltx_font_italic" id="S4.p1.1.4">Ximea xiX</span> PCIe camera and an <span class="ltx_text ltx_font_italic" id="S4.p1.1.5">Ouster OS0</span> <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>. For detailed information on the robot, refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Panoptic Segmentation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In the following, we report three metrics:  <span class="ltx_glossaryref" title="Panoptic Quality"><span class="ltx_text ltx_glossary_long">Panoptic Quality</span></span> (<abbr class="ltx_glossaryref" title="Panoptic Quality"><span class="ltx_text ltx_glossary_short">PQ</span></abbr>),  <span class="ltx_glossaryref" title="Segmentation Quality"><span class="ltx_text ltx_glossary_long">Segmentation Quality</span></span> (<abbr class="ltx_glossaryref" title="Segmentation Quality"><span class="ltx_text ltx_glossary_short">SQ</span></abbr>), and  <span class="ltx_glossaryref" title="Recognition Quality"><span class="ltx_text ltx_glossary_long">Recognition Quality</span></span> (<abbr class="ltx_glossaryref" title="Recognition Quality"><span class="ltx_text ltx_glossary_short">RQ</span></abbr>). PQ measures overall performance by evaluating the segmentation and correct identification of each object instance. SQ assesses the accuracy of the segmented shapes regardless of their classification, while the RQ evaluates the ability to classify segmented objects correctly.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Model Configurations</h4>
<div class="ltx_para" id="S4.SS1.SSSx1.p1">
<p class="ltx_p" id="S4.SS1.SSSx1.p1.1">For DETR, we use a ResNet50 (25M parameters) backbone pre-trained on ImageNet 1k. For Mask2Former, we train three variants, each equipped with a different-sized backbone: Swin-Tiny (29M), Swin-Big (88M), and Swin-Large (200M), all initially pre-trained on ImageNet 21k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx1.p2">
<p class="ltx_p" id="S4.SS1.SSSx1.p2.1">Initially, we employ DETR to determine the optimal training configuration for the best downstream performance. Our findings indicate that a three-stage training process—first focusing on the box detector, followed by the segmentation head, and finally fine-tuning the entire network—produces better outcomes than merely completing the first two stages without adjusting the backbone. This improvement is quantified in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.T1" title="In Model Configurations ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, suggesting that fine-tuning the backbone is beneficial, especially for our robot’s first-person perspective dataset, which features unique objects and surfaces not widely represented in datasets like ImageNet 21k and COCO.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation of the two DETR training methods.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">PQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">SQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">RQ</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Freeze backbone</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.44</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.1.3.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Fine-tune backbone</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.3.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.2.1">0.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.3.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.3.1">0.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.3.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.4.1">0.52</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSSx1.p3">
<p class="ltx_p" id="S4.SS1.SSSx1.p3.1">For Mask2Former, we found the default fine-tuning procedure<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib4" title="">4</a>]</cite> to work well in practice.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results of DETR and Mask2Former (M2F) on the ”construction site” validation set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">PQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">SQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">RQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">Images/s</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">DETR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.5.1">12</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">M2F Swin-Tiny</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.2.1">0.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.3.1">0.79</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.4.1">0.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">M2F Swin-Big</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.1.5.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">M2F Swin-Large</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.5.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.2.1">0.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.5.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.3.1">0.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.5.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.5.4.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">1</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Performance Comparison</h4>
<div class="ltx_para" id="S4.SS1.SSSx2.p1">
<p class="ltx_p" id="S4.SS1.SSSx2.p1.1">Our results, summarized in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.T2" title="In Model Configurations ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, highlight the superior performance of the fully transformer-based Mask2Former models. All models are trained until convergence. Due to memory constraints on the training GPU (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSSx2.p1.1.1">Nvidia RTX 3090</span>), the Swin-Big and Swin-Large models were fine-tuned with a batch size of only 1.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx2.p2">
<p class="ltx_p" id="S4.SS1.SSSx2.p2.1">Despite its smaller size, the Swin-Tiny backbone performs well, suggesting that the larger models may require more extensive data and computational resources for optimal training. Additionally, Mask2Former with Swin-Tiny exhibits favorable inference throughput rates, making it an ideal candidate for real-time applications on our robot equipped with an <span class="ltx_text ltx_font_italic" id="S4.SS1.SSSx2.p2.1.1">Nvidia RTX 3080Ti</span>. <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.F5" title="In Performance Comparison ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows two exemplary segmentation estimates.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="679" id="S4.F5.1.g1" src="extracted/5904143/figures/results/fig5/image_0290.jpg" width="568"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="679" id="S4.F5.2.g1" src="extracted/5904143/figures/results/fig5/image_0464.jpg" width="568"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Two exemplary segmentation masks produced by Mask2Former.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSSx2.p3">
<p class="ltx_p" id="S4.SS1.SSSx2.p3.1">A critical aspect of all segmentation systems is setting the suitable confidence threshold, which determines whether a prediction is certain enough to be considered valid. We observed that Mask2Former often struggles to classify terrain types like dirt, gravel, or pavement, which could plausibly belong to multiple categories. This occurs because the model distributes its confidence across several classes, resulting in no single prediction meeting the confidence threshold. This challenge is illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.F6" title="In Performance Comparison ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, highlighting a limitation inherent to current vision systems with a fixed set of categories.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="679" id="S4.F6.1.g1" src="extracted/5904143/figures/results/fig6/image_0476.jpg" width="568"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="679" id="S4.F6.2.g1" src="extracted/5904143/figures/results/fig6/frame026931_rotated.jpg" width="568"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Problematic segmentation due to confidence threshold: Mask2Former’s inability to decisively segment ambiguous scenes such as dirt tracks and potentially misclassified debris.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Dataset Evaluation</h4>
<div class="ltx_para" id="S4.SS1.SSSx3.p1">
<p class="ltx_p" id="S4.SS1.SSSx3.p1.1">Next, we examine how the distribution of classes in the training dataset impacts the performance of downstream tasks. Given our dataset’s modest size of around 500 images, we investigated whether models trained solely on our data would overfit, diminishing their effectiveness in segmenting unfamiliar scenes. By training DETR on either solely our dataset or a combination with a subset of the COCO dataset, we observed a significant performance degradation when using only our dataset, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.T3" title="In Dataset Evaluation ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>. This outcome points to severe overfitting. We discovered that a ratio of about 3:1 (COCO to our dataset) best improves generalization without compromising segmentation accuracy in our specific domain. Hence, we included a COCO dataset subset in all training runs. We advise incorporating parts of a larger, diverse dataset when working with small datasets to mitigate the risk of catastrophic forgetting.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>DETR performance on construction site and COCO-only validation sets, with different training sets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Train set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Val set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">PQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">SQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1">RQ</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.1.1" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T3.1.2.1.1.1">Custom</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">COCO val</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.09</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.3.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Custom val</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.41</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.54</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.51</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.1.4.3.1" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T3.1.4.3.1.1">Custom + COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.4.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">COCO val</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.4.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.4.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.4.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.36</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T3.1.5.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Custom val</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.5.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.5.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.5.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.52</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSSx3.p2">
<p class="ltx_p" id="S4.SS1.SSSx3.p2.1">Another interesting aspect is determining the optimal size of the dataset. The necessary volume of data depends on the application’s tolerance for false positives and negatives. For navigation tasks, errors can dramatically compromise system reliability (triggering unnecessary replanning or stops) or safety (overlooking obstacles).</p>
</div>
<div class="ltx_para" id="S4.SS1.SSSx3.p3">
<p class="ltx_p" id="S4.SS1.SSSx3.p3.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.T4" title="In Dataset Evaluation ‣ 4.1 Panoptic Segmentation ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> displays how Mask2Former’s performance scales with training dataset size on our validation set. The model’s PQ and RQ show near-linear improvement with increasing training images without showing signs of plateauing even at approximately 400 images, indicating potential benefits from an even larger dataset. On the other hand, the insensitivity of SQ to dataset size in this context can be attributed to the pre-trained model’s already developed competence in capturing the geometric properties of objects, which generalizes well across different object categories.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of the Mask2Former Swin-Tiny model on panoptic segmentation for different ”construction site” dataset fractions.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Fraction</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">20%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">40%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.4.1">60%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.5.1">80%</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.6.1">100%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.1.1.1">PQ</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.5">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.1.6.1">0.68</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.2.1.1">RQ</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3">0.67</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.5">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.2.6.1">0.81</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T4.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.3.1.1">SQ</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.4.3.2">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.4.3.3">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.4.3.4">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.3.5.1">0.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.4.3.6">0.78</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Motion Planning with Semantic Understanding</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We conducted autonomous navigation trials in a controlled testing field and at the Rescue Troop Training Center in Avully, Switzerland. In the testing field, the system’s performance was evaluated in narrow, obstacle-dense environments containing challenging objects like fences, poles, and buckets. These obstacles, difficult to distinguish geometrically or using <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>, provided a rigorous test of the system’s perception capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">At Avully, we assessed navigation in open spaces, focusing on road-following and dynamic obstacle handling, such as emergency stops or rerouting around people. The supplementary video provides a demonstration of the field deployment.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">During all tests, Mask2Former and the <abbr class="ltx_glossaryref" title="Light Detection and Ranging"><span class="ltx_text ltx_glossary_short">LiDAR</span></abbr>-based obstacle detection system performed without critical misclassifications. The online planner consistently found reliable solutions. Although occasional attempts at pathfinding would fail due to dynamic changes, the planner’s continuous re-evaluation ensured robust navigation suitable for long-duration operations, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.04250v1#S4.F7" title="In 4.2 Motion Planning with Semantic Understanding ‣ 4 Experimental Results ‣ ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows snapshots from an ”adversarial test” during our field deployment in Avully, where the navigation system recalculated a new plan when dynamic obstacles (people) obstructed the initial path.</p>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="5362" id="S4.F7.g1" src="x5.jpg" width="383"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="234" id="S4.F7.g2" src="x6.jpg" width="383"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_missing ltx_missing_image" id="S4.F7.g3" src=""/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="5362" id="S4.F7.g4" src="x8.jpg" width="383"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="234" id="S4.F7.g5" src="x9.jpg" width="383"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_missing ltx_missing_image" id="S4.F7.g6" src=""/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Two consecutive snapshots from an ”adversarial test” of the navigation system conducted in Avully, where humans attempt to obstruct the robot by crossing its planned paths. The first column shows the system’s semantic mapping and object tracking, with different colors for the semantic classes (untraversable areas elevated). The planned path is illustrated in black. The third column offers a segmented, first-person view. The online RRT* planner dynamically generates alternative routes.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions &amp; Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This work introduced a comprehensive panoptic scene understanding system tailored for construction environments. We presented a fine-tuned Mask2Former-based segmentation model that integrates visual and geometric data to generate dynamic and static semantic maps. The system tracks objects over time, maintaining their semantic identities outside the camera’s field of view and effectively segmenting the environment into static and dynamic layers.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We demonstrated the practical use of this system by applying it to a navigation task. The generated panoptic maps were utilized in an online RRT* planner for robust, real-time path planning in dynamic environments. While the navigation task served as a case study, the panoptic segmentation system is generalizable and can be applied to other robotics and perception tasks.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Future improvements include addressing ambiguities in classifying similar terrain by incorporating open-vocabulary systems and enhancing point cloud detection with <abbr class="ltx_glossaryref" title="Neural Network"><span class="ltx_text ltx_glossary_short">NN</span></abbr>-based methods. Extending the 2D semantic map to a full 3D representation would enable navigation under complex structures. The code and dataset are publicly available to facilitate further research and application in this domain.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caesar <span class="ltx_text ltx_font_italic" id="bib.bib1.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">nuscenes: A multimodal dataset for autonomous driving.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 11621–11631, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion <span class="ltx_text ltx_font_italic" id="bib.bib2.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.3.1">European conference on computer vision</span>, pages 213–229. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span class="ltx_text ltx_font_italic" id="bib.bib3.2.2.1">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Rethinking atrous convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1">CoRR</span>, abs/1706.05587, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span class="ltx_text ltx_font_italic" id="bib.bib4.2.2.1">et al.</span> [2021a]</span>
<span class="ltx_bibblock">
Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar.

</span>
<span class="ltx_bibblock">Masked-attention mask transformer for universal image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1">arXiv</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2.1">et al.</span> [2021b]</span>
<span class="ltx_bibblock">
Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov.

</span>
<span class="ltx_bibblock">Per-pixel classification is not all you need for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.3.1">NeurIPS</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng <span class="ltx_text ltx_font_italic" id="bib.bib6.2.2.1">et al.</span> [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.3.1">2009 IEEE conference on computer vision and pattern recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ester <span class="ltx_text ltx_font_italic" id="bib.bib7.2.2.1">et al.</span> [1996]</span>
<span class="ltx_bibblock">
Martin Ester, Hans-Peter Kriegel, Jiirg Sander, and Xiaowei Xu.

</span>
<span class="ltx_bibblock">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.3.1">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</span>, pages 226–231. AAAI Press, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fankhauser and Hutter [2016]</span>
<span class="ltx_bibblock">
Péter Fankhauser and Marco Hutter.

</span>
<span class="ltx_bibblock">A Universal Grid Map Library: Implementation and Use Case for Rough Terrain Navigation.

</span>
<span class="ltx_bibblock">In Anis Koubaa, editor, <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Robot Operating System (ROS) – The Complete Reference (Volume 1)</span>, chapter 5. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frey <span class="ltx_text ltx_font_italic" id="bib.bib9.2.2.1">et al.</span> [2024]</span>
<span class="ltx_bibblock">
Jonas Frey, Shehryar Khattak, Manthan Patel, Deegan Atha, Julian Nubert, Curtis Padgett, Marco Hutter, and Patrick Spieler.

</span>
<span class="ltx_bibblock">Roadrunner–learning traversability estimation for autonomous off-road driving.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:2402.19341</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan <span class="ltx_text ltx_font_italic" id="bib.bib10.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Tianrui Guan, Zhenpeng He, Ruitao Song, Dinesh Manocha, and Liangjun Zhang.

</span>
<span class="ltx_bibblock">TNS: Terrain Traversability Mapping and Navigation System for Autonomous Excavators.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.3.1">Proceedings of Robotics: Science and Systems</span>, New York City, NY, USA, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He <span class="ltx_text ltx_font_italic" id="bib.bib11.2.2.1">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.3.1">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</span>, Oct 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu <span class="ltx_text ltx_font_italic" id="bib.bib12.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Haotian Hu, Fanyi Wang, Jingwen Su, Yaonong Wang, Laifeng Hu, Weiye Fang, Jingwei Xu, and Zhiwang Zhang.

</span>
<span class="ltx_bibblock">Ea-lss: Edge-aware lift-splat-shot framework for 3d bev object detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2303.17895</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jelavic <span class="ltx_text ltx_font_italic" id="bib.bib13.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Edo Jelavic, Julian Nubert, and Marco Hutter.

</span>
<span class="ltx_bibblock">Open3d slam: Point cloud based mapping and localization for education.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.3.1">Robotic Perception and Mapping: Emerging Techniques, ICRA 2022 Workshop</span>, page 24. ETH Zurich, Robotic Systems Lab, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jud <span class="ltx_text ltx_font_italic" id="bib.bib14.2.2.1">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Dominic Jud, Simon Kerscher, Martin Wermelinger, Edo Jelavic, Pascal Egli, Philipp Leemann, Gabriel Hottiger, and Marco Hutter.

</span>
<span class="ltx_bibblock">Heap-the autonomous walking excavator.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1">Automation in Construction</span>, 129:103783, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov <span class="ltx_text ltx_font_italic" id="bib.bib15.2.2.1">et al.</span> [2019a]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar.

</span>
<span class="ltx_bibblock">Panoptic feature pyramid networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov <span class="ltx_text ltx_font_italic" id="bib.bib16.2.2.1">et al.</span> [2019b]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollar.

</span>
<span class="ltx_bibblock">Panoptic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span class="ltx_text ltx_font_italic" id="bib.bib17.2.2.1">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Tong Lu, and Ping Luo.

</span>
<span class="ltx_bibblock">Panoptic segformer: Delving deeper into panoptic segmentation with transformers, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span class="ltx_text ltx_font_italic" id="bib.bib18.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Jinyu Li, Chenxu Luo, and Xiaodong Yang.

</span>
<span class="ltx_bibblock">PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span class="ltx_text ltx_font_italic" id="bib.bib19.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song Han.

</span>
<span class="ltx_bibblock">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long <span class="ltx_text ltx_font_italic" id="bib.bib20.2.2.1">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.3.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>, June 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maturana <span class="ltx_text ltx_font_italic" id="bib.bib21.2.2.1">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Daniel Maturana, Po-Wei Chou, Masashi Uenoyama, and Sebastian Scherer.

</span>
<span class="ltx_bibblock">Real-time semantic mapping for autonomous off-road navigation.

</span>
<span class="ltx_bibblock">In Marco Hutter and Roland Siegwart, editors, <span class="ltx_text ltx_font_italic" id="bib.bib21.3.1">Field and Service Robotics</span>, pages 335–350, Cham, 2018. Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei <span class="ltx_text ltx_font_italic" id="bib.bib22.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Jieru Mei, Alex Zihao Zhu, Xinchen Yan, Hang Yan, Siyuan Qiao, Liang-Chieh Chen, and Henrik Kretzschmar.

</span>
<span class="ltx_bibblock">Waymo open dataset: Panoramic video panoptic segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.3.1">European Conference on Computer Vision</span>, pages 53–72. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohan and Valada [2021]</span>
<span class="ltx_bibblock">
Rohit Mohan and Abhinav Valada.

</span>
<span class="ltx_bibblock">Efficientps: Efficient panoptic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">International Journal of Computer Vision (IJCV)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nubert <span class="ltx_text ltx_font_italic" id="bib.bib24.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Julian Nubert, Shehryar Khattak, and Marco Hutter.

</span>
<span class="ltx_bibblock">Graph-based multi-sensor fusion for consistent localization of autonomous construction robots.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.3.1">IEEE International Conference on Robotics and Automation (ICRA)</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paz <span class="ltx_text ltx_font_italic" id="bib.bib25.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
David Paz, Hengyuan Zhang, Qinru Li, Hao Xiang, and Henrik I. Christensen.

</span>
<span class="ltx_bibblock">Probabilistic semantic mapping for urban autonomous driving applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.3.1">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>, pages 2059–2064, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philion and Fidler [2020]</span>
<span class="ltx_bibblock">
Jonah Philion and Sanja Fidler.

</span>
<span class="ltx_bibblock">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the European Conference on Computer Vision</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roddick and Cipolla [2020]</span>
<span class="ltx_bibblock">
Thomas Roddick and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 11135–11144, Seattle, WA, USA, June 2020. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth <span class="ltx_text ltx_font_italic" id="bib.bib28.2.2.1">et al.</span> [2024]</span>
<span class="ltx_bibblock">
Pascal Roth, Julian Nubert, Fan Yang, Mayank Mittal, and Marco Hutter.

</span>
<span class="ltx_bibblock">Viplanner: Visual semantic imperative learning for local navigation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.3.1">International Conference on Robotics and Automation (ICRA)</span>. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaban <span class="ltx_text ltx_font_italic" id="bib.bib29.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Amirreza Shaban, Xiangyun Meng, JoonHo Lee, Byron Boots, and Dieter Fox.

</span>
<span class="ltx_bibblock">Semantic terrain classification for off-road autonomous driving.

</span>
<span class="ltx_bibblock">In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, <span class="ltx_text ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 5th Conference on Robot Learning</span>, volume 164 of <span class="ltx_text ltx_font_italic" id="bib.bib29.4.2">Proceedings of Machine Learning Research</span>, pages 619–629. PMLR, 08–11 Nov 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Terenzi and Hutter [2023]</span>
<span class="ltx_bibblock">
Lorenzo Terenzi and Marco Hutter.

</span>
<span class="ltx_bibblock">Towards autonomous excavation planning, August 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani <span class="ltx_text ltx_font_italic" id="bib.bib31.2.2.1">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span class="ltx_text ltx_font_italic" id="bib.bib32.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Haiyang Wang, Chen Shi, Shaoshuai Shi, Meng Lei, Sen Wang, Di He, Bernt Schiele, and Liwei Wang.

</span>
<span class="ltx_bibblock">DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets, March 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu <span class="ltx_text ltx_font_italic" id="bib.bib33.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">Point Transformer V3: Simpler, Faster, Stronger, December 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Oct  5 18:09:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
