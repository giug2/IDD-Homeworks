<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.17767] Federated Learning under Attack: Improving Gradient Inversion for Batch of Images</title><meta property="og:description" content="Federated Learning (FL) has emerged as a machine learning approach able to preserve the privacy of user’s data. Applying FL, clients train machine learning models on a local dataset and a central server aggregates the …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning under Attack: Improving Gradient Inversion for Batch of Images">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning under Attack: Improving Gradient Inversion for Batch of Images">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.17767">

<!--Generated on Sat Oct  5 23:00:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning; gradient inversion attack; security.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning under Attack: Improving Gradient Inversion for Batch of Images
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luiz Leite1, Yuri Santo1, Bruno L. Dalmazo2, André Riker1


<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1Federal University of Pará, Brazil
</span>
<span class="ltx_contact ltx_role_affiliation">2 Federal University of Rio Grande, Brazil
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) has emerged as a machine learning approach able to preserve the privacy of user’s data. Applying FL, clients train machine learning models on a local dataset and a central server aggregates the learned parameters coming from the clients, training a global machine learning model without sharing user’s data. However, the state-of-the-art shows several approaches to promote attacks on FL systems. For instance, inverting or leaking gradient attacks can find, with high precision, the local dataset used during the training phase of the FL. This paper presents an approach, called Deep Leakage from Gradients with Feedback Blending (DLG-FB), which is able to improve the inverting gradient attack, considering the spatial correlation that typically exists in batches of images. The performed evaluation shows an improvement of 19.18% and 48,82% in terms of attack success rate and the number of iterations per attacked image, respectively.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning; gradient inversion attack; security.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Privacy-preserving solutions are a key requirement for almost all computer applications, whether for legislation compliance or due to the mistrust of how sensitive data can be used. In an era marked by escalating concerns over data breaches and privacy violations, ensuring the confidentiality and integrity of personal information has become paramount for businesses and individuals alike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Moreover, as technology continues to advance, the need for robust privacy-preserving techniques becomes even more pressing.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) emerges as a solution to provide data-privacy for smart systems because it enables distributed Machine Learning (ML) training, without sending user’s data to a central point, providing an extra level of user data-privacy protection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In FL, multiple ML models run on local privacy-sensitive datasets, simultaneously, and a global ML model, running on a server is built without sharing the local datasets with the server.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Some FL methods rely on gradient or weight sharing between clients and servers to train the global ML model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Historically, there was a prevalent belief that sharing gradients was inherently secure, implying that the exchange does not compromise the confidentiality of the training data. However, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed a method called Deep Leakage from Gradients (DLG), showing how to invert the gradient to reconstruct the input data used in the training, becoming one of the most famous methods to attack FL models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper aims to enhance the performance of DLG-based methods, proposing a new attack algorithm called Deep Leakage from Gradients with Feedback Blending (DLG-FB). The proposed approach takes advantage of the spatial redundancies in a batch of images. Assuming the attacker aims to access the whole batch of images in a local dataset, DLG-FB does not initialize the input image-matrix with dummy data, i.e. pure random values. Instead, DLG-FB computes a blend of images that have already been accessed by the attacker after the sequence of attack is greater than two reconstructed images. The performance evaluation shows that DLG-FB improves the capacity to attack images and reduces the number of iterations to reach a successful attack.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper is organized as follows: Section <a href="#S2" title="II Related Work ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides a review of the main related works. Section <a href="#S3" title="III Threat Model ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the threat model considered in this work.
Section <a href="#S4" title="IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> describes the proposed attack. Section <a href="#S5" title="V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the conducted evaluation and the obtained results. Section <a href="#S6" title="VI Conclusion and Future Works ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> introduces the conclusions and outlines potential directions for future research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Gradient exchange is one of the prevalent techniques in contemporary multi-node machine learning setups, such as distributed training and collaborative learning, as Federated Learning. This section aims to describe the most relevant attacks able to exploit gradient in FL systems.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Deep Leakage from Gradients (DLG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Improved Deep Leakage from Gradients (iDLG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> show how it is possible to leak private training data if an attacker has access to the shared gradients.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Deep Leakage from Gradients (DLG)</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The work proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> presents an iterative method, called Deep Leakage from Gradients (DLG), based on an optimization algorithm that can obtain both the training inputs and the labels, considering an attacker accessing the gradient coming from a FL client, defined as <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">W</span>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.5" class="ltx_p">The first step of DLG, after accessing the client gradient, is to randomly initialize a dummy input and label input. With the “dummy data” it is possible to compute “dummy gradients”, defined as <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p2.5.1" class="ltx_text ltx_font_italic">W’</span>. As depicted in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A Deep Leakage from Gradients (DLG) ‣ II Related Work ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this attack seeks to approximate <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mo id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p2.5.2" class="ltx_text ltx_font_italic">W’</span> to <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mo id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p2.5.3" class="ltx_text ltx_font_italic">W</span>, changing iteratively the input and the label data. When the <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mo id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p2.5.4" class="ltx_text ltx_font_italic">W’</span> is close to <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="\nabla" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mo id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">∇</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">∇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">\nabla</annotation></semantics></math><span id="S2.SS1.p2.5.5" class="ltx_text ltx_font_italic">W</span>, it is possible to extract the data used by the client to train the machine learning model.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.17767/assets/figs/DLG.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>DLG Algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">As can be noticed, this is an optimization problem, where the distance <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="||\nabla W^{\prime}-\nabla W||^{2}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><msup id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mrow id="S2.SS1.p3.1.m1.1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.1.m1.1.1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.SS1.p3.1.m1.1.1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.1.1.cmml"><mrow id="S2.SS1.p3.1.m1.1.1.1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S2.SS1.p3.1.m1.1.1.1.1.1.2.1" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.1.cmml">∇</mo><msup id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.cmml"><mi id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.2" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.2.cmml">W</mi><mo id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.3" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.3.cmml">′</mo></msup></mrow><mo id="S2.SS1.p3.1.m1.1.1.1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.SS1.p3.1.m1.1.1.1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3.cmml"><mo rspace="0.167em" id="S2.SS1.p3.1.m1.1.1.1.1.1.3.1" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3.1.cmml">∇</mo><mi id="S2.SS1.p3.1.m1.1.1.1.1.1.3.2" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml">W</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.1.m1.1.1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1">superscript</csymbol><apply id="S2.SS1.p3.1.m1.1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p3.1.m1.1.1.1.2.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.2">norm</csymbol><apply id="S2.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1"><minus id="S2.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.1"></minus><apply id="S2.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2"><ci id="S2.SS1.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.1">∇</ci><apply id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.2.2.3">′</ci></apply></apply><apply id="S2.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3"><ci id="S2.SS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3.1">∇</ci><ci id="S2.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p3.1.m1.1.1.1.1.1.3.2">𝑊</ci></apply></apply></apply><cn type="integer" id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">||\nabla W^{\prime}-\nabla W||^{2}</annotation></semantics></math> is differentiable concerning inputs and labels. This optimization is computed by the solver called Limited-memory Broyden, Fletcher, Goldfarb, and Shanno (L-BFGS).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Improved Deep Leakage from Gradients (iDLG)</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The work proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> aims to improve the DLG method. In DLG, the attacker generates dummy data and corresponding labels under the guidance of shared gradients. Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed Improved Deep Leakage from Gradients (iDLG), which exploits the relationship between the ground-truth labels and the signs of the gradients. This work shows that label information can be computed analytically from the gradients, and this information can be used to obtain train data more close to the original. As another advantage, this method is suitable for any differentiable model trained with cross-entropy loss on one-hot labels.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Other Relevant Attacks</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Recovery of image data from gradient information was first discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for neural networks. In this work, authors have proven that recovery is possible for a single neuron or linear layer.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, the authors also address the leakage problem in deep learning. This work proposes a method to obtain the sample trained data for deep-learning models based on the ReLu function. However, for this method, it is necessary to access the entire learning process.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In another effort, the authors discuss the recovery of training data from shared gradients in distributed machine learning systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The original Deep Leakage from Gradients (DLG) method faces issues with accuracy and stability due to exploding gradients and high learning rates. To address these issues, this study proposes the WDLG method, which uses the Wasserstein distance to calculate loss, enabling more faithful and efficient recovery of training data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Threat Model</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">For the proposed approach in this work, we consider the scenario of an honest but curious federated learning server seeking to access user data, i.e. a batch of images. The batch of image, which is the target of the attacker, is defined as <math id="S3.p1.1.m1.4" class="ltx_Math" alttext="S=\{s_{1},s_{2},...,s_{k}\}" display="inline"><semantics id="S3.p1.1.m1.4a"><mrow id="S3.p1.1.m1.4.4" xref="S3.p1.1.m1.4.4.cmml"><mi id="S3.p1.1.m1.4.4.5" xref="S3.p1.1.m1.4.4.5.cmml">S</mi><mo id="S3.p1.1.m1.4.4.4" xref="S3.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.p1.1.m1.4.4.3.3" xref="S3.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.p1.1.m1.4.4.3.3.4" xref="S3.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.p1.1.m1.2.2.1.1.1" xref="S3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.p1.1.m1.2.2.1.1.1.2" xref="S3.p1.1.m1.2.2.1.1.1.2.cmml">s</mi><mn id="S3.p1.1.m1.2.2.1.1.1.3" xref="S3.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.1.m1.4.4.3.3.5" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p1.1.m1.3.3.2.2.2" xref="S3.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.p1.1.m1.3.3.2.2.2.2" xref="S3.p1.1.m1.3.3.2.2.2.2.cmml">s</mi><mn id="S3.p1.1.m1.3.3.2.2.2.3" xref="S3.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p1.1.m1.4.4.3.3.6" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">…</mi><mo id="S3.p1.1.m1.4.4.3.3.7" xref="S3.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.p1.1.m1.4.4.3.3.3" xref="S3.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.p1.1.m1.4.4.3.3.3.2" xref="S3.p1.1.m1.4.4.3.3.3.2.cmml">s</mi><mi id="S3.p1.1.m1.4.4.3.3.3.3" xref="S3.p1.1.m1.4.4.3.3.3.3.cmml">k</mi></msub><mo stretchy="false" id="S3.p1.1.m1.4.4.3.3.8" xref="S3.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.4b"><apply id="S3.p1.1.m1.4.4.cmml" xref="S3.p1.1.m1.4.4"><eq id="S3.p1.1.m1.4.4.4.cmml" xref="S3.p1.1.m1.4.4.4"></eq><ci id="S3.p1.1.m1.4.4.5.cmml" xref="S3.p1.1.m1.4.4.5">𝑆</ci><set id="S3.p1.1.m1.4.4.3.4.cmml" xref="S3.p1.1.m1.4.4.3.3"><apply id="S3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.p1.1.m1.2.2.1.1.1.2">𝑠</ci><cn type="integer" id="S3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.p1.1.m1.3.3.2.2.2.cmml" xref="S3.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.p1.1.m1.3.3.2.2.2.2">𝑠</ci><cn type="integer" id="S3.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">…</ci><apply id="S3.p1.1.m1.4.4.3.3.3.cmml" xref="S3.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.p1.1.m1.4.4.3.3.3.2">𝑠</ci><ci id="S3.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.p1.1.m1.4.4.3.3.3.3">𝑘</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.4c">S=\{s_{1},s_{2},...,s_{k}\}</annotation></semantics></math>, where <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="s_{k}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">s</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝑠</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">s_{k}</annotation></semantics></math> is the last image to be reconstructed. The attacker has access to the same machine learning model architecture, as it is expected for a federated learning server. Besides, the attacker can store and process updates sent by individual users independently but lacks the ability to interfere with the collaborative learning algorithm. Moreover, the attacker is unable to alter the model architecture to facilitate their attack or send deceptive global parameters that do not accurately represent the learned global model.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Deep Leakage from Gradients with Feedback Blending (DLG-FB)</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">In prior gradient-based attack methodologies, random noise, also known as dummy data, has traditionally been used to initialize the data input fed into the first iteration of the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) solver. However, with random data initialization, when attempting to reconstruct two consecutive attacked images, i.e., <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="s_{n}" display="inline"><semantics id="S4.p1.1.m1.1a"><msub id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">s</mi><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑠</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">s_{n}</annotation></semantics></math> and <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="s_{n+1}" display="inline"><semantics id="S4.p1.2.m2.1a"><msub id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">s</mi><mrow id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mi id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml">n</mi><mo id="S4.p1.2.m2.1.1.3.1" xref="S4.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S4.p1.2.m2.1.1.3.3" xref="S4.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">𝑠</ci><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><plus id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3.1"></plus><ci id="S4.p1.2.m2.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.3.2">𝑛</ci><cn type="integer" id="S4.p1.2.m2.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">s_{n+1}</annotation></semantics></math>, traditional approaches start with a new random guess for each iteration, meaning that each image reconstruction is entirely independent of the others.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Consequently, although it is possible to gather information from the successful reconstructed images during the attacked sequence, this knowledge is not effectively used by the attacker.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In response to this scenario, we introduce a novel DLG-based attack strategy defined as Deep Leakage from Gradients with Feedback Blending (DLG-FB), as illustrated in Figure <a href="#S4.F2" title="Figure 2 ‣ IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This approach is designed for attacks that target a batch of images, which means the attacker aims to reconstruct a set of images from the same federated learning client. We assume these images have some level of spatial redundancy, keeping a certain degree of similarity. For instance, it is expected the attacked client has a batch of images with the same set of people or place. During the sequence to attack a batch of images, the main idea of DLG-FB is to blend successful reconstructed images and feed the solver with the blended image, making the blended image a better starting point for the attack than pure random data.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.17767/assets/figs/FB_ilustration_drawing.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>DLG-FB algorithm illustration.</figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">DLG-FB algorithm requires two successfully obtained images, as can be observed in labels 1 and 2 in Figure <a href="#S4.F2" title="Figure 2 ‣ IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. After two reconstructed images, DLG-FB performs the images blending (see label 3) from attack 1 and 2 and uses it as initial guess for the attack 3 (label 4). This process keeps repeating for the next elements of the attacked batch. However, it is important to notice that the reconstruction can fail, due to a vast number of factors.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">If an image is successfully reconstructed, it blends the new image with the previous blend to create a new composite, as can be seen in labels 5, 6 and 7 of Figure <a href="#S4.F2" title="Figure 2 ‣ IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In case of failure, it proceeds to the next image without incorporating the failed reconstructed image, which would introduce excessive noise.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.4" class="ltx_p">Regarding the image blending performed by DLG-FB, <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p6.1.m1.1a"><mi id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><ci id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">C</annotation></semantics></math> represents the pixel matrix of an image, where <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="C_{o}" display="inline"><semantics id="S4.p6.2.m2.1a"><msub id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><mi id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml">C</mi><mi id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1">subscript</csymbol><ci id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2">𝐶</ci><ci id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">C_{o}</annotation></semantics></math> is the resulting composite image, <math id="S4.p6.3.m3.1" class="ltx_Math" alttext="C_{a}" display="inline"><semantics id="S4.p6.3.m3.1a"><msub id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml"><mi id="S4.p6.3.m3.1.1.2" xref="S4.p6.3.m3.1.1.2.cmml">C</mi><mi id="S4.p6.3.m3.1.1.3" xref="S4.p6.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><apply id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p6.3.m3.1.1.1.cmml" xref="S4.p6.3.m3.1.1">subscript</csymbol><ci id="S4.p6.3.m3.1.1.2.cmml" xref="S4.p6.3.m3.1.1.2">𝐶</ci><ci id="S4.p6.3.m3.1.1.3.cmml" xref="S4.p6.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">C_{a}</annotation></semantics></math> and <math id="S4.p6.4.m4.1" class="ltx_Math" alttext="C_{b}" display="inline"><semantics id="S4.p6.4.m4.1a"><msub id="S4.p6.4.m4.1.1" xref="S4.p6.4.m4.1.1.cmml"><mi id="S4.p6.4.m4.1.1.2" xref="S4.p6.4.m4.1.1.2.cmml">C</mi><mi id="S4.p6.4.m4.1.1.3" xref="S4.p6.4.m4.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p6.4.m4.1b"><apply id="S4.p6.4.m4.1.1.cmml" xref="S4.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p6.4.m4.1.1.1.cmml" xref="S4.p6.4.m4.1.1">subscript</csymbol><ci id="S4.p6.4.m4.1.1.2.cmml" xref="S4.p6.4.m4.1.1.2">𝐶</ci><ci id="S4.p6.4.m4.1.1.3.cmml" xref="S4.p6.4.m4.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.4.m4.1c">C_{b}</annotation></semantics></math> are the two reconstructed images. In this work, the blending is determined by the following equation:</p>
</div>
<div id="S4.p7" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="C_{o}=\alpha C_{a}+(1-\alpha)C_{b}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">o</mi></msub><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.3.1.cmml">​</mo><msub id="S4.E1.m1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.3.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.3.3.3.cmml">a</mi></msub></mrow><mo id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">​</mo><msub id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml">b</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">𝑜</ci></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><plus id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></plus><apply id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3"><times id="S4.E1.m1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2">𝛼</ci><apply id="S4.E1.m1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.3.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3.3">𝑎</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">C_{o}=\alpha C_{a}+(1-\alpha)C_{b}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">In equation <a href="#S4.E1" title="In IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p8.1.m1.1a"><mi id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><ci id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">\alpha</annotation></semantics></math> is an arbitrarily chosen value that determines which image will be more dominant in the composition.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation and Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section presents the evaluation of the proposed attack algorithm. First, the test environment and the dataset used are introduced. Then, the obtained results are discussed.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Test Environment and Dataset</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In our testing environment, we employed the PyTorch API within a Python virtual environment running on Manjaro Linux x86 64 OS, powered by an AMD Ryzen 5 5600G CPU, 4.464GHz. Besides, we utilized PyTorch’s LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) optimizer with a learning rate set to 1.
</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">For the performance evaluation of the approaches, we have used two well-known datasets: CIFAR100 and MNIST.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">CIFAR-100 is a widely used computer vision dataset comprising 60,000 color images grouped into 100 classes, each with 600 images. These 32x32 pixel images span diverse objects, animals, and scenes, organized into 20 superclasses.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">MNIST is a well-known dataset utilized in computer vision which comprises 70,000 grayscale images of handwritten digits, categorized into 10 classes. MNIST serves as a foundational benchmark for machine learning models.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">This optimization algorithm is well-suited for large-scale problems, particularly in machine learning. Unlike the full BFGS algorithm, LBFGS conserves memory by storing only a few vectors to approximate the Hessian matrix, exploiting curvature information for faster convergence. It belongs to the family of quasi-Newton methods, efficiently approximating second-order derivatives using first-order information.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Obtained Results</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We have compared the proposed DLG-FB with the following approaches: (i) Original DLG, (ii) original iDLG, (iii) iDLG-FB (iv) DLG-FB-Noise Factor (iDLG-FB-NF), and (v) DLG-FB-Noise Factor (DLG-FB-NF).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">It is important to note that iDLG-FB represents the implementation of the proposed Feedback Blending strategy in iDLG. Additionally, for comparison purposes, DLG-FB-NF and iDLG-FB-NF are versions of the proposed Feedback Blending strategy that do not discard the dummy data from unsuccessful attack attempts. Instead, they blend it in, aiming to reduce overfitting. These versions always blend the output image, even if the attack sequence fails. Moreover, for the obtained results shown in this section, <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">α</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><eq id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></eq><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝛼</ci><cn type="float" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\alpha=0.5</annotation></semantics></math> (see eq. <a href="#S4.E1" title="In IV Deep Leakage from Gradients with Feedback Blending (DLG-FB) ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.F4" title="Figure 4 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> present the obtained results in terms of the number of successfully attacked images for the CIFAR-100 and MNIST datasets, respectively. For CIFAR-100, the original iDLG recovered 996 images, while iDLG-FB recovered 1187 images, a +19.18% improvement. For DLG-based approaches, original DLG was able to recover 769 images compared to 908 images by the DLG-FB, a +14.07% increase. Regarding MNIST, which consists of simpler grayscale images with no color channels, original iDLG recovered 1088 images, while iDLG-FB recovered 1186 images, a +9.01% improvement. For MNIST, original DLG recovered 929 images compared to 934 by the DLG-FB version. The FB versions assist the solver by providing better initial guesses for the pixels. However, since grayscale images have significantly fewer pixels, the new strategy has a reduced impact.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2409.17767/assets/x1.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Cumulative Number of Successful Reconstructed Image (CIFAR-100).</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2409.17767/assets/x2.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Cumulative Number of Successful Reconstructed Image (MNIST).</figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">As depicted in Figure<a href="#S5.F3" title="Figure 3 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.F4" title="Figure 4 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the two approaches using the Feedback Blending Noise Factor (FB-NF) demonstrate a very peculiar behavior: at a certain point when noise becomes too prevalent, it fails to reconstruct the targeted image, since it is designed to not take into consideration if the output image has converged to the attacked image.
Figure <a href="#S5.F5" title="Figure 5 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S5.F6" title="Figure 6 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> present the mean number of iterations to reach a successful image attack for CIFAR100 and MNIST datasets, respectively. In CIFAR100 dataset, DLG-FB and iDLG-FB had 48,82% and 44,26% less iterations than original DLG and iDLG, respectively. In MNIST, the proposed Feedback Blending strategy has shown 52.94% and 29,19% less iterations when applied to iDLG and DLG, respectively.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2409.17767/assets/x3.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Mean Number of Iterations to Successful Image Reconstruction.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2409.17767/assets/x4.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Mean Number of Iterations to Successful Image Reconstruction.</figcaption>
</figure>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">It is worth to mention that LBFGS faces greater challenges with CIFAR-100 than MNIST due to its more complexity images and higher pixel count from multiple color channels. MNIST is single grayscale channel and lower pixel count make it more susceptible to such attacks.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">Fig. <a href="#S5.F7" title="Figure 7 ‣ V-B Obtained Results ‣ V Evaluation and Results ‣ Federated Learning under Attack: Improving Gradient Inversion for Batch of Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a sequence of iterations and the feedback blending performed by DLG-FB for three sample images in CIFAR100 dataset.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2409.17767/assets/figs/FB_cifar100_drawing.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="349" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Samples of DLG-FB Reconstructing Images (CIFAR100).</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Works</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Privacy-preserving solutions have become a critical necessity in almost all computer applications. Among the possible solutions to this issue, Federated Learning (FL) has emerged as a promising approach for safeguarding data privacy in smart systems. In this context, this paper introduces a novel approach, named Deep Leakage from Gradients with Feedback Blending (DLG-FB), aimed at enhancing the effectiveness of DLG-based methods in federated learning systems. DLG-FB takes advantage on the spatial redundancies present in batches of images. Unlike conventional methods that initialize input image-matrices with random data to attack a single image, DLG-FB employs a strategy to attack a entire batch of image. After more than two successful image reconstructions, DLG-FB computes a blend of images and uses it as the initial data rather than utilizing pure random values. DLG-FB reveals gains in both the number of images successfully attacked and the iterations required to achieve a successful attack.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">As future work, the authors intend to add more mechanisms to the proposed DLG-FB, improving the criteria to perform the image blend. Also, a machine learning model can be used to make the approach more efficient. Moreover, the new version of the attack will be tested using other image datasets.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This study was financed in part by: (i) FAPERGS/CNPq (23/2551-0000773-8), (ii) UE iTec/FURG (iTec-80), (iii) PROAP/UFPA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and G. Srivastava, “A survey on security and privacy of federated learning,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Huang, D. Li, and Z. Yan, “Wireless federated learning with asynchronous and quantized updates,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Communications Letters</span>, vol. 27, no. 9, pp. 2393–2397, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Geiping, H. Bauermeister, H. Dröge, and M. Moeller, “Inverting gradients-how easy is it to break privacy in federated learning?,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 16937–16947, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Zhu, Z. Liu, and S. Han, “Deep leakage from gradients,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Zhao, K. R. Mopuri, and H. Bilen, “idlg: Improved deep leakage from gradients,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.02610</span>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “Privacy-preserving deep learning: Revisited and enhanced,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Applications and Techniques in Information Security: 8th International Conference, ATIS 2017, Auckland, New Zealand, July 6–7, 2017, Proceedings</span>, pp. 100–110, Springer, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Sannai, “Reconstruction of training samples from loss functions,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1805.07337</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X. He, C. Peng, W. Tan, and Y.-a. Tan, “Fast and accurate deep leakage from gradients based on wasserstein distance,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Int. J. Intell. Syst.</span>, vol. 2023, jan 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.17766" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.17767" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.17767">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.17767" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.17768" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 23:00:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
