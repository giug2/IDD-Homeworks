<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition</title>
<!--Generated on Fri Sep 20 11:14:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Synthetic Image Generation,  Diffusion Models,  Kidney Stone Classification,  Endoscopic Stone Recognition
" lang="en" name="keywords"/>
<base href="/html/2409.13409v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.SS1" title="In I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_italic">Medical Context</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.SS2" title="In I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-B</span> </span><span class="ltx_text ltx_font_italic">Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.SS3" title="In I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-C</span> </span><span class="ltx_text ltx_font_italic">Contributions of this paper</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S2" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S2.SS1" title="In II Related Work ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Automatic kidney stone recognition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S2.SS2" title="In II Related Work ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Generative models for data augmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed Approach for Image Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS1" title="In III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Homogenize image dimensions with padding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS2" title="In III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Synthetic Image Generation with SinDDM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS3" title="In III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Homogening Image Size via Super-Resolution</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS4" title="In III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Quantitative evaluation of generated images</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS5" title="In III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Used Datasets</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Evaluation protocol</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS1" title="In IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Patch selection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS2" title="In IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Baseline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS3" title="In IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Two-step Transfer Learning</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results and Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.SS1" title="In V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.SS2" title="In V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Classification</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S6" title="In Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusions and Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ruben Gonzalez-Perez<sup class="ltx_sup" id="id17.17.id1"><span class="ltx_text ltx_font_italic" id="id17.17.id1.1">†,1</span></sup>, Francisco Lopez-Tiro<sup class="ltx_sup" id="id18.18.id2"><span class="ltx_text ltx_font_italic" id="id18.18.id2.1">†,1,2</span></sup>,
Ivan Reyes-Amezcua<sup class="ltx_sup" id="id19.19.id3"><span class="ltx_text ltx_font_italic" id="id19.19.id3.1">3</span></sup>, Eduardo Falcon-Morales<sup class="ltx_sup" id="id20.20.id4"><span class="ltx_text ltx_font_italic" id="id20.20.id4.1">1</span></sup>
<br class="ltx_break"/>Rosa-Maria Rodríguez-Guéant<sup class="ltx_sup" id="id21.21.id5"><span class="ltx_text ltx_font_italic" id="id21.21.id5.1">5</span></sup>,
Jacques Hubert<sup class="ltx_sup" id="id22.22.id6"><span class="ltx_text ltx_font_italic" id="id22.22.id6.1">4</span></sup>, Michel Daudon<sup class="ltx_sup" id="id23.23.id7"><span class="ltx_text ltx_font_italic" id="id23.23.id7.1">6</span></sup>,
Gilberto Ochoa-Ruiz*<sup class="ltx_sup" id="id24.24.id8"><span class="ltx_text ltx_font_italic" id="id24.24.id8.1">,1</span></sup>, Christian Daul*<sup class="ltx_sup" id="id25.25.id9"><span class="ltx_text ltx_font_italic" id="id25.25.id9.1">,2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id26.26.id10"><span class="ltx_text ltx_font_italic" id="id26.26.id10.1">1</span></sup>Tecnologico de Monterrey, School of Sciences and Engineering, Mexico 
<br class="ltx_break"/><sup class="ltx_sup" id="id27.27.id11"><span class="ltx_text ltx_font_italic" id="id27.27.id11.1">2</span></sup>CRAN (UMR 7039, Université de Lorraine and CNRS), Nancy, France 
<br class="ltx_break"/><sup class="ltx_sup" id="id28.28.id12"><span class="ltx_text ltx_font_italic" id="id28.28.id12.1">3</span></sup>CINVESTAV, Computer Sciences Department, Guadalajara, Mexico 
<br class="ltx_break"/><sup class="ltx_sup" id="id29.29.id13"><span class="ltx_text ltx_font_italic" id="id29.29.id13.1">4</span></sup>CHRU Nancy, Service d’urologie de Brabois, Vandœuvre-lès-Nancy, France

<br class="ltx_break"/><sup class="ltx_sup" id="id30.30.id14"><span class="ltx_text ltx_font_italic" id="id30.30.id14.1">5</span></sup>NGERE - Nutrition-Génétique et Exposition aux Risques Environnementaux Nancy, France 
<br class="ltx_break"/><sup class="ltx_sup" id="id31.31.id15"><span class="ltx_text ltx_font_italic" id="id31.31.id15.1">6</span></sup>Hôpital Tenon, AP-HP, Paris, France
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id32.32.id1"><span class="ltx_text ltx_font_italic" id="id32.32.id1.1">†</span></sup>Equal contribution*Corresponding authors:gilberto.ochoa@tec.mx, christian.daul@univ-lorraine.fr</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id33.id1">Currently, the Morpho-Constitutional Analysis (MCA) is the de facto approach for the etiological diagnosis of kidney stone formation, and it is an important step for establishing personalized treatment to avoid relapses. More recently, research has focused on performing such tasks intra-operatively, an approach known as Endoscopic Stone Recognition (ESR). Both methods rely on features observed in the surface and the section of kidney stones to separate the analyzed samples into several sub-groups. However, given the high intra-observer variability and the complex operating conditions found in ESR, there is a lot of interest in using AI for computer-aided diagnosis. However, current AI models require large datasets to attain a good performance and for generalizing to unseen distributions. This is a major problem as large labeled datasets are very difficult to acquire, and some classes of kidney stones are very rare. Thus, in this paper, we present a method based on diffusion as a way of augmenting pre-existing ex-vivo kidney stone datasets. Our aim is to create plausible diverse kidney stone images that can be used for pre-training models using ex-vivo data. We show that by mixing natural and synthetic images of CCD images, it is possible to train models capable of performing very well on unseen intra-operative data. Our results show that is possible to attain an improvement of 10% in terms of accuracy compared to a baseline model pre-trained only on ImageNet. Moreover, our results show an improvement of 6% for surface images and 10% for section images compared to a model train on CCD images only, which demonstrates the effectiveness of using synthetic images.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Synthetic Image Generation, Diffusion Models, Kidney Stone Classification, Endoscopic Stone Recognition

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S1.F1.1.g1" src="extracted/5868160/images/synthetic.png" width="467"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of synthetic image generation for surface (external) and section (internal) views of kidney stones. The images were generated using the SinDDM model from a dataset of ex-vivo kidney stone images acquired with a charge-coupled device (CCD) camera. The synthetic images present characteristics highly similar in color and texture to the CCD-camera images.</figcaption>
</figure>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.5.1.1">I-A</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS1.6.2">Medical Context</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">The use of artificial intelligence (AI) is increasingly gaining importance in medical imaging applications at large. In particular deep learning (DL) methods have been investigated for implementing Computer Aided Diagnosis (CAD) tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib1" title="">1</a>]</cite>. For medical imaging applications in endoscopy and ureteroscopy, there is a great interest in developing methods for the classification, detection, and segmentation in diverse areas of interest, such as early cancer assessment or kidney stone identification, to name a few <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib2" title="">2</a>]</cite>. However, one of the main drawbacks of DL methods is their reliance on large amounts of data for training and testing high-performance models. This is particularly evident in the medical domain, where it is difficult to gather enough training images of certain diseases or medical conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. Another issue is related to domain shift: models trained with data from a given acquisition device will not generalize well when faced with data from another hospital center or equipment vendor.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">This problem is particularly evident in the context of endoscopic examinations for kidney stone identification: the prevalent diagnostic technique for such purpose, known as the Morpho-Constitutional Analysis (MCA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib4" title="">4</a>]</cite>, relies on the visual inspection of extracted kidney stones (surface and section views), in tandem with an FTIR spectral analysis for fine-grained biochemical information.
Given the high intra-observer variability of the process, as well as the long processing time in the hospital to obtain the outcome of this study, several researchers have investigated the use of computer vision and deep learning for automating the identification of the extracted samples, either using ex-vivo or in-vivo data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">However, most automated MCA systems have been used under strictly controlled acquisition conditions, which might limit their applicability in more realistic scenarios. In addition, the MCA analysis presents large class imbalances, due to the fact that certain types of kidney stones are extremely rare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib6" title="">6</a>]</cite>. Such issue makes it difficult to train and validate systems due to model bias, poor performance in the minority class, and inaccurate evaluation of model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib7" title="">7</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS2.5.1.1">I-B</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS2.6.2">Motivation</span>
</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Several recent works have proposed the use of Generative Adversarial Networks (GANs) or more recently, Diffusion Models to create new samples for copying with data imbalance and out-of-distribution issues in medical systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib8" title="">8</a>]</cite>. In this contribution, we present the results of a preliminary study for the assessment of synthetically generated images of kidney stones from a pre-existing ex-vivo dataset (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">This source dataset is comprised of high-resolution charge-coupled device (CCD) images with uniform background and controlled illumination conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib6" title="">6</a>]</cite>.
The objective of our work is to assess if the use of synthetically generated images can improve the performance of the model in an out-of-distribution setting: our aim is to pre-train deep learning models on ex-vivo data and test the models on in-vivo (i.e., acquired using and ureteroscope) images. The rationale for our proposal is that urologists are increasingly interested in using endoscopic images for performing the visual inspection component of the MCA analysis during the extraction surgery (Endoscopic Stone Recognition or ESR). Such automated procedures for ESR are nowadays vital as urologists are increasingly employing laser lithotripsy for converting the stone into dust, losing valuable visual information for diagnosis in the process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib9" title="">9</a>]</cite>. Therefore, our aim in this work is to assess whether the fusion of the source and synthetically generated images for surface and section images improves the overall performance of the model.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="150" id="S1.F2.sf1.g1" src="extracted/5868160/images/method_generation.png" width="449"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Generation of synthetic images</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="154" id="S1.F2.sf2.g1" src="extracted/5868160/images/method_classification.png" width="449"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Classification of kidney stones</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Evaluating the plausibility of synthetically generated images for automated endoscopic stone recognition pipeline. (a) Generation of synthetic images: The process begins with a set of standard CCD images that are adjusted to dimensions of 2848<math alttext="\times" class="ltx_Math" display="inline" id="S1.F2.3.m1.1"><semantics id="S1.F2.3.m1.1b"><mo id="S1.F2.3.m1.1.1" xref="S1.F2.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F2.3.m1.1c"><times id="S1.F2.3.m1.1.1.cmml" xref="S1.F2.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.3.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F2.3.m1.1e">×</annotation></semantics></math>4288 pixels through padding. These images are then used to generate low-resolution synthetic versions (264<math alttext="\times" class="ltx_Math" display="inline" id="S1.F2.4.m2.1"><semantics id="S1.F2.4.m2.1b"><mo id="S1.F2.4.m2.1.1" xref="S1.F2.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F2.4.m2.1c"><times id="S1.F2.4.m2.1.1.cmml" xref="S1.F2.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.4.m2.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F2.4.m2.1e">×</annotation></semantics></math>200 pixels) using SinDDM. Subsequently, these synthetic images undergo an x4 super-resolution process. Finally, the similarity between the input dataset (CCD camera images) and the output dataset (synthetic images) is evaluated using DeepChecks. To carry out the (b) automatic classification of kidney stones in endoscopic images for six different classes, training of models I and II is required. Model I is trained on the synthetic dataset and weights learned from ImageNet (1st TL step). Then, the endoscopic dataset is trained on Model II along with the weights learned in the synthetic distribution (2nd TL step).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS3.5.1.1">I-C</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS3.6.2">Contributions of this paper</span>
</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">Although several methods the been presented in the literature for automating both MCA and ESR, such models have been trained and tested on image patches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>. Despite the promising results yielded by these methods, they all present a common problem: insufficient and unbalanced images of samples per class, which makes it difficult to train and validate DL models. Although solutions such as patch-based data augmentation have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>, it is more clinically realistic to test such models on whole images, and not on portions of the same image, which does not match the procedure followed by experts to perform the visual inspection of MCA. In order to train and validate models on complete images, in this contribution, a preliminary approach for the generation of synthetic images of kidney stones in the style of images acquired with a standard CCD-camera is presented.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">To accomplish this, herein we propose an approach based on unconditional image generation models trained on ex-vivo kidney stones; for our experiments, we made use of a Single Image Denoising Diffusion Model (SinDDM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib10" title="">10</a>]</cite>, which is a hierarchical generative model that uses the multi-scale approach of SinGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib11" title="">11</a>]</cite> combined with the power of denoising diffusion models (DDMs) to generate synthetic images using only images from the training set.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.1">The results obtained from this experimental protocol are promising, as they allowed the generation of highly realistic images that were validated experimentally and by experts in the field of MCA which is the standard procedure to determine the kidney stone type.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Automatic kidney stone recognition</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Several deep learning (DL) methods have shown promising results in automated kidney stone classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>. However, the effectiveness of DL models is highly dependent on the availability of large datasets for training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. Acquiring such a large dataset poses a challenge in ureteroscopy due to practical limitations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib12" title="">12</a>]</cite>.
To address this challenge, techniques such as transfer learning (TL) and fine-tuning from pre-trained models such as ImageNet have been proposed as effective weight initialization strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. These methods offer the advantage of avoiding the need to train models from scratch. However, for automated ESR, these initialization techniques may not be applicable, given the substantial disparity between ImageNet distributions and endoscopic (ureteroscopic) images. Consequently, tailored TL methods that initialize weights closer to the target domain are imperative. To address this limitation, Two-Step Transfer Learning has been proposed for kidney stone identification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. Two-step TL allows learning a target distribution (e.g., endoscopic images) using an intermediate distribution (e.g., CCD-camera images). This technique has shown that using an intermediate domain (similar to the target distribution) substantially improves (up to 10%) the model over traditional training weights learned from ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. In addition, most existing models for automated ESR have been trained separately on surface or section images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib9" title="">9</a>]</cite>. However, visual assessment in MCA (by biologists) and ESR (by urologists) takes advantage of both surface and section fragments, exploiting information from both views<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Generative models for data augmentation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Traditional image data augmentation techniques based on basic image manipulations have been widely used and proven effective in many scenarios but they have limitations in capturing the full diversity and complexity of real-world data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib13" title="">13</a>]</cite>.
Advanced methods such as generative models can produce novel and diverse samples, effectively expanding the dataset’s diversity. Generative models can be trained on a dataset consisting of examples of the data to be generated and the model learns to capture the patterns and statistical properties present in the training data. This learned knowledge is then used to generate synthetic data with similar characteristics to the original dataset Generative models offer a promising solution to the challenge of scarce data by creating new data samples that closely resemble the original dataset. In scenarios where labeled data is limited or expensive to acquire, generative models can be used to augment the training set, thereby improving model performance and generalization. Different variants of Generative Adversarial Networks (GANs) have been widely used in the medical field for data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib14" title="">14</a>]</cite>, but often have problems with mode collapse and artifact generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib15" title="">15</a>]</cite>. The possibility of using Denoising Diffusion Models (DDMs) for medical data augmentation has also been explored, which are more stable models than GANs and have the ability to generate high-quality and diverse synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib15" title="">15</a>]</cite>. However, GANs and DDMs require large training datasets, making them inappropriate in cases where a limited training dataset is available. Several studies have proposed the use of different generative models such as SinGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib11" title="">11</a>]</cite>, MinimalGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib16" title="">16</a>]</cite>, among others, to explore their use in generating synthetic images from a small training dataset.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed Approach for Image Generation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In order to evaluate the plausibility of synthetically generated images for improving the automated ESR of kidney stones, we follow the pipeline shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2" title="Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2</span></a>. Our hypothesis is that by increasing and balancing the number of images in ex-vivo kidney stone datasets (CCD-camera images) and using synthetic images as intermediate distribution for two-step transfer learning, an improvement over a baseline trained solely on source images can be attained. In this section, the process of synthetic image generation based on CCD-camera images (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2.sf1" title="In Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2a</span></a>) is described in detail. In addition, To determine the performance of the generated images, the evaluation protocol of the synthetic images (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2.sf2" title="In Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2b</span></a>) is proposed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4" title="IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The generation of synthetic images (see, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2.sf1" title="In Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2a</span></a>) consists of four fundamental steps: <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS1" title="III-A Homogenize image dimensions with padding ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> homogenizing the dimensions of the input set through padding, <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS2" title="III-B Synthetic Image Generation with SinDDM ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> generating low-resolution synthetic images, <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS3" title="III-C Homogening Image Size via Super-Resolution ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> resolution enhancement for the synthetic dataset, and <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS4" title="III-D Quantitative evaluation of generated images ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a> similarity assessment of the distributions of the synthetic images generated with SinDDM and the input set images with DeepChecks.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Homogenize image dimensions with padding</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Often in DL models, images from a training set are required to have homogeneous dimensions. However, in kidney stone image acquisition it is difficult to establish a standard size, as it is highly dependent on acquisition devices in hospitals (such as digital cameras with different capabilities) to perform MCA. Assuming that visual inspection based on MCA is performed on images with multiple resolutions, high quality, controlled acquisition conditions, and a dark background to highlight the colors of kidney stones, padding is applied to all images in the input dataset (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2.sf1" title="In Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2a</span></a>) to homogenize their dimensions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Synthetic Image Generation with SinDDM</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1" title="I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">I</span></a>, due to the nature of kidney stones, there is a strong class imbalance in the image sets. Therefore, in order to balance and increase the number of samples per class (especially in minority classes), the Single Image Denoising Diffusion Model (SinDDM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib10" title="">10</a>]</cite> is implemented. SinDDM is a generative model that learns the internal statistics of the images on the training set to gradually turn the output image into White Gaussian noise (in a similar way to Denoising Diffusion Models (DDMs)) , but hierarchically combining blur and noise. In addition, SinDDM combines the multi-scale approach of SinGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib11" title="">11</a>]</cite> with the power of DDMs to generate high-quality and diverse synthetic images from a single training image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Homogening Image Size via Super-Resolution</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The images generated by the SinDDM model have a lower resolution than the original images. For this reason, we used the SwinV2 Transformer for Compressed Image Super-Resolution and Restoration (Swin2SR) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib17" title="">17</a>]</cite> to upscale our synthetic images by 4. This is a modified version of SwinIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib18" title="">18</a>]</cite>, which improves the Swin Transformer abilities in Super-Resolution tasks, particularly in handling Compressed Input SR scenarios. The Swin2SR model uses a traditional upscaling branch utilizing bicubic interpolation, capable of recovering basic structural information. Then, we combine the output of the model with the basic upscaled image to improve its quality.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="175" id="S3.F3.sf1.g1" src="x1.png" width="332"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Dataset A: CCD-camera images (ex-vivo) </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="175" id="S3.F3.sf2.g1" src="x2.png" width="332"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Dataset B: Endoscopic images (ex-vivo)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of ex-vivo kidney stone images acquired with (a) a CCD camera and (b) an endoscope. SEC and SUR stand for section and surface views. The first two rows show whole images and in the bottom rows, the sampled image patches</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Quantitative evaluation of generated images</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">Deepchecks.</span> This is a Python library used to test and validate Machine Learning Models and Data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib19" title="">19</a>]</cite> and it is composed by checks, conditions, and suites. A check is used to examine a particular property of the data or model. A condition is a function that can be incorporated into a check to assess whether the result meets a predefined standard or criteria. Finally, a suite refers to a structured compilation of checks that can have conditions. Upon execution, a suite generates a detailed report with high-level results and detailed findings.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">We used the Train-Test Validation Suite to compare the distributions across our training dataset and the generated images from the SinDDM Model. This Suite contains the following checks: Image Property Drift, Property Label Correlation Change, Label Drift, New Labels, Heatmap Comparison and Image Dataset Drift. For this evaluation, we modified the pre-defined Train-Test suite to obtain a custom suite, which only uses the Heatmap Comparison and the Image Property Drift.
The image properties used by this custom suite are the following: brightness, RMS contrast, mean relative intensity, mean green relative intensity, and mean blue intensity. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Single Image Fréchet Inception Distance (SIFID).</span> This is a variation of the Fréchet Inception Distance (FID), which is a metric used to evaluate the images generated by generative models. SIFID measures the deviation between the internal distribution of deep features extracted at the output of the convolutional layer just before the second pooling layer.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Used Datasets</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Three ex-vivo kidney stone datasets were utilized to perform the evaluation protocol. The images were acquired either with standard CCD cameras or with a ureteroscope (i.e., an endoscope), as described as follows in more detail.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.1">Dataset A (CCD-camera images).</span> The ex-vivo dataset described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib6" title="">6</a>]</cite> consists of 366 CCD camera images acquired ex-vivo. These images, as depicted in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.F3.sf1" title="In Figure 3 ‣ III-C Homogening Image Size via Super-Resolution ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">3a</span></a> are divided into 209 surface images and 157 section images. The dataset comprises six different stone types categorized by subtypes denoted by WW (Whewellite, sub-type Ia), CAR (Carbapatite, IVa), CAR2 (Carbapatite, IVa2), STR (Struvite, IVc), BRU (Brushite, IVd), and CYS (Cystine, Va). The acquisition of stone fragment images was carried out using a digital camera under controlled lighting conditions and against a uniform background. The annotation of the images used in this work was statistically confirmed with a study exploiting MCA of extracted kidney stone fragments using microscopy and FTIR analysis.
</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.1.1">Dataset B (Synthetic images with sinDDM).</span>
The synthetic dataset obtained in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS2" title="III-B Synthetic Image Generation with SinDDM ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, consists of 300 synthetic images simulating CCD camera images acquired ex-vivo from dataset A. The images were generated with the sinDDM method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib10" title="">10</a>]</cite>. These images are divided into two sets of 150 images for surface and section. Each view contains the six subtypes presented in dataset A. Some examples of the generated images are presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">1</span></a>. The dimensions of the images in dataset B are 1056<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><mo id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><times id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">×</annotation></semantics></math>800 pixels.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.1">Dataset C (Endoscopic images)</span>. The endoscopic dataset comprises 409 images, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.F3.sf2" title="In Figure 3 ‣ III-C Homogening Image Size via Super-Resolution ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">3b</span></a>. Within this dataset, there are 246 surface images and 163 section images. Dataset B features the same classes as dataset A, except for substituting Carbapatite fragments (subtypes IVa1 and IVa2) with the Weddelite (subtype IIa) and Uric Acid (IIIa) classes. The images in dataset C were captured using an endoscope, with the kidney stone fragments placed in an environment designed to simulate in-vivo conditions quite realistically (for additional information, refer to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib12" title="">12</a>]</cite>). The dimensions of images in dataset B are 576<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.p4.1.m1.1"><semantics id="S3.SS5.p4.1.m1.1a"><mo id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><times id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.1.m1.1d">×</annotation></semantics></math>768 pixels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Evaluation protocol</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In order to evaluate the performance of the synthetic images of kidney stones generated in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS2" title="III-B Synthetic Image Generation with SinDDM ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> with respect to datasets A (CCD-camera images) and C (endoscopic images), two batteries of experiments were performed. The first set of experiments (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS2" title="IV-B Baseline ‣ IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>) were carried out to create a comparison baseline for each dataset. The aim of the second set of experiments (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS3" title="IV-C Two-step Transfer Learning ‣ IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>) is to assess the plausibility of using synthetic images for training models for endoscopic stone recognition that are able to generalize well to an unseen distribution. For training and testing of both schemes, datasets A, B and C (described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS5" title="III-E Used Datasets ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a>) are processed as patches (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS1" title="IV-A Patch selection ‣ IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Patch selection</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Automatic kidney stone classification typically doesn’t analyze full images due to the limited size of datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite>. Instead, patches of 256<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">×</annotation></semantics></math>256 pixels are often extracted from original images to augment the training dataset size, as detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>. A total of 12,000 patches were generated for each dataset, which is categorized into six classes as follows: Datasets A and B include (WW, STR, CYS, BRU, CAR, CAR2), while dataset C includes (WW, WD, UA, STR, BRU, CYS).
A thousand patches are allocated for each class and view (SUR, SEC). However, it is important to note that patches from the same images were excluded from both the training/validation and test datasets.
For each dataset, 80% of the patches (9600) are utilized for the training and validation phases, while the remaining 20% (2400) serve as test data (200 patch-images for each class). Patches from the same image are exclusively assigned to either the training/validation or test data.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Baseline</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the first set of experiments, three independent models were trained from scratch on datasets A, B, and C, to determine the performance of each model by testing on its distribution (i.e., training on synthetic images, and testing on synthetic images). See <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>]</cite> for implementation details or proceed to “First Step TL” in the following section.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Two-step Transfer Learning</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">TL has proven to be an efficient technique to adapt a target domain using an intermediate distribution (similar to the target domain), and perform kidney stone classification with acceptable performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib5" title="">5</a>]</cite>. In this contribution, two-step TL (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F2.sf2" title="In Figure 2 ‣ I-B Motivation ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">2b</span></a>) is adopted to evaluate the performance of datasets A and B (described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS5" title="III-E Used Datasets ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a>) as an intermediate distribution to classify kidney stones from endoscopic images (Dataset C, target domain).</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">First TL Step: </span> The first step of TL consists of training a ResNet50 architecture using a large dataset as ImageNet (a far distribution of kidney stone images). Then, a fine-tuning with CCD-camera or synthetic patches (near endoscopic images) and learned weights from ImageNet are used as input to train Model I. The implementation details for the first TL step are as follows: A batch size of 24 was employed alongside an SGD optimizer having a learning rate set to 0.001 and a momentum of 0.9. Fully connected layers incorporating 768, 256, 128, and 6 neurons were introduced, accompanied by batch normalization, ReLU activation, and a dropout rate of 0.5.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Second TL Step: </span> In a second TL step, Model II learns the distribution of the endoscopic image dataset (target domain) and complements it with the weights learned from the distribution of CCD-camera or synthetic images (intermediate domain) obtained in the first step. The initial weights of Model II are those after Model I is fitted with dataset A or B, and Model II is finally refined with dataset C.
This method aims to enhance the overall generalization capability of Model II and simplify the process of extracting robust features.
Additionally, 30 epochs were conducted using an SGD optimizer with an increased learning rate of 0.01. Fully connected layers were omitted, as the intention was to utilize the model without any additional alterations to its architecture.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results and Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Generation</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Input dataset and padding:</span> In order to generate synthetic images from a distribution of CCD-camera images, eight images per class (see, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS5" title="III-E Used Datasets ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a> “Dataset A”) for each view (SUR and SEC) were used. The images in the training were randomly selected.
A uniform size in the image dimensions is usually required to train the different DL models. Therefore, padding was applied to standardize the dimensions (see Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS1" title="III-A Homogenize image dimensions with padding ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>). By applying padding to the input images, a training set with 2448<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><times id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">×</annotation></semantics></math>4288 pixel images was obtained.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Synthetic image generation: </span> For the generation of synthetic images based on CCD-camera images, a total of 12 models were trained (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">1</span></a>).
Six models (one for each subtype) for each view (SUR, and SEC) were trained using SinDDM with the same implementation details described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib10" title="">10</a>]</cite>.
Eight CCD-camera images with padding were used as training sets for each model. A total of 25 images were generated by the SinDDM model, which presents similar shape, color, and texture characteristics to the input set. Despite generating synthetic images with high detail preservation, the output dimensions are reduced (264<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><times id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">×</annotation></semantics></math>200 pixels), which is not useful for the classification tasks that will be addressed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4" title="IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.3.1">SuperResolution and output dataset: </span> In order to have synthetic images that can be used in patch-based classification tasks (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4" title="IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">IV</span></a>), the synthetic images are processed by a SuperResolution model (Swin2SR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib17" title="">17</a>]</cite>). The aim of using this model is to increase the image size by a factor of 4 and obtain a set of images with a resolution of 1056<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><times id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">×</annotation></semantics></math>800 pixels. Although a <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mo id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><times id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">×</annotation></semantics></math>8 scale would be more similar to CCD-camera images, we opted for a <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mo id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><times id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">×</annotation></semantics></math>4 scale which is more similar to the size of the target domain (images acquired with endoscopes).</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">DeepChecks evaluation: </span> The evaluation of the generated dataset (synthetic images) is performed with DeepChecks against the dataset of CCD-camera images with padding. DeepChecks provided a drift score for 5 characteristics described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S3.SS4" title="III-D Quantitative evaluation of generated images ‣ III Proposed Approach for Image Generation ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>, which measures the difference between two distributions, obtained for each image property. A drift score greater than 0.2 means that there is a large difference between the distributions, so the ideal is to obtain a value lower than this limit.
For the SUR view, values of 0.142 brightness, 0.124 RMS Contrast, 0.060 Mean Red Relative Intensity, 0.112 Mean Green Relative Intensity, and 0.094 Mean Blue Relative Intensity were obtained. For the SEC view, values of 0.137 brightness, 0.145 RMS Contrast, 0.077 Mean Red Relative Intensity, 0.055 Mean Green Relative Intensity, and 0.092 Mean Blue Relative Intensity were obtained. For this evaluation, all the properties obtained a drift score of less than 0.2, which means that the distributions of each of these properties are similar. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.F4" title="Figure 4 ‣ V-A Generation ‣ V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">4</span></a> presents the distribution graphs for each evaluated image property, showing us the difference between the distribution of the real images and the distribution of the generated images in the SUR (surface) view.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance comparison (mean ± standard deviation measured as accuracy classification of six different classes) for different Two-Step TL configurations. The baseline is defined to establish a model reference. Two-Step TL* configurations (with a star) denote that the intermediate and target distributions are similar. While a Two-Step TL configuration (without a star) is used to denote an intermediate distribution (such as CCD-camera or synthetic) towards an endoscopic dataset. The best results for each view are denoted in bold. Accuracy I and II, correspond to the test on the 1st and 2nd step TL, respectively. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T1.26">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.26.27.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.1">View</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.2">Initialization</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.3">Dataset I</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.4">Dataset II</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.5">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.6">Accuracy I</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.7">Accuracy II</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T1.26.27.1.8">Configuration</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2" rowspan="8"><span class="ltx_text" id="S5.T1.1.1.2.1">SUR</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.3">ImageNet</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.4">Synthetic</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.5">No-TL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.6">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1">82.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">±</annotation></semantics></math>08.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.7">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.1.1.8">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.2">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.3">CCD-camera</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.4">No-TL</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.5">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.1">81.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.2.2.1.m1.1"><semantics id="S5.T1.2.2.1.m1.1a"><mo id="S5.T1.2.2.1.m1.1.1" xref="S5.T1.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.1.m1.1d">±</annotation></semantics></math>09.91</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.2.6">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.2.2.7">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3">
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.2">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.3">Endoscopic</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4">No-TL</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.5">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.1">68.98<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.3.3.1.m1.1"><semantics id="S5.T1.3.3.1.m1.1a"><mo id="S5.T1.3.3.1.m1.1.1" xref="S5.T1.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.1.m1.1d">±</annotation></semantics></math>12.61</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.6">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.3.3.7">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.3">ImageNet</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.4">Synthetic</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5">CCD-camera</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.6">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.4.1">82.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.4.4.1.m1.1"><semantics id="S5.T1.4.4.1.m1.1a"><mo id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.1.m1.1d">±</annotation></semantics></math>08.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.2">83.81<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.5.5.2.m1.1"><semantics id="S5.T1.5.5.2.m1.1a"><mo id="S5.T1.5.5.2.m1.1.1" xref="S5.T1.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.2.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.2.m1.1.1.cmml" xref="S5.T1.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.2.m1.1d">±</annotation></semantics></math>04.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.5.5.7">Two-Step TL*</td>
</tr>
<tr class="ltx_tr" id="S5.T1.7.7">
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.3">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.4">CCD-camera</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.5">Synthetic</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.6">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.6.6.1">81.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.6.6.1.m1.1"><semantics id="S5.T1.6.6.1.m1.1a"><mo id="S5.T1.6.6.1.m1.1.1" xref="S5.T1.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.1.m1.1d">±</annotation></semantics></math>09.91</td>
<td class="ltx_td ltx_align_center" id="S5.T1.7.7.2">82.62<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.7.7.2.m1.1"><semantics id="S5.T1.7.7.2.m1.1a"><mo id="S5.T1.7.7.2.m1.1.1" xref="S5.T1.7.7.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.2.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.2.m1.1.1.cmml" xref="S5.T1.7.7.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.2.m1.1d">±</annotation></semantics></math>03.87</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.7.7.7">Two-Step TL*</td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9">
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.3"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.4.1">Synthetic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.5"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.6"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.1"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.1.1">82.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.8.8.1.1.m1.1"><semantics id="S5.T1.8.8.1.1.m1.1a"><mo id="S5.T1.8.8.1.1.m1.1.1" xref="S5.T1.8.8.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.8.8.1.1.m1.1.1.cmml" xref="S5.T1.8.8.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.1.1.m1.1d">±</annotation></semantics></math>04.16</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.2.1">85.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.9.9.2.1.m1.1"><semantics id="S5.T1.9.9.2.1.m1.1a"><mo id="S5.T1.9.9.2.1.m1.1.1" xref="S5.T1.9.9.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.9.9.2.1.m1.1.1.cmml" xref="S5.T1.9.9.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.2.1.m1.1d">±</annotation></semantics></math>05.75</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.7.1">Two-Step TL</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.11.11">
<td class="ltx_td ltx_align_center" id="S5.T1.11.11.3"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.11.11.4"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.4.1">CCD-camera</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.11.11.5"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.11.11.6"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.10.10.1"><span class="ltx_text ltx_font_bold" id="S5.T1.10.10.1.1">81.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.10.10.1.1.m1.1"><semantics id="S5.T1.10.10.1.1.m1.1a"><mo id="S5.T1.10.10.1.1.m1.1.1" xref="S5.T1.10.10.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.10.10.1.1.m1.1.1.cmml" xref="S5.T1.10.10.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.10.10.1.1.m1.1d">±</annotation></semantics></math>05.51</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.11.11.2"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.2.1">86.19<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.11.11.2.1.m1.1"><semantics id="S5.T1.11.11.2.1.m1.1a"><mo id="S5.T1.11.11.2.1.m1.1.1" xref="S5.T1.11.11.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.11.11.2.1.m1.1.1.cmml" xref="S5.T1.11.11.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.11.11.2.1.m1.1d">±</annotation></semantics></math>06.12</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.11.11.7"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.7.1">Two-Step TL</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.13.13">
<td class="ltx_td ltx_align_center" id="S5.T1.13.13.3"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.13.13.4"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.4.1">Synthetic + CCD-camera</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.13.13.5"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.13.13.6"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.12.12.1"><span class="ltx_text ltx_font_bold" id="S5.T1.12.12.1.1">82.34<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.12.12.1.1.m1.1"><semantics id="S5.T1.12.12.1.1.m1.1a"><mo id="S5.T1.12.12.1.1.m1.1.1" xref="S5.T1.12.12.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.12.12.1.1.m1.1.1.cmml" xref="S5.T1.12.12.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.12.12.1.1.m1.1d">±</annotation></semantics></math>04.70</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.13.13.2"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.2.1">87.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.13.13.2.1.m1.1"><semantics id="S5.T1.13.13.2.1.m1.1a"><mo id="S5.T1.13.13.2.1.m1.1.1" xref="S5.T1.13.13.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.13.13.2.1.m1.1.1.cmml" xref="S5.T1.13.13.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.13.13.2.1.m1.1d">±</annotation></semantics></math>03.65</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.13.13.7"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.7.1">Two-Step TL</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.14">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.14.14.2" rowspan="8"><span class="ltx_text" id="S5.T1.14.14.2.1">SEC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.3">ImageNet</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.4">Synthetic</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.5">No-TL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.6">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.1">71.53<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.14.14.1.m1.1"><semantics id="S5.T1.14.14.1.m1.1a"><mo id="S5.T1.14.14.1.m1.1.1" xref="S5.T1.14.14.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.1.m1.1b"><csymbol cd="latexml" id="S5.T1.14.14.1.m1.1.1.cmml" xref="S5.T1.14.14.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.14.14.1.m1.1d">±</annotation></semantics></math>10.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.14.14.7">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.14.14.8">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.15.15">
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.2">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.3">CCD-camera</td>
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.4">No-TL</td>
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.5">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.1">71.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.15.15.1.m1.1"><semantics id="S5.T1.15.15.1.m1.1a"><mo id="S5.T1.15.15.1.m1.1.1" xref="S5.T1.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.15.15.1.m1.1b"><csymbol cd="latexml" id="S5.T1.15.15.1.m1.1.1.cmml" xref="S5.T1.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.15.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.15.15.1.m1.1d">±</annotation></semantics></math>11.73</td>
<td class="ltx_td ltx_align_center" id="S5.T1.15.15.6">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.15.15.7">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.16.16">
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.2">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.3">Endoscopic</td>
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.4">No-TL</td>
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.5">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.1">57.29<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.16.16.1.m1.1"><semantics id="S5.T1.16.16.1.m1.1a"><mo id="S5.T1.16.16.1.m1.1.1" xref="S5.T1.16.16.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.16.16.1.m1.1b"><csymbol cd="latexml" id="S5.T1.16.16.1.m1.1.1.cmml" xref="S5.T1.16.16.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.16.16.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.16.16.1.m1.1d">±</annotation></semantics></math>17.87</td>
<td class="ltx_td ltx_align_center" id="S5.T1.16.16.6">No-TL</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.16.16.7">Baseline</td>
</tr>
<tr class="ltx_tr" id="S5.T1.18.18">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.18.18.3">ImageNet</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.18.18.4">Synthetic</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.18.18.5">CCD-camera</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.18.18.6">ResNet50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.17.17.1">71.53<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.17.17.1.m1.1"><semantics id="S5.T1.17.17.1.m1.1a"><mo id="S5.T1.17.17.1.m1.1.1" xref="S5.T1.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.17.17.1.m1.1b"><csymbol cd="latexml" id="S5.T1.17.17.1.m1.1.1.cmml" xref="S5.T1.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.17.17.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.17.17.1.m1.1d">±</annotation></semantics></math>10.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.18.18.2">72.69<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.18.18.2.m1.1"><semantics id="S5.T1.18.18.2.m1.1a"><mo id="S5.T1.18.18.2.m1.1.1" xref="S5.T1.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.18.18.2.m1.1b"><csymbol cd="latexml" id="S5.T1.18.18.2.m1.1.1.cmml" xref="S5.T1.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.18.18.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.18.18.2.m1.1d">±</annotation></semantics></math>09.22</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.18.18.7">Two-Step TL*</td>
</tr>
<tr class="ltx_tr" id="S5.T1.20.20">
<td class="ltx_td ltx_align_center" id="S5.T1.20.20.3">ImageNet</td>
<td class="ltx_td ltx_align_center" id="S5.T1.20.20.4">CCD-camera</td>
<td class="ltx_td ltx_align_center" id="S5.T1.20.20.5">Synthetic</td>
<td class="ltx_td ltx_align_center" id="S5.T1.20.20.6">ResNet50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.19.19.1">71.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.19.19.1.m1.1"><semantics id="S5.T1.19.19.1.m1.1a"><mo id="S5.T1.19.19.1.m1.1.1" xref="S5.T1.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.19.19.1.m1.1b"><csymbol cd="latexml" id="S5.T1.19.19.1.m1.1.1.cmml" xref="S5.T1.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.19.19.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.19.19.1.m1.1d">±</annotation></semantics></math>11.73</td>
<td class="ltx_td ltx_align_center" id="S5.T1.20.20.2">72.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.20.20.2.m1.1"><semantics id="S5.T1.20.20.2.m1.1a"><mo id="S5.T1.20.20.2.m1.1.1" xref="S5.T1.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.20.20.2.m1.1b"><csymbol cd="latexml" id="S5.T1.20.20.2.m1.1.1.cmml" xref="S5.T1.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.20.20.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.20.20.2.m1.1d">±</annotation></semantics></math>08.89</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.20.20.7">Two-Step TL*</td>
</tr>
<tr class="ltx_tr" id="S5.T1.22.22">
<td class="ltx_td ltx_align_center" id="S5.T1.22.22.3"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.22.22.4"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.4.1">Synthetic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.22.22.5"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.22.22.6"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.21.21.1"><span class="ltx_text ltx_font_bold" id="S5.T1.21.21.1.1">73.53<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.21.21.1.1.m1.1"><semantics id="S5.T1.21.21.1.1.m1.1a"><mo id="S5.T1.21.21.1.1.m1.1.1" xref="S5.T1.21.21.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.21.21.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.21.21.1.1.m1.1.1.cmml" xref="S5.T1.21.21.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.21.21.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.21.21.1.1.m1.1d">±</annotation></semantics></math>07.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.22.22.2"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.2.1">79.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.22.22.2.1.m1.1"><semantics id="S5.T1.22.22.2.1.m1.1a"><mo id="S5.T1.22.22.2.1.m1.1.1" xref="S5.T1.22.22.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.22.22.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.22.22.2.1.m1.1.1.cmml" xref="S5.T1.22.22.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.22.22.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.22.22.2.1.m1.1d">±</annotation></semantics></math>05.58</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.22.22.7"><span class="ltx_text ltx_font_bold" id="S5.T1.22.22.7.1">Two-Step TL</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.24.24">
<td class="ltx_td ltx_align_center" id="S5.T1.24.24.3"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.24.24.4"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.4.1">CCD-camera</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.24.24.5"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.24.24.6"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.23.23.1"><span class="ltx_text ltx_font_bold" id="S5.T1.23.23.1.1">74.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.23.23.1.1.m1.1"><semantics id="S5.T1.23.23.1.1.m1.1a"><mo id="S5.T1.23.23.1.1.m1.1.1" xref="S5.T1.23.23.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.23.23.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.23.23.1.1.m1.1.1.cmml" xref="S5.T1.23.23.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.23.23.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.23.23.1.1.m1.1d">±</annotation></semantics></math>08.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.24.24.2"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.2.1">78.27<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.24.24.2.1.m1.1"><semantics id="S5.T1.24.24.2.1.m1.1a"><mo id="S5.T1.24.24.2.1.m1.1.1" xref="S5.T1.24.24.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.24.24.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.24.24.2.1.m1.1.1.cmml" xref="S5.T1.24.24.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.24.24.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.24.24.2.1.m1.1d">±</annotation></semantics></math>04.27</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.24.24.7"><span class="ltx_text ltx_font_bold" id="S5.T1.24.24.7.1">Two-Step TL</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.26.26">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.26.26.3"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.3.1">ImageNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.26.26.4"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.4.1">Synthetic + CCD-camera</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.26.26.5"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.5.1">Endoscopic</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.26.26.6"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.6.1">ResNet50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.25.25.1"><span class="ltx_text ltx_font_bold" id="S5.T1.25.25.1.1">75.88<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.25.25.1.1.m1.1"><semantics id="S5.T1.25.25.1.1.m1.1a"><mo id="S5.T1.25.25.1.1.m1.1.1" xref="S5.T1.25.25.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.25.25.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.25.25.1.1.m1.1.1.cmml" xref="S5.T1.25.25.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.25.25.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.25.25.1.1.m1.1d">±</annotation></semantics></math>07.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.26.26.2"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.2.1">80.76<math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.26.26.2.1.m1.1"><semantics id="S5.T1.26.26.2.1.m1.1a"><mo id="S5.T1.26.26.2.1.m1.1.1" xref="S5.T1.26.26.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.26.26.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.26.26.2.1.m1.1.1.cmml" xref="S5.T1.26.26.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.26.26.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.26.26.2.1.m1.1d">±</annotation></semantics></math>04.91</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T1.26.26.7"><span class="ltx_text ltx_font_bold" id="S5.T1.26.26.7.1">Two-Step TL</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="428" id="S5.F4.1.g1" src="x3.png" width="373"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution plot for each image property showing the difference between the train and the synthetic image (SUR view).</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.F5" title="Figure 5 ‣ V-A Generation ‣ V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">5</span></a> shows the Heatmap Comparison, which is an average representation of images within each dataset displayed side by side for comparison. This allows us to easily observe differences in brightness distribution between the datasets and the region in which the dataset images are being generated. As can be seen, most of the images are centered and have a region of interest of up to 50% of the image.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.2">Finally, an evaluation was performed with SIFID, a metric to evaluate the similarity between the training set images (CCD images) and the generated set (synthetic images). For the SUR view classes, a SIFID of 3.14<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS1.p6.1.m1.1"><semantics id="S5.SS1.p6.1.m1.1a"><mo id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.1.m1.1d">±</annotation></semantics></math>0.93 was obtained, while for the SEC view it was as high as 3.79<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS1.p6.2.m2.1"><semantics id="S5.SS1.p6.2.m2.1a"><mo id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.2.m2.1d">±</annotation></semantics></math>1.71.
A lower SIFID indicates that the synthetic images are more similar to the training set images. Hence, the SUR images are more similar to the CCD-camera images. In order to qualitatively compare the images generated in this contribution with the state of the art, DDPM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib20" title="">20</a>]</cite> was implemented. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.F6" title="Figure 6 ‣ V-A Generation ‣ V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">6</span></a> shows the comparison of the synthetic images generated with DDPM and SinDDM models concerning the CCD-camera image. As can be seen, the DDPM model lacks colors and textures characteristic of the input images, which presents a high FID (around 90). On the other hand, the SinDDM model presents a high similarity with the training set (a lower SIFID, around 3).</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="93" id="S5.F5.1.g1" src="extracted/5868160/images/HeatmapComparisonSUR.png" width="287"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Heatmap Comparison between the Train Dataset and Synthetic Dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S5.F6.1.g1" src="extracted/5868160/images/comparison.png" width="240"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of the synthetic images generated with DDPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#bib.bib20" title="">20</a>]</cite> and SinDDM (this contribution) models for the CCD-camera image.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Classification</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.6"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.6.1">Baseline:</span> Three models (CCD-camera, synthetic, and endoscopic) based on ResNet50 and ImageNet weights were trained to determine the baseline. As can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.T1" title="TABLE I ‣ V-A Generation ‣ V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">I</span></a> (denoted by “Baseline” in the column “Configuration”), the performance of the model trained with synthetic images is very similar to the model trained with CCD-camera images. However, the performance of the endoscopic model falls below that of models based on CCD-camera or synthetic images. For SUR, the performance of the models is 82.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">±</annotation></semantics></math>08.72% and 81.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mo id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">±</annotation></semantics></math>09.91% for synthetic and CCD-camera images, respectively. While for the endoscopic dataset, a performance of 68.98<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mo id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">±</annotation></semantics></math>12.61% is obtained. On the other hand, for SEC, performance declines as follows: 71.53<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mo id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><csymbol cd="latexml" id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">±</annotation></semantics></math>10.21% and 71.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.5.m5.1"><semantics id="S5.SS2.p1.5.m5.1a"><mo id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><csymbol cd="latexml" id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.5.m5.1d">±</annotation></semantics></math>11.73% for synthetic and CCD-camera, while endoscope barely achieves 57.29<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p1.6.m6.1"><semantics id="S5.SS2.p1.6.m6.1a"><mo id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><csymbol cd="latexml" id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.6.m6.1d">±</annotation></semantics></math>17.87%.
However, although the performance of synthetic images is high and similar to CCD-camera images, the real interest lies in enhancing the performance of models trained with endoscopic images (dataset C). For the second set of experiments, two-step transfer learning was performed, to learn in the first step a dataset (CCD-camera or synthetic), and transfer the knowledge to a target distribution such as an endoscopic dataset (Dataset C) as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S4.SS3" title="IV-C Two-step Transfer Learning ‣ IV Evaluation protocol ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.12"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.12.1">Two-step Transfer Learning: </span> To ascertain the performance between the synthetic and CCD-camera datasets, and vice versa, the two-step TL* (with star) configuration was used (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13409v1#S5.T1" title="TABLE I ‣ V-A Generation ‣ V Results and Discussion ‣ Evaluating the plausibility of synthetic images for improving automated endoscopic stone recognition"><span class="ltx_text ltx_ref_tag">I</span></a>). For both views, it can be observed that performing TL between similar distributions does not significantly improve the training (similar to the performance obtained in baseline). This could be because there is no more relevant color or texture information to be learned from the distribution.
Finally, to evaluate the performance of the synthetic images concerning the CCD-camera as an intermediate distribution, Two-Step TL (no star) is performed towards the endoscopic dataset. In addition, a further test is performed by combining the images from the CCD-camera and synthetic distribution, to determine if there is any positive effect when combining both sets. The results for the SUR view suggest an improvement from 82.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mo id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">±</annotation></semantics></math>04.16% to 85.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mo id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">±</annotation></semantics></math>05.75% with the synthetic to endoscopic configuration. While the CCD-camera to Endoscopic configuration improves from 81.72<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mo id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">±</annotation></semantics></math>05.51% to 86.19<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mo id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">±</annotation></semantics></math>06.12%. For the third configuration (Synthetic+CCD-camera to Endoscopic), there is a significant increase from 82.34<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mo id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><csymbol cd="latexml" id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">±</annotation></semantics></math>04.70% to 87.82<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.6.m6.1"><semantics id="S5.SS2.p2.6.m6.1a"><mo id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><csymbol cd="latexml" id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.6.m6.1d">±</annotation></semantics></math>03.65%. On the other hand, for the SEC view, they follow a similar trend from 73.53<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.7.m7.1"><semantics id="S5.SS2.p2.7.m7.1a"><mo id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><csymbol cd="latexml" id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.7.m7.1d">±</annotation></semantics></math>07.32% to 79.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.8.m8.1"><semantics id="S5.SS2.p2.8.m8.1a"><mo id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><csymbol cd="latexml" id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.8.m8.1d">±</annotation></semantics></math>05.58% in the synthetic to Endoscopic configuration. Also, there is an increase from 74.95<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.9.m9.1"><semantics id="S5.SS2.p2.9.m9.1a"><mo id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><csymbol cd="latexml" id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.9.m9.1d">±</annotation></semantics></math>08.58% to 78.27<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.10.m10.1"><semantics id="S5.SS2.p2.10.m10.1a"><mo id="S5.SS2.p2.10.m10.1.1" xref="S5.SS2.p2.10.m10.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m10.1b"><csymbol cd="latexml" id="S5.SS2.p2.10.m10.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m10.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.10.m10.1d">±</annotation></semantics></math>04.27% from CCD-camera to Endoscopic, respectively. Finally, for Synthetic to CCD-camera to Endoscopic distribution, there is an increase from 75.88<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.11.m11.1"><semantics id="S5.SS2.p2.11.m11.1a"><mo id="S5.SS2.p2.11.m11.1.1" xref="S5.SS2.p2.11.m11.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.11.m11.1b"><csymbol cd="latexml" id="S5.SS2.p2.11.m11.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.11.m11.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.11.m11.1d">±</annotation></semantics></math>07.33% to 80.76<math alttext="\pm" class="ltx_Math" display="inline" id="S5.SS2.p2.12.m12.1"><semantics id="S5.SS2.p2.12.m12.1a"><mo id="S5.SS2.p2.12.m12.1.1" xref="S5.SS2.p2.12.m12.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.12.m12.1b"><csymbol cd="latexml" id="S5.SS2.p2.12.m12.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.12.m12.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.12.m12.1d">±</annotation></semantics></math>04.91%.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusions and Future Work</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The identification of kidney stones using machine learning techniques is an open problem. One of the main limitations is the lack of data to train models, especially when there is a class imbalance, which is common in the context of kidney stones. In this work, we proposed the generation of synthetic images to balance and increase the number of samples in datasets. Furthermore, the generation of synthetic CCD-camera images was evaluated using DeepChecks, which shows that the characteristics such as color and texture are similar between the two sets. Subsequently, a Two-Step Transfer Learning method was implemented to evaluate the automatic classification performance on endoscopic images using the synthetic images as an intermediate domain. It was demonstrated that synthetic images are useful for achieving performance similar to CCD-camera images. Combining both sets represents a good starting point for evaluating endoscopic images.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In future work, it is desired to generate and classify whole CCD-camera or endoscopic images. As well as, to control the percentage of the kidney stone with respect to the image size (useful for patch-based classification). In addition, it is expected to generate synthetic endoscopic images to train models with real artifacts in clinical practice (such as illumination changes and blur), and also compare with models such as Generative Adversary Networks. Perhaps the greatest challenge will be to insert kidney stones that appear on CCD-camera images in an endoscopic context, where the surrounding tissue is observed and not a dark background.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The authors wish to acknowledge the Mexican Council for Humanities, Science, and Technology (CONAHCYT) for their support in terms of postgraduate scholarships in this project, and the Data Science Hub at Tecnologico de Monterrey for their support on this project.
This work has been supported by Azure Sponsorship credits granted by Microsoft’s AI for Good Research Lab through the AI for Health program. The project was also supported by the French-Mexican ANUIES CONAHCYT Ecos Nord grant (MX 322537/FR M022M01).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
F. Shaikh, J. Dehmeshki, S. Bisdas, D. Roettger-Dupont, O. Kubassova, M. Aziz, and O. Awan, “Artificial intelligence-based clinical decision support systems using advanced medical imaging and radiomics,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Current Problems in Diagnostic Radiology</em>, vol. 50, no. 2, pp. 262–267, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Ali, “Where do we stand in ai for endoscopic image analysis? deciphering gaps and future directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">npj Digital Medicine</em>, vol. 5, no. 1, p. 184, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Lopez-Tiro, D. Flores-Araiza, J. P. Betancur-Rengifo, I. Reyes-Amezcua, J. Hubert, G. Ochoa-Ruiz, and C. Daul, “Boosting kidney stone identification in endoscopic images using two-step transfer learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Mexican International Conference on Artificial Intelligence</em>.   Springer, 2023, pp. 131–141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Daudon, A. Dessombz, V. Frochot, E. Letavernier, J.-P. Haymann, P. Jungers, and D. Bazin, “Comprehensive morpho-constitutional analysis of urinary stones improves etiological diagnosis and therapeutic strategy of nephrolithiasis,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Comptes Rendus Chimie</em>, vol. 19, no. 11-12, pp. 1470–1491, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Lopez-Tiro, V. Estrade, J. Hubert, D. Flores-Araiza, M. Gonzalez-Mendoza, G. Ochoa-Ruiz, and C. Daul, “On the in vivo recognition of kidney stones using machine learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Access</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Corrales, S. Doizi, Y. Barghouthy, O. Traxer, and M. Daudon, “Classification of stones according to michel daudon: a narrative review,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">European Urology Focus</em>, vol. 7, no. 1, pp. 13–21, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Huynh, A. Nibali, and Z. He, “Semi-supervised learning for medical image classification using imbalanced training data,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Computer methods and programs in biomedicine</em>, vol. 216, p. 106628, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Vats, M. Pedersen, A. Mohammed, and Ø. Hovde, “Evaluating clinical diversity and plausibility of synthetic capsule endoscopic images,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Scientific Reports</em>, vol. 13, no. 1, p. 10857, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
V. Estrade, M. Daudon, E. Richard, J.-c. Bernhard, F. Bladou, G. Robert, and B. Denis de Senneville, “Towards automatic recognition of pure and mixed stones using intra-operative endoscopic digital images,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">BJU international</em>, vol. 129, no. 2, pp. 234–242, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
V. Kulikov, S. Yadin, M. Kleiner, and T. Michaeli, “SinDDM: A single image denoising diffusion model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, 2023, pp. 17 920–17 930.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. R. Shaham, T. Dekel, and T. Michaeli, “Singan: Learning a generative model from a single natural image,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 4570–4580.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. El Beze, C. Mazeaud, C. Daul, G. Ochoa-Ruiz, M. Daudon, P. Eschwège, and J. Hubert, “Evaluation and understanding of automated urinary stone recognition methods,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">BJU international</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation for deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Journal of Big Data</em>, vol. 6, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Tiago, A. Gilbert, A. S. Beela, S. A. Aase, S. R. Snare, J. Sprem, and K. McLeod, “A data augmentation pipeline to generate synthetic labeled datasets of 3d echocardiography images using a gan,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Access</em>, vol. 10, p. 98803–98815, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X. Zhang, A. Gangopadhyay, H.-M. Chang, and R. Soni, “Diffusion model-based data augmentation for lung ultrasound classification with limited data,” 12 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Zhang, Q. Wang, and B. Hu, “Minimalgan: diverse medical image synthesis for data augmentation using minimal training data,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Applied Intelligence</em>, vol. 53, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. V. Conde, U.-J. Choi, M. Burchi, and R. Timofte, “Swin2sr: Swinv2 transformer for compressed image super-resolution and restoration,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Liang, J. Cao, G. Sun, K. Zhang, L. V. Gool, and R. Timofte, “Swinir: Image restoration using swin transformer,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Chorev, P. Tannor, D. B. Israel, N. Bressler, I. Gabbay, N. Hutnik, J. Liberman, M. Perlmutter, Y. Romanyshyn, and L. Rokach, “Deepchecks: A library for testing and validating machine learning models and data,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in neural information processing systems</em>, vol. 33, pp. 6840–6851, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 11:14:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
