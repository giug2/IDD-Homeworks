<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sequence Shortening for Context-Aware Machine Translation</title>
<!--Generated on Fri Feb  2 13:31:32 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.01416v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S1" title="1 Introduction ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S2" title="2 Related Work ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S2.SS1" title="2.1 Context-aware Machine Translation ‣ 2 Related Work ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Context-aware Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S2.SS2" title="2.2 Sequence Shortening ‣ 2 Related Work ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Sequence Shortening</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S3" title="3 Background ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S3.SS1" title="3.1 Transformer ‣ 3 Background ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S3.SS2" title="3.2 Pooling-based Shortening ‣ 3 Background ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pooling-based Shortening</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4" title="4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.SS1" title="4.1 Latent Grouping ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Latent Grouping</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.SS2" title="4.2 Latent Selecting ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Latent Selecting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.SS3" title="4.3 Context Shortening ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Context Shortening</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5" title="5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS1" title="5.1 Data ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS2" title="5.2 Models ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS3" title="5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS4" title="5.4 Token Assignment Visualization ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Token Assignment Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS5" title="5.5 Memory Usage ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Memory Usage</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S6" title="6 Conclusions ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S7" title="7 Limitations ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S8" title="8 Acknowledgments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A1" title="Appendix A Models and Training Details ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Models and Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A2" title="Appendix B COMET Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>COMET Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A3" title="Appendix C Detailed Contrastive Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Detailed Contrastive Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A4" title="Appendix D Examples of Translations ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Examples of Translations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A5" title="Appendix E Larger Context Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Larger Context Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6" title="Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Groupings and Selections Visualization</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.01416v1 [cs.CL] 02 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sequence Shortening for Context-Aware Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paweł Mąka
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yusuf Can Semerci
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Scholtes
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gerasimos Spanakis
<br class="ltx_break"/>Department of Advanced Computing Sciences 
<br class="ltx_break"/>Maastricht University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{pawel.maka, y.semerci, j.scholtes, jerry.spanakis}@maastrichtuniversity.nl</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Following the introduction of the Transformer model <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib57" title="">2017</a>)</cite>, Sentence-level Machine Translation, where the task is to translate separate sentences, has seen great success in recent years <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib57" title="">2017</a>; Hassan et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib20" title="">2018</a>; Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib11" title="">2022</a>; Tiedemann et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib53" title="">2022</a>)</cite>. However, real-world applications of the translation systems are often used to translate a whole document or a longer discourse (e.g. a transcribed speech). In those circumstances, Sentence-level Machine Translation processes each sentence separately and is incapable of leveraging the surrounding or previous sentences (referred to as the context sentences). This is in contrast to the Context-aware Machine Translation where the context sentences are available to the system. The information in the previous sentences can be helpful to maintain the coherence of the translation and to resolve ambiguities <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib1" title="">2018</a>; Bawden et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib4" title="">2018</a>; Müller et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>; Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib59" title="">2019b</a>)</cite>. Both the sentences of the text in the source language and the previously translated sentences can be used as context. The former is referred to as source-side context and the latter as target-side context.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Many Context-aware Machine Translation approaches have been proposed including novel architectures that can be broadly categorized into <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">single-encoder</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">multi-encoder</span> types. In single-encoder architectures, the context sentences are concatenated with the current sentence and processed as a long sequence by a single encoder <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann and Scherrer, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib54" title="">2017</a>; Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib1" title="">2018</a>; Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib31" title="">2020</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib65" title="">2020</a>; Majumde et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib32" title="">2022</a>)</cite>. In multi-encoder architectures, the context sentences are processed by a separate encoder than the current sentence <cite class="ltx_cite ltx_citemacro_citep">(Tu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib55" title="">2017</a>; Bawden et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib4" title="">2018</a>; Miculicich et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib37" title="">2018</a>; Maruf et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib34" title="">2019</a>; Huo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib21" title="">2020</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib66" title="">2021</a>)</cite>. Several multi-encoder approaches <cite class="ltx_cite ltx_citemacro_citep">(Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib60" title="">2018</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib26" title="">2020</a>)</cite> involve sharing parameters of encoders. This approach reduces the number of parameters and could also increase the speed of translation when translating the whole document sentence-by-sentence. Inspired by this idea, we investigate multi-encoder architectures where all the encoder parameters are shared <cite class="ltx_cite ltx_citemacro_citep">(Tu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib56" title="">2018</a>; Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib59" title="">2019b</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib63" title="">2022</a>)</cite>, which allows caching the hidden representation of the current sentence and reusing it as the hidden representation of the context when translating subsequent sentences. In this study, we refer to this architecture as <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">caching</span>. We experimentally show that this architecture can achieve comparable results to single- and multi-encoder architectures and is more stable in the realm of larger context sizes.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In Transformers, the number of tokens does not change during the processing of the sequence through the encoder (and decoder) layers. Concurrent to Machine Translation, several techniques have been proposed to shorten the sequence of tokens in the task of language modeling <cite class="ltx_cite ltx_citemacro_citep">(Subramanian et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib48" title="">2020</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>; Nawrot et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite>. In particular, the tokens are combined in the shortening modules that are added between a specified number of encoder layers. Sequence Shortening can lead to the reduction of the computational and memory requirements in the subsequent layers as the requirements of the self-attention module grow quadratically with the number of tokens (although a substantial amount of research is done to mitigate that <cite class="ltx_cite ltx_citemacro_citep">(Kitaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib24" title="">2020</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib61" title="">2020</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we investigate the application of Sequence Shortening to Context-aware Machine Translation. Specifically, we apply the shortening of the cached hidden representations of the context sentences in the caching multi-encoder architectures. The intuition behind this approach is that a compressed representation of the previously seen sentences should be enough to use as a context while possibly decreasing the computational and memory requirements during inference. Sequence Shortening can be seen as related to the concept of <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">chunking</span> from psychology <cite class="ltx_cite ltx_citemacro_citep">(Miller, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib38" title="">1956</a>; Terrace, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib52" title="">2002</a>; Mathy and Feldman, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib35" title="">2012</a>)</cite>. To limit the scope, we consider only the source-side context. Additionally, we introduce <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">Latent Grouping</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">Latent Selecting</span> - new shortening techniques where the network can learn how to group or select tokens to form a shortened sequence. Our experiments indicate that sequence shortening can be leveraged to improve the stability of training for larger context sizes (we tested up to 10 previous sentences as context) while achieving comparable results for smaller context sizes.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Context-aware Machine Translation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A straightforward approach to incorporate context into Machine Translation is to concatenate previous sentences with the current sentence, which has been referred to as <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">concatenation</span> or <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">single-encoder</span> architecture because only a single encoder is used <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann and Scherrer, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib54" title="">2017</a>; Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib31" title="">2020</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib65" title="">2020</a>)</cite>. This architecture has achieved very good results <cite class="ltx_cite ltx_citemacro_citep">(Majumde et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib32" title="">2022</a>)</cite> even on long context sizes (of up to 2000 tokens) when data augmentation was used <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib49" title="">2022</a>)</cite> but even longer context sizes will result in a sharply increasing memory and computational complexity <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib15" title="">2022</a>)</cite>. The <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">multi-encoder</span> approach is to encode the context sentences by a separate encoder <cite class="ltx_cite ltx_citemacro_citep">(Jean et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib23" title="">2017</a>; Miculicich et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib37" title="">2018</a>; Maruf et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib34" title="">2019</a>; Huo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib21" title="">2020</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib66" title="">2021</a>)</cite>. While the encoders are separate in multi-encoder architectures, weight-sharing between them has been investigated in previous works <cite class="ltx_cite ltx_citemacro_citep">(Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib60" title="">2018</a>; Tu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib56" title="">2018</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib26" title="">2020</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib63" title="">2022</a>)</cite>. Existing studies also investigated hierarchical attention <cite class="ltx_cite ltx_citemacro_citep">(Miculicich et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib37" title="">2018</a>; Bawden et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib4" title="">2018</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib63" title="">2022</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib8" title="">2022</a>)</cite>, sparse attention <cite class="ltx_cite ltx_citemacro_citep">(Maruf et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib34" title="">2019</a>; Bao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib3" title="">2021</a>)</cite>, aggregating the hidden representation of the context tokens <cite class="ltx_cite ltx_citemacro_citep">(Morishita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib39" title="">2021</a>)</cite>, and post-processing the translation <cite class="ltx_cite ltx_citemacro_citep">(Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib59" title="">2019b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib58" title="">a</a>)</cite>. Similar to ours, several works use a memory mechanism <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib15" title="">2022</a>; Bulatov et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib6" title="">2022</a>)</cite>. The main differences are that the memory-based techniques rely on the attention mechanism to collect information from the sentences. In addition to that, our method allows the tokens in the current sentence to work as a hub tokens instead of the learned (but fixed) tokens of the memory in the initial step or the memory vectors from the previous steps. In the memory approaches, the number of tokens is constant while in the models employing shortening the number of tokens is dependent on the number of context segments.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Mostly orthogonal to architectural approaches, another line of work concentrates on making the models use the context more effectively. These methods utilize regularization such as dropout of the tokens in the source sentence <cite class="ltx_cite ltx_citemacro_citep">(CoWord dropout; Fernandes et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite>, attention regularization based on human translators <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib64" title="">2021</a>)</cite>, and data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Lupo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib30" title="">2022</a>)</cite> along with contrastive learning <cite class="ltx_cite ltx_citemacro_citep">(Hwang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib22" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">It has been argued that widely used sentence-level metrics (such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib43" title="">2002</a>)</cite>) are ill-equipped to measure the translation quality with regard to the inter-sentential phenomena <cite class="ltx_cite ltx_citemacro_citep">(Hardmeier, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib19" title="">2012</a>; Wong and Kit, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib62" title="">2012</a>)</cite>. For this reason, research has been done to measure the usage of context by machine translation models, where two main avenues have been explored: introducing new metrics <cite class="ltx_cite ltx_citemacro_citep">(Fernandes et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib16" title="">2023</a>)</cite> and contrastive datasets <cite class="ltx_cite ltx_citemacro_cite">Müller et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>); Bawden et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib4" title="">2018</a>); Voita et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib59" title="">2019b</a>); Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite>. In the contrastive datasets, the model is presented with the task of ranking several translations of the same source sentence with the same context. The provided translations differ only partially and the provided context is required to choose the correct translation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sequence Shortening</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Sequence Shortening has been introduced as a way to exploit the hierarchical structure of language to reduce the memory and computational cost of the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Subramanian et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib48" title="">2020</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>; Nawrot et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite>. Shortening can be done by average pooling of the hidden representation of the tokens <cite class="ltx_cite ltx_citemacro_cite">Subramanian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib48" title="">2020</a>)</cite>. Allowing the tokens of the shortened sequence to attend to the hidden representation of the original sequence was found beneficial <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>)</cite>. Replacing average pooling with the linear transformation of the concatenated representation of the tokens of the original sequence has also been used <cite class="ltx_cite ltx_citemacro_citep">(Nawrot et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite>. Another way of shortening the sequence is to find and retain only the most important tokens of the original sequence <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib18" title="">2020</a>)</cite>. Furthermore, a large body of work improve the context size or the efficiency of the Transformer model <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib5" title="">2020</a>; Kitaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib24" title="">2020</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib13" title="">2019</a>)</cite> which has been referenced in comprehensive surveys <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib50" title="">2022</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib27" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The work that is architecturally most closely related to one of our methods <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">Latent Grouping</span> is the Charformer <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib51" title="">2021</a>)</cite> architecture, where the tokenization is performed by a sub-network that learns to select block sizes for characters of the input sequence. The block size representations are subsequently summed with weights predicted by the sub-network. <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">Latent Grouping</span> differs from Charformer in the placement of the grouping (after the encoder in the case of <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.3">Latent Grouping</span>) and the aggregated representation (encoder representations of tokens themselves in the case of <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.4">Latent Grouping</span>).
</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Our work lies in the intersection of Context-aware Machine Translation and Sequence Shortening. We test the performance of caching architecture against single- and multi-encoder architectures and investigate applying shortening to the cached sentences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transformer</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The Transformer architecture, introduced for sentence-level translation, consists of the encoder and decoder <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib57" title="">2017</a>)</cite>. The sentence in the source language is tokenized and embedded before it is passed to the encoder. The encoder processes the sequence by <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_L</annotation></semantics></math> consecutive encoder layers, each consisting of the self-attention module and the element-wise feed-forward network. Residual connection is added around both modules followed by Layer Normalization <cite class="ltx_cite ltx_citemacro_citep">(Ba et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib2" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Hidden representation of the <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_L</annotation></semantics></math>-th encoder layer <math alttext="H^{L}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msup id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">H</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐻</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">H^{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> is fed into the decoder, which auto-regressively produces the output sequence <math alttext="Y=(y_{1},...,y_{T})," class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.2"><semantics id="S3.SS1.p2.3.m3.2a"><mrow id="S3.SS1.p2.3.m3.2.2.1" xref="S3.SS1.p2.3.m3.2.2.1.1.cmml"><mrow id="S3.SS1.p2.3.m3.2.2.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.4" xref="S3.SS1.p2.3.m3.2.2.1.1.4.cmml">Y</mi><mo id="S3.SS1.p2.3.m3.2.2.1.1.3" xref="S3.SS1.p2.3.m3.2.2.1.1.3.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.2.2.1.1.2.2" xref="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml"><mo id="S3.SS1.p2.3.m3.2.2.1.1.2.2.3" stretchy="false" xref="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml">(</mo><msub id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml">y</mi><mn id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.3.m3.2.2.1.1.2.2.4" xref="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml">,</mo><mi id="S3.SS1.p2.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p2.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.p2.3.m3.2.2.1.1.2.2.5" xref="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml">,</mo><msub id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.cmml"><mi id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.3" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.3.cmml">T</mi></msub><mo id="S3.SS1.p2.3.m3.2.2.1.1.2.2.6" stretchy="false" xref="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.3.m3.2.2.1.2" xref="S3.SS1.p2.3.m3.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.2b"><apply id="S3.SS1.p2.3.m3.2.2.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1"><eq id="S3.SS1.p2.3.m3.2.2.1.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.3"></eq><ci id="S3.SS1.p2.3.m3.2.2.1.1.4.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.4">𝑌</ci><vector id="S3.SS1.p2.3.m3.2.2.1.1.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2"><apply id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.2">𝑦</ci><cn id="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.1.3">1</cn></apply><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">…</ci><apply id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.2.2.2.3">𝑇</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.2c">Y=(y_{1},...,y_{T}),</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.2d">italic_Y = ( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ,</annotation></semantics></math> until it reaches the end-of-sequence token. Decoder layers process the currently produced sequence with the self-attention module, followed by the cross-attention module and feed-forward network. Unlike in the encoder, the self-attention module in the decoder uses causal masking (the tokens can not attend to the future tokens). In Cross-attention, multi-head attention uses the decoded sequence as queries and the encoder output as keys and values. Residual connection and Layer Normalization are applied after each module.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pooling-based Shortening</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">Sequence Shortening is a method that results in a reduction in the number of tokens in a sequence by combining the tokens of the hidden representation of the input sequence <math alttext="H^{L}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">H</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">H^{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>. In the pooling-based shortening the sequence (of size <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_M</annotation></semantics></math>) is divided into non-overlapping groups of <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_K</annotation></semantics></math> neighboring tokens each (<math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_K</annotation></semantics></math> is a hyper-parameter). Pooling of the tokens in each group is then performed:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\tilde{G}=\operatorname*{Pooling}(H^{L})," class="ltx_Math" display="inline" id="S3.E1X.2.1.1.m1.2"><semantics id="S3.E1X.2.1.1.m1.2a"><mrow id="S3.E1X.2.1.1.m1.2.2.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml"><mrow id="S3.E1X.2.1.1.m1.2.2.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E1X.2.1.1.m1.2.2.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.3.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.2.cmml">G</mi><mo id="S3.E1X.2.1.1.m1.2.2.1.1.3.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.1.cmml">~</mo></mover><mo id="S3.E1X.2.1.1.m1.2.2.1.1.2" rspace="0.1389em" xref="S3.E1X.2.1.1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.1.1" lspace="0.1389em" rspace="0em" xref="S3.E1X.2.1.1.m1.1.1.cmml">Pooling</mo><mrow id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml">(</mo><msup id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">H</mi><mi id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3.cmml">L</mi></msup><mo id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.2.2.1.2" xref="S3.E1X.2.1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.2b"><apply id="S3.E1X.2.1.1.m1.2.2.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1"><eq id="S3.E1X.2.1.1.m1.2.2.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.2"></eq><apply id="S3.E1X.2.1.1.m1.2.2.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3"><ci id="S3.E1X.2.1.1.m1.2.2.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.1">~</ci><ci id="S3.E1X.2.1.1.m1.2.2.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.3.2">𝐺</ci></apply><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1"><ci id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">Pooling</ci><apply id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.2">𝐻</ci><ci id="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.2.2.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.2c">\displaystyle\tilde{G}=\operatorname*{Pooling}(H^{L}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.2.1.1.m1.2d">over~ start_ARG italic_G end_ARG = roman_Pooling ( italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.7">where <math alttext="\tilde{G}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m1.1"><semantics id="S3.SS2.p1.5.m1.1a"><mover accent="true" id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml"><mi id="S3.SS2.p1.5.m1.1.1.2" xref="S3.SS2.p1.5.m1.1.1.2.cmml">G</mi><mo id="S3.SS2.p1.5.m1.1.1.1" xref="S3.SS2.p1.5.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><apply id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1"><ci id="S3.SS2.p1.5.m1.1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1.1">~</ci><ci id="S3.SS2.p1.5.m1.1.1.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">\tilde{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m1.1d">over~ start_ARG italic_G end_ARG</annotation></semantics></math> is the sequence of size <math alttext="\lceil M/K\rceil" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m2.1"><semantics id="S3.SS2.p1.6.m2.1a"><mrow id="S3.SS2.p1.6.m2.1.1.1" xref="S3.SS2.p1.6.m2.1.1.2.cmml"><mo id="S3.SS2.p1.6.m2.1.1.1.2" stretchy="false" xref="S3.SS2.p1.6.m2.1.1.2.1.cmml">⌈</mo><mrow id="S3.SS2.p1.6.m2.1.1.1.1" xref="S3.SS2.p1.6.m2.1.1.1.1.cmml"><mi id="S3.SS2.p1.6.m2.1.1.1.1.2" xref="S3.SS2.p1.6.m2.1.1.1.1.2.cmml">M</mi><mo id="S3.SS2.p1.6.m2.1.1.1.1.1" xref="S3.SS2.p1.6.m2.1.1.1.1.1.cmml">/</mo><mi id="S3.SS2.p1.6.m2.1.1.1.1.3" xref="S3.SS2.p1.6.m2.1.1.1.1.3.cmml">K</mi></mrow><mo id="S3.SS2.p1.6.m2.1.1.1.3" stretchy="false" xref="S3.SS2.p1.6.m2.1.1.2.1.cmml">⌉</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.1b"><apply id="S3.SS2.p1.6.m2.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.1"><ceiling id="S3.SS2.p1.6.m2.1.1.2.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1.2"></ceiling><apply id="S3.SS2.p1.6.m2.1.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1"><divide id="S3.SS2.p1.6.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1.1"></divide><ci id="S3.SS2.p1.6.m2.1.1.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1.2">𝑀</ci><ci id="S3.SS2.p1.6.m2.1.1.1.1.3.cmml" xref="S3.SS2.p1.6.m2.1.1.1.1.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.1c">\lceil M/K\rceil</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m2.1d">⌈ italic_M / italic_K ⌉</annotation></semantics></math> of the pooled tokens. Subsequently, the pooled tokens <math alttext="\tilde{G}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m3.1"><semantics id="S3.SS2.p1.7.m3.1a"><mover accent="true" id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml"><mi id="S3.SS2.p1.7.m3.1.1.2" xref="S3.SS2.p1.7.m3.1.1.2.cmml">G</mi><mo id="S3.SS2.p1.7.m3.1.1.1" xref="S3.SS2.p1.7.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><apply id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1"><ci id="S3.SS2.p1.7.m3.1.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1.1">~</ci><ci id="S3.SS2.p1.7.m3.1.1.2.cmml" xref="S3.SS2.p1.7.m3.1.1.2">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">\tilde{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m3.1d">over~ start_ARG italic_G end_ARG</annotation></semantics></math> attend to the hidden representation of the original sequence using the attention module followed by the residual connection and the Layer Normalization:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle G=\operatorname*{LayerNorm}(\tilde{G}+\operatorname*{Attn}(%
\tilde{G},H^{L},H^{L}))," class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1.4"><semantics id="S3.E2X.2.1.1.m1.4a"><mrow id="S3.E2X.2.1.1.m1.4.4.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.4.4.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.cmml">G</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.2" rspace="0.1389em" xref="S3.E2X.2.1.1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml"><mo id="S3.E2X.2.1.1.m1.3.3" lspace="0.1389em" rspace="0em" xref="S3.E2X.2.1.1.m1.3.3.cmml">LayerNorm</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.2.cmml">G</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.1.cmml">~</mo></mover><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E2X.2.1.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.E2X.2.1.1.m1.1.1.cmml">Attn</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml">(</mo><mover accent="true" id="S3.E2X.2.1.1.m1.2.2" xref="S3.E2X.2.1.1.m1.2.2.cmml"><mi id="S3.E2X.2.1.1.m1.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.cmml">G</mi><mo id="S3.E2X.2.1.1.m1.2.2.1" xref="S3.E2X.2.1.1.m1.2.2.1.cmml">~</mo></mover><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.4" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">H</mi><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">L</mi></msup><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.5" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml">H</mi><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml">L</mi></msup><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.6" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.3" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.4b"><apply id="S3.E2X.2.1.1.m1.4.4.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1"><eq id="S3.E2X.2.1.1.m1.4.4.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.2"></eq><ci id="S3.E2X.2.1.1.m1.4.4.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.3">𝐺</ci><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1"><ci id="S3.E2X.2.1.1.m1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3">LayerNorm</ci><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1"><plus id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.3"></plus><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4"><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.1">~</ci><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.4.2">𝐺</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2"><ci id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">Attn</ci><apply id="S3.E2X.2.1.1.m1.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2"><ci id="S3.E2X.2.1.1.m1.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.2.2.1">~</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2">𝐺</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">𝐻</ci><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">𝐿</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2">𝐻</ci><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.4c">\displaystyle G=\operatorname*{LayerNorm}(\tilde{G}+\operatorname*{Attn}(%
\tilde{G},H^{L},H^{L})),</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.2.1.1.m1.4d">italic_G = roman_LayerNorm ( over~ start_ARG italic_G end_ARG + roman_Attn ( over~ start_ARG italic_G end_ARG , italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.8">where <math alttext="G" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m1.1"><semantics id="S3.SS2.p1.8.m1.1a"><mi id="S3.SS2.p1.8.m1.1.1" xref="S3.SS2.p1.8.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m1.1b"><ci id="S3.SS2.p1.8.m1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m1.1d">italic_G</annotation></semantics></math> is the final shortened sequence. Commonly used pooling operations are average <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>)</cite> and linear pooling <cite class="ltx_cite ltx_citemacro_citep">(Nawrot et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite> (learned linear transformation of the concatenated tokens).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Latent Grouping</h3>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="492" id="S4.F1.g1" src="extracted/5385333/Figures/Shortening_Grouping.png" width="341"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of Latent Grouping shortening with the number of groups set to three.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.4">In contrast to pooling, Latent Grouping, illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.F1" title="Figure 1 ‣ 4.1 Latent Grouping ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, results in a fixed number of tokens in the shortened sequence corresponding to the number of groups <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_K</annotation></semantics></math>, which is a hyper-parameter. Each token is categorized into a group by the feed-forward network with the number of outputs equal to the number of groups. We obtain the categorization for the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_i</annotation></semantics></math>-th token to <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_k</annotation></semantics></math>-th group <math alttext="c_{i,k}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.2"><semantics id="S4.SS1.p1.4.m4.2a"><msub id="S4.SS1.p1.4.m4.2.3" xref="S4.SS1.p1.4.m4.2.3.cmml"><mi id="S4.SS1.p1.4.m4.2.3.2" xref="S4.SS1.p1.4.m4.2.3.2.cmml">c</mi><mrow id="S4.SS1.p1.4.m4.2.2.2.4" xref="S4.SS1.p1.4.m4.2.2.2.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.1.cmml">i</mi><mo id="S4.SS1.p1.4.m4.2.2.2.4.1" xref="S4.SS1.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S4.SS1.p1.4.m4.2.2.2.2" xref="S4.SS1.p1.4.m4.2.2.2.2.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.2b"><apply id="S4.SS1.p1.4.m4.2.3.cmml" xref="S4.SS1.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.2.3.1.cmml" xref="S4.SS1.p1.4.m4.2.3">subscript</csymbol><ci id="S4.SS1.p1.4.m4.2.3.2.cmml" xref="S4.SS1.p1.4.m4.2.3.2">𝑐</ci><list id="S4.SS1.p1.4.m4.2.2.2.3.cmml" xref="S4.SS1.p1.4.m4.2.2.2.4"><ci id="S4.SS1.p1.4.m4.1.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1">𝑖</ci><ci id="S4.SS1.p1.4.m4.2.2.2.2.cmml" xref="S4.SS1.p1.4.m4.2.2.2.2">𝑘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.2c">c_{i,k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.2d">italic_c start_POSTSUBSCRIPT italic_i , italic_k end_POSTSUBSCRIPT</annotation></semantics></math> by applying the Softmax function to the outputs in the dimension of the groups:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbf{c}_{i}=\operatorname*{Softmax}(\operatorname*{FFN}(%
\mathbf{h}^{L}_{i}))," class="ltx_Math" display="inline" id="S4.E3X.2.1.1.m1.3"><semantics id="S4.E3X.2.1.1.m1.3a"><mrow id="S4.E3X.2.1.1.m1.3.3.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S4.E3X.2.1.1.m1.3.3.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml"><msub id="S4.E3X.2.1.1.m1.3.3.1.1.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.3.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.2.cmml">𝐜</mi><mi id="S4.E3X.2.1.1.m1.3.3.1.1.3.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.3.cmml">i</mi></msub><mo id="S4.E3X.2.1.1.m1.3.3.1.1.2" rspace="0.1389em" xref="S4.E3X.2.1.1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml"><mo id="S4.E3X.2.1.1.m1.2.2" lspace="0.1389em" rspace="0em" xref="S4.E3X.2.1.1.m1.2.2.cmml">Softmax</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml"><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml">(</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml"><mo id="S4.E3X.2.1.1.m1.1.1" lspace="0em" rspace="0em" xref="S4.E3X.2.1.1.m1.1.1.cmml">FFN</mo><mrow id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml"><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">(</mo><msubsup id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi><mi id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml">L</mi></msubsup><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3X.2.1.1.m1.3.3.1.2" xref="S4.E3X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3X.2.1.1.m1.3b"><apply id="S4.E3X.2.1.1.m1.3.3.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1"><eq id="S4.E3X.2.1.1.m1.3.3.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.2"></eq><apply id="S4.E3X.2.1.1.m1.3.3.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3">subscript</csymbol><ci id="S4.E3X.2.1.1.m1.3.3.1.1.3.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.2">𝐜</ci><ci id="S4.E3X.2.1.1.m1.3.3.1.1.3.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.3.3">𝑖</ci></apply><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1"><ci id="S4.E3X.2.1.1.m1.2.2.cmml" xref="S4.E3X.2.1.1.m1.2.2">Softmax</ci><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1"><ci id="S4.E3X.2.1.1.m1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1">FFN</ci><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">𝐡</ci><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.2.3">𝐿</ci></apply><ci id="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3X.2.1.1.m1.3c">\displaystyle\mathbf{c}_{i}=\operatorname*{Softmax}(\operatorname*{FFN}(%
\mathbf{h}^{L}_{i})),</annotation><annotation encoding="application/x-llamapun" id="S4.E3X.2.1.1.m1.3d">bold_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_Softmax ( roman_FFN ( bold_h start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\forall i=1,...,M," class="ltx_Math" display="inline" id="S4.E3Xa.2.1.1.m1.4"><semantics id="S4.E3Xa.2.1.1.m1.4a"><mrow id="S4.E3Xa.2.1.1.m1.4.4.1" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E3Xa.2.1.1.m1.4.4.1.1" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E3Xa.2.1.1.m1.4.4.1.1.2" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2.cmml"><mo id="S4.E3Xa.2.1.1.m1.4.4.1.1.2.1" rspace="0.167em" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2.1.cmml">∀</mo><mi id="S4.E3Xa.2.1.1.m1.4.4.1.1.2.2" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2.2.cmml">i</mi></mrow><mo id="S4.E3Xa.2.1.1.m1.4.4.1.1.1" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E3Xa.2.1.1.m1.4.4.1.1.3.2" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.3.1.cmml"><mn id="S4.E3Xa.2.1.1.m1.1.1" xref="S4.E3Xa.2.1.1.m1.1.1.cmml">1</mn><mo id="S4.E3Xa.2.1.1.m1.4.4.1.1.3.2.1" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E3Xa.2.1.1.m1.2.2" mathvariant="normal" xref="S4.E3Xa.2.1.1.m1.2.2.cmml">…</mi><mo id="S4.E3Xa.2.1.1.m1.4.4.1.1.3.2.2" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E3Xa.2.1.1.m1.3.3" xref="S4.E3Xa.2.1.1.m1.3.3.cmml">M</mi></mrow></mrow><mo id="S4.E3Xa.2.1.1.m1.4.4.1.2" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3Xa.2.1.1.m1.4b"><apply id="S4.E3Xa.2.1.1.m1.4.4.1.1.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1"><eq id="S4.E3Xa.2.1.1.m1.4.4.1.1.1.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.1"></eq><apply id="S4.E3Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2"><csymbol cd="latexml" id="S4.E3Xa.2.1.1.m1.4.4.1.1.2.1.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2.1">for-all</csymbol><ci id="S4.E3Xa.2.1.1.m1.4.4.1.1.2.2.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.2.2">𝑖</ci></apply><list id="S4.E3Xa.2.1.1.m1.4.4.1.1.3.1.cmml" xref="S4.E3Xa.2.1.1.m1.4.4.1.1.3.2"><cn id="S4.E3Xa.2.1.1.m1.1.1.cmml" type="integer" xref="S4.E3Xa.2.1.1.m1.1.1">1</cn><ci id="S4.E3Xa.2.1.1.m1.2.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2">…</ci><ci id="S4.E3Xa.2.1.1.m1.3.3.cmml" xref="S4.E3Xa.2.1.1.m1.3.3">𝑀</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3Xa.2.1.1.m1.4c">\displaystyle\forall i=1,...,M,</annotation><annotation encoding="application/x-llamapun" id="S4.E3Xa.2.1.1.m1.4d">∀ italic_i = 1 , … , italic_M ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.11">where <math alttext="\mathbf{h}^{L}" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m1.1"><semantics id="S4.SS1.p1.5.m1.1a"><msup id="S4.SS1.p1.5.m1.1.1" xref="S4.SS1.p1.5.m1.1.1.cmml"><mi id="S4.SS1.p1.5.m1.1.1.2" xref="S4.SS1.p1.5.m1.1.1.2.cmml">𝐡</mi><mi id="S4.SS1.p1.5.m1.1.1.3" xref="S4.SS1.p1.5.m1.1.1.3.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m1.1b"><apply id="S4.SS1.p1.5.m1.1.1.cmml" xref="S4.SS1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m1.1.1.1.cmml" xref="S4.SS1.p1.5.m1.1.1">superscript</csymbol><ci id="S4.SS1.p1.5.m1.1.1.2.cmml" xref="S4.SS1.p1.5.m1.1.1.2">𝐡</ci><ci id="S4.SS1.p1.5.m1.1.1.3.cmml" xref="S4.SS1.p1.5.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m1.1c">\mathbf{h}^{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m1.1d">bold_h start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> is the hidden representation of the last encoder layer and <math alttext="\mathbf{c}_{i}" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m2.1"><semantics id="S4.SS1.p1.6.m2.1a"><msub id="S4.SS1.p1.6.m2.1.1" xref="S4.SS1.p1.6.m2.1.1.cmml"><mi id="S4.SS1.p1.6.m2.1.1.2" xref="S4.SS1.p1.6.m2.1.1.2.cmml">𝐜</mi><mi id="S4.SS1.p1.6.m2.1.1.3" xref="S4.SS1.p1.6.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m2.1b"><apply id="S4.SS1.p1.6.m2.1.1.cmml" xref="S4.SS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m2.1.1.1.cmml" xref="S4.SS1.p1.6.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.6.m2.1.1.2.cmml" xref="S4.SS1.p1.6.m2.1.1.2">𝐜</ci><ci id="S4.SS1.p1.6.m2.1.1.3.cmml" xref="S4.SS1.p1.6.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m2.1c">\mathbf{c}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m2.1d">bold_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the vector of size <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p1.7.m3.1"><semantics id="S4.SS1.p1.7.m3.1a"><mi id="S4.SS1.p1.7.m3.1.1" xref="S4.SS1.p1.7.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m3.1b"><ci id="S4.SS1.p1.7.m3.1.1.cmml" xref="S4.SS1.p1.7.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.7.m3.1d">italic_K</annotation></semantics></math> representing the categorizations of the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p1.8.m4.1"><semantics id="S4.SS1.p1.8.m4.1a"><mi id="S4.SS1.p1.8.m4.1.1" xref="S4.SS1.p1.8.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m4.1b"><ci id="S4.SS1.p1.8.m4.1.1.cmml" xref="S4.SS1.p1.8.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.8.m4.1d">italic_i</annotation></semantics></math>-th token to all the groups. As an alternative to Softmax, Sparsemax function <cite class="ltx_cite ltx_citemacro_citep">(Martins and Astudillo, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib33" title="">2016</a>)</cite> can also be used resulting in the categorizations of tokens that are more sparse, which means that a token is categorized into a smaller number of groups, and most categorizations are equal to zero. Subsequently, the groups <math alttext="\tilde{G}" class="ltx_Math" display="inline" id="S4.SS1.p1.9.m5.1"><semantics id="S4.SS1.p1.9.m5.1a"><mover accent="true" id="S4.SS1.p1.9.m5.1.1" xref="S4.SS1.p1.9.m5.1.1.cmml"><mi id="S4.SS1.p1.9.m5.1.1.2" xref="S4.SS1.p1.9.m5.1.1.2.cmml">G</mi><mo id="S4.SS1.p1.9.m5.1.1.1" xref="S4.SS1.p1.9.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m5.1b"><apply id="S4.SS1.p1.9.m5.1.1.cmml" xref="S4.SS1.p1.9.m5.1.1"><ci id="S4.SS1.p1.9.m5.1.1.1.cmml" xref="S4.SS1.p1.9.m5.1.1.1">~</ci><ci id="S4.SS1.p1.9.m5.1.1.2.cmml" xref="S4.SS1.p1.9.m5.1.1.2">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m5.1c">\tilde{G}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.9.m5.1d">over~ start_ARG italic_G end_ARG</annotation></semantics></math> are constructed as the sum of the hidden representations <math alttext="\mathbf{h}^{L}" class="ltx_Math" display="inline" id="S4.SS1.p1.10.m6.1"><semantics id="S4.SS1.p1.10.m6.1a"><msup id="S4.SS1.p1.10.m6.1.1" xref="S4.SS1.p1.10.m6.1.1.cmml"><mi id="S4.SS1.p1.10.m6.1.1.2" xref="S4.SS1.p1.10.m6.1.1.2.cmml">𝐡</mi><mi id="S4.SS1.p1.10.m6.1.1.3" xref="S4.SS1.p1.10.m6.1.1.3.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m6.1b"><apply id="S4.SS1.p1.10.m6.1.1.cmml" xref="S4.SS1.p1.10.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.10.m6.1.1.1.cmml" xref="S4.SS1.p1.10.m6.1.1">superscript</csymbol><ci id="S4.SS1.p1.10.m6.1.1.2.cmml" xref="S4.SS1.p1.10.m6.1.1.2">𝐡</ci><ci id="S4.SS1.p1.10.m6.1.1.3.cmml" xref="S4.SS1.p1.10.m6.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m6.1c">\mathbf{h}^{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.10.m6.1d">bold_h start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> with categorizations <math alttext="c_{i,k}" class="ltx_Math" display="inline" id="S4.SS1.p1.11.m7.2"><semantics id="S4.SS1.p1.11.m7.2a"><msub id="S4.SS1.p1.11.m7.2.3" xref="S4.SS1.p1.11.m7.2.3.cmml"><mi id="S4.SS1.p1.11.m7.2.3.2" xref="S4.SS1.p1.11.m7.2.3.2.cmml">c</mi><mrow id="S4.SS1.p1.11.m7.2.2.2.4" xref="S4.SS1.p1.11.m7.2.2.2.3.cmml"><mi id="S4.SS1.p1.11.m7.1.1.1.1" xref="S4.SS1.p1.11.m7.1.1.1.1.cmml">i</mi><mo id="S4.SS1.p1.11.m7.2.2.2.4.1" xref="S4.SS1.p1.11.m7.2.2.2.3.cmml">,</mo><mi id="S4.SS1.p1.11.m7.2.2.2.2" xref="S4.SS1.p1.11.m7.2.2.2.2.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.11.m7.2b"><apply id="S4.SS1.p1.11.m7.2.3.cmml" xref="S4.SS1.p1.11.m7.2.3"><csymbol cd="ambiguous" id="S4.SS1.p1.11.m7.2.3.1.cmml" xref="S4.SS1.p1.11.m7.2.3">subscript</csymbol><ci id="S4.SS1.p1.11.m7.2.3.2.cmml" xref="S4.SS1.p1.11.m7.2.3.2">𝑐</ci><list id="S4.SS1.p1.11.m7.2.2.2.3.cmml" xref="S4.SS1.p1.11.m7.2.2.2.4"><ci id="S4.SS1.p1.11.m7.1.1.1.1.cmml" xref="S4.SS1.p1.11.m7.1.1.1.1">𝑖</ci><ci id="S4.SS1.p1.11.m7.2.2.2.2.cmml" xref="S4.SS1.p1.11.m7.2.2.2.2">𝑘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.11.m7.2c">c_{i,k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.11.m7.2d">italic_c start_POSTSUBSCRIPT italic_i , italic_k end_POSTSUBSCRIPT</annotation></semantics></math> used as weights:
</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\tilde{\mathbf{g}}_{k}=\sum_{i}c_{i,k}\mathbf{h}^{L}_{i}," class="ltx_Math" display="inline" id="S4.E4X.2.1.1.m1.3"><semantics id="S4.E4X.2.1.1.m1.3a"><mrow id="S4.E4X.2.1.1.m1.3.3.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S4.E4X.2.1.1.m1.3.3.1.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.cmml"><msub id="S4.E4X.2.1.1.m1.3.3.1.1.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.cmml"><mover accent="true" id="S4.E4X.2.1.1.m1.3.3.1.1.2.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2.cmml"><mi id="S4.E4X.2.1.1.m1.3.3.1.1.2.2.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2.2.cmml">𝐠</mi><mo id="S4.E4X.2.1.1.m1.3.3.1.1.2.2.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2.1.cmml">~</mo></mover><mi id="S4.E4X.2.1.1.m1.3.3.1.1.2.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.3.cmml">k</mi></msub><mo id="S4.E4X.2.1.1.m1.3.3.1.1.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S4.E4X.2.1.1.m1.3.3.1.1.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.cmml"><mstyle displaystyle="true" id="S4.E4X.2.1.1.m1.3.3.1.1.3.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.cmml"><munder id="S4.E4X.2.1.1.m1.3.3.1.1.3.1a" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.cmml"><mo id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.2" movablelimits="false" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.2.cmml">∑</mo><mi id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.3.cmml">i</mi></munder></mstyle><mrow id="S4.E4X.2.1.1.m1.3.3.1.1.3.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.cmml"><msub id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.cmml"><mi id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.2.cmml">c</mi><mrow id="S4.E4X.2.1.1.m1.2.2.2.4" xref="S4.E4X.2.1.1.m1.2.2.2.3.cmml"><mi id="S4.E4X.2.1.1.m1.1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.cmml">i</mi><mo id="S4.E4X.2.1.1.m1.2.2.2.4.1" xref="S4.E4X.2.1.1.m1.2.2.2.3.cmml">,</mo><mi id="S4.E4X.2.1.1.m1.2.2.2.2" xref="S4.E4X.2.1.1.m1.2.2.2.2.cmml">k</mi></mrow></msub><mo id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.1" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.1.cmml">⁢</mo><msubsup id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.cmml"><mi id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.2.cmml">𝐡</mi><mi id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.3.cmml">i</mi><mi id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.3" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.3.cmml">L</mi></msubsup></mrow></mrow></mrow><mo id="S4.E4X.2.1.1.m1.3.3.1.2" xref="S4.E4X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4X.2.1.1.m1.3b"><apply id="S4.E4X.2.1.1.m1.3.3.1.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1"><eq id="S4.E4X.2.1.1.m1.3.3.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.1"></eq><apply id="S4.E4X.2.1.1.m1.3.3.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.3.3.1.1.2.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2">subscript</csymbol><apply id="S4.E4X.2.1.1.m1.3.3.1.1.2.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2"><ci id="S4.E4X.2.1.1.m1.3.3.1.1.2.2.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2.1">~</ci><ci id="S4.E4X.2.1.1.m1.3.3.1.1.2.2.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.2.2">𝐠</ci></apply><ci id="S4.E4X.2.1.1.m1.3.3.1.1.2.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.2.3">𝑘</ci></apply><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3"><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.2"></sum><ci id="S4.E4X.2.1.1.m1.3.3.1.1.3.1.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.1.3">𝑖</ci></apply><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2"><times id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.1"></times><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2">subscript</csymbol><ci id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.2.2">𝑐</ci><list id="S4.E4X.2.1.1.m1.2.2.2.3.cmml" xref="S4.E4X.2.1.1.m1.2.2.2.4"><ci id="S4.E4X.2.1.1.m1.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1">𝑖</ci><ci id="S4.E4X.2.1.1.m1.2.2.2.2.cmml" xref="S4.E4X.2.1.1.m1.2.2.2.2">𝑘</ci></list></apply><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3">subscript</csymbol><apply id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.1.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3">superscript</csymbol><ci id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.2.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.2">𝐡</ci><ci id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.2.3">𝐿</ci></apply><ci id="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.3.cmml" xref="S4.E4X.2.1.1.m1.3.3.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.2.1.1.m1.3c">\displaystyle\tilde{\mathbf{g}}_{k}=\sum_{i}c_{i,k}\mathbf{h}^{L}_{i},</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.2.1.1.m1.3d">over~ start_ARG bold_g end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i , italic_k end_POSTSUBSCRIPT bold_h start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\forall k=1,...,K," class="ltx_Math" display="inline" id="S4.E4Xa.2.1.1.m1.4"><semantics id="S4.E4Xa.2.1.1.m1.4a"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml"><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml"><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.1" rspace="0.167em" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.1.cmml">∀</mo><mi id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.cmml">k</mi></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.1.cmml"><mn id="S4.E4Xa.2.1.1.m1.1.1" xref="S4.E4Xa.2.1.1.m1.1.1.cmml">1</mn><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.2.1" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E4Xa.2.1.1.m1.2.2" mathvariant="normal" xref="S4.E4Xa.2.1.1.m1.2.2.cmml">…</mi><mo id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.2.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.1.cmml">,</mo><mi id="S4.E4Xa.2.1.1.m1.3.3" xref="S4.E4Xa.2.1.1.m1.3.3.cmml">K</mi></mrow></mrow><mo id="S4.E4Xa.2.1.1.m1.4.4.1.2" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4Xa.2.1.1.m1.4b"><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1"><eq id="S4.E4Xa.2.1.1.m1.4.4.1.1.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.1"></eq><apply id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2"><csymbol cd="latexml" id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.1">for-all</csymbol><ci id="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.2.2">𝑘</ci></apply><list id="S4.E4Xa.2.1.1.m1.4.4.1.1.3.1.cmml" xref="S4.E4Xa.2.1.1.m1.4.4.1.1.3.2"><cn id="S4.E4Xa.2.1.1.m1.1.1.cmml" type="integer" xref="S4.E4Xa.2.1.1.m1.1.1">1</cn><ci id="S4.E4Xa.2.1.1.m1.2.2.cmml" xref="S4.E4Xa.2.1.1.m1.2.2">…</ci><ci id="S4.E4Xa.2.1.1.m1.3.3.cmml" xref="S4.E4Xa.2.1.1.m1.3.3">𝐾</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4Xa.2.1.1.m1.4c">\displaystyle\forall k=1,...,K,</annotation><annotation encoding="application/x-llamapun" id="S4.E4Xa.2.1.1.m1.4d">∀ italic_k = 1 , … , italic_K ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.14">where <math alttext="\tilde{\mathbf{g}}_{k}" class="ltx_Math" display="inline" id="S4.SS1.p1.12.m1.1"><semantics id="S4.SS1.p1.12.m1.1a"><msub id="S4.SS1.p1.12.m1.1.1" xref="S4.SS1.p1.12.m1.1.1.cmml"><mover accent="true" id="S4.SS1.p1.12.m1.1.1.2" xref="S4.SS1.p1.12.m1.1.1.2.cmml"><mi id="S4.SS1.p1.12.m1.1.1.2.2" xref="S4.SS1.p1.12.m1.1.1.2.2.cmml">𝐠</mi><mo id="S4.SS1.p1.12.m1.1.1.2.1" xref="S4.SS1.p1.12.m1.1.1.2.1.cmml">~</mo></mover><mi id="S4.SS1.p1.12.m1.1.1.3" xref="S4.SS1.p1.12.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.12.m1.1b"><apply id="S4.SS1.p1.12.m1.1.1.cmml" xref="S4.SS1.p1.12.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m1.1.1.1.cmml" xref="S4.SS1.p1.12.m1.1.1">subscript</csymbol><apply id="S4.SS1.p1.12.m1.1.1.2.cmml" xref="S4.SS1.p1.12.m1.1.1.2"><ci id="S4.SS1.p1.12.m1.1.1.2.1.cmml" xref="S4.SS1.p1.12.m1.1.1.2.1">~</ci><ci id="S4.SS1.p1.12.m1.1.1.2.2.cmml" xref="S4.SS1.p1.12.m1.1.1.2.2">𝐠</ci></apply><ci id="S4.SS1.p1.12.m1.1.1.3.cmml" xref="S4.SS1.p1.12.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.12.m1.1c">\tilde{\mathbf{g}}_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.12.m1.1d">over~ start_ARG bold_g end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is a <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p1.13.m2.1"><semantics id="S4.SS1.p1.13.m2.1a"><mi id="S4.SS1.p1.13.m2.1.1" xref="S4.SS1.p1.13.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.13.m2.1b"><ci id="S4.SS1.p1.13.m2.1.1.cmml" xref="S4.SS1.p1.13.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.13.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.13.m2.1d">italic_k</annotation></semantics></math>-th grouped token composing the sequence <math alttext="\tilde{G}" class="ltx_Math" display="inline" id="S4.SS1.p1.14.m3.1"><semantics id="S4.SS1.p1.14.m3.1a"><mover accent="true" id="S4.SS1.p1.14.m3.1.1" xref="S4.SS1.p1.14.m3.1.1.cmml"><mi id="S4.SS1.p1.14.m3.1.1.2" xref="S4.SS1.p1.14.m3.1.1.2.cmml">G</mi><mo id="S4.SS1.p1.14.m3.1.1.1" xref="S4.SS1.p1.14.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.14.m3.1b"><apply id="S4.SS1.p1.14.m3.1.1.cmml" xref="S4.SS1.p1.14.m3.1.1"><ci id="S4.SS1.p1.14.m3.1.1.1.cmml" xref="S4.SS1.p1.14.m3.1.1.1">~</ci><ci id="S4.SS1.p1.14.m3.1.1.2.cmml" xref="S4.SS1.p1.14.m3.1.1.2">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.14.m3.1c">\tilde{G}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.14.m3.1d">over~ start_ARG italic_G end_ARG</annotation></semantics></math> in the equation (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S3.E1" title="1 ‣ 3.2 Pooling-based Shortening ‣ 3 Background ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>). The network learns how to soft-assign each token to the groups. A group representation is computed using the weighted average of tokens, which makes backpropagation into the categorizing network possible. Finally, the attention module is applied as in equation (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S3.E2" title="2 ‣ 3.2 Pooling-based Shortening ‣ 3 Background ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S4.F2.g1" src="extracted/5385333/Figures/Shortening_Architecture.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The illustration of a Shortening Architecture with the representation of the two previous sentences being cached. The dashed line represents the optional blocking of the gradient during training.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Latent Selecting</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Latent Selecting differs from Latent Grouping by enabling the groups to select tokens to aggregate rather than assigning each token to a group and allowing the model to ignore tokens entirely rather than assigning them to at least one group. This is similar to selecting the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">hub</span> tokens in Power-BERT <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib18" title="">2020</a>)</cite>, where the selection is based on the attention scores of the previous layer. Although Latent Selecting can be achieved by maintaining a categorizing feed-forward network for each group, we utilize the same network as described for Latent Grouping but apply the Softmax (or Sparsemax) function in equation (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.E3" title="3 ‣ 4.1 Latent Grouping ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>) in the sequence dimension instead of the token dimension.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Context Shortening</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The architecture we use, illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.F2" title="Figure 2 ‣ 4.1 Latent Grouping ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, is based on caching the hidden representations produced by the encoder, where the representations of the tokens of the current sentence are stored and can be reused as context when the subsequent sentences are translated. Although this architecture uses only a single encoder, it is different from the single-encoder models because the current sentence and the context sentences are processed separately. While in the standard caching architecture the hidden representation of all the tokens is stored, we introduce a Sequence Shortening module directly after the encoder, which returns the compressed hidden representation usually containing fewer tokens than the original sequence. We consider: mean pooling <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>)</cite>, max pooling, linear pooling <cite class="ltx_cite ltx_citemacro_cite">Nawrot et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite>, Latent Grouping, and Latent Selecting. Additionally, we also test the simple aggregation of the whole context sequences into a single vector by averaging the tokens. Conceptually, Sequence Shortening of the context can be seen as a middle-ground between storing tokens and sentence aggregations.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.5">The integration of the context with the decoder can also be done in several ways. Firstly, the context sentences can be concatenated to the current sentence. This method is similar to the single-encoder (concatenating) architecture, where the difference is that the encoder does not have access to other sentences in the case of caching architecture. In this case, the decoder layers are the same as in the vanilla transformer with the self- and cross-attention modules. Secondly, the context sentences can be processed in the decoder layers by a separate context-attention module, where the decoder tokens attend to the context tokens. We experiment with the parallel and serial alignment of the cross- and context-attention modules. Additionally, we also experiment with gating the representation resulting from applying context-attention using the following equation:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\lambda_{i}=\sigma(\operatorname*{FFN}(\mathbf{\hat{h}}_{i}))," class="ltx_Math" display="inline" id="S4.E5X.2.1.1.m1.2"><semantics id="S4.E5X.2.1.1.m1.2a"><mrow id="S4.E5X.2.1.1.m1.2.2.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.cmml"><mrow id="S4.E5X.2.1.1.m1.2.2.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.cmml"><msub id="S4.E5X.2.1.1.m1.2.2.1.1.3" xref="S4.E5X.2.1.1.m1.2.2.1.1.3.cmml"><mi id="S4.E5X.2.1.1.m1.2.2.1.1.3.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.3.2.cmml">λ</mi><mi id="S4.E5X.2.1.1.m1.2.2.1.1.3.3" xref="S4.E5X.2.1.1.m1.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S4.E5X.2.1.1.m1.2.2.1.1.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E5X.2.1.1.m1.2.2.1.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.cmml"><mi id="S4.E5X.2.1.1.m1.2.2.1.1.1.3" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.3.cmml">σ</mi><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.cmml"><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S4.E5X.2.1.1.m1.1.1" lspace="0em" rspace="0em" xref="S4.E5X.2.1.1.m1.1.1.cmml">FFN</mo><mrow id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><msub id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E5X.2.1.1.m1.2.2.1.2" xref="S4.E5X.2.1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5X.2.1.1.m1.2b"><apply id="S4.E5X.2.1.1.m1.2.2.1.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1"><eq id="S4.E5X.2.1.1.m1.2.2.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.2"></eq><apply id="S4.E5X.2.1.1.m1.2.2.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.2.2.1.1.3.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.3">subscript</csymbol><ci id="S4.E5X.2.1.1.m1.2.2.1.1.3.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.3.2">𝜆</ci><ci id="S4.E5X.2.1.1.m1.2.2.1.1.3.3.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.3.3">𝑖</ci></apply><apply id="S4.E5X.2.1.1.m1.2.2.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1"><times id="S4.E5X.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.2"></times><ci id="S4.E5X.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.3">𝜎</ci><apply id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1"><ci id="S4.E5X.2.1.1.m1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1">FFN</ci><apply id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">𝐡</ci></apply><ci id="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.2.1.1.m1.2c">\displaystyle\lambda_{i}=\sigma(\operatorname*{FFN}(\mathbf{\hat{h}}_{i})),</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.2.1.1.m1.2d">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_σ ( roman_FFN ( over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(5)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbf{\hat{h}}^{\prime}_{i}=\lambda_{i}\mathbf{\hat{h}}_{i}," class="ltx_Math" display="inline" id="S4.E5Xa.2.1.1.m1.1"><semantics id="S4.E5Xa.2.1.1.m1.1a"><mrow id="S4.E5Xa.2.1.1.m1.1.1.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E5Xa.2.1.1.m1.1.1.1.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml"><msubsup id="S4.E5Xa.2.1.1.m1.1.1.1.1.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.cmml"><mover accent="true" id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.cmml"><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.2.cmml">𝐡</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.1.cmml">^</mo></mover><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.3.cmml">i</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.3.cmml">′</mo></msubsup><mo id="S4.E5Xa.2.1.1.m1.1.1.1.1.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E5Xa.2.1.1.m1.1.1.1.1.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.cmml"><msub id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.cmml"><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.2.cmml">λ</mi><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.1.cmml">⁢</mo><msub id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.cmml"><mover accent="true" id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.cmml"><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.2.cmml">𝐡</mi><mo id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.1" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.1.cmml">^</mo></mover><mi id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.3" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S4.E5Xa.2.1.1.m1.1.1.1.2" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5Xa.2.1.1.m1.1b"><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1"><eq id="S4.E5Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.1"></eq><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2">subscript</csymbol><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2"><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.1">^</ci><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.2.2">𝐡</ci></apply><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.2.3">′</ci></apply><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.2.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3"><times id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.1"></times><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.2">𝜆</ci><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2"><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.1">^</ci><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.2.2">𝐡</ci></apply><ci id="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E5Xa.2.1.1.m1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5Xa.2.1.1.m1.1c">\displaystyle\mathbf{\hat{h}}^{\prime}_{i}=\lambda_{i}\mathbf{\hat{h}}_{i},</annotation><annotation encoding="application/x-llamapun" id="S4.E5Xa.2.1.1.m1.1d">over^ start_ARG bold_h end_ARG start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\forall i=1,...,M" class="ltx_Math" display="inline" id="S4.E5Xb.2.1.1.m1.3"><semantics id="S4.E5Xb.2.1.1.m1.3a"><mrow id="S4.E5Xb.2.1.1.m1.3.4" xref="S4.E5Xb.2.1.1.m1.3.4.cmml"><mrow id="S4.E5Xb.2.1.1.m1.3.4.2" xref="S4.E5Xb.2.1.1.m1.3.4.2.cmml"><mo id="S4.E5Xb.2.1.1.m1.3.4.2.1" rspace="0.167em" xref="S4.E5Xb.2.1.1.m1.3.4.2.1.cmml">∀</mo><mi id="S4.E5Xb.2.1.1.m1.3.4.2.2" xref="S4.E5Xb.2.1.1.m1.3.4.2.2.cmml">i</mi></mrow><mo id="S4.E5Xb.2.1.1.m1.3.4.1" xref="S4.E5Xb.2.1.1.m1.3.4.1.cmml">=</mo><mrow id="S4.E5Xb.2.1.1.m1.3.4.3.2" xref="S4.E5Xb.2.1.1.m1.3.4.3.1.cmml"><mn id="S4.E5Xb.2.1.1.m1.1.1" xref="S4.E5Xb.2.1.1.m1.1.1.cmml">1</mn><mo id="S4.E5Xb.2.1.1.m1.3.4.3.2.1" xref="S4.E5Xb.2.1.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.E5Xb.2.1.1.m1.2.2" mathvariant="normal" xref="S4.E5Xb.2.1.1.m1.2.2.cmml">…</mi><mo id="S4.E5Xb.2.1.1.m1.3.4.3.2.2" xref="S4.E5Xb.2.1.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.E5Xb.2.1.1.m1.3.3" xref="S4.E5Xb.2.1.1.m1.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5Xb.2.1.1.m1.3b"><apply id="S4.E5Xb.2.1.1.m1.3.4.cmml" xref="S4.E5Xb.2.1.1.m1.3.4"><eq id="S4.E5Xb.2.1.1.m1.3.4.1.cmml" xref="S4.E5Xb.2.1.1.m1.3.4.1"></eq><apply id="S4.E5Xb.2.1.1.m1.3.4.2.cmml" xref="S4.E5Xb.2.1.1.m1.3.4.2"><csymbol cd="latexml" id="S4.E5Xb.2.1.1.m1.3.4.2.1.cmml" xref="S4.E5Xb.2.1.1.m1.3.4.2.1">for-all</csymbol><ci id="S4.E5Xb.2.1.1.m1.3.4.2.2.cmml" xref="S4.E5Xb.2.1.1.m1.3.4.2.2">𝑖</ci></apply><list id="S4.E5Xb.2.1.1.m1.3.4.3.1.cmml" xref="S4.E5Xb.2.1.1.m1.3.4.3.2"><cn id="S4.E5Xb.2.1.1.m1.1.1.cmml" type="integer" xref="S4.E5Xb.2.1.1.m1.1.1">1</cn><ci id="S4.E5Xb.2.1.1.m1.2.2.cmml" xref="S4.E5Xb.2.1.1.m1.2.2">…</ci><ci id="S4.E5Xb.2.1.1.m1.3.3.cmml" xref="S4.E5Xb.2.1.1.m1.3.3">𝑀</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5Xb.2.1.1.m1.3c">\displaystyle\forall i=1,...,M</annotation><annotation encoding="application/x-llamapun" id="S4.E5Xb.2.1.1.m1.3d">∀ italic_i = 1 , … , italic_M</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS3.p2.4">where <math alttext="\mathbf{\hat{h}}_{i}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mover accent="true" id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2.cmml">𝐡</mi><mo id="S4.SS3.p2.1.m1.1.1.2.1" xref="S4.SS3.p2.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><ci id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1.2.1">^</ci><ci id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">𝐡</ci></apply><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\mathbf{\hat{h}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">over^ start_ARG bold_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_i</annotation></semantics></math>-th token representation returned by the context-attention module, <math alttext="\operatorname*{FFN}" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mo id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">FFN</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">FFN</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\operatorname*{FFN}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">roman_FFN</annotation></semantics></math> is a token-wise linear layer with one output, <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mi id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><ci id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">italic_σ</annotation></semantics></math> is the Sigmoid function.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">For Sentence Aggregation and Shortening architectures, the aggregated or shortened representation of the current sentence can be included in context sentences. This helps with the training, as often none of the previous sentences has an effect on the translation, known as the two-fold sparsity problem <cite class="ltx_cite ltx_citemacro_citep">(Lupo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib30" title="">2022</a>)</cite>, and the context attention module can still be trained to attend to the representation of the current sentence. To allow the decoder to distinguish between context sentences we employ learned segment embeddings <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib14" title="">2019</a>)</cite>. Similarly, we also add learned positional encoding for the shortened tokens inside context sentences.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">During training, caching is not used, meaning that the model receives tokenized context sentences and processes them using the same encoder. This implies that the weights of the encoder receive the backpropagated error from multiple sources - the current sentence and each of the context sentences, which can lead to difficulties in training. Therefore, we consider blocking the gradient after the encoder and before shortening (where applicable) by allowing the gradient information to flow for a specified number of context sentences, after which, the gradient is blocked.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">All our experiments are implemented<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code for this paper (based on <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/neulab/contextual-mt" title="">https://github.com/neulab/contextual-mt</a>) can be found on Github <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Pawel-M/shortening-context-mt" title="">https://github.com/Pawel-M/shortening-context-mt</a>.</span></span></span> in <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">fairseq</span> framework <cite class="ltx_cite ltx_citemacro_citep">(Ott et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib42" title="">2019</a>)</cite>. We used the code repository of <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite> as the base for our implementation.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Data</h3>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Docs</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">Sent/Doc</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1">Tok/Sent</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.1.1">En-De Train</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.1.2">1698</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.2.1.3">121.4</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.2.1.4">21.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.3.2.1">En-De Valid</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S5.T1.1.3.2.2">62</th>
<td class="ltx_td ltx_align_right" id="S5.T1.1.3.2.3">87.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.3.2.4">20.6</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.3.1">En-De Test</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S5.T1.1.4.3.2">12</th>
<td class="ltx_td ltx_align_right" id="S5.T1.1.4.3.3">90.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.4.3.4">20.8</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.5.4.1">En-Fr Train</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.5.4.2">1914</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.5.4.3">121.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T1.1.5.4.4">22.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.6.5.1">En-Fr Valid</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S5.T1.1.6.5.2">66</th>
<td class="ltx_td ltx_align_right" id="S5.T1.1.6.5.3">88.2</td>
<td class="ltx_td ltx_align_right" id="S5.T1.1.6.5.4">20.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T1.1.7.6.1">En-Fr Test</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b" id="S5.T1.1.7.6.2">12</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T1.1.7.6.3">100.8</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T1.1.7.6.4">21.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The details of the IWSLT 2017 datasets.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">Accuracy</span></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.1.1.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.1.1.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.1.1.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.1.1.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.2.2.1">Sentence-level</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.2.2.2">28.11</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.2.2.3">43.67%</td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.2.2.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.2.2.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.2.2.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.1.2.2.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.3.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.3.3.2">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.3.3.2.1">Context: 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.3.3.3">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.3.3.3.1">Context: 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.3.3.4">
<span class="ltx_text ltx_font_bold" id="S5.T2.1.3.3.4.1">Context: 3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.1.1">Model</span></th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.3.1">Accuracy</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.4.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.5.1">Accuracy</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.6.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.4.4.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.4.7.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.5.5.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.2">28.31</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.3">47.42%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.4">27.95</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.5">48.18%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.6">27.88</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.5.5.7">48.88%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.6.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.6.2.1">28.67</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.3">44.93%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.4">28.50</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.5">46.65%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.6">28.26</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.6.6.7">45.00%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.7.7.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.2">28.35</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.3">54.06%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.4">28.50</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.5">54.13%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.7.6.1">29.08</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.7.7.7">51.23%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.8.8.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.2">28.38</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.3">45.72%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.4">26.73</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.5">45.26%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.6">26.70</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.8.8.7">44.91%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.9.9.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.2">27.62</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.3">51.67%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.4">27.88</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.9.5.1">55.08%</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.6">28.26</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.9.9.7">50.89%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.10.10.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.2">28.09</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.3">53.37%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.4">27.85</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.5">54.81%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.6">28.38</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.10.10.7">50.54%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.11.11.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.2">27.62</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.3">52.71%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.4">28.03</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.5">52.13%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.6">28.18</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.11.11.7">51.27%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.12.12.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.2">28.21</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.12.12.3.1">56.98%</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.12.12.4.1">28.70</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.5">54.51%</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.6">28.49</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.12.12.7">51.16%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T2.1.13.13.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.2">28.15</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.3">54.48%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.4">28.55</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.5">54.21%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.6">28.01</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T2.1.13.13.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.13.7.1">51.95%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of the <span class="ltx_text ltx_font_bold" id="S5.T2.3.1">En-De</span> IWSLT 2017 experiment. The models were trained to use only the source-side context. We report BLEU of the test subset and the accuracy of the ContraPro <cite class="ltx_cite ltx_citemacro_citep">(Müller et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>)</cite> contrastive dataset.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We used the English-to-German and English-to-French directions of the IWSLT 2017 <cite class="ltx_cite ltx_citemacro_citep">(Cettolo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib7" title="">2017</a>)</cite> document-level dataset that is based on the subtitles of the TED Talks<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ted.com/" title="">https://www.ted.com/</a></span></span></span>. Following <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite>, we used <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">tst2011</span>-<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">tst2014</span> as validation subset and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">tst2015</span> as the test subset. The data is byte-pair encoded <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib47" title="">2016</a>)</cite> using SentencePiece framework <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib25" title="">2018</a>)</cite> on the training subset with 20,000 vocabulary size for each language separately (see Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.T1" title="Table 1 ‣ 5.1 Data ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>). We measured BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib43" title="">2002</a>)</cite> using <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">sacreBleu</span> library <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib44" title="">2018</a>)</cite>. We also report COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A2" title="Appendix B COMET Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To measure the context usage of the trained models, we employed ContraPro <cite class="ltx_cite ltx_citemacro_citep">(Müller et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>)</cite> contrastive dataset for the English-to-German direction, and the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite> for the English-to-French direction. Both are based on the OpenSubtitles 2018 dataset <cite class="ltx_cite ltx_citemacro_citep">(Lison et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib28" title="">2018</a>)</cite>. These datasets consist of the source sentence with the context (previous sentences on the source and target side) with several translations differing only in a pronoun that requires context to be correctly translated. Models rank the translations by assigning probabilities to each of them. The translation is considered to be accurate when the right translation is ranked the highest by the model.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Models</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Based on the described methods, we trained the following caching models:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Caching Tokens</span> - where the encoder representations of the context sentences are stored directly,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Caching Sentence</span> - where the representations of the context sentences are averaged and stored,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Shortening - Avg Pooling</span> - Sequence shortening with mean pooling applied to the outputs of the encoder, based on <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib12" title="">2020</a>)</cite>,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Shortening - Max Pooling</span> - shortening with max pooling,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">Shortening - Linear Pooling</span> - shortening with linear pooling, based on <cite class="ltx_cite ltx_citemacro_citep">(Nawrot et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib41" title="">2022</a>)</cite>,</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i6.p1.1.1">Shortening - Grouping</span> - shortening with Latent Grouping (Section <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.SS1" title="4.1 Latent Grouping ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">4.1</span></a>),</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i7.p1">
<p class="ltx_p" id="S5.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i7.p1.1.1">Shortening - Selecting</span> - shortening with Latent Selecting (Section <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.SS2" title="4.2 Latent Selecting ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS2.p1.2">For all the aggregating models, the current sentence is also used as context and is concatenated with the context sentences after embedding.
Moreover, we also test the following baseline models:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">Sentence-level Transformer</span> - where context sentences are ignored,</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">Single-encoder Transformer</span> - where context sentences are prepended to the current sentence and processed by the encoder, we used <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite> implementation,</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">Multi-encoder Transformer</span> - with the separate encoder (without weights-sharing) used to encode the context sentences, again based on the <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite> implementation, where the context and the current sentence are concatenated in the decoder. Our experiments revealed that this integration yields better results than with the separate context-attention module.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.7">All tested models are based on the Transformer base architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib57" title="">2017</a>)</cite>. The hyper-parameters and model details can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A1" title="Appendix A Models and Training Details ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>. We tuned the hyper-parameters of the models based on the performance on the validation subset. From the K values of <math alttext="[2,3,4]" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.3"><semantics id="S5.SS2.p2.1.m1.3a"><mrow id="S5.SS2.p2.1.m1.3.4.2" xref="S5.SS2.p2.1.m1.3.4.1.cmml"><mo id="S5.SS2.p2.1.m1.3.4.2.1" stretchy="false" xref="S5.SS2.p2.1.m1.3.4.1.cmml">[</mo><mn id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">2</mn><mo id="S5.SS2.p2.1.m1.3.4.2.2" xref="S5.SS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS2.p2.1.m1.2.2" xref="S5.SS2.p2.1.m1.2.2.cmml">3</mn><mo id="S5.SS2.p2.1.m1.3.4.2.3" xref="S5.SS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS2.p2.1.m1.3.3" xref="S5.SS2.p2.1.m1.3.3.cmml">4</mn><mo id="S5.SS2.p2.1.m1.3.4.2.4" stretchy="false" xref="S5.SS2.p2.1.m1.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.3b"><list id="S5.SS2.p2.1.m1.3.4.1.cmml" xref="S5.SS2.p2.1.m1.3.4.2"><cn id="S5.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p2.1.m1.1.1">2</cn><cn id="S5.SS2.p2.1.m1.2.2.cmml" type="integer" xref="S5.SS2.p2.1.m1.2.2">3</cn><cn id="S5.SS2.p2.1.m1.3.3.cmml" type="integer" xref="S5.SS2.p2.1.m1.3.3">4</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.3c">[2,3,4]</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.3d">[ 2 , 3 , 4 ]</annotation></semantics></math> for pooling architectures <math alttext="2" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn id="S5.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS2.p2.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">2</annotation></semantics></math> was selected. For grouping and selecting architectures, we considered K values of <math alttext="[8,9,10,11]" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.4"><semantics id="S5.SS2.p2.3.m3.4a"><mrow id="S5.SS2.p2.3.m3.4.5.2" xref="S5.SS2.p2.3.m3.4.5.1.cmml"><mo id="S5.SS2.p2.3.m3.4.5.2.1" stretchy="false" xref="S5.SS2.p2.3.m3.4.5.1.cmml">[</mo><mn id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">8</mn><mo id="S5.SS2.p2.3.m3.4.5.2.2" xref="S5.SS2.p2.3.m3.4.5.1.cmml">,</mo><mn id="S5.SS2.p2.3.m3.2.2" xref="S5.SS2.p2.3.m3.2.2.cmml">9</mn><mo id="S5.SS2.p2.3.m3.4.5.2.3" xref="S5.SS2.p2.3.m3.4.5.1.cmml">,</mo><mn id="S5.SS2.p2.3.m3.3.3" xref="S5.SS2.p2.3.m3.3.3.cmml">10</mn><mo id="S5.SS2.p2.3.m3.4.5.2.4" xref="S5.SS2.p2.3.m3.4.5.1.cmml">,</mo><mn id="S5.SS2.p2.3.m3.4.4" xref="S5.SS2.p2.3.m3.4.4.cmml">11</mn><mo id="S5.SS2.p2.3.m3.4.5.2.5" stretchy="false" xref="S5.SS2.p2.3.m3.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.4b"><list id="S5.SS2.p2.3.m3.4.5.1.cmml" xref="S5.SS2.p2.3.m3.4.5.2"><cn id="S5.SS2.p2.3.m3.1.1.cmml" type="integer" xref="S5.SS2.p2.3.m3.1.1">8</cn><cn id="S5.SS2.p2.3.m3.2.2.cmml" type="integer" xref="S5.SS2.p2.3.m3.2.2">9</cn><cn id="S5.SS2.p2.3.m3.3.3.cmml" type="integer" xref="S5.SS2.p2.3.m3.3.3">10</cn><cn id="S5.SS2.p2.3.m3.4.4.cmml" type="integer" xref="S5.SS2.p2.3.m3.4.4">11</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.4c">[8,9,10,11]</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.4d">[ 8 , 9 , 10 , 11 ]</annotation></semantics></math> and selected <math alttext="9" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mn id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><cn id="S5.SS2.p2.4.m4.1.1.cmml" type="integer" xref="S5.SS2.p2.4.m4.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">9</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">9</annotation></semantics></math> and <math alttext="10" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mn id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><cn id="S5.SS2.p2.5.m5.1.1.cmml" type="integer" xref="S5.SS2.p2.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">10</annotation></semantics></math> respectively for he English-to-German direction and <math alttext="11" class="ltx_Math" display="inline" id="S5.SS2.p2.6.m6.1"><semantics id="S5.SS2.p2.6.m6.1a"><mn id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><cn id="S5.SS2.p2.6.m6.1.1.cmml" type="integer" xref="S5.SS2.p2.6.m6.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">11</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.6.m6.1d">11</annotation></semantics></math> (for both models) for the English-to-French direction. For the categorizing network, we used one hidden layer with <math alttext="512" class="ltx_Math" display="inline" id="S5.SS2.p2.7.m7.1"><semantics id="S5.SS2.p2.7.m7.1a"><mn id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><cn id="S5.SS2.p2.7.m7.1.1.cmml" type="integer" xref="S5.SS2.p2.7.m7.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">512</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.7.m7.1d">512</annotation></semantics></math> units and the Sparsemax activation function to obtain more sparse categorizations in an effort to increase the interpretability of the models <cite class="ltx_cite ltx_citemacro_citep">(Correia et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib10" title="">2019</a>; Meister et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib36" title="">2021</a>)</cite>. We performed preliminary experiments to find the architectural choices (gradient stopping and the decoder integration) for each caching model. In Caching Tokens, Caching Sentence, and Pooling architectures, we block gradient past the encoder for context sentences. Additionally, we allow gradient into the shortening from one and two context sentences for Selecting and Grouping architectures respectively. All models apart from Caching Sentence use sequential attention modules in the decoder (self-attention, cross-attention, and context-attention) without any gating mechanism. Caching Sentence yields the highest performance when parallel cross- and context-attention decoder is used with the gate on the context branch (see equation (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S4.E5" title="5 ‣ 4.3 Context Shortening ‣ 4 Method ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>)).
</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Accuracy</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.1.1.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.1.1.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.1.1.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.1.1.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.2.1">Sentence-level</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.2">37.64</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.3">75.92%</td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.2.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.3.3.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T3.1.3.3.2">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.3.3.2.1">Context: 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T3.1.3.3.3">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.3.3.3.1">Context: 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T3.1.3.3.4">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.3.3.4.1">Context: 3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.1.1">Model</span></th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.3.1">Accuracy</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.4.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.5.1">Accuracy</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.6.1">BLEU</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.7.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.5.5.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.2">37.25</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.3">77.27%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.4">37.18</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.5">78.98%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.6">37.12</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.5.5.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.5.7.1">80.87%</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.6.6.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.2">37.44</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.3">75.72%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.4">37.12</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.5">77.23%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.6">37.34</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.7">75.76%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.7.7.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.2">36.88</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.3">79.67%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.4">37.29</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.5">80.14%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.6">37.73</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.7.7.7">79.90%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.8.8.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.2">36.50</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.3">77.33%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.4">34.21</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.5">76.25%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.6">34.78</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.7">75.71%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.9.9.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.9.2.1">37.48</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.3">79.51%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.4">36.72</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.5">80.59%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.6">37.85</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.9.9.7">79.71%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.10.10.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.2">37.13</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.3">77.75%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.4">37.12</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.5">80.16%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.10.10.6.1">38.18</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.10.10.7">80.41%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.11.11.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.2">37.02</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.3">80.47%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.4">37.12</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.5">79.37%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.6">37.42</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.7">79.64%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.12.12.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.2">37.05</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.3">79.91%</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.12.12.4.1">37.98</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.12.12.5.1">81.13%</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.6">37.18</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.7">79.54%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.13.13.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.2">37.38</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.13.3.1">80.89%</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.4">37.83</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.5">80.32%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.6">37.81</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S5.T3.1.13.13.7">80.09%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the <span class="ltx_text ltx_font_bold" id="S5.T3.3.1">En-Fr</span> IWSLT 2017 experiment. The models were trained to use only the source-side context. We report BLEU of the test subset and the accuracy of the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The results of the single run (with the predetermined seed) of the English-to-German translation on the IWSLT 2017 dataset up to the context size of three can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.T2" title="Table 2 ‣ 5.1 Data ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. The BLEU score of the context-aware models is generally similar to or slightly higher than the sentence-level Transformer. BLEU does not correlate well with the contrastive accuracy, which is strictly higher for all context-aware models. This confirms that sentence-level metrics do not reflect the context usage of the models.
The highest contrastive dataset accuracy was achieved by the Grouping Shortening model for the context size of one, the Max Pooling Shortening model for the context size of two, and the Selecting Shortening model for the context size of three.
The highest accuracy averaged over the context sizes up to three was reached by the model employing Latent Grouping, followed by the Latent Selecting model.
Caching Tokens architecture exhibits comparable BLEU scores to the Single- and Multi-encoder architectures while achieving higher accuracy on the contrastive dataset. Caching Sentence architecture performed worse than other tested models, suggesting that representing the whole sentence as a single vector is not sufficient for contextual translation.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.T3" title="Table 3 ‣ 5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of the English-to-French translation with the context size up to three. The BLEU scores of all models are comparable (apart from the Caching Sentence architecture). Latent Grouping achieved the highest accuracy on the contrastive dataset for the context size of one, and Latent Selecting and Single-encoder architectures for the context sizes of one and three, respectively. The results in terms of COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A2" title="Appendix B COMET Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a>. The detailed results of the performance of the models on the contrastive datasets are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A3" title="Appendix C Detailed Contrastive Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">C</span></a>.We show several examples of translations by the tested models in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A4" title="Appendix D Examples of Translations ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Caching Tokens and Shortening models achieved higher accuracies than the Single- and Multi-encoder architectures (with the exception of Single-encoder on the English-to-French translation with the context size of three). In order to examine the effectiveness of the investigated architectures on even longer contexts we trained the models on the English-to-German IWSLT 2017 dataset with context sizes of up to 10. The results in terms of BLEU can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.F3" title="Figure 3 ‣ 5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. The detailed results (in terms of BLEU, COMET, and the accuracy on the ContraPro dataset) are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A5" title="Appendix E Larger Context Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">E</span></a>. The performance of the models employing Sequence Shortening is relatively high and stable for all tested context sizes. The caching architecture shows the reduction in BLEU for context sizes of 8 to 10 compared to the shortening architectures. We attribute the poor performance of the single-encoder (and to an extent multi-encoder) architecture to the large input sizes and the small size of the training dataset.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Applying Sequence Shortening to the cached sentence does not hurt the performance and exhibits more stable training with the long context sizes while reducing the memory footprint of the inference (Section <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS5" title="5.5 Memory Usage ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5.5</span></a>). Furthermore, Latent Grouping and Latent Selecting are increasing the interpretability of the model through the sparse assignment of tokens into groups (Section <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.SS4" title="5.4 Token Assignment Visualization ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5.4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="488" id="S5.F3.g1" src="extracted/5385333/Figures/extended_results_bleu.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>BLEU of the models trained on the <span class="ltx_text ltx_font_bold" id="S5.F3.2.1">En-De</span> IWSLT 2017 dataset with the context sizes up to 10. Caching Sentence model was not included for clarity. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S5.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="634" id="S5.F3.sf1.g1" src="extracted/5385333/Figures/Groups_En_De/groups_8.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S5.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="634" id="S5.F3.sf2.g1" src="extracted/5385333/Figures/Selecting_En_De/groups_8.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Latent Selecting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of tokens of the sentence from the ContraPro dataset grouped (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.F3.sf1" title="3(a) ‣ Figure 4 ‣ 5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">3(a)</span></a>) and selected (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.F3.sf2" title="3(b) ‣ Figure 4 ‣ 5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">3(b)</span></a>) by the model using Latent Grouping and Latent Selecting.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Token Assignment Visualization</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">An example visualization of groupings and selections of the Latent Grouping and Selecting architectures can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.F4" title="Figure 4 ‣ 5.3 Results ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> and more can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6" title="Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">F</span></a>. Latent Grouping seems to group tokens according to position with nouns given a high categorization score within a group. Furthermore, some groups contain more tokens than other groups. We hypothesize that the groups that contain more tokens are responsible for the general sense of the sentence and the groups with less tokens are responsible for encoding the details. Surprisingly, only four groups out of nine are utilized by the model. We hypothesize that the rest are used as the <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">no-op</span> tokens <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib9" title="">2019</a>)</cite> in the context-attention when the context is not needed. Latent Selecting, by design, has to assign tokens to each group. Again, nouns seem to be included in a group more often than other parts of speech. Some groups select punctuation marks and the <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.2">&lt;eos&gt;</span> token, which could take the role of the <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.3">no-op</span> tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Memory Usage</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We measured the memory used by the tested models as the value returned by the <code class="ltx_verbatim ltx_font_typewriter" id="S5.SS5.p1.1.1">torch.cuda.max_memory_allocated()</code> function. For clarity we omit the Caching Sentence model (as the worst performing) and the Max Pooling model (with results the same as the Avg Pooling model). We report the operation memory - the memory during inference on top of the memory taken by the model itself - on the examples from the test subset of the English-to-German IWSLT 2017 dataset with different numbers of context sentences. For context sizes above three, we used the models trained on the context size of three in order to not disadvantage the Single- and Multi-encoder architectures that were not able to learn on the dataset for large context sizes. The results are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5.F5" title="Figure 5 ‣ 5.5 Memory Usage ‣ 5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>. Although the number of parameters (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A1" title="Appendix A Models and Training Details ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>) is a dominant factor determining the overall memory usage, the operation memory grows at different paces for different architectures with the increased context size. The operational memory of the Single- and Multi-encoder models grows quadratically, while for caching and shortening architectures it grows linearly. Furthermore, the rate of increase is slower for shortening architectures compared to the Caching Tokens architecture, which can allow the significant advantage of shortening in the setting of long sentences or large contexts.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="488" id="S5.F5.g1" src="extracted/5385333/Figures/memory_operation_gpu_mean_mb.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The mean operation memory of the models when performing inference on the examples from the <span class="ltx_text ltx_font_bold" id="S5.F5.2.1">En-De</span> IWSLT 2017 test subset with the varying context sizes. For the context sizes above three, we used the models trained on the context size of three.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Caching architectures for Context-aware Machine Translation have not been widely explored in the literature so far. In this study, we show that a simple method of remembering the hidden representations of the previous sentences is comparable with more established Single- and Multi-encoder approaches in terms of BLEU and can be more effective in capturing context (up to 6 percentage points of the accuracy on the contrastive dataset for the context size of one) in the relatively low-resource training scenario. Furthermore, the caching architectures are more stable to train in the regime of larger context sizes according to our experiments.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Pooling-based shortening of the cached sentence maintains the comparable results to the caching architecture, while our introduced shortening methods - Latent Grouping and Selecting - show on average a strong performance both in terms of BLEU and accuracy while maintaining slower growth of the memory usage during inference, and potential increased interpretability of the model through sparse assignment of tokens into groups. Sequence Shortening, in general, exhibit stable training in the regime of large context sizes compared to other tested methods. In future work, we will explore the integration of Sequence Shortening with the target-side context.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our investigation is limited to the source-side context. There exist linguistic phenomena that can only be addressed by using target-side context <cite class="ltx_cite ltx_citemacro_cite">Voita et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib59" title="">2019b</a>)</cite>. While both caching and shortening could be applied to the target side as well, we do not provide an empirical evaluation of the performance of this approach.
</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Additionally, we do not apply sentence-level pre-training to our models. Architectures using Sequence Shortening could benefit from multiple stages of pre-training.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Lastly, our experiments involve language pairs from the same language family (English-to-German and English-to-French). We trained the models using the relatively low-resource datasets (IWSLT 2017) and the contrastive datasets used in this work target only the pronoun disambiguation task.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgments</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The research presented in this paper was conducted as part of VOXReality project<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://voxreality.eu/" title="">https://voxreality.eu/</a></span></span></span>, which was funded by the European Union Horizon Europe program under grant agreement No 101070521.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Ruchit Agrawal, Marco Turchi, and Matteo Negri. 2018.

</span>
<span class="ltx_bibblock">Contextual handling in neural machine translation: Look behind, ahead
and on both sides.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 21st Annual Conference of the European
Association for Machine Translation</em>, pages 31–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et al. (2016)</span>
<span class="ltx_bibblock">
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:1607.06450</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. (2021)</span>
<span class="ltx_bibblock">
Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and Weihua Luo. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.267" title="">G-transformer for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 3442–3455,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden et al. (2018)</span>
<span class="ltx_bibblock">
Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1118" title="">Evaluating discourse
phenomena in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1304–1313, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al. (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2004.05150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulatov et al. (2022)</span>
<span class="ltx_bibblock">
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.

</span>
<span class="ltx_bibblock">Recurrent memory transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>,
35:11079–11091.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cettolo et al. (2017)</span>
<span class="ltx_bibblock">
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian
Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2017.iwslt-1.1" title="">Overview of the
IWSLT 2017 evaluation campaign</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 14th International Conference on Spoken
Language Translation</em>, pages 2–14, Tokyo, Japan. International Workshop on
Spoken Language Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Linqing Chen, Junhui Li, Zhengxian Gong, Min Zhang, and Guodong Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3526215" title="">One type context is not
enough: Global context-aware neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 21(6).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2019)</span>
<span class="ltx_bibblock">
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-4828" title="">What does BERT look
at? an analysis of BERT’s attention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP</em>, pages 276–286, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Correia et al. (2019)</span>
<span class="ltx_bibblock">
Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1223" title="">Adaptively sparse
transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2174–2184, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth
Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
Maillard, et al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2020)</span>
<span class="ltx_bibblock">
Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020.

</span>
<span class="ltx_bibblock">Funnel-transformer: Filtering out sequential redundancy for efficient
language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Advances in neural information processing systems</em>,
33:4271–4282.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
Salakhutdinov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1285" title="">Transformer-XL:
Attentive language models beyond a fixed-length context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2978–2988, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Yukun Feng, Feng Li, Ziang Song, Boyuan Zheng, and Philipp Koehn. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-naacl.105" title="">Learn to
remember: Transformer with recurrent memory for document-level machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Findings of the Association for Computational Linguistics:
NAACL 2022</em>, pages 1409–1420, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2023)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Kayo Yin, Emmy Liu, André Martins, and Graham Neubig.
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.36" title="">When does
translation require context? a data-driven, multilingual exploration</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 606–626,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Kayo Yin, Graham Neubig, and André F. T. Martins. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.505" title="">Measuring and
increasing context usage in context-aware machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 6467–6478,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2020)</span>
<span class="ltx_bibblock">
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy,
Yogish Sabharwal, and Ashish Verma. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v119/goyal20a.html" title="">PoWER-BERT: Accelerating BERT inference via progressive word-vector
elimination</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 37th International Conference on Machine
Learning</em>, volume 119 of <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">Proceedings of Machine Learning Research</em>,
pages 3690–3699. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardmeier (2012)</span>
<span class="ltx_bibblock">
Christian Hardmeier. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.4000/discours.8726" title="">Discourse in
statistical machine translation: A survey and a case study</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Discours-Revue de linguistique, psycholinguistique et
informatique</em>, 11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan et al. (2018)</span>
<span class="ltx_bibblock">
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark,
Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis,
Mu Li, et al. 2018.

</span>
<span class="ltx_bibblock">Achieving human parity on automatic chinese to english news
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1803.05567</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huo et al. (2020)</span>
<span class="ltx_bibblock">
Jingjing Huo, Christian Herold, Yingbo Gao, Leonard Dahlmann, Shahram Khadivi,
and Hermann Ney. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.71" title="">Diving deep into
context-aware neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Fifth Conference on Machine Translation</em>,
pages 604–616, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al. (2021)</span>
<span class="ltx_bibblock">
Yongkeun Hwang, Hyeongu Yun, and Kyomin Jung. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.121" title="">Contrastive learning
for context-aware neural machine translation using coreference information</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 1135–1144, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jean et al. (2017)</span>
<span class="ltx_bibblock">
Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun Cho. 2017.

</span>
<span class="ltx_bibblock">Does neural machine translation benefit from larger context?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1704.05135</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitaev et al. (2020)</span>
<span class="ltx_bibblock">
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.

</span>
<span class="ltx_bibblock">Reformer: The efficient transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2001.04451</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A
simple and language independent subword tokenizer and detokenizer for neural
text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels,
Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu,
and Changliang Li. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.322" title="">Does
multi-encoder help? a case study on context-aware neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3512–3518, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022.

</span>
<span class="ltx_bibblock">A survey of transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">AI Open</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison et al. (2018)</span>
<span class="ltx_bibblock">
Pierre Lison, Jörg Tiedemann, and Milen Kouylekov. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L18-1275" title="">OpenSubtitles2018:
Statistical rescoring of sentence alignments in large, noisy parallel
corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European
Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopes et al. (2020)</span>
<span class="ltx_bibblock">
António Lopes, M. Amin Farajian, Rachel Bawden, Michael Zhang, and
André F. T. Martins. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.24" title="">Document-level
neural MT: A systematic comparison</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 22nd Annual Conference of the European
Association for Machine Translation</em>, pages 225–234, Lisboa, Portugal.
European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lupo et al. (2022)</span>
<span class="ltx_bibblock">
Lorenzo Lupo, Marco Dinarelli, and Laurent Besacier. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.312" title="">Divide and
rule: Effective pre-training for context-aware multi-encoder translation
models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 4557–4572,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2020)</span>
<span class="ltx_bibblock">
Shuming Ma, Dongdong Zhang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.321" title="">A simple and
effective unified encoder for document-level machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3505–3511, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumde et al. (2022)</span>
<span class="ltx_bibblock">
Suvodeep Majumde, Stanislas Lauly, Maria Nadejde, Marcello Federico, and
Georgiana Dinu. 2022.

</span>
<span class="ltx_bibblock">A baseline revisited: Pushing the limits of multi-segment models for
context-aware translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2210.10906</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martins and Astudillo (2016)</span>
<span class="ltx_bibblock">
André FT Martins and Ramón F Astudillo. 2016.

</span>
<span class="ltx_bibblock">From softmax to sparsemax: a sparse model of attention and
multi-label classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 33rd International Conference on
International Conference on Machine Learning-Volume 48</em>, pages 1614–1623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maruf et al. (2019)</span>
<span class="ltx_bibblock">
Sameen Maruf, André F. T. Martins, and Gholamreza Haffari. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1313" title="">Selective attention for
context-aware neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 3092–3102,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathy and Feldman (2012)</span>
<span class="ltx_bibblock">
Fabien Mathy and Jacob Feldman. 2012.

</span>
<span class="ltx_bibblock">What’s magic about magic numbers? chunking and data compression in
short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Cognition</em>, 122(3):346–362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meister et al. (2021)</span>
<span class="ltx_bibblock">
Clara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-short.17" title="">Is sparse
attention more interpretable?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 2: Short Papers)</em>, pages 122–129,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miculicich et al. (2018)</span>
<span class="ltx_bibblock">
Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1325" title="">Document-level neural
machine translation with hierarchical attention networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2947–2954, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (1956)</span>
<span class="ltx_bibblock">
George A Miller. 1956.

</span>
<span class="ltx_bibblock">The magical number seven, plus or minus two: Some limits on our
capacity for processing information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Psychological review</em>, 63(2):81.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morishita et al. (2021)</span>
<span class="ltx_bibblock">
Makoto Morishita, Jun Suzuki, Tomoharu Iwata, and Masaaki Nagata. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.214" title="">Context-aware
neural machine translation with mini-batch embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume</em>, pages
2513–2521, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al. (2018)</span>
<span class="ltx_bibblock">
Mathias Müller, Annette Rios, Elena Voita, and Rico Sennrich. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6307" title="">A large-scale test set
for the evaluation of context-aware pronoun translation in neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 61–72, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nawrot et al. (2022)</span>
<span class="ltx_bibblock">
Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Lukasz Kaiser, Yuhuai Wu,
Christian Szegedy, and Henryk Michalewski. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-naacl.117" title="">Hierarchical transformers are more efficient language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Findings of the Association for Computational Linguistics:
NAACL 2022</em>, pages 1559–1571, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast,
extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics (Demonstrations)</em>,
pages 48–53, Minneapolis, Minnesota. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in
reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186–191, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022)</span>
<span class="ltx_bibblock">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C
Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T.
Martins. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22:
Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the Seventh Conference on Machine Translation
(WMT)</em>, pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A
neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1162" title="">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramanian et al. (2020)</span>
<span class="ltx_bibblock">
Sandeep Subramanian, Ronan Collobert, Marc’Aurelio Ranzato, and Y-Lan Boureau.
2020.

</span>
<span class="ltx_bibblock">Multi-scale transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2005.00581</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022)</span>
<span class="ltx_bibblock">
Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen,
and Lei Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.279" title="">Rethinking
document-level neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 3537–3548, Dublin, Ireland. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3530811" title="">Efficient transformers: A
survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACM Comput. Surv.</em>, 55(6).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2021)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri,
Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021.

</span>
<span class="ltx_bibblock">Charformer: Fast character transformers via gradient-based subword
tokenization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2106.12672</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Terrace (2002)</span>
<span class="ltx_bibblock">
H. S. Terrace. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-1-4615-0821-2_2" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1.1">The
Comparative Psychology of Chunking</em></a>, pages 23–55. Springer US, Boston, MA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann et al. (2022)</span>
<span class="ltx_bibblock">
Jörg Tiedemann, Mikko Aulamo, Daria Bakshandaeva, Michele Boggia, Stig-Arne
Grönroos, Tommi Nieminen, Alessandro Raganato, Yves Scherrer, Raul
Vazquez, and Sami Virpioja. 2022.

</span>
<span class="ltx_bibblock">Democratizing machine translation with opus-mt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2212.01936</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Scherrer (2017)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Yves Scherrer. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4811" title="">Neural machine
translation with extended context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the Third Workshop on Discourse in Machine
Translation</em>, pages 82–92, Copenhagen, Denmark. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. (2017)</span>
<span class="ltx_bibblock">
Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu, and Hang Li. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00048" title="">Context gates for
neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Transactions of the Association for Computational Linguistics</em>,
5:87–99.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. (2018)</span>
<span class="ltx_bibblock">
Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00029" title="">Learning to remember
translation history with a continuous cache</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Transactions of the Association for Computational Linguistics</em>,
6:407–420.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et al. (2019a)</span>
<span class="ltx_bibblock">
Elena Voita, Rico Sennrich, and Ivan Titov. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1081" title="">Context-aware
monolingual repair for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 877–886, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et al. (2019b)</span>
<span class="ltx_bibblock">
Elena Voita, Rico Sennrich, and Ivan Titov. 2019b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1116" title="">When a good translation
is wrong in context: Context-aware machine translation improves on deixis,
ellipsis, and lexical cohesion</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1198–1212, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et al. (2018)</span>
<span class="ltx_bibblock">
Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1117" title="">Context-aware neural
machine translation learns anaphora resolution</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1264–1274,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.

</span>
<span class="ltx_bibblock">Linformer: Self-attention with linear complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2006.04768</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wong and Kit (2012)</span>
<span class="ltx_bibblock">
Billy T. M. Wong and Chunyu Kit. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/D12-1097" title="">Extending machine
translation evaluation metrics with lexical cohesion to document level</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language
Learning</em>, pages 1060–1068, Jeju Island, Korea. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2022)</span>
<span class="ltx_bibblock">
Xueqing Wu, Yingce Xia, Jinhua Zhu, Lijun Wu, Shufang Xie, and Tao Qin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10994-021-06070-y" title="">A study of bert
for context-aware neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Machine Learning</em>, 111(3):917–935.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2021)</span>
<span class="ltx_bibblock">
Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T.
Martins, and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.65" title="">Do
context-aware translation models pay the right attention?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 788–801, Online.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Pei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.81" title="">Long-short
term masking transformer: A simple but effective baseline for document-level
neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1081–1087, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2021)</span>
<span class="ltx_bibblock">
Zaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun Chen, and Alexandra Birch.
2021.

</span>
<span class="ltx_bibblock">Towards making the most of context in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the Twenty-Ninth International Conference on
International Joint Conferences on Artificial Intelligence</em>, pages
3983–3989.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Models and Training Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">To implement and train our models we used fairseq framework <cite class="ltx_cite ltx_citemacro_citep">(Ott et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib42" title="">2019</a>)</cite> and based our code on the codebase of <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib17" title="">2021</a>)</cite>. All models were based on the transformer-base configuration. The shared hyper-parameters are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A1.T4" title="Table 4 ‣ Appendix A Models and Training Details ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>. We trained each model on a single GPU (NVIDIA GeForce RTX 3090 24GB).</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For Latent Grouping and Shortening, we used a categorizing FFN with 512 hidden units, the number of inputs equal to the Embed Dim, and the number of outputs equal to the number of groups.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A1.T5" title="Table 5 ‣ Appendix A Models and Training Details ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> shows the number of parameters for each model.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Hyper-parameter</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.2.1.1">Encoder Layers</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T4.1.2.1.2">6</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.3.2.1">Decoder Layers</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.3.2.2">6</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.4.3.1">Attention Heads</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.4.3.2">8</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.5.4.1">Embed Dim</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.5.4.2">512</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.6.5.1">FFN Embed Dim</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.6.5.2">2048</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.7.6.1">Dropout</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.7.6.2">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.8.7.1">Share Decoder In/Out Embed</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.8.7.2">True</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.9.8.1">Optimizer</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.9.8.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.10.9.1">Adam Betas</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.10.9.2">0.9, 0.98</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.11.10.1">Adam Epsilon</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.11.10.2">1e-8</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.12.11.1">Learning Rate</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.12.11.2">5e-4</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.13.12.1">LR Scheduler</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.13.12.2">Inverse Sqrt</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.14.13.1">LR Warmup Updates</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.14.13.2">2500</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.15.14.1">Weight Decay</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.15.14.2">0.0001</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.16.15.1">Label Smoothing</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.16.15.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.17.16.1">Clip Norm</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.17.16.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.18.17.1">Batch Max Tokens</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.18.17.2">4096</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.19.18.1">Update Frequency</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.19.18.2">8</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.20.19.1">Max Epoch</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.20.19.2">-</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.21.20.1">Patience</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.21.20.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.22.21.1">Beam</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.22.21.2">5</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.23.22.1">Max Vocab Size</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.23.22.2">20000</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A1.T4.1.24.23.1">Seed</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A1.T4.1.24.23.2">42</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The shared hyper-parameters of the tested models.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A1.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1">Parameters</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.1.2.1.1">Sentence-level</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.2.1.2">64.42M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.3.2.1">Single-encoder</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.3.2.2">64.42M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.4.3.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.4.3.2">83.33M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.5.4.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.5.4.2">71.25M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.6.5.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.6.5.2">71.26M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.7.6.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.7.6.2">72.83M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.8.7.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.8.7.2">72.83M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.9.8.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.9.8.2">73.35M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.10.9.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.10.9.2">72.58M</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A1.T5.1.11.10.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A1.T5.1.11.10.2">72.58M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The number of parameters in the tested models.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>COMET Results</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Apart from BLEU and contrastive dataset accuracy presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#S5" title="5 Experiments ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, we also measured COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> based on <span class="ltx_text ltx_font_typewriter" id="A2.p1.1.1">Unbabel/wmt22-comet-da</span> model <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib45" title="">2022</a>)</cite>. See Tables <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A2.T6" title="Table 6 ‣ Appendix B COMET Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A2.T7" title="Table 7 ‣ Appendix B COMET Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a> for the results on English-to-German and English-to-French respectively.</p>
</div>
<figure class="ltx_table" id="A2.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.2.1">Context: 0</span></td>
<td class="ltx_td ltx_border_t" id="A2.T6.1.1.1.3"></td>
<td class="ltx_td ltx_border_t" id="A2.T6.1.1.1.4"></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.2.2.1">Sentence-level</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.2.2.2">0.7778</td>
<td class="ltx_td ltx_border_t" id="A2.T6.1.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="A2.T6.1.2.2.4"></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A2.T6.1.3.3.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T6.1.3.3.2.1">Context: 1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.3.3.3"><span class="ltx_text ltx_font_bold" id="A2.T6.1.3.3.3.1">Context: 2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.3.3.4"><span class="ltx_text ltx_font_bold" id="A2.T6.1.3.3.4.1">Context: 3</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.4.4.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.4.4.2">0.7831</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.4.4.3">0.7789</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.4.4.4">0.7758</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.5.5.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.5.5.2">0.7831</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A2.T6.1.5.5.3.1">0.7871</span></td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.5.5.4"><span class="ltx_text ltx_font_bold" id="A2.T6.1.5.5.4.1">0.7856</span></td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T6.1.6.6.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.6.6.2">0.7806</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.6.6.3">0.7776</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T6.1.6.6.4">0.7821</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.7.7.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.7.7.2">0.7712</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.7.7.3">0.7640</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.7.7.4">0.7673</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.8.8.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.8.8.2">0.7743</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.8.8.3">0.7772</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.8.8.4">0.7799</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.9.9.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.9.9.2">0.7774</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.9.9.3">0.7770</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.9.9.4">0.7844</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.10.10.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.10.10.2">0.7757</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.10.10.3">0.7745</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.10.10.4">0.7823</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T6.1.11.11.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A2.T6.1.11.11.2"><span class="ltx_text ltx_font_bold" id="A2.T6.1.11.11.2.1">0.7842</span></td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.11.11.3">0.7828</td>
<td class="ltx_td ltx_align_right" id="A2.T6.1.11.11.4">0.7811</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A2.T6.1.12.12.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T6.1.12.12.2">0.7774</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T6.1.12.12.3">0.7826</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T6.1.12.12.4">0.7836</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results in terms of COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> based on <span class="ltx_text ltx_font_typewriter" id="A2.T6.4.1">Unbabel/wmt22-comet-da</span> model <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib45" title="">2022</a>)</cite> of the <span class="ltx_text ltx_font_bold" id="A2.T6.5.2">En-De</span> IWSLT 2017 experiment.</figcaption>
</figure>
<figure class="ltx_table" id="A2.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.2.1">Context: 0</span></td>
<td class="ltx_td ltx_border_t" id="A2.T7.1.1.1.3"></td>
<td class="ltx_td ltx_border_t" id="A2.T7.1.1.1.4"></td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.2.2.1">Sentence-level</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.2.2.2">0.7943</td>
<td class="ltx_td ltx_border_t" id="A2.T7.1.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="A2.T7.1.2.2.4"></td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A2.T7.1.3.3.1.1">Model</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.3.3.2"><span class="ltx_text ltx_font_bold" id="A2.T7.1.3.3.2.1">Context: 1</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.3.3.3"><span class="ltx_text ltx_font_bold" id="A2.T7.1.3.3.3.1">Context: 2</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.3.3.4"><span class="ltx_text ltx_font_bold" id="A2.T7.1.3.3.4.1">Context: 3</span></td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.4.4.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.4.4.2">0.7930</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.4.4.3"><span class="ltx_text ltx_font_bold" id="A2.T7.1.4.4.3.1">0.7979</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.4.4.4">0.7913</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.5.5.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.5.5.2"><span class="ltx_text ltx_font_bold" id="A2.T7.1.5.5.2.1">0.7968</span></td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.5.5.3">0.7934</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.5.5.4">0.7934</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.6.6.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.6.6.2">0.7923</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.6.6.3">0.7935</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T7.1.6.6.4">0.7945</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.7.7.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.7.7.2">0.7845</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.7.7.3">0.7654</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.7.7.4">0.7737</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.8.8.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.8.8.2">0.7911</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.8.8.3">0.7913</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.8.8.4"><span class="ltx_text ltx_font_bold" id="A2.T7.1.8.8.4.1">0.7974</span></td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.9.9.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.9.9.2">0.7920</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.9.9.3">0.7924</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.9.9.4">0.7952</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.10.10.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.10.10.2">0.7933</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.10.10.3">0.7951</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.10.10.4">0.7927</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.11.11.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A2.T7.1.11.11.2">0.7933</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.11.11.3">0.7976</td>
<td class="ltx_td ltx_align_right" id="A2.T7.1.11.11.4">0.7921</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A2.T7.1.12.12.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T7.1.12.12.2">0.7951</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T7.1.12.12.3">0.7945</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T7.1.12.12.4">0.7935</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results in terms of COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> based on <span class="ltx_text ltx_font_typewriter" id="A2.T7.4.1">Unbabel/wmt22-comet-da</span> model <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib45" title="">2022</a>)</cite> of the <span class="ltx_text ltx_font_bold" id="A2.T7.5.2">En-Fr</span> IWSLT 2017 experiment.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed Contrastive Results</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section we report the accuracy on the contrastive datasets for the different placements of the antecedent. The antecedent distance of zero corresponds to the examples where the antecedent is in the current sentence. The value of one represent the antecedent in the first context sentence (counting backward from the current sentence), etc. The results of the ContraPro dataset (English-to-German) and the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite> (English-to-French) are presented in Tables <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A3.T8" title="Table 8 ‣ Appendix C Detailed Contrastive Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A3.T9" title="Table 9 ‣ Appendix C Detailed Contrastive Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">9</span></a> respectively.</p>
</div>
<figure class="ltx_table" id="A3.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T8.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="A3.T8.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.3.1">Antecedent Distance</span></th>
</tr>
<tr class="ltx_tr" id="A3.T8.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A3.T8.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="A3.T8.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.2.1">Context</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T8.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.3.1">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T8.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.4.1">1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T8.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.5.1">2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T8.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.6.1">3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T8.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A3.T8.1.2.2.7.1">&gt;3</span></th>
</tr>
<tr class="ltx_tr" id="A3.T8.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.1.3.3.1">Sentence-level</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.1.3.3.2">0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.3.3.3">72.21%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.3.3.4">31.82%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.3.3.5">44.90%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.3.3.6">48.87%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.1.3.3.7">67.42%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T8.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.4.1.1">Single-encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.4.1.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.4.1.3">70.08%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.4.1.4">38.42%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.4.1.5">46.16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.4.1.6">49.04%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.4.1.7">70.59%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.5.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.5.2.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.2.3">73.96%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.2.4">37.87%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.2.5">48.48%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.2.6">50.79%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.5.2.7">69.00%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.6.3">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.6.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.6.3.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.3.3">71.79%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.3.4">40.00%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.3.5">47.88%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.3.6">52.01%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.6.3.7">66.06%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.7.4.1">Multi-encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.7.4.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.7.4.3">75.17%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.7.4.4">33.16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.7.4.5">44.64%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.7.4.6">47.47%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.7.4.7">66.97%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.8.5">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.8.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.8.5.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.8.5.3">73.54%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.8.5.4">35.63%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.8.5.5">47.42%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.8.5.6">50.79%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.8.5.7">69.00%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.9.6">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.9.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.9.6.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.6.3">70.88%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.6.4">33.99%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.6.5">46.16%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.6.6">50.61%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.9.6.7">69.46%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.10.7.1">Caching Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.10.7.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.10.7.3">72.21%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.10.7.4">49.07%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.10.7.5">45.03%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.10.7.6">50.09%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.10.7.7">71.27%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.11.8">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.11.8.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.11.8.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.8.3">70.75%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.8.4">47.17%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.8.5">58.74%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.8.6">48.69%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.11.8.7">66.74%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.12.9">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.12.9.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.12.9.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.9.3">70.25%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.9.4">42.53%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.9.5">52.98%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.9.6">60.91%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.12.9.7">68.78%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.13.10.1">Caching Sentence</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.13.10.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.10.3">66.83%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.10.4">36.78%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.10.5">45.63%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.10.6">50.26%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.13.10.7">68.55%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.14.11">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.14.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.14.11.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.11.3">66.83%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.11.4">35.42%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.11.5">47.81%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.11.6">49.74%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.14.11.7">71.04%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.15.12">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.15.12.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.15.12.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.12.3">60.17%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.12.4">37.16%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.12.5">47.95%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.12.6">50.96%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.15.12.7">67.87%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.16.13.1">Shortening - Max Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.16.13.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.16.13.3">68.92%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.16.13.4">46.33%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.16.13.5">44.64%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.16.13.6">48.17%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.16.13.7">71.95%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.17.14">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.17.14.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.17.14.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.14.3">72.83%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.14.4">47.63%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.14.5">62.12%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.14.6">47.64%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.17.14.7">63.57%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.18.15">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.18.15.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.18.15.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.18.15.3">72.13%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.18.15.4">40.83%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.18.15.5">53.71%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.18.15.6">63.00%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.18.15.7">71.27%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.19.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.19.16.1">Shortening - Avg Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.19.16.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.19.16.3">70.04%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.19.16.4">48.58%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.19.16.5">45.50%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.19.16.6">48.52%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.19.16.7">72.62%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.20.17">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.20.17.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.20.17.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.17.3">72.67%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.17.4">47.04%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.17.5">62.58%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.17.6">47.64%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.20.17.7">64.93%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.21.18">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.21.18.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.21.18.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.18.3">70.88%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.18.4">40.71%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.18.5">54.24%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.18.6">60.56%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.21.18.7">71.95%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.22.19.1">Shortening - Linear Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.22.19.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.22.19.3">69.13%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.22.19.4">47.84%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.22.19.5">44.64%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.22.19.6">49.21%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.22.19.7">73.53%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.23.20">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.23.20.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.23.20.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.23.20.3">70.38%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.23.20.4">43.75%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.23.20.5">59.34%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.23.20.6">47.99%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.23.20.7">67.87%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.24.21">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.24.21.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.24.21.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.21.3">72.58%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.21.4">41.06%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.21.5">54.90%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.21.6">64.05%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.24.21.7">69.91%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.25.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.25.22.1">Shortening - Grouping</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.25.22.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.25.22.3">73.67%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.25.22.4">53.64%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.25.22.5">45.56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.25.22.6">46.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.25.22.7">71.72%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.26.23">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.26.23.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.26.23.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.23.3">69.17%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.23.4">47.66%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.23.5">61.85%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.23.6">47.29%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.26.23.7">68.78%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.27.24">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.27.24.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.27.24.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.27.24.3">71.21%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.27.24.4">41.58%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.27.24.5">55.03%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.27.24.6">62.13%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.27.24.7">68.10%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.28.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.28.25.1">Shortening - Selecting</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.1.28.25.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.28.25.3">72.88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.28.25.4">50.16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.28.25.5">43.77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.28.25.6">47.64%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.28.25.7">69.00%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.29.26">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T8.1.29.26.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.1.29.26.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.1.29.26.3">71.75%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.29.26.4">45.85%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.29.26.5">64.04%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.29.26.6">47.99%</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.29.26.7">67.19%</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.30.27">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="A3.T8.1.30.27.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="A3.T8.1.30.27.2">3</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T8.1.30.27.3">73.29%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T8.1.30.27.4">42.04%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T8.1.30.27.5">54.57%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T8.1.30.27.6">65.10%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T8.1.30.27.7">68.78%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Detailed results of the accuracy on the ContraPro contrastive dataset for different antecedent locations of the models trained on the <span class="ltx_text ltx_font_bold" id="A3.T8.3.1">En-De</span> IWSLT 2017 dataset.</figcaption>
</figure>
<figure class="ltx_table" id="A3.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T9.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T9.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="A3.T9.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="A3.T9.1.1.1.3.1">Antecedent Distance</span></th>
</tr>
<tr class="ltx_tr" id="A3.T9.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A3.T9.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="A3.T9.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.2.1">Context</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T9.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.3.1">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T9.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.4.1">1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T9.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.5.1">2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T9.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.6.1">3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T9.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.2.7.1">&gt;3</span></th>
</tr>
<tr class="ltx_tr" id="A3.T9.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T9.1.3.3.1">Sentence-level</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T9.1.3.3.2">0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T9.1.3.3.3">75.86%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T9.1.3.3.4">75.76%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T9.1.3.3.5">76.98%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T9.1.3.3.6">76.70%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T9.1.3.3.7">74.55%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T9.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.4.1.1">Single-encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.4.1.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.4.1.3">76.88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.4.1.4">77.16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.4.1.5">78.39%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.4.1.6">78.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.4.1.7">76.89%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.5.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.5.2.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2.3">78.92%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2.4">78.69%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2.5">80.17%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2.6">78.98%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.5.2.7">78.70%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.6.3">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.6.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.6.3.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3.3">80.37%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3.4">80.99%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3.5">81.77%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3.6">81.70%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.6.3.7">81.15%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.7.4.1">Multi-encoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.7.4.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.7.4.3">75.71%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.7.4.4">75.08%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.7.4.5">76.92%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.7.4.6">76.93%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.7.4.7">75.72%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.8.5">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.8.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.8.5.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.8.5.3">77.03%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.8.5.4">77.16%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.8.5.5">78.08%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.8.5.6">78.52%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.8.5.7">76.14%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.9.6">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.9.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.9.6.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.9.6.3">75.08%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.9.6.4">76.11%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.9.6.5">77.53%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.9.6.6">77.27%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.9.6.7">74.01%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.10.7.1">Caching Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.10.7.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.10.7.3">79.80%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.10.7.4">79.11%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.10.7.5">80.72%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.10.7.6">80.68%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.10.7.7">78.81%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.11.8">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.11.8.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.11.8.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.11.8.3">79.77%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.11.8.4">80.44%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.11.8.5">80.79%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.11.8.6">81.25%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.11.8.7">78.81%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.12.9">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.12.9.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.12.9.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.12.9.3">79.27%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.12.9.4">80.27%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.12.9.5">81.28%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.12.9.6">80.34%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.12.9.7">79.34%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.13.10.1">Caching Sentence</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.13.10.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.13.10.3">76.81%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.13.10.4">77.18%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.13.10.5">78.21%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.13.10.6">80.23%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.13.10.7">77.10%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.14.11">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.14.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.14.11.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.14.11.3">75.73%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.14.11.4">76.52%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.14.11.5">76.98%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.14.11.6">78.64%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.14.11.7">74.76%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.15.12">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.15.12.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.15.12.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.15.12.3">75.01%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.15.12.4">75.87%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.15.12.5">78.27%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.15.12.6">75.68%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.15.12.7">74.97%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.16.13.1">Shortening - Max Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.16.13.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.16.13.3">80.49%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.16.13.4">80.38%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.16.13.5">80.11%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.16.13.6">80.11%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.16.13.7">81.90%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.17.14">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.17.14.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.17.14.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.17.14.3">80.25%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.17.14.4">80.73%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.17.14.5">81.15%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.17.14.6">80.68%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.17.14.7">81.04%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.18.15">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.18.15.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.18.15.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.18.15.3">78.98%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.18.15.4">80.27%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.18.15.5">81.28%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.18.15.6">80.80%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.18.15.7">77.96%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.19.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.19.16.1">Shortening - Avg Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.19.16.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.19.16.3">77.36%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.19.16.4">77.70%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.19.16.5">79.19%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.19.16.6">77.61%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.19.16.7">78.06%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.20.17">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.20.17.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.20.17.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.20.17.3">79.94%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.20.17.4">80.00%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.20.17.5">80.79%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.20.17.6">81.70%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.20.17.7">79.77%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.21.18">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.21.18.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.21.18.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.21.18.3">79.94%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.21.18.4">80.77%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.21.18.5">81.65%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.21.18.6">81.02%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.21.18.7">79.02%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.22.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.22.19.1">Shortening - Linear Pooling</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.22.19.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.22.19.3">79.87%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.22.19.4">80.35%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.22.19.5">82.44%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.22.19.6">80.57%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.22.19.7">81.36%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.23.20">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.23.20.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.23.20.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.23.20.3">78.80%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.23.20.4">79.06%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.23.20.5">80.72%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.23.20.6">80.68%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.23.20.7">80.94%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.24.21">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.24.21.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.24.21.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.24.21.3">79.03%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.24.21.4">80.09%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.24.21.5">80.85%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.24.21.6">80.23%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.24.21.7">78.59%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.25.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.25.22.1">Shortening - Grouping</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.25.22.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.25.22.3">79.28%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.25.22.4">80.40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.25.22.5">81.46%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.25.22.6">79.89%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.25.22.7">78.91%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.26.23">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.26.23.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.26.23.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.26.23.3">80.91%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.26.23.4">81.30%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.26.23.5">81.65%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.26.23.6">81.36%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.26.23.7">80.62%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.27.24">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.27.24.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.27.24.2">3</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.27.24.3">79.07%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.27.24.4">80.20%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.27.24.5">78.88%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.27.24.6">81.14%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.27.24.7">78.91%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.28.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.28.25.1">Shortening - Selecting</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.28.25.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.28.25.3">80.30%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.28.25.4">81.03%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.28.25.5">81.89%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.28.25.6">82.73%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.28.25.7">80.40%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.29.26">
<th class="ltx_td ltx_th ltx_th_row" id="A3.T9.1.29.26.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.1.29.26.2">2</th>
<td class="ltx_td ltx_align_center" id="A3.T9.1.29.26.3">80.17%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.29.26.4">80.33%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.29.26.5">81.40%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.29.26.6">81.02%</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.29.26.7">78.70%</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.30.27">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="A3.T9.1.30.27.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="A3.T9.1.30.27.2">3</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T9.1.30.27.3">79.28%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T9.1.30.27.4">80.33%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T9.1.30.27.5">81.89%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T9.1.30.27.6">79.55%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T9.1.30.27.7">81.36%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Detailed results of the accuracy on the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite> for different antecedent locations of the models trained on the <span class="ltx_text ltx_font_bold" id="A3.T9.3.1">En-Fr</span> IWSLT 2017 dataset.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Examples of Translations</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We present the examples of the translation of the sentence-level Transformer, and Selecting and Grouping Shortening architectures on the IWSLT 2017 English-to-German dataset in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A4.T10" title="Table 10 ‣ Appendix D Examples of Translations ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">10</span></a>. We marked the pronoun disambiguation from context sentences.
</p>
</div>
<figure class="ltx_table" id="A4.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T10.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T10.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.1.1.1">Source Context</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.1.1.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.1.1.2.1">This is a nice <span class="ltx_text ltx_font_bold" id="A4.T10.1.1.1.2.1.1" style="color:#0000FF;">building</span>.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A4.T10.1.2.2.1">Source Sentence</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="A4.T10.1.2.2.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.2.2.2.1">But <span class="ltx_text ltx_font_bold" id="A4.T10.1.2.2.2.1.1" style="color:#0000FF;">it</span> doesn’t have much to do with what a library actually does today.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.3.3.1">Target Reference</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.3.3.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.3.3.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.3.3.2.1.1" style="color:#008000;">es</span> hat nicht viel mit dem zu tun, was eine Bibliothek heute leistet.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.4.4.1">Sentence-level</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.4.4.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.4.4.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.4.4.2.1.1" style="color:#008000;">es</span> hat nicht viel damit zu tun, was eine Bibliothek heute tut.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.5.5.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.5.5.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.5.5.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.5.5.2.1.1" style="color:#008000;">es</span> hat nicht viel mit der heutigen Bibliothek zu tun.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.6.6.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.6.6.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.6.6.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.6.6.2.1.1" style="color:#008000;">es</span> hat nicht viel mit dem zu tun, was eine Bibliothek heute tut.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="A4.T10.1.7.7.1">Source Context</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A4.T10.1.7.7.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.7.7.2.1">Zak Ebrahim is not my real <span class="ltx_text ltx_font_bold" id="A4.T10.1.7.7.2.1.1" style="color:#0000FF;">name</span>.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A4.T10.1.8.8.1">Source Sentence</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="A4.T10.1.8.8.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.8.8.2.1">I changed <span class="ltx_text ltx_font_bold" id="A4.T10.1.8.8.2.1.1" style="color:#0000FF;">it</span> when my family decided to end our connection with my father and start a new life.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.9.9.1">Target Reference</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.9.9.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.9.9.2.1">Ich habe <span class="ltx_text ltx_font_bold" id="A4.T10.1.9.9.2.1.1" style="color:#0000FF;">ihn</span> geändert, als meine Familie beschloss, den Kontakt zu meinem Vater abzubrechen und ein neues Leben zu beginnen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.10.10.1">Sentence-level</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.10.10.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.10.10.2.1">Ich änderte <span class="ltx_text ltx_font_bold" id="A4.T10.1.10.10.2.1.1" style="color:#CC1A1A;">es</span>, als meine Familie entschied, unsere Verbindung mit meinem Vater zu beenden und ein neues Leben zu starten.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.11.11.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.11.11.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.11.11.2.1">Ich habe <span class="ltx_text ltx_font_bold" id="A4.T10.1.11.11.2.1.1" style="color:#008000;">ihn</span> verändert, als meine Familie entschied, unsere Verbindung mit meinem Vater zu beenden und ein neues Leben zu beginnen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.12.12.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.12.12.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.12.12.2.1">Ich habe <span class="ltx_text ltx_font_bold" id="A4.T10.1.12.12.2.1.1" style="color:#CC1A1A;">es</span> verändert, als meine Familie beschloss, unsere Verbindung mit meinem Vater zu beenden und ein neues Leben zu beginnen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="A4.T10.1.13.13.1">Source Context</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A4.T10.1.13.13.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.13.13.2.1">And this <span class="ltx_text ltx_font_bold" id="A4.T10.1.13.13.2.1.1" style="color:#0000FF;">work</span> has been wonderful. It’s been great.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A4.T10.1.14.14.1">Source Sentence</th>
<td class="ltx_td ltx_align_justify ltx_border_r" id="A4.T10.1.14.14.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.14.14.2.1">But <span class="ltx_text ltx_font_bold" id="A4.T10.1.14.14.2.1.1" style="color:#0000FF;">it</span> also has some fundamental limitations so far.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.15.15.1">Target Reference</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.15.15.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.15.15.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.15.15.2.1.1" style="color:#0000FF;">sie</span> hat auch noch immer einige grundlegende Grenzen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.16.16.1">Sentence-level</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.16.16.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.16.16.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.16.16.2.1.1" style="color:#CC1A1A;">es</span> hat bis jetzt auch einige fundamentale Grenzen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.17.17.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A4.T10.1.17.17.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.17.17.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.17.17.2.1.1" style="color:#CC1A1A;">es</span> hat bis jetzt noch grundlegende Grenzen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A4.T10.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A4.T10.1.18.18.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="A4.T10.1.18.18.2" style="width:312.2pt;">
<p class="ltx_p ltx_align_top" id="A4.T10.1.18.18.2.1">Aber <span class="ltx_text ltx_font_bold" id="A4.T10.1.18.18.2.1.1" style="color:#008000;">sie</span> hat auch bis jetzt einige fundamentale Grenzen.</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Example translations of sentence-level Transformer and Grouping and Selecting shortening context-aware models of the English sentence with the context size of one to German. We marked <span class="ltx_text ltx_font_bold" id="A4.T10.6.1" style="color:#0000FF;">antecedent</span> and <span class="ltx_text ltx_font_bold" id="A4.T10.7.2" style="color:#0000FF;">pronoun</span> in the source sentence and <span class="ltx_text ltx_font_bold" id="A4.T10.8.3" style="color:#008000;">correct</span> and <span class="ltx_text ltx_font_bold" id="A4.T10.9.4" style="color:#CC1A1A;">incorrect</span> pronoun translations.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Larger Context Results</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">In order to examine the behavior of the tested models in response to larger contexts, we trained the models on the IWSLT 2017 English-to-German dataset with context sizes up to 10. We present the results in terms of BLEU, accuracy on the ContraPro contrastive dataset, and COMET in Tables <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A5.T11" title="Table 11 ‣ Appendix E Larger Context Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A5.T12" title="Table 12 ‣ Appendix E Larger Context Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">12</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A5.T13" title="Table 13 ‣ Appendix E Larger Context Results ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">13</span></a> respectively.
</p>
</div>
<figure class="ltx_table" id="A5.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T11.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T11.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A5.T11.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="A5.T11.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.1.1.2.1">Context Size</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.2.1">4</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.3.1">5</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.4.1">6</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.5.1">7</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.6.1">8</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.7">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.7.1">9</span></td>
<td class="ltx_td ltx_align_center" id="A5.T11.1.2.2.8">
<span class="ltx_text ltx_font_bold" id="A5.T11.1.2.2.8.1">10</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T11.1.3.3.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.2">10.60</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.3">24.89</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.4">1.99</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.5">1.64</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.6">1.43</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.7">1.18</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.3.3.8">0.95</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.4.4.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.2">28.49</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.3">28.34</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.4">27.58</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.5">26.69</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.6">25.23</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.7">8.76</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.4.4.8">7.10</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T11.1.5.5.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.2">28.75</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A5.T11.1.5.5.3.1">28.61</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.4">27.67</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.5">27.90</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.6">27.22</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.7">27.15</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T11.1.5.5.8">26.24</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.6.6.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.2">27.87</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.3">28.30</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.4">27.55</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.5">27.67</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.6">27.20</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.7">25.87</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.6.6.8">5.84</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.7.7.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.2">28.32</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.3">28.42</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.4">28.15</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.5">28.06</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.6">28.03</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.7">28.25</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.7.7.8"><span class="ltx_text ltx_font_bold" id="A5.T11.1.7.7.8.1">28.53</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.8.8.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.2">28.33</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.3">27.66</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.4"><span class="ltx_text ltx_font_bold" id="A5.T11.1.8.8.4.1">28.68</span></td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.5">28.21</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.6">28.29</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.7"><span class="ltx_text ltx_font_bold" id="A5.T11.1.8.8.7.1">28.35</span></td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.8.8.8">28.52</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.9.9.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.2">28.83</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.3">27.91</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.4">28.17</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.5"><span class="ltx_text ltx_font_bold" id="A5.T11.1.9.9.5.1">28.44</span></td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.6"><span class="ltx_text ltx_font_bold" id="A5.T11.1.9.9.6.1">28.24</span></td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.7">28.28</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.9.9.8">28.05</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.1.10.10.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.2">28.73</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.3">28.15</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.4">28.27</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.5">28.21</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.6">27.85</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.7">27.65</td>
<td class="ltx_td ltx_align_right" id="A5.T11.1.10.10.8">28.10</td>
</tr>
<tr class="ltx_tr" id="A5.T11.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A5.T11.1.11.11.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.2"><span class="ltx_text ltx_font_bold" id="A5.T11.1.11.11.2.1">28.85</span></td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.3">28.15</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.4">27.93</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.5">28.18</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.6">27.67</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.7">28.04</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T11.1.11.11.8">28.23</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Results in terms of BLEU of the <span class="ltx_text ltx_font_bold" id="A5.T11.3.1">En-De</span> IWSLT 2017 experiment for larger context sizes.</figcaption>
</figure>
<figure class="ltx_table" id="A5.T12">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T12.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T12.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A5.T12.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="A5.T12.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.1.1.2.1">Context Size</span></td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.2.1">4</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.3.1">5</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.4.1">6</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.5.1">7</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.6.1">8</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.7">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.7.1">9</span></td>
<td class="ltx_td ltx_align_center" id="A5.T12.1.2.2.8">
<span class="ltx_text ltx_font_bold" id="A5.T12.1.2.2.8.1">10</span></td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T12.1.3.3.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.2">46.09%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.3">44.03%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.4">43.05%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.5">42.07%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.6">42.00%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.7">38.49%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.3.3.8">37.03%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.4.4.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.2">47.02%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.3">44.92%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.4">46.25%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.5">46.48%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.6">43.63%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.7">41.53%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.4.4.8">41.44%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T12.1.5.5.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.2"><span class="ltx_text ltx_font_bold" id="A5.T12.1.5.5.2.1">53.54%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.3">47.68%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.4">46.88%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.5">47.04%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.6">45.79%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.7"><span class="ltx_text ltx_font_bold" id="A5.T12.1.5.5.7.1">48.15%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T12.1.5.5.8"><span class="ltx_text ltx_font_bold" id="A5.T12.1.5.5.8.1">48.88%</span></td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.6.6.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.2">46.57%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.3">46.20%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.4">44.59%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.5">44.91%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.6">43.29%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.7">41.03%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.6.6.8">43.01%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.7.7.1">Shortening - Max P.</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.2">51.75%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.3">47.13%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.4">46.78%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.5">46.73%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.6">46.38%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.7">46.38%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.7.7.8">45.03%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.8.8.1">Shortening - Avg P.</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.2">49.53%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.3"><span class="ltx_text ltx_font_bold" id="A5.T12.1.8.8.3.1">49.43%</span></td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.4">47.90%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.5">45.88%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.6">45.59%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.7">46.27%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.8.8.8">44.66%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.9.9.1">Shortening - Linear P.</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.2">48.45%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.3">46.40%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.4"><span class="ltx_text ltx_font_bold" id="A5.T12.1.9.9.4.1">49.31%</span></td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.5">46.35%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.6">46.90%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.7">45.23%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.9.9.8">45.79%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T12.1.10.10.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.2">49.55%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.3">46.06%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.4">45.10%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.5"><span class="ltx_text ltx_font_bold" id="A5.T12.1.10.10.5.1">47.66%</span></td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.6"><span class="ltx_text ltx_font_bold" id="A5.T12.1.10.10.6.1">47.19%</span></td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.7">46.47%</td>
<td class="ltx_td ltx_align_right" id="A5.T12.1.10.10.8">46.53%</td>
</tr>
<tr class="ltx_tr" id="A5.T12.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A5.T12.1.11.11.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.2">47.88%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.3">48.98%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.4">47.58%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.5">45.58%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.6">45.91%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.7">45.52%</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T12.1.11.11.8">47.43%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Results in terms of the accuracy on the ContraPro contrastive dataset of the models trained on the <span class="ltx_text ltx_font_bold" id="A5.T12.3.1">En-De</span> IWSLT 2017 dataset for larger context sizes.</figcaption>
</figure>
<figure class="ltx_table" id="A5.T13">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T13.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T13.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A5.T13.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="A5.T13.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.1.1.2.1">Context Size</span></td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.2.1">4</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.3.1">5</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.4.1">6</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.5.1">7</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.6.1">8</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.7">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.7.1">9</span></td>
<td class="ltx_td ltx_align_center" id="A5.T13.1.2.2.8">
<span class="ltx_text ltx_font_bold" id="A5.T13.1.2.2.8.1">10</span></td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T13.1.3.3.1">Single-encoder</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.2">0.6266</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.3">0.7376</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.4">0.4425</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.5">0.4253</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.6">0.3950</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.7">0.3738</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.3.3.8">0.3597</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.4.4.1">Multi-encoder</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.2"><span class="ltx_text ltx_font_bold" id="A5.T13.1.4.4.2.1">0.7830</span></td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.3">0.7809</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.4">0.7692</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.5">0.7621</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.6">0.7280</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.7">0.5682</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.4.4.8">0.5187</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T13.1.5.5.1">Caching Tokens</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.2">0.7824</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A5.T13.1.5.5.3.1">0.7826</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.4">0.7773</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.5">0.7744</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.6">0.7682</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.7">0.7560</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A5.T13.1.5.5.8">0.7450</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.6.6.1">Caching Sentence</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.2">0.7766</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.3">0.7741</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.4">0.7680</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.5">0.7680</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.6">0.7637</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.7">0.7413</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.6.6.8">0.5403</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.7.7.1">Shortening - Max Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.2">0.7784</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.3">0.7782</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.4">0.7799</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.5">0.7804</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.6"><span class="ltx_text ltx_font_bold" id="A5.T13.1.7.7.6.1">0.7824</span></td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.7"><span class="ltx_text ltx_font_bold" id="A5.T13.1.7.7.7.1">0.7825</span></td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.7.7.8">0.7790</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.8.8.1">Shortening - Avg Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.2">0.7815</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.3">0.7806</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.4"><span class="ltx_text ltx_font_bold" id="A5.T13.1.8.8.4.1">0.7812</span></td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.5">0.7812</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.6">0.7776</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.7">0.7781</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.8.8.8"><span class="ltx_text ltx_font_bold" id="A5.T13.1.8.8.8.1">0.7814</span></td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.9.9.1">Shortening - Linear Pooling</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.2">0.7803</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.3">0.7810</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.4">0.7802</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.5"><span class="ltx_text ltx_font_bold" id="A5.T13.1.9.9.5.1">0.7816</span></td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.6">0.7780</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.7">0.7808</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.9.9.8">0.7783</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T13.1.10.10.1">Shortening - Grouping</th>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.2">0.7815</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.3">0.7808</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.4">0.7794</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.5">0.7742</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.6">0.7785</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.7">0.7757</td>
<td class="ltx_td ltx_align_right" id="A5.T13.1.10.10.8">0.7789</td>
</tr>
<tr class="ltx_tr" id="A5.T13.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A5.T13.1.11.11.1">Shortening - Selecting</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.2">0.7811</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.3">0.7793</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.4">0.7782</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.5">0.7771</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.6">0.7759</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.7">0.7750</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A5.T13.1.11.11.8">0.7791</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Results in terms of COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib46" title="">2020</a>)</cite> based on <span class="ltx_text ltx_font_typewriter" id="A5.T13.4.1">Unbabel/wmt22-comet-da</span> model <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib45" title="">2022</a>)</cite> of the <span class="ltx_text ltx_font_bold" id="A5.T13.5.2">En-De</span> IWSLT 2017 experiment for larger context sizes.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Groupings and Selections Visualization</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">The visualizations of groupings and selections done by the models using Latent Grouping and Selecting of the additional examples from the ContraPro dataset <cite class="ltx_cite ltx_citemacro_citep">(Müller et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>)</cite> can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6" title="Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F7" title="Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a> shows the visualizations of the groupings and selections of the sentences from the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite>.
</p>
</div>
<figure class="ltx_figure" id="A6.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A6.F5.sf1.g1" src="extracted/5385333/Figures/Groups_En_De/groups_4.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A6.F5.sf2.g1" src="extracted/5385333/Figures/Selecting_En_De/groups_4.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Latent Selecting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="706" id="A6.F5.sf3.g1" src="extracted/5385333/Figures/Groups_En_De/groups_16.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="706" id="A6.F5.sf4.g1" src="extracted/5385333/Figures/Selecting_En_De/groups_16.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Latent Selecting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="706" id="A6.F5.sf5.g1" src="extracted/5385333/Figures/Groups_En_De/groups_18.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F5.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="706" id="A6.F5.sf6.g1" src="extracted/5385333/Figures/Selecting_En_De/groups_18.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Latent Selecting</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of tokens of the sentences from the ContraPro dataset <cite class="ltx_cite ltx_citemacro_citep">(Müller et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib40" title="">2018</a>)</cite> grouped (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf1" title="5(a) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf3" title="5(c) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(c)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf5" title="5(e) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(e)</span></a>) and selected (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf2" title="5(b) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(b)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf4" title="5(d) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(d)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F5.sf6" title="5(f) ‣ Figure 6 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">5(f)</span></a>) by the model using Latent Grouping and Latent Selecting.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="564" id="A6.F6.sf1.g1" src="extracted/5385333/Figures/Groups_En_Fr/groups_18.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="564" id="A6.F6.sf2.g1" src="extracted/5385333/Figures/Selecting_En_Fr/groups_18.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Latent Selecting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="564" id="A6.F6.sf3.g1" src="extracted/5385333/Figures/Groups_En_Fr/groups_21.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="564" id="A6.F6.sf4.g1" src="extracted/5385333/Figures/Selecting_En_Fr/groups_21.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Latent Selecting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="903" id="A6.F6.sf5.g1" src="extracted/5385333/Figures/Groups_En_Fr/groups_19.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Latent Grouping</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A6.F6.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="903" id="A6.F6.sf6.g1" src="extracted/5385333/Figures/Selecting_En_Fr/groups_19.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Latent Selecting</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of tokens of the sentences from the contrastive dataset by <cite class="ltx_cite ltx_citemacro_citet">Lopes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#bib.bib29" title="">2020</a>)</cite> grouped (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf1" title="6(a) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf3" title="6(c) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf5" title="6(e) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(e)</span></a>) and selected (<a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf2" title="6(b) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf4" title="6(d) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(d)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.01416v1#A6.F6.sf6" title="6(f) ‣ Figure 7 ‣ Appendix F Groupings and Selections Visualization ‣ Sequence Shortening for Context-Aware Machine Translation"><span class="ltx_text ltx_ref_tag">6(f)</span></a>) by the model using Latent Grouping and Latent Selecting.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Feb  2 13:31:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
