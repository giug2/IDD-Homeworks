<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.01718] A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge</title><meta property="og:description" content="The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.01718">

<!--Generated on Mon Mar 11 20:03:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id8.id1" class="ltx_text">A-OKVQA</span>: A Benchmark for Visual Question Answering using World Knowledge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Dustin Schwenk<sup id="id9.8.id1" class="ltx_sup">1</sup>, Apoorv Khandelwal<sup id="id10.9.id2" class="ltx_sup">1</sup>, Christopher Clark<sup id="id11.10.id3" class="ltx_sup">1</sup> 
<br class="ltx_break"><span id="id4.4.1" class="ltx_text ltx_font_bold">Kenneth Marino<sup id="id4.4.1.1" class="ltx_sup"><span id="id4.4.1.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>, <span id="id5.5.2" class="ltx_text ltx_font_bold">Roozbeh Mottaghi<sup id="id5.5.2.1" class="ltx_sup"><span id="id5.5.2.1.1" class="ltx_text ltx_font_medium">1</span></sup></span> 
<br class="ltx_break"><sup id="id12.11.id4" class="ltx_sup">1</sup> PRIOR @ Allen Institute for AI
<br class="ltx_break"><sup id="id13.12.id5" class="ltx_sup">2</sup> Carnegie Mellon University
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">The Visual Question Answering (VQA) task aspires to provide a meaningful testbed for the development of AI models that can jointly reason over visual and natural language inputs. Despite a proliferation of VQA datasets, this goal is hindered by a set of common limitations. These include a reliance on relatively simplistic questions that are repetitive in both concepts and linguistic structure, little world knowledge needed outside of the paired image, and limited reasoning required to arrive at the correct answer. We introduce <span id="id14.id1.1" class="ltx_text">A-OKVQA</span>, a crowdsourced dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer. In contrast to the existing knowledge-based VQA datasets, the questions generally cannot be answered by simply querying a knowledge base, and instead require some form of commonsense reasoning about the scene depicted in the image. We demonstrate the potential of this new dataset through a detailed analysis of its contents and baseline performance measurements over a variety of state-of-the-art vision–language models. 
<br class="ltx_break"></p>
<p id="id15.id2" class="ltx_p ltx_align_center"><span id="id15.id2.1" class="ltx_text"><a target="_blank" href="http://a-okvqa.allenai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:120%;">http://a-okvqa.allenai.org/</a></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The original conception of the Visual Question Answering (VQA) problem was as a Visual Turing Test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Can we give a computer an image and expect it to answer any question we ask to fool us into thinking it is a human? To truly solve this Turing Test, the computer would need to mimic several human capabilities including: visual recognition in the wild, language understanding, basic reasoning capabilities and a background knowledge about the world. Since the VQA problem was formulated, many of these aspects have been studied. Early datasets mostly studied the perception and language understanding problem on natural image datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Other datasets studied complex chains of reasoning about procedurally generated images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. More recently, datasets include questions which require factual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> or commonsense knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">But, to a large extent, VQA has been a victim of its own success. With the advent of large-scale pre-training of vision and language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and other breakthroughs in multi-modal architectures, much of the low-hanging fruit in the field has been plucked and many of the benchmark datasets have seen saturated performance. Even performance on the newer knowledge-based datasets has been improved by such models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. So how can we continue developing yet more challenging datasets? What human capabilities are not yet expressed by current models?</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2206.01718/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="421" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">A-OKVQA</span><span id="S1.F1.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;"> dataset.<span id="S1.F1.6.3.1" class="ltx_text ltx_font_medium"> The dataset includes questions that require reasoning using a variety of knowledge types such as commonsense, world knowledge and visual knowledge. We provide Multiple-Choice (MC) as well as Direct Answer evaluation settings. There is a rationale associated to each question in the train set providing the explanation/knowledge for answering the question.</span></span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose the following. First, continuing the direction of past work in knowledge-requiring VQA, we further expand the areas of knowledge required. Our dataset requires diverse forms of outside knowledge including explicit fact-based knowledge that is likely to be contained in knowledge bases, commonsense knowledge about human social behavior, knowledge about the physics of the world, and visual knowledge. Not only do we expand the variety of knowledge our agent needs, but we also increase the complexity of reasoning systems needed to answer questions. We need models to recognize the image, understand the question, recall relevant knowledge, and use reasoning to arrive at an answer. For instance, in the first question shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the model should reason that people use that type of cart to avoid walking. Therefore, the old man should have trouble walking. In general, our dataset requires additional types of world knowledge compared to our previous work OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Hence, we call it Augmented OK-VQA (<span id="S1.p3.1.1" class="ltx_text">A-OKVQA</span>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text">A-OKVQA</span> is composed of about 25K questions paired with both multiple choice (MC) answer options and ten free-form answers to allow for direct answer (DA) evaluation. The MC component of the dataset bypasses many difficulties inherent in direct answer evaluation and allows for a simple, clean accuracy score. This is particularly helpful given the greater variety in answers in <span id="S1.p4.1.2" class="ltx_text">A-OKVQA</span> questions. At the same time, we believe direct answer evaluation is important to encourage models with more real-world applicability. In addition to the questions and answers, we provide <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">rationales</em> for each question. This is to allow for models to use this extra annotation to train reasoning or knowledge retrieval methods or to build more explainable VQA models. The rationales also validate that both reasoning and knowledge are required for answering questions in the dataset.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, our contributions are: (i) A new benchmark VQA dataset requiring diverse sources of outside knowledge and reasoning; (ii) A detailed analysis of the dataset that highlights its diversity and difficulty; (iii) An evaluation of a variety of recent baseline approaches in the context of the challenging questions in <span id="S1.p5.1.1" class="ltx_text">A-OKVQA</span>; (iv) An extensive analysis of the results leading to interesting findings (e.g., how well models perform when answers are in the tail of the distribution, and also the complementarity of the studied models).
We will release this dataset publicly.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering.</span>
Visual Question Answering (VQA) has been a common and popular form of vision and language reasoning. Many datasets on this task have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> but most of these do not require much outside knowledge or reasoning, often focusing on recognition tasks such as classification, attribute detection and counting.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Knowledge-based VQA datasets.</span> Several previous works have studied the problem of knowledge-based VQA. The earliest explicitly knowledge-based VQA datasets were KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. While these benchmarks did specifically require knowledge for questions, the knowledge required for these benchmarks is completely “closed”. FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> is annotated by selecting a triplet from a fixed knowledge graph. This forces the questions to require knowledge, but because the question is written based on this knowledge, these questions are fairly trivial once the knowledge is known and do not require much reasoning. In addition, the knowledge required is explicitly closed to the knowledge graphs used to generate the dataset, so these datasets can only test knowledge retrieval on those specific graphs. KVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> is based on images in Wikipedia articles. Because of the source of the images, these questions tend to mostly test recognizing specific named entities (e.g., Barrack Obama) and then retrieving Wikipedia knowledge about that entity rather than commonsense knowledge.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Most similar to our work is OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. This dataset was an improvement over prior work in terms of scale, and the quality of questions and images. It also has the property that the required knowledge was not “closed” or explicitly drawn from a particular source, and could be called “open”-domain knowledge. While this is an improvement over the previous works, it still suffers from problems which we address in this work. The knowledge required, while “open” is still biased towards simple lookup knowledge (e.g., what is the capital of this country?) and most questions do not require much reasoning. In contrast, our dataset is explicitly drawn to rely on more common-sense knowledge and to require more reasoning to solve. In addition, our dataset includes “rationale” annotations, which allow knowledge-based VQA systems to more densely annotate their knowledge acquisition and reasoning capabilities.
S3VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> analyzes OK-VQA and creates a new dataset which includes questions that require detecting an object in the image, replacing the question with the word for that object and then querying the web to find the answer. Like OK-VQA, it even more explicitly has the problem of questions usually requiring a single retrieval rather than much commonsense knowledge or reasoning.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Another related line of work is Visual Commonsense Reasoning (VCR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and VisualCOMET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. VCR is also a VQA dataset, but is collected from movie scenes and is quite focused on humans and their intentions (e.g. “why is [PERSON2] doing this”), whereas our dataset considers questions and knowledge about a variety of objects. Similarly, VisualCOMET tests commonsense language and vision models on a movie dataset, but its expected output is a scene graph for the image (e.g., “After, [PERSON] is likely to”). Additionally, the Ads Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a dataset requiring knowledge about the topic and sentiments of the ads. Other datasets have considered knowledge-based question answering for a sitcom <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and by using web queries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Explanation / Reasoning VQA.</span> Visual reasoning on its own has been studied in several VQA datasets. In CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the image and question are automatically generated from templates and explicitly require models to go through multiple steps of reasoning to correctly answer. This dataset and similar datasets which rely on simulated images suffer from lack of visual realism and lack of richness in the images and questions and are thus prone to be overfit to with methods achieving nearly 100% accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. Our dataset requires reasoning on real images and free-form language. Other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> have collected or extracted justifications on the VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> dataset. However, VQAv2 mostly focuses on questions about object attributes, counting and activities, which do not require reasoning on outside knowledge.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Knowledge / Commonsense in NLP.</span>
Question answering with knowledge and commonsense is also a well-studied problem in natural language processing. This takes the form of knowledge base completion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, knowledge-based question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to open-domain question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> address question answering from specific knowledge sources. This includes open-domain question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and question answering from Wikipedia SQu–AD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Much work has also been done in commonsense question answering as in CommonsenseQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, where there is no direct source of knowledge but the agent must have general “commonsense” to answer the question.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text">A-OKVQA</span> Collection</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold">Image source.</span> The first requirement of an image source for this knowledge-based VQA task is that it has an abundance of visually rich and interesting images. Images containing a small number of objects are typically quite challenging to write questions requiring outside knowledge to answer. We used images from the 2017 partitioning of the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> in the creation of <span id="S3.p1.1.2" class="ltx_text">A-OKVQA</span> because: (1) it has many images cluttered with multiple objects and entity types, (2) it is an established dataset with many associated models already in existence. To ensure suitable images for annotation, we do some additional filtering to remove uninteresting images: For the training and validation sets, we define images with more than three objects as “interesting” and select those for question writing. For the test set, which lacks object annotation, we train a ResNet-50 classifier to distinguish “interesting” images based on this criteria, achieving an accuracy of <span id="S3.p1.1.3" class="ltx_text ltx_markedasmath ltx_font_bold">78%</span> on the validation set. After multiple rounds of filtering (described below), we obtain 23.7K unique images.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Question collection &amp; filtering.</span>
The questions in <span id="S3.p2.1.2" class="ltx_text">A-OKVQA</span> were written and refined over several rounds of annotation by 437 crowd-workers on the Amazon Mechanical Turk platform and refined through several manual and automated filtering steps to increase overall quality. As a first quality assurance measure, workers completed a qualification task to demonstrate their ability to write questions that met our criteria, namely that questions require: (1) looking at the image to answer, (2) some commonsense or specialized knowledge, (3) some thinking beyond merely recognizing an object, and (4) not be too similar to previous questions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To help ensure the last point, we clustered images by CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> visual features and batched similar images together so that the same worker wrote questions sequentially for related images (e.g., a worker might write questions for several images showing baseball games in one task) to cut down on repetitive questions. As an added measure to encourage question diversity, we maintained a database of questions written and required users to check a new question against these by displaying the five previous questions most similar in terms of their RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> embeddings. We used Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> pre-trained on VQAv2 as a first automated check for questions we considered trivial, removing any question for which the model predicted the correct answer choice. Next, questions were screened by three other workers and only included if the majority agreed that it met our criteria for inclusion. In all, 37,687 questions, or <span id="S3.p3.1.1" class="ltx_text ltx_markedasmath ltx_font_bold">60%</span> of post-qualification questions were excluded from the dataset by this process. After questions and their multiple choice options were complete, nine additional free-form answers were collected for each question by a separate pool of workers.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Rationales.</span> After questions and multiple-choice answer options were collected and validated, we initiated a task to collect rationales. Workers were given a question and answer options and asked to explain in one to two simple sentences why a particular answer was correct, including any necessary facts or knowledge about the world not contained in the images. Workers were given examples and went through a qualification process to assure high-quality output. For each question, we collected three rationales.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Statistics</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Question/Answer/Rationale statistics.</span>
After all rounds of annotation, the <span id="S4.p1.1.2" class="ltx_text">A-OKVQA</span> dataset contains 24,903 Question/Answer/Rationale triplets, split into 17.1K/1.1K/6.7K for train, validation and test. These preserve the COCO 2017 train/val/test splits. The average length of the questions, answers, and rationales, and the number of their unique words are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.01718/assets/x2.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="426" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F2.sf1.4.2" class="ltx_text" style="font-size:90%;">Answer occurrence distribution of <span id="S4.F2.sf1.4.2.1" class="ltx_text">A-OKVQA</span>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.01718/assets/x3.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Knowledge type distribution for a random subset of the dataset.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset statistics.</span></figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In Figure <a href="#S4.F2.sf1" title="In Figure 2 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> we show the distribution of answer options in our dataset. What we see is a fairly typical long-tail distribution of labels, as is seen in many open-labeled image tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. A few answers occur quite often in the dataset, but overall, most answers in the dataset fall into the long tail of the distribution.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.2" class="ltx_p">We are also interested to know the amount of overlap in the answer set between the train set and val and test sets. We find that in the val set, the ground-truth answer for <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="87.6\%" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">87.6</mn><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">87.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">87.6\%</annotation></semantics></math> of questions appears in the train set while in the test set <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="82.7\%" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mn id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">82.7</mn><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">82.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">82.7\%</annotation></semantics></math> do. This shows that there is indeed some reasonable similarity between the sets, but also that a significant portion of the held out sets require an answer that the model will not have seen during training, requiring the model to be able to generate out-of-distribution answers or generate answers based on some knowledge outside of the dataset.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Comparison with other datasets.</span> In Table <a href="#S4.T1" title="Table 1 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we show dataset properties and statistics for <span id="S4.p4.1.2" class="ltx_text">A-OKVQA</span> compared to related datasets. We see that compared to the more knowledge-focused natural image datasets such as OK-VQA, we have between 2-10x more questions while VCR (focused on images of people in movies) has about 10x as ours. This is unsurprising because we intentionally filter similar questions, making our questions more diverse (see Table <a href="#S4.T2" title="Table 2 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), but difficult to collect at scale. Our dataset has annotations for both multiple choice and direct answer evaluation. Our dataset also has rationales, unlike OK-VQA, S3VQA and KB-VQA. FVQA has knowledge tuples as rationales rather than full sentences. Most similar to our rationales is VCR. Unlike all of these, we collect 3 rationales rather than just 1 as the rationales are more knowledge-based and have more possible variation within the same question. Our average question lengths are long compared to many of these datasets except for S3VQA which has the longest and VCR which is about on par. Ours also contains the most unique words except for VCR, likely because that dataset has more questions.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.2.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:8.5pt;">
<span id="S4.T1.2.1.1.2.1.1.1" class="ltx_p">Q</span>
</span></span></th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.3.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.3.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:2.8pt;">
<span id="S4.T1.2.1.1.3.1.1.1" class="ltx_p">I</span>
</span></span></th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.4.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.4.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:25.6pt;">
<span id="S4.T1.2.1.1.4.1.1.1" class="ltx_p">Rationale</span>
</span></span></th>
<th id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.5.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.5.1.1" class="ltx_inline-block">
<span id="S4.T1.2.1.1.5.1.1.1" class="ltx_p">Knowledge</span>
<span id="S4.T1.2.1.1.5.1.1.2" class="ltx_p">type</span>
</span></span></th>
<th id="S4.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.6.1" class="ltx_text" style="font-size:70%;">Ans type</span></th>
<th id="S4.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.7.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.7.1.1" class="ltx_inline-block">
<span id="S4.T1.2.1.1.7.1.1.1" class="ltx_p">Avg. length</span>
<span id="S4.T1.2.1.1.7.1.1.2" class="ltx_p">(Q/A/R)</span>
</span></span></th>
<th id="S4.T1.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.2.1.1.8.1" class="ltx_text" style="font-size:70%;">
<span id="S4.T1.2.1.1.8.1.1" class="ltx_inline-block">
<span id="S4.T1.2.1.1.8.1.1.1" class="ltx_p">unique words</span>
<span id="S4.T1.2.1.1.8.1.1.2" class="ltx_p">(Q/A/R)</span>
</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.1.1.1" class="ltx_text" style="font-size:70%;">KB-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S4.T1.2.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.2.1" class="ltx_text" style="font-size:70%;">2,402</span></td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.3.1" class="ltx_text" style="font-size:70%;">700</span></td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.4.1" class="ltx_text" style="font-size:70%;">✘</span></td>
<td id="S4.T1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.5.1" class="ltx_text" style="font-size:70%;">fixed KB</span></td>
<td id="S4.T1.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.6.1" class="ltx_text" style="font-size:70%;">DA</span></td>
<td id="S4.T1.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.7.1" class="ltx_text" style="font-size:70%;">6.8/2.0/NA</span></td>
<td id="S4.T1.2.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.1.8.1" class="ltx_text" style="font-size:70%;">530/1,296/NA</span></td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<th id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.2.3.2.1.1" class="ltx_text" style="font-size:70%;">FVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S4.T1.2.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.2.1" class="ltx_text" style="font-size:70%;">5,826</span></td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.3.1" class="ltx_text" style="font-size:70%;">2,190</span></td>
<td id="S4.T1.2.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S4.T1.2.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.5.1" class="ltx_text" style="font-size:70%;">fixed KB</span></td>
<td id="S4.T1.2.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.6.1" class="ltx_text" style="font-size:70%;">DA</span></td>
<td id="S4.T1.2.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.7.1" class="ltx_text" style="font-size:70%;">9.5/1.2/NA</span></td>
<td id="S4.T1.2.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T1.2.3.2.8.1" class="ltx_text" style="font-size:70%;">3,010/1,287/NA</span></td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<th id="S4.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.2.4.3.1.1" class="ltx_text" style="font-size:70%;">OK-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T1.2.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.2.1" class="ltx_text" style="font-size:70%;">14,055</span></td>
<td id="S4.T1.2.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.3.1" class="ltx_text" style="font-size:70%;">14,031</span></td>
<td id="S4.T1.2.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.4.1" class="ltx_text" style="font-size:70%;">✘</span></td>
<td id="S4.T1.2.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.5.1" class="ltx_text" style="font-size:70%;">factoid</span></td>
<td id="S4.T1.2.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.6.1" class="ltx_text" style="font-size:70%;">DA</span></td>
<td id="S4.T1.2.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.7.1" class="ltx_text" style="font-size:70%;">8.1/1.3/NA</span></td>
<td id="S4.T1.2.4.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.2.4.3.8.1" class="ltx_text" style="font-size:70%;">5,703/11,125/NA</span></td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<th id="S4.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.2.5.4.1.1" class="ltx_text" style="font-size:70%;">S3VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.T1.2.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.2.1" class="ltx_text" style="font-size:70%;">7,515</span></td>
<td id="S4.T1.2.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.3.1" class="ltx_text" style="font-size:70%;">7,515</span></td>
<td id="S4.T1.2.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.4.1" class="ltx_text" style="font-size:70%;">✘</span></td>
<td id="S4.T1.2.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.5.1" class="ltx_text" style="font-size:70%;">factoid</span></td>
<td id="S4.T1.2.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.6.1" class="ltx_text" style="font-size:70%;">DA</span></td>
<td id="S4.T1.2.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.7.1" class="ltx_text" style="font-size:70%;">12.7/2.8/NA</span></td>
<td id="S4.T1.2.5.4.8" class="ltx_td ltx_align_center"><span id="S4.T1.2.5.4.8.1" class="ltx_text" style="font-size:70%;">7,515/8,301/NA</span></td>
</tr>
<tr id="S4.T1.2.6.5" class="ltx_tr">
<th id="S4.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.2.6.5.1.1" class="ltx_text" style="font-size:70%;">VCR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.6.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib66" title="" class="ltx_ref">66</a><span id="S4.T1.2.6.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T1.2.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.2.1" class="ltx_text" style="font-size:70%;">290k</span></td>
<td id="S4.T1.2.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.3.1" class="ltx_text" style="font-size:70%;">99,904</span></td>
<td id="S4.T1.2.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S4.T1.2.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.5.1" class="ltx_text" style="font-size:70%;">people actions</span></td>
<td id="S4.T1.2.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.6.1" class="ltx_text" style="font-size:70%;">MC</span></td>
<td id="S4.T1.2.6.5.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.7.1" class="ltx_text" style="font-size:70%;">8.7/7.7/16.8</span></td>
<td id="S4.T1.2.6.5.8" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.5.8.1" class="ltx_text" style="font-size:70%;">11,254/18,861/28,751</span></td>
</tr>
<tr id="S4.T1.2.7.6" class="ltx_tr">
<th id="S4.T1.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.2.7.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">A-OKVQA</span></th>
<td id="S4.T1.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.2.1" class="ltx_text" style="font-size:70%;">24,903</span></td>
<td id="S4.T1.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.3.1" class="ltx_text" style="font-size:70%;">23,692</span></td>
<td id="S4.T1.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.4.1" class="ltx_text" style="font-size:70%;">✓</span></td>
<td id="S4.T1.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.5.1" class="ltx_text" style="font-size:70%;">common/world</span></td>
<td id="S4.T1.2.7.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.6.1" class="ltx_text" style="font-size:70%;">DA/MC</span></td>
<td id="S4.T1.2.7.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.7.1" class="ltx_text" style="font-size:70%;">8.8/1.3/11.0</span></td>
<td id="S4.T1.2.7.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.7.6.8.1" class="ltx_text" style="font-size:70%;">7,248/17,683/20,629</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.8.1.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S4.T1.9.2" class="ltx_text ltx_font_bold" style="font-size:129%;">Comparison of various knowledge-based VQA datasets.<span id="S4.T1.9.2.1" class="ltx_text ltx_font_medium"> Data based on publicly reported numbers and/or our analysis of publicly available annotations (therefore some answer statistics may exclude test sets). Answer statistics for <span id="S4.T1.9.2.1.1" class="ltx_text">A-OKVQA</span> based on the direct answer set. Q: question, I: image, A: answer, R: rationale, DA: Direct Answer, MC: Multiple Choice. NA indicates lack of rationales or, for FVQA are KB triplets, so we do not compare the lengths.</span></span></figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Knowledge types.</span> The most significant factor differentiating our dataset is the kind of knowledge required. Datasets such as FVQA have fixed knowledge bases that are used to write the questions, and so the knowledge required can be found in e.g. ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> directly. OK-VQA and S3VQA focus on more factoid knowledge (e.g., years of invention or countries of origin). In S3VQA in particular, researchers found that these datasets take the form of finding an entity in the image and/or question and searching and retrieving knowledge about that particular entity (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>). VCR is overwhelmingly images of people interacting in television shows and movies and requires images to have people in them. Thus, the required knowledge is very focused on commonsense about human behavior and intentions. In our dataset, we require broader areas of knowledge including the factoid knowledge likely to be contained in knowledge bases (as in FVQA, KBVQA, OKVQA and S3VQA), and commonsense knowledge (like VCR but broader than just about people).</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">To analyze the knowledge required in <span id="S4.p6.1.1" class="ltx_text">A-OKVQA</span> more quantitatively, we annotated a randomly sampled subset of 1,000 questions in the <span id="S4.p6.1.2" class="ltx_text">A-OKVQA</span> test. In this experiment, we ask the annotators to label what kind of knowledge type was required to answer the questions. The choices were: (1) <span id="S4.p6.1.3" class="ltx_text ltx_font_bold">Commonsense</span> knowledge about human social behavior (e.g., that many donuts being made in a cart implies they are for sale rather than for personal consumption), (2) <span id="S4.p6.1.4" class="ltx_text ltx_font_bold">Visual knowledge</span> (e.g., muted color pallets are associated with the 1950s), (3) <span id="S4.p6.1.5" class="ltx_text ltx_font_bold">Knowledge bases</span> (e.g., hot dogs were invented in Austria), (4) <span id="S4.p6.1.6" class="ltx_text ltx_font_bold">Physical knowledge</span> about the world that humans learn from their everyday experiences (e.g., shaded areas have a lower temperature than other areas). The distribution is shown in Figure <a href="#S4.F2.sf2" title="In Figure 2 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. Most of our questions focus around commonsense and visual knowledge. It should be noted that sometimes there is no clear distinction between these two categories, and a question can belong to either category.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">Question diversity.</span> To analyze the diversity of <span id="S4.p7.1.2" class="ltx_text">A-OKVQA</span> compared to other datasets, as a proxy, we use the average pairwise cosine distance between the questions in the dataset. We run our questions through a sentence transformer<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Specifically multi-qa-MiniLM-L6-cos-v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to avoid overlap with RoBERTa.</span></span></span> and compute the cosine distance between all pairs in the dataset. We then take the average of these. We see from Table <a href="#S4.T2" title="Table 2 ‣ 4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> that our dataset has the most diversity on this metric. In particular, we see a large difference compared to VCR which has many similar questions such as “What is going to happen next?” and questions relating to what specific people in the scene are doing and why. We also compare the diversity of rationales to VCR and VQAv2 (using rationales from VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> rationales). We also find that our rationales are much more diverse than in these datasets. Qualitatively, we also find that our dataset tends to have much more varied questions because it is taken from the more visually diverse COCO dataset (a quality shared by OK-VQA and VQAv2 which do almost as well on this metric) and requires more diverse kinds of knowledge.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">Mean Q distance</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.1.1.3.1" class="ltx_text" style="font-size:70%;">Mean rationale distance</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<th id="S4.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">FVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S4.T2.2.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.2.1.2.1" class="ltx_text" style="font-size:70%;">0.6199</span></td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.2.1.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<th id="S4.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.3.2.1.1" class="ltx_text" style="font-size:70%;">VCR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib66" title="" class="ltx_ref">66</a><span id="S4.T2.2.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.3.2.2.1" class="ltx_text" style="font-size:70%;">0.7095</span></td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.3.2.3.1" class="ltx_text" style="font-size:70%;">0.8017</span></td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<th id="S4.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.4.3.1.1" class="ltx_text" style="font-size:70%;">KB-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S4.T2.2.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.4.3.2.1" class="ltx_text" style="font-size:70%;">0.7192</span></td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.4.3.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<th id="S4.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.5.4.1.1" class="ltx_text" style="font-size:70%;">S3VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.T2.2.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.5.4.2.1" class="ltx_text" style="font-size:70%;">0.8050</span></td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.5.4.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
</tr>
<tr id="S4.T2.2.6.5" class="ltx_tr">
<th id="S4.T2.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.6.5.1.1" class="ltx_text" style="font-size:70%;">VQAv2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.6.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T2.2.6.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.6.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.6.5.2.1" class="ltx_text" style="font-size:70%;">0.8405</span></td>
<td id="S4.T2.2.6.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.6.5.3.1" class="ltx_text" style="font-size:70%;">0.8228</span></td>
</tr>
<tr id="S4.T2.2.7.6" class="ltx_tr">
<th id="S4.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S4.T2.2.7.6.1.1" class="ltx_text" style="font-size:70%;">OK-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.7.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S4.T2.2.7.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S4.T2.2.7.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.7.6.2.1" class="ltx_text" style="font-size:70%;">0.8428</span></td>
<td id="S4.T2.2.7.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.7.6.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
</tr>
<tr id="S4.T2.2.8.7" class="ltx_tr">
<th id="S4.T2.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.8.7.1.1" class="ltx_text" style="font-size:70%;">A-OKVQA</span></th>
<td id="S4.T2.2.8.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.8.7.2.1" class="ltx_text" style="font-size:70%;">0.8564</span></td>
<td id="S4.T2.2.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.2.8.7.3.1" class="ltx_text" style="font-size:70%;">0.8779</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.11.1.1" class="ltx_text" style="font-size:129%;">Table 2</span>: </span><span id="S4.T2.12.2" class="ltx_text ltx_font_bold" style="font-size:129%;">Question and Rationale Diversity.<span id="S4.T2.12.2.1" class="ltx_text ltx_font_medium"> Mean pairwise cosine distances in a sentence transformer space for various datasets. ✗ indicates lack of rationale. We choose one rationale per question on <span id="S4.T2.12.2.1.1" class="ltx_text">A-OKVQA</span> to make the comparison to other datasets with only one rationale. Rationales for VQAv2 come from the VQA-X. dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</span></span></figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.2" class="ltx_p">Finally, we use the same mean pairwise distance to look in particular at how different our questions are from OK-VQA which is the most similar prior work to ours. To do this we compare the minimum pairwise distance between every question in the OK-VQA training set to every question in the OK-VQA test set and our test set. We find that the average minimum distance from OK-VQA train to test is <span id="S4.p8.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.256</span> compared to <span id="S4.p8.2.2" class="ltx_text ltx_markedasmath ltx_font_bold">0.311</span> between OK-VQA train and our test set<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>To make this comparison even, we chose a random subset of our test set to be the same size as OK-VQA test set so that the minimum is over the same number of possible choices in both cases.</span></span></span>. This shows that there is in fact a significant difference between our question set and OK-VQA in this feature space.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Next, we benchmark the <span id="S5.p1.1.1" class="ltx_text">A-OKVQA</span> dataset and compare the performance of different models. We consider three classes of methods: (1) <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">large-scale pre-trained models</span> such as CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, (2) <span id="S5.p1.1.3" class="ltx_text ltx_font_bold">models that generate and use rationales</span>, and (3) <span id="S5.p1.1.4" class="ltx_text ltx_font_bold">specialized models</span> that are designed for knowledge-based VQA (KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>) or tested for VQA (e.g., VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In the <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">multiple choice (MC)</em> setting, a model chooses its answer from one of four options and we compute accuracy as the evaluation metric. In the <em id="S5.SS1.p1.1.2" class="ltx_emph ltx_font_italic">direct answer (DA)</em> setting, a model can generate any text as its answer and we use the standard VQA evaluation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Large-scale Pre-trained Models</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We compare three types of large-scale pre-trained models (discriminative, contrastive, and generative) in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We also test these models in different input settings (where questions, images, or both are provided).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We compute BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and CLIP ViT-B/32 text encoder representations for questions. We also compute ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and CLIP ViT-B/32 features for images. These are provided as inputs to the appropriate discriminative and contrastive models. We provide questions as tokens and CLIP RN50x4 image representations as inputs to the generative models. We generate a vocabulary from a subset of training set answers and choices to use across all appropriate models. We describe this vocabulary further in Appx. <a href="#A2" title="Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Discriminative models.</span>
We train a multi-label linear classifier (i.e. MLP with one hidden layer and sigmoid activation function) on top of BERT (row <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">d</span>), ResNet (row <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">i</span>), and CLIP (rows <span id="S5.SS2.p3.1.4" class="ltx_text ltx_font_italic">e/j/m</span>) representations to score answers from the vocabulary. When questions and images are both provided, we first concatenate their representations. For the DA setting, we predict the top scoring vocabulary answer. For the MC setting, we instead predict the nearest neighbor<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Cosine similarity between mean GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> word embeddings.</span></span></span> choice to the top scoring vocabulary answer.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Contrastive models.</span>
We also evaluate models which match input questions and/or images with answers using their CLIP encodings. First, we evaluate the zero-shot setting (rows <span id="S5.SS2.p4.1.2" class="ltx_text ltx_font_italic">f/k/n</span>). If both questions and images are provided as inputs, we first add their representations. We select the answer whose encoding has the greatest cosine similarity to our input representation. We select from vocabulary answers in DA and the given choices in MC.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">We also train a single-layer MLP on top of our input representations (rows <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_italic">g/l/o</span>). If both questions and images are provided, we first concatenate their representations. Our MLP produces a 512-d embedding and we train this with a CLIP-style contrastive loss between embeddings and their corresponding answers. We describe this loss further in Appx. <a href="#A2" title="Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. We repeat the evaluation from the zero-shot setting, using these learned embeddings.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T3.2.1.1.2.1" class="ltx_text" style="font-size:70%;color:#3B78D6;">Multiple Choice</span></td>
<td id="S5.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T3.2.1.1.3.1" class="ltx_text" style="font-size:70%;color:#E68F36;">Direct Answer</span></td>
</tr>
<tr id="S5.T3.2.2.2" class="ltx_tr">
<th id="S5.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
<td id="S5.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
</tr>
<tr id="S5.T3.2.3.3" class="ltx_tr">
<th id="S5.T3.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.3.3.1.1" class="ltx_text" style="font-size:70%;">(a) Random</span></th>
<td id="S5.T3.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.3.3.2.1" class="ltx_text" style="font-size:70%;">26.70</span></td>
<td id="S5.T3.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.3.3.3.1" class="ltx_text" style="font-size:70%;">25.36</span></td>
<td id="S5.T3.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.3.3.4.1" class="ltx_text" style="font-size:70%;">0.03</span></td>
<td id="S5.T3.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.3.3.5.1" class="ltx_text" style="font-size:70%;">0.06</span></td>
</tr>
<tr id="S5.T3.2.4.4" class="ltx_tr">
<th id="S5.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.4.4.1.1" class="ltx_text" style="font-size:70%;">(b) Random (weighted)</span></th>
<td id="S5.T3.2.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.4.4.2.1" class="ltx_text" style="font-size:70%;">29.49</span></td>
<td id="S5.T3.2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.4.4.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">30.87</span></td>
<td id="S5.T3.2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.4.4.4.1" class="ltx_text" style="font-size:70%;">0.15</span></td>
<td id="S5.T3.2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.4.4.5.1" class="ltx_text" style="font-size:70%;">0.10</span></td>
</tr>
<tr id="S5.T3.2.5.5" class="ltx_tr">
<th id="S5.T3.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.5.5.1.1" class="ltx_text" style="font-size:70%;">(c) Most Common</span></th>
<td id="S5.T3.2.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">30.70</span></td>
<td id="S5.T3.2.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.5.5.3.1" class="ltx_text" style="font-size:70%;">30.33</span></td>
<td id="S5.T3.2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">1.75</span></td>
<td id="S5.T3.2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">1.26</span></td>
</tr>
<tr id="S5.T3.2.6.6" class="ltx_tr">
<th id="S5.T3.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Question</span></th>
<td id="S5.T3.2.6.6.2" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.6.6.3" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.6.6.4" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.6.6.5" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
</tr>
<tr id="S5.T3.2.7.7" class="ltx_tr">
<th id="S5.T3.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.7.7.1.1" class="ltx_text" style="font-size:70%;">(d) BERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.7.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S5.T3.2.7.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.7.7.1.4" class="ltx_text" style="font-size:70%;"> (classifier)</span>
</th>
<td id="S5.T3.2.7.7.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.7.7.2.1" class="ltx_text" style="font-size:70%;">32.93</span></td>
<td id="S5.T3.2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.7.7.3.1" class="ltx_text" style="font-size:70%;">33.54</span></td>
<td id="S5.T3.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.7.7.4.1" class="ltx_text" style="font-size:70%;">9.52</span></td>
<td id="S5.T3.2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.7.7.5.1" class="ltx_text" style="font-size:70%;">8.41</span></td>
</tr>
<tr id="S5.T3.2.8.8" class="ltx_tr">
<th id="S5.T3.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.8.8.1.1" class="ltx_text" style="font-size:70%;">(e) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.8.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.8.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.8.8.1.4" class="ltx_text" style="font-size:70%;"> (classifier)</span>
</th>
<td id="S5.T3.2.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.8.8.2.1" class="ltx_text" style="font-size:70%;">32.74</span></td>
<td id="S5.T3.2.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.8.8.3.1" class="ltx_text" style="font-size:70%;">33.54</span></td>
<td id="S5.T3.2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">13.10</span></td>
<td id="S5.T3.2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.8.8.5.1" class="ltx_text" style="font-size:70%;">10.24</span></td>
</tr>
<tr id="S5.T3.2.9.9" class="ltx_tr">
<th id="S5.T3.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.9.9.1.1" class="ltx_text" style="font-size:70%;">(f) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.9.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.9.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.9.9.1.4" class="ltx_text" style="font-size:70%;"> (zero-shot)</span>
</th>
<td id="S5.T3.2.9.9.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.9.9.2.1" class="ltx_text" style="font-size:70%;">30.42</span></td>
<td id="S5.T3.2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.9.9.3.1" class="ltx_text" style="font-size:70%;">30.58</span></td>
<td id="S5.T3.2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.9.9.4.1" class="ltx_text" style="font-size:70%;">0.44</span></td>
<td id="S5.T3.2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.9.9.5.1" class="ltx_text" style="font-size:70%;">0.57</span></td>
</tr>
<tr id="S5.T3.2.10.10" class="ltx_tr">
<th id="S5.T3.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.10.10.1.1" class="ltx_text" style="font-size:70%;">(g) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.10.10.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.10.10.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.10.10.1.4" class="ltx_text" style="font-size:70%;"> (contrastive)</span>
</th>
<td id="S5.T3.2.10.10.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.10.10.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">37.40</span></td>
<td id="S5.T3.2.10.10.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.10.10.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">38.58</span></td>
<td id="S5.T3.2.10.10.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.10.10.4.1" class="ltx_text" style="font-size:70%;">5.56</span></td>
<td id="S5.T3.2.10.10.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.10.10.5.1" class="ltx_text" style="font-size:70%;">3.83</span></td>
</tr>
<tr id="S5.T3.2.11.11" class="ltx_tr">
<th id="S5.T3.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.11.11.1.1" class="ltx_text" style="font-size:70%;">(h) GPT-3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.11.11.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S5.T3.2.11.11.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T3.2.11.11.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.11.11.2.1" class="ltx_text" style="font-size:70%;">35.07</span></td>
<td id="S5.T3.2.11.11.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.11.11.3.1" class="ltx_text" style="font-size:70%;">35.21</span></td>
<td id="S5.T3.2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.11.11.4.1" class="ltx_text" style="font-size:70%;">12.98</span></td>
<td id="S5.T3.2.11.11.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">11.49</span></td>
</tr>
<tr id="S5.T3.2.12.12" class="ltx_tr">
<th id="S5.T3.2.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.12.12.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Image</span></th>
<td id="S5.T3.2.12.12.2" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.12.12.3" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.12.12.4" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.12.12.5" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
</tr>
<tr id="S5.T3.2.13.13" class="ltx_tr">
<th id="S5.T3.2.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.13.13.1.1" class="ltx_text" style="font-size:70%;">(i) ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.13.13.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S5.T3.2.13.13.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.13.13.1.4" class="ltx_text" style="font-size:70%;"> (classifier)</span>
</th>
<td id="S5.T3.2.13.13.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.13.13.2.1" class="ltx_text" style="font-size:70%;">28.19</span></td>
<td id="S5.T3.2.13.13.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.13.13.3.1" class="ltx_text" style="font-size:70%;">28.81</span></td>
<td id="S5.T3.2.13.13.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.13.13.4.1" class="ltx_text" style="font-size:70%;">2.68</span></td>
<td id="S5.T3.2.13.13.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.13.13.5.1" class="ltx_text" style="font-size:70%;">2.30</span></td>
</tr>
<tr id="S5.T3.2.14.14" class="ltx_tr">
<th id="S5.T3.2.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.14.14.1.1" class="ltx_text" style="font-size:70%;">(j) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.14.14.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.14.14.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.14.14.1.4" class="ltx_text" style="font-size:70%;"> (classifier)</span>
</th>
<td id="S5.T3.2.14.14.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.14.14.2.1" class="ltx_text" style="font-size:70%;">33.21</span></td>
<td id="S5.T3.2.14.14.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.14.14.3.1" class="ltx_text" style="font-size:70%;">32.56</span></td>
<td id="S5.T3.2.14.14.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.14.14.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">5.15</span></td>
<td id="S5.T3.2.14.14.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.14.14.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">4.38</span></td>
</tr>
<tr id="S5.T3.2.15.15" class="ltx_tr">
<th id="S5.T3.2.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.15.15.1.1" class="ltx_text" style="font-size:70%;">(k) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.15.15.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.15.15.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.15.15.1.4" class="ltx_text" style="font-size:70%;"> (zero-shot)</span>
</th>
<td id="S5.T3.2.15.15.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.15.15.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">56.28</span></td>
<td id="S5.T3.2.15.15.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.15.15.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">53.94</span></td>
<td id="S5.T3.2.15.15.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.15.15.4.1" class="ltx_text" style="font-size:70%;">2.24</span></td>
<td id="S5.T3.2.15.15.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.15.15.5.1" class="ltx_text" style="font-size:70%;">2.29</span></td>
</tr>
<tr id="S5.T3.2.16.16" class="ltx_tr">
<th id="S5.T3.2.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.16.16.1.1" class="ltx_text" style="font-size:70%;">(l) CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.16.16.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S5.T3.2.16.16.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T3.2.16.16.1.4" class="ltx_text" style="font-size:70%;"> (contrastive)</span>
</th>
<td id="S5.T3.2.16.16.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.16.16.2.1" class="ltx_text" style="font-size:70%;">52.56</span></td>
<td id="S5.T3.2.16.16.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.16.16.3.1" class="ltx_text" style="font-size:70%;">50.09</span></td>
<td id="S5.T3.2.16.16.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.16.16.4.1" class="ltx_text" style="font-size:70%;">2.33</span></td>
<td id="S5.T3.2.16.16.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.16.16.5.1" class="ltx_text" style="font-size:70%;">2.45</span></td>
</tr>
<tr id="S5.T3.2.17.17" class="ltx_tr">
<th id="S5.T3.2.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.17.17.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Question &amp; Image</span></th>
<td id="S5.T3.2.17.17.2" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.17.17.3" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.17.17.4" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T3.2.17.17.5" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
</tr>
<tr id="S5.T3.2.18.18" class="ltx_tr">
<th id="S5.T3.2.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.18.18.1.1" class="ltx_text" style="font-size:70%;">(m) CLIP (classifier)</span></th>
<td id="S5.T3.2.18.18.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.18.18.2.1" class="ltx_text" style="font-size:70%;">40.84</span></td>
<td id="S5.T3.2.18.18.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.18.18.3.1" class="ltx_text" style="font-size:70%;">38.30</span></td>
<td id="S5.T3.2.18.18.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.18.18.4.1" class="ltx_text" style="font-size:70%;">18.95</span></td>
<td id="S5.T3.2.18.18.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.18.18.5.1" class="ltx_text" style="font-size:70%;">14.27</span></td>
</tr>
<tr id="S5.T3.2.19.19" class="ltx_tr">
<th id="S5.T3.2.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.19.19.1.1" class="ltx_text" style="font-size:70%;">(n) CLIP (zero-shot)</span></th>
<td id="S5.T3.2.19.19.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.19.19.2.1" class="ltx_text" style="font-size:70%;">48.19</span></td>
<td id="S5.T3.2.19.19.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.19.19.3.1" class="ltx_text" style="font-size:70%;">45.72</span></td>
<td id="S5.T3.2.19.19.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.19.19.4.1" class="ltx_text" style="font-size:70%;">1.08</span></td>
<td id="S5.T3.2.19.19.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.19.19.5.1" class="ltx_text" style="font-size:70%;">0.71</span></td>
</tr>
<tr id="S5.T3.2.20.20" class="ltx_tr">
<th id="S5.T3.2.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.20.20.1.1" class="ltx_text" style="font-size:70%;">(o) CLIP (contrastive)</span></th>
<td id="S5.T3.2.20.20.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.20.20.2.1" class="ltx_text" style="font-size:70%;">53.77</span></td>
<td id="S5.T3.2.20.20.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.20.20.3.1" class="ltx_text" style="font-size:70%;">51.01</span></td>
<td id="S5.T3.2.20.20.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.20.20.4.1" class="ltx_text" style="font-size:70%;">10.36</span></td>
<td id="S5.T3.2.20.20.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.20.20.5.1" class="ltx_text" style="font-size:70%;">7.10</span></td>
</tr>
<tr id="S5.T3.2.21.21" class="ltx_tr">
<th id="S5.T3.2.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T3.2.21.21.1.1" class="ltx_text" style="font-size:70%;">(p) ClipCap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T3.2.21.21.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.T3.2.21.21.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T3.2.21.21.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.21.21.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">56.93</span></td>
<td id="S5.T3.2.21.21.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.21.21.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">51.43</span></td>
<td id="S5.T3.2.21.21.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.21.21.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">30.89</span></td>
<td id="S5.T3.2.21.21.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T3.2.21.21.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">25.90</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.8.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Large-scale pre-trained models.<span id="S5.T3.9.2.1" class="ltx_text ltx_font_medium">
We also compare with no input heuristics (rows <span id="S5.T3.9.2.1.1" class="ltx_text ltx_font_italic">a-c</span>) with choices (for MC) or vocabulary answers (for DA). <span id="S5.T3.9.2.1.2" class="ltx_text ltx_font_italic">Random</span> is a uniform sampling. <span id="S5.T3.9.2.1.3" class="ltx_text ltx_font_italic">Random (weighted)</span> uses weighted sampling proportional to correct answer frequencies in train. <span id="S5.T3.9.2.1.4" class="ltx_text ltx_font_italic">Most Common</span> selects the most frequent answer in train.</span></span></figcaption>
</figure>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Generative models.</span>
We also evaluate models (GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and ClipCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>) that generate answers directly as text. For both models, we predict the generated text for DA and the generated text’s nearest neighbor choice for MC.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p">We prompt GPT-3<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We use the second largest available GPT-3 model, Curie, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.</span></span></span> (row <span id="S5.SS2.p7.1.1" class="ltx_text ltx_font_italic">h</span>) with 10 random questions and answers from the training set, followed by a new question, and let GPT-3 generate an answer to that question, in a manner similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
We provide GPT-3 with the prompt template “Question: … Answer: […]”, expecting it to complete the answer for each evaluation question.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>During MC, we also tried prompting GPT-3 with “Choices: …”, but find that this actually hurts performance.</span></span></span></p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.1" class="ltx_p">ClipCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> (row <span id="S5.SS2.p8.1.1" class="ltx_text ltx_font_italic">p</span>) is an image captioning method that passes CLIP image features through a trained network to GPT-2 (as input tokens). We adapt this model by adding question tokens (and answer choices if applicable) to the prompt of GPT-2, generate answers instead of captions, and fine-tune on our data. We provide additional details, diagrams, and variations in Appx. <a href="#A2" title="Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S5.SS2.p9" class="ltx_para">
<p id="S5.SS2.p9.1" class="ltx_p"><span id="S5.SS2.p9.1.1" class="ltx_text ltx_font_bold">Results.</span>
Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of our evaluation of these models. Rows <span id="S5.SS2.p9.1.2" class="ltx_text ltx_font_italic">a-c</span> show the biases in our dataset, but that the direct answer setting is appropriately challenging. Question-only baselines (rows <span id="S5.SS2.p9.1.3" class="ltx_text ltx_font_italic">d-h</span>) show poor performance in both MC and DA settings. However, it is interesting that GPT-3 performs similarly to the fine-tuned CLIP models (whichever is better per setting). The zero-shot CLIP model (row <span id="S5.SS2.p9.1.4" class="ltx_text ltx_font_italic">f</span>) is least effective, indicating that training is necessary to repurpose CLIP text encodings for language-only tasks.
Unsurprisingly, CLIP image features are very strong for zero-shot multiple choice matching (row <span id="S5.SS2.p9.1.5" class="ltx_text ltx_font_italic">k</span>). However, they are not as strong as for the fine-tuned classifier (row <span id="S5.SS2.p9.1.6" class="ltx_text ltx_font_italic">j</span>) in DA. ClipCap (row <span id="S5.SS2.p9.1.7" class="ltx_text ltx_font_italic">p</span>) outperforms all other baselines in DA, because we use powerful image features and also fine-tune a strong language model for our task.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Rationale Generation</h3>

<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.6.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.6.6.7.1" class="ltx_tr">
<th id="S5.T4.6.6.7.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td id="S5.T4.6.6.7.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T4.6.6.7.1.2.1" class="ltx_text" style="font-size:70%;color:#3B78D6;">Multiple Choice</span></td>
<td id="S5.T4.6.6.7.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T4.6.6.7.1.3.1" class="ltx_text" style="font-size:70%;color:#E68F36;">Direct Answer</span></td>
</tr>
<tr id="S5.T4.6.6.8.2" class="ltx_tr">
<th id="S5.T4.6.6.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.8.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<td id="S5.T4.6.6.8.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.8.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T4.6.6.8.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.8.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
<td id="S5.T4.6.6.8.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.8.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T4.6.6.8.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.8.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<th id="S5.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T4.2.2.2.2.1" class="ltx_text" style="font-size:70%;">(a) ClipCap </span><math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.2.2.2.2.2" class="ltx_text" style="font-size:70%;"> Cap. </span><math id="S5.T4.2.2.2.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.2.2.2.2.m2.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.2.2.2.2.m2.1.1" xref="S5.T4.2.2.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m2.1b"><ci id="S5.T4.2.2.2.2.m2.1.1.cmml" xref="S5.T4.2.2.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m2.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.2.2.2.2.3" class="ltx_text" style="font-size:70%;"> GPT</span>
</th>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.2.2.2.3.1" class="ltx_text" style="font-size:70%;">42.51</span></td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.2.2.2.4.1" class="ltx_text" style="font-size:70%;">43.61</span></td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.2.2.2.5.1" class="ltx_text" style="font-size:70%;">16.59</span></td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.2.2.2.6.1" class="ltx_text" style="font-size:70%;">15.79</span></td>
</tr>
<tr id="S5.T4.4.4.4" class="ltx_tr">
<th id="S5.T4.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T4.4.4.4.2.1" class="ltx_text" style="font-size:70%;">(b) ClipCap </span><math id="S5.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.3.3.3.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.3.3.3.1.m1.1.1" xref="S5.T4.3.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><ci id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.4.4.4.2.2" class="ltx_text" style="font-size:70%;"> Ratl. </span><math id="S5.T4.4.4.4.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.4.4.4.2.m2.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.4.4.4.2.m2.1.1" xref="S5.T4.4.4.4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.2.m2.1b"><ci id="S5.T4.4.4.4.2.m2.1.1.cmml" xref="S5.T4.4.4.4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.2.m2.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.4.4.4.2.3" class="ltx_text" style="font-size:70%;"> GPT</span>
</th>
<td id="S5.T4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.4.4.4.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">44.00</span></td>
<td id="S5.T4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.4.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">43.84</span></td>
<td id="S5.T4.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.4.4.4.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">18.11</span></td>
<td id="S5.T4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.4.4.4.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">15.81</span></td>
</tr>
<tr id="S5.T4.6.6.9.3" class="ltx_tr">
<th id="S5.T4.6.6.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.9.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Oracles</span></th>
<td id="S5.T4.6.6.9.3.2" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T4.6.6.9.3.3" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T4.6.6.9.3.4" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T4.6.6.9.3.5" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
</tr>
<tr id="S5.T4.5.5.5" class="ltx_tr">
<th id="S5.T4.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T4.5.5.5.1.1" class="ltx_text" style="font-size:70%;">(c) GT Caption </span><math id="S5.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.5.5.5.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.5.5.5.1.m1.1.1" xref="S5.T4.5.5.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.1.m1.1b"><ci id="S5.T4.5.5.5.1.m1.1.1.cmml" xref="S5.T4.5.5.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.5.5.5.1.2" class="ltx_text" style="font-size:70%;"> GPT</span>
</th>
<td id="S5.T4.5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.5.5.5.2.1" class="ltx_text" style="font-size:70%;">45.40</span></td>
<td id="S5.T4.5.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.5.5.5.3.1" class="ltx_text" style="font-size:70%;">—</span></td>
<td id="S5.T4.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.5.5.5.4.1" class="ltx_text" style="font-size:70%;">16.39</span></td>
<td id="S5.T4.5.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.5.5.5.5.1" class="ltx_text" style="font-size:70%;">—</span></td>
</tr>
<tr id="S5.T4.6.6.6" class="ltx_tr">
<th id="S5.T4.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T4.6.6.6.1.1" class="ltx_text" style="font-size:70%;">(d) GT Rationale </span><math id="S5.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.6.6.6.1.m1.1a"><mo mathsize="70%" stretchy="false" id="S5.T4.6.6.6.1.m1.1.1" xref="S5.T4.6.6.6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.1.m1.1b"><ci id="S5.T4.6.6.6.1.m1.1.1.cmml" xref="S5.T4.6.6.6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S5.T4.6.6.6.1.2" class="ltx_text" style="font-size:70%;"> GPT</span>
</th>
<td id="S5.T4.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">56.74</span></td>
<td id="S5.T4.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.6.3.1" class="ltx_text" style="font-size:70%;">56.75</span></td>
<td id="S5.T4.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">24.02</span></td>
<td id="S5.T4.6.6.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T4.6.6.6.5.1" class="ltx_text" style="font-size:70%;">20.75</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.11.2.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Models using generated and GT rationales<span id="S5.T4.8.1.1" class="ltx_text ltx_font_medium"> as described in Sec. <a href="#S5.SS3" title="5.3 Rationale Generation ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>. We are unable to evaluate the GT Caption <math id="S5.T4.8.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T4.8.1.1.m1.1b"><mo stretchy="false" id="S5.T4.8.1.1.m1.1.1" xref="S5.T4.8.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.1.1.m1.1c"><ci id="S5.T4.8.1.1.m1.1.1.cmml" xref="S5.T4.8.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.1.1.m1.1d">\rightarrow</annotation></semantics></math> GPT setting on the test set, as captions are not available in the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> test set.</span></span></figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.8" class="ltx_p">We are interested in whether we can improve GPT-3 prompting results by providing additional image- and question- specific context and report results for the following methods in Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Rationale Generation ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. So, we fine-tune ClipCap (given images and questions, but not choices) as above, but for the task of generating rationales instead of answers. Our model scores <span id="S5.SS3.p1.8.1" class="ltx_text ltx_markedasmath ltx_font_bold">10.2</span> (val) / <span id="S5.SS3.p1.8.2" class="ltx_text ltx_markedasmath ltx_font_bold">9.58</span> (test) on SacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and <span id="S5.SS3.p1.8.3" class="ltx_text ltx_markedasmath ltx_font_bold">0.271</span> (val) / <span id="S5.SS3.p1.8.4" class="ltx_text ltx_markedasmath ltx_font_bold">0.256</span> (test) on METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. We can then prompt GPT-3 (as above) but also provide these generated rationales as “Context: …”. This model is denoted by ‘ClipCap <math id="S5.SS3.p1.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS3.p1.5.m5.1a"><mo stretchy="false" id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><ci id="S5.SS3.p1.5.m5.1.1.cmml" xref="S5.SS3.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">\rightarrow</annotation></semantics></math> Ratl. <math id="S5.SS3.p1.6.m6.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS3.p1.6.m6.1a"><mo stretchy="false" id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><ci id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">\rightarrow</annotation></semantics></math> GPT’. We provide additional details, diagrams, and examples of generated rationales in Appx. <a href="#A3" title="Appendix C Additional Details for Rationale Generation ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>. We repeat this experiment using captions (generated from only images) from the original ClipCap model: ‘ClipCap <math id="S5.SS3.p1.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS3.p1.7.m7.1a"><mo stretchy="false" id="S5.SS3.p1.7.m7.1.1" xref="S5.SS3.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m7.1b"><ci id="S5.SS3.p1.7.m7.1.1.cmml" xref="S5.SS3.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m7.1c">\rightarrow</annotation></semantics></math> Cap. <math id="S5.SS3.p1.8.m8.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS3.p1.8.m8.1a"><mo stretchy="false" id="S5.SS3.p1.8.m8.1.1" xref="S5.SS3.p1.8.m8.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.8.m8.1b"><ci id="S5.SS3.p1.8.m8.1.1.cmml" xref="S5.SS3.p1.8.m8.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.8.m8.1c">\rightarrow</annotation></semantics></math> GPT’.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Results.</span> We show results from these experiments in Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Rationale Generation ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Interestingly, prompting GPT-3 with ground-truth rationales (row <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">d</span>) is competitive with the best model in Sec. <a href="#S5.SS2" title="5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> (Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, row <span id="S5.SS3.p2.1.3" class="ltx_text ltx_font_italic">p</span>) in MC and significantly outperforms the question-only GPT-3 method (Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, row <span id="S5.SS3.p2.1.4" class="ltx_text ltx_font_italic">h</span>). When we prompt GPT-3 with ground-truth rationales (row <span id="S5.SS3.p2.1.5" class="ltx_text ltx_font_italic">d</span>), we see higher performance than when we provide ground-truth captions (row <span id="S5.SS3.p2.1.6" class="ltx_text ltx_font_italic">c</span>). This affirms that rationales contain useful information (i.e. specific to our questions and answers) in addition to captions. However, the additional performance of prompting GPT-3 using generated rationales (row <span id="S5.SS3.p2.1.7" class="ltx_text ltx_font_italic">b</span>) over generated captions (row <span id="S5.SS3.p2.1.8" class="ltx_text ltx_font_italic">a</span>) is not as significant. This indicates potential room for improvement in our approach for generating rationales.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Specialized Models</h3>

<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.2.1.1" class="ltx_tr">
<th id="S5.T5.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td id="S5.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T5.2.1.1.2.1" class="ltx_text" style="font-size:70%;color:#3B78D6;">Multiple-Choice</span></td>
<td id="S5.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.7pt;padding-right:5.7pt;" colspan="2"><span id="S5.T5.2.1.1.3.1" class="ltx_text" style="font-size:70%;color:#E68F36;">Direct Answer</span></td>
</tr>
<tr id="S5.T5.2.2.2" class="ltx_tr">
<th id="S5.T5.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T5.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
<td id="S5.T5.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Val</span></td>
<td id="S5.T5.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Test</span></td>
</tr>
<tr id="S5.T5.2.3.3" class="ltx_tr">
<th id="S5.T5.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.3.3.1.1" class="ltx_text" style="font-size:70%;">(a) Pythia </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.3.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S5.T5.2.3.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T5.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.3.3.2.1" class="ltx_text" style="font-size:70%;">49.0</span></td>
<td id="S5.T5.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.3.3.3.1" class="ltx_text" style="font-size:70%;">40.1</span></td>
<td id="S5.T5.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.3.3.4.1" class="ltx_text" style="font-size:70%;">25.2</span></td>
<td id="S5.T5.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.3.3.5.1" class="ltx_text" style="font-size:70%;">21.9</span></td>
</tr>
<tr id="S5.T5.2.4.4" class="ltx_tr">
<th id="S5.T5.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.4.4.1.1" class="ltx_text" style="font-size:70%;">(b) ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.4.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S5.T5.2.4.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T5.2.4.4.1.4" class="ltx_text" style="font-size:70%;"> - OK-VQA</span>
</th>
<td id="S5.T5.2.4.4.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.4.4.2.1" class="ltx_text" style="font-size:70%;">32.8</span></td>
<td id="S5.T5.2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.4.4.3.1" class="ltx_text" style="font-size:70%;">34.1</span></td>
<td id="S5.T5.2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.4.4.4.1" class="ltx_text" style="font-size:70%;">9.1</span></td>
<td id="S5.T5.2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.4.4.5.1" class="ltx_text" style="font-size:70%;">9.2</span></td>
</tr>
<tr id="S5.T5.2.5.5" class="ltx_tr">
<th id="S5.T5.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.5.5.1.1" class="ltx_text" style="font-size:70%;">(c) ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.5.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S5.T5.2.5.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T5.2.5.5.1.4" class="ltx_text" style="font-size:70%;"> - VQA</span>
</th>
<td id="S5.T5.2.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.5.5.2.1" class="ltx_text" style="font-size:70%;">47.7</span></td>
<td id="S5.T5.2.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.5.5.3.1" class="ltx_text" style="font-size:70%;">42.1</span></td>
<td id="S5.T5.2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.5.5.4.1" class="ltx_text" style="font-size:70%;">17.7</span></td>
<td id="S5.T5.2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.5.5.5.1" class="ltx_text" style="font-size:70%;">12.0</span></td>
</tr>
<tr id="S5.T5.2.6.6" class="ltx_tr">
<th id="S5.T5.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.6.6.1.1" class="ltx_text" style="font-size:70%;">(d) ViLBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.6.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S5.T5.2.6.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T5.2.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.6.6.2.1" class="ltx_text" style="font-size:70%;">49.1</span></td>
<td id="S5.T5.2.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.6.6.3.1" class="ltx_text" style="font-size:70%;">41.5</span></td>
<td id="S5.T5.2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.6.6.4.1" class="ltx_text" style="font-size:70%;">30.6</span></td>
<td id="S5.T5.2.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.6.6.5.1" class="ltx_text" style="font-size:70%;">25.9</span></td>
</tr>
<tr id="S5.T5.2.7.7" class="ltx_tr">
<th id="S5.T5.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.7.7.1.1" class="ltx_text" style="font-size:70%;">(e) LXMERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.7.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S5.T5.2.7.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T5.2.7.7.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.7.7.2.1" class="ltx_text" style="font-size:70%;">51.4</span></td>
<td id="S5.T5.2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.7.7.3.1" class="ltx_text" style="font-size:70%;">41.6</span></td>
<td id="S5.T5.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.7.7.4.1" class="ltx_text" style="font-size:70%;">30.7</span></td>
<td id="S5.T5.2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.7.7.5.1" class="ltx_text" style="font-size:70%;">25.9</span></td>
</tr>
<tr id="S5.T5.2.8.8" class="ltx_tr">
<th id="S5.T5.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.8.8.1.1" class="ltx_text" style="font-size:70%;">(f) KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.8.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S5.T5.2.8.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T5.2.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.8.8.2.1" class="ltx_text" style="font-size:70%;">51.9</span></td>
<td id="S5.T5.2.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.8.8.3.1" class="ltx_text" style="font-size:70%;">42.2</span></td>
<td id="S5.T5.2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.8.8.4.1" class="ltx_text" style="font-size:70%;">33.7</span></td>
<td id="S5.T5.2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.8.8.5.1" class="ltx_text" style="font-size:70%;">27.1</span></td>
</tr>
<tr id="S5.T5.2.9.9" class="ltx_tr">
<th id="S5.T5.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.9.9.1.1" class="ltx_text" style="font-size:70%;">(g) GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.9.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S5.T5.2.9.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T5.2.9.9.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.9.9.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">60.3</span></td>
<td id="S5.T5.2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.9.9.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">53.7</span></td>
<td id="S5.T5.2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.9.9.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">48.6</span></td>
<td id="S5.T5.2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.9.9.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">40.7</span></td>
</tr>
<tr id="S5.T5.2.10.10" class="ltx_tr">
<th id="S5.T5.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.10.10.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Oracles</span></th>
<td id="S5.T5.2.10.10.2" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T5.2.10.10.3" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T5.2.10.10.4" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
<td id="S5.T5.2.10.10.5" class="ltx_td ltx_border_t" style="padding-left:5.7pt;padding-right:5.7pt;"></td>
</tr>
<tr id="S5.T5.2.11.11" class="ltx_tr">
<th id="S5.T5.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.11.11.1.1" class="ltx_text" style="font-size:70%;">(h) GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.11.11.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S5.T5.2.11.11.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T5.2.11.11.1.4" class="ltx_text" style="font-size:70%;"> + Masked Ans.</span>
</th>
<td id="S5.T5.2.11.11.2" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.11.11.2.1" class="ltx_text" style="font-size:70%;">65.1</span></td>
<td id="S5.T5.2.11.11.3" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.11.11.3.1" class="ltx_text" style="font-size:70%;">58.3</span></td>
<td id="S5.T5.2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.11.11.4.1" class="ltx_text" style="font-size:70%;">52.7</span></td>
<td id="S5.T5.2.11.11.5" class="ltx_td ltx_align_center" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.11.11.5.1" class="ltx_text" style="font-size:70%;">43.9</span></td>
</tr>
<tr id="S5.T5.2.12.12" class="ltx_tr">
<th id="S5.T5.2.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.7pt;padding-right:5.7pt;">
<span id="S5.T5.2.12.12.1.1" class="ltx_text" style="font-size:70%;">(i) GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.2.12.12.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S5.T5.2.12.12.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S5.T5.2.12.12.1.4" class="ltx_text" style="font-size:70%;"> + GT Ratl.</span>
</th>
<td id="S5.T5.2.12.12.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.12.12.2.1" class="ltx_text" style="font-size:70%;">73.4</span></td>
<td id="S5.T5.2.12.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.12.12.3.1" class="ltx_text" style="font-size:70%;">67.2</span></td>
<td id="S5.T5.2.12.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.12.12.4.1" class="ltx_text" style="font-size:70%;">58.9</span></td>
<td id="S5.T5.2.12.12.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.7pt;padding-right:5.7pt;"><span id="S5.T5.2.12.12.5.1" class="ltx_text" style="font-size:70%;">51.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.5.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Specialized models results. <span id="S5.T5.6.2.1" class="ltx_text ltx_font_medium"> Baselines trained for VQA or knowledge-based VQA, and fine-tuned on <span id="S5.T5.6.2.1.1" class="ltx_text">A-OKVQA</span>. The bottom two rows are not comparable with the others since they use ground-truth rationales at test time. </span></span></figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In this section, we evaluate some recent high-performing, open-source models trained on knowledge-based VQA or the traditional VQA. The models we consider are Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and GPV-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. As the first four models are part of MMF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, it is easier to compare them fairly. KRISP is a high-performing model on OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. It provides a suitable baseline as it addresses knowledge-based VQA. GPV-2 performs multiple vision and vision–language tasks and has learned a large number of concepts, so it can be a strong baseline for <span id="S5.SS4.p1.1.1" class="ltx_text">A-OKVQA</span>. All of these models are fine-tuned on <span id="S5.SS4.p1.1.2" class="ltx_text">A-OKVQA</span> to predict answers directly for DA evaluation. We adapt them to MC using the nearest choice method described above. See Appx. <a href="#A4" title="Appendix D Additional Details for Specialized Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> for the details of each model.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Results.</span> Unsurprisingly, these models, which are specialized for DA and some of which are specialized for knowledge-based VQA perform very well on the DA evaluation and quite well on MC. Of the models trained only on <span id="S5.SS4.p2.1.2" class="ltx_text">A-OKVQA</span>  KRISP does the best, likely because it is trained to directly use outside knowledge graphs. GPV-2, however, performs best of all, beating all other models (that do not use ground-truth rationales) in all settings, possibly because of the large number of concepts it has learned.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Transfer results.</span>
We train ViLBERT on VQAv2 and OK-VQA datasets (denoted by ‘ViLBERT-VQA’ and ‘ViLBERT-OK-VQA’ in Table <a href="#S5.T5" title="Table 5 ‣ 5.4 Specialized Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) to evaluate whether the knowledge from those datasets is sufficient for <span id="S5.SS4.p3.1.2" class="ltx_text">A-OKVQA</span>. The low performance shows the significant difference between these datasets.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.2" class="ltx_p"><span id="S5.SS4.p4.2.1" class="ltx_text ltx_font_bold">Ground-truth Rationales.</span>
To evaluate how well the model performs if it is provided with high-quality rationales, we use ground-truth rationales at test. We show these results with GPV-2 (our best model). Ground-truth rationales are appended to questions as additional input text (‘GPV-2 + GT Ratl.’). For this experiment, we used only one of the rationales. Comparing rows <span id="S5.SS4.p4.2.2" class="ltx_text ltx_font_italic">g</span> and <span id="S5.SS4.p4.2.3" class="ltx_text ltx_font_italic">i</span> of Table <a href="#S5.T5" title="Table 5 ‣ 5.4 Specialized Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows rationales are helpful. To evaluate how much of this improvement can be attributed to rationales and not the fact that sometimes rationales contain the answer, we replaced answers in the rationales with <math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="[" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mo stretchy="false" id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><ci id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">[</annotation></semantics></math>answer<math id="S5.SS4.p4.2.m2.1" class="ltx_Math" alttext="]" display="inline"><semantics id="S5.SS4.p4.2.m2.1a"><mo stretchy="false" id="S5.SS4.p4.2.m2.1.1" xref="S5.SS4.p4.2.m2.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.2.m2.1b"><ci id="S5.SS4.p4.2.m2.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.2.m2.1c">]</annotation></semantics></math> token. The performance drops (row <span id="S5.SS4.p4.2.4" class="ltx_text ltx_font_italic">i</span> vs row <span id="S5.SS4.p4.2.5" class="ltx_text ltx_font_italic">h</span>), however, it is still higher than the case that we do not use rationales (row <span id="S5.SS4.p4.2.6" class="ltx_text ltx_font_italic">h</span> vs row <span id="S5.SS4.p4.2.7" class="ltx_text ltx_font_italic">g</span>).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis of Models</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.2" class="ltx_p">Next, we analyze the predictions that our baseline models make to see if we can learn more about <span id="S6.p1.2.1" class="ltx_text">A-OKVQA</span>: what kinds of questions do different types of approaches do better / worse on? For these experiments, we choose some of the best performing models on Direct Answer: VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, ClipCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and GPV-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. We also use the ClipCap <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S6.p1.1.m1.1a"><mo stretchy="false" id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\rightarrow</annotation></semantics></math> Rationale <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S6.p1.2.m2.1a"><mo stretchy="false" id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><ci id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">\rightarrow</annotation></semantics></math> GPT model from Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Rationale Generation ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which will be referred to as ‘GR-GPT’ for Generated Rationales GPT.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Answer Frequency.</span>
First, we look at how answer frequency affects performance in Table <a href="#S6.T6" title="Table 6 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We first count the number of times any answer appears in the direct answers in the training set. We then divide these into bins and look at the direct DA test accuracy of our baselines for each of these frequency bins. We find that GPV-2, and to a lesser extent ClipCap and GR-GPT perform better on questions whose answers do not appear often in the training set (1-5 and 6-10 columns of Table <a href="#S6.T6" title="Table 6 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). GPV-2 in particular (which is fine-tuned on several vision and language tasks) is able to predict these tail answers much better than other methods, especially the discriminative methods such as LXMERT.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.2.1.1" class="ltx_tr">
<th id="S6.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S6.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.2.1" class="ltx_text" style="font-size:70%;">1-5</span></th>
<th id="S6.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.3.1" class="ltx_text" style="font-size:70%;">6-10</span></th>
<th id="S6.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.4.1" class="ltx_text" style="font-size:70%;">11-20</span></th>
<th id="S6.T6.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.5.1" class="ltx_text" style="font-size:70%;">21-50</span></th>
<th id="S6.T6.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.6.1" class="ltx_text" style="font-size:70%;">51-100</span></th>
<th id="S6.T6.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.7.1" class="ltx_text" style="font-size:70%;">101-200</span></th>
<th id="S6.T6.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.1.1.8.1" class="ltx_text" style="font-size:70%;">201+</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.2.2.1" class="ltx_tr">
<th id="S6.T6.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T6.2.2.1.1.1" class="ltx_text" style="font-size:70%;">VilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S6.T6.2.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T6.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.2.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T6.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.3.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T6.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.4.1" class="ltx_text" style="font-size:70%;">3.68</span></td>
<td id="S6.T6.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.5.1" class="ltx_text" style="font-size:70%;">10.97</span></td>
<td id="S6.T6.2.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.6.1" class="ltx_text" style="font-size:70%;">19.95</span></td>
<td id="S6.T6.2.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.7.1" class="ltx_text" style="font-size:70%;">26.53</span></td>
<td id="S6.T6.2.2.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.2.1.8.1" class="ltx_text" style="font-size:70%;">35.91</span></td>
</tr>
<tr id="S6.T6.2.3.2" class="ltx_tr">
<th id="S6.T6.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T6.2.3.2.1.1" class="ltx_text" style="font-size:70%;">LXMERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S6.T6.2.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T6.2.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.2.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T6.2.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.3.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T6.2.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.4.1" class="ltx_text" style="font-size:70%;">4.29</span></td>
<td id="S6.T6.2.3.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.5.1" class="ltx_text" style="font-size:70%;">13.73</span></td>
<td id="S6.T6.2.3.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.6.1" class="ltx_text" style="font-size:70%;">20.18</span></td>
<td id="S6.T6.2.3.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.7.1" class="ltx_text" style="font-size:70%;">26.69</span></td>
<td id="S6.T6.2.3.2.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.3.2.8.1" class="ltx_text" style="font-size:70%;">34.31</span></td>
</tr>
<tr id="S6.T6.2.4.3" class="ltx_tr">
<th id="S6.T6.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T6.2.4.3.1.1" class="ltx_text" style="font-size:70%;">KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S6.T6.2.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T6.2.4.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.2.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T6.2.4.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.3.1" class="ltx_text" style="font-size:70%;">0.61</span></td>
<td id="S6.T6.2.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.4.1" class="ltx_text" style="font-size:70%;">6.34</span></td>
<td id="S6.T6.2.4.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.5.1" class="ltx_text" style="font-size:70%;">13.99</span></td>
<td id="S6.T6.2.4.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.6.1" class="ltx_text" style="font-size:70%;">21.78</span></td>
<td id="S6.T6.2.4.3.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.7.1" class="ltx_text" style="font-size:70%;">28.55</span></td>
<td id="S6.T6.2.4.3.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.4.3.8.1" class="ltx_text" style="font-size:70%;">35.22</span></td>
</tr>
<tr id="S6.T6.2.5.4" class="ltx_tr">
<th id="S6.T6.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T6.2.5.4.1.1" class="ltx_text" style="font-size:70%;">ClipCap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S6.T6.2.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T6.2.5.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.2.1" class="ltx_text" style="font-size:70%;">4.71</span></td>
<td id="S6.T6.2.5.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.3.1" class="ltx_text" style="font-size:70%;">4.24</span></td>
<td id="S6.T6.2.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.4.1" class="ltx_text" style="font-size:70%;">9.10</span></td>
<td id="S6.T6.2.5.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.5.1" class="ltx_text" style="font-size:70%;">17.90</span></td>
<td id="S6.T6.2.5.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.6.1" class="ltx_text" style="font-size:70%;">25.93</span></td>
<td id="S6.T6.2.5.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.7.1" class="ltx_text" style="font-size:70%;">29.44</span></td>
<td id="S6.T6.2.5.4.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.5.4.8.1" class="ltx_text" style="font-size:70%;">33.99</span></td>
</tr>
<tr id="S6.T6.2.6.5" class="ltx_tr">
<th id="S6.T6.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.1.1" class="ltx_text" style="font-size:70%;">GR-GPT</span></th>
<td id="S6.T6.2.6.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.2.1" class="ltx_text" style="font-size:70%;">8.18</span></td>
<td id="S6.T6.2.6.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.3.1" class="ltx_text" style="font-size:70%;">9.29</span></td>
<td id="S6.T6.2.6.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.4.1" class="ltx_text" style="font-size:70%;">9.41</span></td>
<td id="S6.T6.2.6.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.5.1" class="ltx_text" style="font-size:70%;">17.39</span></td>
<td id="S6.T6.2.6.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.6.1" class="ltx_text" style="font-size:70%;">18.31</span></td>
<td id="S6.T6.2.6.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.7.1" class="ltx_text" style="font-size:70%;">21.98</span></td>
<td id="S6.T6.2.6.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.6.5.8.1" class="ltx_text" style="font-size:70%;">24.65</span></td>
</tr>
<tr id="S6.T6.2.7.6" class="ltx_tr">
<th id="S6.T6.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T6.2.7.6.1.1" class="ltx_text" style="font-size:70%;">GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.7.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S6.T6.2.7.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T6.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">10.16</span></td>
<td id="S6.T6.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">12.12</span></td>
<td id="S6.T6.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">22.60</span></td>
<td id="S6.T6.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">31.04</span></td>
<td id="S6.T6.2.7.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">38.40</span></td>
<td id="S6.T6.2.7.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">41.60</span></td>
<td id="S6.T6.2.7.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T6.2.7.6.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">44.69</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T6.6.1.1" class="ltx_text" style="font-size:129%;">Table 6</span>: </span><span id="S6.T6.7.2" class="ltx_text ltx_font_bold" style="font-size:129%;">Results across different answer frequencies.<span id="S6.T6.7.2.1" class="ltx_text ltx_font_medium"> The questions are categorized based on the frequency of the GT answer in the training set. Columns show accuracy for answers that appear 1-5 times, 6-10 times, etc. If multiple direct choices, we default to most common one.</span></span></figcaption>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Knowledge Types.</span>
Next, we use the subset of test that we collected knowledge types on (see Sec. <a href="#S4" title="4 Dataset Statistics ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) to look at the direct answer accuracy of these models for different types of knowledge. In Table <a href="#S6.T7" title="Table 7 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we see that while again GPV is the best overall and in every category, the results show some interesting distinctions. KRISP, which is specifically designed with access to explicit knowledge sources such as ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> performs better on “Knowledge Base” questions compared with other discriminative multi-modal transformer methods such as VilBERT and LXMERT as well compared to ClipCap which has an overall higher performance. It also performs better on “Physical Knowledge” which also tends to overlap with the knowledge sources it has.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.2.1.1" class="ltx_tr">
<th id="S6.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S6.T7.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.1.1.2.1" class="ltx_text" style="font-size:70%;">Commonsense</span></th>
<th id="S6.T7.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.1.1.3.1" class="ltx_text" style="font-size:70%;">Knowledge Base</span></th>
<th id="S6.T7.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.1.1.4.1" class="ltx_text" style="font-size:70%;">Physical Knowledge</span></th>
<th id="S6.T7.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.1.1.5.1" class="ltx_text" style="font-size:70%;">Visual Knowledge</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.2.2.1" class="ltx_tr">
<th id="S6.T7.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T7.2.2.1.1.1" class="ltx_text" style="font-size:70%;">VilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S6.T7.2.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T7.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.2.1.2.1" class="ltx_text" style="font-size:70%;">24.30</span></td>
<td id="S6.T7.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.2.1.3.1" class="ltx_text" style="font-size:70%;">19.96</span></td>
<td id="S6.T7.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.2.1.4.1" class="ltx_text" style="font-size:70%;">29.76</span></td>
<td id="S6.T7.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.2.1.5.1" class="ltx_text" style="font-size:70%;">26.55</span></td>
</tr>
<tr id="S6.T7.2.3.2" class="ltx_tr">
<th id="S6.T7.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T7.2.3.2.1.1" class="ltx_text" style="font-size:70%;">LXMERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S6.T7.2.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T7.2.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.3.2.2.1" class="ltx_text" style="font-size:70%;">25.51</span></td>
<td id="S6.T7.2.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.3.2.3.1" class="ltx_text" style="font-size:70%;">16.01</span></td>
<td id="S6.T7.2.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.3.2.4.1" class="ltx_text" style="font-size:70%;">27.38</span></td>
<td id="S6.T7.2.3.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.3.2.5.1" class="ltx_text" style="font-size:70%;">27.23</span></td>
</tr>
<tr id="S6.T7.2.4.3" class="ltx_tr">
<th id="S6.T7.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T7.2.4.3.1.1" class="ltx_text" style="font-size:70%;">KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S6.T7.2.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T7.2.4.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.4.3.2.1" class="ltx_text" style="font-size:70%;">26.63</span></td>
<td id="S6.T7.2.4.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.4.3.3.1" class="ltx_text" style="font-size:70%;">20.72</span></td>
<td id="S6.T7.2.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.4.3.4.1" class="ltx_text" style="font-size:70%;">39.29</span></td>
<td id="S6.T7.2.4.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.4.3.5.1" class="ltx_text" style="font-size:70%;">26.09</span></td>
</tr>
<tr id="S6.T7.2.5.4" class="ltx_tr">
<th id="S6.T7.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T7.2.5.4.1.1" class="ltx_text" style="font-size:70%;">ClipCap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S6.T7.2.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T7.2.5.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.5.4.2.1" class="ltx_text" style="font-size:70%;">27.19</span></td>
<td id="S6.T7.2.5.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.5.4.3.1" class="ltx_text" style="font-size:70%;">16.57</span></td>
<td id="S6.T7.2.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.5.4.4.1" class="ltx_text" style="font-size:70%;">30.95</span></td>
<td id="S6.T7.2.5.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.5.4.5.1" class="ltx_text" style="font-size:70%;">33.41</span></td>
</tr>
<tr id="S6.T7.2.6.5" class="ltx_tr">
<th id="S6.T7.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.6.5.1.1" class="ltx_text" style="font-size:70%;">GR-GPT</span></th>
<td id="S6.T7.2.6.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.6.5.2.1" class="ltx_text" style="font-size:70%;">21.42</span></td>
<td id="S6.T7.2.6.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.6.5.3.1" class="ltx_text" style="font-size:70%;">12.99</span></td>
<td id="S6.T7.2.6.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.6.5.4.1" class="ltx_text" style="font-size:70%;">17.86</span></td>
<td id="S6.T7.2.6.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.6.5.5.1" class="ltx_text" style="font-size:70%;">24.79</span></td>
</tr>
<tr id="S6.T7.2.7.6" class="ltx_tr">
<th id="S6.T7.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T7.2.7.6.1.1" class="ltx_text" style="font-size:70%;">GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.7.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S6.T7.2.7.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T7.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.7.6.2.1" class="ltx_text" style="font-size:70%;">39.76</span></td>
<td id="S6.T7.2.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.7.6.3.1" class="ltx_text" style="font-size:70%;">25.24</span></td>
<td id="S6.T7.2.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.7.6.4.1" class="ltx_text" style="font-size:70%;">44.05</span></td>
<td id="S6.T7.2.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T7.2.7.6.5.1" class="ltx_text" style="font-size:70%;">41.19</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T7.6.1.1" class="ltx_text" style="font-size:129%;">Table 7</span>: </span><span id="S6.T7.7.2" class="ltx_text ltx_font_bold" style="font-size:129%;">Analysis of results based on knowledge type.<span id="S6.T7.7.2.1" class="ltx_text ltx_font_medium"> </span></span></figcaption>
</figure>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Prediction overlap/difference.</span> Finally, we look at some statistics on a question by question level in the <span id="S6.p4.1.2" class="ltx_text">A-OKVQA</span> test set. Specifically we look at the overlap in which methods answered which questions correctly<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>For ease of analysis we count a binary yes/no of whether a model answered correctly if it answered any possible answer in the direct answer set.</span></span></span>. We use the same models as in Tables <a href="#S6.T6" title="Table 6 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> &amp; <a href="#S6.T7" title="Table 7 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.4" class="ltx_p">First, we find that only <span id="S6.p5.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">5.85%</span> of questions in test were answered correctly by all models and <span id="S6.p5.4.2" class="ltx_text ltx_markedasmath ltx_font_bold">30.96%</span> of questions had no model predict a correct answer for. Considering the worst performing model of these gets <span id="S6.p5.4.3" class="ltx_text ltx_markedasmath ltx_font_bold">15.81%</span> DA accuracy and the best gets <span id="S6.p5.4.4" class="ltx_text ltx_markedasmath ltx_font_bold">40.7%</span>, it implies that there is actually a large variation between these models beyond some just being generally better than others and thus getting “hard” questions right and keeping performance on “easy” questions.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">In Table <a href="#S6.T8" title="Table 8 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show the difference between the questions each model gets right on <span id="S6.p6.1.1" class="ltx_text">A-OKVQA</span> test. Each row shows the percentage of that method’s correctly answered questions that were not correctly answered by the comparison model in each column. If we look at the row for the lowest performing model (GR-GPT) for the column for the best performing model (GPV-2), we still see that <span id="S6.p6.1.2" class="ltx_text ltx_markedasmath ltx_font_bold">29.2%</span> of GR-GPT’s correctly answered questions are answered wrong by GPV-2!</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.6" class="ltx_p">Finally, to further illustrate the point that different models have very different mistake patterns, we take the prediction of all of these models except for GPV-2 for each question and take the majority vote between these. This majority vote combination gets an accuracy of <span id="S6.p7.6.1" class="ltx_text ltx_markedasmath ltx_font_bold">29.5</span> compared to the best of these models which gets <span id="S6.p7.6.2" class="ltx_text ltx_markedasmath ltx_font_bold">27.1</span>. This does not work when GPV-2 is added (this majority model gets <span id="S6.p7.6.3" class="ltx_text ltx_markedasmath ltx_font_bold">35.60</span> which is lower than GPV-2’s <span id="S6.p7.6.4" class="ltx_text ltx_markedasmath ltx_font_bold">40.7</span>). We can also look at the Oracle combination accuracy. That is, from our six models, choose the answer with the highest ground-truth value and take that as the oracle combination answer. This DA accuracy is <span id="S6.p7.6.5" class="ltx_text ltx_markedasmath ltx_font_bold">56.87</span> versus the single best performance of <span id="S6.p7.6.6" class="ltx_text ltx_markedasmath ltx_font_bold">40.7</span>, again showing that even worse performing models get lots of questions right that the best model gets wrong.</p>
</div>
<figure id="S6.T8" class="ltx_table">
<table id="S6.T8.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T8.10.1.1" class="ltx_tr">
<th id="S6.T8.10.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S6.T8.10.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.2.1" class="ltx_text" style="font-size:70%;">VilBERT</span></th>
<th id="S6.T8.10.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.3.1" class="ltx_text" style="font-size:70%;">LXMERT</span></th>
<th id="S6.T8.10.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.4.1" class="ltx_text" style="font-size:70%;">KRISP</span></th>
<th id="S6.T8.10.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.5.1" class="ltx_text" style="font-size:70%;">ClipCap</span></th>
<th id="S6.T8.10.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.6.1" class="ltx_text" style="font-size:70%;">GR-GPT</span></th>
<th id="S6.T8.10.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.1.1.7.1" class="ltx_text" style="font-size:70%;">GPV-2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T8.10.2.1" class="ltx_tr">
<th id="S6.T8.10.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T8.10.2.1.1.1" class="ltx_text" style="font-size:70%;">VilBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.10.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S6.T8.10.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T8.10.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.2.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T8.10.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.3.1" class="ltx_text" style="font-size:70%;">29.00</span></td>
<td id="S6.T8.10.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.4.1" class="ltx_text" style="font-size:70%;">27.19</span></td>
<td id="S6.T8.10.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.5.1" class="ltx_text" style="font-size:70%;">43.72</span></td>
<td id="S6.T8.10.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.6.1" class="ltx_text" style="font-size:70%;">59.72</span></td>
<td id="S6.T8.10.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.2.1.7.1" class="ltx_text" style="font-size:70%;">26.33</span></td>
</tr>
<tr id="S6.T8.10.3.2" class="ltx_tr">
<th id="S6.T8.10.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T8.10.3.2.1.1" class="ltx_text" style="font-size:70%;">LXMERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.10.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="S6.T8.10.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T8.10.3.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.2.1" class="ltx_text" style="font-size:70%;">28.07</span></td>
<td id="S6.T8.10.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.3.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T8.10.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.4.1" class="ltx_text" style="font-size:70%;">26.57</span></td>
<td id="S6.T8.10.3.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.5.1" class="ltx_text" style="font-size:70%;">44.39</span></td>
<td id="S6.T8.10.3.2.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.6.1" class="ltx_text" style="font-size:70%;">59.73</span></td>
<td id="S6.T8.10.3.2.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.3.2.7.1" class="ltx_text" style="font-size:70%;">27.44</span></td>
</tr>
<tr id="S6.T8.10.4.3" class="ltx_tr">
<th id="S6.T8.10.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T8.10.4.3.1.1" class="ltx_text" style="font-size:70%;">KRISP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.10.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S6.T8.10.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T8.10.4.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.2.1" class="ltx_text" style="font-size:70%;">30.44</span></td>
<td id="S6.T8.10.4.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.3.1" class="ltx_text" style="font-size:70%;">30.76</span></td>
<td id="S6.T8.10.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.4.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T8.10.4.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.5.1" class="ltx_text" style="font-size:70%;">44.18</span></td>
<td id="S6.T8.10.4.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.6.1" class="ltx_text" style="font-size:70%;">60.29</span></td>
<td id="S6.T8.10.4.3.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.4.3.7.1" class="ltx_text" style="font-size:70%;">27.43</span></td>
</tr>
<tr id="S6.T8.10.5.4" class="ltx_tr">
<th id="S6.T8.10.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T8.10.5.4.1.1" class="ltx_text" style="font-size:70%;">ClipCap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.10.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S6.T8.10.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T8.10.5.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.2.1" class="ltx_text" style="font-size:70%;">48.72</span></td>
<td id="S6.T8.10.5.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.3.1" class="ltx_text" style="font-size:70%;">49.98</span></td>
<td id="S6.T8.10.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.4.1" class="ltx_text" style="font-size:70%;">46.76</span></td>
<td id="S6.T8.10.5.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.5.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T8.10.5.4.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.6.1" class="ltx_text" style="font-size:70%;">55.94</span></td>
<td id="S6.T8.10.5.4.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.5.4.7.1" class="ltx_text" style="font-size:70%;">26.64</span></td>
</tr>
<tr id="S6.T8.10.6.5" class="ltx_tr">
<th id="S6.T8.10.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.1.1" class="ltx_text" style="font-size:70%;">GR-GPT</span></th>
<td id="S6.T8.10.6.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.2.1" class="ltx_text" style="font-size:70%;">50.27</span></td>
<td id="S6.T8.10.6.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.3.1" class="ltx_text" style="font-size:70%;">50.91</span></td>
<td id="S6.T8.10.6.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.4.1" class="ltx_text" style="font-size:70%;">48.67</span></td>
<td id="S6.T8.10.6.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.5.1" class="ltx_text" style="font-size:70%;">40.30</span></td>
<td id="S6.T8.10.6.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.6.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
<td id="S6.T8.10.6.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.6.5.7.1" class="ltx_text" style="font-size:70%;">29.20</span></td>
</tr>
<tr id="S6.T8.10.7.6" class="ltx_tr">
<th id="S6.T8.10.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S6.T8.10.7.6.1.1" class="ltx_text" style="font-size:70%;">GPV-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.10.7.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S6.T8.10.7.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S6.T8.10.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.2.1" class="ltx_text" style="font-size:70%;">51.09</span></td>
<td id="S6.T8.10.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.3.1" class="ltx_text" style="font-size:70%;">52.46</span></td>
<td id="S6.T8.10.7.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.4.1" class="ltx_text" style="font-size:70%;">49.57</span></td>
<td id="S6.T8.10.7.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.5.1" class="ltx_text" style="font-size:70%;">46.56</span></td>
<td id="S6.T8.10.7.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.6.1" class="ltx_text" style="font-size:70%;">61.94</span></td>
<td id="S6.T8.10.7.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S6.T8.10.7.6.7.1" class="ltx_text" style="font-size:70%;">0.00</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S6.T8.18.5.1" class="ltx_text" style="font-size:129%;">Table 8</span>: </span><span id="S6.T8.8.4" class="ltx_text ltx_font_bold" style="font-size:129%;">Pairwise difference between correctly answered questions.<span id="S6.T8.8.4.4" class="ltx_text ltx_font_medium"> For row <math id="S6.T8.5.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S6.T8.5.1.1.m1.1b"><mi id="S6.T8.5.1.1.m1.1.1" xref="S6.T8.5.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.T8.5.1.1.m1.1c"><ci id="S6.T8.5.1.1.m1.1.1.cmml" xref="S6.T8.5.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.5.1.1.m1.1d">i</annotation></semantics></math> and column <math id="S6.T8.6.2.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S6.T8.6.2.2.m2.1b"><mi id="S6.T8.6.2.2.m2.1.1" xref="S6.T8.6.2.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S6.T8.6.2.2.m2.1c"><ci id="S6.T8.6.2.2.m2.1.1.cmml" xref="S6.T8.6.2.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.6.2.2.m2.1d">j</annotation></semantics></math> of this table the value is percentage of questions answered correctly by model <math id="S6.T8.7.3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S6.T8.7.3.3.m3.1b"><mi id="S6.T8.7.3.3.m3.1.1" xref="S6.T8.7.3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.T8.7.3.3.m3.1c"><ci id="S6.T8.7.3.3.m3.1.1.cmml" xref="S6.T8.7.3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.7.3.3.m3.1d">i</annotation></semantics></math> that <math id="S6.T8.8.4.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S6.T8.8.4.4.m4.1b"><mi id="S6.T8.8.4.4.m4.1.1" xref="S6.T8.8.4.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S6.T8.8.4.4.m4.1c"><ci id="S6.T8.8.4.4.m4.1.1.cmml" xref="S6.T8.8.4.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.8.4.4.m4.1d">j</annotation></semantics></math> did not answer correctly. </span></span></figcaption>
</figure>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p"><span id="S6.p8.1.1" class="ltx_text ltx_font_bold">Qualitative Analysis.</span> We analyzed our models and extracted questions that all of the discussed models fail at. Figure <a href="#S6.F3" title="Figure 3 ‣ 6 Analysis of Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example from each knowledge type. This qualitative example shows what type of reasoning is missing in our current top performing models.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2206.01718/assets/x4.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="421" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S6.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Example questions that all discussed models fail at.<span id="S6.F3.4.2.1" class="ltx_text ltx_font_medium"> </span></span></figcaption>
</figure>
<div id="S6.p9" class="ltx_para">
<p id="S6.p9.1" class="ltx_p">All of these analyses together provide several interesting findings. First, aside from being generally difficult, the <span id="S6.p9.1.1" class="ltx_text">A-OKVQA</span> dataset shows a surprising lack of overlap in the specific questions different models answer correctly. Second, we see that different methods handle rare answers very differently. Moreover, different methods perform differently based on the type of knowledge. All of this suggests that <span id="S6.p9.1.2" class="ltx_text">A-OKVQA</span> provides many different kinds of challenging questions which bring out different strengths and weaknesses of methods.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Vision and language models have become progressively more powerful, however, evaluation of the reasoning capabilities of these models have not received adequate attention. To take a step in this direction, we propose a new knowledge-based VQA benchmark called <span id="S7.p1.1.1" class="ltx_text">A-OKVQA</span>, which primarily includes questions that require reasoning using commonsense and world knowledge. We provide <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">rationales</span> for each question so models can learn the line of reasoning that leads to the answer. We evaluate a large set of recent, high performance baselines. While they show impressive performance on the proposed task, it is evident that they lack the reasoning capability and/or the knowledge required to answer the questions, and there is a large room for improvement. Through extensive analyses, we show different models have different weaknesses and strengths. To solve <span id="S7.p1.1.3" class="ltx_text">A-OKVQA</span> and to move towards general multi-modal intelligence, we need to combine many types of capabilities from many different methods.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">VQA: visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Satanjeev Banerjee and Alon Lavie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Semantic parsing on freebase from question-answer pairs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Antoine Bordes, Sumit Chopra, and Jason Weston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Question answering with subgraph embeddings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Learning structured embeddings of knowledge bases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J.
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Yingshan Chang, Mridu Baldevraj Narang, Hisami Suzuki, Guihong Cao, Jianfeng
Gao, and Yonatan Bisk.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">WebQA: Multihop and multimodal qa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Reading Wikipedia to answer open-domain questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO Captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">BERT: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Are you talking to a machine? dataset and methods for multilingual
image question.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Noa García, Mayu Otani, Chenhui Chu, and Yuta Nakashima.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">KnowIT VQA: Answering knowledge-based questions about videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Visual turing test for computer vision systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the National Academy of Sciences</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Drew A. Hudson and Christopher D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1</a><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/sentence-transformers/nli-bert-base" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/sentence-transformers/nli-bert-base</a><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d</a><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, C. Thomas, Zuha Agha,
Nathan Ong, and Adriana Kovashka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Automatic understanding of image and video advertisements.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh
Ramakrishnan, and Soumen Chakrabarti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Select, substitute, search: A new benchmark for knowledge-augmented
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SIGIR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Pythia v0.1: the winning entry to the VQA challenge 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and
Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Webly supervised concept expansion for general purpose vision models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Visual Genome: connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Tell-and-answer: Towards explainable visual question answering using
attributes and captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Hugo Liu and Push Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">ConceptNet—a practical commonsense reasoning tool-kit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">BT technology journal</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">RoBERTa: A robustly optimized bert pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">VilBERT: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">12-in-1: Multi-task vision and language representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Mateusz Malinowski and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Kumar Gupta, and Marcus
Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">KRISP: Integrating implicit and symbolic knowledge for open-domain
knowledge-based VQA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">OK-VQA: A visual question answering benchmark requiring external
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Ron Mokady, Amir Hertz, and Amit H. Bermano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">ClipCap: CLIP prefix for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele,
Trevor Darrell, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Multimodal explanations: Justifying decisions and pointing to the
evidence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin
Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">VisualCOMET: Reasoning about the dynamic context of a still image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">GloVe: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Matt Post.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A call for clarity in reporting BLEU scores.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Machine Translation</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Language models are unsupervised multitask learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">OpenAI blog</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text
transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">JMLR</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Pranav Rajpurkar, Robin Jia, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Know what you don’t know: Unanswerable questions for SQuAD.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">SQuAD: 100,000+ questions for machine comprehension of text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Mengye Ren, Jamie Kiros, and Richard S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">KVQA: Knowledge-aware visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vedanuj Goswami, Vivek Natarajan, Yu Jiang, Xinlei Chen, Meet
Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">MMF: A multimodal framework for vision and language research.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/mmf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/mmf</a><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Towards VQA models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan
Salakhutdinov, and William W. Cohen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Open domain question answering using early fusion of knowledge bases
and text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">CommonsenseQA: A question answering challenge targeting commonsense
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL-HLT</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Hao Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">LXMERT: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
Urtasun, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">MovieQA: understanding stories in movies through
question-answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Explicit knowledge-based reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">FVQA: fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu
Chang, Gerald Tesauro, Bowen Zhou, and Jing Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">R3: Reinforced reader-ranker for open-domain question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Peter West, Chandra Bhagavatula, Jack Hessel, Jena D Hwang, Liwei Jiang,
Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Symbolic knowledge distillation: from general language models to
commonsense models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.07178</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li,
and Jimmy Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">End-to-end open-domain question answering with BERTserini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Yi Yang, Wen-tau Yih, and Christopher Meek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">WikiQA: A challenge dataset for open-domain question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">An empirical study of GPT-3 for few-shot knowledge-based VQA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Xuchen Yao and Benjamin Van Durme.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Information extraction over structured data: Question answering with
Freebase.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and
Joshua B. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Neural-symbolic VQA: Disentangling reasoning from vision and
language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Eunbyung Park, Alexander C. Berg, and Tamara L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Visual madlibs: Fill in the blank description generation and question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">From recognition to cognition: Visual commonsense reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">VinVL: Revisiting visual representations in vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Capturing long-tail distributions of object subcategories.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Visual7W: grounded question answering in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional details of dataset collection</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Examples of rejected questions</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">With a focus on overall question quality, we removed around 60% of questions written for having any of several flaws. The vast majority of questions removed exhibited one or more four flaws: 1) Only required recognition of a common object, 2) only required counting a readily specified object, 3) did not require looking at the image to answer, 4) only asked about the color of a readily specified object. Examples of questions from each of these categories are shown in Fig. <a href="#A1.F4" title="Figure 4 ‣ A.1 Examples of rejected questions ‣ Appendix A Additional details of dataset collection ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2206.01718/assets/x5.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A1.F4.3.2" class="ltx_text" style="font-size:90%;">Examples of questions rejected for not meeting our criteria.</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Data collection interface</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">The data-collection interface used by crowdworkers to write questions is shown in Fig. <a href="#A1.F5" title="Figure 5 ‣ A.2 Data collection interface ‣ Appendix A Additional details of dataset collection ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Detailed instructions along with examples of good and bad questions were provided. After writing a question, workers were required to press the “Check for similar question" button. This sent a request to a server which returned the five questions closest to those already written in our growing dataset. We asked workers to rewrite or rephrase questions that were too similar, but did not enforce a minimum distance cutoff. The set of questions queried were reset when collecting the val and test sets to allow a greater degree of overlap with the training set. After satisfied with their question, workers advanced to the next image. Each task workers performed included four images, nearby neighbors in a CLIP embedding space, which encouraged creative differences in questions written for similar images. Workers were only required to write two questions (out of four possible images) to allow them to skip images they didn’t feel they could write a suitable questions for. This cut down on unsuitable questions that they would have otherwise been forced to write in order to complete the task. After completing two questions, workers were allowed to submit their work and advance to the next image set.</p>
</div>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2206.01718/assets/x6.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A1.F5.3.2" class="ltx_text" style="font-size:90%;">Instructions and interface used for question collection.</span></figcaption>
</figure>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">The data-collection interface used by crowdworkers to write rationales is shown in Fig. <a href="#A1.F6" title="Figure 6 ‣ A.2 Data collection interface ‣ Appendix A Additional details of dataset collection ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Detailed instructions along with examples of good rationales were provided. We first asked workers to confirm the correct answer or provide the answer they thought was correct. This allowed a check on the correctness of the original question, and questions with a disagreement were removed from the dataset. Workers then provided a 1-2 sentence explanation of why the answer was correct that included any external knowledge needed to arrive there.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2206.01718/assets/x7.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A1.F6.3.2" class="ltx_text" style="font-size:90%;">Instructions and interface used for rationale collection.</span></figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details for Large-scale Pre-trained Models</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We produce the vocabulary for the experiments in Sec. <a href="#S5.SS2" title="5.2 Large-scale Pre-trained Models ‣ 5 Experiments ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> from the training set by selecting all correct choices, as well as all choices and direct answers that appear in at least three questions. This results in a vocabulary with 10,424 answers.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Discriminative models</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.3" class="ltx_p">We train all of our discriminative models for 500 epochs with a learning rate of <math id="A2.SS1.p1.1.m1.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="A2.SS1.p1.1.m1.1a"><mn id="A2.SS1.p1.1.m1.1.1" xref="A2.SS1.p1.1.m1.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.1.m1.1b"><cn type="float" id="A2.SS1.p1.1.m1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.1.m1.1c">0.01</annotation></semantics></math> and batch size of <math id="A2.SS1.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="A2.SS1.p1.2.m2.1a"><mn id="A2.SS1.p1.2.m2.1.1" xref="A2.SS1.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.2.m2.1b"><cn type="integer" id="A2.SS1.p1.2.m2.1.1.cmml" xref="A2.SS1.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.2.m2.1c">128</annotation></semantics></math>, except the model with ResNet input features, which is trained with a learning rate of <math id="A2.SS1.p1.3.m3.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="A2.SS1.p1.3.m3.1a"><mn id="A2.SS1.p1.3.m3.1.1" xref="A2.SS1.p1.3.m3.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.3.m3.1b"><cn type="float" id="A2.SS1.p1.3.m3.1.1.cmml" xref="A2.SS1.p1.3.m3.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.3.m3.1c">0.001</annotation></semantics></math>.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Contrastive models</h3>

<figure id="A2.F7" class="ltx_figure"><img src="/html/2206.01718/assets/x8.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A2.F7.3.2" class="ltx_text" style="font-size:90%;">As described in Sec. <a href="#A2.SS2" title="B.2 Contrastive models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>. CLIP-style contrastive loss between embeddings (of questions and images) and CLIP text encodings (of answers). Shown for a batch size of 4.</span></figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.2" class="ltx_p">The CLIP zero-shot setting requires no training. In the trained setting, we train our linear layer for 500 epochs with a learning rate of <math id="A2.SS2.p1.1.m1.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="A2.SS2.p1.1.m1.1a"><mn id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><cn type="float" id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">0.01</annotation></semantics></math> and batch size of <math id="A2.SS2.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="A2.SS2.p1.2.m2.1a"><mn id="A2.SS2.p1.2.m2.1.1" xref="A2.SS2.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.2.m2.1b"><cn type="integer" id="A2.SS2.p1.2.m2.1.1.cmml" xref="A2.SS2.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.2.m2.1c">128</annotation></semantics></math>. We further elaborate on our “CLIP-style contrastive loss” below and visualize it in Fig. <a href="#A2.F7" title="Figure 7 ‣ B.2 Contrastive models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.5" class="ltx_p">Recall that we have passed CLIP representations (for questions and/or images) through a linear layer to produce a 512-d embedding (the same size as a CLIP text encoding). For a batch of embeddings <math id="A2.SS2.p2.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="A2.SS2.p2.1.m1.1a"><mi id="A2.SS2.p2.1.m1.1.1" xref="A2.SS2.p2.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.1.m1.1b"><ci id="A2.SS2.p2.1.m1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.1.m1.1c">E</annotation></semantics></math> and the CLIP text encodings of their corresponding answers <math id="A2.SS2.p2.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="A2.SS2.p2.2.m2.1a"><mi id="A2.SS2.p2.2.m2.1.1" xref="A2.SS2.p2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.2.m2.1b"><ci id="A2.SS2.p2.2.m2.1.1.cmml" xref="A2.SS2.p2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.2.m2.1c">A</annotation></semantics></math>, we produce a cosine similarity matrix between <math id="A2.SS2.p2.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="A2.SS2.p2.3.m3.1a"><mi id="A2.SS2.p2.3.m3.1.1" xref="A2.SS2.p2.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.3.m3.1b"><ci id="A2.SS2.p2.3.m3.1.1.cmml" xref="A2.SS2.p2.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.3.m3.1c">E</annotation></semantics></math> and <math id="A2.SS2.p2.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="A2.SS2.p2.4.m4.1a"><mi id="A2.SS2.p2.4.m4.1.1" xref="A2.SS2.p2.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.4.m4.1b"><ci id="A2.SS2.p2.4.m4.1.1.cmml" xref="A2.SS2.p2.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.4.m4.1c">A</annotation></semantics></math> (i.e. the purple matrix in Fig. <a href="#A2.F7" title="Figure 7 ‣ B.2 Contrastive models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, showing a batch size of 4). We apply softmax over each matrix row (producing embedding–answer matching probabilities per embedding over answers in <math id="A2.SS2.p2.5.m5.1" class="ltx_Math" alttext="A" display="inline"><semantics id="A2.SS2.p2.5.m5.1a"><mi id="A2.SS2.p2.5.m5.1.1" xref="A2.SS2.p2.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.5.m5.1b"><ci id="A2.SS2.p2.5.m5.1.1.cmml" xref="A2.SS2.p2.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.5.m5.1c">A</annotation></semantics></math>) and compute a cross-entropy loss to maximize the similarity between each embedding and its corresponding answer.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Generative models</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">We show our modified ClipCap model in Fig. <a href="#A2.F8" title="Figure 8 ‣ B.3 Generative models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. As in ClipCap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we provide CLIP image representations to a mapping network, which produces prefix tokens as input for GPT-2. We then tokenize our question and ground-truth answer (appended with an end-of-sequence string, <math id="A2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\langle\textrm{EOS}\rangle" display="inline"><semantics id="A2.SS3.p1.1.m1.1a"><mrow id="A2.SS3.p1.1.m1.1.2.2" xref="A2.SS3.p1.1.m1.1.2.1.cmml"><mo stretchy="false" id="A2.SS3.p1.1.m1.1.2.2.1" xref="A2.SS3.p1.1.m1.1.2.1.1.cmml">⟨</mo><mtext id="A2.SS3.p1.1.m1.1.1" xref="A2.SS3.p1.1.m1.1.1a.cmml">EOS</mtext><mo stretchy="false" id="A2.SS3.p1.1.m1.1.2.2.2" xref="A2.SS3.p1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.1.m1.1b"><apply id="A2.SS3.p1.1.m1.1.2.1.cmml" xref="A2.SS3.p1.1.m1.1.2.2"><csymbol cd="latexml" id="A2.SS3.p1.1.m1.1.2.1.1.cmml" xref="A2.SS3.p1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A2.SS3.p1.1.m1.1.1a.cmml" xref="A2.SS3.p1.1.m1.1.1"><mtext id="A2.SS3.p1.1.m1.1.1.cmml" xref="A2.SS3.p1.1.m1.1.1">EOS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.1.m1.1c">\langle\textrm{EOS}\rangle</annotation></semantics></math>) and also provide these tokens as input. The remaining input tokens (in black) are zero-padding. As mentioned in our paper, we also appended the (pre-tokenized) question string with “Choices: …” during the MC setting.</p>
</div>
<div id="A2.SS3.p2" class="ltx_para">
<p id="A2.SS3.p2.6" class="ltx_p">This model is trained autoregressively. I.e., <math id="A2.SS3.p2.1.m1.1" class="ltx_Math" alttext="O_{i}" display="inline"><semantics id="A2.SS3.p2.1.m1.1a"><msub id="A2.SS3.p2.1.m1.1.1" xref="A2.SS3.p2.1.m1.1.1.cmml"><mi id="A2.SS3.p2.1.m1.1.1.2" xref="A2.SS3.p2.1.m1.1.1.2.cmml">O</mi><mi id="A2.SS3.p2.1.m1.1.1.3" xref="A2.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.1.m1.1b"><apply id="A2.SS3.p2.1.m1.1.1.cmml" xref="A2.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS3.p2.1.m1.1.1.1.cmml" xref="A2.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="A2.SS3.p2.1.m1.1.1.2.cmml" xref="A2.SS3.p2.1.m1.1.1.2">𝑂</ci><ci id="A2.SS3.p2.1.m1.1.1.3.cmml" xref="A2.SS3.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.1.m1.1c">O_{i}</annotation></semantics></math> is generated conditionally, given <math id="A2.SS3.p2.2.m2.1" class="ltx_Math" alttext="I_{0}\cdots I_{i}" display="inline"><semantics id="A2.SS3.p2.2.m2.1a"><mrow id="A2.SS3.p2.2.m2.1.1" xref="A2.SS3.p2.2.m2.1.1.cmml"><msub id="A2.SS3.p2.2.m2.1.1.2" xref="A2.SS3.p2.2.m2.1.1.2.cmml"><mi id="A2.SS3.p2.2.m2.1.1.2.2" xref="A2.SS3.p2.2.m2.1.1.2.2.cmml">I</mi><mn id="A2.SS3.p2.2.m2.1.1.2.3" xref="A2.SS3.p2.2.m2.1.1.2.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="A2.SS3.p2.2.m2.1.1.1" xref="A2.SS3.p2.2.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A2.SS3.p2.2.m2.1.1.3" xref="A2.SS3.p2.2.m2.1.1.3.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p2.2.m2.1.1.1a" xref="A2.SS3.p2.2.m2.1.1.1.cmml">​</mo><msub id="A2.SS3.p2.2.m2.1.1.4" xref="A2.SS3.p2.2.m2.1.1.4.cmml"><mi id="A2.SS3.p2.2.m2.1.1.4.2" xref="A2.SS3.p2.2.m2.1.1.4.2.cmml">I</mi><mi id="A2.SS3.p2.2.m2.1.1.4.3" xref="A2.SS3.p2.2.m2.1.1.4.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.2.m2.1b"><apply id="A2.SS3.p2.2.m2.1.1.cmml" xref="A2.SS3.p2.2.m2.1.1"><times id="A2.SS3.p2.2.m2.1.1.1.cmml" xref="A2.SS3.p2.2.m2.1.1.1"></times><apply id="A2.SS3.p2.2.m2.1.1.2.cmml" xref="A2.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS3.p2.2.m2.1.1.2.1.cmml" xref="A2.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS3.p2.2.m2.1.1.2.2.cmml" xref="A2.SS3.p2.2.m2.1.1.2.2">𝐼</ci><cn type="integer" id="A2.SS3.p2.2.m2.1.1.2.3.cmml" xref="A2.SS3.p2.2.m2.1.1.2.3">0</cn></apply><ci id="A2.SS3.p2.2.m2.1.1.3.cmml" xref="A2.SS3.p2.2.m2.1.1.3">⋯</ci><apply id="A2.SS3.p2.2.m2.1.1.4.cmml" xref="A2.SS3.p2.2.m2.1.1.4"><csymbol cd="ambiguous" id="A2.SS3.p2.2.m2.1.1.4.1.cmml" xref="A2.SS3.p2.2.m2.1.1.4">subscript</csymbol><ci id="A2.SS3.p2.2.m2.1.1.4.2.cmml" xref="A2.SS3.p2.2.m2.1.1.4.2">𝐼</ci><ci id="A2.SS3.p2.2.m2.1.1.4.3.cmml" xref="A2.SS3.p2.2.m2.1.1.4.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.2.m2.1c">I_{0}\cdots I_{i}</annotation></semantics></math> (for input tokens <math id="A2.SS3.p2.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="A2.SS3.p2.3.m3.1a"><mi id="A2.SS3.p2.3.m3.1.1" xref="A2.SS3.p2.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.3.m3.1b"><ci id="A2.SS3.p2.3.m3.1.1.cmml" xref="A2.SS3.p2.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.3.m3.1c">I</annotation></semantics></math> and output logits <math id="A2.SS3.p2.4.m4.1" class="ltx_Math" alttext="O" display="inline"><semantics id="A2.SS3.p2.4.m4.1a"><mi id="A2.SS3.p2.4.m4.1.1" xref="A2.SS3.p2.4.m4.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.4.m4.1b"><ci id="A2.SS3.p2.4.m4.1.1.cmml" xref="A2.SS3.p2.4.m4.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.4.m4.1c">O</annotation></semantics></math>), and supervised with a cross-entropy loss against the next sequence token <math id="A2.SS3.p2.5.m5.1" class="ltx_Math" alttext="I_{i+1}" display="inline"><semantics id="A2.SS3.p2.5.m5.1a"><msub id="A2.SS3.p2.5.m5.1.1" xref="A2.SS3.p2.5.m5.1.1.cmml"><mi id="A2.SS3.p2.5.m5.1.1.2" xref="A2.SS3.p2.5.m5.1.1.2.cmml">I</mi><mrow id="A2.SS3.p2.5.m5.1.1.3" xref="A2.SS3.p2.5.m5.1.1.3.cmml"><mi id="A2.SS3.p2.5.m5.1.1.3.2" xref="A2.SS3.p2.5.m5.1.1.3.2.cmml">i</mi><mo id="A2.SS3.p2.5.m5.1.1.3.1" xref="A2.SS3.p2.5.m5.1.1.3.1.cmml">+</mo><mn id="A2.SS3.p2.5.m5.1.1.3.3" xref="A2.SS3.p2.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.5.m5.1b"><apply id="A2.SS3.p2.5.m5.1.1.cmml" xref="A2.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS3.p2.5.m5.1.1.1.cmml" xref="A2.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="A2.SS3.p2.5.m5.1.1.2.cmml" xref="A2.SS3.p2.5.m5.1.1.2">𝐼</ci><apply id="A2.SS3.p2.5.m5.1.1.3.cmml" xref="A2.SS3.p2.5.m5.1.1.3"><plus id="A2.SS3.p2.5.m5.1.1.3.1.cmml" xref="A2.SS3.p2.5.m5.1.1.3.1"></plus><ci id="A2.SS3.p2.5.m5.1.1.3.2.cmml" xref="A2.SS3.p2.5.m5.1.1.3.2">𝑖</ci><cn type="integer" id="A2.SS3.p2.5.m5.1.1.3.3.cmml" xref="A2.SS3.p2.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.5.m5.1c">I_{i+1}</annotation></semantics></math>. In our case, we only compute this cross-entropy loss for outputs corresponding with the ground-truth answer tokens (including <math id="A2.SS3.p2.6.m6.1" class="ltx_Math" alttext="\langle\textrm{EOS}\rangle" display="inline"><semantics id="A2.SS3.p2.6.m6.1a"><mrow id="A2.SS3.p2.6.m6.1.2.2" xref="A2.SS3.p2.6.m6.1.2.1.cmml"><mo stretchy="false" id="A2.SS3.p2.6.m6.1.2.2.1" xref="A2.SS3.p2.6.m6.1.2.1.1.cmml">⟨</mo><mtext id="A2.SS3.p2.6.m6.1.1" xref="A2.SS3.p2.6.m6.1.1a.cmml">EOS</mtext><mo stretchy="false" id="A2.SS3.p2.6.m6.1.2.2.2" xref="A2.SS3.p2.6.m6.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p2.6.m6.1b"><apply id="A2.SS3.p2.6.m6.1.2.1.cmml" xref="A2.SS3.p2.6.m6.1.2.2"><csymbol cd="latexml" id="A2.SS3.p2.6.m6.1.2.1.1.cmml" xref="A2.SS3.p2.6.m6.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A2.SS3.p2.6.m6.1.1a.cmml" xref="A2.SS3.p2.6.m6.1.1"><mtext id="A2.SS3.p2.6.m6.1.1.cmml" xref="A2.SS3.p2.6.m6.1.1">EOS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p2.6.m6.1c">\langle\textrm{EOS}\rangle</annotation></semantics></math>).</p>
</div>
<div id="A2.SS3.p3" class="ltx_para">
<p id="A2.SS3.p3.2" class="ltx_p">At inference time, we prompt GPT-2 with our image prefix and question tokens. We have the model predict the most likely next token (i.e. generating a token in the answer) from the output logits. We append this token to the input and repeat this step, until the model predicts <math id="A2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\langle\textrm{EOS}\rangle" display="inline"><semantics id="A2.SS3.p3.1.m1.1a"><mrow id="A2.SS3.p3.1.m1.1.2.2" xref="A2.SS3.p3.1.m1.1.2.1.cmml"><mo stretchy="false" id="A2.SS3.p3.1.m1.1.2.2.1" xref="A2.SS3.p3.1.m1.1.2.1.1.cmml">⟨</mo><mtext id="A2.SS3.p3.1.m1.1.1" xref="A2.SS3.p3.1.m1.1.1a.cmml">EOS</mtext><mo stretchy="false" id="A2.SS3.p3.1.m1.1.2.2.2" xref="A2.SS3.p3.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.1.m1.1b"><apply id="A2.SS3.p3.1.m1.1.2.1.cmml" xref="A2.SS3.p3.1.m1.1.2.2"><csymbol cd="latexml" id="A2.SS3.p3.1.m1.1.2.1.1.cmml" xref="A2.SS3.p3.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A2.SS3.p3.1.m1.1.1a.cmml" xref="A2.SS3.p3.1.m1.1.1"><mtext id="A2.SS3.p3.1.m1.1.1.cmml" xref="A2.SS3.p3.1.m1.1.1">EOS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.1.m1.1c">\langle\textrm{EOS}\rangle</annotation></semantics></math>. We can use the tokenizer to decode these output tokens (excluding <math id="A2.SS3.p3.2.m2.1" class="ltx_Math" alttext="\langle\textrm{EOS}\rangle" display="inline"><semantics id="A2.SS3.p3.2.m2.1a"><mrow id="A2.SS3.p3.2.m2.1.2.2" xref="A2.SS3.p3.2.m2.1.2.1.cmml"><mo stretchy="false" id="A2.SS3.p3.2.m2.1.2.2.1" xref="A2.SS3.p3.2.m2.1.2.1.1.cmml">⟨</mo><mtext id="A2.SS3.p3.2.m2.1.1" xref="A2.SS3.p3.2.m2.1.1a.cmml">EOS</mtext><mo stretchy="false" id="A2.SS3.p3.2.m2.1.2.2.2" xref="A2.SS3.p3.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.2.m2.1b"><apply id="A2.SS3.p3.2.m2.1.2.1.cmml" xref="A2.SS3.p3.2.m2.1.2.2"><csymbol cd="latexml" id="A2.SS3.p3.2.m2.1.2.1.1.cmml" xref="A2.SS3.p3.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="A2.SS3.p3.2.m2.1.1a.cmml" xref="A2.SS3.p3.2.m2.1.1"><mtext id="A2.SS3.p3.2.m2.1.1.cmml" xref="A2.SS3.p3.2.m2.1.1">EOS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.2.m2.1c">\langle\textrm{EOS}\rangle</annotation></semantics></math>), producing our model’s textual answer prediction. Note that beam search is an alternative way to generate text from autoregressive language models, but we found that it led to worse results, likely because the answers we are trying to generate are short (e.g. 1-3 words).</p>
</div>
<div id="A2.SS3.p4" class="ltx_para">
<p id="A2.SS3.p4.1" class="ltx_p">We fine-tuned the models in our experiments (choosing the checkpoint with the best F1 validation score for generated answers over 10 epochs), using the settings and COCO pre-trained weights (for the MLP mapping network) made available by the ClipCap authors <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/rmokady/CLIP_prefix_caption" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rmokady/CLIP_prefix_caption</a></span></span></span>. For the pre-trained MLP model, they used CLIP ViT-B/32 features, produced 10 image prefix tokens, and had also fine-tuned GPT-2 (for their image captioning task). We further fine-tuned the GPT-2 weights on our task.</p>
</div>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2206.01718/assets/x9.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A2.F8.3.2" class="ltx_text" style="font-size:90%;">Diagram of modified ClipCap architecture for VQA tasks.</span></figcaption>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details for Rationale Generation</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We generated rationales from ClipCap in a nearly identical manner to how we generated answers (see Sec. <a href="#A2.SS3" title="B.3 Generative models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a> and Fig. <a href="#A2.F8" title="Figure 8 ‣ B.3 Generative models ‣ Appendix B Additional Details for Large-scale Pre-trained Models ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> above). However, we replace the ground-truth answer string/tokens with a ground-truth rationale. And, we don’t provide “Choices: …” in the ClipCap prompt for the MC setting. We also use beam search during generation, as it seems to perform better for these longer strings. We also use the MLP mapping network and continue to fine-tune GPT-2, as it demonstrates the best performance for this task. We again fine-tuned this model on our training data for 10 epochs and picked the checkpoints with best BLEU and METEOR validation scores.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">We show some examples of generated rationales in Fig. <a href="#A3.F9" title="Figure 9 ‣ Appendix C Additional Details for Rationale Generation ‣ A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="A3.F9" class="ltx_figure"><img src="/html/2206.01718/assets/x10.png" id="A3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A3.F9.3.2" class="ltx_text" style="font-size:90%;">Examples of rationales generated by our modified ClipCap method for examples in our validation set.</span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Details for Specialized Models</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">For all of these models, we use the same training hyperparameters as the original implementation. For all of the discriminative methods in the paper we use a fixed vocabulary constructed from direct answers that appeared two or more times in the training set. This includes 2,133 bi-grams or unigrams, with 1,937 words.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite></span> Pythia is a modification of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> that introduces changes to the architecture and learning schedule and utilizes more training data. We fine-tune it on the <span id="A4.p2.1.2" class="ltx_text">A-OKVQA</span> dataset. For fine-tuning, we replace the top classification layer with a randomly initialized layer for our set of answer vocabulary.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.1" class="ltx_p"><span id="A4.p3.1.1" class="ltx_text ltx_font_bold">LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite></span> LXMERT is a Transformer-based vision and language model pre-trained using a large amount of image-sentence pairs for a set of pre-training tasks such as masked language modeling and object prediction. The model is pre-trained on VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, VG-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, COCO captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and Visual Genome captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We then fine-tune the model using the training set of <span id="A4.p3.1.2" class="ltx_text">A-OKVQA</span>.</p>
</div>
<div id="A4.p4" class="ltx_para">
<p id="A4.p4.1" class="ltx_p"><span id="A4.p4.1.1" class="ltx_text ltx_font_bold">VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span> ViLBERT is an extension of the BERT architecture to process vision and language modalities for learning a joint representation for them. ViLBERT has been pre-trained on proxy tasks, but it has been evaluated on VQA as a downstream task. ViLBERT is pre-trained using Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and fine-tuned on <span id="A4.p4.1.2" class="ltx_text">A-OKVQA</span>. To evaluate how well a model trained on VQAv2 or OK-VQA performs on <span id="A4.p4.1.3" class="ltx_text">A-OKVQA</span>, we fine-tune ViLBERT after training them on thoese datasets. These models are referred to as ‘ViLBERT-VQA’ and ‘ViLBERT-OK-VQA’ in Table <span id="A4.p4.1.4" class="ltx_text" style="color:#FF0000;">5</span>.</p>
</div>
<div id="A4.p5" class="ltx_para">
<p id="A4.p5.1" class="ltx_p"><span id="A4.p5.1.1" class="ltx_text ltx_font_bold">KRISP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></span>
KRISP is a method for knowledge-based VQA which combines multi-modal Transformers with graph neural networks methods on knowledge graphs. We use the same models and data and knowledge sources and pre-processing steps as in that work, but filter the knowledge graph based on <span id="A4.p5.1.2" class="ltx_text">A-OKVQA</span> rather than OK-VQA (see Sec. 3.2 of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>).</p>
</div>
<div id="A4.p6" class="ltx_para">
<p id="A4.p6.1" class="ltx_p"><span id="A4.p6.1.1" class="ltx_text ltx_font_bold">GPV-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></span> GPV-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is a generative vision and language model built using the T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> language model and VinVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> image features. It was pre-trained on Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and then fine-tuned in a multi-task setting on image captioning, visual question answering, object localization, and classification, as well on web-search images for 10,000 visual concepts.</p>
</div>
<div id="A4.p7" class="ltx_para">
<p id="A4.p7.1" class="ltx_p">We fine-tune the fully-trained model on <span id="A4.p7.1.1" class="ltx_text">A-OKVQA</span> by training it to generate the most common answer for each question. For direct answer evaluations, answers are then generated using beam search with 20 beams. For multiple choice, the answers are ranked by the log-probability score assigned to them by the model.</p>
</div>
<div id="A4.p8" class="ltx_para">
<p id="A4.p8.2" class="ltx_p">We perform two additional experiments with rationales with this model. First, ground-truth rationales are appended to the question as additional input text. Recall that we do not provide rationales at test time. However, for this experiment we use them during test. We refer to this model as ‘GPV-2 + GT Ratl.’. Second, we use the same setting, but we replace every occurrence of the ground-truth answer in the rationale with the <math id="A4.p8.1.m1.1" class="ltx_Math" alttext="[" display="inline"><semantics id="A4.p8.1.m1.1a"><mo stretchy="false" id="A4.p8.1.m1.1.1" xref="A4.p8.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A4.p8.1.m1.1b"><ci id="A4.p8.1.m1.1.1.cmml" xref="A4.p8.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p8.1.m1.1c">[</annotation></semantics></math>answer<math id="A4.p8.2.m2.1" class="ltx_Math" alttext="]" display="inline"><semantics id="A4.p8.2.m2.1a"><mo stretchy="false" id="A4.p8.2.m2.1.1" xref="A4.p8.2.m2.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A4.p8.2.m2.1b"><ci id="A4.p8.2.m2.1.1.cmml" xref="A4.p8.2.m2.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p8.2.m2.1c">]</annotation></semantics></math> token. We refer to this model as ‘GPV-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> + Masked Ans.’ in Table <span id="A4.p8.2.1" class="ltx_text" style="color:#FF0000;">5</span>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.01717" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.01718" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.01718">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.01718" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.01719" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 20:03:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
