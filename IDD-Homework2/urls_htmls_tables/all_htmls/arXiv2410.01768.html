<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images</title>
<!--Generated on Wed Oct  2 17:25:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.01768v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S1" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S2" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.SS1" title="In 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>CLIP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.SS2" title="In 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>FeatUp</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.SS1" title="In 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>SimFeatUp</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.SS2" title="In 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Alleviating global bias</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.SS1" title="In 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.SS2" title="In 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.SS3" title="In 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Comparison to State-of-the-art</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.SS4" title="In 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Ablation Study and Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S6" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS1" title="In 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS2" title="In 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Building extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS3" title="In 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Road extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS4" title="In 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Flood Detection</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaiyu Li, Ruixun Liu, Xiangyong Cao<sup class="ltx_sup" id="id4.2.id1"><span class="ltx_text ltx_font_italic" id="id4.2.id1.1">†</span></sup>, Deyu Meng, Zhi Wang 
<br class="ltx_break"/>Xi’an Jiaotong University, China.
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.3.id2" style="font-size:90%;">likyoo.ai@gmail.com, liuruixun6343@gmail.com, caoxiangyong@mail.xjtu.edu.cn</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.4.id3" style="font-size:90%;">dymeng@mail.xjtu.edu.cn, zhiwang@xjtu.edu.cn</span>
<br class="ltx_break"/>Project: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://likyoo.github.io/SegEarth-OV" title="">https://likyoo.github.io/SegEarth-OV</a>
</span><span class="ltx_author_notes">†Corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Remote sensing image plays an irreplaceable role in fields such as agriculture, water resources, military, and disaster relief. Pixel-level interpretation is a critical aspect of remote sensing image applications; however, a prevalent limitation remains the need for extensive manual annotation. For this, we try to introduce open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle this issue, we propose a simple and general upsampler, SimFeatUp, to restore lost spatial information in deep features in a training-free style. Further, based on the observation of the abnormal response of local patch tokens to <span class="ltx_text ltx_font_typewriter" id="id7.id1.1">[CLS]</span> token in CLIP, we propose to execute a straightforward subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets spanning semantic segmentation, building extraction, road detection, and flood detection tasks. Our method achieves an average of 5.8%, 8.2%, 4%, and 15.3% improvement over state-of-the-art methods on 4 tasks. All codes are released.</p>
</div>
<div class="ltx_logical-block" id="id3">
<div class="ltx_para" id="id3.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="id2.g1" src="x1.png" width="830"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">Visualization and performance of SegEarth-OV on open-vocabulary semantic segmentation of remote sensing images. We evaluate on 17 remote sensing datasets (including semantic segmentation, building extraction, road extraction, and flood detection tasks), and our SegEarth-OV consistently generates high-quality segmentation masks.</span></figcaption>
</figure>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Remote sensing imagery has changed the way humans observe and understand the Earth. It enables us to monitor land cover/use types, respond effectively to natural disasters (e.g., fires, earthquakes, floods), gain insight into food and water resources, etc. Among the 17 Sustainable Development Goals (SDGs) issued by the United Nations, remote sensing imagery can provide important data support for several goals including “Zero Hunger”, “Clean Water and Sanitation”, “Industry, Innovation and Infrastructure”, “Climate Action”, “Life on Land” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>. Notably, remote sensing data can be considered as a distinct modality in machine learning. It involves more diverse spatial resolutions (from centimeters to kilometers), temporal dimensions (from hours to decades), and object perspectives (overhead and oriented) than natural images. Therefore, solutions designed for other data modalities (e.g., natural images) may be sub-optimal for remote sensing data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In recent years, raw remote sensing images are available from various sources (e.g., QuickBird, WorldView, Landsat, Sentinel), but obtaining large-scale labels is still a challenge due to expensive manual costs. Besides, on the broad surface of the earth, “stuff” (e.g., grassland, cropland, roads, forests, etc.) occupies much more area than “things” (e.g., buildings, ships, airplanes, etc.) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>. Therefore, for remote sensing images, pixel-level perception, i.e., segmentation, is applied more frequently than instance-level perception, and the demand for pixel-level annotation exacerbates the difficulty of obtaining large-scale labels. OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> is a popular solution that aims to create a freely usable, editable and shareable map of the world. However, the completeness of the annotations in OpenStreetMap is affected by regional income levels, which results in limited data availability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. The rise of vision language model (VLM) brings us new inspirations with its capabilities of open-vocabulary semantic segmentation (OVSS). However, through some exploratory experiments, we find that the solution designed for natural images is sub-optimal on remote sensing images. A notable phenomenon is the presence of distorted target shapes and ill-fitting boundaries in the prediction mask, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S1.F2" title="In 1 Introduction ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">Limitations of state-of-the-art OVSS methods in remote sensing images, the two predictions on the left present distorted target shapes and ill-fitting boundaries. (best viewed digitally with zoom, especially for the edges of the object)</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Empirically, we believe that these issues can be largely attributed to excessively low feature resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>: In the current CLIP-based OVSS paradigm, the feature maps from CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite> are downsampled to <math alttext="1/16th" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mrow id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml"><mn id="S1.p3.1.m1.1.1.2.2" xref="S1.p3.1.m1.1.1.2.2.cmml">1</mn><mo id="S1.p3.1.m1.1.1.2.1" xref="S1.p3.1.m1.1.1.2.1.cmml">/</mo><mn id="S1.p3.1.m1.1.1.2.3" xref="S1.p3.1.m1.1.1.2.3.cmml">16</mn></mrow><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">t</mi><mo id="S1.p3.1.m1.1.1.1a" xref="S1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p3.1.m1.1.1.4" xref="S1.p3.1.m1.1.1.4.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><times id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1"></times><apply id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2"><divide id="S1.p3.1.m1.1.1.2.1.cmml" xref="S1.p3.1.m1.1.1.2.1"></divide><cn id="S1.p3.1.m1.1.1.2.2.cmml" type="integer" xref="S1.p3.1.m1.1.1.2.2">1</cn><cn id="S1.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S1.p3.1.m1.1.1.2.3">16</cn></apply><ci id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">𝑡</ci><ci id="S1.p3.1.m1.1.1.4.cmml" xref="S1.p3.1.m1.1.1.4">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">1/16th</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">1 / 16 italic_t italic_h</annotation></semantics></math> of the original image (ViT-B/16). Hence, in this paper, we propose a simple and general feature upsampler, SimFeatUp, which is trained with the goal of reconstructing content-invariant high-resolution (HR) features on a few unlabeled images, and can upsample arbitrary remote sensing image features after training. Thanks to this property of SimFeatUp, it can be used as a universal external unit for training-free OVSS framework. Further, CLIP is trained at the image level, it uses the <span class="ltx_text ltx_font_typewriter" id="S1.p3.1.1">[CLS]</span> token as a representation of the entire image, and attaches global properties to the local token <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>. However, this global property biases local features against patch-level inference in OVSS. We find that a simple subtraction operation of local patch features and global features can effectively reduce global bias. Extensive quantitative and qualitative experiments demonstrate the superior segmentation quality of our method over prior works.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Contributions:</span></p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present SimFeatUp, an general upsampler for training-free OVSS which robustly upsamples low-resolution (LR) features and maintains semantic consistency with image content.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We present an extremely simple and straightforward way to alleviate the global bias problem of CLIP, i.e., executing subtraction operations of local and global tokens.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our final presented model, named SegEarth-OV, achieves state-of-the-art performance on 17 remote sensing datasets spanning semantic segmentation, building extraction, road extraction, and flood detection tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Vision-Language Model.</span>
Recently, foundation models, especially visual language models, have energized the field of computer vision. One phenomenal advance is contrastive language-vision pretraining, i.e., CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, which elegantly bridges the gap between images and natural language. By training with massive data in a multimodal embedding space, CLIP gains strong transfer capabilities, achieving leaps in zero-shot learning and making OV learning possible <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>. Subsequently, related research has gradually emerged, from the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>, training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> or model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> side. However, CLIP focuses only on global <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.2">[CLS]</span> tokens, and even though patch-level tokens can be generated, they are inevitably contaminated by global bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, which is detrimental to dense prediction. In addition, several remote sensing VLMs emerge, they adapt general VLMs to remote sensing contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite> or mine the characteristics of remote sensing data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Supervised semantic segmentation.</span>
Semantic segmentation aims to discriminate images at the pixel level. The prediction head (aka decoder), as an essential component of segmentation models, is able to upsample LR feature maps into HR predictions. Typical prediction heads contain upsampling operators (e.g., bilinear interpolation, JBU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>) and HR encoder features (as guidance), e.g., UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>, UperNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite>, Semantic FPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, MaskFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, etc. Some works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite> focus on dynamic, learnable upsampling operators that make this process content-aware. FeatUp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> constructs a model-agnostic upsampling scheme that uses multi-view consistency loss with deep analogies to NeRFs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>. <span class="ltx_text ltx_font_bold" id="S2.p2.1.2">However, it only explores the condition with labels.</span> Inspired by FeatUp and built on top of it, the SimFeatUp proposed in this work is able to significantly improve OVSS without any labels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Open-Vocabulary Semantic Segmentation.</span>
As VLMs have shown remarkable zero-shot inference in image classification, which naturally extends to semantic segmentation. They empower the segmentation pipeline to recognize seen and unseen categories, and users can segment almost any category in an image using prompt vocabulary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>. We divide current CLIP-based OVSS methods into two groups: training-required and training-free. The former allows models to be trained on some base classes in a supervised or weakly supervised manner and is therefore more flexible. Typically, some works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> try to train a localization-aware CLIP which can naturally make dense predictions, while others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> select a subset of the CLIP’s pre-trained parameters and/or introduce a limited number of trainable parameters into the frozen CLIP, i.e., fine-tuning the CLIP to adapt to dense prediction on base classes. Still, training-free OVSS methods emphasize tapping into CLIP’s inherent localization capabilities with limited surgery of features or structures. MaskCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> pioneers the removal of query and key projections at the attention pooling layer of CLIP’s image encoder. Following it, subsequent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> adequately explore self-self attention (i.e., <span class="ltx_text ltx_font_italic" id="S2.p3.1.2">q-q</span>, <span class="ltx_text ltx_font_italic" id="S2.p3.1.3">k-k</span> or <span class="ltx_text ltx_font_italic" id="S2.p3.1.4">v-v</span> self-attention), and these modifications somewhat mitigate noisy activations and spatial invariant perception of CLIP. Another stream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> is the two-stage method, which first generates category-agnostic mask proposals and then classifies the masks. Besides, some other foundation models (e.g. DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>) can be introduced to enhance the localization ability of CLIP, and these explorations also make sense <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Different from previous methods, we focus on the inherent characteristics of remote sensing images rather than the general attributes of natural images. Our SimFeatUp component, although it needs to be trained on a few images-only data beforehand, this process is independent of the semantic segmentation process and the trained weights can be used for almost any remote sensing data (like the foundation model in other works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>), so our method can still be seen as a training-free method.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S2.F3.sf1.g1" src="x3.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S2.F3.sf2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">Illustration of the proposed method. (a) is the training process of SimFeatUp. CLIP is frozen and only SimFeatUp is useful in reasoning. (b) is the reasoning process of SegEarth-OV. The LR feature maps from CLIP are upsampled by SimFeatUp and then the <span class="ltx_text ltx_font_typewriter" id="S2.F3.4.2.1">[CLS]</span> token is subtracted to alleviate global bias. For better presentation, the color renderings follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>CLIP</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5">In ViT-based CLIP, the image encoder consists of a series of Transformer blocks. Let <math alttext="X=\left[x_{cls},x_{1},...,x_{h\times w}\right]^{\mathsf{T}}\in\mathbb{R}^{(hw+%
1,d)}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.6"><semantics id="S3.SS1.p1.1.m1.6a"><mrow id="S3.SS1.p1.1.m1.6.6" xref="S3.SS1.p1.1.m1.6.6.cmml"><mi id="S3.SS1.p1.1.m1.6.6.5" xref="S3.SS1.p1.1.m1.6.6.5.cmml">X</mi><mo id="S3.SS1.p1.1.m1.6.6.6" xref="S3.SS1.p1.1.m1.6.6.6.cmml">=</mo><msup id="S3.SS1.p1.1.m1.6.6.3" xref="S3.SS1.p1.1.m1.6.6.3.cmml"><mrow id="S3.SS1.p1.1.m1.6.6.3.3.3" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml"><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.4" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml">[</mo><msub id="S3.SS1.p1.1.m1.4.4.1.1.1.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.cmml">x</mi><mrow id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.3.cmml">l</mi><mo id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1a" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.4" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.5" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.5.5.2.2.2.2" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.5.5.2.2.2.2.2" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2.2.cmml">x</mi><mn id="S3.SS1.p1.1.m1.5.5.2.2.2.2.3" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.6" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml">,</mo><mi id="S3.SS1.p1.1.m1.3.3" mathvariant="normal" xref="S3.SS1.p1.1.m1.3.3.cmml">…</mi><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.7" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.6.6.3.3.3.3" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.6.6.3.3.3.3.2" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.2.cmml">x</mi><mrow id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.2" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.2.cmml">h</mi><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.3" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.3.cmml">w</mi></mrow></msub><mo id="S3.SS1.p1.1.m1.6.6.3.3.3.8" xref="S3.SS1.p1.1.m1.6.6.3.3.4.cmml">]</mo></mrow><mi id="S3.SS1.p1.1.m1.6.6.3.5" xref="S3.SS1.p1.1.m1.6.6.3.5.cmml">𝖳</mi></msup><mo id="S3.SS1.p1.1.m1.6.6.7" xref="S3.SS1.p1.1.m1.6.6.7.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.6.6.8" xref="S3.SS1.p1.1.m1.6.6.8.cmml"><mi id="S3.SS1.p1.1.m1.6.6.8.2" xref="S3.SS1.p1.1.m1.6.6.8.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml"><mo id="S3.SS1.p1.1.m1.2.2.2.2.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.2.2.2.2.1" xref="S3.SS1.p1.1.m1.2.2.2.2.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.2.2.1.2" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.1.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.2.cmml">h</mi><mo id="S3.SS1.p1.1.m1.2.2.2.2.1.2.1" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.1.cmml">⁢</mo><mi id="S3.SS1.p1.1.m1.2.2.2.2.1.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.3.cmml">w</mi></mrow><mo id="S3.SS1.p1.1.m1.2.2.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.cmml">+</mo><mn id="S3.SS1.p1.1.m1.2.2.2.2.1.3" xref="S3.SS1.p1.1.m1.2.2.2.2.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml">d</mi><mo id="S3.SS1.p1.1.m1.2.2.2.2.4" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.6b"><apply id="S3.SS1.p1.1.m1.6.6.cmml" xref="S3.SS1.p1.1.m1.6.6"><and id="S3.SS1.p1.1.m1.6.6a.cmml" xref="S3.SS1.p1.1.m1.6.6"></and><apply id="S3.SS1.p1.1.m1.6.6b.cmml" xref="S3.SS1.p1.1.m1.6.6"><eq id="S3.SS1.p1.1.m1.6.6.6.cmml" xref="S3.SS1.p1.1.m1.6.6.6"></eq><ci id="S3.SS1.p1.1.m1.6.6.5.cmml" xref="S3.SS1.p1.1.m1.6.6.5">𝑋</ci><apply id="S3.SS1.p1.1.m1.6.6.3.cmml" xref="S3.SS1.p1.1.m1.6.6.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.6.6.3.4.cmml" xref="S3.SS1.p1.1.m1.6.6.3">superscript</csymbol><list id="S3.SS1.p1.1.m1.6.6.3.3.4.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3"><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.2">𝑥</ci><apply id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3"><times id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.1"></times><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.2">𝑐</ci><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.3">𝑙</ci><ci id="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.4.cmml" xref="S3.SS1.p1.1.m1.4.4.1.1.1.1.3.4">𝑠</ci></apply></apply><apply id="S3.SS1.p1.1.m1.5.5.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.5.5.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.5.5.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2.2">𝑥</ci><cn id="S3.SS1.p1.1.m1.5.5.2.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.5.5.2.2.2.2.3">1</cn></apply><ci id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">…</ci><apply id="S3.SS1.p1.1.m1.6.6.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.6.6.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.6.6.3.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.2">𝑥</ci><apply id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3"><times id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.1"></times><ci id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.2">ℎ</ci><ci id="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.6.6.3.3.3.3.3.3">𝑤</ci></apply></apply></list><ci id="S3.SS1.p1.1.m1.6.6.3.5.cmml" xref="S3.SS1.p1.1.m1.6.6.3.5">𝖳</ci></apply></apply><apply id="S3.SS1.p1.1.m1.6.6c.cmml" xref="S3.SS1.p1.1.m1.6.6"><in id="S3.SS1.p1.1.m1.6.6.7.cmml" xref="S3.SS1.p1.1.m1.6.6.7"></in><share href="https://arxiv.org/html/2410.01768v1#S3.SS1.p1.1.m1.6.6.3.cmml" id="S3.SS1.p1.1.m1.6.6d.cmml" xref="S3.SS1.p1.1.m1.6.6"></share><apply id="S3.SS1.p1.1.m1.6.6.8.cmml" xref="S3.SS1.p1.1.m1.6.6.8"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.6.6.8.1.cmml" xref="S3.SS1.p1.1.m1.6.6.8">superscript</csymbol><ci id="S3.SS1.p1.1.m1.6.6.8.2.cmml" xref="S3.SS1.p1.1.m1.6.6.8.2">ℝ</ci><interval closure="open" id="S3.SS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><apply id="S3.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1"><plus id="S3.SS1.p1.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1"></plus><apply id="S3.SS1.p1.1.m1.2.2.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2"><times id="S3.SS1.p1.1.m1.2.2.2.2.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.1"></times><ci id="S3.SS1.p1.1.m1.2.2.2.2.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.2">ℎ</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.3">𝑤</ci></apply><cn id="S3.SS1.p1.1.m1.2.2.2.2.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.2.2.2.2.1.3">1</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">𝑑</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.6c">X=\left[x_{cls},x_{1},...,x_{h\times w}\right]^{\mathsf{T}}\in\mathbb{R}^{(hw+%
1,d)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.6d">italic_X = [ italic_x start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_h × italic_w end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT ( italic_h italic_w + 1 , italic_d ) end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the input of the last block, where <math alttext="h" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_h</annotation></semantics></math> and <math alttext="w" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">w</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_w</annotation></semantics></math> denote the height and width of the feature map, <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_d</annotation></semantics></math> denotes the dimention of tokens, and <math alttext="x_{cls}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">x</mi><mrow id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p1.5.m5.1.1.3.1" xref="S3.SS1.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml">l</mi><mo id="S3.SS1.p1.5.m5.1.1.3.1a" xref="S3.SS1.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p1.5.m5.1.1.3.4" xref="S3.SS1.p1.5.m5.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑥</ci><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><times id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.1"></times><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">𝑐</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3">𝑙</ci><ci id="S3.SS1.p1.5.m5.1.1.3.4.cmml" xref="S3.SS1.p1.5.m5.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">x_{cls}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is a learnable global token and the others are local tokens from different image patches. The forward process of this block can be formulated as:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\begin{array}[]{c}\boldsymbol{q}=\operatorname{Emb}_{q}(%
\boldsymbol{X}),\boldsymbol{k}=\operatorname{Emb}_{k}(\boldsymbol{X}),%
\boldsymbol{v}=\operatorname{Emb}_{v}(\boldsymbol{X}),\\
\boldsymbol{y}=\boldsymbol{x}+\operatorname{SA}\left(\boldsymbol{q},%
\boldsymbol{k},\boldsymbol{v}\right),\\
\boldsymbol{z}=\boldsymbol{y}+\operatorname{FFN}(\operatorname{LN}(\boldsymbol%
{y})),\end{array}" class="ltx_Math" display="inline" id="S3.E1X.2.1.1.m1.13"><semantics id="S3.E1X.2.1.1.m1.13a"><mtable id="S3.E1X.2.1.1.m1.13.13" rowspacing="0pt" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mtr id="S3.E1X.2.1.1.m1.13.13a" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mtd id="S3.E1X.2.1.1.m1.13.13b" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.3.cmml"><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.3.cmml">𝒒</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml"><msub id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.2.cmml">Emb</mi><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1a" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.2.1" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.1.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.1.1.1.1.1.1.cmml">𝑿</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.2.2" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.3a.cmml">,</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.3.cmml"><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.3.cmml">𝒌</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml"><msub id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.2.cmml">Emb</mi><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1a" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.2.1" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.2.2.2.2.2.2" xref="S3.E1X.2.1.1.m1.2.2.2.2.2.2.cmml">𝑿</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.2.2" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.3a.cmml">,</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.3.cmml">𝒗</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml"><msub id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.2.cmml">Emb</mi><mi id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.3" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.3.cmml">v</mi></msub><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1a" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.2" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.2.1" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.3.3.3.3.3.3" xref="S3.E1X.2.1.1.m1.3.3.3.3.3.3.cmml">𝑿</mi><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.2.2" stretchy="false" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.2" xref="S3.E1X.2.1.1.m1.13.13.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1X.2.1.1.m1.13.13c" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mtd id="S3.E1X.2.1.1.m1.13.13d" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mrow id="S3.E1X.2.1.1.m1.9.9.9.5.5.5" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.cmml"><mrow id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.cmml"><mi id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.2" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.2.cmml">𝒚</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.1" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.1.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.cmml"><mi id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.2" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.2.cmml">𝒙</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.1" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.1.cmml">+</mo><mrow id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml"><mi id="S3.E1X.2.1.1.m1.5.5.5.1.1.1" xref="S3.E1X.2.1.1.m1.5.5.5.1.1.1.cmml">SA</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2a" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2.1" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml"><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2.1.1" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.6.6.6.2.2.2" xref="S3.E1X.2.1.1.m1.6.6.6.2.2.2.cmml">𝒒</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2.1.2" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml">,</mo><mi id="S3.E1X.2.1.1.m1.7.7.7.3.3.3" xref="S3.E1X.2.1.1.m1.7.7.7.3.3.3.cmml">𝒌</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2.1.3" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml">,</mo><mi id="S3.E1X.2.1.1.m1.8.8.8.4.4.4" xref="S3.E1X.2.1.1.m1.8.8.8.4.4.4.cmml">𝒗</mi><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2.1.4" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.2" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1X.2.1.1.m1.13.13e" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mtd id="S3.E1X.2.1.1.m1.13.13f" xref="S3.E1X.2.1.1.m1.13.13.cmml"><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.cmml"><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.cmml"><mi id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.3" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.3.cmml">𝒛</mi><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.2" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.3" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.3.cmml">𝒚</mi><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.2" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.2.cmml">+</mo><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml"><mi id="S3.E1X.2.1.1.m1.12.12.12.3.3.3" xref="S3.E1X.2.1.1.m1.12.12.12.3.3.3.cmml">FFN</mi><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1a" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml"><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml">(</mo><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.10.10.10.1.1.1" xref="S3.E1X.2.1.1.m1.10.10.10.1.1.1.cmml">LN</mi><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2a" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml">⁡</mo><mrow id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2.1" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml"><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2.1.1" stretchy="false" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.11.11.11.2.2.2" xref="S3.E1X.2.1.1.m1.11.11.11.2.2.2.cmml">𝒚</mi><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2.1.2" stretchy="false" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.3" stretchy="false" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.2" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.13b"><matrix id="S3.E1X.2.1.1.m1.13.13.cmml" xref="S3.E1X.2.1.1.m1.13.13"><matrixrow id="S3.E1X.2.1.1.m1.13.13a.cmml" xref="S3.E1X.2.1.1.m1.13.13"><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.3a.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.3">formulae-sequence</csymbol><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1"><eq id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.2"></eq><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.3">𝒒</ci><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1"><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.2">Emb</ci><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.1.1.1.1.1.3">𝑞</ci></apply><ci id="S3.E1X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.1.1.1">𝑿</ci></apply></apply><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.3a.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1"><eq id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.2"></eq><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.3">𝒌</ci><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1"><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.2">Emb</ci><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.1.1.1.1.1.3">𝑘</ci></apply><ci id="S3.E1X.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.2.2.2.2.2.2">𝑿</ci></apply></apply><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2"><eq id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.2"></eq><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.3">𝒗</ci><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1"><apply id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.2">Emb</ci><ci id="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.4.4.4.4.4.4.1.2.2.2.2.1.1.1.3">𝑣</ci></apply><ci id="S3.E1X.2.1.1.m1.3.3.3.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.3.3.3.3">𝑿</ci></apply></apply></apply></apply></matrixrow><matrixrow id="S3.E1X.2.1.1.m1.13.13b.cmml" xref="S3.E1X.2.1.1.m1.13.13"><apply id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5"><eq id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.1.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.1"></eq><ci id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.2.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.2">𝒚</ci><apply id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3"><plus id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.1"></plus><ci id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.2">𝒙</ci><apply id="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.9.9.9.5.5.5.1.3.3.2"><ci id="S3.E1X.2.1.1.m1.5.5.5.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.5.5.5.1.1.1">SA</ci><ci id="S3.E1X.2.1.1.m1.6.6.6.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.6.6.6.2.2.2">𝒒</ci><ci id="S3.E1X.2.1.1.m1.7.7.7.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.7.7.7.3.3.3">𝒌</ci><ci id="S3.E1X.2.1.1.m1.8.8.8.4.4.4.cmml" xref="S3.E1X.2.1.1.m1.8.8.8.4.4.4">𝒗</ci></apply></apply></apply></matrixrow><matrixrow id="S3.E1X.2.1.1.m1.13.13c.cmml" xref="S3.E1X.2.1.1.m1.13.13"><apply id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4"><eq id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.2.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.2"></eq><ci id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.3.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.3">𝒛</ci><apply id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1"><plus id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.2"></plus><ci id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.3">𝒚</ci><apply id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1"><ci id="S3.E1X.2.1.1.m1.12.12.12.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.12.12.12.3.3.3">FFN</ci><apply id="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.13.13.13.4.4.4.1.1.1.1.1.1.2"><ci id="S3.E1X.2.1.1.m1.10.10.10.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.10.10.10.1.1.1">LN</ci><ci id="S3.E1X.2.1.1.m1.11.11.11.2.2.2.cmml" xref="S3.E1X.2.1.1.m1.11.11.11.2.2.2">𝒚</ci></apply></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.13c">\displaystyle\begin{array}[]{c}\boldsymbol{q}=\operatorname{Emb}_{q}(%
\boldsymbol{X}),\boldsymbol{k}=\operatorname{Emb}_{k}(\boldsymbol{X}),%
\boldsymbol{v}=\operatorname{Emb}_{v}(\boldsymbol{X}),\\
\boldsymbol{y}=\boldsymbol{x}+\operatorname{SA}\left(\boldsymbol{q},%
\boldsymbol{k},\boldsymbol{v}\right),\\
\boldsymbol{z}=\boldsymbol{y}+\operatorname{FFN}(\operatorname{LN}(\boldsymbol%
{y})),\end{array}</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.2.1.1.m1.13d">start_ARRAY start_ROW start_CELL bold_italic_q = roman_Emb start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( bold_italic_X ) , bold_italic_k = roman_Emb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_X ) , bold_italic_v = roman_Emb start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( bold_italic_X ) , end_CELL end_ROW start_ROW start_CELL bold_italic_y = bold_italic_x + roman_SA ( bold_italic_q , bold_italic_k , bold_italic_v ) , end_CELL end_ROW start_ROW start_CELL bold_italic_z = bold_italic_y + roman_FFN ( roman_LN ( bold_italic_y ) ) , end_CELL end_ROW end_ARRAY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.7">where <math alttext="\boldsymbol{q}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\boldsymbol{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">bold_italic_q</annotation></semantics></math>, <math alttext="\boldsymbol{k}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">𝒌</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝒌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\boldsymbol{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">bold_italic_k</annotation></semantics></math>, <math alttext="\boldsymbol{v}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\boldsymbol{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">bold_italic_v</annotation></semantics></math> denote Query, Key, and Value, respectively. <math alttext="\operatorname{Emb}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">Emb</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">Emb</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\operatorname{Emb}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">roman_Emb</annotation></semantics></math> consists of a layer normalization (LN) layer and a linear layer, and FFN denotes the feed-forward network. <math alttext="\operatorname{SA}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">SA</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">SA</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\operatorname{SA}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">roman_SA</annotation></semantics></math> denotes a standard self-attention module, i.e., <math alttext="\operatorname{SA}(\boldsymbol{q},\boldsymbol{k},\boldsymbol{v})=\text{softmax}%
(\frac{\boldsymbol{q}\cdot\boldsymbol{k}^{\mathsf{T}}}{\sqrt{d}})\cdot%
\boldsymbol{v}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.5"><semantics id="S3.SS1.p3.6.m6.5a"><mrow id="S3.SS1.p3.6.m6.5.6" xref="S3.SS1.p3.6.m6.5.6.cmml"><mrow id="S3.SS1.p3.6.m6.5.6.2.2" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">SA</mi><mo id="S3.SS1.p3.6.m6.5.6.2.2a" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml">⁡</mo><mrow id="S3.SS1.p3.6.m6.5.6.2.2.1" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml"><mo id="S3.SS1.p3.6.m6.5.6.2.2.1.1" stretchy="false" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml">(</mo><mi id="S3.SS1.p3.6.m6.2.2" xref="S3.SS1.p3.6.m6.2.2.cmml">𝒒</mi><mo id="S3.SS1.p3.6.m6.5.6.2.2.1.2" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml">,</mo><mi id="S3.SS1.p3.6.m6.3.3" xref="S3.SS1.p3.6.m6.3.3.cmml">𝒌</mi><mo id="S3.SS1.p3.6.m6.5.6.2.2.1.3" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml">,</mo><mi id="S3.SS1.p3.6.m6.4.4" xref="S3.SS1.p3.6.m6.4.4.cmml">𝒗</mi><mo id="S3.SS1.p3.6.m6.5.6.2.2.1.4" stretchy="false" xref="S3.SS1.p3.6.m6.5.6.2.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.6.m6.5.6.1" xref="S3.SS1.p3.6.m6.5.6.1.cmml">=</mo><mrow id="S3.SS1.p3.6.m6.5.6.3" xref="S3.SS1.p3.6.m6.5.6.3.cmml"><mrow id="S3.SS1.p3.6.m6.5.6.3.2" xref="S3.SS1.p3.6.m6.5.6.3.2.cmml"><mtext id="S3.SS1.p3.6.m6.5.6.3.2.2" xref="S3.SS1.p3.6.m6.5.6.3.2.2a.cmml">softmax</mtext><mo id="S3.SS1.p3.6.m6.5.6.3.2.1" xref="S3.SS1.p3.6.m6.5.6.3.2.1.cmml">⁢</mo><mrow id="S3.SS1.p3.6.m6.5.6.3.2.3.2" xref="S3.SS1.p3.6.m6.5.5.cmml"><mo id="S3.SS1.p3.6.m6.5.6.3.2.3.2.1" stretchy="false" xref="S3.SS1.p3.6.m6.5.5.cmml">(</mo><mfrac id="S3.SS1.p3.6.m6.5.5" xref="S3.SS1.p3.6.m6.5.5.cmml"><mrow id="S3.SS1.p3.6.m6.5.5.2" xref="S3.SS1.p3.6.m6.5.5.2.cmml"><mi id="S3.SS1.p3.6.m6.5.5.2.2" xref="S3.SS1.p3.6.m6.5.5.2.2.cmml">𝒒</mi><mo id="S3.SS1.p3.6.m6.5.5.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.6.m6.5.5.2.1.cmml">⋅</mo><msup id="S3.SS1.p3.6.m6.5.5.2.3" xref="S3.SS1.p3.6.m6.5.5.2.3.cmml"><mi id="S3.SS1.p3.6.m6.5.5.2.3.2" xref="S3.SS1.p3.6.m6.5.5.2.3.2.cmml">𝒌</mi><mi id="S3.SS1.p3.6.m6.5.5.2.3.3" xref="S3.SS1.p3.6.m6.5.5.2.3.3.cmml">𝖳</mi></msup></mrow><msqrt id="S3.SS1.p3.6.m6.5.5.3" xref="S3.SS1.p3.6.m6.5.5.3.cmml"><mi id="S3.SS1.p3.6.m6.5.5.3.2" xref="S3.SS1.p3.6.m6.5.5.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.SS1.p3.6.m6.5.6.3.2.3.2.2" rspace="0.055em" stretchy="false" xref="S3.SS1.p3.6.m6.5.5.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.6.m6.5.6.3.1" rspace="0.222em" xref="S3.SS1.p3.6.m6.5.6.3.1.cmml">⋅</mo><mi id="S3.SS1.p3.6.m6.5.6.3.3" xref="S3.SS1.p3.6.m6.5.6.3.3.cmml">𝒗</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.5b"><apply id="S3.SS1.p3.6.m6.5.6.cmml" xref="S3.SS1.p3.6.m6.5.6"><eq id="S3.SS1.p3.6.m6.5.6.1.cmml" xref="S3.SS1.p3.6.m6.5.6.1"></eq><apply id="S3.SS1.p3.6.m6.5.6.2.1.cmml" xref="S3.SS1.p3.6.m6.5.6.2.2"><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">SA</ci><ci id="S3.SS1.p3.6.m6.2.2.cmml" xref="S3.SS1.p3.6.m6.2.2">𝒒</ci><ci id="S3.SS1.p3.6.m6.3.3.cmml" xref="S3.SS1.p3.6.m6.3.3">𝒌</ci><ci id="S3.SS1.p3.6.m6.4.4.cmml" xref="S3.SS1.p3.6.m6.4.4">𝒗</ci></apply><apply id="S3.SS1.p3.6.m6.5.6.3.cmml" xref="S3.SS1.p3.6.m6.5.6.3"><ci id="S3.SS1.p3.6.m6.5.6.3.1.cmml" xref="S3.SS1.p3.6.m6.5.6.3.1">⋅</ci><apply id="S3.SS1.p3.6.m6.5.6.3.2.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2"><times id="S3.SS1.p3.6.m6.5.6.3.2.1.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2.1"></times><ci id="S3.SS1.p3.6.m6.5.6.3.2.2a.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2.2"><mtext id="S3.SS1.p3.6.m6.5.6.3.2.2.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2.2">softmax</mtext></ci><apply id="S3.SS1.p3.6.m6.5.5.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2.3.2"><divide id="S3.SS1.p3.6.m6.5.5.1.cmml" xref="S3.SS1.p3.6.m6.5.6.3.2.3.2"></divide><apply id="S3.SS1.p3.6.m6.5.5.2.cmml" xref="S3.SS1.p3.6.m6.5.5.2"><ci id="S3.SS1.p3.6.m6.5.5.2.1.cmml" xref="S3.SS1.p3.6.m6.5.5.2.1">⋅</ci><ci id="S3.SS1.p3.6.m6.5.5.2.2.cmml" xref="S3.SS1.p3.6.m6.5.5.2.2">𝒒</ci><apply id="S3.SS1.p3.6.m6.5.5.2.3.cmml" xref="S3.SS1.p3.6.m6.5.5.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.5.5.2.3.1.cmml" xref="S3.SS1.p3.6.m6.5.5.2.3">superscript</csymbol><ci id="S3.SS1.p3.6.m6.5.5.2.3.2.cmml" xref="S3.SS1.p3.6.m6.5.5.2.3.2">𝒌</ci><ci id="S3.SS1.p3.6.m6.5.5.2.3.3.cmml" xref="S3.SS1.p3.6.m6.5.5.2.3.3">𝖳</ci></apply></apply><apply id="S3.SS1.p3.6.m6.5.5.3.cmml" xref="S3.SS1.p3.6.m6.5.5.3"><root id="S3.SS1.p3.6.m6.5.5.3a.cmml" xref="S3.SS1.p3.6.m6.5.5.3"></root><ci id="S3.SS1.p3.6.m6.5.5.3.2.cmml" xref="S3.SS1.p3.6.m6.5.5.3.2">𝑑</ci></apply></apply></apply><ci id="S3.SS1.p3.6.m6.5.6.3.3.cmml" xref="S3.SS1.p3.6.m6.5.6.3.3">𝒗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.5c">\operatorname{SA}(\boldsymbol{q},\boldsymbol{k},\boldsymbol{v})=\text{softmax}%
(\frac{\boldsymbol{q}\cdot\boldsymbol{k}^{\mathsf{T}}}{\sqrt{d}})\cdot%
\boldsymbol{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m6.5d">roman_SA ( bold_italic_q , bold_italic_k , bold_italic_v ) = softmax ( divide start_ARG bold_italic_q ⋅ bold_italic_k start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) ⋅ bold_italic_v</annotation></semantics></math>. Then, a projection layer maps <math alttext="\boldsymbol{z}" class="ltx_Math" display="inline" id="S3.SS1.p3.7.m7.1"><semantics id="S3.SS1.p3.7.m7.1a"><mi id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><ci id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">\boldsymbol{z}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.7.m7.1d">bold_italic_z</annotation></semantics></math> to a multi-modal embedding space:</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{O}=\operatorname{Proj}(\boldsymbol{z})," class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1.3"><semantics id="S3.E2X.2.1.1.m1.3a"><mrow id="S3.E2X.2.1.1.m1.3.3.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.3.3.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.3.3.1.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.2.cmml">𝒪</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.3.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml"><mi id="S3.E2X.2.1.1.m1.1.1" xref="S3.E2X.2.1.1.m1.1.1.cmml">Proj</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.3.2a" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml">⁡</mo><mrow id="S3.E2X.2.1.1.m1.3.3.1.1.3.2.1" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml"><mo id="S3.E2X.2.1.1.m1.3.3.1.1.3.2.1.1" stretchy="false" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml">(</mo><mi id="S3.E2X.2.1.1.m1.2.2" xref="S3.E2X.2.1.1.m1.2.2.cmml">𝒛</mi><mo id="S3.E2X.2.1.1.m1.3.3.1.1.3.2.1.2" stretchy="false" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.3.3.1.2" xref="S3.E2X.2.1.1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.3b"><apply id="S3.E2X.2.1.1.m1.3.3.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1"><eq id="S3.E2X.2.1.1.m1.3.3.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.1"></eq><ci id="S3.E2X.2.1.1.m1.3.3.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.2">𝒪</ci><apply id="S3.E2X.2.1.1.m1.3.3.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.3.3.1.1.3.2"><ci id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">Proj</ci><ci id="S3.E2X.2.1.1.m1.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2">𝒛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.3c">\displaystyle\mathcal{O}=\operatorname{Proj}(\boldsymbol{z}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.2.1.1.m1.3d">caligraphic_O = roman_Proj ( bold_italic_z ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.5">where <math alttext="\mathcal{O}=\left[o_{cls},o_{1},...,o_{h\times w}\right]^{\mathsf{T}}\in%
\mathbb{R}^{(hw+1,c)}" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.6"><semantics id="S3.SS1.p5.1.m1.6a"><mrow id="S3.SS1.p5.1.m1.6.6" xref="S3.SS1.p5.1.m1.6.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p5.1.m1.6.6.5" xref="S3.SS1.p5.1.m1.6.6.5.cmml">𝒪</mi><mo id="S3.SS1.p5.1.m1.6.6.6" xref="S3.SS1.p5.1.m1.6.6.6.cmml">=</mo><msup id="S3.SS1.p5.1.m1.6.6.3" xref="S3.SS1.p5.1.m1.6.6.3.cmml"><mrow id="S3.SS1.p5.1.m1.6.6.3.3.3" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml"><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.4" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml">[</mo><msub id="S3.SS1.p5.1.m1.4.4.1.1.1.1" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.4.4.1.1.1.1.2" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.2.cmml">o</mi><mrow id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.cmml"><mi id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.2" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.3" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.3.cmml">l</mi><mo id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1a" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.4" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.5" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml">,</mo><msub id="S3.SS1.p5.1.m1.5.5.2.2.2.2" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2.cmml"><mi id="S3.SS1.p5.1.m1.5.5.2.2.2.2.2" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2.2.cmml">o</mi><mn id="S3.SS1.p5.1.m1.5.5.2.2.2.2.3" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.6" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml">,</mo><mi id="S3.SS1.p5.1.m1.3.3" mathvariant="normal" xref="S3.SS1.p5.1.m1.3.3.cmml">…</mi><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.7" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml">,</mo><msub id="S3.SS1.p5.1.m1.6.6.3.3.3.3" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.cmml"><mi id="S3.SS1.p5.1.m1.6.6.3.3.3.3.2" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.2.cmml">o</mi><mrow id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.cmml"><mi id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.2" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.2.cmml">h</mi><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.1.cmml">×</mo><mi id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.3" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.3.cmml">w</mi></mrow></msub><mo id="S3.SS1.p5.1.m1.6.6.3.3.3.8" xref="S3.SS1.p5.1.m1.6.6.3.3.4.cmml">]</mo></mrow><mi id="S3.SS1.p5.1.m1.6.6.3.5" xref="S3.SS1.p5.1.m1.6.6.3.5.cmml">𝖳</mi></msup><mo id="S3.SS1.p5.1.m1.6.6.7" xref="S3.SS1.p5.1.m1.6.6.7.cmml">∈</mo><msup id="S3.SS1.p5.1.m1.6.6.8" xref="S3.SS1.p5.1.m1.6.6.8.cmml"><mi id="S3.SS1.p5.1.m1.6.6.8.2" xref="S3.SS1.p5.1.m1.6.6.8.2.cmml">ℝ</mi><mrow id="S3.SS1.p5.1.m1.2.2.2.2" xref="S3.SS1.p5.1.m1.2.2.2.3.cmml"><mo id="S3.SS1.p5.1.m1.2.2.2.2.2" stretchy="false" xref="S3.SS1.p5.1.m1.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.p5.1.m1.2.2.2.2.1" xref="S3.SS1.p5.1.m1.2.2.2.2.1.cmml"><mrow id="S3.SS1.p5.1.m1.2.2.2.2.1.2" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.cmml"><mi id="S3.SS1.p5.1.m1.2.2.2.2.1.2.2" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.2.cmml">h</mi><mo id="S3.SS1.p5.1.m1.2.2.2.2.1.2.1" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.1.cmml">⁢</mo><mi id="S3.SS1.p5.1.m1.2.2.2.2.1.2.3" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.3.cmml">w</mi></mrow><mo id="S3.SS1.p5.1.m1.2.2.2.2.1.1" xref="S3.SS1.p5.1.m1.2.2.2.2.1.1.cmml">+</mo><mn id="S3.SS1.p5.1.m1.2.2.2.2.1.3" xref="S3.SS1.p5.1.m1.2.2.2.2.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p5.1.m1.2.2.2.2.3" xref="S3.SS1.p5.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p5.1.m1.1.1.1.1" xref="S3.SS1.p5.1.m1.1.1.1.1.cmml">c</mi><mo id="S3.SS1.p5.1.m1.2.2.2.2.4" stretchy="false" xref="S3.SS1.p5.1.m1.2.2.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.6b"><apply id="S3.SS1.p5.1.m1.6.6.cmml" xref="S3.SS1.p5.1.m1.6.6"><and id="S3.SS1.p5.1.m1.6.6a.cmml" xref="S3.SS1.p5.1.m1.6.6"></and><apply id="S3.SS1.p5.1.m1.6.6b.cmml" xref="S3.SS1.p5.1.m1.6.6"><eq id="S3.SS1.p5.1.m1.6.6.6.cmml" xref="S3.SS1.p5.1.m1.6.6.6"></eq><ci id="S3.SS1.p5.1.m1.6.6.5.cmml" xref="S3.SS1.p5.1.m1.6.6.5">𝒪</ci><apply id="S3.SS1.p5.1.m1.6.6.3.cmml" xref="S3.SS1.p5.1.m1.6.6.3"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.6.6.3.4.cmml" xref="S3.SS1.p5.1.m1.6.6.3">superscript</csymbol><list id="S3.SS1.p5.1.m1.6.6.3.3.4.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3"><apply id="S3.SS1.p5.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.2">𝑜</ci><apply id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3"><times id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.1"></times><ci id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.2">𝑐</ci><ci id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.3">𝑙</ci><ci id="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.4.cmml" xref="S3.SS1.p5.1.m1.4.4.1.1.1.1.3.4">𝑠</ci></apply></apply><apply id="S3.SS1.p5.1.m1.5.5.2.2.2.2.cmml" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.5.5.2.2.2.2.1.cmml" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p5.1.m1.5.5.2.2.2.2.2.cmml" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2.2">𝑜</ci><cn id="S3.SS1.p5.1.m1.5.5.2.2.2.2.3.cmml" type="integer" xref="S3.SS1.p5.1.m1.5.5.2.2.2.2.3">1</cn></apply><ci id="S3.SS1.p5.1.m1.3.3.cmml" xref="S3.SS1.p5.1.m1.3.3">…</ci><apply id="S3.SS1.p5.1.m1.6.6.3.3.3.3.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.6.6.3.3.3.3.1.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p5.1.m1.6.6.3.3.3.3.2.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.2">𝑜</ci><apply id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3"><times id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.1.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.1"></times><ci id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.2.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.2">ℎ</ci><ci id="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.3.cmml" xref="S3.SS1.p5.1.m1.6.6.3.3.3.3.3.3">𝑤</ci></apply></apply></list><ci id="S3.SS1.p5.1.m1.6.6.3.5.cmml" xref="S3.SS1.p5.1.m1.6.6.3.5">𝖳</ci></apply></apply><apply id="S3.SS1.p5.1.m1.6.6c.cmml" xref="S3.SS1.p5.1.m1.6.6"><in id="S3.SS1.p5.1.m1.6.6.7.cmml" xref="S3.SS1.p5.1.m1.6.6.7"></in><share href="https://arxiv.org/html/2410.01768v1#S3.SS1.p5.1.m1.6.6.3.cmml" id="S3.SS1.p5.1.m1.6.6d.cmml" xref="S3.SS1.p5.1.m1.6.6"></share><apply id="S3.SS1.p5.1.m1.6.6.8.cmml" xref="S3.SS1.p5.1.m1.6.6.8"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.6.6.8.1.cmml" xref="S3.SS1.p5.1.m1.6.6.8">superscript</csymbol><ci id="S3.SS1.p5.1.m1.6.6.8.2.cmml" xref="S3.SS1.p5.1.m1.6.6.8.2">ℝ</ci><interval closure="open" id="S3.SS1.p5.1.m1.2.2.2.3.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2"><apply id="S3.SS1.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1"><plus id="S3.SS1.p5.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1.1"></plus><apply id="S3.SS1.p5.1.m1.2.2.2.2.1.2.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2"><times id="S3.SS1.p5.1.m1.2.2.2.2.1.2.1.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.1"></times><ci id="S3.SS1.p5.1.m1.2.2.2.2.1.2.2.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.2">ℎ</ci><ci id="S3.SS1.p5.1.m1.2.2.2.2.1.2.3.cmml" xref="S3.SS1.p5.1.m1.2.2.2.2.1.2.3">𝑤</ci></apply><cn id="S3.SS1.p5.1.m1.2.2.2.2.1.3.cmml" type="integer" xref="S3.SS1.p5.1.m1.2.2.2.2.1.3">1</cn></apply><ci id="S3.SS1.p5.1.m1.1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1.1">𝑐</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.6c">\mathcal{O}=\left[o_{cls},o_{1},...,o_{h\times w}\right]^{\mathsf{T}}\in%
\mathbb{R}^{(hw+1,c)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.6d">caligraphic_O = [ italic_o start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_o start_POSTSUBSCRIPT italic_h × italic_w end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT ( italic_h italic_w + 1 , italic_c ) end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the output of the image encoder, <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">italic_c</annotation></semantics></math> denotes token dimension after the projection layer, and <math alttext="c&lt;d" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mrow id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">c</mi><mo id="S3.SS1.p5.3.m3.1.1.1" xref="S3.SS1.p5.3.m3.1.1.1.cmml">&lt;</mo><mi id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><lt id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1.1"></lt><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">𝑐</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">c&lt;d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_c &lt; italic_d</annotation></semantics></math>. During CLIP training, <math alttext="o_{cls}" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4.1"><semantics id="S3.SS1.p5.4.m4.1a"><msub id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">o</mi><mrow id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml"><mi id="S3.SS1.p5.4.m4.1.1.3.2" xref="S3.SS1.p5.4.m4.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p5.4.m4.1.1.3.1" xref="S3.SS1.p5.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.4.m4.1.1.3.3" xref="S3.SS1.p5.4.m4.1.1.3.3.cmml">l</mi><mo id="S3.SS1.p5.4.m4.1.1.3.1a" xref="S3.SS1.p5.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.4.m4.1.1.3.4" xref="S3.SS1.p5.4.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">𝑜</ci><apply id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3"><times id="S3.SS1.p5.4.m4.1.1.3.1.cmml" xref="S3.SS1.p5.4.m4.1.1.3.1"></times><ci id="S3.SS1.p5.4.m4.1.1.3.2.cmml" xref="S3.SS1.p5.4.m4.1.1.3.2">𝑐</ci><ci id="S3.SS1.p5.4.m4.1.1.3.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3.3">𝑙</ci><ci id="S3.SS1.p5.4.m4.1.1.3.4.cmml" xref="S3.SS1.p5.4.m4.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">o_{cls}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.4.m4.1d">italic_o start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is used for image-level learning; while during OVSS inference, <math alttext="\mathcal{O}[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S3.SS1.p5.5.m5.1"><semantics id="S3.SS1.p5.5.m5.1a"><mrow id="S3.SS1.p5.5.m5.1b"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p5.5.m5.1.1">𝒪</mi><mrow id="S3.SS1.p5.5.m5.1.2"><mo id="S3.SS1.p5.5.m5.1.2.1" stretchy="false">[</mo><mn id="S3.SS1.p5.5.m5.1.2.2">1</mn><mo id="S3.SS1.p5.5.m5.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S3.SS1.p5.5.m5.1.2.4">h</mi><mi id="S3.SS1.p5.5.m5.1.2.5">w</mi><mo id="S3.SS1.p5.5.m5.1.2.6">+</mo><mn id="S3.SS1.p5.5.m5.1.2.7">1</mn><mo id="S3.SS1.p5.5.m5.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">\mathcal{O}[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.5.m5.1d">caligraphic_O [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math> is used for patch-level prediction.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>FeatUp</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">FeatUp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> aims to train a model-agnostic upsampler. It executes an upsampling operation on LR features <math alttext="\mathcal{O}[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1">𝒪</mi><mrow id="S3.SS2.p1.1.m1.1.2"><mo id="S3.SS2.p1.1.m1.1.2.1" stretchy="false">[</mo><mn id="S3.SS2.p1.1.m1.1.2.2">1</mn><mo id="S3.SS2.p1.1.m1.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S3.SS2.p1.1.m1.1.2.4">h</mi><mi id="S3.SS2.p1.1.m1.1.2.5">w</mi><mo id="S3.SS2.p1.1.m1.1.2.6">+</mo><mn id="S3.SS2.p1.1.m1.1.2.7">1</mn><mo id="S3.SS2.p1.1.m1.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{O}[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">caligraphic_O [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math> from a frozen backbone network via a learnable upsampler <math alttext="\sigma_{\uparrow}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">σ</mi><mo id="S3.SS2.p1.2.m2.1.1.3" stretchy="false" xref="S3.SS2.p1.2.m2.1.1.3.cmml">↑</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝜎</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">↑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\sigma_{\uparrow}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT</annotation></semantics></math>, and then reconstructs the LR features using a learnable downsampler <math alttext="\sigma_{\downarrow}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">σ</mi><mo id="S3.SS2.p1.3.m3.1.1.3" stretchy="false" xref="S3.SS2.p1.3.m3.1.1.3.cmml">↓</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝜎</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">↓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\sigma_{\downarrow}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_σ start_POSTSUBSCRIPT ↓ end_POSTSUBSCRIPT</annotation></semantics></math>. Its critical insights can be briefly summarized by the following loss function:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{rec}=\left\|\mathcal{O}[1:hw+1]-\sigma_{\downarrow}(%
\sigma_{\uparrow}(\mathcal{O}[1:hw+1]))\right\|_{2}^{2}." class="ltx_math_unparsed" display="inline" id="S3.E3X.2.1.1.m1.1"><semantics id="S3.E3X.2.1.1.m1.1a"><mrow id="S3.E3X.2.1.1.m1.1b"><msub id="S3.E3X.2.1.1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.1.1.2">ℒ</mi><mrow id="S3.E3X.2.1.1.m1.1.1.3"><mi id="S3.E3X.2.1.1.m1.1.1.3.2">r</mi><mo id="S3.E3X.2.1.1.m1.1.1.3.1">⁢</mo><mi id="S3.E3X.2.1.1.m1.1.1.3.3">e</mi><mo id="S3.E3X.2.1.1.m1.1.1.3.1a">⁢</mo><mi id="S3.E3X.2.1.1.m1.1.1.3.4">c</mi></mrow></msub><mo id="S3.E3X.2.1.1.m1.1.2" rspace="0em">=</mo><mo id="S3.E3X.2.1.1.m1.1.3" lspace="0em" rspace="0.167em" stretchy="true">∥</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.1.4">𝒪</mi><mrow id="S3.E3X.2.1.1.m1.1.5"><mo id="S3.E3X.2.1.1.m1.1.5.1" stretchy="false">[</mo><mn id="S3.E3X.2.1.1.m1.1.5.2">1</mn><mo id="S3.E3X.2.1.1.m1.1.5.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S3.E3X.2.1.1.m1.1.5.4">h</mi><mi id="S3.E3X.2.1.1.m1.1.5.5">w</mi><mo id="S3.E3X.2.1.1.m1.1.5.6">+</mo><mn id="S3.E3X.2.1.1.m1.1.5.7">1</mn><mo id="S3.E3X.2.1.1.m1.1.5.8" stretchy="false">]</mo></mrow><mo id="S3.E3X.2.1.1.m1.1.6">−</mo><msub id="S3.E3X.2.1.1.m1.1.7"><mi id="S3.E3X.2.1.1.m1.1.7.2">σ</mi><mo id="S3.E3X.2.1.1.m1.1.7.3" stretchy="false">↓</mo></msub><mrow id="S3.E3X.2.1.1.m1.1.8"><mo id="S3.E3X.2.1.1.m1.1.8.1" stretchy="false">(</mo><msub id="S3.E3X.2.1.1.m1.1.8.2"><mi id="S3.E3X.2.1.1.m1.1.8.2.2">σ</mi><mo id="S3.E3X.2.1.1.m1.1.8.2.3" stretchy="false">↑</mo></msub><mrow id="S3.E3X.2.1.1.m1.1.8.3"><mo id="S3.E3X.2.1.1.m1.1.8.3.1" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.1.8.3.2">𝒪</mi><mrow id="S3.E3X.2.1.1.m1.1.8.3.3"><mo id="S3.E3X.2.1.1.m1.1.8.3.3.1" stretchy="false">[</mo><mn id="S3.E3X.2.1.1.m1.1.8.3.3.2">1</mn><mo id="S3.E3X.2.1.1.m1.1.8.3.3.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S3.E3X.2.1.1.m1.1.8.3.3.4">h</mi><mi id="S3.E3X.2.1.1.m1.1.8.3.3.5">w</mi><mo id="S3.E3X.2.1.1.m1.1.8.3.3.6">+</mo><mn id="S3.E3X.2.1.1.m1.1.8.3.3.7">1</mn><mo id="S3.E3X.2.1.1.m1.1.8.3.3.8" stretchy="false">]</mo></mrow><mo id="S3.E3X.2.1.1.m1.1.8.3.4" stretchy="false">)</mo></mrow><mo id="S3.E3X.2.1.1.m1.1.8.4" stretchy="false">)</mo></mrow><msubsup id="S3.E3X.2.1.1.m1.1.9"><mo id="S3.E3X.2.1.1.m1.1.9.2.2" lspace="0em" rspace="0.0835em" stretchy="true">∥</mo><mn id="S3.E3X.2.1.1.m1.1.9.2.3">2</mn><mn id="S3.E3X.2.1.1.m1.1.9.3">2</mn></msubsup><mo id="S3.E3X.2.1.1.m1.1.10" lspace="0.0835em">.</mo></mrow><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{rec}=\left\|\mathcal{O}[1:hw+1]-\sigma_{\downarrow}(%
\sigma_{\uparrow}(\mathcal{O}[1:hw+1]))\right\|_{2}^{2}.</annotation><annotation encoding="application/x-llamapun" id="S3.E3X.2.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT = ∥ caligraphic_O [ 1 : italic_h italic_w + 1 ] - italic_σ start_POSTSUBSCRIPT ↓ end_POSTSUBSCRIPT ( italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT ( caligraphic_O [ 1 : italic_h italic_w + 1 ] ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3">FeatUp instantiates <math alttext="\sigma_{\uparrow}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">σ</mi><mo id="S3.SS2.p3.1.m1.1.1.3" stretchy="false" xref="S3.SS2.p3.1.m1.1.1.3.cmml">↑</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝜎</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">↑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\sigma_{\uparrow}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT</annotation></semantics></math> as stacked parameterized JBU operators. The upsampled HR feature element is estimated by weighting the neighboring elements of the LR feature. For weight generation, JBU considers two factors, the similarity and distance between neighboring elements and the center element in the guidance feature, corresponding to kernel <math alttext="k_{range}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">k</mi><mrow id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.3.2.cmml">r</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.3.3.cmml">a</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1a" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.4" xref="S3.SS2.p3.2.m2.1.1.3.4.cmml">n</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1b" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.5" xref="S3.SS2.p3.2.m2.1.1.3.5.cmml">g</mi><mo id="S3.SS2.p3.2.m2.1.1.3.1c" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.2.m2.1.1.3.6" xref="S3.SS2.p3.2.m2.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝑘</ci><apply id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><times id="S3.SS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.3.1"></times><ci id="S3.SS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.3.2">𝑟</ci><ci id="S3.SS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS2.p3.2.m2.1.1.3.4.cmml" xref="S3.SS2.p3.2.m2.1.1.3.4">𝑛</ci><ci id="S3.SS2.p3.2.m2.1.1.3.5.cmml" xref="S3.SS2.p3.2.m2.1.1.3.5">𝑔</ci><ci id="S3.SS2.p3.2.m2.1.1.3.6.cmml" xref="S3.SS2.p3.2.m2.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">k_{range}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_k start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="k_{spatial}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">k</mi><mrow id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.3.2.cmml">s</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.cmml">p</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1a" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.4" xref="S3.SS2.p3.3.m3.1.1.3.4.cmml">a</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1b" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.5" xref="S3.SS2.p3.3.m3.1.1.3.5.cmml">t</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1c" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.6" xref="S3.SS2.p3.3.m3.1.1.3.6.cmml">i</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1d" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.7" xref="S3.SS2.p3.3.m3.1.1.3.7.cmml">a</mi><mo id="S3.SS2.p3.3.m3.1.1.3.1e" xref="S3.SS2.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.3.m3.1.1.3.8" xref="S3.SS2.p3.3.m3.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝑘</ci><apply id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><times id="S3.SS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3.1"></times><ci id="S3.SS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.2">𝑠</ci><ci id="S3.SS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3">𝑝</ci><ci id="S3.SS2.p3.3.m3.1.1.3.4.cmml" xref="S3.SS2.p3.3.m3.1.1.3.4">𝑎</ci><ci id="S3.SS2.p3.3.m3.1.1.3.5.cmml" xref="S3.SS2.p3.3.m3.1.1.3.5">𝑡</ci><ci id="S3.SS2.p3.3.m3.1.1.3.6.cmml" xref="S3.SS2.p3.3.m3.1.1.3.6">𝑖</ci><ci id="S3.SS2.p3.3.m3.1.1.3.7.cmml" xref="S3.SS2.p3.3.m3.1.1.3.7">𝑎</ci><ci id="S3.SS2.p3.3.m3.1.1.3.8.cmml" xref="S3.SS2.p3.3.m3.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">k_{spatial}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_k start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>. For brevity, we omit the multi-view consistency constraint in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In the following, we introduce SegEarth-OV by first describing SimFeatUp’s training, design and explaining why it is suitable for OVSS. Second, we discuss the impact of global token on dense prediction and present our method for alleviating global bias.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>SimFeatUp</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="680" id="S4.F4.g1" src="x5.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.6.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.7.2" style="font-size:90%;">Comparison of with and without image reconstruction loss (<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.E4" title="In 4.1 SimFeatUp ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). the LR prediction is obtained directly using the output of CLIP (without bilinear interpolation). Color: <span class="ltx_text" id="S4.F4.7.2.1" style="color:#DE1F07;">building</span>, <span class="ltx_text" id="S4.F4.7.2.2" style="color:#226126;">tree</span>, <span class="ltx_text" id="S4.F4.7.2.3" style="color:#4BB549;">cropland</span>, <span class="ltx_text" id="S4.F4.7.2.4" style="color:#00FF24;">grass</span>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">FeatUp provides us with an excellent training paradigm for general upsamplers. However, it lacks some considerations for the traning-free setting, leading it to be sub-optimal for the OVSS task, especially in remote sensing contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.3.1">Image content retention.</span> As described in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.SS2" title="3.2 FeatUp ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the goal of FeatUp is to minimize the original LR features and the LR features after the up-down-sampling (i.e. <math alttext="\sigma_{\downarrow}(\sigma_{\uparrow}(\mathcal{O}[1:hw+1]))" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1b"><msub id="S4.SS1.p2.1.m1.1.1"><mi id="S4.SS1.p2.1.m1.1.1.2">σ</mi><mo id="S4.SS1.p2.1.m1.1.1.3" stretchy="false">↓</mo></msub><mrow id="S4.SS1.p2.1.m1.1.2"><mo id="S4.SS1.p2.1.m1.1.2.1" stretchy="false">(</mo><msub id="S4.SS1.p2.1.m1.1.2.2"><mi id="S4.SS1.p2.1.m1.1.2.2.2">σ</mi><mo id="S4.SS1.p2.1.m1.1.2.2.3" stretchy="false">↑</mo></msub><mrow id="S4.SS1.p2.1.m1.1.2.3"><mo id="S4.SS1.p2.1.m1.1.2.3.1" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.1.2.3.2">𝒪</mi><mrow id="S4.SS1.p2.1.m1.1.2.3.3"><mo id="S4.SS1.p2.1.m1.1.2.3.3.1" stretchy="false">[</mo><mn id="S4.SS1.p2.1.m1.1.2.3.3.2">1</mn><mo id="S4.SS1.p2.1.m1.1.2.3.3.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.SS1.p2.1.m1.1.2.3.3.4">h</mi><mi id="S4.SS1.p2.1.m1.1.2.3.3.5">w</mi><mo id="S4.SS1.p2.1.m1.1.2.3.3.6">+</mo><mn id="S4.SS1.p2.1.m1.1.2.3.3.7">1</mn><mo id="S4.SS1.p2.1.m1.1.2.3.3.8" stretchy="false">]</mo></mrow><mo id="S4.SS1.p2.1.m1.1.2.3.4" stretchy="false">)</mo></mrow><mo id="S4.SS1.p2.1.m1.1.2.4" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\sigma_{\downarrow}(\sigma_{\uparrow}(\mathcal{O}[1:hw+1]))</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_σ start_POSTSUBSCRIPT ↓ end_POSTSUBSCRIPT ( italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT ( caligraphic_O [ 1 : italic_h italic_w + 1 ] ) )</annotation></semantics></math>). Since both <math alttext="\sigma_{\uparrow}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><msub id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">σ</mi><mo id="S4.SS1.p2.2.m2.1.1.3" stretchy="false" xref="S4.SS1.p2.2.m2.1.1.3.cmml">↑</mo></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝜎</ci><ci id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">↑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\sigma_{\uparrow}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\sigma_{\downarrow}" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">σ</mi><mo id="S4.SS1.p2.3.m3.1.1.3" stretchy="false" xref="S4.SS1.p2.3.m3.1.1.3.cmml">↓</mo></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝜎</ci><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">↓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\sigma_{\downarrow}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_σ start_POSTSUBSCRIPT ↓ end_POSTSUBSCRIPT</annotation></semantics></math> are learnable, with such a weak constraint, the up-down-sampling process becomes a black box, and there is no guarantee that the intermediate HR features are complete and consistent with the original image in content. A direct example is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.F4" title="In 4.1 SimFeatUp ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, where a small building in the original image is present in the LR prediction but disappears in the HR prediction (top right). To solve this issue, we introduce an additional image reconstruction loss to constrain the HR features:</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{img}=\left\|I-\operatorname{CRN}(\sigma_{\uparrow}(%
\mathcal{O}[1:hw+1])))\right\|_{2}^{2}," class="ltx_math_unparsed" display="inline" id="S4.E4X.2.1.1.m1.1"><semantics id="S4.E4X.2.1.1.m1.1a"><mrow id="S4.E4X.2.1.1.m1.1b"><msub id="S4.E4X.2.1.1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E4X.2.1.1.m1.1.1.2">ℒ</mi><mrow id="S4.E4X.2.1.1.m1.1.1.3"><mi id="S4.E4X.2.1.1.m1.1.1.3.2">i</mi><mo id="S4.E4X.2.1.1.m1.1.1.3.1">⁢</mo><mi id="S4.E4X.2.1.1.m1.1.1.3.3">m</mi><mo id="S4.E4X.2.1.1.m1.1.1.3.1a">⁢</mo><mi id="S4.E4X.2.1.1.m1.1.1.3.4">g</mi></mrow></msub><mo id="S4.E4X.2.1.1.m1.1.2" rspace="0em">=</mo><mo id="S4.E4X.2.1.1.m1.1.3" lspace="0em" rspace="0.167em" stretchy="true">∥</mo><mi id="S4.E4X.2.1.1.m1.1.4">I</mi><mo id="S4.E4X.2.1.1.m1.1.5">−</mo><mi id="S4.E4X.2.1.1.m1.1.6">CRN</mi><mrow id="S4.E4X.2.1.1.m1.1.7"><mo id="S4.E4X.2.1.1.m1.1.7.1" stretchy="false">(</mo><msub id="S4.E4X.2.1.1.m1.1.7.2"><mi id="S4.E4X.2.1.1.m1.1.7.2.2">σ</mi><mo id="S4.E4X.2.1.1.m1.1.7.2.3" stretchy="false">↑</mo></msub><mrow id="S4.E4X.2.1.1.m1.1.7.3"><mo id="S4.E4X.2.1.1.m1.1.7.3.1" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.E4X.2.1.1.m1.1.7.3.2">𝒪</mi><mrow id="S4.E4X.2.1.1.m1.1.7.3.3"><mo id="S4.E4X.2.1.1.m1.1.7.3.3.1" stretchy="false">[</mo><mn id="S4.E4X.2.1.1.m1.1.7.3.3.2">1</mn><mo id="S4.E4X.2.1.1.m1.1.7.3.3.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.E4X.2.1.1.m1.1.7.3.3.4">h</mi><mi id="S4.E4X.2.1.1.m1.1.7.3.3.5">w</mi><mo id="S4.E4X.2.1.1.m1.1.7.3.3.6">+</mo><mn id="S4.E4X.2.1.1.m1.1.7.3.3.7">1</mn><mo id="S4.E4X.2.1.1.m1.1.7.3.3.8" stretchy="false">]</mo></mrow><mo id="S4.E4X.2.1.1.m1.1.7.3.4" stretchy="false">)</mo></mrow><mo id="S4.E4X.2.1.1.m1.1.7.4" stretchy="false">)</mo></mrow><mo id="S4.E4X.2.1.1.m1.1.8" stretchy="false">)</mo><mo id="S4.E4X.2.1.1.m1.1.9" lspace="0em" rspace="0.167em" stretchy="true">∥</mo><msub id="S4.E4X.2.1.1.m1.1.10"><mi id="S4.E4X.2.1.1.m1.1.10a"></mi><mn id="S4.E4X.2.1.1.m1.1.10.1">2</mn></msub><msup id="S4.E4X.2.1.1.m1.1.11"><mi id="S4.E4X.2.1.1.m1.1.11a"></mi><mn id="S4.E4X.2.1.1.m1.1.11.1">2</mn></msup><mo id="S4.E4X.2.1.1.m1.1.12">,</mo></mrow><annotation encoding="application/x-tex" id="S4.E4X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{img}=\left\|I-\operatorname{CRN}(\sigma_{\uparrow}(%
\mathcal{O}[1:hw+1])))\right\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.2.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT = ∥ italic_I - roman_CRN ( italic_σ start_POSTSUBSCRIPT ↑ end_POSTSUBSCRIPT ( caligraphic_O [ 1 : italic_h italic_w + 1 ] ) ) ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.7">where <math alttext="I" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">italic_I</annotation></semantics></math> denotes the input image, <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S4.SS1.p4.2.m2.1"><semantics id="S4.SS1.p4.2.m2.1a"><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.2.m2.1d">roman_CRN</annotation></semantics></math> denotes a content retention net. <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S4.SS1.p4.3.m3.1"><semantics id="S4.SS1.p4.3.m3.1a"><mi id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><ci id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.3.m3.1d">roman_CRN</annotation></semantics></math> is a very lightweight network that receives HR features as input and reconstructs the original image. Specifically, <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S4.SS1.p4.4.m4.1"><semantics id="S4.SS1.p4.4.m4.1a"><mi id="S4.SS1.p4.4.m4.1.1" xref="S4.SS1.p4.4.m4.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><ci id="S4.SS1.p4.4.m4.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.4.m4.1d">roman_CRN</annotation></semantics></math> consists of two 2D convolutional layers with activation and a <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.7.1">Tanh</span> activation layer, where the <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.7.2">Tanh</span> layer is designed to constrain the output to [-1, 1], cf. VAEs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. Finally, the loss for training SimFeatUp consists of <math alttext="\mathcal{L}_{rec}" class="ltx_Math" display="inline" id="S4.SS1.p4.5.m5.1"><semantics id="S4.SS1.p4.5.m5.1a"><msub id="S4.SS1.p4.5.m5.1.1" xref="S4.SS1.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.5.m5.1.1.2" xref="S4.SS1.p4.5.m5.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p4.5.m5.1.1.3" xref="S4.SS1.p4.5.m5.1.1.3.cmml"><mi id="S4.SS1.p4.5.m5.1.1.3.2" xref="S4.SS1.p4.5.m5.1.1.3.2.cmml">r</mi><mo id="S4.SS1.p4.5.m5.1.1.3.1" xref="S4.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.5.m5.1.1.3.3" xref="S4.SS1.p4.5.m5.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p4.5.m5.1.1.3.1a" xref="S4.SS1.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.5.m5.1.1.3.4" xref="S4.SS1.p4.5.m5.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m5.1b"><apply id="S4.SS1.p4.5.m5.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.5.m5.1.1.1.cmml" xref="S4.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p4.5.m5.1.1.2.cmml" xref="S4.SS1.p4.5.m5.1.1.2">ℒ</ci><apply id="S4.SS1.p4.5.m5.1.1.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3"><times id="S4.SS1.p4.5.m5.1.1.3.1.cmml" xref="S4.SS1.p4.5.m5.1.1.3.1"></times><ci id="S4.SS1.p4.5.m5.1.1.3.2.cmml" xref="S4.SS1.p4.5.m5.1.1.3.2">𝑟</ci><ci id="S4.SS1.p4.5.m5.1.1.3.3.cmml" xref="S4.SS1.p4.5.m5.1.1.3.3">𝑒</ci><ci id="S4.SS1.p4.5.m5.1.1.3.4.cmml" xref="S4.SS1.p4.5.m5.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m5.1c">\mathcal{L}_{rec}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{img}" class="ltx_Math" display="inline" id="S4.SS1.p4.6.m6.1"><semantics id="S4.SS1.p4.6.m6.1a"><msub id="S4.SS1.p4.6.m6.1.1" xref="S4.SS1.p4.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.6.m6.1.1.2" xref="S4.SS1.p4.6.m6.1.1.2.cmml">ℒ</mi><mrow id="S4.SS1.p4.6.m6.1.1.3" xref="S4.SS1.p4.6.m6.1.1.3.cmml"><mi id="S4.SS1.p4.6.m6.1.1.3.2" xref="S4.SS1.p4.6.m6.1.1.3.2.cmml">i</mi><mo id="S4.SS1.p4.6.m6.1.1.3.1" xref="S4.SS1.p4.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.6.m6.1.1.3.3" xref="S4.SS1.p4.6.m6.1.1.3.3.cmml">m</mi><mo id="S4.SS1.p4.6.m6.1.1.3.1a" xref="S4.SS1.p4.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p4.6.m6.1.1.3.4" xref="S4.SS1.p4.6.m6.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m6.1b"><apply id="S4.SS1.p4.6.m6.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m6.1.1.1.cmml" xref="S4.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p4.6.m6.1.1.2.cmml" xref="S4.SS1.p4.6.m6.1.1.2">ℒ</ci><apply id="S4.SS1.p4.6.m6.1.1.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3"><times id="S4.SS1.p4.6.m6.1.1.3.1.cmml" xref="S4.SS1.p4.6.m6.1.1.3.1"></times><ci id="S4.SS1.p4.6.m6.1.1.3.2.cmml" xref="S4.SS1.p4.6.m6.1.1.3.2">𝑖</ci><ci id="S4.SS1.p4.6.m6.1.1.3.3.cmml" xref="S4.SS1.p4.6.m6.1.1.3.3">𝑚</ci><ci id="S4.SS1.p4.6.m6.1.1.3.4.cmml" xref="S4.SS1.p4.6.m6.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m6.1c">\mathcal{L}_{img}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.6.m6.1d">caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT</annotation></semantics></math> with a weight <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS1.p4.7.m7.1"><semantics id="S4.SS1.p4.7.m7.1a"><mi id="S4.SS1.p4.7.m7.1.1" xref="S4.SS1.p4.7.m7.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.7.m7.1b"><ci id="S4.SS1.p4.7.m7.1.1.cmml" xref="S4.SS1.p4.7.m7.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.7.m7.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.7.m7.1d">italic_γ</annotation></semantics></math>, i.e.,</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}=\mathcal{L}_{rec}+\gamma\mathcal{L}_{img}," class="ltx_Math" display="inline" id="S4.E5X.2.1.1.m1.1"><semantics id="S4.E5X.2.1.1.m1.1a"><mrow id="S4.E5X.2.1.1.m1.1.1.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E5X.2.1.1.m1.1.1.1.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5X.2.1.1.m1.1.1.1.1.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E5X.2.1.1.m1.1.1.1.1.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.cmml"><msub id="S4.E5X.2.1.1.m1.1.1.1.1.3.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.2.cmml">ℒ</mi><mrow id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.2.cmml">r</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.3.cmml">e</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1a" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.4" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.4.cmml">c</mi></mrow></msub><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E5X.2.1.1.m1.1.1.1.1.3.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.cmml"><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.2.cmml">γ</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.3" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.3.cmml">m</mi><mo id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1a" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.4" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.4.cmml">g</mi></mrow></msub></mrow></mrow></mrow><mo id="S4.E5X.2.1.1.m1.1.1.1.2" xref="S4.E5X.2.1.1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5X.2.1.1.m1.1b"><apply id="S4.E5X.2.1.1.m1.1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1"><eq id="S4.E5X.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.1"></eq><ci id="S4.E5X.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.2">ℒ</ci><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3"><plus id="S4.E5X.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.1"></plus><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.2">ℒ</ci><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3"><times id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.1"></times><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.2">𝑟</ci><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.3">𝑒</ci><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.4.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.2.3.4">𝑐</ci></apply></apply><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3"><times id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.1"></times><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.2">𝛾</ci><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.2">ℒ</ci><apply id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3"><times id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.2">𝑖</ci><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.3">𝑚</ci><ci id="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S4.E5X.2.1.1.m1.1.1.1.1.3.3.3.3.4">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.2.1.1.m1.1c">\displaystyle\mathcal{L}=\mathcal{L}_{rec}+\gamma\mathcal{L}_{img},</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.2.1.1.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT + italic_γ caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(5)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.6"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.6.1">Which feature to upsample?</span> FeatUp takes the final output of CLIP, i.e., <math alttext="\mathcal{O}[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S4.SS1.p6.1.m1.1"><semantics id="S4.SS1.p6.1.m1.1a"><mrow id="S4.SS1.p6.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p6.1.m1.1.1">𝒪</mi><mrow id="S4.SS1.p6.1.m1.1.2"><mo id="S4.SS1.p6.1.m1.1.2.1" stretchy="false">[</mo><mn id="S4.SS1.p6.1.m1.1.2.2">1</mn><mo id="S4.SS1.p6.1.m1.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.SS1.p6.1.m1.1.2.4">h</mi><mi id="S4.SS1.p6.1.m1.1.2.5">w</mi><mo id="S4.SS1.p6.1.m1.1.2.6">+</mo><mn id="S4.SS1.p6.1.m1.1.2.7">1</mn><mo id="S4.SS1.p6.1.m1.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">\mathcal{O}[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.1.m1.1d">caligraphic_O [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.E2" title="In 3.1 CLIP ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, as input to the upsampler. This can work well in training-based settings, e.g., linear probe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. However, in training-free OVSS, as described in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S2" title="2 Related Work ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, vanilla self-attention leads to inferior performance. Therefore, the current OVSS method modulates it to self-self attention, and this law also works in remote sensing images. Under this premise, the <math alttext="\operatorname{SA}" class="ltx_Math" display="inline" id="S4.SS1.p6.2.m2.1"><semantics id="S4.SS1.p6.2.m2.1a"><mi id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml">SA</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><ci id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">SA</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">\operatorname{SA}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.2.m2.1d">roman_SA</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.E1" title="In 3.1 CLIP ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a> would be replaced by other modules, and direct upsampling of <math alttext="\mathcal{O}[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S4.SS1.p6.3.m3.1"><semantics id="S4.SS1.p6.3.m3.1a"><mrow id="S4.SS1.p6.3.m3.1b"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p6.3.m3.1.1">𝒪</mi><mrow id="S4.SS1.p6.3.m3.1.2"><mo id="S4.SS1.p6.3.m3.1.2.1" stretchy="false">[</mo><mn id="S4.SS1.p6.3.m3.1.2.2">1</mn><mo id="S4.SS1.p6.3.m3.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.SS1.p6.3.m3.1.2.4">h</mi><mi id="S4.SS1.p6.3.m3.1.2.5">w</mi><mo id="S4.SS1.p6.3.m3.1.2.6">+</mo><mn id="S4.SS1.p6.3.m3.1.2.7">1</mn><mo id="S4.SS1.p6.3.m3.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m3.1c">\mathcal{O}[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.3.m3.1d">caligraphic_O [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math> would lead to the mismatch between training and inference. Motivated by this, we propose to upsample CLIP features at an earlier layer. Specifically, we select the input of the last Transformer block of the CLIP’s image encoder, i.e., <math alttext="X[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S4.SS1.p6.4.m4.1"><semantics id="S4.SS1.p6.4.m4.1a"><mrow id="S4.SS1.p6.4.m4.1b"><mi id="S4.SS1.p6.4.m4.1.1">X</mi><mrow id="S4.SS1.p6.4.m4.1.2"><mo id="S4.SS1.p6.4.m4.1.2.1" stretchy="false">[</mo><mn id="S4.SS1.p6.4.m4.1.2.2">1</mn><mo id="S4.SS1.p6.4.m4.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.SS1.p6.4.m4.1.2.4">h</mi><mi id="S4.SS1.p6.4.m4.1.2.5">w</mi><mo id="S4.SS1.p6.4.m4.1.2.6">+</mo><mn id="S4.SS1.p6.4.m4.1.2.7">1</mn><mo id="S4.SS1.p6.4.m4.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS1.p6.4.m4.1c">X[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.4.m4.1d">italic_X [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.E1" title="In 3.1 CLIP ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Further, the high dimension of tokens in <math alttext="X" class="ltx_Math" display="inline" id="S4.SS1.p6.5.m5.1"><semantics id="S4.SS1.p6.5.m5.1a"><mi id="S4.SS1.p6.5.m5.1.1" xref="S4.SS1.p6.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.5.m5.1b"><ci id="S4.SS1.p6.5.m5.1.1.cmml" xref="S4.SS1.p6.5.m5.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.5.m5.1c">X</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.5.m5.1d">italic_X</annotation></semantics></math> leads to a high cost upsampler. Therefore, we retain the projection layer. Ultimately, the features <math alttext="\mathcal{O}^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.p6.6.m6.1"><semantics id="S4.SS1.p6.6.m6.1a"><msup id="S4.SS1.p6.6.m6.1.1" xref="S4.SS1.p6.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p6.6.m6.1.1.2" xref="S4.SS1.p6.6.m6.1.1.2.cmml">𝒪</mi><mo id="S4.SS1.p6.6.m6.1.1.3" xref="S4.SS1.p6.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.6.m6.1b"><apply id="S4.SS1.p6.6.m6.1.1.cmml" xref="S4.SS1.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p6.6.m6.1.1.1.cmml" xref="S4.SS1.p6.6.m6.1.1">superscript</csymbol><ci id="S4.SS1.p6.6.m6.1.1.2.cmml" xref="S4.SS1.p6.6.m6.1.1.2">𝒪</ci><ci id="S4.SS1.p6.6.m6.1.1.3.cmml" xref="S4.SS1.p6.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.6.m6.1c">\mathcal{O}^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.6.m6.1d">caligraphic_O start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> which need to upsample can be formulated as:</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E6">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E6X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{O}^{\prime}=\operatorname{Proj}(X[1:hw+1])." class="ltx_math_unparsed" display="inline" id="S4.E6X.2.1.1.m1.1"><semantics id="S4.E6X.2.1.1.m1.1a"><mrow id="S4.E6X.2.1.1.m1.1b"><msup id="S4.E6X.2.1.1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E6X.2.1.1.m1.1.1.2">𝒪</mi><mo id="S4.E6X.2.1.1.m1.1.1.3">′</mo></msup><mo id="S4.E6X.2.1.1.m1.1.2">=</mo><mi id="S4.E6X.2.1.1.m1.1.3">Proj</mi><mrow id="S4.E6X.2.1.1.m1.1.4"><mo id="S4.E6X.2.1.1.m1.1.4.1" stretchy="false">(</mo><mi id="S4.E6X.2.1.1.m1.1.4.2">X</mi><mrow id="S4.E6X.2.1.1.m1.1.4.3"><mo id="S4.E6X.2.1.1.m1.1.4.3.1" stretchy="false">[</mo><mn id="S4.E6X.2.1.1.m1.1.4.3.2">1</mn><mo id="S4.E6X.2.1.1.m1.1.4.3.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.E6X.2.1.1.m1.1.4.3.4">h</mi><mi id="S4.E6X.2.1.1.m1.1.4.3.5">w</mi><mo id="S4.E6X.2.1.1.m1.1.4.3.6">+</mo><mn id="S4.E6X.2.1.1.m1.1.4.3.7">1</mn><mo id="S4.E6X.2.1.1.m1.1.4.3.8" stretchy="false">]</mo></mrow><mo id="S4.E6X.2.1.1.m1.1.4.4" stretchy="false">)</mo></mrow><mo id="S4.E6X.2.1.1.m1.1.5" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S4.E6X.2.1.1.m1.1c">\displaystyle\mathcal{O}^{\prime}=\operatorname{Proj}(X[1:hw+1]).</annotation><annotation encoding="application/x-llamapun" id="S4.E6X.2.1.1.m1.1d">caligraphic_O start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = roman_Proj ( italic_X [ 1 : italic_h italic_w + 1 ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(6)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p8.4.1">Larger upsampling kernel.</span> We follow the upsampling operator in FeatUp, i.e., the parameterized JBU. As mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.SS2" title="3.2 FeatUp ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the upsampling kernels <math alttext="k_{range}" class="ltx_Math" display="inline" id="S4.SS1.p8.1.m1.1"><semantics id="S4.SS1.p8.1.m1.1a"><msub id="S4.SS1.p8.1.m1.1.1" xref="S4.SS1.p8.1.m1.1.1.cmml"><mi id="S4.SS1.p8.1.m1.1.1.2" xref="S4.SS1.p8.1.m1.1.1.2.cmml">k</mi><mrow id="S4.SS1.p8.1.m1.1.1.3" xref="S4.SS1.p8.1.m1.1.1.3.cmml"><mi id="S4.SS1.p8.1.m1.1.1.3.2" xref="S4.SS1.p8.1.m1.1.1.3.2.cmml">r</mi><mo id="S4.SS1.p8.1.m1.1.1.3.1" xref="S4.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.1.m1.1.1.3.3" xref="S4.SS1.p8.1.m1.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p8.1.m1.1.1.3.1a" xref="S4.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.1.m1.1.1.3.4" xref="S4.SS1.p8.1.m1.1.1.3.4.cmml">n</mi><mo id="S4.SS1.p8.1.m1.1.1.3.1b" xref="S4.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.1.m1.1.1.3.5" xref="S4.SS1.p8.1.m1.1.1.3.5.cmml">g</mi><mo id="S4.SS1.p8.1.m1.1.1.3.1c" xref="S4.SS1.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.1.m1.1.1.3.6" xref="S4.SS1.p8.1.m1.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.1.m1.1b"><apply id="S4.SS1.p8.1.m1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.1.m1.1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p8.1.m1.1.1.2.cmml" xref="S4.SS1.p8.1.m1.1.1.2">𝑘</ci><apply id="S4.SS1.p8.1.m1.1.1.3.cmml" xref="S4.SS1.p8.1.m1.1.1.3"><times id="S4.SS1.p8.1.m1.1.1.3.1.cmml" xref="S4.SS1.p8.1.m1.1.1.3.1"></times><ci id="S4.SS1.p8.1.m1.1.1.3.2.cmml" xref="S4.SS1.p8.1.m1.1.1.3.2">𝑟</ci><ci id="S4.SS1.p8.1.m1.1.1.3.3.cmml" xref="S4.SS1.p8.1.m1.1.1.3.3">𝑎</ci><ci id="S4.SS1.p8.1.m1.1.1.3.4.cmml" xref="S4.SS1.p8.1.m1.1.1.3.4">𝑛</ci><ci id="S4.SS1.p8.1.m1.1.1.3.5.cmml" xref="S4.SS1.p8.1.m1.1.1.3.5">𝑔</ci><ci id="S4.SS1.p8.1.m1.1.1.3.6.cmml" xref="S4.SS1.p8.1.m1.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.1.m1.1c">k_{range}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.1.m1.1d">italic_k start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="k_{spatial}" class="ltx_Math" display="inline" id="S4.SS1.p8.2.m2.1"><semantics id="S4.SS1.p8.2.m2.1a"><msub id="S4.SS1.p8.2.m2.1.1" xref="S4.SS1.p8.2.m2.1.1.cmml"><mi id="S4.SS1.p8.2.m2.1.1.2" xref="S4.SS1.p8.2.m2.1.1.2.cmml">k</mi><mrow id="S4.SS1.p8.2.m2.1.1.3" xref="S4.SS1.p8.2.m2.1.1.3.cmml"><mi id="S4.SS1.p8.2.m2.1.1.3.2" xref="S4.SS1.p8.2.m2.1.1.3.2.cmml">s</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.3" xref="S4.SS1.p8.2.m2.1.1.3.3.cmml">p</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1a" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.4" xref="S4.SS1.p8.2.m2.1.1.3.4.cmml">a</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1b" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.5" xref="S4.SS1.p8.2.m2.1.1.3.5.cmml">t</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1c" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.6" xref="S4.SS1.p8.2.m2.1.1.3.6.cmml">i</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1d" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.7" xref="S4.SS1.p8.2.m2.1.1.3.7.cmml">a</mi><mo id="S4.SS1.p8.2.m2.1.1.3.1e" xref="S4.SS1.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.2.m2.1.1.3.8" xref="S4.SS1.p8.2.m2.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.2.m2.1b"><apply id="S4.SS1.p8.2.m2.1.1.cmml" xref="S4.SS1.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.2.m2.1.1.1.cmml" xref="S4.SS1.p8.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p8.2.m2.1.1.2.cmml" xref="S4.SS1.p8.2.m2.1.1.2">𝑘</ci><apply id="S4.SS1.p8.2.m2.1.1.3.cmml" xref="S4.SS1.p8.2.m2.1.1.3"><times id="S4.SS1.p8.2.m2.1.1.3.1.cmml" xref="S4.SS1.p8.2.m2.1.1.3.1"></times><ci id="S4.SS1.p8.2.m2.1.1.3.2.cmml" xref="S4.SS1.p8.2.m2.1.1.3.2">𝑠</ci><ci id="S4.SS1.p8.2.m2.1.1.3.3.cmml" xref="S4.SS1.p8.2.m2.1.1.3.3">𝑝</ci><ci id="S4.SS1.p8.2.m2.1.1.3.4.cmml" xref="S4.SS1.p8.2.m2.1.1.3.4">𝑎</ci><ci id="S4.SS1.p8.2.m2.1.1.3.5.cmml" xref="S4.SS1.p8.2.m2.1.1.3.5">𝑡</ci><ci id="S4.SS1.p8.2.m2.1.1.3.6.cmml" xref="S4.SS1.p8.2.m2.1.1.3.6">𝑖</ci><ci id="S4.SS1.p8.2.m2.1.1.3.7.cmml" xref="S4.SS1.p8.2.m2.1.1.3.7">𝑎</ci><ci id="S4.SS1.p8.2.m2.1.1.3.8.cmml" xref="S4.SS1.p8.2.m2.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.2.m2.1c">k_{spatial}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.2.m2.1d">italic_k start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math> of the JBU are computed from the elements within a window in the guidance feature. the generation of <math alttext="k_{range}" class="ltx_Math" display="inline" id="S4.SS1.p8.3.m3.1"><semantics id="S4.SS1.p8.3.m3.1a"><msub id="S4.SS1.p8.3.m3.1.1" xref="S4.SS1.p8.3.m3.1.1.cmml"><mi id="S4.SS1.p8.3.m3.1.1.2" xref="S4.SS1.p8.3.m3.1.1.2.cmml">k</mi><mrow id="S4.SS1.p8.3.m3.1.1.3" xref="S4.SS1.p8.3.m3.1.1.3.cmml"><mi id="S4.SS1.p8.3.m3.1.1.3.2" xref="S4.SS1.p8.3.m3.1.1.3.2.cmml">r</mi><mo id="S4.SS1.p8.3.m3.1.1.3.1" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.3.m3.1.1.3.3" xref="S4.SS1.p8.3.m3.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p8.3.m3.1.1.3.1a" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.3.m3.1.1.3.4" xref="S4.SS1.p8.3.m3.1.1.3.4.cmml">n</mi><mo id="S4.SS1.p8.3.m3.1.1.3.1b" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.3.m3.1.1.3.5" xref="S4.SS1.p8.3.m3.1.1.3.5.cmml">g</mi><mo id="S4.SS1.p8.3.m3.1.1.3.1c" xref="S4.SS1.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.3.m3.1.1.3.6" xref="S4.SS1.p8.3.m3.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.3.m3.1b"><apply id="S4.SS1.p8.3.m3.1.1.cmml" xref="S4.SS1.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.3.m3.1.1.1.cmml" xref="S4.SS1.p8.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p8.3.m3.1.1.2.cmml" xref="S4.SS1.p8.3.m3.1.1.2">𝑘</ci><apply id="S4.SS1.p8.3.m3.1.1.3.cmml" xref="S4.SS1.p8.3.m3.1.1.3"><times id="S4.SS1.p8.3.m3.1.1.3.1.cmml" xref="S4.SS1.p8.3.m3.1.1.3.1"></times><ci id="S4.SS1.p8.3.m3.1.1.3.2.cmml" xref="S4.SS1.p8.3.m3.1.1.3.2">𝑟</ci><ci id="S4.SS1.p8.3.m3.1.1.3.3.cmml" xref="S4.SS1.p8.3.m3.1.1.3.3">𝑎</ci><ci id="S4.SS1.p8.3.m3.1.1.3.4.cmml" xref="S4.SS1.p8.3.m3.1.1.3.4">𝑛</ci><ci id="S4.SS1.p8.3.m3.1.1.3.5.cmml" xref="S4.SS1.p8.3.m3.1.1.3.5">𝑔</ci><ci id="S4.SS1.p8.3.m3.1.1.3.6.cmml" xref="S4.SS1.p8.3.m3.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.3.m3.1c">k_{range}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.3.m3.1d">italic_k start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="k_{spatial}" class="ltx_Math" display="inline" id="S4.SS1.p8.4.m4.1"><semantics id="S4.SS1.p8.4.m4.1a"><msub id="S4.SS1.p8.4.m4.1.1" xref="S4.SS1.p8.4.m4.1.1.cmml"><mi id="S4.SS1.p8.4.m4.1.1.2" xref="S4.SS1.p8.4.m4.1.1.2.cmml">k</mi><mrow id="S4.SS1.p8.4.m4.1.1.3" xref="S4.SS1.p8.4.m4.1.1.3.cmml"><mi id="S4.SS1.p8.4.m4.1.1.3.2" xref="S4.SS1.p8.4.m4.1.1.3.2.cmml">s</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.3" xref="S4.SS1.p8.4.m4.1.1.3.3.cmml">p</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1a" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.4" xref="S4.SS1.p8.4.m4.1.1.3.4.cmml">a</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1b" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.5" xref="S4.SS1.p8.4.m4.1.1.3.5.cmml">t</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1c" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.6" xref="S4.SS1.p8.4.m4.1.1.3.6.cmml">i</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1d" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.7" xref="S4.SS1.p8.4.m4.1.1.3.7.cmml">a</mi><mo id="S4.SS1.p8.4.m4.1.1.3.1e" xref="S4.SS1.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p8.4.m4.1.1.3.8" xref="S4.SS1.p8.4.m4.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.4.m4.1b"><apply id="S4.SS1.p8.4.m4.1.1.cmml" xref="S4.SS1.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p8.4.m4.1.1.1.cmml" xref="S4.SS1.p8.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p8.4.m4.1.1.2.cmml" xref="S4.SS1.p8.4.m4.1.1.2">𝑘</ci><apply id="S4.SS1.p8.4.m4.1.1.3.cmml" xref="S4.SS1.p8.4.m4.1.1.3"><times id="S4.SS1.p8.4.m4.1.1.3.1.cmml" xref="S4.SS1.p8.4.m4.1.1.3.1"></times><ci id="S4.SS1.p8.4.m4.1.1.3.2.cmml" xref="S4.SS1.p8.4.m4.1.1.3.2">𝑠</ci><ci id="S4.SS1.p8.4.m4.1.1.3.3.cmml" xref="S4.SS1.p8.4.m4.1.1.3.3">𝑝</ci><ci id="S4.SS1.p8.4.m4.1.1.3.4.cmml" xref="S4.SS1.p8.4.m4.1.1.3.4">𝑎</ci><ci id="S4.SS1.p8.4.m4.1.1.3.5.cmml" xref="S4.SS1.p8.4.m4.1.1.3.5">𝑡</ci><ci id="S4.SS1.p8.4.m4.1.1.3.6.cmml" xref="S4.SS1.p8.4.m4.1.1.3.6">𝑖</ci><ci id="S4.SS1.p8.4.m4.1.1.3.7.cmml" xref="S4.SS1.p8.4.m4.1.1.3.7">𝑎</ci><ci id="S4.SS1.p8.4.m4.1.1.3.8.cmml" xref="S4.SS1.p8.4.m4.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.4.m4.1c">k_{spatial}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p8.4.m4.1d">italic_k start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math> can be formulated as follows:</p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E7">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E7X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle k_{spatial}(p,q)=\exp\left(\frac{-\|p-q\|_{2}^{2}}{2\tau_{%
spatial}^{2}}\right)," class="ltx_Math" display="inline" id="S4.E7X.2.1.1.m1.5"><semantics id="S4.E7X.2.1.1.m1.5a"><mrow id="S4.E7X.2.1.1.m1.5.5.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.cmml"><mrow id="S4.E7X.2.1.1.m1.5.5.1.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.cmml"><mrow id="S4.E7X.2.1.1.m1.5.5.1.1.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.cmml"><msub id="S4.E7X.2.1.1.m1.5.5.1.1.2.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.cmml"><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.2.cmml">k</mi><mrow id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.cmml"><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.2.cmml">s</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.3" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.3.cmml">p</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1a" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.4" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.4.cmml">a</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1b" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.5" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.5.cmml">t</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1c" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.6" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.6.cmml">i</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1d" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.7" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.7.cmml">a</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1e" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.8" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.8.cmml">l</mi></mrow></msub><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.1.cmml">⁢</mo><mrow id="S4.E7X.2.1.1.m1.5.5.1.1.2.3.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.3.1.cmml"><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.3.2.1" stretchy="false" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S4.E7X.2.1.1.m1.2.2" xref="S4.E7X.2.1.1.m1.2.2.cmml">p</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.3.2.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S4.E7X.2.1.1.m1.3.3" xref="S4.E7X.2.1.1.m1.3.3.cmml">q</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.2.3.2.3" stretchy="false" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E7X.2.1.1.m1.5.5.1.1.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E7X.2.1.1.m1.5.5.1.1.3.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml"><mi id="S4.E7X.2.1.1.m1.4.4" xref="S4.E7X.2.1.1.m1.4.4.cmml">exp</mi><mo id="S4.E7X.2.1.1.m1.5.5.1.1.3.2a" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml">⁡</mo><mrow id="S4.E7X.2.1.1.m1.5.5.1.1.3.2.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml"><mo id="S4.E7X.2.1.1.m1.5.5.1.1.3.2.1.1" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml">(</mo><mstyle displaystyle="true" id="S4.E7X.2.1.1.m1.1.1" xref="S4.E7X.2.1.1.m1.1.1.cmml"><mfrac id="S4.E7X.2.1.1.m1.1.1a" xref="S4.E7X.2.1.1.m1.1.1.cmml"><mrow id="S4.E7X.2.1.1.m1.1.1.1" xref="S4.E7X.2.1.1.m1.1.1.1.cmml"><mo id="S4.E7X.2.1.1.m1.1.1.1a" xref="S4.E7X.2.1.1.m1.1.1.1.cmml">−</mo><msubsup id="S4.E7X.2.1.1.m1.1.1.1.1" xref="S4.E7X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mo id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mo id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml">q</mi></mrow><mo id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E7X.2.1.1.m1.1.1.1.1.1.3" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.3.cmml">2</mn><mn id="S4.E7X.2.1.1.m1.1.1.1.1.3" xref="S4.E7X.2.1.1.m1.1.1.1.1.3.cmml">2</mn></msubsup></mrow><mrow id="S4.E7X.2.1.1.m1.1.1.3" xref="S4.E7X.2.1.1.m1.1.1.3.cmml"><mn id="S4.E7X.2.1.1.m1.1.1.3.2" xref="S4.E7X.2.1.1.m1.1.1.3.2.cmml">2</mn><mo id="S4.E7X.2.1.1.m1.1.1.3.1" xref="S4.E7X.2.1.1.m1.1.1.3.1.cmml">⁢</mo><msubsup id="S4.E7X.2.1.1.m1.1.1.3.3" xref="S4.E7X.2.1.1.m1.1.1.3.3.cmml"><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.2" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.2.cmml">τ</mi><mrow id="S4.E7X.2.1.1.m1.1.1.3.3.2.3" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.cmml"><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.2" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.2.cmml">s</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.3" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.3.cmml">p</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1a" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.4" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.4.cmml">a</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1b" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.5" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.5.cmml">t</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1c" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.6" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.6.cmml">i</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1d" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.7" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.7.cmml">a</mi><mo id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1e" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.8" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.8.cmml">l</mi></mrow><mn id="S4.E7X.2.1.1.m1.1.1.3.3.3" xref="S4.E7X.2.1.1.m1.1.1.3.3.3.cmml">2</mn></msubsup></mrow></mfrac></mstyle><mo id="S4.E7X.2.1.1.m1.5.5.1.1.3.2.1.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E7X.2.1.1.m1.5.5.1.2" xref="S4.E7X.2.1.1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7X.2.1.1.m1.5b"><apply id="S4.E7X.2.1.1.m1.5.5.1.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1"><eq id="S4.E7X.2.1.1.m1.5.5.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.1"></eq><apply id="S4.E7X.2.1.1.m1.5.5.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2"><times id="S4.E7X.2.1.1.m1.5.5.1.1.2.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.1"></times><apply id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2">subscript</csymbol><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.2.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.2">𝑘</ci><apply id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3"><times id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.1"></times><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.2.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.2">𝑠</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.3.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.3">𝑝</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.4.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.4">𝑎</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.5.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.5">𝑡</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.6.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.6">𝑖</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.7.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.7">𝑎</ci><ci id="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.8.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.2.3.8">𝑙</ci></apply></apply><interval closure="open" id="S4.E7X.2.1.1.m1.5.5.1.1.2.3.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.2.3.2"><ci id="S4.E7X.2.1.1.m1.2.2.cmml" xref="S4.E7X.2.1.1.m1.2.2">𝑝</ci><ci id="S4.E7X.2.1.1.m1.3.3.cmml" xref="S4.E7X.2.1.1.m1.3.3">𝑞</ci></interval></apply><apply id="S4.E7X.2.1.1.m1.5.5.1.1.3.1.cmml" xref="S4.E7X.2.1.1.m1.5.5.1.1.3.2"><exp id="S4.E7X.2.1.1.m1.4.4.cmml" xref="S4.E7X.2.1.1.m1.4.4"></exp><apply id="S4.E7X.2.1.1.m1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1"><divide id="S4.E7X.2.1.1.m1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1"></divide><apply id="S4.E7X.2.1.1.m1.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1"><minus id="S4.E7X.2.1.1.m1.1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.1"></minus><apply id="S4.E7X.2.1.1.m1.1.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7X.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1">superscript</csymbol><apply id="S4.E7X.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1">subscript</csymbol><apply id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1"><minus id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.1.1.1.3">𝑞</ci></apply></apply><cn id="S4.E7X.2.1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E7X.2.1.1.m1.1.1.1.1.1.3">2</cn></apply><cn id="S4.E7X.2.1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S4.E7X.2.1.1.m1.1.1.1.1.3">2</cn></apply></apply><apply id="S4.E7X.2.1.1.m1.1.1.3.cmml" xref="S4.E7X.2.1.1.m1.1.1.3"><times id="S4.E7X.2.1.1.m1.1.1.3.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.1"></times><cn id="S4.E7X.2.1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.E7X.2.1.1.m1.1.1.3.2">2</cn><apply id="S4.E7X.2.1.1.m1.1.1.3.3.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E7X.2.1.1.m1.1.1.3.3.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3">superscript</csymbol><apply id="S4.E7X.2.1.1.m1.1.1.3.3.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E7X.2.1.1.m1.1.1.3.3.2.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3">subscript</csymbol><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.2">𝜏</ci><apply id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3"><times id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.1"></times><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.2.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.2">𝑠</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.3.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.3">𝑝</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.4.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.4">𝑎</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.5.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.5">𝑡</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.6.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.6">𝑖</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.7.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.7">𝑎</ci><ci id="S4.E7X.2.1.1.m1.1.1.3.3.2.3.8.cmml" xref="S4.E7X.2.1.1.m1.1.1.3.3.2.3.8">𝑙</ci></apply></apply><cn id="S4.E7X.2.1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S4.E7X.2.1.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7X.2.1.1.m1.5c">\displaystyle k_{spatial}(p,q)=\exp\left(\frac{-\|p-q\|_{2}^{2}}{2\tau_{%
spatial}^{2}}\right),</annotation><annotation encoding="application/x-llamapun" id="S4.E7X.2.1.1.m1.5d">italic_k start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT ( italic_p , italic_q ) = roman_exp ( divide start_ARG - ∥ italic_p - italic_q ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_τ start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E8">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E8X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle k_{range}(p,q)=" class="ltx_Math" display="inline" id="S4.E8X.2.1.1.m1.2"><semantics id="S4.E8X.2.1.1.m1.2a"><mrow id="S4.E8X.2.1.1.m1.2.3" xref="S4.E8X.2.1.1.m1.2.3.cmml"><mrow id="S4.E8X.2.1.1.m1.2.3.2" xref="S4.E8X.2.1.1.m1.2.3.2.cmml"><msub id="S4.E8X.2.1.1.m1.2.3.2.2" xref="S4.E8X.2.1.1.m1.2.3.2.2.cmml"><mi id="S4.E8X.2.1.1.m1.2.3.2.2.2" xref="S4.E8X.2.1.1.m1.2.3.2.2.2.cmml">k</mi><mrow id="S4.E8X.2.1.1.m1.2.3.2.2.3" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.cmml"><mi id="S4.E8X.2.1.1.m1.2.3.2.2.3.2" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.2.cmml">r</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.2.3.1" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8X.2.1.1.m1.2.3.2.2.3.3" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.3.cmml">a</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.2.3.1a" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8X.2.1.1.m1.2.3.2.2.3.4" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.4.cmml">n</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.2.3.1b" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8X.2.1.1.m1.2.3.2.2.3.5" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.5.cmml">g</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.2.3.1c" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.1.cmml">⁢</mo><mi id="S4.E8X.2.1.1.m1.2.3.2.2.3.6" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.6.cmml">e</mi></mrow></msub><mo id="S4.E8X.2.1.1.m1.2.3.2.1" xref="S4.E8X.2.1.1.m1.2.3.2.1.cmml">⁢</mo><mrow id="S4.E8X.2.1.1.m1.2.3.2.3.2" xref="S4.E8X.2.1.1.m1.2.3.2.3.1.cmml"><mo id="S4.E8X.2.1.1.m1.2.3.2.3.2.1" stretchy="false" xref="S4.E8X.2.1.1.m1.2.3.2.3.1.cmml">(</mo><mi id="S4.E8X.2.1.1.m1.1.1" xref="S4.E8X.2.1.1.m1.1.1.cmml">p</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.3.2.2" xref="S4.E8X.2.1.1.m1.2.3.2.3.1.cmml">,</mo><mi id="S4.E8X.2.1.1.m1.2.2" xref="S4.E8X.2.1.1.m1.2.2.cmml">q</mi><mo id="S4.E8X.2.1.1.m1.2.3.2.3.2.3" stretchy="false" xref="S4.E8X.2.1.1.m1.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E8X.2.1.1.m1.2.3.1" xref="S4.E8X.2.1.1.m1.2.3.1.cmml">=</mo><mi id="S4.E8X.2.1.1.m1.2.3.3" xref="S4.E8X.2.1.1.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E8X.2.1.1.m1.2b"><apply id="S4.E8X.2.1.1.m1.2.3.cmml" xref="S4.E8X.2.1.1.m1.2.3"><eq id="S4.E8X.2.1.1.m1.2.3.1.cmml" xref="S4.E8X.2.1.1.m1.2.3.1"></eq><apply id="S4.E8X.2.1.1.m1.2.3.2.cmml" xref="S4.E8X.2.1.1.m1.2.3.2"><times id="S4.E8X.2.1.1.m1.2.3.2.1.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.1"></times><apply id="S4.E8X.2.1.1.m1.2.3.2.2.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S4.E8X.2.1.1.m1.2.3.2.2.1.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2">subscript</csymbol><ci id="S4.E8X.2.1.1.m1.2.3.2.2.2.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.2">𝑘</ci><apply id="S4.E8X.2.1.1.m1.2.3.2.2.3.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3"><times id="S4.E8X.2.1.1.m1.2.3.2.2.3.1.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.1"></times><ci id="S4.E8X.2.1.1.m1.2.3.2.2.3.2.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.2">𝑟</ci><ci id="S4.E8X.2.1.1.m1.2.3.2.2.3.3.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.3">𝑎</ci><ci id="S4.E8X.2.1.1.m1.2.3.2.2.3.4.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.4">𝑛</ci><ci id="S4.E8X.2.1.1.m1.2.3.2.2.3.5.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.5">𝑔</ci><ci id="S4.E8X.2.1.1.m1.2.3.2.2.3.6.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.2.3.6">𝑒</ci></apply></apply><interval closure="open" id="S4.E8X.2.1.1.m1.2.3.2.3.1.cmml" xref="S4.E8X.2.1.1.m1.2.3.2.3.2"><ci id="S4.E8X.2.1.1.m1.1.1.cmml" xref="S4.E8X.2.1.1.m1.1.1">𝑝</ci><ci id="S4.E8X.2.1.1.m1.2.2.cmml" xref="S4.E8X.2.1.1.m1.2.2">𝑞</ci></interval></apply><csymbol cd="latexml" id="S4.E8X.2.1.1.m1.2.3.3.cmml" xref="S4.E8X.2.1.1.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8X.2.1.1.m1.2c">\displaystyle k_{range}(p,q)=</annotation><annotation encoding="application/x-llamapun" id="S4.E8X.2.1.1.m1.2d">italic_k start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT ( italic_p , italic_q ) =</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(8)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E8Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\operatorname{softmax}_{(a,b)\in\Omega}\left(\frac{1}{\tau_{range%
}^{2}}MLP(G[i,j])\cdot MLP(G[a,b])\right)," class="ltx_Math" display="inline" id="S4.E8Xa.2.1.1.m1.7"><semantics id="S4.E8Xa.2.1.1.m1.7a"><mrow id="S4.E8Xa.2.1.1.m1.7.7.1"><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml"><msub id="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.cmml"><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.2.cmml">softmax</mi><mrow id="S4.E8Xa.2.1.1.m1.2.2.2" xref="S4.E8Xa.2.1.1.m1.2.2.2.cmml"><mrow id="S4.E8Xa.2.1.1.m1.2.2.2.4.2" xref="S4.E8Xa.2.1.1.m1.2.2.2.4.1.cmml"><mo id="S4.E8Xa.2.1.1.m1.2.2.2.4.2.1" stretchy="false" xref="S4.E8Xa.2.1.1.m1.2.2.2.4.1.cmml">(</mo><mi id="S4.E8Xa.2.1.1.m1.1.1.1.1" xref="S4.E8Xa.2.1.1.m1.1.1.1.1.cmml">a</mi><mo id="S4.E8Xa.2.1.1.m1.2.2.2.4.2.2" xref="S4.E8Xa.2.1.1.m1.2.2.2.4.1.cmml">,</mo><mi id="S4.E8Xa.2.1.1.m1.2.2.2.2" xref="S4.E8Xa.2.1.1.m1.2.2.2.2.cmml">b</mi><mo id="S4.E8Xa.2.1.1.m1.2.2.2.4.2.3" stretchy="false" xref="S4.E8Xa.2.1.1.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S4.E8Xa.2.1.1.m1.2.2.2.3" xref="S4.E8Xa.2.1.1.m1.2.2.2.3.cmml">∈</mo><mi id="S4.E8Xa.2.1.1.m1.2.2.2.5" mathvariant="normal" xref="S4.E8Xa.2.1.1.m1.2.2.2.5.cmml">Ω</mi></mrow></msub><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2a" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml">⁡</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml"><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml">(</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.cmml"><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.cmml"><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.cmml"><mfrac id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3a" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.cmml"><mn id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.2.cmml">1</mn><msubsup id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.cmml"><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.2.cmml">τ</mi><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.cmml"><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.2.cmml">r</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.3.cmml">a</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1a" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.4" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.4.cmml">n</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1b" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.5" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.5.cmml">g</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1c" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.6" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.6.cmml">e</mi></mrow><mn id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.3.cmml">2</mn></msubsup></mfrac></mstyle><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.4" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.4.cmml">M</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2a" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.5" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.5.cmml">L</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2b" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.6" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.6.cmml">P</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2c" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.cmml">G</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.1.cmml"><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.1.cmml">[</mo><mi id="S4.E8Xa.2.1.1.m1.3.3" xref="S4.E8Xa.2.1.1.m1.3.3.cmml">i</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.2.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S4.E8Xa.2.1.1.m1.4.4" xref="S4.E8Xa.2.1.1.m1.4.4.cmml">j</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.2.3" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.1.cmml">]</mo></mrow></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.2" rspace="0.222em" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.2.cmml">⋅</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.3.cmml">M</mi></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.4" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.4.cmml">L</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3a" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3.cmml">⁢</mo><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.5" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.5.cmml">P</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3b" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3.cmml">⁢</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.cmml"><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.2" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.cmml">(</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.cmml"><mi id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.2.cmml">G</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.1" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.1.cmml">⁢</mo><mrow id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.1.cmml"><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.2.1" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.1.cmml">[</mo><mi id="S4.E8Xa.2.1.1.m1.5.5" xref="S4.E8Xa.2.1.1.m1.5.5.cmml">a</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.2.2" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.1.cmml">,</mo><mi id="S4.E8Xa.2.1.1.m1.6.6" xref="S4.E8Xa.2.1.1.m1.6.6.cmml">b</mi><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.2.3" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.1.cmml">]</mo></mrow></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.3" stretchy="false" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.3" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E8Xa.2.1.1.m1.7.7.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E8Xa.2.1.1.m1.7b"><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2"><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1">subscript</csymbol><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.1.1.2">softmax</ci><apply id="S4.E8Xa.2.1.1.m1.2.2.2.cmml" xref="S4.E8Xa.2.1.1.m1.2.2.2"><in id="S4.E8Xa.2.1.1.m1.2.2.2.3.cmml" xref="S4.E8Xa.2.1.1.m1.2.2.2.3"></in><interval closure="open" id="S4.E8Xa.2.1.1.m1.2.2.2.4.1.cmml" xref="S4.E8Xa.2.1.1.m1.2.2.2.4.2"><ci id="S4.E8Xa.2.1.1.m1.1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.1.1.1.1">𝑎</ci><ci id="S4.E8Xa.2.1.1.m1.2.2.2.2.cmml" xref="S4.E8Xa.2.1.1.m1.2.2.2.2">𝑏</ci></interval><ci id="S4.E8Xa.2.1.1.m1.2.2.2.5.cmml" xref="S4.E8Xa.2.1.1.m1.2.2.2.5">Ω</ci></apply></apply><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1"><times id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.3"></times><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1"><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.2">⋅</ci><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1"><times id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.2"></times><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3"><divide id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3"></divide><cn id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.2.cmml" type="integer" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.2">1</cn><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3">superscript</csymbol><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3">subscript</csymbol><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.2">𝜏</ci><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3"><times id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.1"></times><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.2">𝑟</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.3">𝑎</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.4.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.4">𝑛</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.5.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.5">𝑔</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.6.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.2.3.6">𝑒</ci></apply></apply><cn id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.3.cmml" type="integer" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.3.3.3">2</cn></apply></apply><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.4.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.4">𝑀</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.5.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.5">𝐿</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.6.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.6">𝑃</ci><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1"><times id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.1"></times><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.2">𝐺</ci><interval closure="closed" id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.2"><ci id="S4.E8Xa.2.1.1.m1.3.3.cmml" xref="S4.E8Xa.2.1.1.m1.3.3">𝑖</ci><ci id="S4.E8Xa.2.1.1.m1.4.4.cmml" xref="S4.E8Xa.2.1.1.m1.4.4">𝑗</ci></interval></apply></apply><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.3.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.1.3">𝑀</ci></apply><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.4.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.4">𝐿</ci><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.5.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.5">𝑃</ci><apply id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1"><times id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.1"></times><ci id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.2.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.2">𝐺</ci><interval closure="closed" id="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.1.cmml" xref="S4.E8Xa.2.1.1.m1.7.7.1.1.2.2.1.2.1.1.3.2"><ci id="S4.E8Xa.2.1.1.m1.5.5.cmml" xref="S4.E8Xa.2.1.1.m1.5.5">𝑎</ci><ci id="S4.E8Xa.2.1.1.m1.6.6.cmml" xref="S4.E8Xa.2.1.1.m1.6.6">𝑏</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8Xa.2.1.1.m1.7c">\displaystyle\operatorname{softmax}_{(a,b)\in\Omega}\left(\frac{1}{\tau_{range%
}^{2}}MLP(G[i,j])\cdot MLP(G[a,b])\right),</annotation><annotation encoding="application/x-llamapun" id="S4.E8Xa.2.1.1.m1.7d">roman_softmax start_POSTSUBSCRIPT ( italic_a , italic_b ) ∈ roman_Ω end_POSTSUBSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_τ start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG italic_M italic_L italic_P ( italic_G [ italic_i , italic_j ] ) ⋅ italic_M italic_L italic_P ( italic_G [ italic_a , italic_b ] ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.9">where <math alttext="(p,q)" class="ltx_Math" display="inline" id="S4.SS1.p11.1.m1.2"><semantics id="S4.SS1.p11.1.m1.2a"><mrow id="S4.SS1.p11.1.m1.2.3.2" xref="S4.SS1.p11.1.m1.2.3.1.cmml"><mo id="S4.SS1.p11.1.m1.2.3.2.1" stretchy="false" xref="S4.SS1.p11.1.m1.2.3.1.cmml">(</mo><mi id="S4.SS1.p11.1.m1.1.1" xref="S4.SS1.p11.1.m1.1.1.cmml">p</mi><mo id="S4.SS1.p11.1.m1.2.3.2.2" xref="S4.SS1.p11.1.m1.2.3.1.cmml">,</mo><mi id="S4.SS1.p11.1.m1.2.2" xref="S4.SS1.p11.1.m1.2.2.cmml">q</mi><mo id="S4.SS1.p11.1.m1.2.3.2.3" stretchy="false" xref="S4.SS1.p11.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.1.m1.2b"><interval closure="open" id="S4.SS1.p11.1.m1.2.3.1.cmml" xref="S4.SS1.p11.1.m1.2.3.2"><ci id="S4.SS1.p11.1.m1.1.1.cmml" xref="S4.SS1.p11.1.m1.1.1">𝑝</ci><ci id="S4.SS1.p11.1.m1.2.2.cmml" xref="S4.SS1.p11.1.m1.2.2">𝑞</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.1.m1.2c">(p,q)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.1.m1.2d">( italic_p , italic_q )</annotation></semantics></math> denotes the position in the kernel. <math alttext="\Omega" class="ltx_Math" display="inline" id="S4.SS1.p11.2.m2.1"><semantics id="S4.SS1.p11.2.m2.1a"><mi id="S4.SS1.p11.2.m2.1.1" mathvariant="normal" xref="S4.SS1.p11.2.m2.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.2.m2.1b"><ci id="S4.SS1.p11.2.m2.1.1.cmml" xref="S4.SS1.p11.2.m2.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.2.m2.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.2.m2.1d">roman_Ω</annotation></semantics></math> denotes a window centered at <math alttext="(i,j)" class="ltx_Math" display="inline" id="S4.SS1.p11.3.m3.2"><semantics id="S4.SS1.p11.3.m3.2a"><mrow id="S4.SS1.p11.3.m3.2.3.2" xref="S4.SS1.p11.3.m3.2.3.1.cmml"><mo id="S4.SS1.p11.3.m3.2.3.2.1" stretchy="false" xref="S4.SS1.p11.3.m3.2.3.1.cmml">(</mo><mi id="S4.SS1.p11.3.m3.1.1" xref="S4.SS1.p11.3.m3.1.1.cmml">i</mi><mo id="S4.SS1.p11.3.m3.2.3.2.2" xref="S4.SS1.p11.3.m3.2.3.1.cmml">,</mo><mi id="S4.SS1.p11.3.m3.2.2" xref="S4.SS1.p11.3.m3.2.2.cmml">j</mi><mo id="S4.SS1.p11.3.m3.2.3.2.3" stretchy="false" xref="S4.SS1.p11.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.3.m3.2b"><interval closure="open" id="S4.SS1.p11.3.m3.2.3.1.cmml" xref="S4.SS1.p11.3.m3.2.3.2"><ci id="S4.SS1.p11.3.m3.1.1.cmml" xref="S4.SS1.p11.3.m3.1.1">𝑖</ci><ci id="S4.SS1.p11.3.m3.2.2.cmml" xref="S4.SS1.p11.3.m3.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.3.m3.2c">(i,j)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.3.m3.2d">( italic_i , italic_j )</annotation></semantics></math> in the guidance feature <math alttext="G" class="ltx_Math" display="inline" id="S4.SS1.p11.4.m4.1"><semantics id="S4.SS1.p11.4.m4.1a"><mi id="S4.SS1.p11.4.m4.1.1" xref="S4.SS1.p11.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.4.m4.1b"><ci id="S4.SS1.p11.4.m4.1.1.cmml" xref="S4.SS1.p11.4.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.4.m4.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.4.m4.1d">italic_G</annotation></semantics></math>, which is extracted from HR RGB image. <math alttext="\tau_{spatial}" class="ltx_Math" display="inline" id="S4.SS1.p11.5.m5.1"><semantics id="S4.SS1.p11.5.m5.1a"><msub id="S4.SS1.p11.5.m5.1.1" xref="S4.SS1.p11.5.m5.1.1.cmml"><mi id="S4.SS1.p11.5.m5.1.1.2" xref="S4.SS1.p11.5.m5.1.1.2.cmml">τ</mi><mrow id="S4.SS1.p11.5.m5.1.1.3" xref="S4.SS1.p11.5.m5.1.1.3.cmml"><mi id="S4.SS1.p11.5.m5.1.1.3.2" xref="S4.SS1.p11.5.m5.1.1.3.2.cmml">s</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.3" xref="S4.SS1.p11.5.m5.1.1.3.3.cmml">p</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1a" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.4" xref="S4.SS1.p11.5.m5.1.1.3.4.cmml">a</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1b" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.5" xref="S4.SS1.p11.5.m5.1.1.3.5.cmml">t</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1c" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.6" xref="S4.SS1.p11.5.m5.1.1.3.6.cmml">i</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1d" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.7" xref="S4.SS1.p11.5.m5.1.1.3.7.cmml">a</mi><mo id="S4.SS1.p11.5.m5.1.1.3.1e" xref="S4.SS1.p11.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.5.m5.1.1.3.8" xref="S4.SS1.p11.5.m5.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.5.m5.1b"><apply id="S4.SS1.p11.5.m5.1.1.cmml" xref="S4.SS1.p11.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p11.5.m5.1.1.1.cmml" xref="S4.SS1.p11.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p11.5.m5.1.1.2.cmml" xref="S4.SS1.p11.5.m5.1.1.2">𝜏</ci><apply id="S4.SS1.p11.5.m5.1.1.3.cmml" xref="S4.SS1.p11.5.m5.1.1.3"><times id="S4.SS1.p11.5.m5.1.1.3.1.cmml" xref="S4.SS1.p11.5.m5.1.1.3.1"></times><ci id="S4.SS1.p11.5.m5.1.1.3.2.cmml" xref="S4.SS1.p11.5.m5.1.1.3.2">𝑠</ci><ci id="S4.SS1.p11.5.m5.1.1.3.3.cmml" xref="S4.SS1.p11.5.m5.1.1.3.3">𝑝</ci><ci id="S4.SS1.p11.5.m5.1.1.3.4.cmml" xref="S4.SS1.p11.5.m5.1.1.3.4">𝑎</ci><ci id="S4.SS1.p11.5.m5.1.1.3.5.cmml" xref="S4.SS1.p11.5.m5.1.1.3.5">𝑡</ci><ci id="S4.SS1.p11.5.m5.1.1.3.6.cmml" xref="S4.SS1.p11.5.m5.1.1.3.6">𝑖</ci><ci id="S4.SS1.p11.5.m5.1.1.3.7.cmml" xref="S4.SS1.p11.5.m5.1.1.3.7">𝑎</ci><ci id="S4.SS1.p11.5.m5.1.1.3.8.cmml" xref="S4.SS1.p11.5.m5.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.5.m5.1c">\tau_{spatial}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.5.m5.1d">italic_τ start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\tau_{range}" class="ltx_Math" display="inline" id="S4.SS1.p11.6.m6.1"><semantics id="S4.SS1.p11.6.m6.1a"><msub id="S4.SS1.p11.6.m6.1.1" xref="S4.SS1.p11.6.m6.1.1.cmml"><mi id="S4.SS1.p11.6.m6.1.1.2" xref="S4.SS1.p11.6.m6.1.1.2.cmml">τ</mi><mrow id="S4.SS1.p11.6.m6.1.1.3" xref="S4.SS1.p11.6.m6.1.1.3.cmml"><mi id="S4.SS1.p11.6.m6.1.1.3.2" xref="S4.SS1.p11.6.m6.1.1.3.2.cmml">r</mi><mo id="S4.SS1.p11.6.m6.1.1.3.1" xref="S4.SS1.p11.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.6.m6.1.1.3.3" xref="S4.SS1.p11.6.m6.1.1.3.3.cmml">a</mi><mo id="S4.SS1.p11.6.m6.1.1.3.1a" xref="S4.SS1.p11.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.6.m6.1.1.3.4" xref="S4.SS1.p11.6.m6.1.1.3.4.cmml">n</mi><mo id="S4.SS1.p11.6.m6.1.1.3.1b" xref="S4.SS1.p11.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.6.m6.1.1.3.5" xref="S4.SS1.p11.6.m6.1.1.3.5.cmml">g</mi><mo id="S4.SS1.p11.6.m6.1.1.3.1c" xref="S4.SS1.p11.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.6.m6.1.1.3.6" xref="S4.SS1.p11.6.m6.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.6.m6.1b"><apply id="S4.SS1.p11.6.m6.1.1.cmml" xref="S4.SS1.p11.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p11.6.m6.1.1.1.cmml" xref="S4.SS1.p11.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p11.6.m6.1.1.2.cmml" xref="S4.SS1.p11.6.m6.1.1.2">𝜏</ci><apply id="S4.SS1.p11.6.m6.1.1.3.cmml" xref="S4.SS1.p11.6.m6.1.1.3"><times id="S4.SS1.p11.6.m6.1.1.3.1.cmml" xref="S4.SS1.p11.6.m6.1.1.3.1"></times><ci id="S4.SS1.p11.6.m6.1.1.3.2.cmml" xref="S4.SS1.p11.6.m6.1.1.3.2">𝑟</ci><ci id="S4.SS1.p11.6.m6.1.1.3.3.cmml" xref="S4.SS1.p11.6.m6.1.1.3.3">𝑎</ci><ci id="S4.SS1.p11.6.m6.1.1.3.4.cmml" xref="S4.SS1.p11.6.m6.1.1.3.4">𝑛</ci><ci id="S4.SS1.p11.6.m6.1.1.3.5.cmml" xref="S4.SS1.p11.6.m6.1.1.3.5">𝑔</ci><ci id="S4.SS1.p11.6.m6.1.1.3.6.cmml" xref="S4.SS1.p11.6.m6.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.6.m6.1c">\tau_{range}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.6.m6.1d">italic_τ start_POSTSUBSCRIPT italic_r italic_a italic_n italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math> are learnable factors. In remote sensing images, unlike natural images, the size of the target presents a logarithmic scale spanning from the meter scale (e.g., trees, gardens) to the kilometer scale (e.g., forests, rangelands) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>. Therefore, we set larger upsampling kernels to obtain a wider receptive field. Here, we expand the window size to <math alttext="11\times 11" class="ltx_Math" display="inline" id="S4.SS1.p11.7.m7.1"><semantics id="S4.SS1.p11.7.m7.1a"><mrow id="S4.SS1.p11.7.m7.1.1" xref="S4.SS1.p11.7.m7.1.1.cmml"><mn id="S4.SS1.p11.7.m7.1.1.2" xref="S4.SS1.p11.7.m7.1.1.2.cmml">11</mn><mo id="S4.SS1.p11.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p11.7.m7.1.1.1.cmml">×</mo><mn id="S4.SS1.p11.7.m7.1.1.3" xref="S4.SS1.p11.7.m7.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.7.m7.1b"><apply id="S4.SS1.p11.7.m7.1.1.cmml" xref="S4.SS1.p11.7.m7.1.1"><times id="S4.SS1.p11.7.m7.1.1.1.cmml" xref="S4.SS1.p11.7.m7.1.1.1"></times><cn id="S4.SS1.p11.7.m7.1.1.2.cmml" type="integer" xref="S4.SS1.p11.7.m7.1.1.2">11</cn><cn id="S4.SS1.p11.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.p11.7.m7.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.7.m7.1c">11\times 11</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.7.m7.1d">11 × 11</annotation></semantics></math>, compared to <math alttext="7\times 7" class="ltx_Math" display="inline" id="S4.SS1.p11.8.m8.1"><semantics id="S4.SS1.p11.8.m8.1a"><mrow id="S4.SS1.p11.8.m8.1.1" xref="S4.SS1.p11.8.m8.1.1.cmml"><mn id="S4.SS1.p11.8.m8.1.1.2" xref="S4.SS1.p11.8.m8.1.1.2.cmml">7</mn><mo id="S4.SS1.p11.8.m8.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p11.8.m8.1.1.1.cmml">×</mo><mn id="S4.SS1.p11.8.m8.1.1.3" xref="S4.SS1.p11.8.m8.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.8.m8.1b"><apply id="S4.SS1.p11.8.m8.1.1.cmml" xref="S4.SS1.p11.8.m8.1.1"><times id="S4.SS1.p11.8.m8.1.1.1.cmml" xref="S4.SS1.p11.8.m8.1.1.1"></times><cn id="S4.SS1.p11.8.m8.1.1.2.cmml" type="integer" xref="S4.SS1.p11.8.m8.1.1.2">7</cn><cn id="S4.SS1.p11.8.m8.1.1.3.cmml" type="integer" xref="S4.SS1.p11.8.m8.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.8.m8.1c">7\times 7</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.8.m8.1d">7 × 7</annotation></semantics></math> in FeatUp. A possible concern is that a larger receptive field may introduce more irrelevant context, but with <math alttext="k_{spatial}" class="ltx_Math" display="inline" id="S4.SS1.p11.9.m9.1"><semantics id="S4.SS1.p11.9.m9.1a"><msub id="S4.SS1.p11.9.m9.1.1" xref="S4.SS1.p11.9.m9.1.1.cmml"><mi id="S4.SS1.p11.9.m9.1.1.2" xref="S4.SS1.p11.9.m9.1.1.2.cmml">k</mi><mrow id="S4.SS1.p11.9.m9.1.1.3" xref="S4.SS1.p11.9.m9.1.1.3.cmml"><mi id="S4.SS1.p11.9.m9.1.1.3.2" xref="S4.SS1.p11.9.m9.1.1.3.2.cmml">s</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.3" xref="S4.SS1.p11.9.m9.1.1.3.3.cmml">p</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1a" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.4" xref="S4.SS1.p11.9.m9.1.1.3.4.cmml">a</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1b" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.5" xref="S4.SS1.p11.9.m9.1.1.3.5.cmml">t</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1c" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.6" xref="S4.SS1.p11.9.m9.1.1.3.6.cmml">i</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1d" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.7" xref="S4.SS1.p11.9.m9.1.1.3.7.cmml">a</mi><mo id="S4.SS1.p11.9.m9.1.1.3.1e" xref="S4.SS1.p11.9.m9.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p11.9.m9.1.1.3.8" xref="S4.SS1.p11.9.m9.1.1.3.8.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p11.9.m9.1b"><apply id="S4.SS1.p11.9.m9.1.1.cmml" xref="S4.SS1.p11.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.p11.9.m9.1.1.1.cmml" xref="S4.SS1.p11.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.p11.9.m9.1.1.2.cmml" xref="S4.SS1.p11.9.m9.1.1.2">𝑘</ci><apply id="S4.SS1.p11.9.m9.1.1.3.cmml" xref="S4.SS1.p11.9.m9.1.1.3"><times id="S4.SS1.p11.9.m9.1.1.3.1.cmml" xref="S4.SS1.p11.9.m9.1.1.3.1"></times><ci id="S4.SS1.p11.9.m9.1.1.3.2.cmml" xref="S4.SS1.p11.9.m9.1.1.3.2">𝑠</ci><ci id="S4.SS1.p11.9.m9.1.1.3.3.cmml" xref="S4.SS1.p11.9.m9.1.1.3.3">𝑝</ci><ci id="S4.SS1.p11.9.m9.1.1.3.4.cmml" xref="S4.SS1.p11.9.m9.1.1.3.4">𝑎</ci><ci id="S4.SS1.p11.9.m9.1.1.3.5.cmml" xref="S4.SS1.p11.9.m9.1.1.3.5">𝑡</ci><ci id="S4.SS1.p11.9.m9.1.1.3.6.cmml" xref="S4.SS1.p11.9.m9.1.1.3.6">𝑖</ci><ci id="S4.SS1.p11.9.m9.1.1.3.7.cmml" xref="S4.SS1.p11.9.m9.1.1.3.7">𝑎</ci><ci id="S4.SS1.p11.9.m9.1.1.3.8.cmml" xref="S4.SS1.p11.9.m9.1.1.3.8">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p11.9.m9.1c">k_{spatial}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p11.9.m9.1d">italic_k start_POSTSUBSCRIPT italic_s italic_p italic_a italic_t italic_i italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, more distant points consistently contribute lower weights, which makes it more reasonable to use larger upsampling kernels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.3"><span class="ltx_text ltx_font_bold" id="S4.SS1.p12.3.1">Simplify.</span> On the structural side, we simplify the components in FeatUp. In FeatUp, the parameterized JBU modules are stacked 4 times for 16<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p12.1.m1.1"><semantics id="S4.SS1.p12.1.m1.1a"><mo id="S4.SS1.p12.1.m1.1.1" xref="S4.SS1.p12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.1.m1.1b"><times id="S4.SS1.p12.1.m1.1.1.cmml" xref="S4.SS1.p12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.1.m1.1d">×</annotation></semantics></math> upsampling, and the parameters of each JBU module are independent. Although we fed HR features into the <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S4.SS1.p12.2.m2.1"><semantics id="S4.SS1.p12.2.m2.1a"><mi id="S4.SS1.p12.2.m2.1.1" xref="S4.SS1.p12.2.m2.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.2.m2.1b"><ci id="S4.SS1.p12.2.m2.1.1.cmml" xref="S4.SS1.p12.2.m2.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.2.m2.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.2.m2.1d">roman_CRN</annotation></semantics></math> to ensure the integrity of its content, the behavior of each JBU module is indeterminable. Therefore, in SimFeatUp, we change “JBU_Stack” to “JBU_One”, i.e., only one parameterized JBU is used for upsampling. If 16<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p12.3.m3.1"><semantics id="S4.SS1.p12.3.m3.1a"><mo id="S4.SS1.p12.3.m3.1.1" xref="S4.SS1.p12.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p12.3.m3.1b"><times id="S4.SS1.p12.3.m3.1.1.cmml" xref="S4.SS1.p12.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p12.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p12.3.m3.1d">×</annotation></semantics></math> upsampling is required, then it only needs to repeat the execution 4 times. Further, “JBU_One” significantly reduces the number of trainable parameters in the upsampler and provides the possibility of upsampling arbitrary multiples.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Alleviating global bias</h3>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="779" id="S4.F5.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Comparison before and after alleviating the global bias. (a) is the similarity map of patch tokens and cls tokens, some “non-building” regions also present high response, (b) is the original RGB image. Note that the right-hand histograms stretch the raw values for better presentation.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As described in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S3.SS1" title="3.1 CLIP ‣ 3 Preliminaries ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>, in the training phase of CLIP, the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">[CLS]</span> token, which contains the global information of the whole image, is optimized with the text embedding in the multi-modal space via contrastive learning. However, in the inference phase of OVSS, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.2">[CLS]</span> token is generally discarded and only patch tokens are used for similarity computation with the prompt vocabulary. This means that there is a gap between training and inference. Indeed, previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> also demonstrates that: each local visual token in CLIP focuses on a wide range of positions, and the attention maps typically share similar patterns. This suggests that the global attribute is attached to the patch tokens in CLIP. This property is generally not a concern in classification task, but it significantly impairs performance in dense prediction.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The visualization in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.F5" title="In 4.2 Alleviating global bias ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the above elaboration. We extract the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.1">[CLS]</span> token using CLIP for the RGB image in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.F5" title="In 4.2 Alleviating global bias ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>(b), and compute its similarity with the candidate text embeddings. The image is recognized as the building, which is reasonable because the building covers the maximum range in the image. Then, we calculate the similarity of the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.2">[CLS]</span> token with patch tokens as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.F5" title="In 4.2 Alleviating global bias ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>(a). The highly responsive regions are not only the regions with buildings, some roads and pavements are also activated, which indicates that the global bias contaminates the local patch tokens. Motivated by this observation, we propose to “subtract” some global bias from the patch token. This solution is very straightforward and simple, it can be formulated as:</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E9">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E9X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{\mathcal{O}}=\mathcal{O}[1:hw+1]-\lambda\mathcal{O}[0]," class="ltx_math_unparsed" display="inline" id="S4.E9X.2.1.1.m1.1"><semantics id="S4.E9X.2.1.1.m1.1a"><mrow id="S4.E9X.2.1.1.m1.1b"><mover accent="true" id="S4.E9X.2.1.1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E9X.2.1.1.m1.1.1.2">𝒪</mi><mo id="S4.E9X.2.1.1.m1.1.1.1">^</mo></mover><mo id="S4.E9X.2.1.1.m1.1.2">=</mo><mi class="ltx_font_mathcaligraphic" id="S4.E9X.2.1.1.m1.1.3">𝒪</mi><mrow id="S4.E9X.2.1.1.m1.1.4"><mo id="S4.E9X.2.1.1.m1.1.4.1" stretchy="false">[</mo><mn id="S4.E9X.2.1.1.m1.1.4.2">1</mn><mo id="S4.E9X.2.1.1.m1.1.4.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.E9X.2.1.1.m1.1.4.4">h</mi><mi id="S4.E9X.2.1.1.m1.1.4.5">w</mi><mo id="S4.E9X.2.1.1.m1.1.4.6">+</mo><mn id="S4.E9X.2.1.1.m1.1.4.7">1</mn><mo id="S4.E9X.2.1.1.m1.1.4.8" stretchy="false">]</mo></mrow><mo id="S4.E9X.2.1.1.m1.1.5">−</mo><mi id="S4.E9X.2.1.1.m1.1.6">λ</mi><mi class="ltx_font_mathcaligraphic" id="S4.E9X.2.1.1.m1.1.7">𝒪</mi><mrow id="S4.E9X.2.1.1.m1.1.8"><mo id="S4.E9X.2.1.1.m1.1.8.1" stretchy="false">[</mo><mn id="S4.E9X.2.1.1.m1.1.8.2">0</mn><mo id="S4.E9X.2.1.1.m1.1.8.3" stretchy="false">]</mo></mrow><mo id="S4.E9X.2.1.1.m1.1.9">,</mo></mrow><annotation encoding="application/x-tex" id="S4.E9X.2.1.1.m1.1c">\displaystyle\hat{\mathcal{O}}=\mathcal{O}[1:hw+1]-\lambda\mathcal{O}[0],</annotation><annotation encoding="application/x-llamapun" id="S4.E9X.2.1.1.m1.1d">over^ start_ARG caligraphic_O end_ARG = caligraphic_O [ 1 : italic_h italic_w + 1 ] - italic_λ caligraphic_O [ 0 ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(9)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.4">where <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_λ</annotation></semantics></math> denotes a intensity factor. <math alttext="\mathcal{O}[0]" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.2" xref="S4.SS2.p4.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.2.m2.1.2.2" xref="S4.SS2.p4.2.m2.1.2.2.cmml">𝒪</mi><mo id="S4.SS2.p4.2.m2.1.2.1" xref="S4.SS2.p4.2.m2.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p4.2.m2.1.2.3.2" xref="S4.SS2.p4.2.m2.1.2.3.1.cmml"><mo id="S4.SS2.p4.2.m2.1.2.3.2.1" stretchy="false" xref="S4.SS2.p4.2.m2.1.2.3.1.1.cmml">[</mo><mn id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">0</mn><mo id="S4.SS2.p4.2.m2.1.2.3.2.2" stretchy="false" xref="S4.SS2.p4.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.2.cmml" xref="S4.SS2.p4.2.m2.1.2"><times id="S4.SS2.p4.2.m2.1.2.1.cmml" xref="S4.SS2.p4.2.m2.1.2.1"></times><ci id="S4.SS2.p4.2.m2.1.2.2.cmml" xref="S4.SS2.p4.2.m2.1.2.2">𝒪</ci><apply id="S4.SS2.p4.2.m2.1.2.3.1.cmml" xref="S4.SS2.p4.2.m2.1.2.3.2"><csymbol cd="latexml" id="S4.SS2.p4.2.m2.1.2.3.1.1.cmml" xref="S4.SS2.p4.2.m2.1.2.3.2.1">delimited-[]</csymbol><cn id="S4.SS2.p4.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p4.2.m2.1.1">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\mathcal{O}[0]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">caligraphic_O [ 0 ]</annotation></semantics></math> is repeated <math alttext="hw" class="ltx_Math" display="inline" id="S4.SS2.p4.3.m3.1"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">h</mi><mo id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><times id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1"></times><ci id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">ℎ</ci><ci id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">hw</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.3.m3.1d">italic_h italic_w</annotation></semantics></math> times to the same dimension as <math alttext="\mathcal{O}[1:hw+1]" class="ltx_math_unparsed" display="inline" id="S4.SS2.p4.4.m4.1"><semantics id="S4.SS2.p4.4.m4.1a"><mrow id="S4.SS2.p4.4.m4.1b"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p4.4.m4.1.1">𝒪</mi><mrow id="S4.SS2.p4.4.m4.1.2"><mo id="S4.SS2.p4.4.m4.1.2.1" stretchy="false">[</mo><mn id="S4.SS2.p4.4.m4.1.2.2">1</mn><mo id="S4.SS2.p4.4.m4.1.2.3" lspace="0.278em" rspace="0.278em">:</mo><mi id="S4.SS2.p4.4.m4.1.2.4">h</mi><mi id="S4.SS2.p4.4.m4.1.2.5">w</mi><mo id="S4.SS2.p4.4.m4.1.2.6">+</mo><mn id="S4.SS2.p4.4.m4.1.2.7">1</mn><mo id="S4.SS2.p4.4.m4.1.2.8" stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">\mathcal{O}[1:hw+1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.4.m4.1d">caligraphic_O [ 1 : italic_h italic_w + 1 ]</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.5.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S5.T1.6.2" style="font-size:90%;">Open-vocabulary semantic segmentation quantitative comparison on remote sensing datasets. Evaluation metric: mIoU. <span class="ltx_text ltx_font_bold" id="S5.T1.6.2.1" style="color:#D62728;">Best</span> and <span class="ltx_text ltx_font_bold" id="S5.T1.6.2.2" style="color:#1F77B4;">second best</span> performances are highlighted.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:618.2pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.3pt,6.3pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.2">Methods</th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.4">OpenEarthMap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.5">LoveDA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.6">iSAID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.7">Potsdam</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.8">Vaihingen</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1">UAVid<sup class="ltx_sup" id="S5.T1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T1.1.1.1.1.1.1">img</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.9">UDD5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.10">VDD</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.11">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.1.2.1.1">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.1.2.1.2"><span class="ltx_text" id="S5.T1.1.1.2.1.2.1" style="font-size:50%;">ICML’21</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.3">12.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.4">12.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.5">7.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.6">14.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.7">10.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.8">10.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.9">9.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.10">14.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.1.1.2.1.11">11.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.3.2.1">MaskCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.3.2.2"><span class="ltx_text" id="S5.T1.1.1.3.2.2.1" style="font-size:50%;">ECCV’22</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.3">25.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.4">27.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.5">14.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.6">31.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.7">24.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.8">28.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.3.2.9">32.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.10">32.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.1.1.3.2.11">27.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.4.3.1">SCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.4.3.2"><span class="ltx_text" id="S5.T1.1.1.4.3.2.1" style="font-size:50%;">arXiv’23</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.3">29.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.4">30.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.5">16.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.6">36.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.3.7.1" style="color:#1F77B4;">28.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.8">31.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.4.3.9">38.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.10">37.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.1.1.4.3.11">31.1</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.5.4.1">GEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.5.4.2"><span class="ltx_text" id="S5.T1.1.1.5.4.2.1" style="font-size:50%;">CVPR’24</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.3">33.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.4">31.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.5">17.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.6">36.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.7">24.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.8">33.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.5.4.9">41.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.10"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.10.1" style="color:#1F77B4;">39.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.1.1.5.4.11">32.3</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.6.5.1">ClearCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.1.6.5.2"><span class="ltx_text" id="S5.T1.1.1.6.5.2.1" style="font-size:50%;">ECCV’24</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.3.1" style="color:#1F77B4;">31.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.4.1" style="color:#1F77B4;">32.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.5.1" style="color:#1F77B4;">18.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.6.1" style="color:#1F77B4;">40.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.7">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.8.1" style="color:#1F77B4;">36.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.1.6.5.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.9.1" style="color:#1F77B4;">41.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.6.5.10">39.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T1.1.1.6.5.11"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.11.1" style="color:#1F77B4;">33.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.1.1.7.6.1">SegEarth-OV</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.1.1.7.6.2"><span class="ltx_text" id="S5.T1.1.1.7.6.2.1" style="font-size:50%;">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.3.1" style="color:#D62728;">40.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.4.1" style="color:#D62728;">36.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.5.1" style="color:#D62728;">21.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.6.1" style="color:#D62728;">47.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.7.1" style="color:#D62728;">29.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.8.1" style="color:#D62728;">42.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.9.1" style="color:#D62728;">50.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.1.1.7.6.10"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.10.1" style="color:#D62728;">45.3</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T1.1.1.7.6.11"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.7.6.11.1" style="color:#D62728;">39.2</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dataset</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In remote sensing application contexts, not only multi-class semantic segmentation but also extraction of certain land cover types (e.g., buildings, roads, water bodies) is required, e.g., Google’s Open Buildings project<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://sites.research.google/gr/open-buildings/</span></span></span>. Therefore, we select 17 typical datasets covering common semantic segmentation, building extraction, road extraction, and water body segmentation (flood detection) tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Semantic Segmentation.</span> We evaluate SegEarth-OV on 8 remote sensing semantic segmentation datasets including OpenEarthMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>, LoveDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, iSAID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>, Potsdam, Vaihingen<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.isprs.org/education/benchmarks/UrbanSemLab</span></span></span>, UAVid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, UDD5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> and VDD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. Among them, the first 5 datasets consist of mainly satellite images and the last 3 consist of UAV images. They contain custom foreground classes and a background class. Detailed descriptions of these datasets can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS1" title="7.1 Semantic Segmentation ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">7.1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.2.1">Single-class extraction.</span> We select 4 building extraction datasets (i.e., WHU<sup class="ltx_sup" id="S5.SS1.p3.2.2"><span class="ltx_text ltx_font_italic" id="S5.SS1.p3.2.2.1">Aerial</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, WHU<sup class="ltx_sup" id="S5.SS1.p3.2.3"><span class="ltx_text ltx_font_italic" id="S5.SS1.p3.2.3.1">Sat.II</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, Inria <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, and xBD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>), 4 road extraction datasets (i.e., CHN6-CUG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite>, DeepGlobe<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>http://deepglobe.org</span></span></span>, Massachusetts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, and SpaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite>), and 1 flood detection dataset (i.e., WBS-SI<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.kaggle.com/datasets/shirshmall/water-body-segmentation-in-satellite-images</span></span></span>) for the evaluation of single-class extraction. These datasets contain 1 foreground class (building, road or flood) and 1 background class. Detailed descriptions are given in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS2" title="7.2 Building extraction ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">7.2</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.SS4" title="7.4 Flood Detection ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">7.4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Training dataset for SimFeatUp.</span> SimFeatUp requires only image data for training, moreover, to avoid unfair comparisons, we use a public remote sensing image classification dataset, Million-AID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>, which collects images mainly from Google Earth. We randomly selected only 16k of these images to train SimFeatUp.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.4"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.4.1">Implementation.</span> Our implementations are based on MMSegmentation<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/open-mmlab/mmsegmentation</span></span></span> toolkit. For all experiments, we use the original pretrained weights of CLIP (ViT-B/16) provided by OpenAI. For the text part, we use the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.4.2">OpenAI ImageNet template</span> as input for the text encoder, e.g., “a photo of a {class name}”. In addition, since the definition of certain classes may vary in some datasets, we use slight class rename tricks for all methods. For example, we rename “clutter” to “background” and “building” to {“building”, “house”, “roof”}, and the highest probability sub-class in {} will be the probability of that class. Detailed prompt class names for all datasets are listed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S6.T7" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>. For the image part, we resize input images with a long side of 448 and perform slide inference with a 224 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><times id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">×</annotation></semantics></math> 224 window and 112 stride. For SimFeatUp training, we randomly crop 224 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mo id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><times id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">×</annotation></semantics></math> 224 image patches on the original image. We use two 4090 GPUs to train 1 epoch with batch size set to 8. We retain the multi-view consistency constraint in FeatUp, and random flipping, translation and zoom are applied. For the hyper-parameters mentioned, the value of <math alttext="\gamma" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">italic_γ</annotation></semantics></math> is set to 0.1 and <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">italic_λ</annotation></semantics></math> is set to 0.3 for all datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Evaluation.</span> We evaluate the semantic segmentation using the mean intersection over union (mIoU) metric. For single-class extraction, the IoU of the foreground class is used.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Baseline.</span> We take some lessons from natural image OVSS, which are also suitable for remote sensing scenes: we remove the FFN and residual connection of the last Transformer block, insights from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. In addition, the last self-attention is replaced by our modulated attention, i.e., the summation of <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.2">q-q</span>, <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.3">k-k</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.4">v-v</span> as the weights of <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.5">v</span>:</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<table class="ltx_equationgroup ltx_eqn_table" id="S5.E10">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S5.E10X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\operatorname{M-SA}(\boldsymbol{q},\boldsymbol{k},\boldsymbol{v})%
=\sum_{\boldsymbol{i}\in\{\boldsymbol{q},\boldsymbol{k},\boldsymbol{v}\}}\text%
{softmax}(\frac{\boldsymbol{i}\cdot\boldsymbol{i}^{\mathsf{T}}}{\sqrt{d}})%
\cdot\boldsymbol{v}." class="ltx_Math" display="inline" id="S5.E10X.2.1.1.m1.9"><semantics id="S5.E10X.2.1.1.m1.9a"><mrow id="S5.E10X.2.1.1.m1.9.9.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.cmml"><mrow id="S5.E10X.2.1.1.m1.9.9.1.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.cmml"><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.2.2" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml"><mrow id="S5.E10X.2.1.1.m1.4.4" xref="S5.E10X.2.1.1.m1.4.4.cmml"><mi id="S5.E10X.2.1.1.m1.4.4.2" mathvariant="normal" xref="S5.E10X.2.1.1.m1.4.4.2.cmml">M</mi><mo id="S5.E10X.2.1.1.m1.4.4.1" xref="S5.E10X.2.1.1.m1.4.4.1.cmml">−</mo><mi id="S5.E10X.2.1.1.m1.4.4.3" xref="S5.E10X.2.1.1.m1.4.4.3.cmml">SA</mi></mrow><mo id="S5.E10X.2.1.1.m1.9.9.1.1.2.2a" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml">⁡</mo><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.2.2.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml"><mo id="S5.E10X.2.1.1.m1.9.9.1.1.2.2.1.1" stretchy="false" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml">(</mo><mi id="S5.E10X.2.1.1.m1.5.5" xref="S5.E10X.2.1.1.m1.5.5.cmml">𝒒</mi><mo id="S5.E10X.2.1.1.m1.9.9.1.1.2.2.1.2" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml">,</mo><mi id="S5.E10X.2.1.1.m1.6.6" xref="S5.E10X.2.1.1.m1.6.6.cmml">𝒌</mi><mo id="S5.E10X.2.1.1.m1.9.9.1.1.2.2.1.3" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml">,</mo><mi id="S5.E10X.2.1.1.m1.7.7" xref="S5.E10X.2.1.1.m1.7.7.cmml">𝒗</mi><mo id="S5.E10X.2.1.1.m1.9.9.1.1.2.2.1.4" stretchy="false" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml">)</mo></mrow></mrow><mo id="S5.E10X.2.1.1.m1.9.9.1.1.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.1.cmml">=</mo><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.3" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.cmml"><mstyle displaystyle="true" id="S5.E10X.2.1.1.m1.9.9.1.1.3.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1.cmml"><munder id="S5.E10X.2.1.1.m1.9.9.1.1.3.1a" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1.cmml"><mo id="S5.E10X.2.1.1.m1.9.9.1.1.3.1.2" movablelimits="false" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1.2.cmml">∑</mo><mrow id="S5.E10X.2.1.1.m1.3.3.3" xref="S5.E10X.2.1.1.m1.3.3.3.cmml"><mi id="S5.E10X.2.1.1.m1.3.3.3.5" xref="S5.E10X.2.1.1.m1.3.3.3.5.cmml">𝒊</mi><mo id="S5.E10X.2.1.1.m1.3.3.3.4" xref="S5.E10X.2.1.1.m1.3.3.3.4.cmml">∈</mo><mrow id="S5.E10X.2.1.1.m1.3.3.3.6.2" xref="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml"><mo id="S5.E10X.2.1.1.m1.3.3.3.6.2.1" stretchy="false" xref="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml">{</mo><mi id="S5.E10X.2.1.1.m1.1.1.1.1" xref="S5.E10X.2.1.1.m1.1.1.1.1.cmml">𝒒</mi><mo id="S5.E10X.2.1.1.m1.3.3.3.6.2.2" xref="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml">,</mo><mi id="S5.E10X.2.1.1.m1.2.2.2.2" xref="S5.E10X.2.1.1.m1.2.2.2.2.cmml">𝒌</mi><mo id="S5.E10X.2.1.1.m1.3.3.3.6.2.3" xref="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml">,</mo><mi id="S5.E10X.2.1.1.m1.3.3.3.3" xref="S5.E10X.2.1.1.m1.3.3.3.3.cmml">𝒗</mi><mo id="S5.E10X.2.1.1.m1.3.3.3.6.2.4" stretchy="false" xref="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml">}</mo></mrow></mrow></munder></mstyle><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.3.2" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.cmml"><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.cmml"><mtext id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2a.cmml">softmax</mtext><mo id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.1" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.1.cmml">⁢</mo><mrow id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.3.2" xref="S5.E10X.2.1.1.m1.8.8.cmml"><mo id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.3.2.1" stretchy="false" xref="S5.E10X.2.1.1.m1.8.8.cmml">(</mo><mstyle displaystyle="true" id="S5.E10X.2.1.1.m1.8.8" xref="S5.E10X.2.1.1.m1.8.8.cmml"><mfrac id="S5.E10X.2.1.1.m1.8.8a" xref="S5.E10X.2.1.1.m1.8.8.cmml"><mrow id="S5.E10X.2.1.1.m1.8.8.2" xref="S5.E10X.2.1.1.m1.8.8.2.cmml"><mi id="S5.E10X.2.1.1.m1.8.8.2.2" xref="S5.E10X.2.1.1.m1.8.8.2.2.cmml">𝒊</mi><mo id="S5.E10X.2.1.1.m1.8.8.2.1" lspace="0.222em" rspace="0.222em" xref="S5.E10X.2.1.1.m1.8.8.2.1.cmml">⋅</mo><msup id="S5.E10X.2.1.1.m1.8.8.2.3" xref="S5.E10X.2.1.1.m1.8.8.2.3.cmml"><mi id="S5.E10X.2.1.1.m1.8.8.2.3.2" xref="S5.E10X.2.1.1.m1.8.8.2.3.2.cmml">𝒊</mi><mi id="S5.E10X.2.1.1.m1.8.8.2.3.3" xref="S5.E10X.2.1.1.m1.8.8.2.3.3.cmml">𝖳</mi></msup></mrow><msqrt id="S5.E10X.2.1.1.m1.8.8.3" xref="S5.E10X.2.1.1.m1.8.8.3.cmml"><mi id="S5.E10X.2.1.1.m1.8.8.3.2" xref="S5.E10X.2.1.1.m1.8.8.3.2.cmml">d</mi></msqrt></mfrac></mstyle><mo id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.3.2.2" rspace="0.055em" stretchy="false" xref="S5.E10X.2.1.1.m1.8.8.cmml">)</mo></mrow></mrow><mo id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.1" rspace="0.222em" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.1.cmml">⋅</mo><mi id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.3" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.3.cmml">𝒗</mi></mrow></mrow></mrow><mo id="S5.E10X.2.1.1.m1.9.9.1.2" lspace="0em" xref="S5.E10X.2.1.1.m1.9.9.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E10X.2.1.1.m1.9b"><apply id="S5.E10X.2.1.1.m1.9.9.1.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1"><eq id="S5.E10X.2.1.1.m1.9.9.1.1.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.1"></eq><apply id="S5.E10X.2.1.1.m1.9.9.1.1.2.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.2.2"><apply id="S5.E10X.2.1.1.m1.4.4.cmml" xref="S5.E10X.2.1.1.m1.4.4"><minus id="S5.E10X.2.1.1.m1.4.4.1.cmml" xref="S5.E10X.2.1.1.m1.4.4.1"></minus><ci id="S5.E10X.2.1.1.m1.4.4.2.cmml" xref="S5.E10X.2.1.1.m1.4.4.2">M</ci><ci id="S5.E10X.2.1.1.m1.4.4.3.cmml" xref="S5.E10X.2.1.1.m1.4.4.3">SA</ci></apply><ci id="S5.E10X.2.1.1.m1.5.5.cmml" xref="S5.E10X.2.1.1.m1.5.5">𝒒</ci><ci id="S5.E10X.2.1.1.m1.6.6.cmml" xref="S5.E10X.2.1.1.m1.6.6">𝒌</ci><ci id="S5.E10X.2.1.1.m1.7.7.cmml" xref="S5.E10X.2.1.1.m1.7.7">𝒗</ci></apply><apply id="S5.E10X.2.1.1.m1.9.9.1.1.3.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3"><apply id="S5.E10X.2.1.1.m1.9.9.1.1.3.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1"><csymbol cd="ambiguous" id="S5.E10X.2.1.1.m1.9.9.1.1.3.1.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1">subscript</csymbol><sum id="S5.E10X.2.1.1.m1.9.9.1.1.3.1.2.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.1.2"></sum><apply id="S5.E10X.2.1.1.m1.3.3.3.cmml" xref="S5.E10X.2.1.1.m1.3.3.3"><in id="S5.E10X.2.1.1.m1.3.3.3.4.cmml" xref="S5.E10X.2.1.1.m1.3.3.3.4"></in><ci id="S5.E10X.2.1.1.m1.3.3.3.5.cmml" xref="S5.E10X.2.1.1.m1.3.3.3.5">𝒊</ci><set id="S5.E10X.2.1.1.m1.3.3.3.6.1.cmml" xref="S5.E10X.2.1.1.m1.3.3.3.6.2"><ci id="S5.E10X.2.1.1.m1.1.1.1.1.cmml" xref="S5.E10X.2.1.1.m1.1.1.1.1">𝒒</ci><ci id="S5.E10X.2.1.1.m1.2.2.2.2.cmml" xref="S5.E10X.2.1.1.m1.2.2.2.2">𝒌</ci><ci id="S5.E10X.2.1.1.m1.3.3.3.3.cmml" xref="S5.E10X.2.1.1.m1.3.3.3.3">𝒗</ci></set></apply></apply><apply id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2"><ci id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.1">⋅</ci><apply id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2"><times id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.1"></times><ci id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2a.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2"><mtext id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.2">softmax</mtext></ci><apply id="S5.E10X.2.1.1.m1.8.8.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.3.2"><divide id="S5.E10X.2.1.1.m1.8.8.1.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.2.3.2"></divide><apply id="S5.E10X.2.1.1.m1.8.8.2.cmml" xref="S5.E10X.2.1.1.m1.8.8.2"><ci id="S5.E10X.2.1.1.m1.8.8.2.1.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.1">⋅</ci><ci id="S5.E10X.2.1.1.m1.8.8.2.2.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.2">𝒊</ci><apply id="S5.E10X.2.1.1.m1.8.8.2.3.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.3"><csymbol cd="ambiguous" id="S5.E10X.2.1.1.m1.8.8.2.3.1.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.3">superscript</csymbol><ci id="S5.E10X.2.1.1.m1.8.8.2.3.2.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.3.2">𝒊</ci><ci id="S5.E10X.2.1.1.m1.8.8.2.3.3.cmml" xref="S5.E10X.2.1.1.m1.8.8.2.3.3">𝖳</ci></apply></apply><apply id="S5.E10X.2.1.1.m1.8.8.3.cmml" xref="S5.E10X.2.1.1.m1.8.8.3"><root id="S5.E10X.2.1.1.m1.8.8.3a.cmml" xref="S5.E10X.2.1.1.m1.8.8.3"></root><ci id="S5.E10X.2.1.1.m1.8.8.3.2.cmml" xref="S5.E10X.2.1.1.m1.8.8.3.2">𝑑</ci></apply></apply></apply><ci id="S5.E10X.2.1.1.m1.9.9.1.1.3.2.3.cmml" xref="S5.E10X.2.1.1.m1.9.9.1.1.3.2.3">𝒗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E10X.2.1.1.m1.9c">\displaystyle\operatorname{M-SA}(\boldsymbol{q},\boldsymbol{k},\boldsymbol{v})%
=\sum_{\boldsymbol{i}\in\{\boldsymbol{q},\boldsymbol{k},\boldsymbol{v}\}}\text%
{softmax}(\frac{\boldsymbol{i}\cdot\boldsymbol{i}^{\mathsf{T}}}{\sqrt{d}})%
\cdot\boldsymbol{v}.</annotation><annotation encoding="application/x-llamapun" id="S5.E10X.2.1.1.m1.9d">start_OPFUNCTION roman_M - roman_SA end_OPFUNCTION ( bold_italic_q , bold_italic_k , bold_italic_v ) = ∑ start_POSTSUBSCRIPT bold_italic_i ∈ { bold_italic_q , bold_italic_k , bold_italic_v } end_POSTSUBSCRIPT softmax ( divide start_ARG bold_italic_i ⋅ bold_italic_i start_POSTSUPERSCRIPT sansserif_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) ⋅ bold_italic_v .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(10)</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparison to State-of-the-art</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Since the proposed SegEarth-OV is a traning-free method and there is no previous OVSS method designed for remote sensing images, we select 5 state-of-the-art traning-free OVSS models of natural images for comparison, including vanilla CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, MaskCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>, SCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, GEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> and ClearCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Semantic segmentation.</span> As listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T1" title="In 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, SegEarth-OV achieves the best performance on all 8 semantic segmentation datasets. SegEarth-OV achieves more than 40% mIoU on 5 datasets and more than 50% on the UDD5 dataset, which implies that the OVSS method is usable in remote sensing scenarios. Compared to the previous method, SegEarth-OV achieves a performance gain of more than 5% on 5 datasets and an average gain of 5.8% on 8 datasets. On the iSAID dataset, the mIoU of SegEarth-OV is only 21.7%, which is due to the fine-grained category delineation in this dataset, which covers 16 categories (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S6.T7" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S5.F6.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.5.2.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.2.1" style="font-size:90%;">Qualitative comparison between different training-free OVSS methods on OpenEarthMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>, UDD5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> and WHU<sup class="ltx_sup" id="S5.F6.2.1.1"><span class="ltx_text ltx_font_italic" id="S5.F6.2.1.1.1">Aerial</span></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> datasets. (best viewed digitally with zoom, especially for the edges of the object)</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.9.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.10.2" style="font-size:90%;">Open-vocabulary building / road / flood extraction quantitative comparison on remote sensing datasets. Evaluation metric: IoU of the foreground class, i.e. building, road or flood. <span class="ltx_text ltx_font_bold" id="S5.T2.10.2.1" style="color:#D62728;">Best</span> and <span class="ltx_text ltx_font_bold" id="S5.T2.10.2.2" style="color:#1F77B4;">second best</span> performances are highlighted.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.5" style="width:617.4pt;height:168.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.5pt,14.9pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.5.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.5.5.6.1.1" rowspan="2"><span class="ltx_text" id="S5.T2.5.5.6.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="S5.T2.5.5.6.1.2"><span class="ltx_text" id="S5.T2.5.5.6.1.2.1" style="color:#800000;">Building Extraction</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="S5.T2.5.5.6.1.3"><span class="ltx_text" id="S5.T2.5.5.6.1.3.1" style="color:#808000;">Road Extraction</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.5.5.6.1.4"><span class="ltx_text" id="S5.T2.5.5.6.1.4.1" style="color:#000080;">Flood Detection</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.3">
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1">WHU<sup class="ltx_sup" id="S5.T2.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T2.1.1.1.1.1.1">Aerial</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2">WHU<sup class="ltx_sup" id="S5.T2.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T2.2.2.2.2.1.1">Sat.II</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.4">Inria</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.3.3">xBD<sup class="ltx_sup" id="S5.T2.3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S5.T2.3.3.3.3.1.1">pre</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.5">CHN6-CUG</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.6">DeepGlobe</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.7">Massachusetts</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.3.8">SpaceNet</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.3.3.3.9">WBS-SI</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.4.4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.4.4.4.1.1">448 <math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.4.4.4.1.1.m1.1"><semantics id="S5.T2.4.4.4.1.1.m1.1a"><mo id="S5.T2.4.4.4.1.1.m1.1.1" xref="S5.T2.4.4.4.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.1.m1.1b"><times id="S5.T2.4.4.4.1.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.1.1.m1.1d">×</annotation></semantics></math> 448:</span></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.4.4.4.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.7"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.4.4.4.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.4.4.4.9"></td>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S5.T2.4.4.4.10"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.7.2">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.7.2.1">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.2">17.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.3">3.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.4">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.7.2.5">16.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.6">7.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.7">3.9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.8">4.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.7.2.9">7.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.7.2.10">18.6</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.8.3">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.8.3.1">MaskCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.2">29.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.3">14.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.4">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.8.3.5">29.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.6">28.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.7">13.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.8">10.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.8.3.9">20.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.8.3.10">39.8</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.9.4">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.9.4.1">SCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.2">33.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.3">21.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.4">34.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.9.4.5">25.9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.6">21.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.7">7.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.9.4.8">7.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.9.4.9">14.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.9.4.10">32.1</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.10.5">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.10.5.1">GEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.2">24.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.3">13.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.4">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.10.5.5">20.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.6">13.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.7">4.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.10.5.8">5.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.10.5.9">11.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.10.5.10">39.5</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.11.6">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.11.6.1">ClearCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.2">36.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.3"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.11.6.3.1" style="color:#1F77B4;">20.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.4">39.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.11.6.5">30.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.6">25.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.7">5.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.11.6.8">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.11.6.9">16.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.11.6.10">44.9</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.12.7">
<td class="ltx_td ltx_align_left" id="S5.T2.5.5.12.7.1">SegEarth-OV</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.2.1" style="color:#1F77B4;">49.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.3"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.3.1" style="color:#D62728;">28.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.4"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.4.1" style="color:#1F77B4;">44.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.12.7.5"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.5.1" style="color:#1F77B4;">37.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.6"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.6.1" style="color:#D62728;">35.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.7"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.7.1" style="color:#1F77B4;">17.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.12.7.8"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.8.1" style="color:#1F77B4;">11.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.5.5.12.7.9"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.9.1" style="color:#1F77B4;">23.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.5.5.12.7.10"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.12.7.10.1" style="color:#D62728;">60.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.5.5.5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.5.5.5.1.1">896 <math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.5.5.5.1.1.m1.1"><semantics id="S5.T2.5.5.5.1.1.m1.1a"><mo id="S5.T2.5.5.5.1.1.m1.1.1" xref="S5.T2.5.5.5.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.1.1.m1.1b"><times id="S5.T2.5.5.5.1.1.m1.1.1.cmml" xref="S5.T2.5.5.5.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.1.1.m1.1d">×</annotation></semantics></math> 896:</span></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.5.5.5.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.6"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.7"></td>
<td class="ltx_td ltx_border_t" id="S5.T2.5.5.5.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T2.5.5.5.9"></td>
<td class="ltx_td ltx_nopad_r ltx_border_t" id="S5.T2.5.5.5.10"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.13.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.5.5.13.8.1">SegEarth-OV</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.2.1" style="color:#D62728;">49.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.4"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.4.1" style="color:#D62728;">48.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.5.5.13.8.5"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.5.1" style="color:#D62728;">43.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.6">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.7"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.7.1" style="color:#D62728;">20.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.8"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.8.1" style="color:#D62728;">17.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.5.5.13.8.9"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.9.1" style="color:#D62728;">29.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T2.5.5.13.8.10"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.13.8.10.1" style="color:#1F77B4;">57.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Single-class extraction.</span> In the building extraction task, the increase delivered by SegEarth-OV is more significant, as listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T2" title="In 5.3 Comparison to State-of-the-art ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Considering that the “building” class occupies a small area (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.F7" title="In 7.4 Flood Detection ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>), we evaluate the setup for larger scale images, i.e., resizing the long side of the input image to 896 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mo id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><times id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">×</annotation></semantics></math> 896. This setup significantly improves the IoU of Inria and xBD, which on the other hand supports our view that spatial detail preservation is essential for remote sensing OVSS. In the road extraction task, although SegEarth-OV achieves the best IoU, overall, the performance of all methods on the 4 datasets is unsatisfactory, with a best IoU of only 35.4%. There may be two reasons for this phenomenon: (1) The special shape of the road makes it difficult to be extracted in a training-free OVSS manner; (2) The labels of some data are generated based on OpenStreetMap vector shapes with fixed widths attached, which are inherently imprecise. Again, the extraction of roads can generally benefit from larger size images. For the flood detection task, where “water” class features can be easily recognized, the IoU of SegEarth-OV is improved by 15.3% over the previous best method, up to 60.2%. Due to the small size of the original images in the WBS-SI dataset, resizing to a larger size does not result in a positive gain.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Quantitative comparison of vanilla CLIP and remote sensing CLIPs (ViT-B/32). Evaluation metric: mIoU.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.1" style="width:614.1pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.1pt,4.5pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.2">Models</th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.4">OpenEarthMap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.5">LoveDA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.6">iSAID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.7">Potsdam</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.8">Vaihingen</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.1">UAVid<sup class="ltx_sup" id="S5.T3.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.1.1.1.1.1.1">img</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.9">UDD5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.1.1.10">VDD</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.11">Average</th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.2.1.1">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.2.1.2"><span class="ltx_text" id="S5.T3.1.1.2.1.2.1" style="font-size:50%;">ICML’21</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.3">25.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.4">27.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.5">16.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.6">40.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1.7.1" style="color:#D62728;">25.1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1.8.1" style="color:#1F77B4;">31.6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.9"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1.9.1" style="color:#D62728;">39.7</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.10"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1.10.1" style="color:#1F77B4;">39.1</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.2.1.11">30.6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.3.1.1">RemoteCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.1.3.1.2"><span class="ltx_text" id="S5.T3.1.1.3.1.2.1" style="font-size:50%;">TGRS’23</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.3">18.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.1.4.1" style="color:#D62728;">37.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.1.5.1" style="color:#1F77B4;">18.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.6">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.7">22.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.8">16.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.9">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.1.10">28.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.1.1.3.1.11">23.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.4.2.1">GeoRSCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.4.2.2"><span class="ltx_text" id="S5.T3.1.1.4.2.2.1" style="font-size:50%;">TGRS’24</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.3.1" style="color:#D62728;">35.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.4">30.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.5.1" style="color:#D62728;">23.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.6.1" style="color:#1F77B4;">38.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.7">22.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.8.1" style="color:#D62728;">34.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.4.2.9"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.9.1" style="color:#1F77B4;">39.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.4.2.10"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.10.1" style="color:#D62728;">40.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.4.2.11"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.2.11.1" style="color:#D62728;">32.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.1.5.3.1">SkyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.1.5.3.2"><span class="ltx_text" id="S5.T3.1.1.5.3.2.1" style="font-size:50%;">AAAI’24</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.3.1" style="color:#1F77B4;">28.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.4.1" style="color:#1F77B4;">33.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.5">15.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.6.1" style="color:#D62728;">41.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.7.1" style="color:#1F77B4;">24.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.8.1" style="color:#1F77B4;">31.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.9">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.1.5.3.10">35.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T3.1.1.5.3.11"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.3.11.1" style="color:#1F77B4;">31.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Qualitative results.</span> We present qualitative results for MaskCLIP, ClearCLIP, and SegEarth-OV in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.F6" title="In 5.3 Comparison to State-of-the-art ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>. Some observations are summarized as follows: (1) There are some incorrect category predictions in MaskCLIP, e.g., <span class="ltx_text" id="S5.SS3.p4.1.2" style="color:#0045FF;">water</span> on the road and <span class="ltx_text" id="S5.SS3.p4.1.3" style="color:#800000;">bareland</span> on the <span class="ltx_text" id="S5.SS3.p4.1.4" style="color:#4BB549;">cropland</span>. (2) ClearCLIP can generate correct category predictions, but lacks precise localization capability, with distorted target shapes and ill-fitting boundaries of the prediction mask. (3) SegEarth-OV is capable of generating more fine-grained masks that fit the target edges and maintain correct category discrimination. More visualizations can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.F8" title="In 7.4 Flood Detection ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S7.F10" title="Figure 10 ‣ 7.4 Flood Detection ‣ 7 Datasets ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Study and Analysis</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.8"><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.1">Plug and Play.</span> Two key insights of this work, SimFeatUp and global bias alleviation, which can be attached to other OVSS methods as plug-and-play modules. As listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T4" title="In 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, a revealing observation is that on both the OpenEarthMap and WHU<sup class="ltx_sup" id="S5.SS4.p1.8.2"><span class="ltx_text ltx_font_italic" id="S5.SS4.p1.8.2.1">Aerial</span></sup> datasets, as the base capability of the model improves (from MaskCLIP to ClearCLIP), the increases delivered by our method also improve (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><mo id="S5.SS4.p1.2.m2.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><ci id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.3" style="font-size:70%;color:#008000;">3.3</span>, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.3.m3.1"><semantics id="S5.SS4.p1.3.m3.1a"><mo id="S5.SS4.p1.3.m3.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.3.m3.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><ci id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.3.m3.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.4" style="font-size:70%;color:#008000;">5.1</span>, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.4.m4.1"><semantics id="S5.SS4.p1.4.m4.1a"><mo id="S5.SS4.p1.4.m4.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.4.m4.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><ci id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.4.m4.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.5" style="font-size:70%;color:#008000;">8.1</span> on OpenEarthMap, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.5.m5.1"><semantics id="S5.SS4.p1.5.m5.1a"><mo id="S5.SS4.p1.5.m5.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.5.m5.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><ci id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.5.m5.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.6" style="font-size:70%;color:#008000;">5.6</span>, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.6.m6.1"><semantics id="S5.SS4.p1.6.m6.1a"><mo id="S5.SS4.p1.6.m6.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.6.m6.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><ci id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.6.m6.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.7" style="font-size:70%;color:#008000;">6.1</span>, <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS4.p1.7.m7.1"><semantics id="S5.SS4.p1.7.m7.1a"><mo id="S5.SS4.p1.7.m7.1.1" mathcolor="#008000" stretchy="false" xref="S5.SS4.p1.7.m7.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.7.m7.1b"><ci id="S5.SS4.p1.7.m7.1.1.cmml" xref="S5.SS4.p1.7.m7.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.7.m7.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.7.m7.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.8.8" style="font-size:70%;color:#008000;">14.5</span> on WHU<sup class="ltx_sup" id="S5.SS4.p1.8.9"><span class="ltx_text ltx_font_italic" id="S5.SS4.p1.8.9.1">Aerial</span></sup>). This suggests that our method has the potential to improve localization and discrimination for stronger models.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.12.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.13.2" style="font-size:90%;">The proposed method results as a plug-and-play module. “+ ours” indicates using SimFeatUp (<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.SS1" title="4.1 SimFeatUp ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>) and alleviating the global bias (<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.SS2" title="4.2 Alleviating global bias ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>).</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.10" style="width:214.2pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.9pt,6.3pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.10.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.2">Methods</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3">OpenEarthMap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1"><span class="ltx_text" id="S5.T4.1.1.1.1.1" style="color:#800000;">WHU<sup class="ltx_sup" id="S5.T4.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.1.1.1.1.1.1.1">Aerial</span></sup></span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.4"><span class="ltx_text" id="S5.T4.1.1.1.4.1" style="color:#000080;">WBS-SI</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.10.10.11.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.10.10.11.1.1">MaskCLIP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.11.1.2">25.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.11.1.3">29.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.10.10.11.1.4">39.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.4" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.4.4.4"><span class="ltx_text" id="S5.T4.4.4.4.4.1" style="background-color:#E6E6E6;">+ ours</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.2.1"><span class="ltx_text" id="S5.T4.2.2.2.1.1" style="background-color:#E6E6E6;">28.4<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.1.1.m1.1"><semantics id="S5.T4.2.2.2.1.1.m1.1a"><mo id="S5.T4.2.2.2.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.2.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.1.m1.1b"><ci id="S5.T4.2.2.2.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.1.1.1" style="font-size:70%;color:#008000;">3.3</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3.2"><span class="ltx_text" id="S5.T4.3.3.3.2.1" style="background-color:#E6E6E6;">35.4<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.2.1.m1.1"><semantics id="S5.T4.3.3.3.2.1.m1.1a"><mo id="S5.T4.3.3.3.2.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.3.3.3.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.2.1.m1.1b"><ci id="S5.T4.3.3.3.2.1.m1.1.1.cmml" xref="S5.T4.3.3.3.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.2.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.2.1.1" style="font-size:70%;color:#008000;">5.6</span></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.4.4.3"><span class="ltx_text" id="S5.T4.4.4.4.3.1" style="background-color:#E6E6E6;">48.8<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.4.4.4.3.1.m1.1"><semantics id="S5.T4.4.4.4.3.1.m1.1a"><mo id="S5.T4.4.4.4.3.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.4.4.4.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.3.1.m1.1b"><ci id="S5.T4.4.4.4.3.1.m1.1.1.cmml" xref="S5.T4.4.4.4.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.3.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.4.3.1.1" style="font-size:70%;color:#008000;">9.0</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.12.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.10.10.12.2.1">SCLIP</th>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.2">29.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.3">33.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.10.10.12.2.4">32.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.7.7.7" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.7.7.7.4"><span class="ltx_text" id="S5.T4.7.7.7.4.1" style="background-color:#E6E6E6;">+ ours</span></th>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.5.1"><span class="ltx_text" id="S5.T4.5.5.5.1.1" style="background-color:#E6E6E6;">34.4<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.5.5.5.1.1.m1.1"><semantics id="S5.T4.5.5.5.1.1.m1.1a"><mo id="S5.T4.5.5.5.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.5.5.5.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.1.1.m1.1b"><ci id="S5.T4.5.5.5.1.1.m1.1.1.cmml" xref="S5.T4.5.5.5.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.5.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.5.1.1.1" style="font-size:70%;color:#008000;">5.1</span></span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.6.6.6.2"><span class="ltx_text" id="S5.T4.6.6.6.2.1" style="background-color:#E6E6E6;">39.5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.6.6.6.2.1.m1.1"><semantics id="S5.T4.6.6.6.2.1.m1.1a"><mo id="S5.T4.6.6.6.2.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.6.6.6.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.2.1.m1.1b"><ci id="S5.T4.6.6.6.2.1.m1.1.1.cmml" xref="S5.T4.6.6.6.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.6.2.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.6.6.6.2.1.1" style="font-size:70%;color:#008000;">6.1</span></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.7.7.7.3"><span class="ltx_text" id="S5.T4.7.7.7.3.1" style="background-color:#E6E6E6;">53.4<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.7.7.7.3.1.m1.1"><semantics id="S5.T4.7.7.7.3.1.m1.1a"><mo id="S5.T4.7.7.7.3.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.7.7.7.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.7.3.1.m1.1b"><ci id="S5.T4.7.7.7.3.1.m1.1.1.cmml" xref="S5.T4.7.7.7.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.7.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.7.3.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.7.7.7.3.1.1" style="font-size:70%;color:#008000;">21.3</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.13.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.10.10.13.3.1">ClearCLIP</th>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.2">31.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.3">36.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.10.10.13.3.4">44.9</td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.10" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.10.10.10.4"><span class="ltx_text" id="S5.T4.10.10.10.4.1" style="background-color:#E6E6E6;">+ ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.8.8.8.1"><span class="ltx_text" id="S5.T4.8.8.8.1.1" style="background-color:#E6E6E6;">39.1<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.8.8.8.1.1.m1.1"><semantics id="S5.T4.8.8.8.1.1.m1.1a"><mo id="S5.T4.8.8.8.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.8.8.8.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.8.1.1.m1.1b"><ci id="S5.T4.8.8.8.1.1.m1.1.1.cmml" xref="S5.T4.8.8.8.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.8.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.8.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.8.8.8.1.1.1" style="font-size:70%;color:#008000;">8.1</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.9.9.9.2"><span class="ltx_text" id="S5.T4.9.9.9.2.1" style="background-color:#E6E6E6;">51.1<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.9.9.9.2.1.m1.1"><semantics id="S5.T4.9.9.9.2.1.m1.1a"><mo id="S5.T4.9.9.9.2.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.9.9.9.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.9.2.1.m1.1b"><ci id="S5.T4.9.9.9.2.1.m1.1.1.cmml" xref="S5.T4.9.9.9.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.9.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.9.9.9.2.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.9.2.1.1" style="font-size:70%;color:#008000;">14.5</span></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T4.10.10.10.3"><span class="ltx_text" id="S5.T4.10.10.10.3.1" style="background-color:#E6E6E6;">60.4<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.10.10.10.3.1.m1.1"><semantics id="S5.T4.10.10.10.3.1.m1.1a"><mo id="S5.T4.10.10.10.3.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T4.10.10.10.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.10.10.10.3.1.m1.1b"><ci id="S5.T4.10.10.10.3.1.m1.1.1.cmml" xref="S5.T4.10.10.10.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.10.10.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.10.10.10.3.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.10.3.1.1" style="font-size:70%;color:#008000;">15.5</span></span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.11.2.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.2.1" style="font-size:90%;">Detailed ablation results for each component. “X”<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.2.1.m1.1"><semantics id="S5.T5.2.1.m1.1b"><mo id="S5.T5.2.1.m1.1.1" stretchy="false" xref="S5.T5.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.1.m1.1c"><ci id="S5.T5.2.1.m1.1.1.cmml" xref="S5.T5.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.1.m1.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.1.m1.1e">↑</annotation></semantics></math> indicates upsampling earlier stage features, i.e. <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S4.E6" title="In 4.1 SimFeatUp ‣ 4 Method ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">6</span></a>. “+ RS Data” indicates using Million-AID <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> to train the upsampler, before using the images in COCO-Stuff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.9" style="width:249.4pt;height:153pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.0pt,13.5pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.9.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T5.3.1.1.3">OpenEarthMap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T5.3.1.1.1"><span class="ltx_text" id="S5.T5.3.1.1.1.1" style="color:#800000;">WHU<sup class="ltx_sup" id="S5.T5.3.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.3.1.1.1.1.1.1">Sat.II</span></sup></span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.3.1.1.4"><span class="ltx_text" id="S5.T5.3.1.1.4.1" style="color:#000080;">WBS-SI</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.9.7.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.9.7.8.1.1"><span class="ltx_text ltx_font_italic" id="S5.T5.9.7.8.1.1.1">Baseline</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.9.7.8.1.2">32.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.9.7.8.1.3">22.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.9.7.8.1.4">46.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.7.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.9.7.9.2.1">FeatUp (CLIP)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.7.9.2.2">33.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.7.9.2.3">20.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.9.7.9.2.4">39.6</td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.7.10.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.9.7.10.3.1">FeatUp (MaskCLIP)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.7.10.3.2">33.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.7.10.3.3">25.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.9.7.10.3.4">45.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.4.2.2.1">“X”<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.4.2.2.1.m1.1"><semantics id="S5.T5.4.2.2.1.m1.1a"><mo id="S5.T5.4.2.2.1.m1.1.1" stretchy="false" xref="S5.T5.4.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.2.2.1.m1.1b"><ci id="S5.T5.4.2.2.1.m1.1.1.cmml" xref="S5.T5.4.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.2.2.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.4.2.2.2">34.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.4.2.2.3">26.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.4.2.2.4">54.2</td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.5.3.3.2">+ RS Data</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.5.3.3.1">36.0 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.5.3.3.1.m1.1"><semantics id="S5.T5.5.3.3.1.m1.1a"><mo id="S5.T5.5.3.3.1.m1.1.1" mathcolor="#008000" stretchy="false" xref="S5.T5.5.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.3.3.1.m1.1b"><ci id="S5.T5.5.3.3.1.m1.1.1.cmml" xref="S5.T5.5.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.3.3.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T5.5.3.3.1.1" style="font-size:70%;color:#008000;">1.4</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.5.3.3.3">26.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.5.3.3.4">56.4</td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.6.4.4.2">+ JBU_One</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.6.4.4.1">36.3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.6.4.4.1.m1.1"><semantics id="S5.T5.6.4.4.1.m1.1a"><mo id="S5.T5.6.4.4.1.m1.1.1" mathcolor="#008000" stretchy="false" xref="S5.T5.6.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.4.4.1.m1.1b"><ci id="S5.T5.6.4.4.1.m1.1.1.cmml" xref="S5.T5.6.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.4.4.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T5.6.4.4.1.1" style="font-size:70%;color:#008000;">0.3</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.6.4.4.3">26.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.6.4.4.4">57.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.7.5.5.2">+ Rec. Image</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.7.5.5.1">37.6 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.7.5.5.1.m1.1"><semantics id="S5.T5.7.5.5.1.m1.1a"><mo id="S5.T5.7.5.5.1.m1.1.1" mathcolor="#008000" stretchy="false" xref="S5.T5.7.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.5.5.1.m1.1b"><ci id="S5.T5.7.5.5.1.m1.1.1.cmml" xref="S5.T5.7.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.5.5.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T5.7.5.5.1.1" style="font-size:70%;color:#008000;">1.3</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.7.5.5.3">26.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.7.5.5.4">58.7</td>
</tr>
<tr class="ltx_tr" id="S5.T5.8.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.8.6.6.2">+ Alleviate Global Bias</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.8.6.6.1">39.3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.8.6.6.1.m1.1"><semantics id="S5.T5.8.6.6.1.m1.1a"><mo id="S5.T5.8.6.6.1.m1.1.1" mathcolor="#008000" stretchy="false" xref="S5.T5.8.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.8.6.6.1.m1.1b"><ci id="S5.T5.8.6.6.1.m1.1.1.cmml" xref="S5.T5.8.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.6.6.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T5.8.6.6.1.1" style="font-size:70%;color:#008000;">1.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.8.6.6.3">27.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.8.6.6.4">59.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.9.7.7.2">+ Large Kernel</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.9.7.7.1">40.3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.9.7.7.1.m1.1"><semantics id="S5.T5.9.7.7.1.m1.1a"><mo id="S5.T5.9.7.7.1.m1.1.1" mathcolor="#008000" stretchy="false" xref="S5.T5.9.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.9.7.7.1.m1.1b"><ci id="S5.T5.9.7.7.1.m1.1.1.cmml" xref="S5.T5.9.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.7.7.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T5.9.7.7.1.1" style="font-size:70%;color:#008000;">1.0</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.9.7.7.3">28.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T5.9.7.7.4">60.2</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.8"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.8.1">Ablation Study.</span> To assess each of the proposed components, we perform a detailed ablation analysis, as listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T5" title="In 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. FeatUp (CLIP) denotes the original FeatUp upsampler, which provides a 1.5% improvement on OpenEarthMap but decreases the performance on WHU<sup class="ltx_sup" id="S5.SS4.p2.8.2"><span class="ltx_text ltx_font_italic" id="S5.SS4.p2.8.2.1">Sat.II</span></sup> and WBS-SI (more comparisons between FeatUp and the proposed method are shown in the bottom-right of <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S0.F1" title="In SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). FeatUp (MaskCLIP) denotes using <math alttext="\boldsymbol{v}" class="ltx_Math" display="inline" id="S5.SS4.p2.2.m2.1"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">\boldsymbol{v}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.2.m2.1d">bold_italic_v</annotation></semantics></math> in self-attention as the upsampled feature, which somewhat mitigates the possible negative effects of FeatUp (CLIP). In SimFeatUp, the input feature <math alttext="X" class="ltx_Math" display="inline" id="S5.SS4.p2.3.m3.1"><semantics id="S5.SS4.p2.3.m3.1a"><mi id="S5.SS4.p2.3.m3.1.1" xref="S5.SS4.p2.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m3.1b"><ci id="S5.SS4.p2.3.m3.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.3.m3.1d">italic_X</annotation></semantics></math> of the last block is used to upsample, which presents a significant improvement in all 3 datasets. A substantial improvement is also delivered after replacing the training material for the upsampler from natural images to remote sensing images. “JBU_One” reduces the parameters by nearly <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.SS4.p2.4.m4.1"><semantics id="S5.SS4.p2.4.m4.1a"><mrow id="S5.SS4.p2.4.m4.1b"><mn id="S5.SS4.p2.4.m4.1.1">4</mn><mo id="S5.SS4.p2.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS4.p2.4.m4.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.4.m4.1d">4 ×</annotation></semantics></math> while delivering a slight IoU gain (only <math alttext="&lt;" class="ltx_Math" display="inline" id="S5.SS4.p2.5.m5.1"><semantics id="S5.SS4.p2.5.m5.1a"><mo id="S5.SS4.p2.5.m5.1.1" xref="S5.SS4.p2.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.5.m5.1b"><lt id="S5.SS4.p2.5.m5.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.5.m5.1d">&lt;</annotation></semantics></math> 0.3M parameters). The introduction of <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S5.SS4.p2.6.m6.1"><semantics id="S5.SS4.p2.6.m6.1a"><mi id="S5.SS4.p2.6.m6.1.1" xref="S5.SS4.p2.6.m6.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.6.m6.1b"><ci id="S5.SS4.p2.6.m6.1.1.cmml" xref="S5.SS4.p2.6.m6.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.6.m6.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.6.m6.1d">roman_CRN</annotation></semantics></math> with image reconstruction loss brings 1.7%, 0.4%, and 1.6% improvement on 3 datasets, respectively. Note that the <math alttext="\operatorname{CRN}" class="ltx_Math" display="inline" id="S5.SS4.p2.7.m7.1"><semantics id="S5.SS4.p2.7.m7.1a"><mi id="S5.SS4.p2.7.m7.1.1" xref="S5.SS4.p2.7.m7.1.1.cmml">CRN</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.7.m7.1b"><ci id="S5.SS4.p2.7.m7.1.1.cmml" xref="S5.SS4.p2.7.m7.1.1">CRN</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.7.m7.1c">\operatorname{CRN}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.7.m7.1d">roman_CRN</annotation></semantics></math> only participates in SimFeatUp’s training and is discarded during inference. Global bias alleviation shows significant improvement for all 3 datasets, with an average 1.3% improvement. Finally, expanding the upsampling kernel to <math alttext="11\times 11" class="ltx_Math" display="inline" id="S5.SS4.p2.8.m8.1"><semantics id="S5.SS4.p2.8.m8.1a"><mrow id="S5.SS4.p2.8.m8.1.1" xref="S5.SS4.p2.8.m8.1.1.cmml"><mn id="S5.SS4.p2.8.m8.1.1.2" xref="S5.SS4.p2.8.m8.1.1.2.cmml">11</mn><mo id="S5.SS4.p2.8.m8.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p2.8.m8.1.1.1.cmml">×</mo><mn id="S5.SS4.p2.8.m8.1.1.3" xref="S5.SS4.p2.8.m8.1.1.3.cmml">11</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.8.m8.1b"><apply id="S5.SS4.p2.8.m8.1.1.cmml" xref="S5.SS4.p2.8.m8.1.1"><times id="S5.SS4.p2.8.m8.1.1.1.cmml" xref="S5.SS4.p2.8.m8.1.1.1"></times><cn id="S5.SS4.p2.8.m8.1.1.2.cmml" type="integer" xref="S5.SS4.p2.8.m8.1.1.2">11</cn><cn id="S5.SS4.p2.8.m8.1.1.3.cmml" type="integer" xref="S5.SS4.p2.8.m8.1.1.3">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.8.m8.1c">11\times 11</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.8.m8.1d">11 × 11</annotation></semantics></math> also exhibits consistent improvement across all datasets.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.5.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.6.2" style="font-size:90%;">OVSS quantitative comparison on natural image datasets. The basic results are cited from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.3" style="width:613.3pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.7pt,16.2pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T6.3.3.4.1.1">Methods</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.3.3.4.1.2">Context59 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.3.3.4.1.3">Stuff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.3.4.1.4">Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T6.3.3.4.1.5">Average</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.3.3.5.2.1">TCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.3.5.2.2">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.3.5.2.3">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.3.5.2.4">23.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.3.5.2.5">24.3</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.3.6.3.1">Reco <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.6.3.2">22.3</td>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.6.3.3">14.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.6.3.4">21.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.3.3.6.3.5">19.4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.3.3.7.4.1">MaskCLIP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.3.7.4.2">26.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.3.7.4.3">16.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.3.7.4.4">12.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.3.7.4.5">18.5</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.1" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.1.1.1.2"><span class="ltx_text" id="S5.T6.1.1.1.2.1" style="background-color:#E6E6E6;">+ SimFeatUp</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.1.3"><span class="ltx_text" id="S5.T6.1.1.1.3.1" style="background-color:#E6E6E6;">28.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.1.4"><span class="ltx_text" id="S5.T6.1.1.1.4.1" style="background-color:#E6E6E6;">18.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.1.1.5"><span class="ltx_text" id="S5.T6.1.1.1.5.1" style="background-color:#E6E6E6;">25.8</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.1.1.1"><span class="ltx_text" id="S5.T6.1.1.1.1.1" style="background-color:#E6E6E6;">24.2 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T6.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.1.1" style="font-size:70%;color:#008000;">5.7</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.3.8.5.1">SCLIP</th>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.8.5.2">33.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.8.5.3">21.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.8.5.4">29.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.3.3.8.5.5">27.7</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.2" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.2.2.2"><span class="ltx_text" id="S5.T6.2.2.2.2.1" style="background-color:#E6E6E6;">+ SimFeatUp</span></th>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.2.3"><span class="ltx_text" id="S5.T6.2.2.2.3.1" style="background-color:#E6E6E6;">34.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.2.4"><span class="ltx_text" id="S5.T6.2.2.2.4.1" style="background-color:#E6E6E6;">22.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.2.5"><span class="ltx_text" id="S5.T6.2.2.2.5.1" style="background-color:#E6E6E6;">30.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.2.2.2.1"><span class="ltx_text" id="S5.T6.2.2.2.1.1" style="background-color:#E6E6E6;">28.9 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.2.2.2.1.1.m1.1"><semantics id="S5.T6.2.2.2.1.1.m1.1a"><mo id="S5.T6.2.2.2.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T6.2.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.1.1.m1.1b"><ci id="S5.T6.2.2.2.1.1.m1.1.1.cmml" xref="S5.T6.2.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.2.1.1.1" style="font-size:70%;color:#008000;">1.2</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.3.9.6.1">ClearCLIP</th>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.9.6.2">35.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.3.3.9.6.3">23.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.9.6.4">30.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.3.3.9.6.5">29.9</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.3" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.3.3.3.2"><span class="ltx_text" id="S5.T6.3.3.3.2.1" style="background-color:#E6E6E6;">+ SimFeatUp</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.3.3.3"><span class="ltx_text" id="S5.T6.3.3.3.3.1" style="background-color:#E6E6E6;">37.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.3.3.4"><span class="ltx_text" id="S5.T6.3.3.3.4.1" style="background-color:#E6E6E6;">25.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.3.3.3.5"><span class="ltx_text" id="S5.T6.3.3.3.5.1" style="background-color:#E6E6E6;">30.7</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T6.3.3.3.1"><span class="ltx_text" id="S5.T6.3.3.3.1.1" style="background-color:#E6E6E6;">31.1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.3.3.3.1.1.m1.1"><semantics id="S5.T6.3.3.3.1.1.m1.1a"><mo id="S5.T6.3.3.3.1.1.m1.1.1" mathbackground="#E6E6E6" mathcolor="#008000" stretchy="false" xref="S5.T6.3.3.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.3.1.1.m1.1b"><ci id="S5.T6.3.3.3.1.1.m1.1.1.cmml" xref="S5.T6.3.3.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.3.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.3.3.3.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.3.1.1.1" style="font-size:70%;color:#008000;">1.2</span></span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Results on natural images.</span> We evaluate SimFeatUp as an external unit on 3 natural image datasets: PASCAL Context59 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>, COCOStuff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> and Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. As listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T6" title="In 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, after upsampling the visual features of MaskCLIP, SCLIP, and ClearCLIP using SimFeatUp, their mIoUs are improved by 5.7%, 1.2%, and 1.2%, respectively. This reveals the potential of our method to inspire general vision.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p4.1.1">Remote sensing CLIPs for OVSS.</span> We evaluate the performance of remote sensing CLIPs on OVSS, including RemoteCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, GeoRSCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, and SkyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>, which are trained on 0.8M, 5M, and 5.2M remote sensing data, respectively, without changing the model structure of CLIP. Since these works do not provide the ViT-B/16, we uniformly use ViT-B/32. Hence, we repeat the JBU operation 5 times in SimFeatUp. For fair comparison, we train the respective SimFeatUp for each model. As listed in <a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#S5.T3" title="In 5.3 Comparison to State-of-the-art ‣ 5 Experiments ‣ SegEarth-OV: Towards Traning-Free Open-Vocabulary Segmentation for Remote Sensing Images"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, RemoteCLIP performs suboptimally to vanilla CLIP, which indicates that a small amount of domain data diminishes the model’s transfer capability. GeoRSCLIP achieves the best performance against SkyCLIP, one possible reason is that the images and descriptions used in GeoRSCLIP are more diverse. Moreover, the OVSS task effectively reflects the model’s discrimination and localization capabilities, and can serve as an evaluation metric for remote sensing VLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we present SegEarth-OV, a training-free OVSS method for remote sensing images. The design of SegEarth-OV was motivated by the observation that OVSS methods currently used for natural images do not perform well on remote sensing images. The two key insights of SegEarth-OV, i.e., SimFeatUp and global bias alleviation, exhibit consistent improvements on 17 remote sensing datasets spanning the tasks of semantic segmentation, building extraction, road extraction, and flood detection, well beyond the previous state-of-the-art methods. More importantly, as the first exploration of training-free OVSS method in remote sensing scenarios, this work demonstrates that the OVSS solution is feasible in earth perception tasks even if the VLMs are pre-trained on natural images. We expect that this work will inspire more OVSS methods and more capable remote sensing VLMs, and open up new possibilities for the Earth vision community.


<span class="ltx_text" id="S6.p1.1.1" style="font-size:90%;"></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.4.4.1" style="font-size:90%;">Alain [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.6.1" style="font-size:90%;">
Guillaume Alain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">Understanding intermediate layers using linear classifier probes.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.8.1" style="font-size:90%;">arXiv preprint arXiv:1610.01644</em><span class="ltx_text" id="bib.bib1.9.2" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Barsellotti et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Training-free open-vocabulary segmentation with offline diffusion-augmented prototype generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib2.11.3" style="font-size:90%;">, pages 3689–3698, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Bousselham et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Walid Bousselham, Felix Petersen, Vittorio Ferrari, and Hilde Kuehne.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Grounding everything: Emerging localization properties in vision-language transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib3.11.3" style="font-size:90%;">, pages 3828–3837, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Caesar et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Coco-stuff: Thing and stuff classes in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib4.11.3" style="font-size:90%;">, pages 1209–1218, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Cai et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Wenxiao Cai, Ke Jin, Jinyan Hou, Cong Guo, Letian Wu, and Wankou Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Vdd: Varied drone dataset for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.13608</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Caron et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, pages 9650–9660, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Cha et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Junbum Cha, Jonghwan Mun, and Byungseok Roh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib7.11.3" style="font-size:90%;">, pages 11165–11174, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Chen et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Yu Chen, Yao Wang, Peng Lu, Yisong Chen, and Guoping Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Large-scale structure from motion with semantic constraints of aerial images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 347–359. Springer, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Cheng et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Bowen Cheng, Alex Schwing, and Alexander Kirillov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Per-pixel classification is not all you need for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 34:17864–17875, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Cherti et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Reproducible scaling laws for contrastive language-image learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib10.11.3" style="font-size:90%;">, pages 2818–2829, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Cho et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Cat-seg: Cost aggregation for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 4113–4123, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Cordts et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib12.11.3" style="font-size:90%;">, pages 3213–3223, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Ding et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Decoupling zero-shot semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 11583–11592, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Fan et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Improving clip training with language rewrites.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Fu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Featup: A model-agnostic framework for features at any resolution.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Ghiasi et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Scaling open-vocabulary image segmentation with image-level labels.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib16.11.3" style="font-size:90%;">, pages 540–557. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Guo et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Segnext: Rethinking convolutional attention design for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 35:1140–1156, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Gupta et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, and Matthew Gaston.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">xbd: A dataset for assessing building damage from satellite imagery, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.4.4.1" style="font-size:90%;">Haklay and Weber [2008]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.6.1" style="font-size:90%;">
Mordechai Haklay and Patrick Weber.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">Openstreetmap: User-generated street maps.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.8.1" style="font-size:90%;">IEEE Pervasive computing</em><span class="ltx_text" id="bib.bib19.9.2" style="font-size:90%;">, 7(4):12–18, 2008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Herfort et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Benjamin Herfort, Sven Lautenbach, João Porto de Albuquerque, Jennings Anderson, and Alexander Zipf.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">A spatio-temporal analysis investigating completeness and inequalities of global urban building data in openstreetmap.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">Nature Communications</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 14(1):3985, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Hong et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Naoto Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Spectralgpt: Spectral remote sensing foundation model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Ji et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Shunping Ji, Shiqing Wei, and Meng Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">IEEE Transactions on geoscience and remote sensing</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, 57(1):574–586, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.4.4.1" style="font-size:90%;">Kang and Cho [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.6.1" style="font-size:90%;">
Dahyun Kang and Minsu Cho.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">In defense of lazy visual grounding for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.2" style="font-size:90%;">European Conference on Computer Vision and Pattern Recognition (ECCV)</em><span class="ltx_text" id="bib.bib23.10.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.4.4.1" style="font-size:90%;">Kingma [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.6.1" style="font-size:90%;">
Diederik P Kingma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">Auto-encoding variational bayes.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.8.1" style="font-size:90%;">arXiv preprint arXiv:1312.6114</em><span class="ltx_text" id="bib.bib24.9.2" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Kirillov et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Panoptic feature pyramid networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib25.11.3" style="font-size:90%;">, pages 6399–6408, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Kirillov et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib26.11.3" style="font-size:90%;">, pages 4015–4026, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Kopf et al. [2007]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Johannes Kopf, Michael F Cohen, Dani Lischinski, and Matt Uyttendaele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Joint bilateral upsampling.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">ACM Transactions on Graphics (ToG)</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 26(3):96–es, 2007.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Lan et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Clearclip: Decomposing clip representations for dense vision-language inference.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.9.1" style="font-size:90%;">arXiv preprint arXiv:2407.12442</em><span class="ltx_text" id="bib.bib28.10.2" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Lan et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Proxyclip: Proxy attention improves clip for open-vocabulary segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.1" style="font-size:90%;">arXiv preprint arXiv:2408.04883</em><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Li et al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and René Ranftl.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Language-driven semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.1" style="font-size:90%;">arXiv preprint arXiv:2201.03546</em><span class="ltx_text" id="bib.bib30.10.2" style="font-size:90%;">, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Li et al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib31.11.3" style="font-size:90%;">, pages 12888–12900. PMLR, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib32.11.3" style="font-size:90%;">, pages 19730–19742. PMLR, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Scaling language-image pre-training via masking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib33.11.3" style="font-size:90%;">, pages 23390–23400, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Li et al. [2023c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Clip surgery for better explainability with enhancement in open-vocabulary tasks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.05653</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Lin et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, pages 740–755. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Liu et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Remoteclip: A vision language foundation model for remote sensing.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.1" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</em><span class="ltx_text" id="bib.bib36.10.2" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Learning to upsample by learning to sample.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib37.11.3" style="font-size:90%;">, pages 6027–6037, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Liu et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, and Yansong Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Open-vocabulary segmentation with semantic-assisted calibration.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib38.11.3" style="font-size:90%;">, pages 3491–3500, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Long et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">On creating benchmark dataset for aerial image interpretation: Reviews, guidances and million-aid.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 14:4205–4230, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Lu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">Sapa: Similarity-aware point affiliation for feature upsampling.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib40.10.2" style="font-size:90%;">, 35:20889–20901, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Lu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Xiaoyan Lu, Yanfei Zhong, Zhuo Zheng, JunJue Wang, Dingyuan Chen, and Yu Su.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Global road extraction using a pseudo-label guided framework: from benchmark dataset to cross-region semi-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">Geo-spatial Information Science</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, pages 1–19, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Luo et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, pages 23033–23044. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Lyu et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Ye Lyu, George Vosselman, Gui-Song Xia, Alper Yilmaz, and Michael Ying Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Uavid: A semantic segmentation dataset for uav imagery.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.9.1" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</em><span class="ltx_text" id="bib.bib43.10.2" style="font-size:90%;">, 165:108 – 119, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Maggiori et al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, and Pierre Alliez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.10.2" style="font-size:90%;">2017 IEEE International geoscience and remote sensing symposium (IGARSS)</em><span class="ltx_text" id="bib.bib44.11.3" style="font-size:90%;">, pages 3226–3229. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Mildenhall et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Nerf: Representing scenes as neural radiance fields for view synthesis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.9.1" style="font-size:90%;">Communications of the ACM</em><span class="ltx_text" id="bib.bib45.10.2" style="font-size:90%;">, 65(1):99–106, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.4.4.1" style="font-size:90%;">Mnih [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.6.1" style="font-size:90%;">
Volodymyr Mnih.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.7.1" style="font-size:90%;">Machine Learning for Aerial Image Labeling</em><span class="ltx_text" id="bib.bib46.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">PhD thesis, University of Toronto, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Mottaghi et al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">The role of context for object detection and semantic segmentation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib47.10.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib47.11.3" style="font-size:90%;">, pages 891–898, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Mukhoti et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and Ser-Nam Lim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Open vocabulary semantic segmentation with patch aligned contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib48.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib48.11.3" style="font-size:90%;">, pages 19413–19423, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Pang et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Chao Pang, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Xingxing Weng, Shuai Wang, Litong Feng, Gui-Song Xia, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">H2rsvlm: Towards helpful and honest remote sensing large vision language model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.20213</em><span class="ltx_text" id="bib.bib49.10.2" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Pang et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Li Pang, Datao Tang, Shuang Xu, Deyu Meng, and Xiangyong Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Hsigene: A foundation model for hyperspectral image generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.9.1" style="font-size:90%;">arXiv preprint arXiv:2409.12470</em><span class="ltx_text" id="bib.bib50.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib51.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib51.11.3" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Ranasinghe et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Perceptual grouping in contrastive vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib52.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib52.11.3" style="font-size:90%;">, pages 5571–5584, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Rolf et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Esther Rolf, Konstantin Klemmer, Caleb Robinson, and Hannah Kerner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Mission critical–satellite data is a distinct modality in machine learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.01444</em><span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Rombach et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Ronneberger et al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib55.10.2" style="font-size:90%;">International Conference on Medical image computing and computer-assisted intervention</em><span class="ltx_text" id="bib.bib55.11.3" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Shao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Tong Shao, Zhuotao Tian, Hang Zhao, and Jingyong Su.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Explore the potential of clip for training-free open vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib56.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib56.11.3" style="font-size:90%;">. Springer, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Shin et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Gyungin Shin, Weidi Xie, and Samuel Albanie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">Reco: Retrieve and co-segment for zero-shot transfer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">, 35:33754–33767, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Singh et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, and Stefan Roth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Is synthetic data all we need? benchmarking the robustness of models trained with synthetic images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib58.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib58.11.3" style="font-size:90%;">, pages 2505–2515, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Singh et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Suriya Singh, Anil Batra, Guansong PANG, Lorenzo Torresani, Saikat Basu, Manohar Paluri, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Self-supervised feature learning for semantic segmentation of overhead imagery.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">Sun et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, and Siyang Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">Clip as rnn: Segment countless visual concepts without training endeavor.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib60.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib60.11.3" style="font-size:90%;">, pages 13171–13182, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Toker et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Aysim Toker, Marvin Eisenberger, Daniel Cremers, and Laura Leal-Taixé.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Satsynth: Augmenting image-mask pairs through diffusion models for aerial semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib61.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib61.11.3" style="font-size:90%;">, pages 27695–27705, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.5.5.1" style="font-size:90%;">Van Etten et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">
Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">Spacenet: A remote sensing dataset and challenge series.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.9.1" style="font-size:90%;">arXiv preprint arXiv:1807.01232</em><span class="ltx_text" id="bib.bib62.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Wang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Feng Wang, Jieru Mei, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Sclip: Rethinking self-attention for dense vision-language inference.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.01597</em><span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">Loveda: A remote sensing land-cover dataset for domain adaptive semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib64.10.2" style="font-size:90%;">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em><span class="ltx_text" id="bib.bib64.11.3" style="font-size:90%;">. Curran Associates, Inc., 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Wang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, and Dong Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Diffusion model is secretly a training-free open vocabulary semantic segmenter.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.02773</em><span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Wang et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Zhecheng Wang, Rajanie Prabha, Tianyuan Huang, Jiajun Wu, and Ram Rajagopal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Skyscript: A large and semantically diverse vision-language dataset for remote sensing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, pages 5805–5813, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Waqas Zamir et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">isaid: A large-scale dataset for instance segmentation in aerial images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib67.10.2" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</em><span class="ltx_text" id="bib.bib67.11.3" style="font-size:90%;">, pages 28–37, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Wu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Towards open vocabulary learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib68.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Xia et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Dota: A large-scale dataset for object detection in aerial images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.2" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib69.11.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Xia et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
Junshi Xia, Naoto Yokoya, Bruno Adriano, and Clifford Broni-Bediako.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Openearthmap: A benchmark dataset for global high-resolution land cover mapping.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib70.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em><span class="ltx_text" id="bib.bib70.11.3" style="font-size:90%;">, pages 6254–6264, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="font-size:90%;">Xiao et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Unified perceptual parsing for scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib71.10.2" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib71.11.3" style="font-size:90%;">, pages 418–434, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Xu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Demystifying clip data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib72.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib72.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Xu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">San: Side adapter network for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib73.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Xu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Side adapter network for open-vocabulary semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib74.11.3" style="font-size:90%;">, pages 2945–2954, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Yang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Alip: Adaptive language-image pre-training with synthetic caption.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib75.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib75.11.3" style="font-size:90%;">, pages 2922–2931, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Zanaga et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
D. Zanaga, R. Van De Kerchove, D. Daems, W. De Keersmaecker, C. Brockmann, G. Kirches, J. Wevers, O. Cartus, M. Santoro, S. Fritz, M. Lesiv, M. Herold, NE Tsendbazar, P. Xu, F. Ramoino, and O. Arino.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Esa worldcover 10 m 2021 v200, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Rs5m: A large scale vision-language dataset for remote sensing vision-language foundation model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.11300</em><span class="ltx_text" id="bib.bib77.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Zheng et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma, and Liangpei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Farseg++: Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.9.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib78.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Zhou et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
Chong Zhou, Chen Change Loy, and Bo Dai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">Extract free dense labels from clip.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib79.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib79.11.3" style="font-size:90%;">, pages 696–712. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Zhou et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Minghao Zhou, Hong Wang, Yefeng Zheng, and Deyu Meng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">A refreshed similarity-based upsampler for direct high-ratio feature upsampling.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.9.1" style="font-size:90%;">arXiv preprint arXiv:2407.02283</em><span class="ltx_text" id="bib.bib80.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.4.4.1" style="font-size:90%;">Zhu and Chen [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.6.1" style="font-size:90%;">
Chaoyang Zhu and Long Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">A survey on open-vocabulary detection and segmentation: Past, present, and future.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.8.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib81.9.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Zhu et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
Qiqi Zhu, Yanan Zhang, Lizeng Wang, Yanfei Zhong, Qingfeng Guan, Xiaoyan Lu, Liangpei Zhang, and Deren Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">A global context-aware and batch-independent network for road extraction from vhr satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.9.1" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</em><span class="ltx_text" id="bib.bib82.10.2" style="font-size:90%;">, 175:353–365, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\thetitle</span>
<br class="ltx_break"/>
<p class="ltx_p" id="p2.2"><span class="ltx_text" id="p2.2.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<figure class="ltx_table ltx_align_center" id="S6.T7">
<figcaption class="ltx_caption" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T7.6.1.1" style="font-size:63%;">Table 7</span>: </span><span class="ltx_text" id="S6.T7.7.2" style="font-size:63%;">The prompt class name of the evaluation datasets. {} indicates multiple prompt vocabularies for one class.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S6.T7.2" style="width:126.1pt;height:7954.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.0pt,441.9pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T7.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T7.2.2.3.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S6.T7.2.2.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.3.1.1.1">
<span class="ltx_p" id="S6.T7.2.2.3.1.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.3.1.1.1.1.1" style="font-size:144%;">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S6.T7.2.2.3.1.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.3.1.2.1">
<span class="ltx_p" id="S6.T7.2.2.3.1.2.1.1"><span class="ltx_text" id="S6.T7.2.2.3.1.2.1.1.1" style="font-size:144%;">Class Name</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.2.2.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T7.2.2.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.4.1.1.1">
<span class="ltx_p" id="S6.T7.2.2.4.1.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.4.1.1.1.1.1" style="font-size:144%;">OpenEarthMap</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_t" id="S6.T7.2.2.4.1.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.4.1.2.1">
<span class="ltx_p" id="S6.T7.2.2.4.1.2.1.1"><span class="ltx_text" id="S6.T7.2.2.4.1.2.1.1.1" style="font-size:144%;">background, {bareland, barren},
grass, pavement, road, {tree, forest}, {water, river}, cropland, {building, roof, house}</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.5.2.1.1">
<span class="ltx_p" id="S6.T7.2.2.5.2.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.5.2.1.1.1.1" style="font-size:144%;">LoveDA</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.5.2.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.5.2.2.1">
<span class="ltx_p" id="S6.T7.2.2.5.2.2.1.1"><span class="ltx_text" id="S6.T7.2.2.5.2.2.1.1.1" style="font-size:144%;">background, {building, roof, house}, road, water, barren, forest, agricultural</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.6.3.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.6.3.1.1">
<span class="ltx_p" id="S6.T7.2.2.6.3.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.6.3.1.1.1.1" style="font-size:144%;">iSAID</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.6.3.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.6.3.2.1">
<span class="ltx_p" id="S6.T7.2.2.6.3.2.1.1"><span class="ltx_text" id="S6.T7.2.2.6.3.2.1.1.1" style="font-size:144%;">background, ship, store tank, baseball diamond, tennis court, basketball court, ground track field, bridge, large vehicle, small vehicle, helicopter, swimming pool, roundabout, soccer ball field, plane, harbor</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.7.4.1.1">
<span class="ltx_p" id="S6.T7.2.2.7.4.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.7.4.1.1.1.1" style="font-size:144%;">Potsdam, Vaihingen</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.7.4.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.7.4.2.1">
<span class="ltx_p" id="S6.T7.2.2.7.4.2.1.1"><span class="ltx_text" id="S6.T7.2.2.7.4.2.1.1.1" style="font-size:144%;">{road, parking lot}, building, low vegetation, tree, car, {clutter, background}</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.8.5.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.8.5.1.1">
<span class="ltx_p" id="S6.T7.2.2.8.5.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.8.5.1.1.1.1" style="font-size:144%;">UAVid</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.8.5.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.8.5.2.1">
<span class="ltx_p" id="S6.T7.2.2.8.5.2.1.1"><span class="ltx_text" id="S6.T7.2.2.8.5.2.1.1.1" style="font-size:144%;">background, building, road, car, tree, vegetation, human</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.9.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.9.6.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.9.6.1.1">
<span class="ltx_p" id="S6.T7.2.2.9.6.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.9.6.1.1.1.1" style="font-size:144%;">UDD5</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.9.6.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.9.6.2.1">
<span class="ltx_p" id="S6.T7.2.2.9.6.2.1.1"><span class="ltx_text" id="S6.T7.2.2.9.6.2.1.1.1" style="font-size:144%;">vegetation, building, road, vehicle, background</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.10.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.10.7.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.10.7.1.1">
<span class="ltx_p" id="S6.T7.2.2.10.7.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.10.7.1.1.1.1" style="font-size:144%;">VDD</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.10.7.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.10.7.2.1">
<span class="ltx_p" id="S6.T7.2.2.10.7.2.1.1"><span class="ltx_text" id="S6.T7.2.2.10.7.2.1.1.1" style="font-size:144%;">background, facade, road, vegetation, vehicle, roof, water</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.2.2.2">
<span class="ltx_p" id="S6.T7.2.2.2.2.2.2" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.2.1" style="font-size:144%;">WHU</span><sup class="ltx_sup" id="S6.T7.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_italic" id="S6.T7.2.2.2.2.2.2.2.1" style="font-size:144%;">Aerial</span></sup><span class="ltx_text" id="S6.T7.2.2.2.2.2.2.3" style="font-size:144%;">, WHU</span><sup class="ltx_sup" id="S6.T7.2.2.2.2.2.2.4"><span class="ltx_text ltx_font_italic" id="S6.T7.2.2.2.2.2.2.4.1" style="font-size:144%;">Sat.II</span></sup><span class="ltx_text" id="S6.T7.2.2.2.2.2.2.5" style="font-size:144%;">, Inria, xBD</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.2.3" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.2.3.1">
<span class="ltx_p" id="S6.T7.2.2.2.3.1.1"><span class="ltx_text" id="S6.T7.2.2.2.3.1.1.1" style="font-size:144%;">background, building</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.11.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T7.2.2.11.8.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.11.8.1.1">
<span class="ltx_p" id="S6.T7.2.2.11.8.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.11.8.1.1.1.1" style="font-size:144%;">CHN6-CUG, DeepGlobe, Massachusetts, SpaceNet</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle" id="S6.T7.2.2.11.8.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.11.8.2.1">
<span class="ltx_p" id="S6.T7.2.2.11.8.2.1.1"><span class="ltx_text" id="S6.T7.2.2.11.8.2.1.1.1" style="font-size:144%;">background, road</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T7.2.2.12.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T7.2.2.12.9.1">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.12.9.1.1">
<span class="ltx_p" id="S6.T7.2.2.12.9.1.1.1" style="width:105.3pt;"><span class="ltx_text" id="S6.T7.2.2.12.9.1.1.1.1" style="font-size:144%;">WBS-SI</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_middle ltx_border_bb" id="S6.T7.2.2.12.9.2" style="width:426.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T7.2.2.12.9.2.1">
<span class="ltx_p" id="S6.T7.2.2.12.9.2.1.1"><span class="ltx_text" id="S6.T7.2.2.12.9.2.1.1.1" style="font-size:144%;">background, water</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_section ltx_centering" id="S7">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">7 </span>Datasets</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Semantic Segmentation</h3>
<div class="ltx_para ltx_noindent" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text" id="S7.SS1.p1.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.2" style="font-size:144%;">OpenEarthMap</span><span class="ltx_text" id="S7.SS1.p1.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p1.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a><span class="ltx_text" id="S7.SS1.p1.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p1.1.6" style="font-size:144%;"> includes worldwide satellite and aerial images with a spatial resolution of 0.25-0.5m. It contains 8 foreground classes and one background class. We use its validation set (excluding xBD data) for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text" id="S7.SS1.p2.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.2" style="font-size:144%;">LoveDA</span><span class="ltx_text" id="S7.SS1.p2.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p2.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a><span class="ltx_text" id="S7.SS1.p2.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p2.1.6" style="font-size:144%;"> is constructed using 0.3m images obtained from the Google Earth platform. It contains both urban and rural areas. It contains 6 foreground classes and one background class. We use its validation set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1"><span class="ltx_text" id="S7.SS1.p3.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.1.2" style="font-size:144%;">iSAID</span><span class="ltx_text" id="S7.SS1.p3.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p3.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a><span class="ltx_text" id="S7.SS1.p3.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p3.1.6" style="font-size:144%;"> is manily collected from the Google Earth, some are taken by satellite JL-1, the others are taken by satellite GF-2. Its image data is the same as the DOTA-v1.0 dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p3.1.7.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a><span class="ltx_text" id="S7.SS1.p3.1.8.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p3.1.9" style="font-size:144%;">. It contains 15 foreground classes and one background class. We use its validation set for evaluation, which is cropped to 11,644 images by default (patch_size=896, overlap_area=384).</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1"><span class="ltx_text" id="S7.SS1.p4.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p4.1.2" style="font-size:144%;">Potsdam<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote6.1.1.1" style="font-size:69%;">6</span></span><span class="ltx_text ltx_font_medium" id="footnote6.9" style="font-size:69%;">http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html</span></span></span></span> and Vaihingen<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote7.1.1.1" style="font-size:69%;">7</span></span><span class="ltx_text ltx_font_medium" id="footnote7.9" style="font-size:69%;">http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html</span></span></span></span></span><span class="ltx_text" id="S7.SS1.p4.1.3" style="font-size:144%;"> are for urban semantic segmentation used in the 2D Semantic Labeling Contest. Their spatial resolutions are 5cm and 9cm, respectively, and they contain 5 foreground classes and one background class. We use the validation set for evaluation according to MMSeg’s</span><span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/open-mmlab/mmsegmentation</span></span></span><span class="ltx_text" id="S7.SS1.p4.1.4" style="font-size:144%;"> setting.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1"><span class="ltx_text" id="S7.SS1.p5.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p5.1.2" style="font-size:144%;">UAVid</span><span class="ltx_text" id="S7.SS1.p5.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p5.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a><span class="ltx_text" id="S7.SS1.p5.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p5.1.6" style="font-size:144%;"> consists of 30 video sequences capturing 4K HR images in slanted views. We treat them as images without considering the relationship between frames, and the classes “static car” and “moving car” are converted to “car”. Therefore, it contains 5 foreground classes and one background class. We use its test set for evaluation, which is cropped to 1020 images (patch_height=1280, patch_width=1080, no overlap).</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p6">
<p class="ltx_p" id="S7.SS1.p6.1"><span class="ltx_text" id="S7.SS1.p6.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p6.1.2" style="font-size:144%;">UDD5</span><span class="ltx_text" id="S7.SS1.p6.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p6.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a><span class="ltx_text" id="S7.SS1.p6.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p6.1.6" style="font-size:144%;"> is collected by a professional-grade UAV (DJI-Phantom 4) at altitudes between 60 and 100m. It contains 4 foreground classes and one background class. We use its validation set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p7">
<p class="ltx_p" id="S7.SS1.p7.1"><span class="ltx_text" id="S7.SS1.p7.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS1.p7.1.2" style="font-size:144%;">VDD</span><span class="ltx_text" id="S7.SS1.p7.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS1.p7.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a><span class="ltx_text" id="S7.SS1.p7.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS1.p7.1.6" style="font-size:144%;"> is collected by DJI MAVIC AIR II, including 400 RGB images with 4000*3000 pixel size. All the images are taken at altitudes ranging from 50m to 120m. It contains 6 foreground classes and one background class. We use its test set for evaluation.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Building extraction</h3>
<div class="ltx_para ltx_noindent" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.2"><span class="ltx_text" id="S7.SS2.p1.2.2" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1" style="font-size:144%;">WHU<sup class="ltx_sup" id="S7.SS2.p1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S7.SS2.p1.1.1.1.1">Aerial</span></sup></span><span class="ltx_text" id="S7.SS2.p1.2.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS2.p1.2.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a><span class="ltx_text" id="S7.SS2.p1.2.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS2.p1.2.6" style="font-size:144%;"> consists of more than 220k independent buildings extracted from aerial images with 0.075m spatial resolution and 450 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS2.p1.2.m1.1"><semantics id="S7.SS2.p1.2.m1.1a"><mrow id="S7.SS2.p1.2.m1.1.1" xref="S7.SS2.p1.2.m1.1.1.cmml"><mi id="S7.SS2.p1.2.m1.1.1.2" mathsize="144%" xref="S7.SS2.p1.2.m1.1.1.2.cmml">k</mi><mo id="S7.SS2.p1.2.m1.1.1.1" xref="S7.SS2.p1.2.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS2.p1.2.m1.1.1.3" xref="S7.SS2.p1.2.m1.1.1.3.cmml"><mi id="S7.SS2.p1.2.m1.1.1.3.2" mathsize="144%" xref="S7.SS2.p1.2.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS2.p1.2.m1.1.1.3.3" mathsize="144%" xref="S7.SS2.p1.2.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m1.1b"><apply id="S7.SS2.p1.2.m1.1.1.cmml" xref="S7.SS2.p1.2.m1.1.1"><times id="S7.SS2.p1.2.m1.1.1.1.cmml" xref="S7.SS2.p1.2.m1.1.1.1"></times><ci id="S7.SS2.p1.2.m1.1.1.2.cmml" xref="S7.SS2.p1.2.m1.1.1.2">𝑘</ci><apply id="S7.SS2.p1.2.m1.1.1.3.cmml" xref="S7.SS2.p1.2.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.2.m1.1.1.3.1.cmml" xref="S7.SS2.p1.2.m1.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.2.m1.1.1.3.2.cmml" xref="S7.SS2.p1.2.m1.1.1.3.2">𝑚</ci><cn id="S7.SS2.p1.2.m1.1.1.3.3.cmml" type="integer" xref="S7.SS2.p1.2.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.p1.2.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS2.p1.2.7" style="font-size:144%;"> covering in Christchurch, New Zealand. We use its validation set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.3"><span class="ltx_text" id="S7.SS2.p2.3.2" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1" style="font-size:144%;">WHU<sup class="ltx_sup" id="S7.SS2.p2.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S7.SS2.p2.1.1.1.1">Sat.II</span></sup></span><span class="ltx_text" id="S7.SS2.p2.3.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS2.p2.3.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a><span class="ltx_text" id="S7.SS2.p2.3.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS2.p2.3.6" style="font-size:144%;"> consists of 6 neighboring satellite images covering 860 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS2.p2.2.m1.1"><semantics id="S7.SS2.p2.2.m1.1a"><mrow id="S7.SS2.p2.2.m1.1.1" xref="S7.SS2.p2.2.m1.1.1.cmml"><mi id="S7.SS2.p2.2.m1.1.1.2" mathsize="144%" xref="S7.SS2.p2.2.m1.1.1.2.cmml">k</mi><mo id="S7.SS2.p2.2.m1.1.1.1" xref="S7.SS2.p2.2.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS2.p2.2.m1.1.1.3" xref="S7.SS2.p2.2.m1.1.1.3.cmml"><mi id="S7.SS2.p2.2.m1.1.1.3.2" mathsize="144%" xref="S7.SS2.p2.2.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS2.p2.2.m1.1.1.3.3" mathsize="144%" xref="S7.SS2.p2.2.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.2.m1.1b"><apply id="S7.SS2.p2.2.m1.1.1.cmml" xref="S7.SS2.p2.2.m1.1.1"><times id="S7.SS2.p2.2.m1.1.1.1.cmml" xref="S7.SS2.p2.2.m1.1.1.1"></times><ci id="S7.SS2.p2.2.m1.1.1.2.cmml" xref="S7.SS2.p2.2.m1.1.1.2">𝑘</ci><apply id="S7.SS2.p2.2.m1.1.1.3.cmml" xref="S7.SS2.p2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p2.2.m1.1.1.3.1.cmml" xref="S7.SS2.p2.2.m1.1.1.3">superscript</csymbol><ci id="S7.SS2.p2.2.m1.1.1.3.2.cmml" xref="S7.SS2.p2.2.m1.1.1.3.2">𝑚</ci><cn id="S7.SS2.p2.2.m1.1.1.3.3.cmml" type="integer" xref="S7.SS2.p2.2.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.2.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.p2.2.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS2.p2.3.7" style="font-size:144%;"> on East Asia with 0.45m ground resolution. We use its test set (3726 tiles with 8358 buildings) for evaluation. The original images are cropped to 1000 </span><math alttext="\times" class="ltx_Math" display="inline" id="S7.SS2.p2.3.m2.1"><semantics id="S7.SS2.p2.3.m2.1a"><mo id="S7.SS2.p2.3.m2.1.1" mathsize="144%" xref="S7.SS2.p2.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.3.m2.1b"><times id="S7.SS2.p2.3.m2.1.1.cmml" xref="S7.SS2.p2.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.3.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.p2.3.m2.1d">×</annotation></semantics></math><span class="ltx_text" id="S7.SS2.p2.3.8" style="font-size:144%;"> 1000 without overlap.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1"><span class="ltx_text" id="S7.SS2.p3.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.2" style="font-size:144%;">Inria</span><span class="ltx_text" id="S7.SS2.p3.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS2.p3.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a><span class="ltx_text" id="S7.SS2.p3.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS2.p3.1.6" style="font-size:144%;"> covers dissimilar urban settlements, ranging from densely populated areas (e.g., San Francisco’s financial district) to alpine towns (e.g,. Lienz in Austrian Tyrol). It covers 810 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS2.p3.1.m1.1"><semantics id="S7.SS2.p3.1.m1.1a"><mrow id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml"><mi id="S7.SS2.p3.1.m1.1.1.2" mathsize="144%" xref="S7.SS2.p3.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS2.p3.1.m1.1.1.1" xref="S7.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS2.p3.1.m1.1.1.3" xref="S7.SS2.p3.1.m1.1.1.3.cmml"><mi id="S7.SS2.p3.1.m1.1.1.3.2" mathsize="144%" xref="S7.SS2.p3.1.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS2.p3.1.m1.1.1.3.3" mathsize="144%" xref="S7.SS2.p3.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><apply id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1"><times id="S7.SS2.p3.1.m1.1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1.1"></times><ci id="S7.SS2.p3.1.m1.1.1.2.cmml" xref="S7.SS2.p3.1.m1.1.1.2">𝑘</ci><apply id="S7.SS2.p3.1.m1.1.1.3.cmml" xref="S7.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p3.1.m1.1.1.3.1.cmml" xref="S7.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S7.SS2.p3.1.m1.1.1.3.2.cmml" xref="S7.SS2.p3.1.m1.1.1.3.2">𝑚</ci><cn id="S7.SS2.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S7.SS2.p3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.p3.1.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS2.p3.1.7" style="font-size:144%;"> with a spatial resolution of 0.3m. We use its test set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1"><span class="ltx_text" id="S7.SS2.p4.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.2" style="font-size:144%;">xBD</span><span class="ltx_text" id="S7.SS2.p4.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS2.p4.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a><span class="ltx_text" id="S7.SS2.p4.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS2.p4.1.6" style="font-size:144%;"> covers a diverse set of disasters and geographical locations with over 800k building annotations across over 45k </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS2.p4.1.m1.1"><semantics id="S7.SS2.p4.1.m1.1a"><mrow id="S7.SS2.p4.1.m1.1.1" xref="S7.SS2.p4.1.m1.1.1.cmml"><mi id="S7.SS2.p4.1.m1.1.1.2" mathsize="144%" xref="S7.SS2.p4.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS2.p4.1.m1.1.1.1" xref="S7.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS2.p4.1.m1.1.1.3" xref="S7.SS2.p4.1.m1.1.1.3.cmml"><mi id="S7.SS2.p4.1.m1.1.1.3.2" mathsize="144%" xref="S7.SS2.p4.1.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS2.p4.1.m1.1.1.3.3" mathsize="144%" xref="S7.SS2.p4.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p4.1.m1.1b"><apply id="S7.SS2.p4.1.m1.1.1.cmml" xref="S7.SS2.p4.1.m1.1.1"><times id="S7.SS2.p4.1.m1.1.1.1.cmml" xref="S7.SS2.p4.1.m1.1.1.1"></times><ci id="S7.SS2.p4.1.m1.1.1.2.cmml" xref="S7.SS2.p4.1.m1.1.1.2">𝑘</ci><apply id="S7.SS2.p4.1.m1.1.1.3.cmml" xref="S7.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p4.1.m1.1.1.3.1.cmml" xref="S7.SS2.p4.1.m1.1.1.3">superscript</csymbol><ci id="S7.SS2.p4.1.m1.1.1.3.2.cmml" xref="S7.SS2.p4.1.m1.1.1.3.2">𝑚</ci><cn id="S7.SS2.p4.1.m1.1.1.3.3.cmml" type="integer" xref="S7.SS2.p4.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p4.1.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS2.p4.1.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS2.p4.1.7" style="font-size:144%;"> of imagery. Its spatial resolution is 0.8m. We use the pre-disaster satellite data of test set for evaluation.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Road extraction</h3>
<div class="ltx_para ltx_noindent" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1"><span class="ltx_text" id="S7.SS3.p1.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS3.p1.1.2" style="font-size:144%;">CHN6-CUG</span><span class="ltx_text" id="S7.SS3.p1.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS3.p1.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a><span class="ltx_text" id="S7.SS3.p1.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS3.p1.1.6" style="font-size:144%;"> is a large-scale satellite image data set of representative cities in China, collected from Google Earth. It contains 4511 labeled images of 512 </span><math alttext="\times" class="ltx_Math" display="inline" id="S7.SS3.p1.1.m1.1"><semantics id="S7.SS3.p1.1.m1.1a"><mo id="S7.SS3.p1.1.m1.1.1" mathsize="144%" xref="S7.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.1.m1.1b"><times id="S7.SS3.p1.1.m1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p1.1.m1.1d">×</annotation></semantics></math><span class="ltx_text" id="S7.SS3.p1.1.7" style="font-size:144%;"> 512 size with a spatial resolution of 0.5m. We use its test set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1"><span class="ltx_text" id="S7.SS3.p2.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS3.p2.1.2" style="font-size:144%;">DeepGlobe<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote9.1.1.1" style="font-size:69%;">9</span></span><span class="ltx_text ltx_font_medium" id="footnote9.9" style="font-size:69%;">http://deepglobe.org</span></span></span></span></span><span class="ltx_text" id="S7.SS3.p2.1.3" style="font-size:144%;"> covers images captured over Thailand, Indonesia, and India. Its available data cover 362 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS3.p2.1.m1.1"><semantics id="S7.SS3.p2.1.m1.1a"><mrow id="S7.SS3.p2.1.m1.1.1" xref="S7.SS3.p2.1.m1.1.1.cmml"><mi id="S7.SS3.p2.1.m1.1.1.2" mathsize="144%" xref="S7.SS3.p2.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS3.p2.1.m1.1.1.1" xref="S7.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS3.p2.1.m1.1.1.3" xref="S7.SS3.p2.1.m1.1.1.3.cmml"><mi id="S7.SS3.p2.1.m1.1.1.3.2" mathsize="144%" xref="S7.SS3.p2.1.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS3.p2.1.m1.1.1.3.3" mathsize="144%" xref="S7.SS3.p2.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.1.m1.1b"><apply id="S7.SS3.p2.1.m1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1"><times id="S7.SS3.p2.1.m1.1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1.1"></times><ci id="S7.SS3.p2.1.m1.1.1.2.cmml" xref="S7.SS3.p2.1.m1.1.1.2">𝑘</ci><apply id="S7.SS3.p2.1.m1.1.1.3.cmml" xref="S7.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS3.p2.1.m1.1.1.3.1.cmml" xref="S7.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S7.SS3.p2.1.m1.1.1.3.2.cmml" xref="S7.SS3.p2.1.m1.1.1.3.2">𝑚</ci><cn id="S7.SS3.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S7.SS3.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.1.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.1.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS3.p2.1.4" style="font-size:144%;"> with a spatial resolution of 5m. The roads are precisely annotated with varying road widths. We use the validation set for evaluation according to the setting in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS3.p2.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a><span class="ltx_text" id="S7.SS3.p2.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS3.p2.1.7" style="font-size:144%;">.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS3.p3">
<p class="ltx_p" id="S7.SS3.p3.1"><span class="ltx_text" id="S7.SS3.p3.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS3.p3.1.2" style="font-size:144%;">Massachusetts</span><span class="ltx_text" id="S7.SS3.p3.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS3.p3.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a><span class="ltx_text" id="S7.SS3.p3.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS3.p3.1.6" style="font-size:144%;"> covers a wide variety of urban, suburban, and rural regions and covers an area of over 2,600 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS3.p3.1.m1.1"><semantics id="S7.SS3.p3.1.m1.1a"><mrow id="S7.SS3.p3.1.m1.1.1" xref="S7.SS3.p3.1.m1.1.1.cmml"><mi id="S7.SS3.p3.1.m1.1.1.2" mathsize="144%" xref="S7.SS3.p3.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS3.p3.1.m1.1.1.1" xref="S7.SS3.p3.1.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS3.p3.1.m1.1.1.3" xref="S7.SS3.p3.1.m1.1.1.3.cmml"><mi id="S7.SS3.p3.1.m1.1.1.3.2" mathsize="144%" xref="S7.SS3.p3.1.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS3.p3.1.m1.1.1.3.3" mathsize="144%" xref="S7.SS3.p3.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.1.m1.1b"><apply id="S7.SS3.p3.1.m1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1"><times id="S7.SS3.p3.1.m1.1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1.1"></times><ci id="S7.SS3.p3.1.m1.1.1.2.cmml" xref="S7.SS3.p3.1.m1.1.1.2">𝑘</ci><apply id="S7.SS3.p3.1.m1.1.1.3.cmml" xref="S7.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS3.p3.1.m1.1.1.3.1.cmml" xref="S7.SS3.p3.1.m1.1.1.3">superscript</csymbol><ci id="S7.SS3.p3.1.m1.1.1.3.2.cmml" xref="S7.SS3.p3.1.m1.1.1.3.2">𝑚</ci><cn id="S7.SS3.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S7.SS3.p3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.1.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p3.1.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS3.p3.1.7" style="font-size:144%;"> with a spatial resolution of 1m. Its labels are generated by rasterizing road centerlines obtained from the OpenStreetMap project, and it uses a line thickness of 7 pixels. We use its test set for evaluation.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS3.p4">
<p class="ltx_p" id="S7.SS3.p4.1"><span class="ltx_text" id="S7.SS3.p4.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS3.p4.1.2" style="font-size:144%;">SpaceNet</span><span class="ltx_text" id="S7.SS3.p4.1.3" style="font-size:144%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS3.p4.1.4.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a><span class="ltx_text" id="S7.SS3.p4.1.5.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS3.p4.1.6" style="font-size:144%;"> contains 422 </span><math alttext="km^{2}" class="ltx_Math" display="inline" id="S7.SS3.p4.1.m1.1"><semantics id="S7.SS3.p4.1.m1.1a"><mrow id="S7.SS3.p4.1.m1.1.1" xref="S7.SS3.p4.1.m1.1.1.cmml"><mi id="S7.SS3.p4.1.m1.1.1.2" mathsize="144%" xref="S7.SS3.p4.1.m1.1.1.2.cmml">k</mi><mo id="S7.SS3.p4.1.m1.1.1.1" xref="S7.SS3.p4.1.m1.1.1.1.cmml">⁢</mo><msup id="S7.SS3.p4.1.m1.1.1.3" xref="S7.SS3.p4.1.m1.1.1.3.cmml"><mi id="S7.SS3.p4.1.m1.1.1.3.2" mathsize="144%" xref="S7.SS3.p4.1.m1.1.1.3.2.cmml">m</mi><mn id="S7.SS3.p4.1.m1.1.1.3.3" mathsize="144%" xref="S7.SS3.p4.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p4.1.m1.1b"><apply id="S7.SS3.p4.1.m1.1.1.cmml" xref="S7.SS3.p4.1.m1.1.1"><times id="S7.SS3.p4.1.m1.1.1.1.cmml" xref="S7.SS3.p4.1.m1.1.1.1"></times><ci id="S7.SS3.p4.1.m1.1.1.2.cmml" xref="S7.SS3.p4.1.m1.1.1.2">𝑘</ci><apply id="S7.SS3.p4.1.m1.1.1.3.cmml" xref="S7.SS3.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS3.p4.1.m1.1.1.3.1.cmml" xref="S7.SS3.p4.1.m1.1.1.3">superscript</csymbol><ci id="S7.SS3.p4.1.m1.1.1.3.2.cmml" xref="S7.SS3.p4.1.m1.1.1.3.2">𝑚</ci><cn id="S7.SS3.p4.1.m1.1.1.3.3.cmml" type="integer" xref="S7.SS3.p4.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p4.1.m1.1c">km^{2}</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p4.1.m1.1d">italic_k italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S7.SS3.p4.1.7" style="font-size:144%;"> of very high-resolution imagery with a spatial resolution of 0.3m. It covers Las Vegas, Paris, Shanghai, Khartoum and is designed for the SpaceNet</span><span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://spacenet.ai/challenges/</span></span></span><span class="ltx_text" id="S7.SS3.p4.1.8" style="font-size:144%;"> challenge. We use the test set for evaluation according to the setting in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.SS3.p4.1.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a><span class="ltx_text" id="S7.SS3.p4.1.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="S7.SS3.p4.1.11" style="font-size:144%;">.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Flood Detection</h3>
<div class="ltx_para ltx_noindent" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1"><span class="ltx_text" id="S7.SS4.p1.1.1" style="font-size:144%;">- </span><span class="ltx_text ltx_font_bold" id="S7.SS4.p1.1.2" style="font-size:144%;">WBS-SI<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote11.1.1.1" style="font-size:69%;">11</span></span><span class="ltx_text ltx_font_medium" id="footnote11.9" style="font-size:69%;">https://www.kaggle.com/datasets/shirshmall/water-body-segmentation-in-satellite-images</span></span></span></span></span><span class="ltx_text" id="S7.SS4.p1.1.3" style="font-size:144%;"> is a satellite image dataset for water body segmentation. It contains 2495 images and we randomly divided 20% of the data as a test set for evaluation.</span></p>
</div>
<figure class="ltx_figure" id="S7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="464" id="S7.F7.g1" src="x8.png" width="663"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F7.10.2.1" style="font-size:63%;">Figure 7</span>: </span><span class="ltx_text" id="S7.F7.2.1" style="font-size:63%;">The distribution of instance sizes for natural image datasets (MS COCO, ImageNet Detection, PASCAL VOC and SUN) and remote sensing datasets (Inria and xBD). The data for the natural image is borrowed from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.01768v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, and the data for Inria and xBD are calculated with the image size at 1024 <math alttext="\times" class="ltx_Math" display="inline" id="S7.F7.2.1.m1.1"><semantics id="S7.F7.2.1.m1.1b"><mo id="S7.F7.2.1.m1.1.1" xref="S7.F7.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.F7.2.1.m1.1c"><times id="S7.F7.2.1.m1.1.1.cmml" xref="S7.F7.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.2.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S7.F7.2.1.m1.1e">×</annotation></semantics></math> 1024.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S7.F8.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F8.4.1.1" style="font-size:63%;">Figure 8</span>: </span><span class="ltx_text" id="S7.F8.5.2" style="font-size:63%;">Qualitative comparison between different training-free OVSS methods on OpenEarthMap.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S7.F9.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F9.4.1.1" style="font-size:63%;">Figure 9</span>: </span><span class="ltx_text" id="S7.F9.5.2" style="font-size:63%;">Qualitative comparison between different training-free OVSS methods on UDD5.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S7.F10.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S7.F10.8.2.1" style="font-size:63%;">Figure 10</span>: </span><span class="ltx_text" id="S7.F10.2.1" style="font-size:63%;">Qualitative comparison between different training-free OVSS methods on WHU<sup class="ltx_sup" id="S7.F10.2.1.1"><span class="ltx_text ltx_font_italic" id="S7.F10.2.1.1.1">Aerial</span></sup>.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 17:25:20 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
