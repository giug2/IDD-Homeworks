<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.10035] Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity</title><meta property="og:description" content="Purpose: We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of someâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.10035">

<!--Generated on Tue Mar  5 18:18:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">[1, 2]<span id="p1.1.1" class="ltx_ERROR undefined">\fnm</span>Sanskar <span id="p1.1.2" class="ltx_ERROR undefined">\sur</span>Amgain
<span id="p1.1.3" class="ltx_ERROR undefined">\equalcont</span>These authors contributed equally to this work.</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\equalcont</span>
<p id="p2.2" class="ltx_p">These authors contributed equally to this work.</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">1]<span id="p3.1.1" class="ltx_ERROR undefined">\orgname</span>NAAMII, <span id="p3.1.2" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.3" class="ltx_ERROR undefined">\city</span>Lalitpur, <span id="p3.1.4" class="ltx_ERROR undefined">\country</span>Nepal
2]<span id="p3.1.5" class="ltx_ERROR undefined">\orgname</span>Base Gene Therapeutics Limited, <span id="p3.1.6" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.7" class="ltx_ERROR undefined">\country</span>UK
3]<span id="p3.1.8" class="ltx_ERROR undefined">\orgname</span>University of Aberdeen, UK
4]<span id="p3.1.9" class="ltx_ERROR undefined">\orgname</span>WEISS, and University College London, UK</p>
</div>
<h1 class="ltx_title ltx_title_document">Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sanskar.amgain@naamii.org.np">sanskar.amgain@naamii.org.np</a>
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Prashant <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Shrestha
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Sophia <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Bano
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Ignacio del Valle <span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Torres
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.1.id1" class="ltx_ERROR undefined">\fnm</span>Michael <span id="id8.2.id2" class="ltx_ERROR undefined">\sur</span>Cunniffe
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id9.1.id1" class="ltx_ERROR undefined">\fnm</span>Victor <span id="id10.2.id2" class="ltx_ERROR undefined">\sur</span>Hernandez
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id11.1.id1" class="ltx_ERROR undefined">\fnm</span>Phil <span id="id12.2.id2" class="ltx_ERROR undefined">\sur</span>Beales
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id13.1.id1" class="ltx_ERROR undefined">\fnm</span>Binod <span id="id14.2.id2" class="ltx_ERROR undefined">\sur</span>Bhattarai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p"><span id="id15.id1.1" class="ltx_text ltx_font_bold">Purpose:</span> We apply federated learning to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely.</p>
<p id="id16.id2" class="ltx_p"><span id="id16.id2.1" class="ltx_text ltx_font_bold">Methods:</span> We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two federated learning methods, FedAvg and FedProx for these settings.</p>
<p id="id17.id3" class="ltx_p"><span id="id17.id3.1" class="ltx_text ltx_font_bold">Results:</span>
Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings.</p>
<p id="id18.id4" class="ltx_p"><span id="id18.id4.1" class="ltx_text ltx_font_bold">Conclusion:</span>
Despite the effectiveness of federated learning in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Optical Coherence TomographyÂ (OCT) is a widely used imaging technique for diagnosing retinal abnormalities. An OCT image captures the different cross-sectional layers of the retina, the thickness of which is important for performing diagnosisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. A large number of works have explored the use of artificial intelligence-based methods for assistive and automated diagnosis from OCT images. The performance and generalizability of these models are directly correlated with the amount of data available for training. However, due to the privacy concerns that arise with sharing raw medical data outside an institution, multi-institution collaboration for the curation of a large central database remains a challenging problem.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Few works have recently explored the use of federated learning for OCT image classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> bypassing the need for a central data curation.
For instance, Gholami et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> used federated learning to classify OCT images into normal and age-related macular degeneration (AMD) categories while Ran et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> uses FedAvg to detect glaucoma samples.
However, these works <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">do not consider the statistical heterogeneous setting</em> where the local data in certain clients lack instances of some labels entirely. Furthermore, these works experiment with a <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">small number of clients (3-4)</em> sidestepping the potential issues that arise when dealing with many clients such as the increased communication overhead. Our work studies two prominent federated learning methods on OCT images simulating such label heterogeneity with a larger number of clients.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2402.10035/assets/fedoct2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.6.3.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text" style="font-size:90%;">Illustration of FedAvg and FedProx training process. In each communication round, a subset of clients trained locally on their respective data splits with objective <math id="S2.F1.3.1.m1.1" class="ltx_Math" alttext="L_{method}" display="inline"><semantics id="S2.F1.3.1.m1.1b"><msub id="S2.F1.3.1.m1.1.1" xref="S2.F1.3.1.m1.1.1.cmml"><mi id="S2.F1.3.1.m1.1.1.2" xref="S2.F1.3.1.m1.1.1.2.cmml">L</mi><mrow id="S2.F1.3.1.m1.1.1.3" xref="S2.F1.3.1.m1.1.1.3.cmml"><mi id="S2.F1.3.1.m1.1.1.3.2" xref="S2.F1.3.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.F1.3.1.m1.1.1.3.1" xref="S2.F1.3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.3.1.m1.1.1.3.3" xref="S2.F1.3.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.3.1.m1.1.1.3.1b" xref="S2.F1.3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.3.1.m1.1.1.3.4" xref="S2.F1.3.1.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.F1.3.1.m1.1.1.3.1c" xref="S2.F1.3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.3.1.m1.1.1.3.5" xref="S2.F1.3.1.m1.1.1.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.F1.3.1.m1.1.1.3.1d" xref="S2.F1.3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.3.1.m1.1.1.3.6" xref="S2.F1.3.1.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.F1.3.1.m1.1.1.3.1e" xref="S2.F1.3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.3.1.m1.1.1.3.7" xref="S2.F1.3.1.m1.1.1.3.7.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F1.3.1.m1.1c"><apply id="S2.F1.3.1.m1.1.1.cmml" xref="S2.F1.3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F1.3.1.m1.1.1.1.cmml" xref="S2.F1.3.1.m1.1.1">subscript</csymbol><ci id="S2.F1.3.1.m1.1.1.2.cmml" xref="S2.F1.3.1.m1.1.1.2">ğ¿</ci><apply id="S2.F1.3.1.m1.1.1.3.cmml" xref="S2.F1.3.1.m1.1.1.3"><times id="S2.F1.3.1.m1.1.1.3.1.cmml" xref="S2.F1.3.1.m1.1.1.3.1"></times><ci id="S2.F1.3.1.m1.1.1.3.2.cmml" xref="S2.F1.3.1.m1.1.1.3.2">ğ‘š</ci><ci id="S2.F1.3.1.m1.1.1.3.3.cmml" xref="S2.F1.3.1.m1.1.1.3.3">ğ‘’</ci><ci id="S2.F1.3.1.m1.1.1.3.4.cmml" xref="S2.F1.3.1.m1.1.1.3.4">ğ‘¡</ci><ci id="S2.F1.3.1.m1.1.1.3.5.cmml" xref="S2.F1.3.1.m1.1.1.3.5">â„</ci><ci id="S2.F1.3.1.m1.1.1.3.6.cmml" xref="S2.F1.3.1.m1.1.1.3.6">ğ‘œ</ci><ci id="S2.F1.3.1.m1.1.1.3.7.cmml" xref="S2.F1.3.1.m1.1.1.3.7">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.1.m1.1d">L_{method}</annotation></semantics></math> are selected for aggregation and used to update the global model. The updated global model parameters, <math id="S2.F1.4.2.m2.1" class="ltx_Math" alttext="w_{g}" display="inline"><semantics id="S2.F1.4.2.m2.1b"><msub id="S2.F1.4.2.m2.1.1" xref="S2.F1.4.2.m2.1.1.cmml"><mi id="S2.F1.4.2.m2.1.1.2" xref="S2.F1.4.2.m2.1.1.2.cmml">w</mi><mi id="S2.F1.4.2.m2.1.1.3" xref="S2.F1.4.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F1.4.2.m2.1c"><apply id="S2.F1.4.2.m2.1.1.cmml" xref="S2.F1.4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.F1.4.2.m2.1.1.1.cmml" xref="S2.F1.4.2.m2.1.1">subscript</csymbol><ci id="S2.F1.4.2.m2.1.1.2.cmml" xref="S2.F1.4.2.m2.1.1.2">ğ‘¤</ci><ci id="S2.F1.4.2.m2.1.1.3.cmml" xref="S2.F1.4.2.m2.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.2.m2.1d">w_{g}</annotation></semantics></math> are copied back to all the clients.</span></figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.4" class="ltx_p">We divided the OCT dataset into disjoint subsets, each associated with a separate client; simulating separate private data associated with distinct medical institutions.
As shown in Fig. Â <a href="#S2.F1" title="Figure 1 â€£ 2 Methods â€£ Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, in each communication round, clients initialize their local parameters with the current global model parameters <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="w_{g}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">w</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ğ‘¤</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">w_{g}</annotation></semantics></math>. Each client, <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">i</annotation></semantics></math> is trained on their respective data split with objective <math id="S2.p1.3.m3.2" class="ltx_Math" alttext="L_{i,method}" display="inline"><semantics id="S2.p1.3.m3.2a"><msub id="S2.p1.3.m3.2.3" xref="S2.p1.3.m3.2.3.cmml"><mi id="S2.p1.3.m3.2.3.2" xref="S2.p1.3.m3.2.3.2.cmml">L</mi><mrow id="S2.p1.3.m3.2.2.2.2" xref="S2.p1.3.m3.2.2.2.3.cmml"><mi id="S2.p1.3.m3.1.1.1.1" xref="S2.p1.3.m3.1.1.1.1.cmml">i</mi><mo id="S2.p1.3.m3.2.2.2.2.2" xref="S2.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S2.p1.3.m3.2.2.2.2.1" xref="S2.p1.3.m3.2.2.2.2.1.cmml"><mi id="S2.p1.3.m3.2.2.2.2.1.2" xref="S2.p1.3.m3.2.2.2.2.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.2.2.2.2.1.1" xref="S2.p1.3.m3.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.2.2.2.2.1.3" xref="S2.p1.3.m3.2.2.2.2.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.2.2.2.2.1.1a" xref="S2.p1.3.m3.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.2.2.2.2.1.4" xref="S2.p1.3.m3.2.2.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.2.2.2.2.1.1b" xref="S2.p1.3.m3.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.2.2.2.2.1.5" xref="S2.p1.3.m3.2.2.2.2.1.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.2.2.2.2.1.1c" xref="S2.p1.3.m3.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.2.2.2.2.1.6" xref="S2.p1.3.m3.2.2.2.2.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.2.2.2.2.1.1d" xref="S2.p1.3.m3.2.2.2.2.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.2.2.2.2.1.7" xref="S2.p1.3.m3.2.2.2.2.1.7.cmml">d</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.2b"><apply id="S2.p1.3.m3.2.3.cmml" xref="S2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.2.3.1.cmml" xref="S2.p1.3.m3.2.3">subscript</csymbol><ci id="S2.p1.3.m3.2.3.2.cmml" xref="S2.p1.3.m3.2.3.2">ğ¿</ci><list id="S2.p1.3.m3.2.2.2.3.cmml" xref="S2.p1.3.m3.2.2.2.2"><ci id="S2.p1.3.m3.1.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1.1">ğ‘–</ci><apply id="S2.p1.3.m3.2.2.2.2.1.cmml" xref="S2.p1.3.m3.2.2.2.2.1"><times id="S2.p1.3.m3.2.2.2.2.1.1.cmml" xref="S2.p1.3.m3.2.2.2.2.1.1"></times><ci id="S2.p1.3.m3.2.2.2.2.1.2.cmml" xref="S2.p1.3.m3.2.2.2.2.1.2">ğ‘š</ci><ci id="S2.p1.3.m3.2.2.2.2.1.3.cmml" xref="S2.p1.3.m3.2.2.2.2.1.3">ğ‘’</ci><ci id="S2.p1.3.m3.2.2.2.2.1.4.cmml" xref="S2.p1.3.m3.2.2.2.2.1.4">ğ‘¡</ci><ci id="S2.p1.3.m3.2.2.2.2.1.5.cmml" xref="S2.p1.3.m3.2.2.2.2.1.5">â„</ci><ci id="S2.p1.3.m3.2.2.2.2.1.6.cmml" xref="S2.p1.3.m3.2.2.2.2.1.6">ğ‘œ</ci><ci id="S2.p1.3.m3.2.2.2.2.1.7.cmml" xref="S2.p1.3.m3.2.2.2.2.1.7">ğ‘‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.2c">L_{i,method}</annotation></semantics></math>, the clients send back updated model parameters <math id="S2.p1.4.m4.3" class="ltx_Math" alttext="\{w_{1},w_{2},\ldots w_{k}\}" display="inline"><semantics id="S2.p1.4.m4.3a"><mrow id="S2.p1.4.m4.3.3.3" xref="S2.p1.4.m4.3.3.4.cmml"><mo stretchy="false" id="S2.p1.4.m4.3.3.3.4" xref="S2.p1.4.m4.3.3.4.cmml">{</mo><msub id="S2.p1.4.m4.1.1.1.1" xref="S2.p1.4.m4.1.1.1.1.cmml"><mi id="S2.p1.4.m4.1.1.1.1.2" xref="S2.p1.4.m4.1.1.1.1.2.cmml">w</mi><mn id="S2.p1.4.m4.1.1.1.1.3" xref="S2.p1.4.m4.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.p1.4.m4.3.3.3.5" xref="S2.p1.4.m4.3.3.4.cmml">,</mo><msub id="S2.p1.4.m4.2.2.2.2" xref="S2.p1.4.m4.2.2.2.2.cmml"><mi id="S2.p1.4.m4.2.2.2.2.2" xref="S2.p1.4.m4.2.2.2.2.2.cmml">w</mi><mn id="S2.p1.4.m4.2.2.2.2.3" xref="S2.p1.4.m4.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.p1.4.m4.3.3.3.6" xref="S2.p1.4.m4.3.3.4.cmml">,</mo><mrow id="S2.p1.4.m4.3.3.3.3" xref="S2.p1.4.m4.3.3.3.3.cmml"><mi mathvariant="normal" id="S2.p1.4.m4.3.3.3.3.2" xref="S2.p1.4.m4.3.3.3.3.2.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.3.3.3.3.1" xref="S2.p1.4.m4.3.3.3.3.1.cmml">â€‹</mo><msub id="S2.p1.4.m4.3.3.3.3.3" xref="S2.p1.4.m4.3.3.3.3.3.cmml"><mi id="S2.p1.4.m4.3.3.3.3.3.2" xref="S2.p1.4.m4.3.3.3.3.3.2.cmml">w</mi><mi id="S2.p1.4.m4.3.3.3.3.3.3" xref="S2.p1.4.m4.3.3.3.3.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S2.p1.4.m4.3.3.3.7" xref="S2.p1.4.m4.3.3.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.3b"><set id="S2.p1.4.m4.3.3.4.cmml" xref="S2.p1.4.m4.3.3.3"><apply id="S2.p1.4.m4.1.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.1.1.2.cmml" xref="S2.p1.4.m4.1.1.1.1.2">ğ‘¤</ci><cn type="integer" id="S2.p1.4.m4.1.1.1.1.3.cmml" xref="S2.p1.4.m4.1.1.1.1.3">1</cn></apply><apply id="S2.p1.4.m4.2.2.2.2.cmml" xref="S2.p1.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S2.p1.4.m4.2.2.2.2.1.cmml" xref="S2.p1.4.m4.2.2.2.2">subscript</csymbol><ci id="S2.p1.4.m4.2.2.2.2.2.cmml" xref="S2.p1.4.m4.2.2.2.2.2">ğ‘¤</ci><cn type="integer" id="S2.p1.4.m4.2.2.2.2.3.cmml" xref="S2.p1.4.m4.2.2.2.2.3">2</cn></apply><apply id="S2.p1.4.m4.3.3.3.3.cmml" xref="S2.p1.4.m4.3.3.3.3"><times id="S2.p1.4.m4.3.3.3.3.1.cmml" xref="S2.p1.4.m4.3.3.3.3.1"></times><ci id="S2.p1.4.m4.3.3.3.3.2.cmml" xref="S2.p1.4.m4.3.3.3.3.2">â€¦</ci><apply id="S2.p1.4.m4.3.3.3.3.3.cmml" xref="S2.p1.4.m4.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.3.3.3.3.3.1.cmml" xref="S2.p1.4.m4.3.3.3.3.3">subscript</csymbol><ci id="S2.p1.4.m4.3.3.3.3.3.2.cmml" xref="S2.p1.4.m4.3.3.3.3.3.2">ğ‘¤</ci><ci id="S2.p1.4.m4.3.3.3.3.3.3.cmml" xref="S2.p1.4.m4.3.3.3.3.3.3">ğ‘˜</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.3c">\{w_{1},w_{2},\ldots w_{k}\}</annotation></semantics></math> to the server, which averages the parameters across clients to obtain the updated global model. This process is repeated until convergence. Since the sharing of information during the training process occurs only at the level of model parameters, and not raw medical data, federated learning achieves the aggregation of information from multiple clients/institutions in a privacy-preserving manner.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.2" class="ltx_p">To reduce communication overhead, a random subset of the clients is selected for local training and aggregation. We experiment with two federated learning methods: FedAvgÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and FedProxÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. FedAvg employs standard cross-entropy loss, <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">F</annotation></semantics></math> as the local training objective <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">L</annotation></semantics></math>, while FedProx additionally penalizes the deviation of local parameters from the global model. Our study utilized the OCT images from Kermany et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> comprising 84,484 OCT images categorized as CNV, DRUSEN, DME, or NORMAL (Fig.Â <a href="#S3.F2.sf2" title="In Figure 2 â€£ 3 Experiments â€£ Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2402.10035/assets/distribution_old.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">((a))</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2402.10035/assets/samples.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="448" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">((b))</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">(a) Label distribution of 4 randomly selected clients for IID and 2 SPC Non-IID settings. For the IID setting, the distribution of labels across clients is similar, while for 2 SPC, every client has multiple missing labels and distinct distributions (b) Image samples from the dataset</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">We evaluated the effectiveness of two prominent federated learning methods, FedAvg and FedProx for the OCT image classification task. With a total of 10 clients, we randomly selected 50% for aggregation to minimize communication overhead. We experimented with two data configurations: IID and Non-IID splits.
For the IID setting, we randomly partitioned the data into 10 disjoint subsets, each associated with a separate client.
The data splits are thus less likely to have missing labels and exhibit less heterogeneity in label distribution across clients.
For the Non-IID setting, we grouped the images by labels and divided them into <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="10*k" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mn id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">âˆ—</mo><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">10</cn><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">10*k</annotation></semantics></math> shards, each containing images belonging to a single category. Each client is then assigned <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">k</annotation></semantics></math> shards as the local data. When <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="k&lt;n" display="inline"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">k</mi><mo id="S3.p1.3.m3.1.1.1" xref="S3.p1.3.m3.1.1.1.cmml">&lt;</mo><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><lt id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1"></lt><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">ğ‘˜</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">k&lt;n</annotation></semantics></math>, <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">n</annotation></semantics></math> being the total number of categories, this sharding guarantees that no client possesses samples from all the categories, mirroring real-life statistical heterogeneity where <em id="S3.p1.4.1" class="ltx_emph ltx_font_italic">not all medical centers have the same distribution of patient data, and access to all the categories</em>. FigÂ <a href="#S3.F2.sf1" title="In Figure 2 â€£ 3 Experiments â€£ Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> illustrates the label distribution for a random subset of clients in both IID and Non-IID settings.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For our classification model, we initialized the encoder with ResNet18 parameters pretrained on ImageNet. The backbone of the network is frozen and only the classification head is fine-tuned on the OCT images. We use SGD optimizer with a learning rate of 0.01 and batch size of 64 to train each client model for 2 local epochs per communication round. The training is performed for 100 communication rounds. We use average accuracy to evaluate the performance of the models under IID and <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="k\in\{2,3\}" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3" xref="S3.p2.1.m1.2.3.cmml"><mi id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.2.cmml">k</mi><mo id="S3.p2.1.m1.2.3.1" xref="S3.p2.1.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S3.p2.1.m1.2.3.3.2" xref="S3.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.p2.1.m1.2.3.3.2.1" xref="S3.p2.1.m1.2.3.3.1.cmml">{</mo><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">2</mn><mo id="S3.p2.1.m1.2.3.3.2.2" xref="S3.p2.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">3</mn><mo stretchy="false" id="S3.p2.1.m1.2.3.3.2.3" xref="S3.p2.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><apply id="S3.p2.1.m1.2.3.cmml" xref="S3.p2.1.m1.2.3"><in id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.1"></in><ci id="S3.p2.1.m1.2.3.2.cmml" xref="S3.p2.1.m1.2.3.2">ğ‘˜</ci><set id="S3.p2.1.m1.2.3.3.1.cmml" xref="S3.p2.1.m1.2.3.3.2"><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">2</cn><cn type="integer" id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">3</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">k\in\{2,3\}</annotation></semantics></math> Non-IID settings.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4 Results â€£ Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, both FedAvg and FedProx perform well under IID settings scoring on par with 98.933<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\pm</annotation></semantics></math>0.27 obtained with centralized training.
However as the statistical heterogeneity across the clients is increased, under the 3 shards per client (2SPC) and 2 shards per client (2SPC) Non-IID settings, the performance of both approaches begins to deteriorate.
Notably, FedProx consistently outperforms FedAvg in our Non-IID settings. This is attributed to the proximal loss term of FedProx that constrains the local updates to be closer to the global model. This constraint mitigates the effect of statistical heterogeneity leading to better performance.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.7.8.1" class="ltx_tr">
<th id="S4.T1.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T1.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IID</th>
<th id="S4.T1.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">2SPC</th>
<th id="S4.T1.7.8.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">3SPC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Â FedAvg</td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1" class="ltx_text ltx_font_bold">Â 98.93 <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.05</span></td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Â 87.22 <math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\pm</annotation></semantics></math> 5.97</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Â 88.32 <math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\pm</annotation></semantics></math> 2.30</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Â FedProx (<math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="\mu=0.2" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mrow id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml"><mi id="S4.T1.4.4.1.m1.1.1.2" xref="S4.T1.4.4.1.m1.1.1.2.cmml">Î¼</mi><mo id="S4.T1.4.4.1.m1.1.1.1" xref="S4.T1.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S4.T1.4.4.1.m1.1.1.3" xref="S4.T1.4.4.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><apply id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"><eq id="S4.T1.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1.1"></eq><ci id="S4.T1.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.1.m1.1.1.2">ğœ‡</ci><cn type="float" id="S4.T1.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\mu=0.2</annotation></semantics></math>)</td>
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Â 98.80 <math id="S4.T1.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.5.5.2.m1.1a"><mo id="S4.T1.5.5.2.m1.1.1" xref="S4.T1.5.5.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.2.m1.1b"><csymbol cd="latexml" id="S4.T1.5.5.2.m1.1.1.cmml" xref="S4.T1.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.2.m1.1c">\pm</annotation></semantics></math> 0.10</td>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.6.6.3.1" class="ltx_text ltx_font_bold">Â 90.57 <math id="S4.T1.6.6.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.6.6.3.1.m1.1a"><mo id="S4.T1.6.6.3.1.m1.1.1" xref="S4.T1.6.6.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.3.1.m1.1b"><csymbol cd="latexml" id="S4.T1.6.6.3.1.m1.1.1.cmml" xref="S4.T1.6.6.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.3.1.m1.1c">\pm</annotation></semantics></math> 3.09</span></td>
<td id="S4.T1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.7.7.4.1" class="ltx_text ltx_font_bold">Â 91.01 <math id="S4.T1.7.7.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.7.7.4.1.m1.1a"><mo id="S4.T1.7.7.4.1.m1.1.1" xref="S4.T1.7.7.4.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.4.1.m1.1b"><csymbol cd="latexml" id="S4.T1.7.7.4.1.m1.1.1.cmml" xref="S4.T1.7.7.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.4.1.m1.1c">\pm</annotation></semantics></math> 2.51</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.9.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.10.2" class="ltx_text" style="font-size:90%;">Accuracy on test dataset under IID, 2SPC, 3SPC data splits. Results consist of mean and standard deviation over 3 random seeds.</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Federated learning provides an effective solution to utilize private data across multiple medical institutions, achieving high performance without exposing raw medical data.
Our study investigated the effectiveness of two federated learning methods, FedAvg and FedProx under different heterogeneity settings for OCT image classification. Both methods consistently performed well across diverse scenarios. However, as the statistical heterogeneity across clients increases, the performance of both approaches starts to deteriorate. Notably, FedProx demonstrated greater robustness to the increased heterogeneity than FedAvg.
This suggests that in highly heterogeneous settings, the application of federated learning for OCT image classification necessitates more sophisticated approaches.
Beyond statistical heterogenity, future work can investigate the effectiveness of federated learning methods with separate domain specific medical data in each client, along with dynamic and evolving medical datasets.</p>
</div>
<div id="S5.p2" class="ltx_para">
<br class="ltx_break">
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.1.1" class="ltx_bibitem">
<span class="ltx_bibblock"><span id="bib.1.1.1.1" class="ltx_ERROR undefined">\bibcommenthead</span>
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Mujaini etÂ al. [2013]</span>
<span class="ltx_bibblock">
Al-Mujaini, A.,
Wali, U.K.,
Azeem, S.:
Optical coherence tomography: clinical applications in medical practice.
Oman medical journal
<span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">28</span>(2),
86
(2013)


</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gholami etÂ al. [2023]</span>
<span class="ltx_bibblock">
Gholami, S.,
Lim, J.I.,
Leng, T.,
Ong, S.S.Y.,
Thompson, A.C.,
Alam, M.N.:
Federated learning for diagnosis of age-related macular degeneration.
Frontiers in Medicine
<span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">10</span>
(2023)


</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ran etÂ al. [2023]</span>
<span class="ltx_bibblock">
Ran, A.R.,
Wang, X.,
Chan, P.P.,
Wong, M.O.,
Yuen, H.,
Lam, N.M.,
Chan, N.C.,
Yip, W.W.,
Young, A.L.,
Yung, H.-W., et al.:
Developing a privacy-preserving deep learning model for glaucoma detection: a multicentre study with federated learning.
British Journal of Ophthalmology
(2023)


</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al. [2017]</span>
<span class="ltx_bibblock">
McMahan, B.,
Moore, E.,
Ramage, D.,
Hampson, S.,
Arcas, B.A.:
Communication-efficient learning of deep networks from decentralized data.
In: Artificial Intelligence and Statistics,
pp. 1273â€“1282
(2017).
PMLR


</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2020]</span>
<span class="ltx_bibblock">
Li, T.,
Sahu, A.K.,
Zaheer, M.,
Sanjabi, M.,
Talwalkar, A.,
Smith, V.:
Federated optimization in heterogeneous networks.
MLSys
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">2</span>,
429â€“450
(2020)


</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kermany etÂ al. [2018]</span>
<span class="ltx_bibblock">
Kermany, D.S.,
Goldbaum, M.,
Cai, W.,
Valentim, C.C.,
Liang, H.,
Baxter, S.L.,
McKeown, A.,
Yang, G.,
Wu, X.,
Yan, F., <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>:
Identifying medical diagnoses and treatable diseases by image-based deep learning.
cell
<span id="bib.bib6.2.2" class="ltx_text ltx_font_bold">172</span>(5),
1122â€“1131
(2018)


</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.10034" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.10035" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.10035">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.10035" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.10036" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:18:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
