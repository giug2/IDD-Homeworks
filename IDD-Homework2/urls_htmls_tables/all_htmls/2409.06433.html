<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.06433] Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization</title><meta property="og:description" content="The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress. Integrating the contributions from scholarly articles into a n‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.06433">

<!--Generated on Sun Oct  6 01:10:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs
<br class="ltx_break">for Scholarly Knowledge Organization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Gollam Rabby<sup id="id8.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S√∂ren Auer<sup id="id9.3.id1" class="ltx_sup">1</sup><sup id="id10.4.id2" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer D‚ÄôSouza<sup id="id11.5.id1" class="ltx_sup"><span id="id11.5.id1.1" class="ltx_text ltx_font_italic">2</span></sup>&amp;Allard Oelen<sup id="id12.6.id2" class="ltx_sup">2</sup>
<sup id="id13.7.id3" class="ltx_sup">1</sup>L3S Research Center, Leibniz University Hannover, Hanover, Germany
<br class="ltx_break"><sup id="id14.8.id4" class="ltx_sup">2</sup>Leibniz Information Centre for Science and Technology, Hannover, Germany
¬†gollam.rabby@l3s.de,
{auer, jennifer.dsouza, allard.oelen}@tib.eu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">The increasing amount of published scholarly articles, exceeding 2.5 million yearly, raises the challenge for researchers in following scientific progress. Integrating the contributions from scholarly articles into a novel type of cognitive knowledge graph (CKG) will be a crucial element for accessing and organizing scholarly knowledge, surpassing the insights provided by titles and abstracts. This research focuses on effectively conveying structured scholarly knowledge by utilizing large language models (LLMs) to categorize scholarly articles and describe their contributions in a structured and comparable manner. While previous studies explored language models within specific research domains, the extensive domain-independent knowledge captured by LLMs offers a substantial opportunity for generating structured contribution descriptions as CKGs. Additionally, LLMs offer customizable pathways through prompt engineering or fine-tuning, thus facilitating to leveraging of smaller LLMs known for their efficiency, cost-effectiveness, and environmental considerations. Our methodology involves harnessing LLM knowledge, and complementing it with domain expert-verified scholarly data sourced from a CKG. This strategic fusion significantly enhances LLM performance, especially in tasks like scholarly article categorization and predicate recommendation. Our method involves fine-tuning LLMs with CKG knowledge and additionally injecting knowledge from a CKG with a novel prompting technique significantly increasing the accuracy of scholarly knowledge extraction. We integrated our approach in the Open Research Knowledge Graph (ORKG), thus enabling precise access to organized scholarly knowledge, crucially benefiting domain-independent scholarly knowledge exchange and dissemination among policymakers, industrial practitioners, and the general public.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, we saw a steeply rising popularity of Large Language Models (LLMs) for a variety of applications in natural language understanding and generation, content creation, programming assistance, translation, etc.
However, for many applications, also a number of challenges with respect to the application of LLMs became apparent. Besides potential bias and intransparency these include in particular: (a) the limited <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">context</span> information that can be exploited by the LLM, (b) <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">confabulation</span>, where the LLM generates information or narratives that are plausible-sounding but factually incorrect or misleading, and (c) difficulty in handling specific highly <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">specialized or niche domains</span> where training data is limited or the language used is very specific or technical.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">These issues are particularly pressing for applications in scholarly communication, i.e., the representation, organization, exchange, and usage of scholarly knowledge.
Traditionally, scholarly knowledge is represented primarily in scientific articles of which several hundred million are already available and approx. 2.5 million are furthermore added every year.
While LLMs can and are being trained with scientific articles¬†<cite class="ltx_cite ltx_citemacro_cite">Jungherr (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>); Lee <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> scholarly communication is inherently complex, involving intricate processes and specialized knowledge.
LLMs struggle to capture the nuances and depth required in academic discourse¬†<cite class="ltx_cite ltx_citemacro_cite">Asher <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we address the context and specialized knowledge issues of LLMs with a Neuro-Symbolic approach intertwining two complementary methods ‚Äì (1) fine-tuning LLMs with background knowledge obtained from knowledge graphs and (2) injecting query-specific context knowledge into the prompt.
For both strategies, we leverage a novel type of contextualized knowledge graphs ‚Äì cognitive knowledge graphs, which organize knowledge not only as entities and relationships but also in small reusable cognitive units.
A cognitive knowledge graph is a knowledge graph equipped with an overlay structure, which determines reusable patterns (so-called graphlets), that represent common cognitive information structures, such as research contributions comprising entities such as the tacked research problem, the approach, the evaluation, etc.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We leverage cognitive knowledge graphs for (a) fine-tuning existing base models with scholarly knowledge obtained from a cognitive knowledge graph and (b) injecting contextual knowledge from the CKG into the prompt in order to exploit additional context during inferencing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We evaluate our approach with a comprehensive set of experiments with four different LLMs, viz. Llama 2 (7B and 13B model variants), Mistral (7B), and finally Gemini Pro.
Our subsequent model evaluations follow a two-fold methodology: 1) automatic evaluations using GPT as an evaluator, and 2) manual evaluations using a human expert evaluator.
We observe that tasks relying on sparse background knowledge such as from the scholarly domain significantly benefit from injecting contextual knowledge from a CKG into the prompt, especially the research field prediction task.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, the contributions of this work comprise:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">the definition of the notion of cognitive knowledge graphs, which are capable of capturing contextual knowledge to bridge between neural and symbolic processing as well as human curation,</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">the conceptualization and implementation of a method for injecting contextual knowledge into prompting as well as fine-tuning,</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">a comprehensive empirical evaluation of the method with four different LLM with human and LLM assessment.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.06433/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Overview on the method for knowledge augmentation and discovery comprising context knowledge prompt injection and fine-tuning leveraging a Cognitive Knowledge Graph.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The related work is roughly categorized along the dimensions of Knowledge Graphs, Prompt Engineering for LLMs, and fine-tuning LLMs for specific tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Knowledge Graphs</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts, or contextual aspects¬†<cite class="ltx_cite ltx_citemacro_cite">Choudhary and Reddy (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. They have limitations in representing evolving or multifaceted domains due to challenges in dynamic information updates and integrating multidisciplinary knowledge. These limitations reduce the effectiveness especially for capturing the evolving nature of real-world systems and handling complex information domains, such as scholarly communication. Knowledge graphs are mostly universal and stationary distributed, which does not consider the dynamic nature of many real-world domains¬†<cite class="ltx_cite ltx_citemacro_cite">Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>. Additionally, knowledge graphs face difficulties in updating information in real-time, which is required as domains evolve over time¬†<cite class="ltx_cite ltx_citemacro_cite">Hou <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. Furthermore, integrating multidisciplinary knowledge into knowledge graphs is challenging, limiting their ability to represent diverse domains effectively¬†<cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. With CKG, we address knowledge graph limitations with dynamic updates and multidisciplinary integration and also enhance their effectiveness in representing evolving or multifaceted domains.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Traditional Prompt Engineering: Effective But Limited</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Traditional prompt engineering methods, such as Zero-Shot¬†<cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>, Few-Shot¬†<cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, Chain-of-Thought (CoT)¬†<cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, Tree-of-Thoughts (ToT)¬†<cite class="ltx_cite ltx_citemacro_cite">Yao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> have shown improvements in leveraging LLMs for knowledge extraction, fine-tuning, and text generation. However, these approaches do not address limitations arising during the application to low-resource and new domains.
While deviating from traditional pipelines and offering unique approaches in specific settings like scholarly knowledge graphs¬†<cite class="ltx_cite ltx_citemacro_cite">D‚ÄôSouza <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, these methods suffer from two key shortcomings:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Domain knowledge capture gap:</span> Out-of-the-box LLMs generally underperform in new domains, indicating the inability to capture relevant domain knowledge¬†<cite class="ltx_cite ltx_citemacro_cite">Hu and Levy (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Prompt design constraints:</span> Identifying appropriate prompt templates requires domain expertise and can be time-consuming, creating a bottleneck in the application¬†<cite class="ltx_cite ltx_citemacro_cite">Hu and Levy (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
</ul>
<p id="S2.SS2.p1.2" class="ltx_p">To address these issues within the scholarly domain, we propose a novel knowledge-driven prompt engineering approach.
This approach injects query-specific context knowledge from a domain-specific CKG directly into the prompts.
This not only bridges the knowledge capture gap but also alleviates the need for readily available domain experts and reduces the time required for crafting effective prompts.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fine-Tuning: From Prefix Tweaks to Parameter-Efficient Powerhouses</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">While prompt-based training and fine-tuning have demonstrably enhanced transformer-based LLM performance (up to 40% increase under relaxed settings¬†<cite class="ltx_cite ltx_citemacro_cite">D‚ÄôSouza <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>), different approaches offer varying advantages and limitations.
Prefix-based tuning, adding trainable tokens to sequences, excels for short and moderate lengths but falters on longer passages¬†<cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>.
Prefix-propagation builds on this concept, achieving superior performance on long documents with 50% fewer parameters¬†<cite class="ltx_cite ltx_citemacro_cite">Zhao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>.
However, parameter-efficient fine-tuning methods like adapter-based PEFT bypass training the entire LLM, focusing on tweaking external parameters.
This surprisingly yields comparable or even better results in downstream tasks¬†<cite class="ltx_cite ltx_citemacro_cite">Hu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>, offering a cost-effective and accessible alternative for practical applications.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.06433/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Example of a graphlet retrieved from the ORKG, displaying a scholarly article, the metadata, and the respective properties and entities (simplified version).</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our method comprises three core elements: the novel notion of cognitive knowledge graphs (CKGs), injecting query-specific context knowledge from CKGs into the prompts, and finally fine-tuning LLMs with CKG knowledge.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To illustrate our method, let us consider a scholarly article titled ‚ÄúExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer‚Äù¬†<cite class="ltx_cite ltx_citemacro_cite">Raffel <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, for which we aim to perform the following two tasks: (1) predicting the research field and (2) recommending a list of predicates.
In this scenario, the CKG Open Research Knowledge Graph (ORKG)¬†<cite class="ltx_cite ltx_citemacro_cite">Jaradeh <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> holds metadata and a CKG graphlet for this scholarly article<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://orkg.org/paper/R161808" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://orkg.org/paper/R161808</a></span></span></span>. A visual representation of this specific graphlet is depicted in <a href="#S3.F2" title="Figure 2 ‚Ä£ 3 Method ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†2</span></a>.
The graphlet is essentially a set of typed entities connected through properties such as <span id="S3.p2.1.1" class="ltx_text ltx_font_typewriter">hasTitle</span>, <span id="S3.p2.1.2" class="ltx_text ltx_font_typewriter">followsMethodology</span>, <span id="S3.p2.1.3" class="ltx_text ltx_font_typewriter">usesDataset</span>, <span id="S3.p2.1.4" class="ltx_text ltx_font_typewriter">hasContribution</span>, <span id="S3.p2.1.5" class="ltx_text ltx_font_typewriter">belongsToResearchField</span> etc.
Using ORKG as a CKG, we retrieve this graphlet via SPARQL.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Simultaneously, we improve the prompts by incorporating real-world context, leveraging abstracts from the CORE dataset.
This enriches the LLM understanding of scholarly articles and their properties.
For prompt engineering in this scholarly article, we use the existing CoT prompt framework, injecting it with the extracted query-specific context knowledge for the research field prediction task.
We include the title, abstract, and research field hierarchy to guide the LLM.
Additionally, we examine the impact of task-aware prefixes, such as ‚ÄúResearch_field_prediction‚Äù, to monitor the LLM for specific tasks.
Integrating CKG graphlets into prompts enhances the LLM‚Äôs performance for certain cases without needing additional fine-tuning.
For instance, using the pre-trained Llama 2 13b, this scholarly article can accurately be assigned to the research field of ‚Äúmachine learning‚Äù.
Furthermore, our methodology involves fine-tuning to optimize the LLM‚Äôs performance, especially in scenarios where prompts lack query- and domain-specific knowledge for a specific task.
In the case of this scholarly article, without fine-tuning, it is not possible to determine the most suitable domain-specific properties for describing the research contribution, such as <span id="S3.p3.1.1" class="ltx_text ltx_font_typewriter">usesTrainingCorpus</span>, <span id="S3.p3.1.2" class="ltx_text ltx_font_typewriter">usesTokenization</span>, <span id="S3.p3.1.3" class="ltx_text ltx_font_typewriter">hasNumberofParameters</span> etc.
Through CKG-based fine-tuning, like ORKG_Llama_2_13b, the LLM can extract both domain-specific and domain-independent properties, including <span id="S3.p3.1.4" class="ltx_text ltx_font_typewriter">followsMethodology</span>, <span id="S3.p3.1.5" class="ltx_text ltx_font_typewriter">usesDataset</span>, <span id="S3.p3.1.6" class="ltx_text ltx_font_typewriter">hasContribution</span>, <span id="S3.p3.1.7" class="ltx_text ltx_font_typewriter">usesTrainingCorpus</span>, <span id="S3.p3.1.8" class="ltx_text ltx_font_typewriter">usesTokenization</span>, <span id="S3.p3.1.9" class="ltx_text ltx_font_typewriter">hasNumberofParameters</span>, <span id="S3.p3.1.10" class="ltx_text ltx_font_typewriter">hasModelFamily</span>, <span id="S3.p3.1.11" class="ltx_text ltx_font_typewriter">hasLicense</span> etc.
This integrated approach demonstrates the effectiveness of our methodology in improving the LLM‚Äôs performance across various scholarly tasks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Cognitive Knowledge Graphs</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Traditional knowledge graphs primarily focus on representing explicit facts and relationships between entities, often lacking the depth to capture complex, abstract concepts or contextual nuances.
Additionally, they struggle with dynamic information updates and integrating multidisciplinary knowledge, limiting their effectiveness in representing evolving or multifaceted domains, such as scholarly communication.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Our concept of a Cognitive Knowledge Graph now builds upon traditional knowledge graphs by integrating an overlay structure that identifies and utilizes reusable patterns, referred to as graphlets. These patterns represent common cognitive information structures. The motivation for the development and use of CKGs in addition to traditional knowledge graphs is multifaceted.
Among others, we aim for enhanced representation of complex concepts like research methodologies, arguments, or narratives. CKGs, with their overlay structures, can better represent these multifaceted and complex concepts. In particular, we envision CKGs to capture and represent the context in which information exists thus facilitating the moderation between machine and human intelligence. This is particularly important in domains like research, where understanding the context (e.g., the problem addressed, methodologies used, and the nature of the conclusions) is crucial for accurate interpretation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The base constituents of cognitive knowledge graphs are more complex fabrics of entity descriptions arranged according to certain patterns ‚Äì <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">graphlets</span>.
In network analysis and graph theory, the notions graphlet¬†<cite class="ltx_cite ltx_citemacro_cite">Pr≈æulj <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2004</a>); Pr≈æulj (<a href="#bib.bib19" title="" class="ltx_ref">2007</a>)</cite> and motif¬†<cite class="ltx_cite ltx_citemacro_cite">Milo <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2002</a>)</cite> were introduced to provide a structuring element between whole graphs and individual nodes and edges.
Hence, in order to be able to effectively represent and manage more complex knowledge artefacts, we translate and apply the notion of graphlets to knowledge graphs.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Formally, a <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">CKG graphlet</span> is a tuple of sets of types (classes) and roles (properties) <math id="S3.SS1.p4.1.m1.2" class="ltx_Math" alttext="(C,P)" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow id="S3.SS1.p4.1.m1.2.3.2" xref="S3.SS1.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.2.3.2.1" xref="S3.SS1.p4.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">C</mi><mo id="S3.SS1.p4.1.m1.2.3.2.2" xref="S3.SS1.p4.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml">P</mi><mo stretchy="false" id="S3.SS1.p4.1.m1.2.3.2.3" xref="S3.SS1.p4.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><interval closure="open" id="S3.SS1.p4.1.m1.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3.2"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùê∂</ci><ci id="S3.SS1.p4.1.m1.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2">ùëÉ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">(C,P)</annotation></semantics></math>, where:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.3" class="ltx_p">for each role <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="p\in P" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S3.I1.i1.p1.1.m1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.cmml">‚àà</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><in id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1"></in><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">ùëù</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">p\in P</annotation></semantics></math>, the domain (either explicitly defined or implicitly inferred from a concrete CKG) includes at least one of the types <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mrow id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">c</mi><mo id="S3.I1.i1.p1.2.m2.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.cmml">‚àà</mo><mi id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><in id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1"></in><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">ùëê</ci><ci id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">c\in C</annotation></semantics></math>: <math id="S3.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="domain(p)\subset C" display="inline"><semantics id="S3.I1.i1.p1.3.m3.1a"><mrow id="S3.I1.i1.p1.3.m3.1.2" xref="S3.I1.i1.p1.3.m3.1.2.cmml"><mrow id="S3.I1.i1.p1.3.m3.1.2.2" xref="S3.I1.i1.p1.3.m3.1.2.2.cmml"><mi id="S3.I1.i1.p1.3.m3.1.2.2.2" xref="S3.I1.i1.p1.3.m3.1.2.2.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.3.m3.1.2.2.3" xref="S3.I1.i1.p1.3.m3.1.2.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1a" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.3.m3.1.2.2.4" xref="S3.I1.i1.p1.3.m3.1.2.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1b" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.3.m3.1.2.2.5" xref="S3.I1.i1.p1.3.m3.1.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1c" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.3.m3.1.2.2.6" xref="S3.I1.i1.p1.3.m3.1.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1d" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.3.m3.1.2.2.7" xref="S3.I1.i1.p1.3.m3.1.2.2.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.3.m3.1.2.2.1e" xref="S3.I1.i1.p1.3.m3.1.2.2.1.cmml">‚Äã</mo><mrow id="S3.I1.i1.p1.3.m3.1.2.2.8.2" xref="S3.I1.i1.p1.3.m3.1.2.2.cmml"><mo stretchy="false" id="S3.I1.i1.p1.3.m3.1.2.2.8.2.1" xref="S3.I1.i1.p1.3.m3.1.2.2.cmml">(</mo><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">p</mi><mo stretchy="false" id="S3.I1.i1.p1.3.m3.1.2.2.8.2.2" xref="S3.I1.i1.p1.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.I1.i1.p1.3.m3.1.2.1" xref="S3.I1.i1.p1.3.m3.1.2.1.cmml">‚äÇ</mo><mi id="S3.I1.i1.p1.3.m3.1.2.3" xref="S3.I1.i1.p1.3.m3.1.2.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><apply id="S3.I1.i1.p1.3.m3.1.2.cmml" xref="S3.I1.i1.p1.3.m3.1.2"><subset id="S3.I1.i1.p1.3.m3.1.2.1.cmml" xref="S3.I1.i1.p1.3.m3.1.2.1"></subset><apply id="S3.I1.i1.p1.3.m3.1.2.2.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2"><times id="S3.I1.i1.p1.3.m3.1.2.2.1.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.1"></times><ci id="S3.I1.i1.p1.3.m3.1.2.2.2.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.2">ùëë</ci><ci id="S3.I1.i1.p1.3.m3.1.2.2.3.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.3">ùëú</ci><ci id="S3.I1.i1.p1.3.m3.1.2.2.4.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.4">ùëö</ci><ci id="S3.I1.i1.p1.3.m3.1.2.2.5.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.5">ùëé</ci><ci id="S3.I1.i1.p1.3.m3.1.2.2.6.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.6">ùëñ</ci><ci id="S3.I1.i1.p1.3.m3.1.2.2.7.cmml" xref="S3.I1.i1.p1.3.m3.1.2.2.7">ùëõ</ci><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">ùëù</ci></apply><ci id="S3.I1.i1.p1.3.m3.1.2.3.cmml" xref="S3.I1.i1.p1.3.m3.1.2.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">domain(p)\subset C</annotation></semantics></math> and</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.3" class="ltx_p">all types <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">c</mi><mo id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">‚àà</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><in id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></in><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ùëê</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">ùê∂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">c\in C</annotation></semantics></math> are connected via a property chain in <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">P</annotation></semantics></math>: <math id="S3.I1.i2.p1.3.m3.2" class="ltx_math_unparsed" alttext="\forall c_{1},c_{2}\in C,\;\exists p_{1},...,pj,...,p_{n}\in P:domain(p_{1})=c_{1}\wedge range(p_{n})=c_{2}\wedge\bigwedge\limits_{i=1}^{n-1}range(p_{i})=domain(p_{i+1})" display="inline"><semantics id="S3.I1.i2.p1.3.m3.2a"><mrow id="S3.I1.i2.p1.3.m3.2b"><mo rspace="0.167em" id="S3.I1.i2.p1.3.m3.2.3">‚àÄ</mo><msub id="S3.I1.i2.p1.3.m3.2.4"><mi id="S3.I1.i2.p1.3.m3.2.4.2">c</mi><mn id="S3.I1.i2.p1.3.m3.2.4.3">1</mn></msub><mo id="S3.I1.i2.p1.3.m3.2.5">,</mo><msub id="S3.I1.i2.p1.3.m3.2.6"><mi id="S3.I1.i2.p1.3.m3.2.6.2">c</mi><mn id="S3.I1.i2.p1.3.m3.2.6.3">2</mn></msub><mo id="S3.I1.i2.p1.3.m3.2.7">‚àà</mo><mi id="S3.I1.i2.p1.3.m3.2.8">C</mi><mo rspace="0.447em" id="S3.I1.i2.p1.3.m3.2.9">,</mo><mo rspace="0.167em" id="S3.I1.i2.p1.3.m3.2.10">‚àÉ</mo><msub id="S3.I1.i2.p1.3.m3.2.11"><mi id="S3.I1.i2.p1.3.m3.2.11.2">p</mi><mn id="S3.I1.i2.p1.3.m3.2.11.3">1</mn></msub><mo id="S3.I1.i2.p1.3.m3.2.12">,</mo><mi mathvariant="normal" id="S3.I1.i2.p1.3.m3.1.1">‚Ä¶</mi><mo id="S3.I1.i2.p1.3.m3.2.13">,</mo><mi id="S3.I1.i2.p1.3.m3.2.14">p</mi><mi id="S3.I1.i2.p1.3.m3.2.15">j</mi><mo id="S3.I1.i2.p1.3.m3.2.16">,</mo><mi mathvariant="normal" id="S3.I1.i2.p1.3.m3.2.2">‚Ä¶</mi><mo id="S3.I1.i2.p1.3.m3.2.17">,</mo><msub id="S3.I1.i2.p1.3.m3.2.18"><mi id="S3.I1.i2.p1.3.m3.2.18.2">p</mi><mi id="S3.I1.i2.p1.3.m3.2.18.3">n</mi></msub><mo id="S3.I1.i2.p1.3.m3.2.19">‚àà</mo><mi id="S3.I1.i2.p1.3.m3.2.20">P</mi><mo lspace="0.278em" rspace="0.278em" id="S3.I1.i2.p1.3.m3.2.21">:</mo><mi id="S3.I1.i2.p1.3.m3.2.22">d</mi><mi id="S3.I1.i2.p1.3.m3.2.23">o</mi><mi id="S3.I1.i2.p1.3.m3.2.24">m</mi><mi id="S3.I1.i2.p1.3.m3.2.25">a</mi><mi id="S3.I1.i2.p1.3.m3.2.26">i</mi><mi id="S3.I1.i2.p1.3.m3.2.27">n</mi><mrow id="S3.I1.i2.p1.3.m3.2.28"><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.28.1">(</mo><msub id="S3.I1.i2.p1.3.m3.2.28.2"><mi id="S3.I1.i2.p1.3.m3.2.28.2.2">p</mi><mn id="S3.I1.i2.p1.3.m3.2.28.2.3">1</mn></msub><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.28.3">)</mo></mrow><mo id="S3.I1.i2.p1.3.m3.2.29">=</mo><msub id="S3.I1.i2.p1.3.m3.2.30"><mi id="S3.I1.i2.p1.3.m3.2.30.2">c</mi><mn id="S3.I1.i2.p1.3.m3.2.30.3">1</mn></msub><mo id="S3.I1.i2.p1.3.m3.2.31">‚àß</mo><mi id="S3.I1.i2.p1.3.m3.2.32">r</mi><mi id="S3.I1.i2.p1.3.m3.2.33">a</mi><mi id="S3.I1.i2.p1.3.m3.2.34">n</mi><mi id="S3.I1.i2.p1.3.m3.2.35">g</mi><mi id="S3.I1.i2.p1.3.m3.2.36">e</mi><mrow id="S3.I1.i2.p1.3.m3.2.37"><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.37.1">(</mo><msub id="S3.I1.i2.p1.3.m3.2.37.2"><mi id="S3.I1.i2.p1.3.m3.2.37.2.2">p</mi><mi id="S3.I1.i2.p1.3.m3.2.37.2.3">n</mi></msub><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.37.3">)</mo></mrow><mo id="S3.I1.i2.p1.3.m3.2.38">=</mo><msub id="S3.I1.i2.p1.3.m3.2.39"><mi id="S3.I1.i2.p1.3.m3.2.39.2">c</mi><mn id="S3.I1.i2.p1.3.m3.2.39.3">2</mn></msub><mo rspace="0.055em" id="S3.I1.i2.p1.3.m3.2.40">‚àß</mo><munderover id="S3.I1.i2.p1.3.m3.2.41"><mo movablelimits="false" id="S3.I1.i2.p1.3.m3.2.41.2.2">‚ãÄ</mo><mrow id="S3.I1.i2.p1.3.m3.2.41.2.3"><mi id="S3.I1.i2.p1.3.m3.2.41.2.3.2">i</mi><mo id="S3.I1.i2.p1.3.m3.2.41.2.3.1">=</mo><mn id="S3.I1.i2.p1.3.m3.2.41.2.3.3">1</mn></mrow><mrow id="S3.I1.i2.p1.3.m3.2.41.3"><mi id="S3.I1.i2.p1.3.m3.2.41.3.2">n</mi><mo id="S3.I1.i2.p1.3.m3.2.41.3.1">‚àí</mo><mn id="S3.I1.i2.p1.3.m3.2.41.3.3">1</mn></mrow></munderover><mi id="S3.I1.i2.p1.3.m3.2.42">r</mi><mi id="S3.I1.i2.p1.3.m3.2.43">a</mi><mi id="S3.I1.i2.p1.3.m3.2.44">n</mi><mi id="S3.I1.i2.p1.3.m3.2.45">g</mi><mi id="S3.I1.i2.p1.3.m3.2.46">e</mi><mrow id="S3.I1.i2.p1.3.m3.2.47"><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.47.1">(</mo><msub id="S3.I1.i2.p1.3.m3.2.47.2"><mi id="S3.I1.i2.p1.3.m3.2.47.2.2">p</mi><mi id="S3.I1.i2.p1.3.m3.2.47.2.3">i</mi></msub><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.47.3">)</mo></mrow><mo id="S3.I1.i2.p1.3.m3.2.48">=</mo><mi id="S3.I1.i2.p1.3.m3.2.49">d</mi><mi id="S3.I1.i2.p1.3.m3.2.50">o</mi><mi id="S3.I1.i2.p1.3.m3.2.51">m</mi><mi id="S3.I1.i2.p1.3.m3.2.52">a</mi><mi id="S3.I1.i2.p1.3.m3.2.53">i</mi><mi id="S3.I1.i2.p1.3.m3.2.54">n</mi><mrow id="S3.I1.i2.p1.3.m3.2.55"><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.55.1">(</mo><msub id="S3.I1.i2.p1.3.m3.2.55.2"><mi id="S3.I1.i2.p1.3.m3.2.55.2.2">p</mi><mrow id="S3.I1.i2.p1.3.m3.2.55.2.3"><mi id="S3.I1.i2.p1.3.m3.2.55.2.3.2">i</mi><mo id="S3.I1.i2.p1.3.m3.2.55.2.3.1">+</mo><mn id="S3.I1.i2.p1.3.m3.2.55.2.3.3">1</mn></mrow></msub><mo stretchy="false" id="S3.I1.i2.p1.3.m3.2.55.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.2c">\forall c_{1},c_{2}\in C,\;\exists p_{1},...,pj,...,p_{n}\in P:domain(p_{1})=c_{1}\wedge range(p_{n})=c_{2}\wedge\bigwedge\limits_{i=1}^{n-1}range(p_{i})=domain(p_{i+1})</annotation></semantics></math></p>
</div>
</li>
</ol>
<p id="S3.SS1.p4.2" class="ltx_p">Alternatively, we can also view CKG graphlets as (a) a special type of connected graph patterns (according to the SPARQL algebra), where variables occur in the positions of concrete instances and literals, or (b) as specific sets of SHACL shapes.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Injecting Query-specific Context Knowledge into the Prompts</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our method is based on injecting query-specific context knowledge alongside task-specific context into LLMs prompts to empower them for tasks with sparse knowledge in the foundational models.
We exploit the unique capabilities of CKGs, which represent knowledge not just as isolated facts but as interconnected patterns called graphlets.
These graphlets can capture more complex concepts and nuanced relationships, providing a richer scaffolding for the LLM‚Äôs reasoning process.
Our rationale is to inject query-specific contexts and verified task-driven knowledge with a structured hierarchy from a CKG, such that when both are incorporated into the prompts, the LLMs can achieve increased performance on tasks with sparse prior knowledge or limited training without requiring further fine-tuning.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For this experiment, we utilized a multi-stage pipeline (cf. <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>) to inject query-specific context knowledge into the prompts:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">CKG Knowledge Extraction:</span> We tap into the vast knowledge reservoir of a CKG, extracting relevant graphlets that align with the specific scholarly task. This goes beyond simple fact retrieval, capturing the intricate cognitive structures and relationships within the domain.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">Contextual Grounding:</span> We leverage abstracts from the CORE dataset¬†<cite class="ltx_cite ltx_citemacro_cite">Knoth <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> as factual summaries, providing rich contextual grounding for the subsequent prompt construction.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">Knowledge-Infused CoT Prompts:</span> The existing CoT prompt framework, a proven way to guide the LLM‚Äôs reasoning, is enriched with the extracted CKG graphlets. This new step aims to craft prompts that not only guide the LLM‚Äôs thought process but also equip it with the precise domain-specific knowledge needed for accurate and insightful responses.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_italic">Targeted Knowledge Retrieval:</span> Precise SPARQL queries act as filters, sifting through the vast CKG and extracting only the most relevant information pertinent to each specific prompt. This ensures that the injected knowledge directly aligns with the task.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p"><span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_italic">Task-Aware Prefixes:</span> To explore the impact of task identification, we utilized optional prefixes attached to the knowledge-driven prompts. These prefixes explicitly signal the scholarly domain and task nature, thus potentially further enhancing the LLM‚Äôs ability to focus its reasoning and deliver even more refined responses.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">By systematically constructing this knowledge-driven pipeline, we aim to improve the performance of LLMs for complex tasks with sparse domain knowledge.
Injecting query-specific context enriched with structured knowledge from CKGs empowers LLMs to achieve increased performance without the need for additional fine-tuning.
This approach harnesses the power of LLMs while equipping them with the precise domain-specific knowledge they need to excel in challenging domains with sparse domain knowledge such as scholarly communication.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">Train Data</span></th>
<th id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">Test Data</span></th>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Research Field</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">1,894</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">100</td>
</tr>
<tr id="S3.T1.2.3.3" class="ltx_tr">
<td id="S3.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">List of Predicates</td>
<td id="S3.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_bb">1,740</td>
<td id="S3.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_bb">100</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">Evaluation dataset comprising research field annotations and predicates for research contribution descriptions.</span></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Prompt Style</span></th>
<th id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">MAS (RF)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.1" class="ltx_tr">
<td id="S3.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Zero-Shot Prompting</td>
<td id="S3.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">67%</td>
</tr>
<tr id="S3.T2.2.3.2" class="ltx_tr">
<td id="S3.T2.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Few-Shot Prompting</td>
<td id="S3.T2.2.3.2.2" class="ltx_td ltx_align_center">68%</td>
</tr>
<tr id="S3.T2.2.4.3" class="ltx_tr">
<td id="S3.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_r">Chain-of-Thought Prompting</td>
<td id="S3.T2.2.4.3.2" class="ltx_td ltx_align_center">69%</td>
</tr>
<tr id="S3.T2.2.5.4" class="ltx_tr">
<td id="S3.T2.2.5.4.1" class="ltx_td ltx_align_center ltx_border_r">Zero-shot COT Prompting</td>
<td id="S3.T2.2.5.4.2" class="ltx_td ltx_align_center">73%</td>
</tr>
<tr id="S3.T2.2.6.5" class="ltx_tr">
<td id="S3.T2.2.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">IQCK into COT Prompting</td>
<td id="S3.T2.2.6.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.6.5.2.1" class="ltx_text ltx_font_bold">76%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of our Injecting Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction task (RF) with Mistral 7b. CoT shares the same input as Few-Shot, with the only difference being the inclusion of additional examples; MAS = Mean Average Score.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>LLM Fine-tuning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to increase the LLM performance, we intertwine prompt knowledge injection and fine-tuning leveraging CKGs, specifically in scenarios where the LLM lacks domain-specific knowledge for a task.
Enriching prompts with query-specific context is powerful, but for domain-specific tasks, injecting additional domain-specific knowledge through fine-tuning might further increase performance.
This process not only supplements the LLM knowledge but integrates its interaction, expanding its capabilities, especially for domain-specific tasks.
When faced with domain knowledge gaps, targeted fine-tuning with external domain-specific knowledge proves effective in unlocking the full potential of the LLM. Domain-specific knowledge from a knowledge base like CKG, verified by domain experts, equips the LLM for navigating domain-driven challenges. The LoRA¬†<cite class="ltx_cite ltx_citemacro_cite">Hu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> attention mechanism efficiently handles intricate details in domain-specific knowledge, ensuring focused involvement. To optimize the resource constraints, quantization¬†<cite class="ltx_cite ltx_citemacro_cite">Xu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> is utilized, and the TRL trainer¬†<cite class="ltx_cite ltx_citemacro_cite">von Werra <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> orchestrates the fine-tuning process by setting optimal parameters to overcome learning challenges.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">AI (gpt-4-1106-preview) Perspective on LLM (Pre-trained and Fine-tuned) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T3.st1.3.2" class="ltx_text" style="font-size:90%;">Table 2.1: AI Perspective on LLM (Pre-trained) Evaluation.</span></figcaption>
<table id="S4.T3.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.st1.4.1.1" class="ltx_tr">
<th id="S4.T3.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="S4.T3.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Prefix</span></th>
<th id="S4.T3.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T3.st1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st1.4.1.1.4.1" class="ltx_text ltx_font_bold">MAS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.st1.4.2.1" class="ltx_tr">
<td id="S4.T3.st1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.2.1.1.1" class="ltx_text">LP</span></td>
<td id="S4.T3.st1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.2.1.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T3.st1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Gemini pro</td>
<td id="S4.T3.st1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st1.4.2.1.4.1" class="ltx_text ltx_font_bold">64%</span></td>
</tr>
<tr id="S4.T3.st1.4.3.2" class="ltx_tr">
<td id="S4.T3.st1.4.3.2.1" class="ltx_td ltx_align_center">Llama 2 13b</td>
<td id="S4.T3.st1.4.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.st1.4.3.2.2.1" class="ltx_text ltx_font_bold">64%</span></td>
</tr>
<tr id="S4.T3.st1.4.4.3" class="ltx_tr">
<td id="S4.T3.st1.4.4.3.1" class="ltx_td ltx_align_center">Mistral 7b</td>
<td id="S4.T3.st1.4.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.st1.4.4.3.2.1" class="ltx_text ltx_font_bold">64%</span></td>
</tr>
<tr id="S4.T3.st1.4.5.4" class="ltx_tr">
<td id="S4.T3.st1.4.5.4.1" class="ltx_td ltx_align_center">Llama 2 7b</td>
<td id="S4.T3.st1.4.5.4.2" class="ltx_td ltx_align_center">59%</td>
</tr>
<tr id="S4.T3.st1.4.6.5" class="ltx_tr">
<td id="S4.T3.st1.4.6.5.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.6.5.1.1" class="ltx_text">LP</span></td>
<td id="S4.T3.st1.4.6.5.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.6.5.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T3.st1.4.6.5.3" class="ltx_td ltx_align_center ltx_border_t">Llama 2 13b</td>
<td id="S4.T3.st1.4.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st1.4.6.5.4.1" class="ltx_text ltx_font_bold">65%</span></td>
</tr>
<tr id="S4.T3.st1.4.7.6" class="ltx_tr">
<td id="S4.T3.st1.4.7.6.1" class="ltx_td ltx_align_center">Mistral 7b</td>
<td id="S4.T3.st1.4.7.6.2" class="ltx_td ltx_align_center">64%</td>
</tr>
<tr id="S4.T3.st1.4.8.7" class="ltx_tr">
<td id="S4.T3.st1.4.8.7.1" class="ltx_td ltx_align_center">Llama 2 7b</td>
<td id="S4.T3.st1.4.8.7.2" class="ltx_td ltx_align_center">63%</td>
</tr>
<tr id="S4.T3.st1.4.9.8" class="ltx_tr">
<td id="S4.T3.st1.4.9.8.1" class="ltx_td ltx_align_center">Gemini pro</td>
<td id="S4.T3.st1.4.9.8.2" class="ltx_td ltx_align_center">62%</td>
</tr>
<tr id="S4.T3.st1.4.10.9" class="ltx_tr">
<td id="S4.T3.st1.4.10.9.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.10.9.1.1" class="ltx_text">RF</span></td>
<td id="S4.T3.st1.4.10.9.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.10.9.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T3.st1.4.10.9.3" class="ltx_td ltx_align_center ltx_border_t">Mistral 7b</td>
<td id="S4.T3.st1.4.10.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st1.4.10.9.4.1" class="ltx_text ltx_font_bold">79%</span></td>
</tr>
<tr id="S4.T3.st1.4.11.10" class="ltx_tr">
<td id="S4.T3.st1.4.11.10.1" class="ltx_td ltx_align_center">Gemini pro</td>
<td id="S4.T3.st1.4.11.10.2" class="ltx_td ltx_align_center">78%</td>
</tr>
<tr id="S4.T3.st1.4.12.11" class="ltx_tr">
<td id="S4.T3.st1.4.12.11.1" class="ltx_td ltx_align_center">Llama 2 13b</td>
<td id="S4.T3.st1.4.12.11.2" class="ltx_td ltx_align_center">62%</td>
</tr>
<tr id="S4.T3.st1.4.13.12" class="ltx_tr">
<td id="S4.T3.st1.4.13.12.1" class="ltx_td ltx_align_center">Llama 2 7b</td>
<td id="S4.T3.st1.4.13.12.2" class="ltx_td ltx_align_center">50%</td>
</tr>
<tr id="S4.T3.st1.4.14.13" class="ltx_tr">
<td id="S4.T3.st1.4.14.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.14.13.1.1" class="ltx_text">RF</span></td>
<td id="S4.T3.st1.4.14.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T3.st1.4.14.13.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T3.st1.4.14.13.3" class="ltx_td ltx_align_center ltx_border_t">Gemini pro</td>
<td id="S4.T3.st1.4.14.13.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st1.4.14.13.4.1" class="ltx_text ltx_font_bold">80%</span></td>
</tr>
<tr id="S4.T3.st1.4.15.14" class="ltx_tr">
<td id="S4.T3.st1.4.15.14.1" class="ltx_td ltx_align_center">Mistral 7b</td>
<td id="S4.T3.st1.4.15.14.2" class="ltx_td ltx_align_center">77%</td>
</tr>
<tr id="S4.T3.st1.4.16.15" class="ltx_tr">
<td id="S4.T3.st1.4.16.15.1" class="ltx_td ltx_align_center">Llama 2 13b</td>
<td id="S4.T3.st1.4.16.15.2" class="ltx_td ltx_align_center">61%</td>
</tr>
<tr id="S4.T3.st1.4.17.16" class="ltx_tr">
<td id="S4.T3.st1.4.17.16.1" class="ltx_td ltx_align_center ltx_border_bb">Llama 2 7b</td>
<td id="S4.T3.st1.4.17.16.2" class="ltx_td ltx_align_center ltx_border_bb">43%</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T3.st2.3.2" class="ltx_text" style="font-size:90%;">Table 2.2: AI Perspective on LLM (Fine-tuned) Evaluation.</span></figcaption>
<table id="S4.T3.st2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.st2.4.1.1" class="ltx_tr">
<th id="S4.T3.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="S4.T3.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">Prefix</span></th>
<th id="S4.T3.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st2.4.1.1.3.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T3.st2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.st2.4.1.1.4.1" class="ltx_text ltx_font_bold">MAS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.st2.4.2.1" class="ltx_tr">
<td id="S4.T3.st2.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.2.1.1.1" class="ltx_text">LP</span></td>
<td id="S4.T3.st2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.2.1.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T3.st2.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_13b</td>
<td id="S4.T3.st2.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st2.4.2.1.4.1" class="ltx_text ltx_font_bold">67%</span></td>
</tr>
<tr id="S4.T3.st2.4.3.2" class="ltx_tr">
<td id="S4.T3.st2.4.3.2.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T3.st2.4.3.2.2" class="ltx_td ltx_align_center">42%</td>
</tr>
<tr id="S4.T3.st2.4.4.3" class="ltx_tr">
<td id="S4.T3.st2.4.4.3.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T3.st2.4.4.3.2" class="ltx_td ltx_align_center">42%</td>
</tr>
<tr id="S4.T3.st2.4.5.4" class="ltx_tr">
<td id="S4.T3.st2.4.5.4.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.5.4.1.1" class="ltx_text">LP</span></td>
<td id="S4.T3.st2.4.5.4.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.5.4.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T3.st2.4.5.4.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_13b</td>
<td id="S4.T3.st2.4.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st2.4.5.4.4.1" class="ltx_text ltx_font_bold">57%</span></td>
</tr>
<tr id="S4.T3.st2.4.6.5" class="ltx_tr">
<td id="S4.T3.st2.4.6.5.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T3.st2.4.6.5.2" class="ltx_td ltx_align_center">52%</td>
</tr>
<tr id="S4.T3.st2.4.7.6" class="ltx_tr">
<td id="S4.T3.st2.4.7.6.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T3.st2.4.7.6.2" class="ltx_td ltx_align_center">37%</td>
</tr>
<tr id="S4.T3.st2.4.8.7" class="ltx_tr">
<td id="S4.T3.st2.4.8.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.8.7.1.1" class="ltx_text">RF</span></td>
<td id="S4.T3.st2.4.8.7.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.8.7.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T3.st2.4.8.7.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_2_7b</td>
<td id="S4.T3.st2.4.8.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st2.4.8.7.4.1" class="ltx_text ltx_font_bold">42%</span></td>
</tr>
<tr id="S4.T3.st2.4.9.8" class="ltx_tr">
<td id="S4.T3.st2.4.9.8.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T3.st2.4.9.8.2" class="ltx_td ltx_align_center">38%</td>
</tr>
<tr id="S4.T3.st2.4.10.9" class="ltx_tr">
<td id="S4.T3.st2.4.10.9.1" class="ltx_td ltx_align_center">ORKG_Llama_13b</td>
<td id="S4.T3.st2.4.10.9.2" class="ltx_td ltx_align_center">13%</td>
</tr>
<tr id="S4.T3.st2.4.11.10" class="ltx_tr">
<td id="S4.T3.st2.4.11.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.11.10.1.1" class="ltx_text">RF</span></td>
<td id="S4.T3.st2.4.11.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T3.st2.4.11.10.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T3.st2.4.11.10.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_13b</td>
<td id="S4.T3.st2.4.11.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.st2.4.11.10.4.1" class="ltx_text ltx_font_bold">49%</span></td>
</tr>
<tr id="S4.T3.st2.4.12.11" class="ltx_tr">
<td id="S4.T3.st2.4.12.11.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T3.st2.4.12.11.2" class="ltx_td ltx_align_center">46%</td>
</tr>
<tr id="S4.T3.st2.4.13.12" class="ltx_tr">
<td id="S4.T3.st2.4.13.12.1" class="ltx_td ltx_align_center ltx_border_bb">ORKG_Llama_2_7b</td>
<td id="S4.T3.st2.4.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">19%</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Human Perspective on LLM (Pre-trained and Fine-tune) Evaluation; MAS = Mean Average Score; LP = List of predicates recommendation, RF = Research field prediction.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T4.st1.3.2" class="ltx_text" style="font-size:90%;">Table 3.1: Human Perspective on LLM (Pre-trained) Evaluation.</span></figcaption>
<table id="S4.T4.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.st1.4.1.1" class="ltx_tr">
<th id="S4.T4.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="S4.T4.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Prefix</span></th>
<th id="S4.T4.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T4.st1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st1.4.1.1.4.1" class="ltx_text ltx_font_bold">MAS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.st1.4.2.1" class="ltx_tr">
<td id="S4.T4.st1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.2.1.1.1" class="ltx_text">LP</span></td>
<td id="S4.T4.st1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.2.1.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T4.st1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Llama_2_13b</td>
<td id="S4.T4.st1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st1.4.2.1.4.1" class="ltx_text ltx_font_bold">60%</span></td>
</tr>
<tr id="S4.T4.st1.4.3.2" class="ltx_tr">
<td id="S4.T4.st1.4.3.2.1" class="ltx_td ltx_align_center">Mistral_7b</td>
<td id="S4.T4.st1.4.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.3.2.2.1" class="ltx_text ltx_font_bold">60%</span></td>
</tr>
<tr id="S4.T4.st1.4.4.3" class="ltx_tr">
<td id="S4.T4.st1.4.4.3.1" class="ltx_td ltx_align_center">Llama_2_7b</td>
<td id="S4.T4.st1.4.4.3.2" class="ltx_td ltx_align_center">59%</td>
</tr>
<tr id="S4.T4.st1.4.5.4" class="ltx_tr">
<td id="S4.T4.st1.4.5.4.1" class="ltx_td ltx_align_center">Gemini_pro</td>
<td id="S4.T4.st1.4.5.4.2" class="ltx_td ltx_align_center">57%</td>
</tr>
<tr id="S4.T4.st1.4.6.5" class="ltx_tr">
<td id="S4.T4.st1.4.6.5.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.6.5.1.1" class="ltx_text">LP</span></td>
<td id="S4.T4.st1.4.6.5.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.6.5.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T4.st1.4.6.5.3" class="ltx_td ltx_align_center ltx_border_t">Llama_2_13b</td>
<td id="S4.T4.st1.4.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st1.4.6.5.4.1" class="ltx_text ltx_font_bold">60%</span></td>
</tr>
<tr id="S4.T4.st1.4.7.6" class="ltx_tr">
<td id="S4.T4.st1.4.7.6.1" class="ltx_td ltx_align_center">Llama_2_7b</td>
<td id="S4.T4.st1.4.7.6.2" class="ltx_td ltx_align_center">59%</td>
</tr>
<tr id="S4.T4.st1.4.8.7" class="ltx_tr">
<td id="S4.T4.st1.4.8.7.1" class="ltx_td ltx_align_center">Mistral_7b</td>
<td id="S4.T4.st1.4.8.7.2" class="ltx_td ltx_align_center">59%</td>
</tr>
<tr id="S4.T4.st1.4.9.8" class="ltx_tr">
<td id="S4.T4.st1.4.9.8.1" class="ltx_td ltx_align_center">Gemini_pro</td>
<td id="S4.T4.st1.4.9.8.2" class="ltx_td ltx_align_center">58%</td>
</tr>
<tr id="S4.T4.st1.4.10.9" class="ltx_tr">
<td id="S4.T4.st1.4.10.9.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.10.9.1.1" class="ltx_text">RF</span></td>
<td id="S4.T4.st1.4.10.9.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.10.9.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T4.st1.4.10.9.3" class="ltx_td ltx_align_center ltx_border_t">Gemini_pro</td>
<td id="S4.T4.st1.4.10.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st1.4.10.9.4.1" class="ltx_text ltx_font_bold">78%</span></td>
</tr>
<tr id="S4.T4.st1.4.11.10" class="ltx_tr">
<td id="S4.T4.st1.4.11.10.1" class="ltx_td ltx_align_center">Mistral_7b</td>
<td id="S4.T4.st1.4.11.10.2" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.11.10.2.1" class="ltx_text ltx_font_bold">78%</span></td>
</tr>
<tr id="S4.T4.st1.4.12.11" class="ltx_tr">
<td id="S4.T4.st1.4.12.11.1" class="ltx_td ltx_align_center">Llama_2_13b</td>
<td id="S4.T4.st1.4.12.11.2" class="ltx_td ltx_align_center">70%</td>
</tr>
<tr id="S4.T4.st1.4.13.12" class="ltx_tr">
<td id="S4.T4.st1.4.13.12.1" class="ltx_td ltx_align_center">Llama_2_7b</td>
<td id="S4.T4.st1.4.13.12.2" class="ltx_td ltx_align_center">49%</td>
</tr>
<tr id="S4.T4.st1.4.14.13" class="ltx_tr">
<td id="S4.T4.st1.4.14.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.14.13.1.1" class="ltx_text">RF</span></td>
<td id="S4.T4.st1.4.14.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.14.13.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T4.st1.4.14.13.3" class="ltx_td ltx_align_center ltx_border_t">Mistral_7b</td>
<td id="S4.T4.st1.4.14.13.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st1.4.14.13.4.1" class="ltx_text ltx_font_bold">82%</span></td>
</tr>
<tr id="S4.T4.st1.4.15.14" class="ltx_tr">
<td id="S4.T4.st1.4.15.14.1" class="ltx_td ltx_align_center">Gemini_pro</td>
<td id="S4.T4.st1.4.15.14.2" class="ltx_td ltx_align_center">75%</td>
</tr>
<tr id="S4.T4.st1.4.16.15" class="ltx_tr">
<td id="S4.T4.st1.4.16.15.1" class="ltx_td ltx_align_center">Llama_2_13b</td>
<td id="S4.T4.st1.4.16.15.2" class="ltx_td ltx_align_center">66%</td>
</tr>
<tr id="S4.T4.st1.4.17.16" class="ltx_tr">
<td id="S4.T4.st1.4.17.16.1" class="ltx_td ltx_align_center ltx_border_bb">Llama_2_7b</td>
<td id="S4.T4.st1.4.17.16.2" class="ltx_td ltx_align_center ltx_border_bb">56%</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T4.st2.3.2" class="ltx_text" style="font-size:90%;">Table 3.2: Human Perspective on LLM (Fine-trained) Evaluation.</span></figcaption>
<table id="S4.T4.st2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.st2.4.1.1" class="ltx_tr">
<th id="S4.T4.st2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="S4.T4.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">Prefix</span></th>
<th id="S4.T4.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st2.4.1.1.3.1" class="ltx_text ltx_font_bold">LLM</span></th>
<th id="S4.T4.st2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.st2.4.1.1.4.1" class="ltx_text ltx_font_bold">MAS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.st2.4.2.1" class="ltx_tr">
<td id="S4.T4.st2.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.2.1.1.1" class="ltx_text">LP</span></td>
<td id="S4.T4.st2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.2.1.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T4.st2.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Mistral_7B</td>
<td id="S4.T4.st2.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st2.4.2.1.4.1" class="ltx_text ltx_font_bold">53%</span></td>
</tr>
<tr id="S4.T4.st2.4.3.2" class="ltx_tr">
<td id="S4.T4.st2.4.3.2.1" class="ltx_td ltx_align_center">ORKG_Llama_13b</td>
<td id="S4.T4.st2.4.3.2.2" class="ltx_td ltx_align_center">50%</td>
</tr>
<tr id="S4.T4.st2.4.4.3" class="ltx_tr">
<td id="S4.T4.st2.4.4.3.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T4.st2.4.4.3.2" class="ltx_td ltx_align_center">45%</td>
</tr>
<tr id="S4.T4.st2.4.5.4" class="ltx_tr">
<td id="S4.T4.st2.4.5.4.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.5.4.1.1" class="ltx_text">LP</span></td>
<td id="S4.T4.st2.4.5.4.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.5.4.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T4.st2.4.5.4.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_13b</td>
<td id="S4.T4.st2.4.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st2.4.5.4.4.1" class="ltx_text ltx_font_bold">60%</span></td>
</tr>
<tr id="S4.T4.st2.4.6.5" class="ltx_tr">
<td id="S4.T4.st2.4.6.5.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T4.st2.4.6.5.2" class="ltx_td ltx_align_center">47%</td>
</tr>
<tr id="S4.T4.st2.4.7.6" class="ltx_tr">
<td id="S4.T4.st2.4.7.6.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T4.st2.4.7.6.2" class="ltx_td ltx_align_center">46%</td>
</tr>
<tr id="S4.T4.st2.4.8.7" class="ltx_tr">
<td id="S4.T4.st2.4.8.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.8.7.1.1" class="ltx_text">RF</span></td>
<td id="S4.T4.st2.4.8.7.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.8.7.2.1" class="ltx_text">With Prefix</span></td>
<td id="S4.T4.st2.4.8.7.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Llama_13b</td>
<td id="S4.T4.st2.4.8.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st2.4.8.7.4.1" class="ltx_text ltx_font_bold">50%</span></td>
</tr>
<tr id="S4.T4.st2.4.9.8" class="ltx_tr">
<td id="S4.T4.st2.4.9.8.1" class="ltx_td ltx_align_center">ORKG_Mistral_7B</td>
<td id="S4.T4.st2.4.9.8.2" class="ltx_td ltx_align_center">48%</td>
</tr>
<tr id="S4.T4.st2.4.10.9" class="ltx_tr">
<td id="S4.T4.st2.4.10.9.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T4.st2.4.10.9.2" class="ltx_td ltx_align_center">33%</td>
</tr>
<tr id="S4.T4.st2.4.11.10" class="ltx_tr">
<td id="S4.T4.st2.4.11.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.11.10.1.1" class="ltx_text">RF</span></td>
<td id="S4.T4.st2.4.11.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T4.st2.4.11.10.2.1" class="ltx_text">Without Prefix</span></td>
<td id="S4.T4.st2.4.11.10.3" class="ltx_td ltx_align_center ltx_border_t">ORKG_Mistral_7B</td>
<td id="S4.T4.st2.4.11.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.st2.4.11.10.4.1" class="ltx_text ltx_font_bold">55%</span></td>
</tr>
<tr id="S4.T4.st2.4.12.11" class="ltx_tr">
<td id="S4.T4.st2.4.12.11.1" class="ltx_td ltx_align_center">ORKG_Llama_2_7b</td>
<td id="S4.T4.st2.4.12.11.2" class="ltx_td ltx_align_center">41%</td>
</tr>
<tr id="S4.T4.st2.4.13.12" class="ltx_tr">
<td id="S4.T4.st2.4.13.12.1" class="ltx_td ltx_align_center ltx_border_bb">ORKG_Llama_13b</td>
<td id="S4.T4.st2.4.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">18%</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Dataset.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">Our dataset originates from a large-scale CKG, the Open Research Knowledge Graph (ORKG), encompassing diverse research fields and a comprehensive list of predicates.
To enhance query-specific content injection and facilitate fine-tuning, we utilized 1,894 and 1,740 graphlets related to the research fields and a list of predicates (<a href="#S3.T1" title="Table 1 ‚Ä£ 3.2 Injecting Query-specific Context Knowledge into the Prompts ‚Ä£ 3 Method ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†1</span></a>), respectively.
This dataset creates a knowledge-rich environment for the LLMs, with a comprehensive understanding of various academic fields.
Additionally, we incorporated 100 graphlets in the test set, each carefully curated to showcase a list of research fields and list of predicates.
These graphlets encapsulate the essence of their respective fields, emphasizing key concepts with a list of predicates.
The curated CKG graphlets dataset is employed to unveil the true potential of LLMs when faced with intricate scholarly tasks.
The choice of 100 graphlets for the test set serves multiple purposes.
It not only enables us to assess the LLM‚Äôs performance through automated-generated metrics but also facilitates a rigorous human validation process¬†<cite class="ltx_cite ltx_citemacro_cite">Shen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>¬†<cite class="ltx_cite ltx_citemacro_cite">Chew <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.
By comparing the LLM‚Äôs generated responses with the judgments of human evaluators, we gain a deeper understanding of its strengths and weaknesses.
This cross-validation approach, integrating AI and human evaluation, ensures a comprehensive and granular assessment of the LLM‚Äôs capabilities, ultimately paving the way for its further development and refinement.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation criteria</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">The evaluation criteria encompass two primary dimensions: (1) LLM-based evaluation¬†<cite class="ltx_cite ltx_citemacro_cite">Liusie <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>¬†<cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> and (2) human-based evaluation¬†<cite class="ltx_cite ltx_citemacro_cite">Wu and Aji (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>¬†<cite class="ltx_cite ltx_citemacro_cite">Shen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>.
In LLM-based evaluation, the research field prediction is assessed based on clarity, coverage, relevance to the research field predicted by a domain expert, and granularity of the specified LLM-generated research field, each scored within a range of 0-3.
Similarly, the LLM‚Äôs list of predicate recommendations is evaluated considering clarity, coverage, relevance for domain expert recommended contexts for a scholarly article, granularity, and the LLM‚Äôs ability to recognize all contexts, each scored within the same 0-3 scale.
Human-based evaluation mirrors LLM-based criteria, ensuring a parallel human assessment of clarity, coverage, relevance, and granularity in both research field categorization and a list of predicate recommendations.
This approach aims to comprehensively evaluate the LLM‚Äôs performance, combining automated metrics with human judgment to provide valuable insights and an in-depth understanding of its strengths and weaknesses in scholarly-related tasks.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Tasks for Tuning</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">This research explores two different approaches for injecting query-specific knowledge into prompts: task-independent and task-driven variants.
By employing these two distinct prompt variants in our experiment, we assess the strengths and weaknesses associated with task-independent and task-driven approaches, analyzing how LLMs handle different graphlet structures.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Task-Independent Prompts:</span> We extract query-specific context from the CKG without imposing any task-related constraints. LLMs equipped with general knowledge can readily address diverse tasks. Additionally, the LLM undergoes fine-tuning with this general CKG knowledge, further enhancing its understanding of underlying concepts and relationships.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Task-Driven Prompts:</span> In the second approach, we harness the power of CKGs to formulate task-specific prompts, explicitly incorporating task-oriented prefixes that guide the LLM‚Äôs reasoning toward the desired outcome. For instance, in a research field prediction task, the prompt might commence with ‚ÄúResearch field prediction‚Äù along with all the pertinent CKG-derived context. This targeted approach aims to improve the LLM‚Äôs precision in applying its knowledge to the specific task.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We performed a comparative analysis of different prompt engineering approaches (<a href="#S3.T2" title="Table 2 ‚Ä£ 3.2 Injecting Query-specific Context Knowledge into the Prompts ‚Ä£ 3 Method ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†2</span></a>) and LLM performance for both pre-trained (<a href="#S4.T3.st1" title="3(a) ‚Ä£ Table 3 ‚Ä£ 4 Experimental Setup ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>) and fine-tuned (<a href="#S4.T3.st2" title="3(b) ‚Ä£ Table 3 ‚Ä£ 4 Experimental Setup ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>) models across various tasks, which helps to clarify the capabilities and limitations of these LLMs regarding the CKG knowledge injection approach.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The comparison of our Injection of Query-specific Context Knowledge (IQCK) method with four baseline methods (for a sample set of the CKG graphlets) for the research field prediction task (RF) with Mistral 7b is shown in <a href="#S3.T2" title="Table 2 ‚Ä£ 3.2 Injecting Query-specific Context Knowledge into the Prompts ‚Ä£ 3 Method ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†2</span></a>.
For the baseline methods, we tried different prompting styles but omitted the injection of a research field hierarchy to choose predictions from.
We can observe, that IQCK with the injection of the research field hierarchies into the prompt as contextual knowledge significantly improves the accuracy of the research field prediction to 76% from the 67-73% achieved with the baseline methods.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The evaluation of pre-trained LLMs (<a href="#S4.T3.st1" title="3(a) ‚Ä£ Table 3 ‚Ä£ 4 Experimental Setup ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), including Gemini_pro (without and with Prefix) and Mistral_7b (without and with Prefix) shows impressive mean scores, signifying their capabilities in capturing and leveraging query-specific context knowledge from within the prompts.
Specifically, Gemini_pro (with Prefix) achieved a remarkable mean score of 80% (GPT4 evaluation), positioning it as a top performer in this task.
Mistral_7b (without Prefix) follows closely with a substantial mean score of 79% (GPT4 evaluation) and 60% (human evaluation), demonstrating robust competency.
The models Llama_2_13b (without and with Prefix) and Llama_2_7b (without and with Prefix) achieve lower mean average scores than others by the GPT4 evaluation and are similarly scored by human evaluation.
This pattern indicates that the incorporation of injecting query-specific context knowledge during the prompt engineering phase provides these LLMs with a solid foundation for understanding and recommending research fields.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The evaluation of fine-tuned LLMs (<a href="#S4.T3.st2" title="3(b) ‚Ä£ Table 3 ‚Ä£ 4 Experimental Setup ‚Ä£ Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>) in the research field prediction reveals different dynamics.
For instance, ORKG_Llama_2_13b (With Prefix) achieves a mean score of 49% (GPT4 evaluation), showing competitive performance with regard to other fine-tuned LLMs but falling short from the previous approach.
Similarly, ORKG_Llama_13b (Without Prefix) and ORKG_Mistral_7B (With Prefix) achieve mean scores of 42% (GPT4 evaluation) and 46% (GPT4 evaluation), respectively, suggesting a noteworthy but comparatively lower proficiency.
This variance scores between pre-trained and fine-tuned LLMs underscores the substantial impact of injecting query-specific context knowledge in the pre-train LLMs for research field prediction tasks.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">For the list of predicate recommendations, Llama_2_13b (with Prefix) achieved a mean score of 65% (GPT4 evaluation) and 60% (human evaluation), emphasizing its proficiency in suggesting predicate labels.
Llama_2_7b (With Prefix) follows closely with a mean score of 63% (GPT4 evaluation), while Llama_2_13b (With Prefix) and Mistral_7b (With Prefix) achieve mean scores of 63% (GPT4 evaluation) and 64% (GPT4 evaluation) which is also quite similar to a human evaluation, respectively.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Regarding the fine-tuned LLMs in the list of predicates recommendation unveils a more diverse scenario. ORKG_Llama_2_13b (Without Prefix) achieves 67% of the mean average score, suggesting that fine-tuning can yield competitive results in this task.
However, ORKG_Llama_13b (With Prefix) and ORKG_Mistral_7B (With Prefix) experience lower mean scores, with 57% and 52%, respectively.
This variability in performance indicates that the effectiveness of fine-tuned models in predicate label recommendation is more contingent on specific training conditions, and adding a prefix seems to introduce additional complexity.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">This comprehensive analysis establishes a solid foundation for LLMs, particularly in the scholarly domain for tasks like research field prediction and list of predicate recommendations.
The injection of query-specific context knowledge into the prompts from CKG improves the LLM performance and also provides a robust understanding of complex relationship prompts and LLMs.
Fine-tuned LLMs showcase competitive performance, especially for list of predicate recommendations, highlighting the relation between the LLMs and fine-tuned requirements.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In order to evaluate our approach, we utilized a domain expert to verify data using the previously described approach.
Although the domain expert verified data contains human curated data, it can be misleading to determine to performance of our models.
For the research field prediction task, the domain expert verified data only includes a single field per article.
In the case of interdisciplinary fields, still, only a single field is contained.
Consequently, the LLM recommendation can be correct based on the contents of the title and abstract but is evaluated as incorrect because of the domain expert-verified data.
For the list of predicate recommendations, the domain expert verified data only includes a subset of relevant predicates, since knowledge descriptions in the CKG are generally incomplete.
Therefore, an LLM-recommended list of properties might be relevant based on the title and abstract, but evaluated as incorrect when comparing them to the domain expert-verified data.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, there are several limitations regarding the human evaluation of the data.
The evaluators were instructed on how to score the outcomes, but only high-level guidelines were provided.
Therefore, scores might vary between different articles.
Despite this shortcoming, the human evaluation still provides valuable insights, especially in comparison with the machine evaluation.
Another aspect of the human evaluation is the heterogeneous set of research domains.
Since our approach does not focus on specific domains, the set of selected articles comes from a variety of domains.
The human evaluators were not experts in all domains and therefore had to judge the results to the best of their ability.
However, we believe that a high-level assessment of the resulting LLM responses is also possible without domain knowledge, albeit in lesser granularity.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In addition to the two tasks performed in our approach, we plan to extend this to various other tasks in future work.
In addition to the prediction of predicates, we explored the prediction of objects to form complete contribution description facts.
However, further work is required to perform this task at the desired accuracy. In the future, we also plan to continue working on this specific task by leveraging other approaches, such as fine-tuning the LLMs for each specific task and increasing the data from the CKG which needs to be collected from the domain experts.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work is part of a larger research agenda aiming at creating a comprehensive neuro-symbolic system for describing research contributions in a cognitive knowledge graph ultimately giving rise to novel AI-based research assistance systems, which enable researchers to obtain comprehensive answers to research questions based on the recent corpus of scientific knowledge.
Due to the limitations of LLMs in extracting information in domains with sparse training data, we developed two methods for (1) injecting contextual information into prompts from a preexisting CKG and (2) fine-tuning LLMs with such knowledge for specific tasks.
Our evaluation showed, that in particular the first method is very well suited to improve the accuracy of LLMs for scholarly communication tasks.
More research is required to further improve the fine-tuning methods and expand the evaluation of the approach to further knowledge extraction and augmentation tasks, such as object prediction.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asher <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, and Soumya Paul.

</span>
<span class="ltx_bibblock">Limits for learning with language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.12213</span>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared¬†D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et¬†al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:1877‚Äì1901, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chew <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim.

</span>
<span class="ltx_bibblock">Llm-assisted content analysis: Using large language models to support deductive coding.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.14924</span>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhary and Reddy [2023]</span>
<span class="ltx_bibblock">
Nurendra Choudhary and Chandan¬†K Reddy.

</span>
<span class="ltx_bibblock">Complex logical reasoning over knowledge graphs using large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.01157</span>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D‚ÄôSouza <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Jennifer D‚ÄôSouza, Moussab Hrou, and S√∂ren Auer.

</span>
<span class="ltx_bibblock">Evaluating prompt-based question answering for object prediction in the open research knowledge graph.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">International Conference on Database and Expert Systems Applications</span>, pages 508‚Äì515. Springer, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Yu¬†Hou, Jeremy Yeung, Hua Xu, Chang Su, Fei Wang, and Rui Zhang.

</span>
<span class="ltx_bibblock">From answers to insights: Unveiling the strengths and limitations of chatgpt and biomedical knowledge graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">medRxiv</span>, pages 2023‚Äì06, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu and Levy [2023]</span>
<span class="ltx_bibblock">
Jennifer Hu and Roger Levy.

</span>
<span class="ltx_bibblock">Prompt-based methods may underestimate large language models‚Äô linguistic generalizations.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13264</span>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Edward¬†J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu¬†Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria.

</span>
<span class="ltx_bibblock">Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.01933</span>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaradeh <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Mohamad¬†Yaser Jaradeh, Allard Oelen, Kheir¬†Eddine Farfar, Manuel Prinz, Jennifer D‚ÄôSouza, G√°bor Kismih√≥k, Markus Stocker, and S√∂ren Auer.

</span>
<span class="ltx_bibblock">Open research knowledge graph: next generation infrastructure for semantic scholarly knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">Proceedings of the 10th International Conference on Knowledge Capture</span>, pages 243‚Äì246, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jungherr [2023]</span>
<span class="ltx_bibblock">
Andreas Jungherr.

</span>
<span class="ltx_bibblock">Using chatgpt and other large language model (llm) applications for academic paper assignments.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knoth <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Petr Knoth, Drahomira Herrmannova, Matteo Cancellieri, Lucas Anastasiou, Nancy Pontika, Samuel Pearce, Bikash Gyawali, and David Pride.

</span>
<span class="ltx_bibblock">Core: A global aggregation service for open access papers.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">Nature Scientific Data</span>, 10(1):366, June 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, and Xiaoming Zhai.

</span>
<span class="ltx_bibblock">Gemini pro defeated by gpt-4v: Evidence from education.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Jonathan Li, Will Aitken, Rohan Bhambhoria, and Xiaodan Zhu.

</span>
<span class="ltx_bibblock">Prefix propagation: Parameter-efficient tuning for long sequences.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.12086</span>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liusie <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Adian Liusie, Potsawee Manakul, and Mark¬†JF Gales.

</span>
<span class="ltx_bibblock">Zero-shot nlg evaluation through pairware comparisons with llms.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.07889</span>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milo <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2002]</span>
<span class="ltx_bibblock">
R.¬†Milo, S.¬†Shen-Orr, S.¬†Itzkovitz, N.¬†Kashtan, D.¬†Chklovskii, and U.¬†Alon.

</span>
<span class="ltx_bibblock">Network motifs: Simple building blocks of complex networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Science</span>, 298(5594):824‚Äì827, 2002.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng <span id="bib.bib17.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne.

</span>
<span class="ltx_bibblock">Knowledge graphs: Opportunities and challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">Artificial Intelligence Review</span>, pages 1‚Äì32, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pr≈æulj <span id="bib.bib18.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2004]</span>
<span class="ltx_bibblock">
N.¬†Pr≈æulj, D.¬†G. Corneil, and I.¬†Jurisica.

</span>
<span class="ltx_bibblock">Modeling interactome: scale-free or geometric?

</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">Bioinformatics</span>, 20(18):3508‚Äì3515, 07 2004.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pr≈æulj [2007]</span>
<span class="ltx_bibblock">
Nata≈°a Pr≈æulj.

</span>
<span class="ltx_bibblock">Biological network comparison using graphlet degree distribution.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Bioinformatics</span>, 23(2):e177‚Äìe183, 01 2007.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel <span id="bib.bib20.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter¬†J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 21(1):5485‚Äì5551, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing.

</span>
<span class="ltx_bibblock">Are large language models good evaluators for abstractive summarization?

</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13091</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Werra <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang.

</span>
<span class="ltx_bibblock">Trl: Transformer reinforcement learning.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/trl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/trl</a>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Haohui Wang, Yuzhen Mao, Jianhui Sun, Si¬†Zhang, and Dawei Zhou.

</span>
<span class="ltx_bibblock">Dynamic transfer learning across graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.00664</span>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span id="bib.bib24.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent¬†Y Zhao, Kelvin Guu, Adams¬†Wei Yu, Brian Lester, Nan Du, Andrew¬†M Dai, and Quoc¬†V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.01652</span>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed¬†Chi, Quoc¬†V Le, Denny Zhou, et¬†al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 35:24824‚Äì24837, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Aji [2023]</span>
<span class="ltx_bibblock">
Minghao Wu and Alham¬†Fikri Aji.

</span>
<span class="ltx_bibblock">Style over substance: Evaluation biases for large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.03025</span>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, and Qi¬†Tian.

</span>
<span class="ltx_bibblock">Qa-lora: Quantization-aware low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.14717</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao <span id="bib.bib28.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas¬†L Griffiths, Yuan Cao, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10601</span>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span id="bib.bib29.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Paul Zhang, Brandon Jaipersaud, Jimmy Ba, Andrew Petersen, Lisa Zhang, and Michael¬†R Zhang.

</span>
<span class="ltx_bibblock">Classifying course discussion board questions using llms.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2</span>, pages 658‚Äì658, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao <span id="bib.bib30.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Weilin Zhao, Yuxiang Huang, Xu¬†Han, Zhiyuan Liu, Zhengyan Zhang, and Maosong Sun.

</span>
<span class="ltx_bibblock">Cpet: Effective parameter-efficient tuning for compressed large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.07705</span>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.06432" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.06433" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.06433">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.06433" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.06434" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:10:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
