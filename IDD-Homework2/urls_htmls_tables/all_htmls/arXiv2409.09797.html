<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation</title>
<!--Generated on Sun Sep 15 17:05:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="domain generalization out-of-distribution histopathology COSAS DCAC." lang="en" name="keywords"/>
<base href="/html/2409.09797v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S1" title="In Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S2" title="In Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Challenge Tasks and Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S2.SS0.SSS1" title="In 2 Challenge Tasks and Datasets ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.1 </span>Cross-Organ Dataset (Task 1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S2.SS0.SSS2" title="In 2 Challenge Tasks and Datasets ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.2 </span>Cross-Scanner Dataset (Task 2)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S3" title="In Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S3.SS0.SSS1" title="In 3 Methods ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.0.1 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S4" title="In Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation and Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S4.SS0.SSS1" title="In 4 Evaluation and Results ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S4.SS0.SSS2" title="In 4 Evaluation and Results ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span><span class="ltx_ERROR undefined">\discintname</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department Artificial Intelligence in Biomedical Engineering, Friedrich-Alexander-Universit√§t (FAU) Erlangen-N√ºrnberg, Erlangen, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Pattern Recognition Lab, FAU Erlangen-N√ºrnberg, Erlangen, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Flensburg University of Applied Sciences, Flensburg, Germany </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Center for AI and Data Science, Julius-Maximilians-Universit√§t W√ºrzburg, W√ºrzburg, Germany</span></span></span>
<h1 class="ltx_title ltx_title_document">Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frauke Wilm 
</span><span class="ltx_author_notes">1122**</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mathias √ñttl
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marc Aubreville
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Katharina Breininger
</span><span class="ltx_author_notes">1144
<span class="ltx_contact ltx_role_email"><a href="mailto:frauke.wilm@fau.de">frauke.wilm@fau.de</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">Recent advances in computer-aided diagnosis for histopathology have been largely driven by the use of deep learning models for automated image analysis. While these networks can perform on par with medical experts, their performance can be impeded by out-of-distribution data. The  <a href="https://arxiv.org/html/2409.09797v1#id3.1.id1"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id3.1.id1" title="Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_glossary_long">Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id3.1.id1"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id3.1.id1" title="Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_glossary_short">COSAS</span></abbr></a>) challenge aimed to address the task of cross-domain adenocarcinoma segmentation in the presence of morphological and scanner-induced domain shifts. In this paper, we present a U-Net-based segmentation framework designed to tackle this challenge. Our approach achieved segmentation scores of <math alttext="0.8020" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">0.8020</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn id="id1.1.m1.1.1.cmml" type="float" xref="id1.1.m1.1.1">0.8020</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">0.8020</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">0.8020</annotation></semantics></math> for the cross-organ track and <math alttext="0.8527" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">0.8527</mn><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><cn id="id2.2.m2.1.1.cmml" type="float" xref="id2.2.m2.1.1">0.8527</cn></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">0.8527</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">0.8527</annotation></semantics></math> for the cross-scanner track on the final challenge test sets, ranking it the best-performing submission.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>domain generalization out-of-distribution histopathology COSAS DCAC.
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">footnotetext: </span>*¬†corresponding author: </span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the development of designated slide scanners, traditional pathology has experienced a shift towards digital pathology, allowing histologic samples to be diagnosed on a computer screen rather than under an optical microscope. This transition has not only facilitated global expert collaboration but also enabled the application of machine learning models for computer-aided diagnosis. These models have demonstrated human-like performance across various diagnostically relevant tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib5" title="">5</a>]</cite>, such as mitotic figure assessment or tumor segmentation. However, their performance can be substantially impacted by out-of-distribution samples¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib6" title="">6</a>]</cite>. In histopathology, such samples may originate from the differing morphologies of various organs or from the visual discrepancies introduced by scanners from different vendors¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib1" title="">1</a>]</cite>. The  <a href="https://arxiv.org/html/2409.09797v1#id3.1.id1"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id3.1.id1" title="Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_glossary_long">Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id3.1.id1"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id3.1.id1" title="Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_glossary_short">COSAS</span></abbr></a>) challenge, held as a satellite event of the International Conference on  <a href="https://arxiv.org/html/2409.09797v1#id8.6.id6"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id8.6.id6" title="Medical Image Computing and Computer Assisted Intervention"><span class="ltx_text ltx_glossary_long">Medical Image Computing and Computer Assisted Intervention</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id8.6.id6"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id8.6.id6" title="Medical Image Computing and Computer Assisted Intervention"><span class="ltx_text ltx_glossary_short">MICCAI</span></abbr></a>) 2024, addressed the task of robust cross-domain adenocarcinoma segmentation in histology samples. This manuscript presents a methodological approach to solving this challenge, which ranked among the top-performing submissions on the preliminary test set and achieved the highest segmentation scores on the final test set.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Challenge Tasks and Datasets</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The <a href="https://arxiv.org/html/2409.09797v1#id3.1.id1"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id3.1.id1" title="Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_glossary_short">COSAS</span></abbr></a> challenge comprises two cross-domain histology datasets for two challenge tracks. Each dataset is composed of 290 <a href="https://arxiv.org/html/2409.09797v1#id9.7.id7"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id9.7.id7" title="region of interest"><span class="ltx_text ltx_glossary_long-plural">regions of interest</span></span></a> of human adenocarcinoma samples routinely stained with  <a href="https://arxiv.org/html/2409.09797v1#id7.5.id5"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id7.5.id5" title="Hematoxylin¬†&amp;¬†Eosin"><span class="ltx_text ltx_glossary_long">Hematoxylin¬†&amp;¬†Eosin</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id7.5.id5"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id7.5.id5" title="Hematoxylin¬†&amp;¬†Eosin"><span class="ltx_text ltx_glossary_short">HE</span></abbr></a>). Each <a href="https://arxiv.org/html/2409.09797v1#id9.7.id7"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id9.7.id7" title="region of interest"><span class="ltx_text ltx_glossary_short">ROI</span></abbr></a> has an average size of 1500¬†x¬†1500 pixels and tumor lesions were manually labeled with contour annotations. For the challenge, the organizers divided each dataset into train, preliminary test, and final test sets with 180-20-90 images respectively.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.1 </span>Cross-Organ Dataset (Task 1)</h3>
<div class="ltx_para" id="S2.SS0.SSS1.p1">
<p class="ltx_p" id="S2.SS0.SSS1.p1.1">The images of the cross-organ dataset were all digitized with the TEKSQRAY SQS-600P scanner and comprised images from six different organs. Of these six domains, only three were included in the training dataset (gastric adenocarcinoma, colorectal adenocarcinoma, and pancreatic ductal adenocarcinoma) and the others remained undisclosed to the challenge participants. The preliminary test set was composed of images from four different organs (including two training domains) and the final test set of images from all six organs. In each dataset, a uniform sample distribution across included domains was ensured.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.2 </span>Cross-Scanner Dataset (Task 2)</h3>
<div class="ltx_para" id="S2.SS0.SSS2.p1">
<p class="ltx_p" id="S2.SS0.SSS2.p1.1">The <a href="https://arxiv.org/html/2409.09797v1#id9.7.id7"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id9.7.id7" title="region of interest"><span class="ltx_text ltx_glossary_short-plural">roi</span></abbr></a> of the cross-scanner dataset were all obtained from invasive breast carcinoma samples of no special type acquired with six distinct scanning systems. Of these six scanners, only three were included in the training dataset (TEKSQRAY SQS-600P, KFBIO KF-PRO-400, and 3DHISTECH PANNORAMIC 1000) and the others remained undisclosed to the challenge participants. The preliminary test set was composed of images from four scanners and the final test set of images from all six scanning systems. In each dataset, a uniform sample distribution across included domains was ensured. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS2.p2">
<p class="ltx_p" id="S2.SS0.SSS2.p2.1">The challenge participants were not allowed to use any additional datasets for model development and pre-trained models were restricted to conventional, non-medical image datasets. However, the participants were allowed to combine the datasets of both challenge tracks and submit the same model to both tracks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For the task of cross-domain adenocarcinoma segmentation, we adopted nnU-Net¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib3" title="">3</a>]</cite>, which utilizes a dataset fingerprint to automatically determine the optimal U-Net¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib4" title="">4</a>]</cite> configuration for a given task. Furthermore, we experimented with the  <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_long">Domain and Content Adaptive Convolution</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a>)-based multi-source domain generalization approach, proposed by Hu <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">et al.</em>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib2" title="">2</a>]</cite>, visualized in <a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S3.F1" title="In 3 Methods ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_tag">Fig.</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>. <span class="ltx_ERROR undefined" id="S3.p1.1.2">\Ac</span>dcac utilizes two modules, i.‚Äâe. a  <a href="https://arxiv.org/html/2409.09797v1#id4.2.id2"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id4.2.id2" title="domain-adaptive convolution"><span class="ltx_text ltx_glossary_long">domain-adaptive convolution</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id4.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id4.2.id2" title="domain-adaptive convolution"><span class="ltx_text ltx_glossary_short">DAC</span></abbr></a>) module and a  <a href="https://arxiv.org/html/2409.09797v1#id5.3.id3"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id5.3.id3" title="content-adaptive convolution"><span class="ltx_text ltx_glossary_long">content-adaptive convolution</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id5.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id5.3.id3" title="content-adaptive convolution"><span class="ltx_text ltx_glossary_short">CAC</span></abbr></a>) module, and can be incorporated into any standard encoder-decoder segmentation network.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="387" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the  <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><span class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_long">Domain and Content Adaptive Convolution</span></span></a> (<a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a>) module. Figure adapted from Hu <em class="ltx_emph ltx_font_italic" id="S3.F1.2.1">et al.</em>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib2" title="">2</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">For the <a href="https://arxiv.org/html/2409.09797v1#id4.2.id2"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id4.2.id2" title="domain-adaptive convolution"><span class="ltx_text ltx_glossary_short">DAC</span></abbr></a> module, the multiscale encoder feature maps are global average pooled, concatenated, and passed on to a domain predictor, which computes an individual domain encoding, i.‚Äâe. a vector with probabilistic values for belonging to a source domain. Images from seen source domains are ideally one-hot encoded, whereas domain encodings for unseen target domain images are interpolated from the source domains. This domain encoding then controls the parameters of dynamic filtering kernels in the domain-adaptive head, resulting in domain-adaptive convolutions. Besides domain-adaptive convolutions, <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> also employs content-adaptive convolutions to adapt the architecture to variations across individual samples. The <a href="https://arxiv.org/html/2409.09797v1#id5.3.id3"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id5.3.id3" title="content-adaptive convolution"><span class="ltx_text ltx_glossary_short">CAC</span></abbr></a> module uses the pooled feature map of the last encoder layer to control the parameters of dynamic filtering kernels in the content-adaptive head, which is applied sequentially to the output of the domain-adaptive head.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.1 </span>Implementation Details</h3>
<div class="ltx_para" id="S3.SS0.SSS1.p1">
<p class="ltx_p" id="S3.SS0.SSS1.p1.6">We utilized the default experiment planner of nnU-Net to find the best model configurations. The model was trained with <math alttext="250" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.1.m1.1"><semantics id="S3.SS0.SSS1.p1.1.m1.1a"><mn id="S3.SS0.SSS1.p1.1.m1.1.1" xref="S3.SS0.SSS1.p1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.1.m1.1b"><cn id="S3.SS0.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS0.SSS1.p1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.1.m1.1c">250</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.1.m1.1d">250</annotation></semantics></math> minibatches per epoch and a batch size of <math alttext="2" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.2.m2.1"><semantics id="S3.SS0.SSS1.p1.2.m2.1a"><mn id="S3.SS0.SSS1.p1.2.m2.1.1" xref="S3.SS0.SSS1.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.2.m2.1b"><cn id="S3.SS0.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS0.SSS1.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.2.m2.1d">2</annotation></semantics></math>. We employed the default minibatch sampling method of nnU-Net. For optimization, we used the SGD optimizer with Nesterov momentum (<math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.3.m3.1"><semantics id="S3.SS0.SSS1.p1.3.m3.1a"><mi id="S3.SS0.SSS1.p1.3.m3.1.1" xref="S3.SS0.SSS1.p1.3.m3.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.3.m3.1b"><ci id="S3.SS0.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS1.p1.3.m3.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.3.m3.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.3.m3.1d">italic_Œº</annotation></semantics></math>¬†=¬†<math alttext="0.99" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.4.m4.1"><semantics id="S3.SS0.SSS1.p1.4.m4.1a"><mn id="S3.SS0.SSS1.p1.4.m4.1.1" xref="S3.SS0.SSS1.p1.4.m4.1.1.cmml">0.99</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.4.m4.1b"><cn id="S3.SS0.SSS1.p1.4.m4.1.1.cmml" type="float" xref="S3.SS0.SSS1.p1.4.m4.1.1">0.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.4.m4.1c">0.99</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.4.m4.1d">0.99</annotation></semantics></math>) and a polynomial learning rate scheduler (starting at 0.01). For segmentation training, we combined Dice and cross-entropy loss functions. For the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module, we followed the implementations by Hu <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS1.p1.6.1">et al.</em>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#bib.bib2" title="">2</a>]</cite>, and a standard cross-entropy loss was used for optimizing the domain classifier. The default training period of nnU-Net is <math alttext="1000" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.5.m5.1"><semantics id="S3.SS0.SSS1.p1.5.m5.1a"><mn id="S3.SS0.SSS1.p1.5.m5.1.1" xref="S3.SS0.SSS1.p1.5.m5.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.5.m5.1b"><cn id="S3.SS0.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="S3.SS0.SSS1.p1.5.m5.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.5.m5.1c">1000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.5.m5.1d">1000</annotation></semantics></math> epochs, after which we observed convergence of the segmentation loss. For the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module, however, the domain classifier accuracy still improved, so we trained the model for <math alttext="2500" class="ltx_Math" display="inline" id="S3.SS0.SSS1.p1.6.m6.1"><semantics id="S3.SS0.SSS1.p1.6.m6.1a"><mn id="S3.SS0.SSS1.p1.6.m6.1.1" xref="S3.SS0.SSS1.p1.6.m6.1.1.cmml">2500</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS1.p1.6.m6.1b"><cn id="S3.SS0.SSS1.p1.6.m6.1.1.cmml" type="integer" xref="S3.SS0.SSS1.p1.6.m6.1.1">2500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS1.p1.6.m6.1c">2500</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS1.p1.6.m6.1d">2500</annotation></semantics></math> epochs in total. During training, five-fold cross-validation was used and model selection was guided by the best performance on the validation set, assessed using the exponential moving average of the segmentation Dice score (and additionally domain classification accuracy for the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module). During inference, we applied model ensembling across all folds and test-time augmentation, consistent with the standard nnU-Net implementation. All code for model training and evaluation is publicly available in our GitHub repository: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/DeepMicroscopy/nnUNet</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation and Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We first tested both approaches (the baseline nnU-Net and the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module) for out-of-distribution performance by training them on the training set of task 1 and evaluating them on the training set of task 2. In these cross-domain experiments, the domain encoder of the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module was trained with three domains and demonstrated superior performance to the nnU-Net baseline (fourth row in <a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S4.T1" title="In 4 Evaluation and Results ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>). We then randomly selected ten images of each domain as a hold-out validation set and combined the remaining images of both datasets into a single training set. For this experiment, the domain encoder of the <a href="https://arxiv.org/html/2409.09797v1#id6.4.id4"><abbr class="ltx_glossaryref" href="https://arxiv.org/html/2409.09797v1#id6.4.id4" title="Domain and Content Adaptive Convolution"><span class="ltx_text ltx_glossary_short">DCAC</span></abbr></a> module was trained with six domains. The results of these in-domain experiments are reported in rows 1 and 4 of <a class="ltx_ref" href="https://arxiv.org/html/2409.09797v1#S4.T1" title="In 4 Evaluation and Results ‚Ä£ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_tag">Table</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>. Here, both methods performed comparably.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">For challenge submission, a Docker-based system was used, restricting participants from direct access to the test set. Participants were allowed up to five submissions per task on the preliminary test set and only one submission per task on the final test set. The final challenge ranking was based on a weighted average of the segmentation performance on both test sets, with weights of 0.2 for the preliminary and 0.8 for the final test set. Segmentation performance was evaluated as the average of the Dice and Jaccard scores. We used all of the 360 training images for training the final models and submitted them for evaluation on the preliminary test set. Again, both models performed comparably. We hypothesize that the combined dataset with six domains may have required better optimization of the domain classifier‚Äôs hyperparameters to demonstrate superior performance. We plan to explore this further after the final challenge deadline.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">As only one submission was allowed on the final test set, we decided to submit the slightly better-performing nnU-Net baseline, which ranked first place in both challenge tracks with segmentation scores of <math alttext="0.8020" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">0.8020</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn id="S4.p3.1.m1.1.1.cmml" type="float" xref="S4.p3.1.m1.1.1">0.8020</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">0.8020</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">0.8020</annotation></semantics></math> and <math alttext="0.8527" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">0.8527</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn id="S4.p3.2.m2.1.1.cmml" type="float" xref="S4.p3.2.m2.1.1">0.8527</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">0.8527</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">0.8527</annotation></semantics></math>, respectively.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Segmentation performance assessed as the average of the Dice and Jaccard scores. Evaluation for the internal validation sets was conducted by the authors, whereas the preliminary and final test set performance was evaluated by the challenge organizers.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T1.1.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;">nnU-Net</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T1.1.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;">nnU-Net + DCAC</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.2.1" rowspan="3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S4.T1.1.2.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1.2.2.1.1.1" style="width:6.9pt;height:26.5pt;vertical-align:-9.8pt;"><span class="ltx_transformed_inner" style="width:26.4pt;transform:translate(-9.75pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.1.2.2.1.1.1.1">task 1</span>
</span></span></span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.2.2.2" style="padding-left:0.0pt;padding-right:0.0pt;">in-domain (internal)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.2.2.3" style="padding-left:0.0pt;padding-right:0.0pt;">0.8249</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.2.2.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.4.1">0.8412</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.3.3.1" style="padding-left:0.0pt;padding-right:0.0pt;">preliminary test set</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.3.3.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.3.2.1">0.7776</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.3.3.3" style="padding-left:0.0pt;padding-right:0.0pt;">0.7690</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.4.4.1" style="padding-left:0.0pt;padding-right:0.0pt;">final test set</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.4.4.2" style="padding-left:0.0pt;padding-right:0.0pt;">0.8020</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.4.4.3" style="padding-left:0.0pt;padding-right:0.0pt;">n.a.</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T1.1.5.5.1" rowspan="4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S4.T1.1.5.5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1.5.5.1.1.1" style="width:6.9pt;height:26.5pt;vertical-align:-9.8pt;"><span class="ltx_transformed_inner" style="width:26.4pt;transform:translate(-9.75pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.1.5.5.1.1.1.1">task 2</span>
</span></span></span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.5.5.2" style="padding-left:0.0pt;padding-right:0.0pt;">cross-domain (internal)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.5.5.3" style="padding-left:0.0pt;padding-right:0.0pt;">0.7574</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.1.5.5.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.5.4.1">0.7896</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.6.6.1" style="padding-left:0.0pt;padding-right:0.0pt;">in-domain (internal)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.6.6.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.6.2.1">0.8413</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.6.6.3" style="padding-left:0.0pt;padding-right:0.0pt;">0.8338</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.7.7.1" style="padding-left:0.0pt;padding-right:0.0pt;">preliminary test set</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.7.7.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.7.7.2.1">0.8858</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T1.1.7.7.3" style="padding-left:0.0pt;padding-right:0.0pt;">0.8829</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T1.1.8.8.1" style="padding-left:0.0pt;padding-right:0.0pt;">final test set</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T1.1.8.8.2" style="padding-left:0.0pt;padding-right:0.0pt;">0.8527</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T1.1.8.8.3" style="padding-left:0.0pt;padding-right:0.0pt;">n.a.</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p4">
<span class="ltx_ERROR undefined" id="S4.p4.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Acknowledgements</h3>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit√§t Erlangen-N√ºrnberg (FAU) under the NHR project b209cb. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) ‚Äì 440719683. F.W. M.√ñ., and K.B. acknowledge support by the German Research Foundation (DFG) project 460333672 CRC1540 EBM. K.B. further acknowledges support by d.hip campus - Bavarian aim in form of a faculty endowment.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span><span class="ltx_ERROR undefined" id="S4.SS0.SSS2.1.1">\discintname</span>
</h3>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1">The authors have no competing interests to declare that are
relevant to the content of this article.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aubreville, M., Wilm, F., Stathonikos, N., Breininger, K., Donovan, T.A., Jabari, S., Veta, M., Ganz, J., Ammeling, J., van Diest, P.J., et¬†al.: A comprehensive multi-domain dataset for mitotic figure detection. Scientific data <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">10</span>(1), ¬†484 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Hu, S., Liao, Z., Zhang, J., Xia, Y.: Domain and content adaptive convolution based multi-source domain generalization for medical image segmentation. IEEE Transactions on Medical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">42</span>(1), 233‚Äì244 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation. Nature methods <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">18</span>(2), 203‚Äì211 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In: Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 234‚Äì241. Springer (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Srinidhi, C.L., Ciga, O., Martel, A.L.: Deep neural network models for computational histopathology: A survey. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">67</span>, 101813 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wilm, F., Fragoso, M., Bertram, C.A., Stathonikos, N., √ñttl, M., Qiu, J., Klopfleisch, R., Maier, A., Aubreville, M., Breininger, K.: Mind the gap: Scanner-induced domain shifts pose challenges for representation learning in histopathology. In: Proceedings of the International Symposium on Biomedical Imaging (ISBI). pp.¬†1‚Äì5. IEEE (2023)

</span>
</li>
</ul>
</section>
<section class="ltx_glossary ltx_acronym ltx_list_acronym" id="id9">
<dl class="ltx_glossarylist" id="id9.7">
<dt class="ltx_glossaryentry" id="id3.1.id1">COSAS</dt>
<dd>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation</dd>
<dt class="ltx_glossaryentry" id="id4.2.id2">DAC</dt>
<dd>domain-adaptive convolution</dd>
<dt class="ltx_glossaryentry" id="id5.3.id3">CAC</dt>
<dd>content-adaptive convolution</dd>
<dt class="ltx_glossaryentry" id="id6.4.id4">DCAC</dt>
<dd>Domain and Content Adaptive Convolution</dd>
<dt class="ltx_glossaryentry" id="id7.5.id5">HE</dt>
<dd>Hematoxylin¬†&amp;¬†Eosin</dd>
<dt class="ltx_glossaryentry" id="id8.6.id6">MICCAI</dt>
<dd>Medical Image Computing and Computer Assisted Intervention</dd>
<dt class="ltx_glossaryentry" id="id9.7.id7">ROI</dt>
<dd>region of interest</dd>
</dl>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 15 17:05:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
