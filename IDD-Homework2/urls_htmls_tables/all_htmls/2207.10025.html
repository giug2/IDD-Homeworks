<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.10025] Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks</title><meta property="og:description" content="Facial expression in-the-wild is essential for various interactive computing domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic in the facial expression recognition task. In this paper, we …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.10025">

<!--Generated on Wed Mar 13 12:25:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jae-Yeop Jeong<sup id="id9.9.id1" class="ltx_sup">1</sup>, Yeong-Gi Hong<sup id="id10.10.id2" class="ltx_sup">1</sup>, JiYeon Oh<sup id="id11.11.id3" class="ltx_sup">2</sup>, Sumin Hong<sup id="id12.12.id4" class="ltx_sup">3</sup>, and Jin-Woo Jeong<sup id="id13.13.id5" class="ltx_sup">1</sup>
<br class="ltx_break">Department of Data Science<sup id="id14.14.id6" class="ltx_sup">1</sup>, Division of IISE<sup id="id15.15.id7" class="ltx_sup">2</sup>, Division of ITM<sup id="id16.16.id8" class="ltx_sup">3</sup>
<br class="ltx_break">Seoul National University of Science and Technology
<br class="ltx_break">Seoul, Korea 
<br class="ltx_break"><span id="id17.17.id9" class="ltx_text ltx_font_typewriter">{jaey.jeong, yghong, dhwldus0906, 17101992, jinw.jeong}@seoultech.ac.kr</span> 
<br class="ltx_break">Yuchul Jung 
<br class="ltx_break">Department of Computer Engineering
<br class="ltx_break">Kumoh National Institute of Technology 
<br class="ltx_break">Gumi, Korea 
<br class="ltx_break"><span id="id18.18.id10" class="ltx_text ltx_font_typewriter">jyc@kumoh.ac.kr</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id19.id1" class="ltx_p">Facial expression in-the-wild is essential for various interactive computing domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic in the facial expression recognition task. In this paper, we propose a multi-task learning-based facial expression recognition approach which consists of emotion and appearance learning branches that can share all face information, and present preliminary results for the LSD challenge introduced in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our method achieved the mean F1 score of 0.71.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Facial expression recognition plays an important role in various interactive computing domains such as human-computer interaction, social robots, and a satisfaction survey <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. To achieve more robust and accurate facial expression recognition (FER), a number of studies have been proposed in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. However, there are still many rooms to improve the robustness and performance of FER techniques. One of the most challenging research areas is facial expression recognition in-the-wild. Basically, to get high-level performance in FER, a number of well-aligned and high-resolution face images are necessary. Compared to face images that are gathered in a controlled setting, however, in-the-wild face images have many variations, such as various head poses, illumination, etc. Therefore, facial expression recognition in-the-wild is still challenging and should be studied continuously for real-world applications. Accordingly, face image generation/synthesis for FER tasks has been steadily getting much attention because it can generate unlimited photo-realistic facial images with various expressions under various conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. By learning from synthetic data and evaluating on the real-world data, the problem of construction of large-scale real data sets would be mitigated.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The 4th competition on Affective Behavior Analysis in-the-wild (ABAW), held in conjunction with the European Conference on Computer Vision (ECCV) 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, a continuation of 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) in CVPR 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The ABAW competition contributes to the deployment of in-the-wild affective behavior analysis systems that are robust to video recording conditions, diversity of contexts and timing of display, regardless of human age, gender, ethnicity, and status. The 4th ABAW competition is based on the Aff-Wild2 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which is an extension of the Aff-wild database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and consists of the following tracks: 1) Multi-Task-Learning (MTL) 2) Learning from Synthetic Data (LSD).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we describe our methods for the LSD challenge and present preliminary results. For the LSD challenge, some frames from the Aff-Wild2 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> were selected by the competition organizers and then used to generate artificial face images with various facial expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In total, the synthetic image set consists of approximately 300K images and their corresponding annotations for 6 basic facial expressions (anger, disgust, fear, happiness, sadness, surprise), which will be used in model training/methodology development.
In this year’s LSD challenge, participating teams were allowed to use only the provided synthetic facial images when developing their methodology, while any kind of pre-trained model could be used unless it was not been trained on the Aff-Wild2 database. For validation, a set of original facial images of the subjects who also appeared in the training set was provided. For evaluation, the original facial images of the subjects in the Aff-Wild2 database test set, who did not appear in the given training set, are used. For the LSD challenge, the mean F1 score across all 6 categories was used as metric.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">To achieve a high performance on the task of facial expression recognition, extraction of the robust feature from input facial images is essential. To this end, we employed a multi-task learning approach, jointly optimizing different learning objectives. Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts an overview of the proposed architecture used in our study.
As shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the framework is trained to solve both facial expression recognition task (i.e., Emotion branch) and face landmark detection task (i.e., Appearance branch) using the provided synthetic training data only. Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Database ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the sample provided training images.
Our multi-task learning framework has two output layers for tasks of FER and landmark detection, <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="y_{expr}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">y</mi><mrow id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1a" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.4" xref="S2.SS1.p1.1.m1.1.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1b" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.5" xref="S2.SS1.p1.1.m1.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝑦</ci><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><times id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.2">𝑒</ci><ci id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3">𝑥</ci><ci id="S2.SS1.p1.1.m1.1.1.3.4.cmml" xref="S2.SS1.p1.1.m1.1.1.3.4">𝑝</ci><ci id="S2.SS1.p1.1.m1.1.1.3.5.cmml" xref="S2.SS1.p1.1.m1.1.1.3.5">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">y_{expr}</annotation></semantics></math> and <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="y_{land}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">y</mi><mrow id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1.3.1" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1.3.1a" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.1.1.3.4" xref="S2.SS1.p1.2.m2.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1.3.1b" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.1.1.3.5" xref="S2.SS1.p1.2.m2.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝑦</ci><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><times id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.1"></times><ci id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">𝑙</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3">𝑎</ci><ci id="S2.SS1.p1.2.m2.1.1.3.4.cmml" xref="S2.SS1.p1.2.m2.1.1.3.4">𝑛</ci><ci id="S2.SS1.p1.2.m2.1.1.3.5.cmml" xref="S2.SS1.p1.2.m2.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">y_{land}</annotation></semantics></math> respectively. Here, <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="y_{expr}\in R^{6}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2.2" xref="S2.SS1.p1.3.m3.1.1.2.2.cmml">y</mi><mrow id="S2.SS1.p1.3.m3.1.1.2.3" xref="S2.SS1.p1.3.m3.1.1.2.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2.3.2" xref="S2.SS1.p1.3.m3.1.1.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.2.3.1" xref="S2.SS1.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.2.3.3" xref="S2.SS1.p1.3.m3.1.1.2.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.2.3.1a" xref="S2.SS1.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.2.3.4" xref="S2.SS1.p1.3.m3.1.1.2.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.2.3.1b" xref="S2.SS1.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.2.3.5" xref="S2.SS1.p1.3.m3.1.1.2.3.5.cmml">r</mi></mrow></msub><mo id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">R</mi><mn id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><in id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></in><apply id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.2">𝑦</ci><apply id="S2.SS1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3"><times id="S2.SS1.p1.3.m3.1.1.2.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3.1"></times><ci id="S2.SS1.p1.3.m3.1.1.2.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3.2">𝑒</ci><ci id="S2.SS1.p1.3.m3.1.1.2.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3.3">𝑥</ci><ci id="S2.SS1.p1.3.m3.1.1.2.3.4.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3.4">𝑝</ci><ci id="S2.SS1.p1.3.m3.1.1.2.3.5.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3.5">𝑟</ci></apply></apply><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">𝑅</ci><cn type="integer" id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">y_{expr}\in R^{6}</annotation></semantics></math> and <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="y_{land}\in R^{68\times 2}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><msub id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2.2" xref="S2.SS1.p1.4.m4.1.1.2.2.cmml">y</mi><mrow id="S2.SS1.p1.4.m4.1.1.2.3" xref="S2.SS1.p1.4.m4.1.1.2.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2.3.2" xref="S2.SS1.p1.4.m4.1.1.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.1.1.2.3.1" xref="S2.SS1.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.4.m4.1.1.2.3.3" xref="S2.SS1.p1.4.m4.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.1.1.2.3.1a" xref="S2.SS1.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.4.m4.1.1.2.3.4" xref="S2.SS1.p1.4.m4.1.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.1.1.2.3.1b" xref="S2.SS1.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S2.SS1.p1.4.m4.1.1.2.3.5" xref="S2.SS1.p1.4.m4.1.1.2.3.5.cmml">d</mi></mrow></msub><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">R</mi><mrow id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml"><mn id="S2.SS1.p1.4.m4.1.1.3.3.2" xref="S2.SS1.p1.4.m4.1.1.3.3.2.cmml">68</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.4.m4.1.1.3.3.1" xref="S2.SS1.p1.4.m4.1.1.3.3.1.cmml">×</mo><mn id="S2.SS1.p1.4.m4.1.1.3.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><in id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></in><apply id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2.2">𝑦</ci><apply id="S2.SS1.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3"><times id="S2.SS1.p1.4.m4.1.1.2.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3.1"></times><ci id="S2.SS1.p1.4.m4.1.1.2.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3.2">𝑙</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3.3">𝑎</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.4.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3.4">𝑛</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.5.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3.5">𝑑</ci></apply></apply><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">𝑅</ci><apply id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3"><times id="S2.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.1"></times><cn type="integer" id="S2.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.2">68</cn><cn type="integer" id="S2.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">y_{land}\in R^{68\times 2}</annotation></semantics></math>. Finally, the loss of <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="y_{expr}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">y</mi><mrow id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml"><mi id="S2.SS1.p1.5.m5.1.1.3.2" xref="S2.SS1.p1.5.m5.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.1.1.3.1" xref="S2.SS1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.5.m5.1.1.3.3" xref="S2.SS1.p1.5.m5.1.1.3.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.1.1.3.1a" xref="S2.SS1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.5.m5.1.1.3.4" xref="S2.SS1.p1.5.m5.1.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.1.1.3.1b" xref="S2.SS1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.5.m5.1.1.3.5" xref="S2.SS1.p1.5.m5.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑦</ci><apply id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3"><times id="S2.SS1.p1.5.m5.1.1.3.1.cmml" xref="S2.SS1.p1.5.m5.1.1.3.1"></times><ci id="S2.SS1.p1.5.m5.1.1.3.2.cmml" xref="S2.SS1.p1.5.m5.1.1.3.2">𝑒</ci><ci id="S2.SS1.p1.5.m5.1.1.3.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3.3">𝑥</ci><ci id="S2.SS1.p1.5.m5.1.1.3.4.cmml" xref="S2.SS1.p1.5.m5.1.1.3.4">𝑝</ci><ci id="S2.SS1.p1.5.m5.1.1.3.5.cmml" xref="S2.SS1.p1.5.m5.1.1.3.5">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">y_{expr}</annotation></semantics></math> and <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="y_{land}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">y</mi><mrow id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.2" xref="S2.SS1.p1.6.m6.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1a" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.4" xref="S2.SS1.p1.6.m6.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.3.1b" xref="S2.SS1.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.6.m6.1.1.3.5" xref="S2.SS1.p1.6.m6.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">𝑦</ci><apply id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3"><times id="S2.SS1.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3.1"></times><ci id="S2.SS1.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.2">𝑙</ci><ci id="S2.SS1.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3">𝑎</ci><ci id="S2.SS1.p1.6.m6.1.1.3.4.cmml" xref="S2.SS1.p1.6.m6.1.1.3.4">𝑛</ci><ci id="S2.SS1.p1.6.m6.1.1.3.5.cmml" xref="S2.SS1.p1.6.m6.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">y_{land}</annotation></semantics></math> are computed with weighted cross-entropy and mean squared error, respectively.
During the inference, only the output from the emotion branch is used for classification of facial expression for the given validation/test sample.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To achieve a more generalized performance for in-the-wild data, we applied a bagging approach when generating final predictions.
Each model with a different configuration was trained with sub-sampled data sets (i.e., 20% out of the entire set) and produced its own output. Finally, we aggregate the probabilities of each model through soft voting for the final prediction.
More details on the CNN models used in each branch can be found from Section <a href="#S2.SS3" title="2.3 Model Architecture ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2207.10025/assets/figure/archhhh.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the architecture used in this study</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Database</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In order to train our multi-task learning framework, each image is given 1) facial expression annotation and 2) face landmark annotation. First, the affective state label consists of 6 basic facial expressions (anger, disgust, fear, happiness, sadness, surprise), which were offered by the 4th ABAW competition organizers. Next, the landmark annotations were created through DECA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> framework which is a state-of-the-art 3D face shape reconstruction method that reconstructs both face landmark and 3D mesh from a single facial image. Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2 Database ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows our example of landmark annotation which is composed of 68 coordinate points with (x, y) of the face.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2207.10025/assets/figure/dataset.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Synthesis data used in our study</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2207.10025/assets/figure/facelandmark.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of face landmarks</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Architecture</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">As depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our architecture is composed of two branches: 1) emotion and 2) appearance. In each branch, we utilized a pre-trained backbone for a robust and generalized feature extraction. Finally, we employed a series of shared fully connected layers right after two branches, to exploit all knowledge extracted from the backbone model of each branch.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Emotion branch</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">As depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> emotion branch, we adopted a deep learning-based FER approach called "DAN" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> which is a state-of-the-art method for the AffectNet database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The DAN architecture has two phases: feature extractor and attention parts. In the attention phase, there exist multi-head cross attention units which consist of a combination of spatial and channel attention units. The DAN architecture used in our emotion branch is a modified version pre-trained on AffectNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Expw <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, and AIHUB datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for a categorical FER task. For better performance, we replaced the original feature extractor of the DAN architecture (i.e., ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> pre-trained on MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>) with ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> pre-trained on VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
To prevent overfitting and acquire a generalizable performance, we applied various data augmentation techniques, such as Colorjitter, Horizontal flip, RandomErasing, and Mix-Augment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Finally, the input data go through our deep network in the emotion branch and are fed to the shared fully connected layers.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Appearance branch</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The goal of the appearance branch is to extract the robust feature in terms of visual appearance. For this, we employed various backbone models pre-trained on large scale data sets, ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> pre-trained on VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, DINO ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> pre-trained on VGGFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and MobileVITv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, as a feature extractor for the appearance branch.
As shown in the appearance branch of Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Overview ‣ 2 Method ‣ Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we trained only a single backbone for landmark detection during the multi-task learning phase, rather than utilizing multiple backbones together.
Similar to the emotion branch, we also used a data augmentation strategy in the appearance branch. Due to the characteristics of landmark data, we applied Colorjitter only to prevent unnecessary spatial transformations. After feature extraction, all the landmark-related features are passed to the shared fully connected layers.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">In summary, our multi-task learning model is configured with DAN on the emotion branch and the appearance branch with one of the following backbones: a) ResNet50, b) DINO ResNet50, c) MobileViT-v2.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">All the experiments were conducted using a GPU server with six NVIDIA RTX 3090 GPUs, 128 GB RAM, Intel i9-10940X CPU, and Pytorch framework. Our preliminary results on the official validation set for the LSD challenge was 0.71 in terms of the mean F1 score, which significantly outperforms that of a baseline method (0.50).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we proposed a multi-task learning-based architecture for FER and presented the preliminary results for the LSD challenge in the 4th ABAW competition. Our method produced the mean F1 score of 0.71 for the LSD challenge. The implementation details and validation results may be updated after submission of this paper to arxiv.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Iman Abbasnejad, Sridha Sridharan, Dung Nguyen, Simon Denman, Clinton Fookes,
and Simon Lucey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Using synthetic data to improve facial expression analysis with 3d
convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 1609–1618, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
AI-Hub.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Video dataset for korean facial expressino recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">Available at
</span><a target="_blank" href="https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&amp;topMenu=100&amp;aihubDataSe=realm&amp;dataSetSn=82" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&amp;topMenu=100&amp;aihubDataSe=realm&amp;dataSetSn=82</a><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">
(2022/07/21).
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Vggface2: A dataset for recognising faces across pose and age.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 13th IEEE international conference on automatic face &amp;
gesture recognition (FG 2018)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 67–74. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal,
Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision (ICCV)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning an animatable detailed 3d face model from in-the-wild
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (ToG)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 40(4):1–13, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Haoqi Gao and Koichi Ogawara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Face alignment using a gan-based photorealistic synthetic dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 7th International Conference on Control and Robotics
Engineering (ICCRE)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 147–151. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Ms-celeb-1m: A dataset and benchmark for large-scale face
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 87–102.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jae-Yeop Jeong, Yeong-Gi Hong, Daun Kim, Jin-Woo Jeong, Yuchul Jung, and
Sang-Ho Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Classification of facial expression in-the-wild based on ensemble of
multi-head cross attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 2353–2358, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Abaw: Learning from synthetic data &amp; multi-task learning challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.01138</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Abaw: Valence-arousal estimation, expression recognition, action unit
detection &amp; multi-task learning challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2328–2336, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Shiyang Cheng, Maja Pantic, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Photorealistic facial synthesis in the dimensional affect space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV) Workshops</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Shiyang Cheng, Evangelos Ververas, Irene Kotsia, and
Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep neural network augmentation: Generating faces for affect
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 128(5):1455–1484,
2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoying Zhao, and Stefanos
Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Recognition of affect in the wild using deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 26–33, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Distribution matching for heterogeneous multi-task learning: a
large-scale face study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2105.03790</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou, Athanasios
Papaioannou, Guoying Zhao, Björn Schuller, Irene Kotsia, and Stefanos
Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Deep affect prediction in-the-wild: Aff-wild database and challenge,
deep architectures, and beyond.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 127(6):907–929,
2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Expression, affect, action unit recognition: Aff-wild2, multi-task
learning and arcface.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.04855</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Va-stargan: Continuous affect generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Advanced Concepts for Intelligent
Vision Systems</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 227–238. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Affect analysis in-the-wild: Valence-arousal, expressions, action
units and a unified framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.15792</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 25, 2012.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Shan Li and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Deep facial expression recognition: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on affective computing</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Sachin Mehta and Mohammad Rastegari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Separable self-attention for mobile vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.02680</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Affectnet: A database for facial expression, valence, and arousal
computing in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 10(1):18–31, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Andreas Psaroudakis and Dimitrios Kollias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Mixaugment &amp; mixup: Augmentation methods for facial expression
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 2367–2375, June 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Andrey V Savchenko, Lyudmila V Savchenko, and Ilya Makarov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Classifying emotions and engagement in online learning based on a
single facial expression recognition neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Florian Schroff, Dmitry Kalenichenko, and James Philbin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Facenet: A unified embedding for face recognition and clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 815–823, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Deepface: Closing the gap to human-level performance in face
verification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1701–1708, 2014.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Zhengyao Wen, Wenzhong Lin, Tao Wang, and Ge Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Distract your attention: multi-head cross attention network for
facial expression recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.07270</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou, Athanasios
Papaioannou, Guoying Zhao, and Irene Kotsia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Aff-wild: valence and arousal’in-the-wild’challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition workshops</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 34–41, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Jiabei Zeng, Shiguang Shan, and Xilin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Facial expression recognition with inconsistently annotated datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, September 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Zhanpeng Zhang, Ping Luo, Chen-Change Loy, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Learning social relation traits from face images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 3631–3639, 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">From facial expression recognition to interpersonal relation
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vision</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 126(5):550–569, may 2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Hengshun Zhou, Debin Meng, Yuanyuan Zhang, Xiaojiang Peng, Jun Du, Kai Wang,
and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Exploring emotion features and fusion strategies for audio-video
emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International conference on multimodal interaction</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">,
pages 562–566, 2019.
</span>
</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.10024" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.10025" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.10025">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.10025" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.10026" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 12:25:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
