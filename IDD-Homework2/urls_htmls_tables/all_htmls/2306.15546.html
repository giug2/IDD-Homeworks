<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.15546] When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions</title><meta property="og:description" content="The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and re…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.15546">

<!--Generated on Wed Feb 28 21:22:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weiming Zhuang 
<br class="ltx_break">Sony AI
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">weiming.zhuang@sony.com</span> 
<br class="ltx_break">&amp;Chen Chen 
<br class="ltx_break">Sony AI 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">chena.chen@sony.com</span> 
<br class="ltx_break">&amp;Lingjuan Lyu 
<br class="ltx_break">Sony AI 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">lingjuan.lv@sony.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations, challenges, and future directions. Through an exploration of the challenges faced by FL and FM individually and their interconnections, we aim to inspire future research directions that can further enhance both fields, driving advancements and propelling the development of privacy-preserving and scalable systems for both FL and FM.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The field of artificial intelligence (AI) has witnessed remarkable advancements in recent years, with the emergence of powerful models and innovative learning techniques. Among these, the Foundation Model (FM) represents a significant breakthrough that has revolutionized AI research and applications <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">bommasani2021fm-survey </a></cite>. The FM, exemplified by models such as GPT series <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">zhang2023gpt </a>; <a href="#bib.bib3" title="" class="ltx_ref">radford2019gpt2 </a>; <a href="#bib.bib4" title="" class="ltx_ref">brown2020gpt3 </a>; <a href="#bib.bib5" title="" class="ltx_ref">openai2023gpt4 </a></cite> and CLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">lu2023fedclip </a></cite> from OpenAI , BERT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">devlin2018bert </a></cite> and PaLM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">chowdhery2022palm </a>; <a href="#bib.bib9" title="" class="ltx_ref">anil2023palm </a></cite> from Google, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">rombach2022high </a></cite>, and Segment Anything <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">kirillov2023sam </a></cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">touvron2023llama </a></cite> from Meta, introduces a new era in AI models characterized by enormous size, pre-trained knowledge, and exceptional performance across a wide range of tasks, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. FM has transformed various domains of AI applications with its advantages in processing and understanding natural language, generating coherent and contextually relevant responses, and exhibiting impressive zero-shot capabilities. However, the development and utilization of the FM face challenges such as limited availability of large-scale and high-quality public data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">villalobos2022will </a></cite> and high demand on computation resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">bommasani2021fm-survey </a></cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.15546/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="392" height="386" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Illustration of foundation model.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">fedavg </a></cite>, on the other hand, is a collaborative learning paradigm (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (c)) that has revolutionized learning paradigms and gained substantial attention for its ability to address privacy concerns in centralized learning paradigm (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b)) and low performance issues in standalone learning paradigm (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a)). FL enables multiple clients to train models across decentralized data sources without touching on their original data. It encompasses a wide range of applications, including healthcare <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">li2019brain-tumor1 </a>; <a href="#bib.bib16" title="" class="ltx_ref">sheller2018brain-tumor2 </a></cite>, finance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">zhang2023privacy </a>; <a href="#bib.bib18" title="" class="ltx_ref">liu2023efficient </a></cite>, video surveillance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">zhuang2020fedreid </a>; <a href="#bib.bib20" title="" class="ltx_ref">zhuang2021fedureid </a>; <a href="#bib.bib21" title="" class="ltx_ref">zhuang2022fedfr </a>; <a href="#bib.bib22" title="" class="ltx_ref">zhuang2022fedreid </a></cite>, personalized recommendations <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">muhammad2020fedfast </a></cite>, consumer products <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">hard2018gboard </a></cite>, and so on. FL offers two primary approaches: cross-silo FL and cross-device FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">kairouz2021advances </a></cite>. Cross-silo FL involves training models across multiple institutions, where the number of clients is normally within 100. Cross-device FL trains models across distributed devices, where the number of clients can be up to <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="10^{10}" display="inline"><semantics id="S1.p2.1.m1.1a"><msup id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mn id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">10</mn><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">10</cn><cn type="integer" id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">10^{10}</annotation></semantics></math>. FL supports training a randomly initialized model from scratch or fine-tuning existing models via federated transfer learning. By enabling data privacy and localized learning, FL holds promise for scalable and privacy-preserving AI development.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">FL and FM share a symbiotic relationship, each offering unique advantages to the other. FL plays a crucial role in mitigating challenges encountered in FM development. FL leverages distributed data sources, thereby expanding the availability of data for FMs. It also enables computation sharing and distributes the training burden across multiple FL participants. FL promotes the collaborative development of FMs, democratizing the process and allowing a wider range of participants to contribute, thereby fostering inclusivity and innovation. On the other hand, FM contributes to addressing challenges in FL. With its pre-trained knowledge and exceptional performance, FM serves as a good starting point, facilitating faster convergence and better performance, especially under non-iid (non-independent and identically distributed) setting <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">chen2023importance </a>; <a href="#bib.bib27" title="" class="ltx_ref">nguyen2022begin </a></cite>. Besides, leveraging FM to generate synthetic data can enrich data diversity, reduce overfitting, and preserve privacy by supplementing or replacing sensitive training data with the synthesized or generated data. However, it should be noted that FMs like GPT are not mature enough <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">xie2023defending </a>; <a href="#bib.bib29" title="" class="ltx_ref">sun2023pushing </a></cite>, which poses a series of challenges in this synergy. This mutual reinforcement between FL and FM could create a synergistic relationship that drives advancements in both fields.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper aims to explore the motivations, challenges, and future directions of empowering FM with FL and empowering FL with FM. We delve into the specific challenges faced by FL and FM individually, highlighting their interconnections and potential solutions. Furthermore, we examine the opportunities that arise from this intersection. Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Motivations of FL for FM ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F7" title="Figure 7 ‣ 3 Foundation Models for Federated Learning ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> highlight the motivations, challenges, and future directions of empowering FM with FL and empowering FL with FM, respectively. By examining these challenges and opportunities, we aim to pave the way for a deeper understanding of the potential of FL in conjunction with the FM and inspire future research directions that can further enhance both fields.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x2.png" id="S1.F2.sf1.g1" class="ltx_graphics ltx_img_portrait" width="92" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Standalone</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x3.png" id="S1.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="141" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Centralized</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x4.png" id="S1.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="166" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Federated learning</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Different Learning Paradigms: (a) standalone training, (b) centralized training, and (c) federated learning.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning for Foundation Models</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce the challenges faced by the Foundation Model (FM), such as limited availability of large-scale high-quality legalized data and high computational resource demand. We then delve into how Federated Learning (FL) emerges as a promising solution to address these challenges by leveraging private data, facilitating computation sharing, democratizing FM development access, and effectively handling the continuously growing data. Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Motivations of FL for FM ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a comprehensive summary of the motivations, challenges, and future directions of FL for FM.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Motivations of FL for FM</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Shortage of Large-scale High-Quality Legalized Data.</span>
The foundation model faces a significant challenge in accessing a sufficient amount of high-quality public data for training and improving its performance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">villalobos2022will </a></cite>. Research indicates that the availability of high-quality language data is projected to deplete by 2026, followed by the exhaustion of low-quality language and image data between 2030 and 2050 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">villalobos2022will </a></cite>. As time progresses, the diminishing availability of such data poses a potential obstacle to the continued advancement of FM. While leveraging generated data for further training seems intuitive, using model-generated data in training can lead to model collapse and the forgetting of the underlying data distribution over time <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">shumailov2023curse </a></cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Furthermore, publicly available data sources raise concerns. Previously free data platforms like Twitter and Reddit have started charging high annual premiums for data access <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">twitter </a>; <a href="#bib.bib32" title="" class="ltx_ref">reddit </a></cite>, and even StackOverflow are contemplating charges <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref">stackoverflow </a></cite>. Additionally, the LAION-5B dataset, which many companies relied on to train models like Stability AI’s Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">rombach2022stablediffusion </a></cite> and Google’s Imagen <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">saharia2022imagen </a></cite> for text-to-image tasks, is entangled in serious copyright issues regarding the contained images <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">chen2023pathway </a></cite>. Models trained with these datasets may face legal and copyright-related challenges, further complicating the use of public data sources and impeding the accuracy and relevance of FMs. In addition, using public data could leave individuals uncertain about the usage of their private data for training purposes.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">FL emerges as a viable solution by eliminating the need to illegally scrape large-scale data from the Internet while allowing FMs to leverage abundant data from a broader range of sources. With FL, FMs can benefit from prompt engineering, in-context learning, instruction tuning, fine-tuning, and pre-training using diverse and representative data. This increased access to data leads to the development of more accurate and efficient systems, better equipped to cater to the specific needs and preferences of users across different scenarios. Additionally, through the integration of FL with FM, the utilization of private data becomes more accountable, offering individuals the means to trace the sources of their data. This enhanced transparency empowers users with the ability to unlearn specific data instances from the FM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">liu2020federated </a>; <a href="#bib.bib38" title="" class="ltx_ref">che2023fast </a></cite>, ensuring a higher degree of control and privacy over their personal information.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">FL can also help reduce the labor cost associated with collecting and managing data in a central location for FMs, as data is kept locally by participants.</p>
</div>
<figure id="S2.F3" class="ltx_figure ltx_align_center"><img src="/html/2306.15546/assets/x5.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text" style="font-size:90%;">Motivations, challenges, and future directions of FL for FM.</span></figcaption>
</figure>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">High Computational Resource Demand</span>.
The training of large-scale FMs requires substantial computational resources. For instance, LLaMA (65 billion parameters) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">touvron2023llama </a></cite> utilizes 2048 NVIDIA A100 GPUs over a period of 21 days, while GPT-3-1.3B (1.3 billion parameters) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">brown2020gpt3 </a></cite> necessitates 64 Tesla V100 GPUs for a week-long training process. Additionally, the vision transformer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">dosovitskiy2020image </a></cite> (ViT/L) employed in CLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref">radford2021clip </a></cite> require 8 core TPUv3 for approximately 30 days of training. This poses a challenge for individual organizations that may lack the infrastructure or capacity to train the FM independently. FL mitigates this challenge by facilitating computation sharing, as shown in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.1 Motivations of FL for FM ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. By enabling collaborative training, FL allows participants to pool their computational power, thereby distributing the training process and alleviating the burden on individual organizations. This resource-sharing approach promotes the development and evolution of the FM by making it accessible to a wider range of participants.</p>
</div>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x6.png" id="S2.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="461" height="418" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">FM computation by a single entity</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x7.png" id="S2.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="460" height="319" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">FM computation shared among entities with FL</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Resource sharing empowered by FL for FM.</span></figcaption>
</figure>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">FM Dominance by Affluent Companies.</span>
The development of FM has primarily been driven by industry giants, well-funded startups, and government-supported companies, which limits the participation of smaller organizations and hampers diversity, innovation and democratization. Although there are open-source FM projects (e.g.,<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib41" title="" class="ltx_ref">taori2023alpaca </a></cite>, Vicuna <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref">chiang2023vicuna </a></cite>), most of these projects are based on the FM open-sourced from large companies, such as Meta’s LLaMA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">touvron2023llama </a></cite>. In contrast, OpenAI’s GPT-4 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">openai2023gpt4 </a></cite> and Google’s LaMDA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref">collins2021lamda </a></cite> have not been open sourced due to various reasons. The expertise, resources, and infrastructure required for building and advancing FMs are predominantly concentrated within large and well-funded companies, creating barriers to broader FM development and impeding collective progress in the field. However, FL emerges as a transformative approach that facilitates collaborative FM development among multiple participants. FL dismantles the exclusivity barriers and empowers a wider range of organizations to contribute to FM development. By pooling resources, knowledge and expertise, FL democratizes the FM development process, fostering inclusivity, scalability, broader deployment, and promoting innovation from diverse stakeholders. This shift towards collaboration ensures that FM development is not confined to a few privileged entities but becomes a collective effort, benefiting from diverse perspectives and expertise.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_bold">Outdated FM under Growing Data.</span>
Real-world scenarios involve data that continuously expands over time. For example, edge devices and IoT sensors are continuously collecting new data over time <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref">pellegrini2021continual </a>; <a href="#bib.bib45" title="" class="ltx_ref">aggarwal2023chameleon </a></cite> . It presents a great challenge in continuously updating the FM, especially when these data are distributed. FL provides a solution by facilitating the fine-tuning of FMs with incremental data and adapts FM to these data domains. Rather than starting the training process from scratch each time new data becomes available, FL allows for the seamless integration of newly collected data into existing models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">zhang2023fedcontinual </a></cite>. This continuous model improvement ensures that the FM remains up-to-date and easily adapts to the real world scenarios with continuously changing environment, leading to enhanced performance over time.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p"><span id="S2.SS1.p8.1.1" class="ltx_text ltx_font_bold">Response Delay and FM Service Down.</span>
One of the key advantages of leveraging FL for FM is the ability to provide instant response, thus providing better user experience. Traditional approaches involve running FMs centrally with API access, which can introduce both latency and privacy issues due to network communication between the users and the central server hosting the model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">li2023snapfusion </a></cite>. FMs on the cloud may also suffer from service down due to a series of unexpected circumstances <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">chatgpt-down </a></cite>.
However, by employing FL to train FMs, the models are deployed to edge devices, minimizing or eliminating the need for network communication, significantly reducing latency and potential privacy problems. This local deployment enables faster response and ensures a seamless and privacy-friendly user experience when interacting with the model.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">Overall, FM was motivated to be empowered by FL to overcome limitations related to data availability, hardware resources, continuous data growth, response delay and FM service down. These advancements pave the way for the widespread adoption and further development of the FM, enabling its application across various domains.
</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p">In the following sections, we delve deeper into the challenges and opportunities associated with empowering FM through FL. We explore how FL addresses data availability constraints, mitigates the limitations of computational resources, democratizes FM development, and adapts to the continuous growth of data. By thoroughly examining these aspects, we hope to offer valuable insights into the potential of FL in revolutionizing FM and unlocking its widespread adoption and further development across diverse domains.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Challenges of Empowering FM with FL</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Figure <a href="#S2.F5" title="Figure 5 ‣ 2.2 Challenges of Empowering FM with FL ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates possible approaches of integrating FL into the development of FM. Both cross-silo FL and cross-device FL offer potential benefits for empowering FMs, but they may face similar or distinct challenges. In this section, we explore the challenges associated with empowering FMs through both cross-silo and cross-device FL approaches.</p>
</div>
<figure id="S2.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x8.png" id="S2.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="462" height="534" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Train or fine-tune FM</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x9.png" id="S2.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="462" height="524" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Parallel FM training</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x10.png" id="S2.F5.sf3.g1" class="ltx_graphics ltx_img_square" width="462" height="420" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Fine-tune FM with adapter</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x11.png" id="S2.F5.1.g1" class="ltx_graphics ltx_img_portrait" width="461" height="1442" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.4.2" class="ltx_text" style="font-size:90%;">
Illustration of possible approaches for empowering Foundation Model (FM) with Federated Learning (FL): (a) FL can be utilized to train FM from scratch or fine-tune FM using private client data. (b) Another approach is to utilize FL to train FM from scratch with model parallelisms (splitting the model into parts to train in parallel) using private client data. (c) FL can also be employed to conduct collaborative parameter-efficient fine-tuning for FM by training an additional adapter or using prompt tuning methods. These approaches illustrate the potential of empowering FM with FL.</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Large Model Incurs High Memory, Communication, and Computation.</span>
Empowering FM with FL poses significant challenges due to substantial size of FMs, resulting in high memory, communication, and computation requirements. For instance, models like GPT-2 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">radford2019gpt2 </a></cite> with 1.3 billion parameters occupy around 2.6 GB, while GPT-3 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">brown2020gpt3 </a></cite> with 175 billion parameters takes up approximately 350 GB in half precision (FP16) measurements. Moreover, the GPT-4 surpass one trillion parameters <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">openai2023gpt4 </a></cite>. Figure <a href="#S2.F5.sf1" title="In Figure 5 ‣ 2.2 Challenges of Empowering FM with FL ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a> depicts a typical approach in FL, where the entire FM is transmitted and trained across multiple clients using their local data. However, the large size of FM introduces challenges in storage, model transmission, and computational resources for training or fine-tuning. These challenges can be broadly categorized into two main aspects:</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">1. <em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Difficulty in hosting FMs on FL clients</em>. Large models are resource-intensive to maintain and deploy. A predicament that could be particularly vexing in FL scenarios where clients may be constrained by limited storage and computational capabilities <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib49" title="" class="ltx_ref">lim2020federated </a>; <a href="#bib.bib50" title="" class="ltx_ref">zhuang2022smart </a></cite>. In cross-silo FL, organizations need to provide substantial infrastructure support and computing resources to accommodate the training or fine-tuning of these large models. This issue is even more challenging in cross-device FL, where client devices such as smartphones or IoT gadgets typically possess significantly reduced memory and computational power compared to the servers in cross-silo FL. Managing the computational load associated with large-scale FMs on these devices can lead to issues such as prolonged training times, accelerated battery depletion, and potential device overheating.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">2. <em id="S2.SS2.p4.1.1" class="ltx_emph ltx_font_italic">Huge communication cost associated with sharing FM in FL</em>.
Communication overhead is a significant concern in FL, as it requires transmitting model parameters or updates between clients and a central server. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref">shahid2021communication </a></cite>. Transmitting the extensive parameters of an FM over a network, especially with limited or unreliable bandwidth, can be time-consuming and resource-intensive. Frequent communication also increases power consumption and introduces privacy vulnerabilities, as amplified data transmission raises the risk of data interception. Moreover, the environmental impact caused by huge communication cannot be overlooked either. The substantial communication costs associated with large-scale models contribute to significant carbon emissions, necessitating measures to mitigate the environmental impact.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Overall, while empowering FM with FL provides a promising direction, it also presents substantial challenges related to memory, computation, and communication costs. Effectively addressing the challenges associated with managing large model sizes is crucial.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p"><span id="S2.SS2.p6.1.1" class="ltx_text ltx_font_bold">Challenges in Data Privacy and System Security.</span> Data privacy and system security are two critical concerns when empowering FM with FL. Data privacy concerns are a significant challenge in both cross-silo and cross-device FL. Attacks such as membership inference and model inversion attacks pose threats to privacy when sensitive information can be extracted from FMs such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">carlini2023extracting </a>; <a href="#bib.bib34" title="" class="ltx_ref">rombach2022stablediffusion </a></cite>. While techniques like deduplication, as demonstrated in models like DALL-E 2 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref">ramesh2021dall-e </a>; <a href="#bib.bib54" title="" class="ltx_ref">dall-e-2 </a></cite>, can help mitigate memorization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">carlini2023extracting </a></cite> and its associated privacy risks, applying such techniques in FL is more complex due to the distributed nature of the data across multiple participants and the lack of a centralized data repository. Safeguarding data privacy throughout the iterative FL process requires developing privacy-preserving mechanisms and protocols.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">The security of FL when applied to the FM is a critical concern. In both cross-silo and cross-device FL, there is a need to address potential security vulnerabilities and attacks that may arise from the participating clients <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">lyu2022privacy </a></cite>. The fusion of knowledge from multiple clients into the FM makes it crucial to ensure the integrity, confidentiality, and authenticity of the model. Breaches in security could lead to compromised models and result in severe consequences, as the FM would be utilized for a wide range of downstream tasks, potentially impacting quality, user experiences, even causing harms.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p"><span id="S2.SS2.p8.1.1" class="ltx_text ltx_font_bold">IP and Copyright Issues.</span>
Intellectual property (IP) and copyright issues are another important aspect to consider in order to ensure legal compliance and ethical usage of data. It is crucial to establish mechanisms that verify data ownership and rights of the contributed data by all participants. In the event of problematic data that violates IP or copyright, it is essential to develop methods for unlearning or rectifying the potential impact of the data on the FM.
</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p"><span id="S2.SS2.p9.1.1" class="ltx_text ltx_font_bold">Complex Incentive Mechanisms for Collaboration.</span> Designing effective incentive mechanisms for collaboration in FL presents an additional challenge. Building an ecosystem to train the FM requires careful consideration of how to incentivize entities such as institutions or individuals with varying amounts of data and computing power to actively participate <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">lyu2020towards </a>; <a href="#bib.bib57" title="" class="ltx_ref">lyu2020democratise </a></cite>. Since the FM holds significant power and can be utilized for various downstream tasks, defining the ownership of the model and ensuring fair profit allocation to participating parties is crucial. Moreover, when the FM is deployed to generate profit through APIs or other means, allocating profits and costs fairly among stakeholders becomes a critical aspect. While existing incentive mechanisms in conventional FL training with smaller models provide a good starting point <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref">lyu2020collaborative </a>; <a href="#bib.bib59" title="" class="ltx_ref">sim2020collaborative </a></cite>, adapting and developing new methods for large models in the context of FM may raise new challenges.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Opportunities and Future Directions</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Figure <a href="#S2.F6" title="Figure 6 ‣ 2.3 Opportunities and Future Directions ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the opportunities and future directions of FL for FM. We further illustrate them in detail below.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Integrating FL into the Lifespan of FMs.</span>
FL can empower the entire lifespan of FMs, encompassing pre-training, fine-tuning, and inference.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><em id="S2.SS3.p3.1.1" class="ltx_emph ltx_font_italic">Pre-training:</em> Firstly, FL offers the opportunity to leverage large-scale and decentralized datasets from multiple participants during the pre-training phase. This collaborative learning process leads to the development of more robust and generalized FMs that can effectively handle a wide range of tasks and domains.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><em id="S2.SS3.p4.1.1" class="ltx_emph ltx_font_italic">Fine-tuning:</em> Secondly, FL plays a crucial role in the fine-tuning of FMs by incorporating incremental data from participants. This iterative process ensures that the FMs remain up-to-date, adaptable, and capable of addressing evolving data patterns and challenges. Recent advancements, such as GPT-JT <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref">gpt-jt </a></cite>, have demonstrated the potential of obtaining powerful FMs from decentralized participants. Notably, GPT-JT, with only 6 billion parameters, outperforms many FMs with significantly larger sizes. Besides, we can use personalized FL techniques to obtain a personalized model for each client during the fine-tuning process.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><em id="S2.SS3.p5.1.1" class="ltx_emph ltx_font_italic">Inference:</em> Thirdly, FL enables collaborative model inference, utilizing a combined approach involving a central FM on the server and personalized FMs distributed across clients. Prompting emerges as a new method to interact and make inferences with FMs. Users can employ prompts directed either to the central FM on the server, to specific personalized FMs on clients, or a combination of both. Exploring optimal coordination strategies among these FMs remains a key area for further research to streamline and enhance the efficiency of inference processes.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">The integration of FL into the lifespan of FMs holds tremendous potential for fostering collaborative knowledge sharing among diverse participants. By exploring novel techniques and methodologies, we can unlock new horizons for enhancing the development, adaptability, and effectiveness of FMs. This transformative paradigm ensures that the benefits of FL extend beyond individual models or organizations, enabling collective progress and innovation.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2306.15546/assets/x12.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S2.F6.3.2" class="ltx_text" style="font-size:90%;">Opportunities and future directions of FL for FM.</span></figcaption>
</figure>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p"><span id="S2.SS3.p7.1.1" class="ltx_text ltx_font_bold">Memory, Communication, and Computation Reduction.</span> Addressing the challenges associated with memory, communication, and computation in FL for FM opens up promising avenues for improvement. There are several techniques that can be explored in this regard.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">1. <span id="S2.SS3.p8.1.1" class="ltx_text ltx_font_italic">Efficient distributed learning algorithms</span> are essential for FM under stringent resource constraints <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">zhao2022reduce </a>; <a href="#bib.bib62" title="" class="ltx_ref">wu2022communication </a></cite>. These algorithms should address the communication and computation challenges that arise when training and deploying large FMs across devices with varying capabilities and network conditions. Parallelism, such as model and pipeline parallelism, is a technique used in distributed training to handle large and computationally intensive models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref">dean2012large </a>; <a href="#bib.bib64" title="" class="ltx_ref">huang2019gpipe </a></cite>. Model parallelism involves partitioning the model and distributing it across devices for parallel processing, while pipeline parallelism improves efficiency and scalability. Figure <a href="#S2.F5.sf2" title="In Figure 5 ‣ 2.2 Challenges of Empowering FM with FL ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a> illustrates the concept of parallelism in FL for FM, where participants train partial layers of a model based on their private local data. Recent research shows the feasibility of using pipeline parallelism in decentralized FM training with heterogeneous devices <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref">yuan2022decentralized </a></cite>. Future studies can explore the application of model or pipeline parallelism in FL, considering the distributed nature of private data among participants. This exploration could involve innovations in communication-efficient methods, adaptive model updates, or decentralized optimization techniques.</p>
</div>
<div id="S2.SS3.p9" class="ltx_para">
<p id="S2.SS3.p9.1" class="ltx_p">2. <span id="S2.SS3.p9.1.1" class="ltx_text ltx_font_italic">Parameter-efficient training methods</span> are proposed to adapt FMs to specific domains or tasks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref">houlsby2019petl-adapter </a>; <a href="#bib.bib67" title="" class="ltx_ref">hu2021lora </a></cite>. These methods commonly freeze the parameters of the FM and fine-tune only a small adapter. Figure <a href="#S2.F5.sf3" title="In Figure 5 ‣ 2.2 Challenges of Empowering FM with FL ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(c)</span></a> depicts a possible approach for adopting the adapter into the FL for FM. Recent works, FedCLIP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">lu2023fedclip </a></cite> and FFM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">yu2023federated </a></cite>, adopt this method for fine-tuning the FMs and achieve significant performance improvement. By communicating and fine-tuning only the small adapters, these methods reduce the communication and computation demands. However, participants still require substantial computational resources to fit the FM and execute the fine-tuning.
Exploring novel approaches that optimize the utilization of computational resources and further reduce communication overhead in FL is crucial for advancing the field.</p>
</div>
<div id="S2.SS3.p10" class="ltx_para">
<p id="S2.SS3.p10.1" class="ltx_p">3. <span id="S2.SS3.p10.1.1" class="ltx_text ltx_font_italic">Prompt tuning</span> has been shown to be as effective as model tuning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref">lester2021prompttuning </a></cite>. Instead of sharing model parameters of large FMs, prompt offers a new communication-efficient sharing paradigm.
This technique involves freezing the entire FM’s parameters and fine-tuning lightweight additional tokens. Investigating methods for refining prompts in the FL setting, where knowledge from multiple participants can be leveraged to enhance the prompts used in FM training, holds promise for improving FM performance. The integration of this technique is similar to the approach depicted in Figure <a href="#S2.F5.sf3" title="In Figure 5 ‣ 2.2 Challenges of Empowering FM with FL ‣ 2 Federated Learning for Foundation Models ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(c)</span></a>. Recent studies, including FedPrompt <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref">zhao2023fedprompt </a></cite> and PROMPTFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">guo2022promptfl </a></cite>, have showcased the potential of leveraging FL to enhance the quality and effectiveness of prompts in FM training. However, these studies either assume the presence of large FMs on user devices, which may not be feasible in resource-constrained cross-device settings, or face privacy concerns by relying on cloud FM APIs that require sending the sensitive data. Further research and exploration in this area can unlock new possibilities for leveraging collective knowledge to optimize prompts and enhance FM capabilities.</p>
</div>
<div id="S2.SS3.p11" class="ltx_para">
<p id="S2.SS3.p11.1" class="ltx_p">4. <span id="S2.SS3.p11.1.1" class="ltx_text ltx_font_italic">Advanced model compression techniques</span> are vital for reducing the high memory, communication, and computational costs incurred by large models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib72" title="" class="ltx_ref">song2022resfed </a></cite>. Model compression can reduce the size of FMs without a significant loss in model performance, thereby making them more manageable for deployment in FL. Commonly used compression mechanisms can be considered for adaptation to FMs. For example, through knowledge distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref">hinton2015distilling </a></cite>, a smaller model is trained to replicate the behavior of a larger one; quantization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref">han2015compression </a>; <a href="#bib.bib75" title="" class="ltx_ref">nagel2021white </a></cite> reduces the numerical precision of the model’s parameters, further reducing model size and computational requirements. Future research should aim to push the boundaries of these techniques, striving to compress and quantize FMs to the greatest extent possible without compromising their ability to generate valuable insights.</p>
</div>
<div id="S2.SS3.p12" class="ltx_para">
<p id="S2.SS3.p12.1" class="ltx_p">Moreover, these techniques can be combined in FL to further reduce communication and computation while optimizing the performance of FM. For instance, Offsite-Tuning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib76" title="" class="ltx_ref">xiao2023offsite </a></cite> applies compression methods <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref">han2015compression </a>; <a href="#bib.bib73" title="" class="ltx_ref">hinton2015distilling </a></cite> and the concept of parameter-efficient tuning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref">houlsby2019petl-adapter </a></cite> to gain knowledge from a single private data owner. It demonstrates the potential of combining these techniques, further exploration can be done to leverage the combination of these methods to optimize FMs with FL involving multiple participants.</p>
</div>
<div id="S2.SS3.p13" class="ltx_para">
<p id="S2.SS3.p13.1" class="ltx_p"><span id="S2.SS3.p13.1.1" class="ltx_text ltx_font_bold">Designing FL System and Benchmark for FM.</span> Designing specialized FL systems and benchmarks is important to cater to the unique requirements and challenges of FM. While existing FL systems have made significant progress in both academia and industry <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref">bonawitz2019towards </a>; <a href="#bib.bib78" title="" class="ltx_ref">he2020fedml </a>; <a href="#bib.bib79" title="" class="ltx_ref">huba2022papaya </a>; <a href="#bib.bib80" title="" class="ltx_ref">beutel2020flower </a>; <a href="#bib.bib81" title="" class="ltx_ref">liu2021fate </a>; <a href="#bib.bib82" title="" class="ltx_ref">zhuang2022easyfl </a>; <a href="#bib.bib83" title="" class="ltx_ref">xie2023federatedscope </a></cite>, they may not fully address the optimization of memory, communication, and computation specifically for FM. Platforms like FedML <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib84" title="" class="ltx_ref">fedml-llm </a></cite> and FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib85" title="" class="ltx_ref">fate-llm </a></cite> have started evolving to support FM, but further research is needed to explore the system requirements and integration strategies. This involves considering scalability, efficiency, and privacy aspects in FM training and inference, and devising mechanisms to handle the large model sizes and resource constraints. Additionally, the development of comprehensive benchmarks tailored for FM can facilitate fair comparisons and evaluation of different FL approaches and algorithms. These benchmarks should take into account the specific characteristics of FMs and the data distributions encountered in real-world scenarios. By designing dedicated FL systems and benchmarks for FM, researchers and practitioners can advance the state-of-the-art in FL and enable more efficient and effective training of FMs in a collaborative and privacy-preserving manner.</p>
</div>
<div id="S2.SS3.p14" class="ltx_para">
<p id="S2.SS3.p14.1" class="ltx_p"><span id="S2.SS3.p14.1.1" class="ltx_text ltx_font_bold">Improving FM with Decentralized Data.</span> Leveraging the distributed nature of FL, there is an opportunity to effectively utilize small amounts of scattered data to improve FMs. While FMs are typically trained on large-scale centralized datasets, there may be domain gaps and limitations in their coverage of real-world data distributions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">hong2023mecta </a></cite>. FL can bridge these gaps by incorporating data from diverse sources, ensuring a more comprehensive representation of different scenarios and improving the generalization capabilities of FMs. Moreover, the increasing computational power of
personal devices, such as laptops and mobile phones, presents an opportunity to make FL more accessible for FM.
Cross-device FL can leverage the enhanced computation capabilities to facilitate fine-tuning and incorporate knowledge back into FMs. While training an FM from scratch using cross-device FL may currently be challenging due to memory and computation limitations, advancements in device capabilities and model size reduction pave the way for potential future developments.
</p>
</div>
<div id="S2.SS3.p15" class="ltx_para">
<p id="S2.SS3.p15.1" class="ltx_p"><span id="S2.SS3.p15.1.1" class="ltx_text ltx_font_bold">Advancing Trustworthy FL for FM.</span>
Trustworthy FL for FM requires addressing various aspects, including privacy, security, robustness, bias mitigation, interpretability, model ownership, IP and copyright issues, and more. To enhance the security and robustness of FL for FM, mechanisms need to be developed to detect and prevent attacks from malicious participants and ensure the integrity of FMs. Resolving biases in the training data and improving the interpretability of FMs are important for developing fair and accountable AI systems. Additionally, protocols and frameworks should be established to address IP and copyright concerns when incorporating data from multiple clients in FL, while also considering the issue of model ownership and IP rights. Moreover, investigating techniques such as deduplication in FL for addressing the memorization problem presents an important opportunity for research and development. Deduplication has been previously applicable in centralized settings, such as identifying similar images <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">dall-e-2 </a></cite>. A distributed deduplication mechanism is needed in the setting of FL. Addressing these challenges and considerations is crucial for ensuring the trustworthiness and effectiveness of FL for FM.</p>
</div>
<div id="S2.SS3.p16" class="ltx_para">
<p id="S2.SS3.p16.1" class="ltx_p"><span id="S2.SS3.p16.1.1" class="ltx_text ltx_font_bold">Exploring Incentive Mechanisms in FL for FM.</span>
Exploring incentive mechanisms in the context of FL with FM is a crucial area of research. While incentive mechanisms have been explored in conventional FL with smaller models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref">lyu2020collaborative </a></cite>, the unique challenges posed by large FMs require tailored solutions. Designing effective and fair incentive mechanisms that consider the varying amounts of data and computing power across institutions is essential for encouraging the widespread participation and fostering collaboration among participants. This includes addressing the disparities in data contributions and computational resources, ensuring that incentives are aligned with the effort and value provided by each participant.</p>
</div>
<div id="S2.SS3.p17" class="ltx_para">
<p id="S2.SS3.p17.1" class="ltx_p">In addition to encouraging participation, the allocation of profits and costs associated with deploying FMs via APIs is another important consideration. Fair distribution of rewards and benefits is necessary to maintain trust and encourage ongoing engagement. Mechanisms should be established to determine how profits generated from FMs are allocated among the participants. This ensures an equitable distribution of economic benefits and encourages continued participation and investment in FL for FM.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Foundation Models for Federated Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In addition to the advantages that Federated Learning (FL) brings to the Foundation Model (FM), FM itself can greatly contribute to enhancing the training of FL. Figure <a href="#S3.F7" title="Figure 7 ‣ 3 Foundation Models for Federated Learning ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides a concise overview of the motivations, challenges, opportunities, and future directions of FM for FL. In this section, we delve into the motivation behind FM for FL, examine the challenges that arise in this context, and explore the potential opportunities that FM brings to the field of FL.</p>
</div>
<figure id="S3.F7" class="ltx_figure ltx_align_center"><img src="/html/2306.15546/assets/x13.png" id="S3.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text" style="font-size:90%;">Motivations, challenges, and future directions of FM for FL.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivations of FM for FL</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Data Privacy and Shortage Dilemma in FL.</span>
In FL scenarios, clients may have limited or imbalanced data, such as in federated few-shot learning contexts <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib87" title="" class="ltx_ref">shome2021fedaffect </a></cite>. This data scarcity can lead to suboptimal performance of the model as it might not capture the full diversity of the data distribution <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib88" title="" class="ltx_ref">gao2022survey </a></cite>. Further, several studies have demonstrated that sensitive data information can be recovered from the model updates in FL, raising severe privacy concerns <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">lyu2022privacy </a>; <a href="#bib.bib89" title="" class="ltx_ref">chen2022practical </a></cite>.
These issues become more pronounced in exceptional scenarios where the data is highly sensitive or difficult to collect. For example, in healthcare or financial sectors, data privacy regulations or the sensitive nature of the data itself might limit the availability of training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib90" title="" class="ltx_ref">zhang2022dense </a></cite>. This lack of data presents a significant hurdle to the training process, potentially hindering the effectiveness and applicability of FL in these critical domains.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">One promising approach to addressing these challenges is the generation and use of synthetic data from FMs, such as those created using Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">rombach2022high </a></cite> or GPT. FMs, having been pre-trained on vast datasets and further refined through fine-tuning, prompt engineering, in-context learning, and instruction tuning, have a rich understanding of the data distribution. This enables them to generate synthetic data that accurately reflects the diversity and complexity of real-world scenarios.
In cross-silo FL, where clients have the computational resources to host FMs, each client can use a pre-trained FM to generate synthetic data for local model training. In contrast, in cross-device FL, where clients may lack the memory to host an FM, the server can generate synthetic data using the FM. Figure <a href="#S3.F8.sf1" title="In Figure 8 ‣ 3.1 Motivations of FM for FL ‣ 3 Foundation Models for Federated Learning ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a> illustrates a feasible approach of using FM to generate synthetic data. This synthetic data can then be transmitted to clients for local training or used to fine-tune the global model. Using synthetic data generated by FM in FL offers several key benefits:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">1. <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Alleviating data scarcity.</em> Synthetic data can help overcome the issue of data scarcity, particularly for clients with limited amounts of training data or for knowledge distillation settings that require public data for alignment <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib90" title="" class="ltx_ref">zhang2022dense </a></cite>. By generating more data to enrich original training data or serve as public data, data scarcity problem can be largely alleviated.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">2. <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Introducing diversity and reducing overfitting.</em> Synthetic data can introduce additional diversity into the training process, representing scenarios or cases that may not be covered in the original data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib91" title="" class="ltx_ref">de2022mitigating </a></cite>. This can help to reduce overfitting, enhance the generalization capabilities of the model, and mitigate the issue of suboptimal performance due to limited or imbalanced data.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">3. <em id="S3.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Preserving privacy.</em> Synthetic data can also help preserve data privacy. By supplementing or replacing sensitive training data with synthetic data, it is possible to mitigate privacy concerns <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib92" title="" class="ltx_ref">dong2022privacy </a>; <a href="#bib.bib93" title="" class="ltx_ref">dockhorn2022differentially </a></cite> and defend against various privacy attacks in FL.</p>
</div>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x14.png" id="S3.F8.sf1.g1" class="ltx_graphics ltx_img_square" width="462" height="456" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">FM data generation for FL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x15.png" id="S3.F8.sf2.g1" class="ltx_graphics ltx_img_square" width="461" height="502" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">FM for better FL initialization</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F8.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2306.15546/assets/x16.png" id="S3.F8.1.g1" class="ltx_graphics ltx_img_portrait" width="462" height="1313" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text" style="font-size:90%;">
Illustration of possible approaches for empowering Federated Learning (FL) with Foundation Model (FM): (a) One or multiple FMs can be used to synthesize data and be further used for training on the server or clients. (b) FM can be compressed into a smaller model as initialization for FL, which allows for personalized adaptation and efficient model initialization. These approaches illustrate the potential of empowering FL with FM. The variants and combinations of these approaches could be further considered in future opportunities.</span></figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">In summary, the use of synthetic data generated by FMs in FL systems offers a promising approach to overcoming some of the significant challenges in FL, including data scarcity, privacy concerns, and suboptimal model performance. By harnessing the rich data understanding of FMs, we can generate high-quality synthetic data that enhances the effectiveness and applicability of FL in diverse real-world scenarios.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">Performance Dilemma in FL.</span>
FL can overcome challenges associated with non-iid and biased data by leveraging the advanced capabilities of FMs, leading to improved performance across different tasks and domains <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">yu2023federated </a></cite>. In more detail, FMs can help improve the performance of FL
through multiple ways:</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">1. FMs can serve as a good starting point for FL. Clients can directly conduct fine-tuning on their local data without training from scratch <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">chen2023importance </a>; <a href="#bib.bib27" title="" class="ltx_ref">nguyen2022begin </a>; <a href="#bib.bib94" title="" class="ltx_ref">tan2022federated </a></cite>. Models can converge faster and achieve better performance, without numerous communication rounds, thus avoiding huge communication cost.
By using a pre-trained foundation model, which has been trained on a large and diverse dataset, we can enhance the learning process for the model in the FL system.</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">2. FMs can serve as a strong generator to help synthesize more diverse data to enrich training data in FL. The most recent GPT-FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">zhang2023gpt </a></cite> leverages generative pre-trained models to generate diverse synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data using the standard FL framework. It outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency.</p>
</div>
<div id="S3.SS1.p10" class="ltx_para">
<p id="S3.SS1.p10.1" class="ltx_p">3. FMs can serve as a good teacher to help address the suboptimal performance issue in FL through knowledge distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib95" title="" class="ltx_ref">li2019fedmd </a>; <a href="#bib.bib96" title="" class="ltx_ref">yang2023knowledge </a></cite>.
Knowledge distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref">hinton2015distilling </a></cite> involves utilizing the outputs of the FM (acting as the "teacher") to guide the training of a simpler model within the FL system (the "student"). In Figure <a href="#S3.F8.sf2" title="In Figure 8 ‣ 3.1 Motivations of FM for FL ‣ 3 Foundation Models for Federated Learning ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>, an approach is depicted where the FM serves as a teacher model to compress the student model through methods like knowledge distillation. Each client then utilizes the pre-trained student model to enhance its local training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">chen2023importance </a>; <a href="#bib.bib27" title="" class="ltx_ref">nguyen2022begin </a></cite>. By transferring knowledge from the FM to the smaller model, the performance and generalization abilities of the student model can be improved, mitigating the suboptimal performance issue caused by limited or imbalanced data.</p>
</div>
<div id="S3.SS1.p11" class="ltx_para">
<p id="S3.SS1.p11.1" class="ltx_p"><span id="S3.SS1.p11.1.1" class="ltx_text ltx_font_bold">New Sharing Paradigm Empowered by FM.</span> In contrast to the conventional approach of sharing high-dimensional model parameters in traditional FL, FMs pave the way for a novel sharing paradigm by incorporating the concept of prompt tuning.</p>
</div>
<div id="S3.SS1.p12" class="ltx_para">
<p id="S3.SS1.p12.1" class="ltx_p">PROMPTFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">guo2022promptfl </a></cite> demonstrated how the capabilities of FMs can be harnessed to simultaneously facilitate efficient global aggregation and local training on sparse data. This is achieved by allowing federated participants to focus on training prompts rather than a shared model.</p>
</div>
<div id="S3.SS1.p13" class="ltx_para">
<p id="S3.SS1.p13.1" class="ltx_p">Following a similar trajectory, Zhao et.al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib61" title="" class="ltx_ref">zhao2022reduce </a></cite> introduced FedPrompt, an innovative prompt tuning method tailored for FL. This approach involves keeping pre-trained language models (PLMs) fixed and focusing solely on the aggregation and tuning of ’soft’ prompts, with the objective of reducing communication overhead. This paradigm shift brought by FM, pivoting from shared model parameters to shared prompts, sets a new precedent in the field of FL.</p>
</div>
<div id="S3.SS1.p14" class="ltx_para">
<p id="S3.SS1.p14.1" class="ltx_p"><span id="S3.SS1.p14.1.1" class="ltx_text ltx_font_bold">Multiple-Task Capability Empowered by FM.</span>
Most Federated Learning (FL) models are limited to supporting a single task <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib97" title="" class="ltx_ref">zhuang2023mas </a></cite>, posing challenges for real-world scenarios, such as autonomous driving, which requires handling multiple tasks concurrently. FMs present a promising solution to this constraint. FMs, predominantly equipped with multi-task capabilities <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib98" title="" class="ltx_ref">wang2022image </a>; <a href="#bib.bib99" title="" class="ltx_ref">xiao2023florence </a>; <a href="#bib.bib100" title="" class="ltx_ref">zhang2023next </a></cite>, offer a compelling avenue for addressing diverse task requirements. By adeptly incorporating FMs into FL system, FL models can gain the capacity to handle multiple tasks seamlessly.
</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Challenges in Foundation Models for Federated Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Law Compliance and Responsible Technology Usage of FMs for FL.</span>
While the use of FMs in FL holds immense potential, it also brings forth a multitude of challenges that need to be addressed, stemming primarily from privacy, intellectual property (IP) rights, and the potential misuse of technology. These challenges arise from the fact that many FMs are trained on voluminous data sets, often acquired from the internet without adherence to the proper legal and ethical protocols. This practice exposes federated learning systems to a high risk of violating privacy and intellectual property rights, and opens a window for potential misuse of the technology.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">1. <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">Privacy law violation.</em> One significant challenge is the formidable memory capability of FMs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">carlini2023extracting </a>; <a href="#bib.bib101" title="" class="ltx_ref">somepalli2022diffusion </a>; <a href="#bib.bib102" title="" class="ltx_ref">somepalli2023understanding </a></cite>. These models have the capacity to reproduce data directly from the training sets, a feature that raises serious privacy concerns <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref">stable-diffusion-litigation-url </a></cite>. If the synthetic data generated by the FMs is too closely aligned with the original training data, the risk of revealing sensitive or proprietary information increases substantially. This is not just a potential violation of privacy rights, but also a legal concern as it can lead to copyright infringement and ownership disputes.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">2. <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">IP law violation.</em> Another associated challenge is the need to ensure that synthetic data generated for use in FL do not exist verbatim in the real world. Ensuring this requires the development and application of techniques for analyzing the origin of synthesized data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib104" title="" class="ltx_ref">wang2023alteration </a></cite>. This is not a trivial task as it involves tracing the lineage of synthetic data back to its original sources and verifying its uniqueness. Such processes require substantial computational resources and sophisticated algorithms, posing an additional challenge in resource-constrained federated learning scenarios. In addition to ethical data usage, it is also important to ensure that FM users (e.g., FL clients) do not violate the license or copy FMs for profits <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref">peng2023you </a>; <a href="#bib.bib106" title="" class="ltx_ref">xu2022student </a>; <a href="#bib.bib107" title="" class="ltx_ref">hecater </a></cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">3. <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">Misuse of technology.</em> Moreover, the potential misuse of technology is a challenge that must be acknowledged. As FMs become more powerful and widely used, the risk of these models being exploited for nefarious purposes increases <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">chen2023pathway </a></cite>. Safeguarding against such misuse requires a robust framework of ethical guidelines and regulations, which adds to the complexity of integrating FMs into federated learning.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Problematic Synthetic Data Generated by FMs.</span>
Although FM can generate a lot of synthetic data, the quality of the generated data cannot be guaranteed due to below reasons.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">1. <em id="S3.SS2.p6.1.1" class="ltx_emph ltx_font_italic">Problematic data sources</em>. This is due to the fact that the large-scale data scraped from the Internet may be of low-quality, containing bias, misinformation, toxicity, even poisoned. FMs trained or fine-tuned on these problematic data without mitigation strategies can inherit harmful stereotypes, social biases, toxicity, even make errors. These problems can be inherited even exacerbated in the generated data of FMs. For example, LAION dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib108" title="" class="ltx_ref">schuhmann2022laion </a></cite> used for training Stable Diffusion can unintentionally contain bias, reinforce harmful stereotypes, exclude or marginalize certain groups, and contain toxic data sources, the synthesized images may inherit these problems. Training on the problematic generated data will create new problems or exacerbate current problems in existing FL models rather than mitigating them. Therefore, sifting out clean subsets in the presence of problematic data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib109" title="" class="ltx_ref">zeng2022sift </a></cite> is of paramount importance when FM is used to empower the training of FL.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">2. <em id="S3.SS2.p7.1.1" class="ltx_emph ltx_font_italic">Problematic data synthesis</em>. In addition to the problems caused by the problematic data sources used for training FMs, if the synthetic data does not align with the clients’ data distribution, it could also introduce the bias and noise into the FL process <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib110" title="" class="ltx_ref">mohri2019agnostic </a></cite>. For instance, if certain classes or features are over- or under-represented in the synthetic data, the resulting FL model could become skewed towards these classes or features, leading to poor generalization and biased predictions.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p"><span id="S3.SS2.p8.1.1" class="ltx_text ltx_font_bold">Unique Challenges in the Integration of FMs into FL</span>.
Present research on the integration of FMs into FL systems is still in its nascent stage, with a considerable gap between the current state of research and practical deployment. The complexity and variability of real-world scenarios present numerous challenges that need to be adequately addressed to bridge this gap.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p id="S3.SS2.p9.1" class="ltx_p">1. <em id="S3.SS2.p9.1.1" class="ltx_emph ltx_font_italic">Label Scarcity in FL.</em>
Current research often assumes the availability of labeled data for training, but in practical scenarios, labels may be scarce or completely absent <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref">zhuang2021fedureid </a>; <a href="#bib.bib111" title="" class="ltx_ref">li2022mocosfl </a>; <a href="#bib.bib112" title="" class="ltx_ref">hong2022outsourcing </a>; <a href="#bib.bib113" title="" class="ltx_ref">liu2022optimizing </a></cite>. This discrepancy demands further research into unsupervised or semi-supervised methods, and how these approaches can be incorporated into FL systems using FMs.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.1" class="ltx_p">2. <em id="S3.SS2.p10.1.1" class="ltx_emph ltx_font_italic">Continuously changing environment in FL.</em>
In real-world scenarios, data usually arrives in a continuous stream rather than static batches <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib114" title="" class="ltx_ref">kirkpatrick2017overcoming </a>; <a href="#bib.bib115" title="" class="ltx_ref">bai2023towards </a>; <a href="#bib.bib46" title="" class="ltx_ref">zhang2023fedcontinual </a></cite>. Existing research often overlooks this aspect, leading to a gap in how FMs can be effectively adapted for continual learning in an FL context. Research into
how to leverage the generalizability of FM to address continual learning problem, while managing memory and computational constraints, is essential.</p>
</div>
<div id="S3.SS2.p11" class="ltx_para">
<p id="S3.SS2.p11.1" class="ltx_p">3. <em id="S3.SS2.p11.1.1" class="ltx_emph ltx_font_italic">Domain gap in FL.</em>
The domain gap between training data and test data is another challenge often understated in current research <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib116" title="" class="ltx_ref">glorot2011domain </a>; <a href="#bib.bib117" title="" class="ltx_ref">long2015learning </a>; <a href="#bib.bib118" title="" class="ltx_ref">zhuang2021fedfr </a>; <a href="#bib.bib86" title="" class="ltx_ref">hong2023mecta </a>; <a href="#bib.bib119" title="" class="ltx_ref">tan2023heterogeneity </a>; <a href="#bib.bib120" title="" class="ltx_ref">fan2023federated </a></cite>. In practice, there can be significant differences between the data used for training FMs and the data encountered during deployment in FL systems. Research into domain adaptation techniques, such as transfer learning and domain alignment, can be beneficial to minimize the impact of this gap.</p>
</div>
<div id="S3.SS2.p12" class="ltx_para">
<p id="S3.SS2.p12.1" class="ltx_p">4. <em id="S3.SS2.p12.1.1" class="ltx_emph ltx_font_italic">Data and system heterogeneity in FL.</em>
Current research typically assumes a homogeneous data distribution and system capabilities across clients, which is rarely the case in real-world FL scenarios <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib121" title="" class="ltx_ref">fan2022fedskip </a>; <a href="#bib.bib122" title="" class="ltx_ref">liu2022no </a></cite>. The diverse data distributions and system capacities across clients in FL calls for more personalized and customized solutions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib123" title="" class="ltx_ref">wang2023towards </a></cite>.
Hence, it needs to be considered in the design and application of FMs in FL systems. This calls for more research into methods for handling data and system heterogeneity, such as personalized FL and adaptive learning strategies.</p>
</div>
<div id="S3.SS2.p13" class="ltx_para">
<p id="S3.SS2.p13.1" class="ltx_p">5. <em id="S3.SS2.p13.1.1" class="ltx_emph ltx_font_italic">Staleness in FL</em>.
Another subtle yet important issue is the increased risk of model and data staleness in FL. In a FL setup, where multiple clients are updating the model in a distributed manner, slow transmission of large models can lead to a situation where updates are based on outdated information by the time they are received and integrated, affecting the convergence and overall performance of the model.
Furthermore, in the case of unreliable or intermittent network connections, which are not uncommon in real-world scenarios, the transmission of large models might be frequently interrupted, leading to incomplete updates and further compounding the above issues.</p>
</div>
<div id="S3.SS2.p14" class="ltx_para">
<p id="S3.SS2.p14.1" class="ltx_p">6. <em id="S3.SS2.p14.1.1" class="ltx_emph ltx_font_italic">Static vs dynamic FMs.</em>
Most research currently centers around static FMs, which might not effectively capture evolving business requirements, leading to information lag. The use of dynamic FMs, which can be updated and adapted over time, could be a potential solution. However, the challenges associated with updating FMs in a distributed and privacy-preserving manner while ensuring stability and performance are areas that need further exploration.</p>
</div>
<div id="S3.SS2.p15" class="ltx_para">
<p id="S3.SS2.p15.1" class="ltx_p">7. <em id="S3.SS2.p15.1.1" class="ltx_emph ltx_font_italic">White-box vs Black-box access to FMs.</em>
Due to the huge profit market of FMs, many of the advanced FMs are only available in a black-box manner via API <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib107" title="" class="ltx_ref">hecater </a>; <a href="#bib.bib124" title="" class="ltx_ref">he2022protecting </a></cite>.
In the absence of the white-box access to FMs, it becomes more challenging to bring the benefits of FMs to FL.</p>
</div>
<div id="S3.SS2.p16" class="ltx_para">
<p id="S3.SS2.p16.1" class="ltx_p">In conclusion, while the preliminary research into the use of FMs for FL is promising, there is a clear gap between research and practical deployment. To bridge this gap, more practical research that considers the complexities and nuances of real-world scenarios is necessary. This will ensure that the benefits of integrating FMs into FL systems can be fully realized in a practical, real-world context.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Opportunities and Future Directions</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Law Consideration and Responsible Usage of FM for FL.</span>
The integration of FMs into FL for synthetic data generation and knowledge distillation presents a promising but complex future direction, especially when considering privacy and intellectual property (IP) rights <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib125" title="" class="ltx_ref">wang2023did </a>; <a href="#bib.bib126" title="" class="ltx_ref">wang2023detect </a></cite>. In particular, ensuring that the use of FMs does not infringe upon privacy and IP laws is critical. For instance, synthetic data generated by these models could mimic real-world data to a degree that breaches privacy. Therefore, the challenge lies in striking a balance between leveraging the impressive capabilities of FMs and strictly adhering to privacy regulations and IP rights.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">1. <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">Verification of synthetic data</em>. One future research direction involves the development of advanced methods to verify the synthetic data generated by FMs. It is essential that these methods can accurately determine if the synthetic data is sufficiently different from any real-world data to avoid privacy and IP infringements. This could involve the design of novel statistical methods for comparing data distributions or the application of machine learning techniques to detect similarities between synthetic and real-world data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib104" title="" class="ltx_ref">wang2023alteration </a></cite>. Meanwhile, designing more faithful metrics for the privacy assessment is also worthwhile to explore <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib127" title="" class="ltx_ref">sun2023privacy </a></cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">2. <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">Privacy-enhanced data synthesis or knowledge distillation</em>. Moreover, exploring new privacy-preserving techniques suitable for use with FMs is another key future direction. This could involve research into methods that allow FMs to generate synthetic data or distill knowledge in a way that inherently preserves privacy, such as differential privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib128" title="" class="ltx_ref">abadi2016deep </a></cite> or secure multi-party computation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib129" title="" class="ltx_ref">knott2021crypten </a></cite>. Additionally, privacy-preserving techniques can also be explored in the context of model training and inference, to ensure that the use of FMs does not expose sensitive information.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">3. <em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">Adherence to ethical guidelines</em>. Furthermore, the adherence to ethical guidelines in the use of FMs in FL is a significant area that requires further exploration. As FMs become increasingly prevalent, it is crucial to develop comprehensive ethical guidelines that cover their use in various scenarios, including synthetic data generation and knowledge distillation. These guidelines should encompass not only privacy and IP considerations, but also issues related to fairness, transparency, and accountability.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">4. <em id="S3.SS3.p5.1.1" class="ltx_emph ltx_font_italic">Interdisciplinary collaboration</em>. Finally, addressing these challenges will require a multi-disciplinary approach, involving not only advances in machine learning and statistics, but also contributions from fields such as law, ethics, and social sciences. This underscores the importance of interdisciplinary collaboration in the future of this research direction. As we venture into this new frontier, the need for robust, privacy-preserving, and ethically grounded methods is more critical than ever.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">In conclusion, FMs for FL opens up a multitude of opportunities, but it also brings along significant challenges that need responsible handling. The lawful and ethical use of these powerful models necessitates a collective effort from various disciplines. Achieving the balance between leveraging the potential of these models and complying with privacy and IP laws will be a defining factor for the future of FL and its applications. This is a complex task, but it’s critical to the continued advancement of FL and its use cases. It underscores the importance of legal considerations and responsible usage of FM for FL as we navigate this promising yet challenging path.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p"><span id="S3.SS3.p7.1.1" class="ltx_text ltx_font_bold">Robustness Consideration When Applying FMs to FL.</span>
When FMs are used to generate synthetic data or distill knowledge, the robustness of the resulting FL model could be influenced by the inherent challenges of the FMs.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p">1. <em id="S3.SS3.p8.1.1" class="ltx_emph ltx_font_italic">Robustness against bias.</em>
FMs, particularly those trained on data that is predominantly from a certain demographic, have the potential to generate synthetic data or distill knowledge that disproportionately favors that demographic. This could lead to fairness issues in the FL model, where the model exhibits superior performance for the favored demographic and inferior performance for others. Future research should aim to prevent the amplification of existing biases when FMs are integrated into FL systems. This may involve developing techniques for fair data augmentation, such as the creation of equitable prompts, fair knowledge distillation, and methodologies to assess and minimize any resulting bias.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<p id="S3.SS3.p9.1" class="ltx_p">2. <em id="S3.SS3.p9.1.1" class="ltx_emph ltx_font_italic">Robustness against out-of-domain generation.</em>
The quality and representativeness of synthetic data produced by FMs are essential to the effectiveness of FL. If the synthetic data generated by FM could not accurately represent the underlying distribution of the clients’ data in the FL system, it could lead to model performance degradation. Future research should focus on enhancing the quality and diversity of synthetic data produced by FMs, to better align with the underlying data distribution, domains, and modalities in the FL context.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.1" class="ltx_p">3. <em id="S3.SS3.p10.1.1" class="ltx_emph ltx_font_italic">Robustness against poisoned models.</em>
The integration of a compromised or poisoned FM into FL could lead to disastrous outcomes. For instance, a diffusion model with a concealed “backdoor” could initiate harmful activities when it detects a specific trigger pattern during data generation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib130" title="" class="ltx_ref">chou2022backdoor </a>; <a href="#bib.bib131" title="" class="ltx_ref">sun2023defending </a></cite>. This Trojan effect could inflict severe harm on downstream applications that rely on the compromised diffusion model. Given that research on FM robustness is currently limited, future research should concentrate on developing techniques to improve the robustness of FL models that utilize FMs. This could involve more robust FM development, more secure synthetic data generation, or more robust knowledge distillation techniques.</p>
</div>
<div id="S3.SS3.p11" class="ltx_para">
<p id="S3.SS3.p11.1" class="ltx_p">4. <em id="S3.SS3.p11.1.1" class="ltx_emph ltx_font_italic">Robustness against byzantine attackers and adversarial examples.</em>
Another two important robustness concepts in FL are robustness against byzantine clients <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib132" title="" class="ltx_ref">chen2022calfat </a>; <a href="#bib.bib133" title="" class="ltx_ref">liu2023byzantine </a></cite> and robustness against adversarial examples
 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib134" title="" class="ltx_ref">zhang2023delving </a>; <a href="#bib.bib135" title="" class="ltx_ref">wang2023robustness </a></cite>.
FMs introduce new challenges to the robustness in FL, such as the adversarial prompts introduced by byzantine attackers. It is of vital importance to ensure the loyalty and protocol adherence of FL clients. Otherwise, a simple adversarial prompt can compromise the whole FL system. Similarly, if FMs are vulnerable to adversarial examples, then the whole FL system will also inherit this vulnerability, resulting in wrong outputs from the trained models. Thus, safeguarding both FMs and the subsequent FL empowered by them against adversarial examples assumes utmost importance.</p>
</div>
<div id="S3.SS3.p12" class="ltx_para">
<p id="S3.SS3.p12.1" class="ltx_p">In conclusion, while the use of FMs in FL offers promising opportunities, it also presents robustness challenges that need to be thoroughly addressed. As we continue to explore the intersection of FMs and FL, it is essential that we develop methods and strategies to combat these potential pitfalls. Ensuring robustness against bias, out-of-domain generation, and poisoned models will be key to realizing the full potential of integrating FMs into FL systems. As we look towards the future, continued research and innovation in this area will be critical to advancing the field and unlocking new possibilities in FL with FMs.</p>
</div>
<div id="S3.SS3.p13" class="ltx_para">
<p id="S3.SS3.p13.1" class="ltx_p"><span id="S3.SS3.p13.1.1" class="ltx_text ltx_font_bold">Personalization of FM for FL.</span>
The exploration of personalized FMs adapted to diverse application scenarios in FL environments presents a promising future direction <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref">tan2022towards </a></cite>. This becomes especially pertinent in heterogeneous FL settings, where clients exhibit considerable variations in terms of memory and computation power. Given the impressive capabilities of FMs in few-shot or even zero-shot settings, the personalization of these models could be particularly beneficial to FL clients grappling with imbalanced classes and unseen domains. In more detail, below areas are worthwhile to explore.</p>
</div>
<div id="S3.SS3.p14" class="ltx_para">
<p id="S3.SS3.p14.1" class="ltx_p">1. <em id="S3.SS3.p14.1.1" class="ltx_emph ltx_font_italic">Tailoring FMs to heterogeneous clients in FL</em>. One future direction lies in developing techniques that allow FMs to better cater to the unique data characteristics and computational constraints of each client in the FL network. This could involve developing methods for tailoring the FM to specific application scenarios, or devising algorithms that can dynamically adjust the model’s complexity based on the computational resources available at each client. This line of work could also explore strategies for selectively transferring knowledge from the FM to the local models, focusing on the most relevant aspects for each client’s specific task.</p>
</div>
<div id="S3.SS3.p15" class="ltx_para">
<p id="S3.SS3.p15.1" class="ltx_p">2. <em id="S3.SS3.p15.1.1" class="ltx_emph ltx_font_italic">FM as a problem solver for data imbalance and domain shift in FL.</em> Heterogeneity is a critical aspect in real-world FL scenarios <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref">tan2023heterogeneity </a></cite>. There is a vast scope for exploring how FMs can address the challenge of data imbalance and domain shift in FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib137" title="" class="ltx_ref">zhao2018non-iid </a>; <a href="#bib.bib19" title="" class="ltx_ref">zhuang2020fedreid </a></cite>. Given their powerful capability to extract meaningful representations even from limited samples, FMs could be harnessed to boost the performance of FL clients that have limited or imbalanced local data. Future work could investigate how the rich representations learned by FMs can be leveraged to perform effective transfer learning, particularly for clients that encounter unseen domains.</p>
</div>
<div id="S3.SS3.p16" class="ltx_para">
<p id="S3.SS3.p16.1" class="ltx_p">3. <em id="S3.SS3.p16.1.1" class="ltx_emph ltx_font_italic">Personalized FMs for personalized data synthesis</em>.
Furthermore, the personalization of FMs can be extended to the realm of personalized data synthesis and privacy-aware FL. As FMs are capable of generating high-quality synthetic data, future research could investigate how this capability can be leveraged to create personalized synthetic datasets for each client. These datasets could potentially enhance the learning process while maintaining the privacy of the original data.</p>
</div>
<div id="S3.SS3.p17" class="ltx_para">
<p id="S3.SS3.p17.1" class="ltx_p">In conclusion, while the integration of FMs into FL has already shown promising results, there is much more potential to be unlocked. With a focus on personalization and adaptation to diverse application scenarios, the future of this research direction holds exciting possibilities for making FL more effective, efficient, and privacy-preserving.</p>
</div>
<div id="S3.SS3.p18" class="ltx_para">
<p id="S3.SS3.p18.1" class="ltx_p"><span id="S3.SS3.p18.1.1" class="ltx_text ltx_font_bold">More Advanced Knowledge Distillation Techniques of FMs for FL.</span>
The exploration of advanced knowledge distillation techniques for leveraging FMs in FL presents a fascinating area for future research. The potential that FMs hold for enhancing FL models is immense, given their capacity to encapsulate a rich understanding of data domains. However, the process of distilling this knowledge into FL models can be made more efficient and effective, which opens up new avenues for exploration.</p>
</div>
<div id="S3.SS3.p19" class="ltx_para">
<p id="S3.SS3.p19.1" class="ltx_p">1. <em id="S3.SS3.p19.1.1" class="ltx_emph ltx_font_italic">Specific knowledge distillation from FM to FL</em>. One potential direction for future work involves developing distillation techniques that are specifically tailored for the FL setting. Traditional distillation methods often assume access to large, centralized datasets and may not perform optimally in a federated environment. Therefore, new techniques that account for the unique challenges of FL, such as communication constraints and data heterogeneity across FL clients, could yield significant improvements. Furthermore, as we continue to improve our understanding of how FMs capture and represent knowledge, we can develop more sophisticated distillation techniques. These techniques could potentially target specific layers or components of the FM, allowing for more targeted and effective transfer of knowledge to FL.</p>
</div>
<div id="S3.SS3.p20" class="ltx_para">
<p id="S3.SS3.p20.1" class="ltx_p">2. <em id="S3.SS3.p20.1.1" class="ltx_emph ltx_font_italic">Low-cost knowledge distillation</em>. Additionally, the issue of computational cost in knowledge distillation is another aspect that future work could address. With FMs often being large and resource-demanding, there is a need for distillation techniques that can extract useful information without incurring high computational costs.</p>
</div>
<div id="S3.SS3.p21" class="ltx_para">
<p id="S3.SS3.p21.1" class="ltx_p">3. <em id="S3.SS3.p21.1.1" class="ltx_emph ltx_font_italic">Fair and privacy-preserving knowledge transfer from FM to FL</em>.
Finally, while the focus is often on performance, the future work should also consider the fairness and privacy implications of knowledge distillation in FL. As FMs are integrated into FL, it is critical to ensure that the benefits of this integration are distributed equitably across all FL clients, and that the privacy of each client’s data is respected throughout the distillation process.</p>
</div>
<div id="S3.SS3.p22" class="ltx_para">
<p id="S3.SS3.p22.1" class="ltx_p">4. <em id="S3.SS3.p22.1.1" class="ltx_emph ltx_font_italic">Black-box knowledge distillation from FMs to FL</em>.
Conducting black-box knowledge distillation from FMs within the context of FL involves a specific approach. In this process, the goal is to transfer the knowledge from a pre-trained FM, referred to as the teacher model, to many FL clients, known as the student models, without direct access to the internal parameters or architectural details of the teacher model. To achieve this, several techniques can be employed. One common method is to utilize the outputs of the teacher model as pseudo-labels for the training data in the FL setting. By aligning the predictions of the student model with the pseudo-labels provided by the teacher model, the student model learns to mimic the behavior and knowledge of the teacher model. This process typically involves iterative steps of training the student model, comparing its predictions with the teacher model’s outputs, and adjusting the student model’s parameters to minimize the discrepancy between the two. However, this process also typically involves a public data in the same domain for alignment, which might be not feasible in many sensitive scenarios.
It is appealing to investigate whether data-free black-box knowledge distillation techniques <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib138" title="" class="ltx_ref">zhang2023ideal </a></cite> can be utilized
to transfer the valuable knowledge from black-box FMs to FL clients.</p>
</div>
<div id="S3.SS3.p23" class="ltx_para">
<p id="S3.SS3.p23.1" class="ltx_p">Overall, the advancement of knowledge distillation techniques for integrating FMs into FL presents a promising and rich direction for future research, with the potential to significantly improve the performance, efficiency, fairness and privacy of FL systems.</p>
</div>
<div id="S3.SS3.p24" class="ltx_para">
<p id="S3.SS3.p24.1" class="ltx_p"><span id="S3.SS3.p24.1.1" class="ltx_text ltx_font_bold">Real-world Applications of FMs to FL.</span>
Real-world scenarios present a myriad of challenges when implementing FMs into FL, such as managing unlabeled data, handling continuous data streams, addressing domain discrepancies between training and test data, and navigating substantial heterogeneity in both data and systems. Section <a href="#S3.SS2" title="3.2 Challenges in Foundation Models for Federated Learning ‣ 3 Foundation Models for Federated Learning ‣ When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> delves into these challenges and highlights how FMs offer promising solutions. Furthermore, integrating FMs into FL presents an avenue to elevate real-world applications in various ways.</p>
</div>
<div id="S3.SS3.p25" class="ltx_para">
<p id="S3.SS3.p25.1" class="ltx_p"><em id="S3.SS3.p25.1.1" class="ltx_emph ltx_font_italic">How to effectively utilize multiple FMs for FL?</em>
Recent advancements, particularly with large language models (LLMs), have showcased remarkable capabilities in language understanding, generation, interaction, and reasoning. Shen et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib139" title="" class="ltx_ref">shen2023hugginggpt </a></cite> proposed HuggingGPT, which leveraged LLMs as controllers to effectively manage diverse AI models in addressing intricate tasks. The growing availability of FMs within open-source communities presents an opportunity for the seamless coordination among multiple FMs in FL for real-world applications.</p>
</div>
<div id="S3.SS3.p26" class="ltx_para">
<p id="S3.SS3.p26.1" class="ltx_p"><em id="S3.SS3.p26.1.1" class="ltx_emph ltx_font_italic">How FMs can accelerate FL deployment?</em> Deploying FL in real-world applications demands substantial effort, such as data preprocessing, model development, and communication optimization. FMs possess inherent zero-shot capabilities across diverse data types and excel in transfer learning. Leveraging FMs as initial models or employing them to generate synthetic data could significantly expedite the landing of FL. This acceleration results in faster convergence rates, better performance within FL implementations, ultimately expediting the integration of FL models into real-world applications.</p>
</div>
<div id="S3.SS3.p27" class="ltx_para">
<p id="S3.SS3.p27.1" class="ltx_p"><em id="S3.SS3.p27.1.1" class="ltx_emph ltx_font_italic">How to leverage FMs for FL in specific industries?</em> Further exploration of FMs in FL entails investigating their adaptability within specific industry verticals. For instance, evaluating the effectiveness of FMs in healthcare for predictive analytics or exploring their role in enhancing privacy-preserving mechanisms within FL setups. Moreover, examining FMs’ potential in facilitating cross-modal learning, where information from different data modalities is integrated, can unlock new possibilities for more comprehensive and diverse FL applications.</p>
</div>
<div id="S3.SS3.p28" class="ltx_para">
<p id="S3.SS3.p28.1" class="ltx_p">Moving forward, it is important to have more practical, application-oriented research. To bridge the gap between research and commercial deployment, future studies should not solely concentrate on the theoretical aspects of utilizing FMs in FL. They should also consider the constraints and requisites of real-world applications to ensure the practical viability of these efforts.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The integration of FM and FL presents a mutually beneficial synergy that holds immense potential for advancing AI. FL offers advantages such as data privacy, distributed learning, and scalable model development, while FM brings pre-trained knowledge and exceptional performance. This paper has explored the motivations, challenges, and future directions of empowering FM with FL and empowering FL with FM. On the one hand, FL mitigates challenges faced in FM development by expanding data availability, enabling computation sharing, and fostering collaborative FM development. On the other hand, FM also contributes to FL by serving as a robust starting point for model training, generating synthetic data, and enhancing overall performance. However, the integration of FL and FM also presents challenges that need to be addressed. These challenges include issues related to memory, communication, and computation costs associated with large FMs, as well as considerations on data privacy, system security, and intellectual property. Future research should focus on addressing these challenges and exploring the discussed opportunities.
By further exploring the synergistic relationship between FL and FM, we can drive advancements in both fields, enabling more efficient, effective, secure, privacy-preserving, and personalized model development. This integration has the potential to revolutionize AI across various domains and facilitate the deployment of advanced AI systems that are not only powerful but also ethically responsible.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2108.07258</span>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Tuo Zhang, Tiantian Feng, Samiul Alam, Mi Zhang, Shrikanth S Narayanan, and
Salman Avestimehr.

</span>
<span class="ltx_bibblock">Gpt-fl: Generative pre-trained model-assisted federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.02210</span>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
Wang Lu, Xixu Hu, Jindong Wang, and Xing Xie.

</span>
<span class="ltx_bibblock">Fedclip: Fast generalization and personalization for clip in
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13485</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.02311</span>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
et al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10403</span>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10684–10695, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.02643</span>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius
Hobbhahn, and Anson Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling
datasets in machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2211.04325</span>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao
Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso,
et al.

</span>
<span class="ltx_bibblock">Privacy-preserving federated brain tumour segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">International Workshop on Machine Learning in Medical
Imaging</span>, pages 133–141. Springer, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon
Bakas.

</span>
<span class="ltx_bibblock">Multi-institutional deep learning modeling without sharing patient
data: A feasibility study on brain tumor segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International MICCAI Brainlesion Workshop</span>, pages 92–104.
Springer, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Haobo Zhang, Junyuan Hong, Fan Dong, Steve Drew, Liangjie Xue, and Jiayu Zhou.

</span>
<span class="ltx_bibblock">A privacy-preserving hybrid federated learning framework for
financial crime detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.03654</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
Tao Liu, Zhi Wang, Hui He, Wei Shi, Liangliang Lin, Ran An, and Chenhao Li.

</span>
<span class="ltx_bibblock">Efficient and secure federated learning for financial applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 13(10):5877, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Yonggang Wen, Xuesen Zhang, Xin Gan, Daiying Yin, Dongzhan
Zhou, Shuai Zhang, and Shuai Yi.

</span>
<span class="ltx_bibblock">Performance optimization of federated person re-identification via
benchmark analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</span>, pages 955–963, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Yonggang Wen, and Shuai Zhang.

</span>
<span class="ltx_bibblock">Joint optimization in edge-cloud continuum for federated unsupervised
person re-identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</span>, pages 433–441, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan, Xuesen Zhang, Yonggang Wen, Shuai Zhang, and Shuai Yi.

</span>
<span class="ltx_bibblock">Federated unsupervised domain adaptation for face recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Conference on Multimedia and Expo
(ICME)</span>, pages 1–6. IEEE, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan, Yonggang Wen, and Shuai Zhang.

</span>
<span class="ltx_bibblock">Optimizing performance of federated person re-identification:
Benchmarking and analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM)</span>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
Khalil Muhammad, Qinqin Wang, Diarmuid O’Reilly-Morgan, Elias Tragos, Barry
Smyth, Neil Hurley, James Geraci, and Aonghus Lawlor.

</span>
<span class="ltx_bibblock">Fedfast: Going beyond average for faster training of federated
recommender systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th ACM SIGKDD international conference
on knowledge discovery &amp; data mining</span>, pages 1234–1242, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
Ramage.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.03604</span>, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han Wei Shen, and Wei-Lun Chao.

</span>
<span class="ltx_bibblock">On the importance and applicability of pre-training for federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning
Representations</span>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael Rabbat.

</span>
<span class="ltx_bibblock">Where to begin? on the impact of pre-training and initialization in
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.08090</span>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen,
Xing Xie, and Fangzhao Wu.

</span>
<span class="ltx_bibblock">Defending chatgpt against jailbreak attack via self-reminders.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, pages 1–11, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
Xiaofei Sun, Linfeng Dong, Xiaoya Li, Zhen Wan, Shuhe Wang, Tianwei Zhang,
Jiwei Li, Fei Cheng, Lingjuan Lyu, Fei Wu, et al.

</span>
<span class="ltx_bibblock">Pushing the limits of chatgpt on nlp tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.09719</span>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and
Ross Anderson.

</span>
<span class="ltx_bibblock">The curse of recursion: Training on generated data makes models
forget.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">arXiv preprint arxiv:2305.17493</span>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
Ryan Browne.

</span>
<span class="ltx_bibblock">Twitter will start charging developers for api access as elon musk
seeks to drive revenue.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Brian Fung.

</span>
<span class="ltx_bibblock">Reddit sparks outrage after a popular app developer said it wants him
to pay $20 million a year for data access.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Paresh Dave.

</span>
<span class="ltx_bibblock">Stack overflow will charge ai giants for training data.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10684–10695, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, et al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:36479–36494, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
Chen Chen, Jie Fu, and Lingjuan Lyu.

</span>
<span class="ltx_bibblock">A pathway towards responsible ai generated content.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.01325</span>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
Gaoyang Liu, Xiaoqiang Ma, Yang Yang, Chen Wang, and Jiangchuan Liu.

</span>
<span class="ltx_bibblock">Federated unlearning.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.13891</span>, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Tianshi Che, Yang Zhou, Zijie Zhang, Lingjuan Lyu, Ji Liu, Da Yan, Dejing Dou,
and Jun Huan.

</span>
<span class="ltx_bibblock">Fast federated machine unlearning with nonlinear functional theory.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto.

</span>
<span class="ltx_bibblock">Alpaca: A strong, replicable instruction-following model.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Stanford Center for Research on Foundation Models. https://crfm.
stanford. edu/2023/03/13/alpaca. html</span>, 3(6):7, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">See https://vicuna. lmsys. org (accessed 14 April 2023)</span>, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
Eli Collins and Zoubin Ghahramani.

</span>
<span class="ltx_bibblock">Lamda: our breakthrough conversation technology.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Google AI Blog</span>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(44)</span>
<span class="ltx_bibblock">
Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Graffieti, and Davide Maltoni.

</span>
<span class="ltx_bibblock">Continual learning at the edge: Real-time training on smartphone
devices.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.13127</span>, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
Shivam Aggarwal, Kuluhan Binici, and Tulika Mitra.

</span>
<span class="ltx_bibblock">Chameleon: Dual memory replay for online continual learning on edge
devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">2023 Design, Automation &amp; Test in Europe Conference &amp;
Exhibition (DATE)</span>, pages 1–6. IEEE, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
Jie Zhang, Chen Chen, Weiming Zhuang, and Lingjuan Lv.

</span>
<span class="ltx_bibblock">Addressing catastrophic forgetting in federated class-continual
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.06937</span>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang,
Sergey Tulyakov, and Jian Ren.

</span>
<span class="ltx_bibblock">Snapfusion: Text-to-image diffusion model on mobile devices within
two seconds.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.00980</span>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(48)</span>
<span class="ltx_bibblock">
Hayden Field.

</span>
<span class="ltx_bibblock">Openai says chatgpt downtime caused by targeted attack.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(49)</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 22(3):2031–2063,
2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(50)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Yonggang Wen, and Shuai Zhang.

</span>
<span class="ltx_bibblock">Smart multi-tenant federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2207.04202</span>, 2022.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(51)</span>
<span class="ltx_bibblock">
Osama Shahid, Seyedamin Pouriyeh, Reza M Parizi, Quan Z Sheng, Gautam
Srivastava, and Liang Zhao.

</span>
<span class="ltx_bibblock">Communication efficiency in federated learning: Achievements and
challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.10996</span>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(52)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace.

</span>
<span class="ltx_bibblock">Extracting training data from diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.13188</span>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(53)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
8821–8831. PMLR, 2021.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)</span>
<span class="ltx_bibblock">
Alex Nichol.

</span>
<span class="ltx_bibblock">Dall·e 2 pre-training mitigations.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(55)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang,
and S Yu Philip.

</span>
<span class="ltx_bibblock">Privacy and robustness in federated learning: Attacks and defenses.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>,
2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(56)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong
Jin, Han Yu, and Kee Siong Ng.

</span>
<span class="ltx_bibblock">Towards fair and privacy-preserving federated deep models.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
31(11):2524–2541, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(57)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Yitong Li, Karthik Nandakumar, Jiangshan Yu, and Xingjun Ma.

</span>
<span class="ltx_bibblock">How to democratise and protect ai: Fair and differentially private
decentralised deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Dependable and Secure Computing</span>,
19(2):1003–1017, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(58)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Federated Learning: Privacy and Incentive</span>, pages 189–204,
2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low.

</span>
<span class="ltx_bibblock">Collaborative machine learning with incentive-aware model rewards.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
8927–8936. PMLR, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Together.

</span>
<span class="ltx_bibblock">Releasing gpt-jt powered by open-source ai.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu.

</span>
<span class="ltx_bibblock">Reduce communication costs and preserve privacy: Prompt tuning method
in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.12268</span>, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning via knowledge
distillation.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Nature communications</span>, 13(1):2032, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(63)</span>
<span class="ltx_bibblock">
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al.

</span>
<span class="ltx_bibblock">Large scale distributed deep networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 25, 2012.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(64)</span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline
parallelism.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen,
Percy S Liang, Christopher Re, and Ce Zhang.

</span>
<span class="ltx_bibblock">Decentralized training of foundation models in heterogeneous
environments.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:25464–25477, 2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
2790–2799. PMLR, 2019.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(67)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(68)</span>
<span class="ltx_bibblock">
Sixing Yu, J Pablo Muñoz, and Ali Jannesari.

</span>
<span class="ltx_bibblock">Federated foundation models: Privacy-preserving and collaborative
learning for large models.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.11414</span>, 2023.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(69)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.08691</span>, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(70)</span>
<span class="ltx_bibblock">
Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu.

</span>
<span class="ltx_bibblock">Fedprompt: Communication-efficient and privacy-preserving prompt
tuning in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 1–5. IEEE, 2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(71)</span>
<span class="ltx_bibblock">
Tao Guo, Song Guo, Junxiao Wang, and Wenchao Xu.

</span>
<span class="ltx_bibblock">Promptfl: Let federated participants cooperatively learn prompts
instead of models–federated learning in age of foundation model.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.11625</span>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(72)</span>
<span class="ltx_bibblock">
Rui Song, Liguo Zhou, Lingjuan Lyu, Andreas Festag, and Alois Knoll.

</span>
<span class="ltx_bibblock">Resfed: Communication efficient federated learning by transmitting
deep compressed residuals.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.05602</span>, 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(73)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(74)</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William J Dally.

</span>
<span class="ltx_bibblock">Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.00149</span>, 2015.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(75)</span>
<span class="ltx_bibblock">
Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart
Van Baalen, and Tijmen Blankevoort.

</span>
<span class="ltx_bibblock">A white paper on neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.08295</span>, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(76)</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji Lin, and Song Han.

</span>
<span class="ltx_bibblock">Offsite-tuning: Transfer learning without full model.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.04870</span>, 2023.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(77)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
Mazzocchi, Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proceedings of machine learning and systems</span>, 1:374–388, 2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(78)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang
Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.13518</span>, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(79)</span>
<span class="ltx_bibblock">
Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan
Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas,
et al.

</span>
<span class="ltx_bibblock">Papaya: Practical, private, and scalable federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 4:814–832, 2022.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(80)</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier
Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet,
Pedro Porto Buarque de Gusmão, et al.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.14390</span>, 2020.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(81)</span>
<span class="ltx_bibblock">
Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Fate: An industrial grade platform for collaborative learning with
data protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 22(1):10320–10325,
2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(82)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan, Yonggang Wen, and Shuai Zhang.

</span>
<span class="ltx_bibblock">Easyfl: A low-code federated learning platform for dummies.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 2022.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(83)</span>
<span class="ltx_bibblock">
Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Federatedscope: A flexible federated learning platform for
heterogeneity.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">Proceedings of the VLDB Endowment</span>, 16(5):1059–1072, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(84)</span>
<span class="ltx_bibblock">
FedML TEAM.

</span>
<span class="ltx_bibblock">Releasing fedllm: Build your own large language models on proprietary
data using the fedml platform.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(85)</span>
<span class="ltx_bibblock">
FATE Team.

</span>
<span class="ltx_bibblock">Fate-llm.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(86)</span>
<span class="ltx_bibblock">
Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger.

</span>
<span class="ltx_bibblock">Mecta: Memory-economic continual test-time model adaptation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning
Representations</span>, 2023.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(87)</span>
<span class="ltx_bibblock">
Debaditya Shome and Tejaswini Kar.

</span>
<span class="ltx_bibblock">Fedaffect: Few-shot federated learning for facial expression
recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 4168–4175, 2021.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(88)</span>
<span class="ltx_bibblock">
Dashan Gao, Xin Yao, and Qiang Yang.

</span>
<span class="ltx_bibblock">A survey on heterogeneous federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.04505</span>, 2022.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(89)</span>
<span class="ltx_bibblock">
Chen Chen, Lingjuan Lyu, Han Yu, and Gang Chen.

</span>
<span class="ltx_bibblock">Practical attribute reconstruction attack against federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Big Data</span>, 2022.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(90)</span>
<span class="ltx_bibblock">
Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua
Shen, and Chao Wu.

</span>
<span class="ltx_bibblock">Dense: Data-free one-shot federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:21414–21428, 2022.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(91)</span>
<span class="ltx_bibblock">
Artur Back de Luca, Guojun Zhang, Xi Chen, and Yaoliang Yu.

</span>
<span class="ltx_bibblock">Mitigating data heterogeneity in federated learning with data
augmentation.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2206.09979</span>, 2022.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(92)</span>
<span class="ltx_bibblock">
Tian Dong, Bo Zhao, and Lingjuan Lyu.

</span>
<span class="ltx_bibblock">Privacy for free: How does dataset condensation help privacy?

</span>
<span class="ltx_bibblock">In <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5378–5396. PMLR, 2022.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(93)</span>
<span class="ltx_bibblock">
Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Differentially private diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.09929</span>, 2022.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(94)</span>
<span class="ltx_bibblock">
Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, and Jing Jiang.

</span>
<span class="ltx_bibblock">Federated learning from pre-trained models: A contrastive learning
approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.10083</span>, 2022.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(95)</span>
<span class="ltx_bibblock">
Daliang Li and Junpu Wang.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model distillation.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.03581</span>, 2019.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(96)</span>
<span class="ltx_bibblock">
Xiaoyu Yang, Qiujia Li, Chao Zhang, and Philip C Woodland.

</span>
<span class="ltx_bibblock">Knowledge distillation from multiple foundation models for end-to-end
speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.10917</span>, 2023.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(97)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, and Shuai Zhang.

</span>
<span class="ltx_bibblock">Mas: Towards resource-efficient federated multiple-task learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 23414–23424, 2023.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(98)</span>
<span class="ltx_bibblock">
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti
Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al.

</span>
<span class="ltx_bibblock">Image as a foreign language: Beit pretraining for all vision and
vision-language tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.10442</span>, 2022.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(99)</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael
Zeng, Ce Liu, and Lu Yuan.

</span>
<span class="ltx_bibblock">Florence-2: Advancing a unified representation for a variety of
vision tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.06242</span>, 2023.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(100)</span>
<span class="ltx_bibblock">
Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Next-chat: An lmm for chat, detection and segmentation.

</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.04498</span>, 2023.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(101)</span>
<span class="ltx_bibblock">
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom
Goldstein.

</span>
<span class="ltx_bibblock">Diffusion art or digital forgery? investigating data replication in
diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.03860</span>, 2022.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(102)</span>
<span class="ltx_bibblock">
Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom
Goldstein.

</span>
<span class="ltx_bibblock">Understanding and mitigating copying in diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.20086</span>, 2023.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(103)</span>
<span class="ltx_bibblock">
Matthew Butterick.

</span>
<span class="ltx_bibblock">Stable diffusion litigation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://stablediffusionlitigation.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://stablediffusionlitigation.com</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-30.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(104)</span>
<span class="ltx_bibblock">
Zhenting Wang, Chen Chen, Yi Zeng, Lingjuan Lyu, and Shiqing Ma.

</span>
<span class="ltx_bibblock">Alteration-free and model-agnostic origin attribution of generated
images.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.18439</span>, 2023.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(105)</span>
<span class="ltx_bibblock">
Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu,
Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie.

</span>
<span class="ltx_bibblock">Are you copying my model? protecting the copyright of large language
models for eaas via backdoor watermark.

</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10036</span>, 2023.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(106)</span>
<span class="ltx_bibblock">
Qiongkai Xu, Xuanli He, Lingjuan Lyu, Lizhen Qu, and Gholamreza Haffari.

</span>
<span class="ltx_bibblock">Student surpasses teacher: Imitation attack for black-box nlp apis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th International Conference on
Computational Linguistics</span>, pages 2849–2860, 2022.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(107)</span>
<span class="ltx_bibblock">
Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi
Jia.

</span>
<span class="ltx_bibblock">Cater: Intellectual property protection on text generation apis via
conditional watermarks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2023.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(108)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation
image-text models.

</span>
<span class="ltx_bibblock"><span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.08402</span>, 2022.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(109)</span>
<span class="ltx_bibblock">
Yi Zeng, Minzhou Pan, Himanshu Jahagirdar, Ming Jin, Lingjuan Lyu, and Ruoxi
Jia.

</span>
<span class="ltx_bibblock">How to sift out a clean data subset in the presence of data
poisoning?

</span>
<span class="ltx_bibblock"><span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.06516</span>, 2022.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(110)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4615–4625. PMLR, 2019.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(111)</span>
<span class="ltx_bibblock">
Jingtao Li, Lingjuan Lyu, Daisuke Iso, Chaitali Chakrabarti, and Michael
Spranger.

</span>
<span class="ltx_bibblock">Mocosfl: enabling cross-client collaborative self-supervised
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Workshop on Federated Learning: Recent Advances and New
Challenges (in Conjunction with NeurIPS 2022)</span>.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(112)</span>
<span class="ltx_bibblock">
Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger.

</span>
<span class="ltx_bibblock">Outsourcing training without uploading data via efficient
collaborative open-source sampling.

</span>
<span class="ltx_bibblock"><span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.12575</span>, 2022.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(113)</span>
<span class="ltx_bibblock">
Jiabei Liu, Weiming Zhuang, Yonggang Wen, Jun Huang, and Wei Lin.

</span>
<span class="ltx_bibblock">Optimizing federated unsupervised person re-identification via
camera-aware clustering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">2022 IEEE 24th International Workshop on Multimedia Signal
Processing (MMSP)</span>, pages 1–6. IEEE, 2022.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(114)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">Proceedings of the national academy of sciences</span>,
114(13):3521–3526, 2017.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(115)</span>
<span class="ltx_bibblock">
Tao Bai, Chen Chen, Lingjuan Lyu, Jun Zhao, and Bihan Wen.

</span>
<span class="ltx_bibblock">Towards adversarially robust continual learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 1–5. IEEE, 2023.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(116)</span>
<span class="ltx_bibblock">
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Domain adaptation for large-scale sentiment classification: A deep
learning approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th international conference on machine
learning (ICML-11)</span>, pages 513–520, 2011.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(117)</span>
<span class="ltx_bibblock">
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan.

</span>
<span class="ltx_bibblock">Learning transferable features with deep adaptation networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages 97–105.
PMLR, 2015.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(118)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang, and Shuai Yi.

</span>
<span class="ltx_bibblock">Towards unsupervised domain adaptation for deep face recognition
under privacy constraints via federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.07606</span>, 2021.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(119)</span>
<span class="ltx_bibblock">
Yue Tan, Chen Chen, Weiming Zhuang, Xin Dong, Lingjuan Lyu, and Guodong Long.

</span>
<span class="ltx_bibblock">Is heterogeneity notorious? taming heterogeneity to handle test-time
shift in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</span>, 2023.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(120)</span>
<span class="ltx_bibblock">
Ziqing Fan, Jiangchao Yao, Ruipeng Zhang, Lingjuan Lyu, Yanfeng Wang, and
Ya Zhang.

</span>
<span class="ltx_bibblock">Federated learning under partially disjoint data via manifold
reshaping.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Transactions on Machine Learning Research</span>, 2023.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(121)</span>
<span class="ltx_bibblock">
Ziqing Fan, Yanfeng Wang, Jiangchao Yao, Lingjuan Lyu, Ya Zhang, and Qi Tian.

</span>
<span class="ltx_bibblock">Fedskip: Combatting statistical heterogeneity with federated skip
aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">International Conference on Data Mining</span>, pages 131–140.
IEEE, 2022.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(122)</span>
<span class="ltx_bibblock">
Ruixuan Liu, Fangzhao Wu, Chuhan Wu, Yanlin Wang, Lingjuan Lyu, Hong Chen, and
Xing Xie.

</span>
<span class="ltx_bibblock">No one left behind: Inclusive federated learning over heterogeneous
devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining</span>, pages 3398–3406, 2022.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(123)</span>
<span class="ltx_bibblock">
Jiaqi Wang, Xingyi Yang, Suhan Cui, Liwei Che, Lingjuan Lyu, Dongkuan Xu, and
Fenglong Ma.

</span>
<span class="ltx_bibblock">Towards personalized federated learning via heterogeneous model
reassembly.

</span>
<span class="ltx_bibblock">In <span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</span>, 2023.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(124)</span>
<span class="ltx_bibblock">
Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang.

</span>
<span class="ltx_bibblock">Protecting intellectual property of language generation apis with
lexical watermark.

</span>
<span class="ltx_bibblock">In <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 36, pages 10758–10766, 2022.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(125)</span>
<span class="ltx_bibblock">
Zhenting Wang, Chen Chen, Yi Zeng, Lingjuan Lyu, and Shiqing Ma.

</span>
<span class="ltx_bibblock">Where did i come from? origin attribution of ai-generated images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</span>, 2023.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(126)</span>
<span class="ltx_bibblock">
Zhenting Wang, Chen Chen, Yuchen Liu, Lingjuan Lyu, Dimitris Metaxas, and
Shiqing Ma.

</span>
<span class="ltx_bibblock">How to detect unauthorized data usages in text-to-image diffusion
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.03108</span>, 2023.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(127)</span>
<span class="ltx_bibblock">
Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, and
Liang Zheng.

</span>
<span class="ltx_bibblock">Privacy assessment on reconstructed images: Are existing evaluation
metrics faithful to human perception?

</span>
<span class="ltx_bibblock">In <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</span>, 2023.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(128)</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM SIGSAC conference on computer and
communications security</span>, pages 308–318, 2016.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(129)</span>
<span class="ltx_bibblock">
Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim,
and Laurens van der Maaten.

</span>
<span class="ltx_bibblock">Crypten: Secure multi-party computation meets machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
34:4961–4973, 2021.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(130)</span>
<span class="ltx_bibblock">
Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho.

</span>
<span class="ltx_bibblock">How to backdoor diffusion models?

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.05400</span>, 2022.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(131)</span>
<span class="ltx_bibblock">
Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, and
Tianwei Zhang.

</span>
<span class="ltx_bibblock">Defending against backdoor attacks in natural language generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">AAAI</span>, 2023.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(132)</span>
<span class="ltx_bibblock">
Chen Chen, Yuchen Liu, Xingjun Ma, and Lingjuan Lyu.

</span>
<span class="ltx_bibblock">Calfat: Calibrated federated adversarial training with label
skewness.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2022.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(133)</span>
<span class="ltx_bibblock">
Yuchen Liu, Chen Chen, Lingjuan Lyu, Fangzhao Wu, Sai Wu, and Gang Chen.

</span>
<span class="ltx_bibblock">Byzantine-robust learning on heterogeneous data via gradient
splitting.

</span>
<span class="ltx_bibblock">In <span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2023.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(134)</span>
<span class="ltx_bibblock">
Jie Zhang, Bo Li, Chen Chen, Lingjuan Lyu, Shuang Wu, Shouhong Ding, and Chao
Wu.

</span>
<span class="ltx_bibblock">Delving into the adversarial robustness of federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.09479</span>, 2023.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(135)</span>
<span class="ltx_bibblock">
Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi
Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al.

</span>
<span class="ltx_bibblock">On the robustness of chatgpt: An adversarial and out-of-distribution
perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.12095</span>, 2023.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(136)</span>
<span class="ltx_bibblock">
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.

</span>
<span class="ltx_bibblock">Towards personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2022.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(137)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1806.00582, 2018.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(138)</span>
<span class="ltx_bibblock">
Jie Zhang, Chen Chen, and Lingjuan Lyu.

</span>
<span class="ltx_bibblock">Ideal: Query-efficient data-free learning from black-box models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning
Representations</span>, 2023.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(139)</span>
<span class="ltx_bibblock">
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.

</span>
<span class="ltx_bibblock">Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface.

</span>
<span class="ltx_bibblock"><span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.17580</span>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.15545" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.15546" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.15546">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.15546" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.15547" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 21:22:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
