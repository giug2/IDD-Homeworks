<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ</title>
<!--Generated on Wed Sep 25 09:38:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.16779v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S1" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S2" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS1" title="In 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Base Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS2" title="In 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS2.SSS1" title="In 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Supervised Fine Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS2.SSS2" title="In 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Preference Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS2.SSS3" title="In 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Reward Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS3" title="In 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Retrieval Augmented Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.SS4" title="In 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1" title="In 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS1" title="In 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>SFT Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS2" title="In 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>DPO Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS3" title="In 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>MCQ Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS4" title="In 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>DPR Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS2" title="In 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS3" title="In 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS4" title="In 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS4.SSS1" title="In 4.4 Setup â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>1st SFT â€“ Mathematical Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS4.SSS2" title="In 4.4 Setup â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>DPO Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS4.SSS3" title="In 4.4 Setup â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>2nd SFT â€“ MCQA Reasoning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS5" title="In 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S5" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S6" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ethical considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S7" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A1" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Contribution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Datasets Samples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS1" title="In Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>SFT Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS2" title="In Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>DPO Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS3" title="In Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>MCQ Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS4" title="In Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>DPR Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS5" title="In Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.5 </span>Intruction for DPO Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.SS1" title="In Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Training Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.SS2" title="In Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Training Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.SS2.SSS1" title="In C.2 Training Metrics â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>Maths-SFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.SS2.SSS2" title="In C.2 Training Metrics â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.2 </span>MCQ-SFT</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A4" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Model Samples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A4.SS1" title="In Appendix D Model Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Prompt Format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A4.SS2" title="In Appendix D Model Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A5" title="In LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Generation Tuning</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_rule" style="width:433.6pt;height:1.1pt;background:black;display:inline-block;">Â </span>
<br class="ltx_break"/>
LLaMa-SciQ: 
<br class="ltx_break"/>An Educational Chatbot for Answering Science MCQ
<br class="ltx_break"/><span class="ltx_rule" style="width:433.6pt;height:1.1pt;background:black;display:inline-block;">Â </span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id1.1.id1">Marc-Antoine Allard</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">EPFL, Lausanne, The Wordsmiths
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id2.1.id1">Matin Ansaripour</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">EPFL, Lausanne, The Wordsmiths
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id3.1.id1">Maria Yuffa</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">EPFL, Lausanne, The Wordsmiths
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id4.1.id1">Paul Teiletche</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">EPFL, Lausanne, The Wordsmiths
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Large Language Models (LLMs) often struggle with tasks requiring mathematical reasoning, particularly multiple-choice questions (MCQs). To address this issue, we developed LLaMa-SciQ, an educational chatbot designed to assist college students in solving and understanding MCQs in STEM fields. We begin by fine-tuning and aligning the models to human preferences. After comparing the performance of Mistral-7B and LLaMa-8B, we selected the latter as the base model due to its higher evaluation accuracy. To further enhance accuracy, we implement Retrieval-Augmented Generation (RAG) and apply quantization to compress the model, reducing inference time and increasing accessibility for students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve performance and even reduces it, likely due to retriever issues or the modelâ€™s unfamiliarity with context. Despite this, the quantized model shows only a 5% loss in performance, demonstrating significant efficiency improvements.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) are known to perform poorly on questions requiring advanced mathematical reasoning <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib31" title="">2023</a>)</cite>. This is especially true for the university level problems <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib27" title="">2024</a>)</cite>. In literature, the failure of current approaches is attributed to inability of LLMs to recognize and correct a wrong answer <cite class="ltx_cite ltx_citemacro_cite">Imani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib15" title="">2023</a>)</cite> as well as catastrophic forgetting of linguistic skills when trained on maths data <cite class="ltx_cite ltx_citemacro_cite">Sharma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib23" title="">2023</a>)</cite>. The issues cannot be fully addressed with a simple prompting strategy due to data variability <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib27" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This project explores state-of-the-art LLMs for creation of an accessible chatbot that assists students in mathematics, physics and computer science. Specifically, we fine-tune LLaMa-3-8B model as well as Mistral-7B on a variety of mathematical and scientific datasets further using Direct Preference Optimization (DPO) to align modelâ€™s responses to the ones preferred by a student. We compare the performances of the models and demonstrate the significantly superior performance of the LLaMa model with which we proceed.
We try to enhance the accuracy of fine-tuned LLaMa-3-8B model by applying Retrieval Augmented Generation (RAG). Finally, we quantize the LLM for more efficient inference, making it suitable for students needs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">With recent release of ChatGPT-3.5 and ChatGPT-4, the number of people using LLMs for education has sky-rocketed <cite class="ltx_cite ltx_citemacro_cite">FÃ¼tterer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib11" title="">2023</a>)</cite>. To leverage its capabilities while making user-friendly interfaces vast amount of research is dedicated to creation of LLM based Chatbots for academic purposes <cite class="ltx_cite ltx_citemacro_cite">Odede and Frommholz (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib19" title="">2024</a>)</cite>. Despite success of ChatGPT models on lingustic tasks, their performance was limited on problems involving mathematical reasoning. This is especially true for MCQ questions where the answer is not verbal. This is showcased by the work of <cite class="ltx_cite ltx_citemacro_cite">Savelka etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib22" title="">2023</a>)</cite>, where GPT model struggled to give the correct answer to the questions that do not contain natural language.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">To improve the performance of the pre-trained LLM model on mathematical questions while ensuring the alignment of the responses with the intended purposes and human values, we considered both Supervised Fine-tuning on mathematical and scientific datasets as well as DPO on the preference pairs ranked by students. This approach was inspired by InstructGPT <cite class="ltx_cite ltx_citemacro_cite">Ouyang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib20" title="">2022</a>)</cite> in aligning LLMs with human preferences. We also considered DPO with an offset <cite class="ltx_cite ltx_citemacro_cite">Amini etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib4" title="">2024</a>)</cite>. This approach introdues variability in treatment of preference pairs and could be less robust, since the inferred offset value might be high in a mis-annotated responses and confuse the model. Therefore, to achieve good results on noisy data while keeping the implementation simple, we chose Conservative DPO (cDPO) loss for DPO fine-tuning strategy, primarily due to its robustness on noisy data.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Further refining the model, we have considered RAG. Initially, we aimed to use pre-trained RAG retriever <cite class="ltx_cite ltx_citemacro_cite">Karpukhin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib17" title="">2020</a>)</cite> or adopting the novel concept of Retrieval Augmented Fine-Tuning (RAFT) <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib36" title="">2024</a>)</cite>. Thoroughly reviewing the literature <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib12" title="">2024</a>)</cite> we yielded to the <span class="ltx_text ltx_font_italic" id="S2.p3.1.1">Naive RAG</span> strategy due to good performance and straightforwardness of the approach.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Finally, we considered quantizing the model to reduce the computational costs when using the Chatbot while maintaining good response accuracy. At first, we sought to use QuIP <cite class="ltx_cite ltx_citemacro_cite">Chee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib5" title="">2024</a>)</cite> and recently released QuIP# <cite class="ltx_cite ltx_citemacro_cite">Tseng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib24" title="">2024</a>)</cite> due to its ability to leverage incoherence in weights and Hessian matrices. We have also considered QLoRA <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib14" title="">2021</a>)</cite>, which is not computationally demanding, yet preserves the 16-bit fine-tuning task performance of LLMs. Before attempting the advanced methods, we have tried GPTQ <cite class="ltx_cite ltx_citemacro_cite">Frantar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib10" title="">2023</a>)</cite>. Nonetheless, facing some numerical issues with quantizing our model with GPTQ, we decided to use the 4-bit quantization provided by Unsloth bitsandbytes library.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Overall, our work is a nice step towards creating an efficient, student-oriented educational assistant for questions requiring mathematical reasoning.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our approach consisted of performing SFT training on both Mistral-7B and LLaMa-3-8B. We then compared the performance of two models with SFT and DPO training and proceeded with LLaMa-3-8B which performed better on the evaluation set (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.T1" title="Table 1 â€£ 3.2.3 Reward Model â€£ 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">1</span></a>). In this section, we outline the details of the models and their fine-tuning strategy with emphasis on LLaMa-3-8B.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Base Model Architecture</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">LLaMa-3-8B</span>â€ƒ <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib2" title="">2024</a>)</cite> is an auto-regressive language model featuring an enhanced transformer architecture with a standard decoder-only design. The model integrates supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to better align with human preferences regarding safety and helpfulness. Llama 3, which uses a tokenizer with a 128K-token vocabulary for more efficient language encoding, shows significant performance improvements over its predecessor. The model, trained on sequences up to 8,192 tokens with boundary-aware self-attention, uses Grouped-Query Attention (GQA) to enhance inference scalability.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Mistral-7B</span>â€ƒ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib16" title="">2023</a>)</cite>, a language model with 7 billion parameters, utilizes a transformer-based architecture comprising multiple transformer blocks. It employs sliding window attention that allows the model to attend to tokens outside of the window, Rolling Buffer Cache to reduce the cache memory usage while keeping the model quality. It also utilizes pre-fill and chunking, which involve loading known parts of a prompt into the (k, v) cache to facilitate token generation. If the prompt is lengthy, it is segmented into smaller chunks, each pre-filled into the cache to enhance processing efficiency during token prediction.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Pipeline</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The training pipeline for LLaMa-3-8B model is demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.F1" title="Figure 1 â€£ 3.2.3 Reward Model â€£ 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">1</span></a>. We first performed Supervised Fine-tuning on a mix of specialized maths and science datasets. We then performed DPO training using preference data generated and annotated by students via cDPO loss. Finally, we gauged the performance of the model on <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">AQuA-Rat</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ling etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib18" title="">2017</a>)</cite> dataset which contains STEM-related MCQ questions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Mistral-7B used the same process as LLaMa, except for the final SFT, since LLaMa showed superior performance (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.T1" title="Table 1 â€£ 3.2.3 Reward Model â€£ 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">1</span></a>). Ultimately, we have not implemented both due to time constraints.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Supervised Fine Tuning</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The results of supervised fine-tuning of two models are demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.T1" title="Table 1 â€£ 3.2.3 Reward Model â€£ 3.2 Training Pipeline â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Preference Data Collection</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To collect preference data, a cohort of 300 students was asked to generate two responses, a better one and a slightly worse one but preferably still correct, to the question using GPT-wrapper. The students were further asked to rank the responses.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">To generate answers for the dataset of questions in mathematics, physics, and computer science, we have developed a prompting strategy that incorporates several techniques. Firstly, we create a separate chat for each subject id. Secondly, we use Chain-of-Thought (CoT)Â <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib30" title="">2022</a>)</cite>, which guides the model to reach conclusions in a step-by-step manner. Thirdly, the model is prompted with the instruction provided in <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2.SS5" title="B.5 Intruction for DPO Generation â€£ Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">B.5</span></a>. Finally, for generating preference pairs, we employ the following method: to achieve a better response, we prompt the model to re-read the question before attempting to solve it. This methods was shown by <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib32" title="">2024</a>)</cite> to consistently improve performance for LLMs, except for Vanilla ChatGPT. For the worst answer, the model is instructed to provide a very brief explanation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Reward Model</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.2">The reward model is a critical component of the DPO fine-tuning strategy. The reward model is based on the policy that maximizes the reward with KL constraint to the reference policy:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\pi^{*}=\arg\max_{\pi}\mathbb{E}_{y\sim\pi}\left[r(y)-\beta\log\frac{\pi(y)}{%
\pi_{\text{ref}}(y)}\right]" class="ltx_Math" display="block" id="S3.Ex1.m1.4"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><msup id="S3.Ex1.m1.4.4.3" xref="S3.Ex1.m1.4.4.3.cmml"><mi id="S3.Ex1.m1.4.4.3.2" xref="S3.Ex1.m1.4.4.3.2.cmml">Ï€</mi><mo id="S3.Ex1.m1.4.4.3.3" xref="S3.Ex1.m1.4.4.3.3.cmml">âˆ—</mo></msup><mo id="S3.Ex1.m1.4.4.2" xref="S3.Ex1.m1.4.4.2.cmml">=</mo><mrow id="S3.Ex1.m1.4.4.1" xref="S3.Ex1.m1.4.4.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.3" xref="S3.Ex1.m1.4.4.1.3.cmml"><mi id="S3.Ex1.m1.4.4.1.3.1" xref="S3.Ex1.m1.4.4.1.3.1.cmml">arg</mi><mo id="S3.Ex1.m1.4.4.1.3a" lspace="0.167em" xref="S3.Ex1.m1.4.4.1.3.cmml">â¡</mo><mrow id="S3.Ex1.m1.4.4.1.3.2" xref="S3.Ex1.m1.4.4.1.3.2.cmml"><munder id="S3.Ex1.m1.4.4.1.3.2.1" xref="S3.Ex1.m1.4.4.1.3.2.1.cmml"><mi id="S3.Ex1.m1.4.4.1.3.2.1.2" xref="S3.Ex1.m1.4.4.1.3.2.1.2.cmml">max</mi><mi id="S3.Ex1.m1.4.4.1.3.2.1.3" xref="S3.Ex1.m1.4.4.1.3.2.1.3.cmml">Ï€</mi></munder><mo id="S3.Ex1.m1.4.4.1.3.2a" lspace="0.167em" xref="S3.Ex1.m1.4.4.1.3.2.cmml">â¡</mo><msub id="S3.Ex1.m1.4.4.1.3.2.2" xref="S3.Ex1.m1.4.4.1.3.2.2.cmml"><mi id="S3.Ex1.m1.4.4.1.3.2.2.2" xref="S3.Ex1.m1.4.4.1.3.2.2.2.cmml">ğ”¼</mi><mrow id="S3.Ex1.m1.4.4.1.3.2.2.3" xref="S3.Ex1.m1.4.4.1.3.2.2.3.cmml"><mi id="S3.Ex1.m1.4.4.1.3.2.2.3.2" xref="S3.Ex1.m1.4.4.1.3.2.2.3.2.cmml">y</mi><mo id="S3.Ex1.m1.4.4.1.3.2.2.3.1" xref="S3.Ex1.m1.4.4.1.3.2.2.3.1.cmml">âˆ¼</mo><mi id="S3.Ex1.m1.4.4.1.3.2.2.3.3" xref="S3.Ex1.m1.4.4.1.3.2.2.3.3.cmml">Ï€</mi></mrow></msub></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.2" xref="S3.Ex1.m1.4.4.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.4.4.1.1.1" xref="S3.Ex1.m1.4.4.1.1.2.cmml"><mo id="S3.Ex1.m1.4.4.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.2.2" xref="S3.Ex1.m1.4.4.1.1.1.1.2.2.cmml">r</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.2.1" xref="S3.Ex1.m1.4.4.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.2.3.2" xref="S3.Ex1.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.Ex1.m1.4.4.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.Ex1.m1.4.4.1.1.1.1.2.cmml">(</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">y</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.2.3.2.2" stretchy="false" xref="S3.Ex1.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.3" xref="S3.Ex1.m1.4.4.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.3.2" xref="S3.Ex1.m1.4.4.1.1.1.1.3.2.cmml">Î²</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.3.1" lspace="0.167em" xref="S3.Ex1.m1.4.4.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.3.3" xref="S3.Ex1.m1.4.4.1.1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.3.3.1" xref="S3.Ex1.m1.4.4.1.1.1.1.3.3.1.cmml">log</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.3.3a" lspace="0.167em" xref="S3.Ex1.m1.4.4.1.1.1.1.3.3.cmml">â¡</mo><mfrac id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.3.cmml">Ï€</mi><mo id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.1.1.1.4.2" xref="S3.Ex1.m1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.4.2.1" stretchy="false" xref="S3.Ex1.m1.1.1.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">y</mi><mo id="S3.Ex1.m1.1.1.1.4.2.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2.cmml"><msub id="S3.Ex1.m1.2.2.2.3" xref="S3.Ex1.m1.2.2.2.3.cmml"><mi id="S3.Ex1.m1.2.2.2.3.2" xref="S3.Ex1.m1.2.2.2.3.2.cmml">Ï€</mi><mtext id="S3.Ex1.m1.2.2.2.3.3" xref="S3.Ex1.m1.2.2.2.3.3a.cmml">ref</mtext></msub><mo id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.2.2.2.4.2" xref="S3.Ex1.m1.2.2.2.cmml"><mo id="S3.Ex1.m1.2.2.2.4.2.1" stretchy="false" xref="S3.Ex1.m1.2.2.2.cmml">(</mo><mi id="S3.Ex1.m1.2.2.2.1" xref="S3.Ex1.m1.2.2.2.1.cmml">y</mi><mo id="S3.Ex1.m1.2.2.2.4.2.2" stretchy="false" xref="S3.Ex1.m1.2.2.2.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.1.3" xref="S3.Ex1.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><eq id="S3.Ex1.m1.4.4.2.cmml" xref="S3.Ex1.m1.4.4.2"></eq><apply id="S3.Ex1.m1.4.4.3.cmml" xref="S3.Ex1.m1.4.4.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.3.1.cmml" xref="S3.Ex1.m1.4.4.3">superscript</csymbol><ci id="S3.Ex1.m1.4.4.3.2.cmml" xref="S3.Ex1.m1.4.4.3.2">ğœ‹</ci><times id="S3.Ex1.m1.4.4.3.3.cmml" xref="S3.Ex1.m1.4.4.3.3"></times></apply><apply id="S3.Ex1.m1.4.4.1.cmml" xref="S3.Ex1.m1.4.4.1"><times id="S3.Ex1.m1.4.4.1.2.cmml" xref="S3.Ex1.m1.4.4.1.2"></times><apply id="S3.Ex1.m1.4.4.1.3.cmml" xref="S3.Ex1.m1.4.4.1.3"><arg id="S3.Ex1.m1.4.4.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.3.1"></arg><apply id="S3.Ex1.m1.4.4.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.3.2"><apply id="S3.Ex1.m1.4.4.1.3.2.1.cmml" xref="S3.Ex1.m1.4.4.1.3.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.3.2.1.1.cmml" xref="S3.Ex1.m1.4.4.1.3.2.1">subscript</csymbol><max id="S3.Ex1.m1.4.4.1.3.2.1.2.cmml" xref="S3.Ex1.m1.4.4.1.3.2.1.2"></max><ci id="S3.Ex1.m1.4.4.1.3.2.1.3.cmml" xref="S3.Ex1.m1.4.4.1.3.2.1.3">ğœ‹</ci></apply><apply id="S3.Ex1.m1.4.4.1.3.2.2.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.3.2.2.1.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2">subscript</csymbol><ci id="S3.Ex1.m1.4.4.1.3.2.2.2.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2.2">ğ”¼</ci><apply id="S3.Ex1.m1.4.4.1.3.2.2.3.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2.3"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.1.3.2.2.3.1.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2.3.1">similar-to</csymbol><ci id="S3.Ex1.m1.4.4.1.3.2.2.3.2.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2.3.2">ğ‘¦</ci><ci id="S3.Ex1.m1.4.4.1.3.2.2.3.3.cmml" xref="S3.Ex1.m1.4.4.1.3.2.2.3.3">ğœ‹</ci></apply></apply></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.1.1.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S3.Ex1.m1.4.4.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1"><minus id="S3.Ex1.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1"></minus><apply id="S3.Ex1.m1.4.4.1.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.2"><times id="S3.Ex1.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.2.1"></times><ci id="S3.Ex1.m1.4.4.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">ğ‘¦</ci></apply><apply id="S3.Ex1.m1.4.4.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.3"><times id="S3.Ex1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.3.1"></times><ci id="S3.Ex1.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.3.2">ğ›½</ci><apply id="S3.Ex1.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.3.3"><log id="S3.Ex1.m1.4.4.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.3.3.1"></log><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><divide id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2"></divide><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.2"></times><ci id="S3.Ex1.m1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.3">ğœ‹</ci><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">ğ‘¦</ci></apply><apply id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2"><times id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2"></times><apply id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.3.1.cmml" xref="S3.Ex1.m1.2.2.2.3">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.3.2.cmml" xref="S3.Ex1.m1.2.2.2.3.2">ğœ‹</ci><ci id="S3.Ex1.m1.2.2.2.3.3a.cmml" xref="S3.Ex1.m1.2.2.2.3.3"><mtext id="S3.Ex1.m1.2.2.2.3.3.cmml" mathsize="70%" xref="S3.Ex1.m1.2.2.2.3.3">ref</mtext></ci></apply><ci id="S3.Ex1.m1.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.1">ğ‘¦</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\pi^{*}=\arg\max_{\pi}\mathbb{E}_{y\sim\pi}\left[r(y)-\beta\log\frac{\pi(y)}{%
\pi_{\text{ref}}(y)}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.4d">italic_Ï€ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_Ï€ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_y âˆ¼ italic_Ï€ end_POSTSUBSCRIPT [ italic_r ( italic_y ) - italic_Î² roman_log divide start_ARG italic_Ï€ ( italic_y ) end_ARG start_ARG italic_Ï€ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ( italic_y ) end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS3.p1.3">Considering a small probability that the preference pair could be flipped, the preferred response is in reality less correct or explicit than the other one, we can derive the following DPO loss to optimise the reward model:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}^{\epsilon}_{DPO}(\theta,y_{w},y_{l})" class="ltx_Math" display="inline" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><msubsup id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.4.2.2" xref="S3.E1.m1.3.3.4.2.2.cmml">â„’</mi><mrow id="S3.E1.m1.3.3.4.3" xref="S3.E1.m1.3.3.4.3.cmml"><mi id="S3.E1.m1.3.3.4.3.2" xref="S3.E1.m1.3.3.4.3.2.cmml">D</mi><mo id="S3.E1.m1.3.3.4.3.1" xref="S3.E1.m1.3.3.4.3.1.cmml">â¢</mo><mi id="S3.E1.m1.3.3.4.3.3" xref="S3.E1.m1.3.3.4.3.3.cmml">P</mi><mo id="S3.E1.m1.3.3.4.3.1a" xref="S3.E1.m1.3.3.4.3.1.cmml">â¢</mo><mi id="S3.E1.m1.3.3.4.3.4" xref="S3.E1.m1.3.3.4.3.4.cmml">O</mi></mrow><mi id="S3.E1.m1.3.3.4.2.3" xref="S3.E1.m1.3.3.4.2.3.cmml">Ïµ</mi></msubsup><mo id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.3.cmml"><mo id="S3.E1.m1.3.3.2.2.3" stretchy="false" xref="S3.E1.m1.3.3.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Î¸</mi><mo id="S3.E1.m1.3.3.2.2.4" xref="S3.E1.m1.3.3.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">y</mi><mi id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">w</mi></msub><mo id="S3.E1.m1.3.3.2.2.5" xref="S3.E1.m1.3.3.2.3.cmml">,</mo><msub id="S3.E1.m1.3.3.2.2.2" xref="S3.E1.m1.3.3.2.2.2.cmml"><mi id="S3.E1.m1.3.3.2.2.2.2" xref="S3.E1.m1.3.3.2.2.2.2.cmml">y</mi><mi id="S3.E1.m1.3.3.2.2.2.3" xref="S3.E1.m1.3.3.2.2.2.3.cmml">l</mi></msub><mo id="S3.E1.m1.3.3.2.2.6" stretchy="false" xref="S3.E1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><times id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"></times><apply id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.4.1.cmml" xref="S3.E1.m1.3.3.4">subscript</csymbol><apply id="S3.E1.m1.3.3.4.2.cmml" xref="S3.E1.m1.3.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.4.2.1.cmml" xref="S3.E1.m1.3.3.4">superscript</csymbol><ci id="S3.E1.m1.3.3.4.2.2.cmml" xref="S3.E1.m1.3.3.4.2.2">â„’</ci><ci id="S3.E1.m1.3.3.4.2.3.cmml" xref="S3.E1.m1.3.3.4.2.3">italic-Ïµ</ci></apply><apply id="S3.E1.m1.3.3.4.3.cmml" xref="S3.E1.m1.3.3.4.3"><times id="S3.E1.m1.3.3.4.3.1.cmml" xref="S3.E1.m1.3.3.4.3.1"></times><ci id="S3.E1.m1.3.3.4.3.2.cmml" xref="S3.E1.m1.3.3.4.3.2">ğ·</ci><ci id="S3.E1.m1.3.3.4.3.3.cmml" xref="S3.E1.m1.3.3.4.3.3">ğ‘ƒ</ci><ci id="S3.E1.m1.3.3.4.3.4.cmml" xref="S3.E1.m1.3.3.4.3.4">ğ‘‚</ci></apply></apply><vector id="S3.E1.m1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.2.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğœƒ</ci><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2">ğ‘¦</ci><ci id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3">ğ‘¤</ci></apply><apply id="S3.E1.m1.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2.2">ğ‘¦</ci><ci id="S3.E1.m1.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.2.3">ğ‘™</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\mathcal{L}^{\epsilon}_{DPO}(\theta,y_{w},y_{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">caligraphic_L start_POSTSUPERSCRIPT italic_Ïµ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_D italic_P italic_O end_POSTSUBSCRIPT ( italic_Î¸ , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-(1-\epsilon)\log\hat{p}_{\theta}(y_{w}&gt;y_{l})" class="ltx_Math" display="inline" id="S3.E1.m2.2"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2" xref="S3.E1.m2.2.2.cmml"><mi id="S3.E1.m2.2.2.4" xref="S3.E1.m2.2.2.4.cmml"></mi><mo id="S3.E1.m2.2.2.3" xref="S3.E1.m2.2.2.3.cmml">=</mo><mrow id="S3.E1.m2.2.2.2" xref="S3.E1.m2.2.2.2.cmml"><mo id="S3.E1.m2.2.2.2a" xref="S3.E1.m2.2.2.2.cmml">âˆ’</mo><mrow id="S3.E1.m2.2.2.2.2" xref="S3.E1.m2.2.2.2.2.cmml"><mrow id="S3.E1.m2.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m2.1.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m2.1.1.1.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m2.1.1.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E1.m2.1.1.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.1.1.3.cmml">Ïµ</mi></mrow><mo id="S3.E1.m2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m2.2.2.2.2.3" lspace="0.167em" xref="S3.E1.m2.2.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m2.2.2.2.2.4" xref="S3.E1.m2.2.2.2.2.4.cmml"><mi id="S3.E1.m2.2.2.2.2.4.1" xref="S3.E1.m2.2.2.2.2.4.1.cmml">log</mi><mo id="S3.E1.m2.2.2.2.2.4a" lspace="0.167em" xref="S3.E1.m2.2.2.2.2.4.cmml">â¡</mo><msub id="S3.E1.m2.2.2.2.2.4.2" xref="S3.E1.m2.2.2.2.2.4.2.cmml"><mover accent="true" id="S3.E1.m2.2.2.2.2.4.2.2" xref="S3.E1.m2.2.2.2.2.4.2.2.cmml"><mi id="S3.E1.m2.2.2.2.2.4.2.2.2" xref="S3.E1.m2.2.2.2.2.4.2.2.2.cmml">p</mi><mo id="S3.E1.m2.2.2.2.2.4.2.2.1" xref="S3.E1.m2.2.2.2.2.4.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m2.2.2.2.2.4.2.3" xref="S3.E1.m2.2.2.2.2.4.2.3.cmml">Î¸</mi></msub></mrow><mo id="S3.E1.m2.2.2.2.2.3a" xref="S3.E1.m2.2.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m2.2.2.2.2.2.1" xref="S3.E1.m2.2.2.2.2.2.1.1.cmml"><mo id="S3.E1.m2.2.2.2.2.2.1.2" stretchy="false" xref="S3.E1.m2.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m2.2.2.2.2.2.1.1" xref="S3.E1.m2.2.2.2.2.2.1.1.cmml"><msub id="S3.E1.m2.2.2.2.2.2.1.1.2" xref="S3.E1.m2.2.2.2.2.2.1.1.2.cmml"><mi id="S3.E1.m2.2.2.2.2.2.1.1.2.2" xref="S3.E1.m2.2.2.2.2.2.1.1.2.2.cmml">y</mi><mi id="S3.E1.m2.2.2.2.2.2.1.1.2.3" xref="S3.E1.m2.2.2.2.2.2.1.1.2.3.cmml">w</mi></msub><mo id="S3.E1.m2.2.2.2.2.2.1.1.1" xref="S3.E1.m2.2.2.2.2.2.1.1.1.cmml">&gt;</mo><msub id="S3.E1.m2.2.2.2.2.2.1.1.3" xref="S3.E1.m2.2.2.2.2.2.1.1.3.cmml"><mi id="S3.E1.m2.2.2.2.2.2.1.1.3.2" xref="S3.E1.m2.2.2.2.2.2.1.1.3.2.cmml">y</mi><mi id="S3.E1.m2.2.2.2.2.2.1.1.3.3" xref="S3.E1.m2.2.2.2.2.2.1.1.3.3.cmml">l</mi></msub></mrow><mo id="S3.E1.m2.2.2.2.2.2.1.3" stretchy="false" xref="S3.E1.m2.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.cmml" xref="S3.E1.m2.2.2"><eq id="S3.E1.m2.2.2.3.cmml" xref="S3.E1.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E1.m2.2.2.4.cmml" xref="S3.E1.m2.2.2.4">absent</csymbol><apply id="S3.E1.m2.2.2.2.cmml" xref="S3.E1.m2.2.2.2"><minus id="S3.E1.m2.2.2.2.3.cmml" xref="S3.E1.m2.2.2.2"></minus><apply id="S3.E1.m2.2.2.2.2.cmml" xref="S3.E1.m2.2.2.2.2"><times id="S3.E1.m2.2.2.2.2.3.cmml" xref="S3.E1.m2.2.2.2.2.3"></times><apply id="S3.E1.m2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1"><minus id="S3.E1.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.1"></minus><cn id="S3.E1.m2.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E1.m2.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.1.1.1.1.1.1.1.3">italic-Ïµ</ci></apply><apply id="S3.E1.m2.2.2.2.2.4.cmml" xref="S3.E1.m2.2.2.2.2.4"><log id="S3.E1.m2.2.2.2.2.4.1.cmml" xref="S3.E1.m2.2.2.2.2.4.1"></log><apply id="S3.E1.m2.2.2.2.2.4.2.cmml" xref="S3.E1.m2.2.2.2.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.2.2.4.2.1.cmml" xref="S3.E1.m2.2.2.2.2.4.2">subscript</csymbol><apply id="S3.E1.m2.2.2.2.2.4.2.2.cmml" xref="S3.E1.m2.2.2.2.2.4.2.2"><ci id="S3.E1.m2.2.2.2.2.4.2.2.1.cmml" xref="S3.E1.m2.2.2.2.2.4.2.2.1">^</ci><ci id="S3.E1.m2.2.2.2.2.4.2.2.2.cmml" xref="S3.E1.m2.2.2.2.2.4.2.2.2">ğ‘</ci></apply><ci id="S3.E1.m2.2.2.2.2.4.2.3.cmml" xref="S3.E1.m2.2.2.2.2.4.2.3">ğœƒ</ci></apply></apply><apply id="S3.E1.m2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.2.2.2.1"><gt id="S3.E1.m2.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.1"></gt><apply id="S3.E1.m2.2.2.2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.2.2">ğ‘¦</ci><ci id="S3.E1.m2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.2.3">ğ‘¤</ci></apply><apply id="S3.E1.m2.2.2.2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m2.2.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.3.2">ğ‘¦</ci><ci id="S3.E1.m2.2.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m2.2.2.2.2.2.1.1.3.3">ğ‘™</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle=-(1-\epsilon)\log\hat{p}_{\theta}(y_{w}&gt;y_{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m2.2d">= - ( 1 - italic_Ïµ ) roman_log over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT &gt; italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\quad-\epsilon\log(1-\hat{p}_{\theta}(y_{w}&gt;y_{l}))" class="ltx_Math" display="inline" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mo id="S3.E2.m1.2.2a" xref="S3.E2.m1.2.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">Ïµ</mi><mo id="S3.E2.m1.2.2.1.2" lspace="0.167em" xref="S3.E2.m1.2.2.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">log</mi><mo id="S3.E2.m1.2.2.1.1.1a" xref="S3.E2.m1.2.2.1.1.2.cmml">â¡</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.2.cmml">p</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml">Î¸</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">w</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">&gt;</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></msub></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><minus id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2"></minus><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">italic-Ïµ</ci><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1"><log id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></log><apply id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2"></minus><cn id="S3.E2.m1.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.1.1.3">1</cn><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.2">ğ‘</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3">ğœƒ</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"><gt id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"></gt><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ğ‘¦</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">ğ‘¤</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘™</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle\quad-\epsilon\log(1-\hat{p}_{\theta}(y_{w}&gt;y_{l}))</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">- italic_Ïµ roman_log ( 1 - over^ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT &gt; italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\epsilon)\mathcal{L}_{DPO}(\theta,y_{w},y_{l})" class="ltx_Math" display="inline" id="S3.E3.m1.4"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mi id="S3.E3.m1.4.4.5" xref="S3.E3.m1.4.4.5.cmml"></mi><mo id="S3.E3.m1.4.4.4" xref="S3.E3.m1.4.4.4.cmml">=</mo><mrow id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml"><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><mn id="S3.E3.m1.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.3.cmml">Ïµ</mi></mrow><mo id="S3.E3.m1.2.2.1.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.4.4.3.4" xref="S3.E3.m1.4.4.3.4.cmml">â¢</mo><msub id="S3.E3.m1.4.4.3.5" xref="S3.E3.m1.4.4.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.3.5.2" xref="S3.E3.m1.4.4.3.5.2.cmml">â„’</mi><mrow id="S3.E3.m1.4.4.3.5.3" xref="S3.E3.m1.4.4.3.5.3.cmml"><mi id="S3.E3.m1.4.4.3.5.3.2" xref="S3.E3.m1.4.4.3.5.3.2.cmml">D</mi><mo id="S3.E3.m1.4.4.3.5.3.1" xref="S3.E3.m1.4.4.3.5.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.3.5.3.3" xref="S3.E3.m1.4.4.3.5.3.3.cmml">P</mi><mo id="S3.E3.m1.4.4.3.5.3.1a" xref="S3.E3.m1.4.4.3.5.3.1.cmml">â¢</mo><mi id="S3.E3.m1.4.4.3.5.3.4" xref="S3.E3.m1.4.4.3.5.3.4.cmml">O</mi></mrow></msub><mo id="S3.E3.m1.4.4.3.4a" xref="S3.E3.m1.4.4.3.4.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.3.3.2" xref="S3.E3.m1.4.4.3.3.3.cmml"><mo id="S3.E3.m1.4.4.3.3.2.3" stretchy="false" xref="S3.E3.m1.4.4.3.3.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">Î¸</mi><mo id="S3.E3.m1.4.4.3.3.2.4" xref="S3.E3.m1.4.4.3.3.3.cmml">,</mo><msub id="S3.E3.m1.3.3.2.2.1.1" xref="S3.E3.m1.3.3.2.2.1.1.cmml"><mi id="S3.E3.m1.3.3.2.2.1.1.2" xref="S3.E3.m1.3.3.2.2.1.1.2.cmml">y</mi><mi id="S3.E3.m1.3.3.2.2.1.1.3" xref="S3.E3.m1.3.3.2.2.1.1.3.cmml">w</mi></msub><mo id="S3.E3.m1.4.4.3.3.2.5" xref="S3.E3.m1.4.4.3.3.3.cmml">,</mo><msub id="S3.E3.m1.4.4.3.3.2.2" xref="S3.E3.m1.4.4.3.3.2.2.cmml"><mi id="S3.E3.m1.4.4.3.3.2.2.2" xref="S3.E3.m1.4.4.3.3.2.2.2.cmml">y</mi><mi id="S3.E3.m1.4.4.3.3.2.2.3" xref="S3.E3.m1.4.4.3.3.2.2.3.cmml">l</mi></msub><mo id="S3.E3.m1.4.4.3.3.2.6" stretchy="false" xref="S3.E3.m1.4.4.3.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.4.cmml" xref="S3.E3.m1.4.4.4"></eq><csymbol cd="latexml" id="S3.E3.m1.4.4.5.cmml" xref="S3.E3.m1.4.4.5">absent</csymbol><apply id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"><times id="S3.E3.m1.4.4.3.4.cmml" xref="S3.E3.m1.4.4.3.4"></times><apply id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"></minus><cn id="S3.E3.m1.2.2.1.1.1.1.2.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.1.2">1</cn><ci id="S3.E3.m1.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.3">italic-Ïµ</ci></apply><apply id="S3.E3.m1.4.4.3.5.cmml" xref="S3.E3.m1.4.4.3.5"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.3.5.1.cmml" xref="S3.E3.m1.4.4.3.5">subscript</csymbol><ci id="S3.E3.m1.4.4.3.5.2.cmml" xref="S3.E3.m1.4.4.3.5.2">â„’</ci><apply id="S3.E3.m1.4.4.3.5.3.cmml" xref="S3.E3.m1.4.4.3.5.3"><times id="S3.E3.m1.4.4.3.5.3.1.cmml" xref="S3.E3.m1.4.4.3.5.3.1"></times><ci id="S3.E3.m1.4.4.3.5.3.2.cmml" xref="S3.E3.m1.4.4.3.5.3.2">ğ·</ci><ci id="S3.E3.m1.4.4.3.5.3.3.cmml" xref="S3.E3.m1.4.4.3.5.3.3">ğ‘ƒ</ci><ci id="S3.E3.m1.4.4.3.5.3.4.cmml" xref="S3.E3.m1.4.4.3.5.3.4">ğ‘‚</ci></apply></apply><vector id="S3.E3.m1.4.4.3.3.3.cmml" xref="S3.E3.m1.4.4.3.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğœƒ</ci><apply id="S3.E3.m1.3.3.2.2.1.1.cmml" xref="S3.E3.m1.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.2.2.1.1.1.cmml" xref="S3.E3.m1.3.3.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.2.2.1.1.2.cmml" xref="S3.E3.m1.3.3.2.2.1.1.2">ğ‘¦</ci><ci id="S3.E3.m1.3.3.2.2.1.1.3.cmml" xref="S3.E3.m1.3.3.2.2.1.1.3">ğ‘¤</ci></apply><apply id="S3.E3.m1.4.4.3.3.2.2.cmml" xref="S3.E3.m1.4.4.3.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.3.3.2.2.1.cmml" xref="S3.E3.m1.4.4.3.3.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.3.3.2.2.2.cmml" xref="S3.E3.m1.4.4.3.3.2.2.2">ğ‘¦</ci><ci id="S3.E3.m1.4.4.3.3.2.2.3.cmml" xref="S3.E3.m1.4.4.3.3.2.2.3">ğ‘™</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\displaystyle=(1-\epsilon)\mathcal{L}_{DPO}(\theta,y_{w},y_{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.4d">= ( 1 - italic_Ïµ ) caligraphic_L start_POSTSUBSCRIPT italic_D italic_P italic_O end_POSTSUBSCRIPT ( italic_Î¸ , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\epsilon\mathcal{L}_{DPO}(\theta,y_{l},y_{w})," class="ltx_Math" display="inline" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1a" xref="S3.E4.m1.2.2.1.1.cmml">+</mo><mrow id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.4" xref="S3.E4.m1.2.2.1.1.2.4.cmml">Ïµ</mi><mo id="S3.E4.m1.2.2.1.1.2.3" xref="S3.E4.m1.2.2.1.1.2.3.cmml">â¢</mo><msub id="S3.E4.m1.2.2.1.1.2.5" xref="S3.E4.m1.2.2.1.1.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.1.1.2.5.2" xref="S3.E4.m1.2.2.1.1.2.5.2.cmml">â„’</mi><mrow id="S3.E4.m1.2.2.1.1.2.5.3" xref="S3.E4.m1.2.2.1.1.2.5.3.cmml"><mi id="S3.E4.m1.2.2.1.1.2.5.3.2" xref="S3.E4.m1.2.2.1.1.2.5.3.2.cmml">D</mi><mo id="S3.E4.m1.2.2.1.1.2.5.3.1" xref="S3.E4.m1.2.2.1.1.2.5.3.1.cmml">â¢</mo><mi id="S3.E4.m1.2.2.1.1.2.5.3.3" xref="S3.E4.m1.2.2.1.1.2.5.3.3.cmml">P</mi><mo id="S3.E4.m1.2.2.1.1.2.5.3.1a" xref="S3.E4.m1.2.2.1.1.2.5.3.1.cmml">â¢</mo><mi id="S3.E4.m1.2.2.1.1.2.5.3.4" xref="S3.E4.m1.2.2.1.1.2.5.3.4.cmml">O</mi></mrow></msub><mo id="S3.E4.m1.2.2.1.1.2.3a" xref="S3.E4.m1.2.2.1.1.2.3.cmml">â¢</mo><mrow id="S3.E4.m1.2.2.1.1.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.E4.m1.2.2.1.1.2.2.2.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">Î¸</mi><mo id="S3.E4.m1.2.2.1.1.2.2.2.4" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml">l</mi></msub><mo id="S3.E4.m1.2.2.1.1.2.2.2.5" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.cmml">y</mi><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.3" xref="S3.E4.m1.2.2.1.1.2.2.2.2.3.cmml">w</mi></msub><mo id="S3.E4.m1.2.2.1.1.2.2.2.6" stretchy="false" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><plus id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1"></plus><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"><times id="S3.E4.m1.2.2.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.3"></times><ci id="S3.E4.m1.2.2.1.1.2.4.cmml" xref="S3.E4.m1.2.2.1.1.2.4">italic-Ïµ</ci><apply id="S3.E4.m1.2.2.1.1.2.5.cmml" xref="S3.E4.m1.2.2.1.1.2.5"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.5.1.cmml" xref="S3.E4.m1.2.2.1.1.2.5">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.2.5.2.cmml" xref="S3.E4.m1.2.2.1.1.2.5.2">â„’</ci><apply id="S3.E4.m1.2.2.1.1.2.5.3.cmml" xref="S3.E4.m1.2.2.1.1.2.5.3"><times id="S3.E4.m1.2.2.1.1.2.5.3.1.cmml" xref="S3.E4.m1.2.2.1.1.2.5.3.1"></times><ci id="S3.E4.m1.2.2.1.1.2.5.3.2.cmml" xref="S3.E4.m1.2.2.1.1.2.5.3.2">ğ·</ci><ci id="S3.E4.m1.2.2.1.1.2.5.3.3.cmml" xref="S3.E4.m1.2.2.1.1.2.5.3.3">ğ‘ƒ</ci><ci id="S3.E4.m1.2.2.1.1.2.5.3.4.cmml" xref="S3.E4.m1.2.2.1.1.2.5.3.4">ğ‘‚</ci></apply></apply><vector id="S3.E4.m1.2.2.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğœƒ</ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.3">ğ‘™</ci></apply><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2">ğ‘¦</ci><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.3">ğ‘¤</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\displaystyle+\epsilon\mathcal{L}_{DPO}(\theta,y_{l},y_{w}),</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">+ italic_Ïµ caligraphic_L start_POSTSUBSCRIPT italic_D italic_P italic_O end_POSTSUBSCRIPT ( italic_Î¸ , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">where <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">italic_Ïµ</annotation></semantics></math> indicates the probability of the answer being wrong (or flipping the pair).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S3.F1.g1" src="extracted/5878566/image/pipeline_train.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The Training Pipeline: Organized into three consecutive stages; Supervised Fine-Tuning, Direct Preference Optimization Training, and Multiple Choice Question Answering Specialization.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Strategy</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Test Rwrds Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.1">LLaMa-3-8B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.2">DPO</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.3.1">79.7%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.1">LLaMa-3-8B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.2">SFT+DPO</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.3">79.3%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.1">Mistral-7B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.2">DPO</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.3">76.5%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.5.4.1">Mistral-7B</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.5.4.2">SFT+DPO</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.5.4.3">71.3%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy scores of the models on 1000 samples of the test set.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Retrieval Augmented Generation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We augment LLaMa-SciQ by incorporating Retrieval-Augmented Generation (RAG) methods. This approach stands out as one of the most effective means to enhance the predictive capabilities of our model. RAG combines the capabilities of generative models, dense vector indices of a racorpus of documents, and pre-trained neural retrievers.
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3.F2" title="Figure 2 â€£ 3.3 Retrieval Augmented Generation â€£ 3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes our RAG pipeline, known as the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">Naive RAG</span>.
We use the dataset described inÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS4" title="4.1.4 DPR Dataset â€£ 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">4.1.4</span></a> as our Dense Passage Retrieval (DPR) corpus of documents over which we create an index using Facebookâ€™s FAISS libraryÂ <cite class="ltx_cite ltx_citemacro_cite">Douze etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib9" title="">2024</a>)</cite>. Documents are then retrieved using Facebookâ€™s DPR question encoderÂ <cite class="ltx_cite ltx_citemacro_cite">Karpukhin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib17" title="">2020</a>)</cite> and added to the prompt in the format detailed in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A4.SS1" title="D.1 Prompt Format â€£ Appendix D Model Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">D.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We observed that our model is getting biased by saying to use the provided information. So we changed the prompt for RAG to tell the model to consider the model but not get biased on the information and try to fulfill the objective of the questions.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="718" id="S3.F2.g1" src="extracted/5878566/image/rag_pipe.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The RAG Pipeline</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Quantization</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We took an alternative root compared to standard quantization techniques. In particular, we specified the bytes-and-bits (bnb) parameter when loading the model using Unlosth package. We, therefore, reduced the weights to 4-bits while sustaining the accuracy.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">When you enable "load_in_4bits" in the "from_pretrained" function of the unsloth repository, the model utilizes a quantization technique facilitated by the bitsandbytes library. This technique allows the model weights to be represented with 4-bit precision, significantly reducing the modelâ€™s memory footprint while attempting to preserve performance.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">This 4-bit quantization primarily involves the transformation of model weights, previously in full-precision formats like fp16 or bf16, into a 4-bit format. The process entails creating instances of linear layers designed for 4-bit operations (e.g., "Linear4bit"), and then loading the original modelâ€™s weights into these quantized modules. The actual quantization happens when these modified models are transferred to a computation device like a GPU.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">This quantization approach can utilize different data types for quantization, like FP4 (Float4) or NF4 (NormalFloat4), which are tailored for different kinds of data distributions and usage scenarios. For example, NF4 is designed for data that naturally follows a normal distribution, offering potential performance improvements in such cases.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">This section outlines the datasets created for our modelâ€™s alignment stages. Samples of the datasets can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A2" title="Appendix B Datasets Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>SFT Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We first introduce <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p1.1.1">StemQA</span>, a specialized dataset to extend our modelâ€™s performance on math and coding questions. This dataset is a blend of <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p1.1.2">MetaMathQA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib35" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p1.1.3">CodeFeedback-Filtered-Instruction</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zheng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib37" title="">2024</a>)</cite> datasets. It is balanced so that 75% of the questions are math-related, while the remaining 25% are coding-related. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T2" title="Table 2 â€£ 4.1.1 SFT Dataset â€£ 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">2</span></a> presents these proportions. The answers now include the rationale followed by <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.4">"The answer is: &lt;Maths/Code&gt;"</span> to simplify future answer extraction.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Ratio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S4.T2.1.2.1.1.1">MetaMathQA</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">375,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">75%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1"><span class="ltx_text ltx_font_typewriter" id="S4.T2.1.3.2.1.1">CodeFeedback</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">125,000</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">25%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.4.3.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T2.1.4.3.1.1">StemQA<span class="ltx_text ltx_font_serif" id="S4.T2.1.4.3.1.1.1"> (ours)</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.4.3.2">500,000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.4.3.3">â€“</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Dataset sizes and their ratios of the SFT dataset</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">MetaMathQA</span>â€ƒ
Augmented version of the <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.SS1.SSS1.p2.1.2">training</span> sets from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.3">GSM8K</span>Â <cite class="ltx_cite ltx_citemacro_cite">Cobbe etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib7" title="">2021</a>)</cite> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS1.p2.1.4">MATH</span>Â <cite class="ltx_cite ltx_citemacro_cite">Hendrycks etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib13" title="">2021</a>)</cite>. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">CodeFeedback-Filtered-Instruction</span>â€ƒ
Curated collection of code instruction queries extracted from four prominent open-source code instruction tuning datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>DPO Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Then, we introduce <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p1.1.1">StemDPO</span>, a dataset to align our model with human preferences, focusing particularly on STEM questions. This dataset combines our class preference pairs with the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p1.1.2">PyDPO</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p1.1.3">MetaMathDPO</span> datasets. Our objective was to expand this dataset to a size of 50,000 samples, maintaining the same distribution proportions as the SFT dataset, assuming our class preferences are similarly balanced (see TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T3" title="Table 3 â€£ 4.1.2 DPO Dataset â€£ 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">3</span></a>).
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">Ratio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S4.T3.1.2.1.1.1">ClassPreferences</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">21,596</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">43%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1"><span class="ltx_text ltx_font_typewriter" id="S4.T3.1.3.2.1.1">PyDPO</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">7,101</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">14%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.3.1"><span class="ltx_text ltx_font_typewriter" id="S4.T3.1.4.3.1.1">MetaMathDPO</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">21,303</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">43%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T3.1.5.4.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.1.5.4.1.1">StemDPO<span class="ltx_text ltx_font_serif" id="S4.T3.1.5.4.1.1.1"> (ours)</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.5.4.2">50,000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.5.4.3">â€“</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Dataset sizes and their ratios of the DPO dataset</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">PyDPO</span>â€ƒ
DPO dataset meant to enhance python coding abilities. This dataset uses the excellent <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p2.1.2">Tested-22k-Python-Alpaca</span> dataset as the "chosen" responses and generates the "rejected" values with a mix of <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p2.1.3">airoboros-l2-13b-3.1</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p2.1.4">bagel-7b-v0.1</span>.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p3.1.1">MetaMathDPO</span>â€ƒ
Paired version of the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS2.p3.1.2">MetaMathQA</span> dataset. To construct the paired preferences, the original responses are taken as the preferred completions and randomly corrupted (at an intermediate calculation) so that it is less preferable.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>MCQ Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">We present <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.p1.1.1">StemMCQ</span>, a modified version of the well-known <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.p1.1.2">AQuA-RAT</span> datasetÂ <cite class="ltx_cite ltx_citemacro_cite">Ling etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib18" title="">2017</a>)</cite>, specifically designed to align the model with its primary purpose: answering STEM multiple-choice questions. The answers include the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS3.p1.1.3">AQuA-RAT</span> rationale followed by our extraction flag: <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.4">"The answer is: &lt;MCQ Letter&gt;"</span>. We chose to include the rationale in our responses, as the Chain-of-Thought approach has demonstrated improved results compared to simply providing the answerÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib30" title="">2022</a>)</cite>. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T4" title="Table 4 â€£ 4.1.3 MCQ Dataset â€£ 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">4</span></a> presents the dataset size.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">Ratio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S4.T4.1.2.1.1.1">AQuA-RAT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">97,500</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">100%</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T4.1.3.2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.1.3.2.1.1">StemMCQ<span class="ltx_text ltx_font_serif" id="S4.T4.1.3.2.1.1.1"> (ours)</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.3.2.2">97,500</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.3.2.3">â€“</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Dataset sizes and their ratios of the MCQ dataset</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.1">AQuA-RAT</span>â€ƒ
A large-scale dataset consisting of approximately 100,000 algebraic word problems. The solution to each question is explained step-by-step using natural language.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>DPR Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">To enable RAG in our model, we developed <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS4.p1.1.1">StemDPR</span>, a DPR corpus of Wikipedia science documents. This dataset is built from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS4.p1.1.2">WikiStemCorpus<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote1.1.1.1">1</span></span><span class="ltx_text ltx_font_serif" id="footnote1.5">See the dataset </span><a class="ltx_ref ltx_href ltx_font_serif" href="https://www.kaggle.com/datasets/conjuring92/wiki-stem-corpus" title="">here</a><span class="ltx_text ltx_font_serif" id="footnote1.6">.</span></span></span></span></span>, a science-focused subset of the well-known RAG dataset <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS4.p1.1.3">wiki_dpr</span>Â <cite class="ltx_cite ltx_citemacro_cite">Karpukhin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib17" title="">2020</a>)</cite>. We compute the document embeddings of <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS4.p1.1.4">WikiStemCorpus</span> using Facebookâ€™s DPR context encoderÂ <cite class="ltx_cite ltx_citemacro_cite">Karpukhin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib17" title="">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this section, we define the evaluation process, which is divided into multiple steps. The initial step involves selecting the best model based on its generation quality. The final step assesses the predictability power of our MCQA model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To select the best generation models, we need to assess the quality of their generation in terms of correctness and reasoning.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I1.i1.p1.1.1">DPO Reward Accuracies</span> <cite class="ltx_cite ltx_citemacro_cite">Rafailov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib21" title="">2023</a>)</cite>: This allows us to assess the preference alignment of the modelâ€™s generation in terms of human alignment.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">To thoroughly assess our modelâ€™s performance on <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">STEM QA</span>, we choose diverse datasets that represent various skills the model should have acquired. First, we use benchmark datasets to evaluate the correctness of our first-stage model in answering open STEM questions:</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I2.i1.p1.1.1">MATH</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib13" title="">2021</a>)</cite>: This dataset of 5k advanced mathematics questions to assess the modelâ€™s mathematical step-by-step reasoning skills.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I2.i2.p1.1.1">GSM8K</span> <cite class="ltx_cite ltx_citemacro_cite">Cobbe etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib7" title="">2021</a>)</cite>: A dataset of 8.5K (1k testing split) high quality linguistically diverse grade school math word problems created by human problem writers. Used to further evaluate the modelâ€™s mathematical reasoning abilities.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Then, we use MCQA datasets to assess the MCQA performance of our final specialized model:</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I3.i1.p1.1.1">MCQA Examples EPFL</span>: A dataset of around 350 samples designed to measure general knowledge and reasoning across multiple domains(It covers 57 subjects across STEM), used to test both world knowledge and problem solving ability.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">We use accuracy as our metric since it was our target performance metric throughout the project and is best suited for evaluating unique MCQA answers.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baseline</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We compare LLaMa-SciQ with the candidate base models: <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.1">LLaMa-3-8B</span>Â <cite class="ltx_cite ltx_citemacro_cite">AI (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib1" title="">2024</a>)</cite> and <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.2">Mistral-7B</span>Â <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib16" title="">2023</a>)</cite>. At each step of the training pipeline (described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S3" title="3 Approach â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">3</span></a>), we conduct ablation studies by comparing the newly trained model with the model from the previous step.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Setup</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We adapt our SFT and DPO procedures to run on a single V-100 GPU with 32 GB of VRAM and a single A-100 GPU with 40 GB of VRAM, respectively. We utilize the Unsloth library <cite class="ltx_cite ltx_citemacro_cite">unslothai and contributors (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib25" title="">2024</a>)</cite>, designed for fast and resource-efficient training of large language models. Combining Unslothâ€™s techniques with LoRa adaptors allows us to efficiently align LLaMa-3-8B and Mistral-7B within our resource constraints. In addition, due to the extended duration of the training process (more than 15 hours), extensive hyperparameter tuning is not practical.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>1st SFT â€“ Mathematical Reasoning</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">Therefore, the SFT hyperparameters (see TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.T9" title="Table 9 â€£ C.1 Training Hyperparameters â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">9</span></a> in the appendix) are chosen based on the state-of-the-art SFT of the models. For similar reasons, we train our models using two relatively small, random sample sizes from the full SFT dataset (described in Section 2.1): 10,000 and 100,000 examples.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">We conduct two SFT sessions for each model. The best models are selected from the 100,000-sample-size runs, showing the best results in the generation (see an example in <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A4.SS2" title="D.2 Generation â€£ Appendix D Model Samples â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">D.2</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>DPO Alignment</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">For the DPO training procedure, we split the DPO dataset described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.SS1.SSS2" title="4.1.2 DPO Dataset â€£ 4.1 Data â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">4.1.2</span></a> into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.T9" title="Table 9 â€£ C.1 Training Hyperparameters â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>2nd SFT â€“ MCQA Reasoning</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">Finally, using the same hyperparameter setup as in the first SFT sessions, we perform the final SFT training for MCQA specialization using 97,500 MCQ samples. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.F3" title="Figure 3 â€£ 4.4.3 2nd SFT â€“ MCQA Reasoning â€£ 4.4 Setup â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">3</span></a> presents the training loss of the kept run.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S4.F3.g1" src="extracted/5878566/image/llamasciq-loss.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>MCQ-SFT Training Loss</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Results</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The intermediate and final results can be found in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T6" title="Table 6 â€£ 4.5 Results â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">6</span></a>, TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T5" title="Table 5 â€£ 4.5 Results â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">5</span></a>, and TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T7" title="Table 7 â€£ 4.5 Results â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I4.i1.p1.1.1">MATH</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib13" title="">2021</a>)</cite>: On the MATH dataset, known for its complexity and depth, we managed to achieve the performance announced by Meta on their introduction page of LLaMA-3-8B, with a score of 30%. This demonstrates the power of LLaMA-3, especially in comparison to the Mistral-7B, where our results were consistent with Mistralâ€™s research, showing a score of around 11%.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I4.i2.p1.1.1">GSM8K</span> <cite class="ltx_cite ltx_citemacro_cite">Cobbe etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib7" title="">2021</a>)</cite>: For the GSM8K dataset, which is less challenging than MATH, our score was slightly below Metaâ€™s results by 5.1%, but still more than 40% higher than Mistralâ€™s performance. Note that we used 0-shot prompting for both evaluations, whereas Meta used few-shot prompting.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Finally, for the evaluation of LLaMa-SciQ on MCQA from the EPFL course, the results were decent but somewhat disappointing compared to the general math benchmarks.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<ul class="ltx_itemize" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.I5.i1.p1.1.1">MCQA Examples EPFL</span>: The best score was achieved by the policy model, outperforming the two specializations by around 5%. The RAG and Quantized models showed similar performance, with a difference of approximately 0.555 in accuracy. The RAG system did not improve accuracy and seemed to lead to poorer decisions, possibly due to the low similarity power of the retriever, inadequate content of the STEM DPR dataset, or the modelâ€™s unfamiliarity with using context in prompts. However, the Quantized model, despite a significant reduction in size, only showed a 5% loss in performance, which is a notable result.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">MATH</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.1">LLaMa-3-8B-Instruct</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T5.1.2.1.2.1">30%</span> <span class="ltx_text ltx_font_italic" id="S4.T5.1.2.1.2.2">(4-shot, CoT)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T5.1.3.2.1">LLaMa-3-8B(SFT+DPO)</td>
<td class="ltx_td ltx_align_left" id="S4.T5.1.3.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T5.1.3.2.2.1">30%</span> <span class="ltx_text ltx_font_italic" id="S4.T5.1.3.2.2.2">(0-shot)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T5.1.4.3.1">Mistral-7B-Instruct</td>
<td class="ltx_td ltx_align_left" id="S4.T5.1.4.3.2">11% <span class="ltx_text ltx_font_italic" id="S4.T5.1.4.3.2.1">(4-shot, CoT)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.5.4.1">Mistral-7B(SFT+DPO)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.5.4.2">10.3% <span class="ltx_text ltx_font_italic" id="S4.T5.1.5.4.2.1">(0-shot)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison of different models on MATH benchmark</figcaption>
</figure>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.2.1">GSM8k</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.1.2.1.1">LLaMa-3-8B-Instruct</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T6.1.2.1.2.1">79.6%</span> <span class="ltx_text ltx_font_italic" id="S4.T6.1.2.1.2.2">(8-shot, CoT)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T6.1.3.2.1">LLaMa-3-8B(SFT+DPO)</td>
<td class="ltx_td ltx_align_left" id="S4.T6.1.3.2.2">74.5% <span class="ltx_text ltx_font_italic" id="S4.T6.1.3.2.2.1">(0-shot)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T6.1.4.3.1">Mistral-7B-Instruct</td>
<td class="ltx_td ltx_align_left" id="S4.T6.1.4.3.2">39.9% <span class="ltx_text ltx_font_italic" id="S4.T6.1.4.3.2.1">(8-shot, CoT)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.1.5.4.1">Mistral-7B(SFT+DPO)</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.1.5.4.2">28.5% <span class="ltx_text ltx_font_italic" id="S4.T6.1.5.4.2.1">(0-shot)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance comparison of different models on GSM8k benchmark</figcaption>
</figure>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.2.1">EPFL MCQA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.2.1.1">LLaMa-SciQ</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.2.1">45.21%</span> <span class="ltx_text ltx_font_italic" id="S4.T7.1.2.1.2.2">(0-shot)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T7.1.3.2.1">LLaMa-SciQ+RAG</td>
<td class="ltx_td ltx_align_left" id="S4.T7.1.3.2.2">40.62% <span class="ltx_text ltx_font_italic" id="S4.T7.1.3.2.2.1">(0-shot)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.4.3.1">LLaMa-Sci+Quanttize</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T7.1.4.3.2">40.07% <span class="ltx_text ltx_font_italic" id="S4.T7.1.4.3.2.1">(0-shot)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Models performance on MCQA EPFL benchmark</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We noted that our model exhibits reasonable generation capabilities and demonstrates sound reasoning when answering questions. During our SFT and DPO training, which frequently involved mathematical questions, our model proved particularly adept at handling them. However, as the benchmark (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S4.T7" title="Table 7 â€£ 4.5 Results â€£ 4 Experiments â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">7</span></a>) included questions from a wide range of disciplines, the results were generally acceptable.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We believe our quantized model maintained the accuracy of the LLaMa-SciQ model, as it occasionally achieved higher accuracy in our tests. During development, we experimented with various configuration settings, including adjustments to the temperature, to optimize performance. TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#S5.T8" title="Table 8 â€£ 5 Analysis â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">8</span></a> presents the best results of the generation tested on a 10-subsample of the EPFL MCQ dataset; the full test results are presented in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A5" title="Appendix E Generation Tuning â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">E</span></a>). Despite these efforts, we think the generation configuration could still benefit from fine-tuning. With a large beam size in the beam search, the quantized modelâ€™s performance was comparable to that of LLaMa-SciQ. However, due to resource constraints, we reduced the beam size to 1 for our final benchmark.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The RAG model did not meet our goal of enhancing accuracy. We attribute this to the encoder used for information retrieval, which was not specifically fine-tuned for our model. Consequently, the encoder sometimes retrieved irrelevant information, potentially biasing the model towards incorrect data. Additionally, our model was trained to adhere to a specific template rather than to utilize the provided information effectively, which likely contributed to its underperformance.</p>
</div>
<figure class="ltx_table" id="S5.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T8.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.1.1.1.1">
<span class="ltx_p" id="S5.T8.1.1.1.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.1.1.1.1">Generation Configuration</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.1.1.2.1">
<span class="ltx_p" id="S5.T8.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.2.1.1.1">Accuracy</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T8.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.2.1.1.1">
<span class="ltx_p" id="S5.T8.1.2.1.1.1.1" style="width:128.0pt;">Greedy</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T8.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.2.1.2.1">
<span class="ltx_p" id="S5.T8.1.2.1.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T8.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.3.2.1.1">
<span class="ltx_p" id="S5.T8.1.3.2.1.1.1" style="width:128.0pt;">Sample (default)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T8.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.3.2.2.1">
<span class="ltx_p" id="S5.T8.1.3.2.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T8.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.4.3.1.1">
<span class="ltx_p" id="S5.T8.1.4.3.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T8.1.4.3.1.1.1.1">Sample (default, temp=0.3)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T8.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.4.3.2.1">
<span class="ltx_p" id="S5.T8.1.4.3.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T8.1.4.3.2.1.1.1">50%</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T8.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.5.4.1.1">
<span class="ltx_p" id="S5.T8.1.5.4.1.1.1" style="width:128.0pt;">Sample (default, top_p=0.95, temp=0.3)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T8.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T8.1.5.4.2.1">
<span class="ltx_p" id="S5.T8.1.5.4.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Accuracy of Different Sampling Methods on the 10-sample of EFPL MCQA</figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ethical considerations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we address the ethical considerations relevant to LLaMa-SciQ.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Low-Resources Language Performances</span>â€ƒThe high performance of LLaMa-3-8b on high-resource languagesÂ <cite class="ltx_cite ltx_citemacro_cite">AI (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib1" title="">2024</a>)</cite> suggests that LLaMa-SciQ should be capable of handling questions in most of these languages (with the best performance on English MCQs, as the SFT dataset is English-based). However, additional work is needed to extend its capabilities to low-resource languages, such as Urdu and Swahili. This could be achieved by expanding our SFT datasets to teach the model multilingual scientific reasoning. Furthermore, although more challenging and costly, we could extend our DPO dataset to include low-resource languages preferences to improve the modelâ€™s generations in these latest.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Accessibility for Deaf Community</span>â€ƒ The exclusion of signed languages from modern language technologies marginalizes Deaf communities, who prefer to communicate in signed languages onlineÂ <cite class="ltx_cite ltx_citemacro_cite">Yin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib33" title="">2021</a>)</cite>. Therefore, it is essential to include signed language compatibility in our model to respect this community and support its communication preferences. One potential approach to achieve this is by harnessing Sign Language Translation (SLT), which has seen advancements through deep learning techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">Al-Qurishi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib3" title="">2021</a>); Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib6" title="">2022</a>)</cite>, such as the <span class="ltx_text ltx_font_typewriter" id="S6.p3.1.2">STMC-Transformer</span> modelÂ <cite class="ltx_cite ltx_citemacro_cite">Yin and Read (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib34" title="">2020</a>)</cite>. By integrating SLT into LLaMa-SciQâ€™s pipeline, we could easily address signed questions. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Social Bias &amp; Harmful Content</span>â€ƒ The model, designed for the MCQA task, should not exhibit more harmful content or social bias than its inherent base model. However, for broader usages, studies indicated that LLM presents vulnerabilities exploitable to output harmful content or social biasÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib29" title="">2024</a>); Deng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib8" title="">2023</a>)</cite>. Therefore, future work should involve additional training to mitigate LLaMa-SciQâ€™s potential biases or harmful content that may arise from out-of-scope usages. This can be achieved using Metaâ€™s Responsible Use Guide (RUG)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>See the RUG <a class="ltx_ref ltx_href" href="https://llama.meta.com/responsible-use-guide/" title="">here</a>.</span></span></span> and LLaMa-GuardÂ <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">inan2023llama</span></cite>, an LLM-based safeguard model designed for Human-AI conversation use cases.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we propose LLaMa-SciQ: an educational chatbot designed for science multiple-choice question answering (MCQA). The model is a fine-tuned <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.1">LLaMa-3-8B</span> aligned with human preferences using the novel STEM datasets introduced (<span class="ltx_text ltx_font_typewriter" id="S7.p1.1.2">StemQA</span>, <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.3">StemDPO</span>, <span class="ltx_text ltx_font_typewriter" id="S7.p1.1.4">StemMCQ</span>). It also employs cost-reducing training techniques such as UnslothÂ <cite class="ltx_cite ltx_citemacro_cite">unslothai and contributors (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib25" title="">2024</a>)</cite> to address limitation in resources. LLaMa-SciQ maintains the performance of state-of-the-art large language models in scientific question answering, achieving up to 74.5% on the GSM8k benchmark and 30% on the MATH benchmark using zero-shot prompting. These results are comparable to the base model using eight-shot prompting on these benchmarks. Exploring few-shot prompting could be a promising direction for future work. While the modelâ€™s performance on the MCQA task yielded relatively low results, they are acceptable considering the complexity of such a specialized task.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Future work includes enhancing the modelâ€™s performance by exploring various prompting strategiesÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib28" title="">2022</a>); Wan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#bib.bib26" title="">2023</a>)</cite>. Additionally, adapting the model to more languages â€“ with an emphasis on signed languages â€“ and evaluating its social biases will be essential to make it accessible to all, thereby strengthening its educational impact.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Meta AI. 2024.

</span>
<span class="ltx_bibblock">Llama 3: Open source language models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama3" title="">https://github.com/meta-llama/llama3</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="">Llama 3 model card</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Qurishi etÂ al. (2021)</span>
<span class="ltx_bibblock">
Muhammad Al-Qurishi, Thariq Khalid, and Riad Souissi. 2021.

</span>
<span class="ltx_bibblock">Deep learning for sign language recognition: Current techniques, benchmarks, and open issues.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Access</em>, 9:126917â€“126951.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini etÂ al. (2024)</span>
<span class="ltx_bibblock">
Afra Amini, Tim Vieira, and Ryan Cotterell. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.10571" title="">Direct preference optimization with an offset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chee etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and ChristopherÂ De Sa. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.13304" title="">Quip: 2-bit quantization of large language models with guarantees</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yutong Chen, Fangyun Wei, Xiao Sun, Zhirong Wu, and Stephen Lin. 2022.

</span>
<span class="ltx_bibblock">A simple multi-modality transfer learning baseline for sign language translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 5120â€“5130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gelei Deng, YiÂ Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023.

</span>
<span class="ltx_bibblock">Jailbreaker: Automated jailbreak across multiple large language model chatbots.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2307.08715</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze etÂ al. (2024)</span>
<span class="ltx_bibblock">
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.08281" title="">The faiss library</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar etÂ al. (2023)</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.17323" title="">Gptq: Accurate post-training quantization for generative pre-trained transformers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FÃ¼tterer etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tim FÃ¼tterer, Christian Fischer, Anastasiia Alekseeva, Xiaobin Chen, Tamara Tate, Mark Warschauer, and Peter Gerjets. 2023.

</span>
<span class="ltx_bibblock">Chatgpt in education: global reactions to ai innovations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Scientific reports</em>, 13(1):15310.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks etÂ al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2009.03300" title="">Measuring massive multitask language understanding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2106.09685" title="">Lora: Low-rank adaptation of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imani etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shima Imani, Liang Du, and Harsh Shrivastava. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.05398" title="">Mathprompter: Mathematical reasoning using large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
AlbertÂ Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lioÂ Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, TevenÂ Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and WilliamÂ El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.550" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6769â€“6781, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling etÂ al. (2017)</span>
<span class="ltx_bibblock">
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017.

</span>
<span class="ltx_bibblock">Program induction by rationale generation: Learning to solve and explain algebraic word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1705.04146</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Odede and Frommholz (2024)</span>
<span class="ltx_bibblock">
Julius Odede and Ingo Frommholz. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3627508.3638293" title="">Jaybot â€“ aiding university students and admission with an llm-based chatbot</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2024 Conference on Human Information Interaction and Retrieval</em>, CHIIR â€™24, page 391â€“395, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in neural information processing systems</em>, 35:27730â€“27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, ChristopherÂ D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.18290" title="">Direct preference optimization: Your language model is secretly a reward model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savelka etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jaromir Savelka, Arav Agarwal, Christopher Bogart, and Majd Sakr. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08033" title="">Large language models (gpt) struggle to answer multiple-choice questions about code</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma etÂ al. (2023)</span>
<span class="ltx_bibblock">
Mandar Sharma, Nikhil Muralidhar, and Naren Ramakrishnan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.08246" title="">Learning non-linguistic skills without sacrificing linguistic proficiency</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tseng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and ChristopherÂ De Sa. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.04396" title="">Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">unslothai and contributors (2024)</span>
<span class="ltx_bibblock">
unslothai and contributors. 2024.

</span>
<span class="ltx_bibblock">unsloth.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/unslothai/unsloth" title="">https://github.com/unslothai/unsloth</a>.

</span>
<span class="ltx_bibblock">Original source code and documentation of the unsloth project.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xingchen Wan, Ruoxi Sun, Hanjun Dai, SercanÂ O Arik, and Tomas Pfister. 2023.

</span>
<span class="ltx_bibblock">Better zero-shot reasoning with self-adaptive prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2305.14106</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, ArjunÂ R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.10635" title="">Scibench: Evaluating college-level scientific problem-solving abilities of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, EdÂ Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2203.11171</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2024)</span>
<span class="ltx_bibblock">
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024.

</span>
<span class="ltx_bibblock">Jailbroken: How does llm safety training fail?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in neural information processing systems</em>, 35:24824â€“24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, YinÂ Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.01337" title="">An empirical study on challenging math problem solving with gpt-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian guang Lou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.06275" title="">Re-reading improves reasoning in large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2021)</span>
<span class="ltx_bibblock">
Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. 2021.

</span>
<span class="ltx_bibblock">Including signed languages in natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2105.05222</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin and Read (2020)</span>
<span class="ltx_bibblock">
Kayo Yin and Jesse Read. 2020.

</span>
<span class="ltx_bibblock">Better sign language translation with stmc-transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2004.00588</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, YuÂ Zhang, JamesÂ T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2309.12284</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tianjun Zhang, ShishirÂ G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and JosephÂ E. Gonzalez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.10131" title="">Raft: Adapting language model to domain specific rag</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tianyu Zheng, GeÂ Zhang, Tianhao Shen, Xueling Liu, BillÂ Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024.

</span>
<span class="ltx_bibblock">Opencodeinterpreter: Integrating code generation with execution and refinement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2402.14658</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Contribution</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Each member of the group contributed equally to all of the aspects of the project.
<br class="ltx_break"/>Matin Ansaripour: DPO training, LLaMa-adapter supervised training, Quantization coding, Report Writing.
<br class="ltx_break"/>Paul Teiletche: Dataset processing, external dataset adaptation, RAG specialisation, Report writing, Evaluation coding.
<br class="ltx_break"/>Marc-Antoine Allard: Dataset processing, external dataset adaptation, RAG specialisation, Report writing, Evaluation coding.
<br class="ltx_break"/>Maria Yuffa: DPO training, Quantization coding, Literature review, Report writing.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Datasets Samples</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>SFT Dataset</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<svg class="ltx_picture" height="126.41" id="A2.SS1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,126.41) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 117.75 C 0 122.53 3.88 126.41 8.66 126.41 L 591.34 126.41 C 596.12 126.41 600 122.53 600 117.75 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 104.66 L 599.21 104.66 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 109.39)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS1.p1.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS1.p1.pic1.1.1.1.1.1.1">Sample from the SFT dataset</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="80.25" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS1.p1.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS1.p1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.SS1.p1.pic1.2.2.2.1.1.1.1">{
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A2.SS1.p1.pic1.2.2.2.1.1.1.1.1">"problem"</span>: "Determine the sum of the positive factors of 48.",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS1.p1.pic1.2.2.2.1.1.1.1.2">"solution"</span>: "To find the sum of the positive factors of 48, we can [...]. The answer is: 124"
</span></span>
<span class="ltx_p" id="A2.SS1.p1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A2.SS1.p1.pic1.2.2.2.1.1.2.1">}</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>DPO Dataset</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<svg class="ltx_picture" height="159.62" id="A2.SS2.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,159.62) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 150.96 C 0 155.74 3.88 159.62 8.66 159.62 L 591.34 159.62 C 596.12 159.62 600 155.74 600 150.96 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 137.87 L 599.21 137.87 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 142.6)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS2.p1.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS2.p1.pic1.1.1.1.1.1.1">Sample from the DPO dataset</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="113.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS2.p1.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS2.p1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.pic1.2.2.2.1.1.1.1">{
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A2.SS2.p1.pic1.2.2.2.1.1.1.1.1">"prompt"</span>: "Tom eats a pound of carrots [...] how many calories did he eat in total?",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS2.p1.pic1.2.2.2.1.1.1.1.2">"chosen"</span>: "Tom eats 1 pound of carrots, which have 51 calories per pound, so he eats 1*51 = 51 calories [...] The answer is: 85",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS2.p1.pic1.2.2.2.1.1.1.1.3">"rejected"</span>: "Tom eats 1 pound of carrots, which have 51 calories per pound, so he eats 1*51 = 97 calories [...] The answer is: 85"
</span></span>
<span class="ltx_p" id="A2.SS2.p1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.pic1.2.2.2.1.1.2.1">}</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>MCQ Dataset</h3>
<div class="ltx_para ltx_noindent" id="A2.SS3.p1">
<svg class="ltx_picture" height="159.62" id="A2.SS3.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,159.62) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 150.96 C 0 155.74 3.88 159.62 8.66 159.62 L 591.34 159.62 C 596.12 159.62 600 155.74 600 150.96 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 137.87 L 599.21 137.87 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 142.6)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS3.p1.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS3.p1.pic1.1.1.1.1.1.1">Sample from the MCQ dataset</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="113.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS3.p1.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS3.p1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.pic1.2.2.2.1.1.1.1">{
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A2.SS3.p1.pic1.2.2.2.1.1.1.1.1">"subject"</span>: "maths",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS3.p1.pic1.2.2.2.1.1.1.1.2">"question"</span>: "There are 8 players in a chess group [...] how many total games will be played?",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS3.p1.pic1.2.2.2.1.1.1.1.3">"options"</span>: ["10","30","28","60","90"]
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS3.p1.pic1.2.2.2.1.1.1.1.4">"answer"</span>: "10 players are there. two players [...] The answer is: C."
</span></span>
<span class="ltx_p" id="A2.SS3.p1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.pic1.2.2.2.1.1.2.1">}</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>DPR Dataset</h3>
<div class="ltx_para ltx_noindent" id="A2.SS4.p1">
<svg class="ltx_picture" height="126.41" id="A2.SS4.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,126.41) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 117.75 C 0 122.53 3.88 126.41 8.66 126.41 L 591.34 126.41 C 596.12 126.41 600 122.53 600 117.75 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 104.66 L 599.21 104.66 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 109.39)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS4.p1.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS4.p1.pic1.1.1.1.1.1.1">Sample from the DPR dataset</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="80.25" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS4.p1.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS4.p1.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.pic1.2.2.2.1.1.1.1">{
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="A2.SS4.p1.pic1.2.2.2.1.1.1.1.1">"text"</span>: "In mathematical analysis, the Cauchy index is [...] the degree of q.",
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS4.p1.pic1.2.2.2.1.1.1.1.2">"title"</span>: "Cauchy index"
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A2.SS4.p1.pic1.2.2.2.1.1.1.1.3">"embeddings"</span>: [-0.6179105639457703, ..., 0.35533231496810913]
</span></span>
<span class="ltx_p" id="A2.SS4.p1.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.pic1.2.2.2.1.1.2.1">}</span></span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Intruction for DPO Generation</h3>
<div class="ltx_para ltx_noindent" id="A2.SS5.p1">
<svg class="ltx_picture" height="125.03" id="A2.SS5.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,125.03) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 116.37 C 0 121.15 3.88 125.03 8.66 125.03 L 591.34 125.03 C 596.12 125.03 600 121.15 600 116.37 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 103.28 L 599.21 103.28 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 108)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS5.p1.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS5.p1.pic1.1.1.1.1.1.1">Instruction to generate examples for DPO</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="78.87" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.SS5.p1.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A2.SS5.p1.pic1.2.2.2.1.1.1">"Imagine youâ€™re a teaching assistant for a <span class="ltx_text ltx_font_italic" id="A2.SS5.p1.pic1.2.2.2.1.1.1.1">&lt;course_topic&gt;</span> course. A student has just asked the question above. Your goal is to provide a comprehensive and detailed explanation, similar to how you would guide a student in understanding the concept thoroughly. Use scientific reasoning and relevant examples to clarify the topic and ensure a deep understanding by the student."</span>
</span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Training Details</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Here we present more details for SFT and DPO training.</p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Training Hyperparameters</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.T9" title="Table 9 â€£ C.1 Training Hyperparameters â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">9</span></a> presents the hyperparameters that we used for each training.</p>
</div>
<figure class="ltx_table" id="A3.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T9.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T9.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.T9.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T9.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.1.2.1">SFT Values</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T9.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A3.T9.1.2.1.3.1">DPO Values</span></td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T9.1.3.2.1">Epochs</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.3.2.2">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.3.2.3">1</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.4.3.1">Batch Size</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.4.3.2">4</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.4.3.3">2</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.5.4.1">Warmup Ratio</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.5.4.2">0.1</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.5.4.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.6.5.1">Learning Rate</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.6.5.2">2e-4</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.6.5.3">5e-5</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.7.6.1">LR Scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.7.6.2">Linear</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.7.6.3">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.8.7.1">Weight Decay</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.8.7.2">1e-2</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.8.7.3">1e-2</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T9.1.1.1">Neftune Noise <math alttext="\alpha" class="ltx_Math" display="inline" id="A3.T9.1.1.1.m1.1"><semantics id="A3.T9.1.1.1.m1.1a"><mi id="A3.T9.1.1.1.m1.1.1" xref="A3.T9.1.1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="A3.T9.1.1.1.m1.1b"><ci id="A3.T9.1.1.1.m1.1.1.cmml" xref="A3.T9.1.1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A3.T9.1.1.1.m1.1d">italic_Î±</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.2">5</td>
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.3">-</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T9.1.9.8.1">GA Steps</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T9.1.9.8.2">1</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T9.1.9.8.3">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>SFT and DPO Hyperparameters</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Training Metrics</h3>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1 </span>Maths-SFT</h4>
<div class="ltx_para" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.F4" title="Figure 4 â€£ C.2.1 Maths-SFT â€£ C.2 Training Metrics â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">4</span></a> presents the most important training metrics values of the best Maths-SFT runs of each model.</p>
</div>
<figure class="ltx_figure" id="A3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="368" id="A3.F4.sf1.g1" src="extracted/5878566/image/lr_sft.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Learning Rate vs Steps</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="387" id="A3.F4.sf2.g1" src="extracted/5878566/image/train_loss_sft.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Training Loss vs Steps</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>SFT Training statistics for Llama and Mistral models on 100,000 samples.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.2 </span>MCQ-SFT</h4>
<div class="ltx_para" id="A3.SS2.SSS2.p1">
<p class="ltx_p" id="A3.SS2.SSS2.p1.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A3.F5" title="Figure 5 â€£ C.2.2 MCQ-SFT â€£ C.2 Training Metrics â€£ Appendix C Training Details â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">5</span></a> presents the training metrics of the MCQ-SFT.</p>
</div>
<figure class="ltx_figure" id="A3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="315" id="A3.F5.sf1.g1" src="extracted/5878566/image/llamasciq-loss.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Training Loss</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="315" id="A3.F5.sf2.g1" src="extracted/5878566/image/llamasciq-lr.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Learning Rate</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="315" id="A3.F5.sf3.g1" src="extracted/5878566/image/llamasciq-normgrad.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Gradients Norm</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Training Analytics: Transformers Models</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Model Samples</h2>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Prompt Format</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1">Below is the prompt format used for the generations. If a section is not relevant for the task (e.g. context for non-RAG generation or options for non-MCQ), it is removed.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS1.p2">
<svg class="ltx_picture" height="209.28" id="A4.SS1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,209.28) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 200.62 C 0 205.4 3.88 209.28 8.66 209.28 L 591.34 209.28 C 596.12 209.28 600 205.4 600 200.62 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 187.69 L 599.21 187.69 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 192.41)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A4.SS1.p2.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A4.SS1.p2.pic1.1.1.1.1.1.1">Prompt Format</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A4.SS1.p2.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A4.SS1.p2.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS1.p2.pic1.2.2.2.1.1.1.1">Context Information:
<br class="ltx_break"/></span>{RAG Context}</span>
<span class="ltx_p" id="A4.SS1.p2.pic1.2.2.2.1.1.2">Below is a MCQ that you will need to answer using the above context information. Write an answer that fully explains your reasoning.</span>
<span class="ltx_p" id="A4.SS1.p2.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS1.p2.pic1.2.2.2.1.1.3.1">### Question:
<br class="ltx_break"/></span>{Question}</span>
<span class="ltx_p" id="A4.SS1.p2.pic1.2.2.2.1.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS1.p2.pic1.2.2.2.1.1.4.1">### Options:
<br class="ltx_break"/></span>{Options}</span>
<span class="ltx_p" id="A4.SS1.p2.pic1.2.2.2.1.1.5"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS1.p2.pic1.2.2.2.1.1.5.1">### Answer:
<br class="ltx_break"/></span>{Answer}</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Generation</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1">Here we present a sample of our LLaMa-3 maths fine-tuned modelâ€™s generation. The blue part is the one generated by the model.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS2.p2">
<svg class="ltx_picture" height="241.11" id="A4.SS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,241.11) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 8.66 L 0 232.44 C 0 237.23 3.88 241.11 8.66 241.11 L 591.34 241.11 C 596.12 241.11 600 237.23 600 232.44 L 600 8.66 C 600 3.88 596.12 0 591.34 0 L 8.66 0 C 3.88 0 0 3.88 0 8.66 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 0.79 8.66 L 0.79 219.36 L 599.21 219.36 L 599.21 8.66 C 599.21 4.31 595.69 0.79 591.34 0.79 L 8.66 0.79 C 4.31 0.79 0.79 4.31 0.79 8.66 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 224.08)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A4.SS2.p2.pic1.1.1.1.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A4.SS2.p2.pic1.1.1.1.1.1.1">Aligned LLaMa-3 Model Generation</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.47 12.6)"><foreignobject color="#000000" height="194.95" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.06">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A4.SS2.p2.pic1.2.2.2.1.1" style="width:404.0pt;">
<span class="ltx_p" id="A4.SS2.p2.pic1.2.2.2.1.1.1">Below is a question that you will need to answer. Write an answer that fully explains your reasoning.</span>
<span class="ltx_p" id="A4.SS2.p2.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS2.p2.pic1.2.2.2.1.1.2.1">###Question:
<br class="ltx_break"/></span>The town of Belize has 400 homes. One fourth of the townâ€™s homes are white. One fifth of the non-white homes have a fireplace. How many of the non-white homes do not have a fireplace?</span>
<span class="ltx_p" id="A4.SS2.p2.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A4.SS2.p2.pic1.2.2.2.1.1.3.1">###Answer:
<br class="ltx_break"/></span>One fourth of the townâ€™s homes are white, so there are 400/4 = 100 white homes. The remaining homes are non-white, so there are 400 - 100 = 300 non-white homes. One fifth of the non-white homes have a fireplace, so there are 300/5 = 60 non-white homes with a fireplace. Therefore, the number of non-white homes that do not have a fireplace is 300 - 60 = 240. ### 240 The answer is: 240</span>
</span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Generation Tuning</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.16779v1#A5.T10" title="Table 10 â€£ Appendix E Generation Tuning â€£ LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"><span class="ltx_text ltx_ref_tag">10</span></a> presents the MCQA accuracy of each configuration tested on a 10-subset of the EPFL MCQA dataset</p>
</div>
<figure class="ltx_table" id="A5.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T10.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A5.T10.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.1.1.1.1">
<span class="ltx_p" id="A5.T10.1.1.1.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.1.1.1">Generation Configuration</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A5.T10.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.1.1.2.1">
<span class="ltx_p" id="A5.T10.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.2.1.1.1">Accuracy</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T10.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.T10.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.2.1.1.1">
<span class="ltx_p" id="A5.T10.1.2.1.1.1.1" style="width:128.0pt;">Greedy</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A5.T10.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.2.1.2.1">
<span class="ltx_p" id="A5.T10.1.2.1.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.3.2.1.1">
<span class="ltx_p" id="A5.T10.1.3.2.1.1.1" style="width:128.0pt;">Beam Search (B=3)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.3.2.2.1">
<span class="ltx_p" id="A5.T10.1.3.2.2.1.1" style="width:71.1pt;">30%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.4.3.1.1">
<span class="ltx_p" id="A5.T10.1.4.3.1.1.1" style="width:128.0pt;">Beam Search (B=5)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.4.3.2.1">
<span class="ltx_p" id="A5.T10.1.4.3.2.1.1" style="width:71.1pt;">30%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.5.4.1.1">
<span class="ltx_p" id="A5.T10.1.5.4.1.1.1" style="width:128.0pt;">Sample (default)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.5.4.2.1">
<span class="ltx_p" id="A5.T10.1.5.4.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.6.5.1.1">
<span class="ltx_p" id="A5.T10.1.6.5.1.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.6.5.1.1.1.1">Sample (default, temp=0.3)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.6.5.2.1">
<span class="ltx_p" id="A5.T10.1.6.5.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.6.5.2.1.1.1">50%</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.7.6.1.1">
<span class="ltx_p" id="A5.T10.1.7.6.1.1.1" style="width:128.0pt;">Sample (default, temp=0.1)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.7.6.2.1">
<span class="ltx_p" id="A5.T10.1.7.6.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.8.7.1.1">
<span class="ltx_p" id="A5.T10.1.8.7.1.1.1" style="width:128.0pt;">Sample (default, temp=1.2)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.8.7.2.1">
<span class="ltx_p" id="A5.T10.1.8.7.2.1.1" style="width:71.1pt;">30%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.9.8.1.1">
<span class="ltx_p" id="A5.T10.1.9.8.1.1.1" style="width:128.0pt;">Sample (default, top_p=0.95, temp=0.3)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A5.T10.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.9.8.2.1">
<span class="ltx_p" id="A5.T10.1.9.8.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T10.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A5.T10.1.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.10.9.1.1">
<span class="ltx_p" id="A5.T10.1.10.9.1.1.1" style="width:128.0pt;">Sample (default, top_p=0.85, temp=0.3)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A5.T10.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="A5.T10.1.10.9.2.1">
<span class="ltx_p" id="A5.T10.1.10.9.2.1.1" style="width:71.1pt;">40%</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Accuracy of Different Sampling Methods on 10-sample of EFPL MCQA</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 09:38:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
