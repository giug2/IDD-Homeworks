<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data</title>
<!--Generated on Sun Oct 13 14:54:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.09865v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S1" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S2" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S3" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS1" title="In 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overall Pipeline for FER Data Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2" title="In 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Diffusion Model Training for FER Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS1" title="In 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>FEText Data Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS2" title="In 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Diffusion Model Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS3" title="In 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Explicit Control Signals via Facial Action Units</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS3" title="In 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Semantic Guidance for Precise Expression Control</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS4" title="In 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Diffusion-based Label Calibrator (FERAnno)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.SS1" title="In 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Generation Quality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.SS2" title="In 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Effectiveness of Synthetic Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.SS3" title="In 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S6" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1" title="In SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.SS1" title="In Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Hyper-parameter Studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.SS2" title="In Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Experiment Setting and Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.SS3" title="In Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Over-smoothing of Super Resolution Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.SS4" title="In Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Limitations and future work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.SS5" title="In Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>FEText</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xilin He<sup class="ltx_sup" id="id17.17.id1"><span class="ltx_text ltx_font_italic" id="id17.17.id1.1">1∗</span></sup>, Cheng Luo<sup class="ltx_sup" id="id18.18.id2"><span class="ltx_text ltx_font_italic" id="id18.18.id2.1">2∗</span></sup>, Xiaole Xian<sup class="ltx_sup" id="id19.19.id3">1</sup>, Bing Li<sup class="ltx_sup" id="id20.20.id4">3</sup>, Siyang Song<sup class="ltx_sup" id="id21.21.id5"><span class="ltx_text ltx_font_italic" id="id21.21.id5.1">4†</span></sup>, Muhammad Haris Khan<sup class="ltx_sup" id="id22.22.id6">5</sup>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id10.10.4">Weicheng Xie<sup class="ltx_sup" id="id10.10.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.4.1.1">1†</span></sup>, Linlin Shen<sup class="ltx_sup" id="id10.10.4.2"><span class="ltx_text ltx_font_medium" id="id10.10.4.2.1">1</span></sup>, Zongyuan Ge<sup class="ltx_sup" id="id10.10.4.3"><span class="ltx_text ltx_font_medium" id="id10.10.4.3.1">2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.4.4"><span class="ltx_text ltx_font_medium" id="id10.10.4.4.1">1</span></sup></span>Computer Vision Institute, School of Computer Science &amp; Software Engineering,
<br class="ltx_break"/>Shenzhen University   <sup class="ltx_sup" id="id23.23.id7">2</sup>AIM Lab, Faculty of IT, Monash University   <sup class="ltx_sup" id="id24.24.id8">3</sup>KAUST 
<br class="ltx_break"/>  <sup class="ltx_sup" id="id25.25.id9">4</sup>University of Exter   <sup class="ltx_sup" id="id26.26.id10">5</sup>MBZUAI
<br class="ltx_break"/><math alttext="*" class="ltx_Math" display="inline" id="id15.15.m11.1"><semantics id="id15.15.m11.1a"><mo id="id15.15.m11.1.1" xref="id15.15.m11.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id15.15.m11.1b"><times id="id15.15.m11.1.1.cmml" xref="id15.15.m11.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id15.15.m11.1c">*</annotation><annotation encoding="application/x-llamapun" id="id15.15.m11.1d">∗</annotation></semantics></math> Equal Contribution <sup class="ltx_sup" id="id27.27.id11">†</sup> Corresponding Author
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id28.id1">Facial expression datasets remain limited in scale due to privacy concerns, the subjectivity of annotations, and the labor-intensive nature of data collection. This limitation poses a significant challenge for developing modern deep learning-based facial expression analysis models, particularly foundation models, that rely on large-scale data for optimal performance. To tackle the overarching and complex challenge, we introduce SynFER (Synthesis of Facial Expressions with Refined Control), a novel framework for synthesizing facial expression image data based on high-level textual descriptions as well as more fine-grained and precise control through facial action units.
To ensure the quality and reliability of the synthetic data, we propose a semantic guidance technique to steer the generation process and a pseudo-label generator to help rectify the facial expression labels for the synthetic images.
To demonstrate the generation fidelity and the effectiveness of the synthetic data from SynFER, we conduct extensive experiments on representation learning using both synthetic data and real-world data. Experiment results validate the efficacy of the proposed approach and the synthetic data.
Notably, our approach achieves a 67.23% classification accuracy on AffectNet when training solely with synthetic data equivalent to the AffectNet training set size, which increases to 69.84% when scaling up to five times the original size.
Our code will be made publicly available.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Facial Expression Recognition (FER) is at the forefront of advancing AI’s ability to interpret human emotions, opening new frontiers for various human-centered applications. From automatic emotion detection to early interventions in mental health <cite class="ltx_cite ltx_citemacro_cite">Ringeval et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib66" title="">2019</a>)</cite>, accurate pain assessment <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib36" title="">2024</a>)</cite>, and enhancing human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">Abdat et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib1" title="">2011</a>)</cite>, the potential impact of FER systems is profound <cite class="ltx_cite ltx_citemacro_cite">Moin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib60" title="">2023</a>); Sajjad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib70" title="">2023</a>); Zhu &amp; Luo (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib95" title="">2023</a>)</cite>.
In recent years, learning-based FER models have gained significant traction due to their promising performances <cite class="ltx_cite ltx_citemacro_cite">Li &amp; Deng (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib45" title="">2020</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib89" title="">2021</a>); Farzaneh &amp; Qi (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib26" title="">2021</a>)</cite>. However, despite recent advancements in network architectures and learning methodologies, the progress of existing FER models has been hindered by the inadequate scale and quality of available training data, underscoring the need to expand datasets with high-quality data to push the boundaries of FER capabilities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Existing FER datasets, such as CK+ (953 sequences) <cite class="ltx_cite ltx_citemacro_cite">Lucey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib56" title="">2010</a>)</cite>, FER-2013 (30,000 48<math alttext="\times" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mo id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><times id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">×</annotation></semantics></math>48 images) <cite class="ltx_cite ltx_citemacro_cite">Barsoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib5" title="">2016</a>)</cite>, RAF-DB (29,672 images) <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib46" title="">2017</a>)</cite>, AFEW (113,355 images) <cite class="ltx_cite ltx_citemacro_cite">Dhall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib21" title="">2017</a>)</cite>, and SFEW (1,766 images) <cite class="ltx_cite ltx_citemacro_cite">Dhall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib20" title="">2011</a>)</cite>,
are small compared to popular image datasets for general image processing (e.g., ImageNet <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib19" title="">2009</a>)</cite> with 1.4 million images and Laion <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib71" title="">2022</a>)</cite> with billion-level data).
While AffectNet <cite class="ltx_cite ltx_citemacro_cite">Mollahosseini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib61" title="">2017</a>)</cite> compiles a large number of facial images from the web,
it still suffers from vital drawbacks. A considerable portion of AffectNet’s images are low-quality, and its annotations often contain incorrect labels, which impairs the training process of FER models <cite class="ltx_cite ltx_citemacro_cite">Le et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib40" title="">2023</a>); Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib82" title="">2022</a>)</cite>.
Consequently, the absence of high-quality and large-scale FER datasets has delayed the development of FER foundation models. However, collecting a large-scale FER dataset with high-quality facial images and meticulous annotations is almost an unrealistic endeavor due to substantial financial and time costs, ethical concerns around facial data collection, and limited resources for large-scale acquisition. Additionally, the subjective interpretation of facial expressions results in inconsistent labeling by annotators, which exacerbates variability and hinders the creation of reliable datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the challenges in developing FER models, we turn to synthesizing high-quality facial expression images paired with reliable labels. This approach draws inspiration from successful strategies employed to expand annotated datasets for other computer vision tasks, such as semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">Baranchuk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib4" title="">2021</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib15" title="">2019</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib42" title="">2022a</a>)</cite> and depth estimation <cite class="ltx_cite ltx_citemacro_cite">Atapour-Abarghouei &amp; Breckon (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib2" title="">2018</a>); Cheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib16" title="">2020</a>); Guizilini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib32" title="">2022</a>)</cite>.
These advances leverage powerful generative models such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">Rombach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib67" title="">2022a</a>)</cite> and DALL-E <cite class="ltx_cite ltx_citemacro_cite">Betker et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib6" title="">2023</a>)</cite>, which capture intricate natural image patterns. By tapping into these models, researchers have generated realistic images with their corresponding annotations, thereby boosting model performance.
However, applying diffusion models to synthesize facial expression images with reliable FER labels presents two major challenges.
(1) the training sets used by these generative models often lack diverse facial expression data, limiting their ability to produce images that capture subtle and nuanced emotional semantics;
and (2) prior approaches to generate annotations for synthetic images focused on tangible attributes such as pixel-wise layouts, or depth maps. In contrast, facial expressions convey abstract and subjective emotions, making the generation of precise and reliable expression labels much more complex.
To the best of our knowledge, none of the existing methods can simultaneously conduct fine-grained control for facial expression generation and generate robust categorical facial expression labels for face images.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) Examples of synthetic facial expression data generated by our SynFER model, (b) Comparison of training paradigms: training with real-world data versus training with synthetic facial expression data and (c) Performance boost from SynFER generated Data in supervised, self-supervised, zero-shot, and few-shot (5-shot) learning tasks.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we present SynFER, the first framework capable of synthesizing unlimited, diverse and realistic facial expression images paired with reliable expression labels, to drive advancements in FER models.
To address the shortcomings of existing FER datasets, which often lack expression-related text paired with facial images, we introduce FEText, a unique hybrid dataset created by curating and filtering data from existing FER and high-quality face datasets. This vision-language dataset serves as the foundation for training our generative model to synthesize facial expression data.
To ensure fine-grained control and faithful generation of facial expression images, we inject facial action unit (FAU) information and semantic guidance from external pre-trained FER models. Building upon this, we propose FERAnno, the first diffusion-based label calibrator for FER, which automatically generates reliable annotations for the synthesized images.
Together, these innovations position SynFER as a powerful tool for producing large-scale, high-quality facial expression data, offering a significant resource for the development of FER models.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We investigate the effectiveness of the synthetic data across various learning paradigms, demonstrating consistent and modest improvement in model performance.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>(c), training with the synthetic data yields significant performance boosts across various learning paradigms.
Notably, pre-training on the synthetic data (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>(b)) with MoCo v3 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib14" title="">2021</a>)</cite> yields a significant performance boost of +2.90% on AffectNet, surpassing real-world data pre-training. In supervised learning, SynFER improves accuracy by +1.55% for the state-of-the-art FER model, POSTER++ <cite class="ltx_cite ltx_citemacro_cite">Mao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib59" title="">2024</a>)</cite>, on AffectNet.
We further explore performance scaling of the synthetic data, revealing further gains as dataset size increases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our key contributions are as follows: (1) we introduce FEText, the first dataset of facial expression-related image-text pairs, providing a crucial resource for advancing FER tasks. (2) we propose SynFER, the first diffusion-based data synthesis pipeline for FER, integrating FAUs information and semantic guidance to achieve fine-grained control and faithful expression generation. (3) we develop FERAnno, a novel diffusion-based label calibrator, designed to automatically refine and enhance the annotations of synthesized facial expression images. (4) extensive experiments across various datasets and learning paradigms demonstrate the effectiveness of the proposed SynFER, validating the quality and scalability of its synthesized FER data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Facial Expression Recognition (FER):</span>
Recent success in deep learning (DL) has largely boosted the performance of the FER task, despite the substantial data requirements for training DL models. To address the limited training data in FER, previous methods mainly focus on developing different learning paradigms, including semi-supervised learning <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib44" title="">2022b</a>); Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib85" title="">2023</a>); Cho et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib17" title="">2024</a>)</cite>, transfer learning <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib48" title="">2022c</a>); Ruan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib69" title="">2022</a>)</cite> and multi-task learning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib54" title="">2023b</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib47" title="">2023a</a>)</cite>.
For example, Ada-CM <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib44" title="">2022b</a>)</cite> learns a confidence margin to make full use of the unlabeled facial expression data in a semi-supervised manner.
Despite achieving performance gains for FER, these methods remain constrained by limited data. Recently, researchers have explored an alternative data-driven perspective of introducing large-scale face datasets from other facial analysis tasks (e.g., face recognition <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib87" title="">2022</a>)</cite>). Meta-Face2Exp <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib87" title="">2022</a>)</cite> utilizes large-scale face recognition data to enhance FER by matching the feature distribution between face recognition and FER. However, face data drawn from these datasets lack diverse facial expressions, and thereby couldn’t fully unlock the potential of large-scale data in FER.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Synthetic Data:</span>
Recently, growing attention has been paid to the advanced generative models (e.g., Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">Goodfellow et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib30" title="">2020</a>)</cite> and Diffusion Models <cite class="ltx_cite ltx_citemacro_cite">Rombach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib68" title="">2022b</a>)</cite>), which are typically flexible to synthesize training images for a wider range of downstream tasks, including classification <cite class="ltx_cite ltx_citemacro_cite">Frid-Adar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib28" title="">2018</a>); Azizi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib3" title="">2023</a>)</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib39" title="">2023</a>); Boutros et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib7" title="">2023</a>)</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib63" title="">2023</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib79" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib78" title="">2023a</a>)</cite> and human pose estimation <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib27" title="">2023</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib94" title="">2023</a>)</cite>. In particular, some studies pioneer to investigate the capabilities of powerful pre-trained diffusion generative models on natural images <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib63" title="">2023</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib79" title="">2024</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib49" title="">2023b</a>)</cite>.
For example, DatasetDM <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib79" title="">2024</a>)</cite> further introduces a generalized perception decoder to parse the rich latent space of the pre-trained diffusion model for various downstream tasks.
However, there remains a significant gap in research on using diffusion models to generate facial expression data, including both images and corresponding labels.
In this paper, we address this gap by exploring diffusion-based synthetic data for the first time in FER.
Specifically, we propose to apply a fine-tuned diffusion model to facial expression synthesis and introduce the first diffusion-based pseudo-label generator for FER.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminaries</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.6">Diffusion models include a forward process that adds Gaussian noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_ϵ</annotation></semantics></math> to convert a clean sample <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">x</mi><mn id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝑥</ci><cn id="S3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> to noise sample <math alttext="x_{T}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">x</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑥</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">x_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>, and a backward process that iteratively performs denoising from <math alttext="x_{T}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">x</mi><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝑥</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">x_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><msub id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">x</mi><mn id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">𝑥</ci><cn id="S3.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="T" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_T</annotation></semantics></math> represents the total number of timesteps. The forward process of injecting noise can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">x</mi><mi id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><msqrt id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><msub id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.2.cmml">α</mi><mi id="S3.E1.m1.1.1.3.2.2.2.3" xref="S3.E1.m1.1.1.3.2.2.2.3.cmml">t</mi></msub></msqrt><mo id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">x</mi><mn id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><msqrt id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mrow id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml"><mn id="S3.E1.m1.1.1.3.3.2.2.2" xref="S3.E1.m1.1.1.3.3.2.2.2.cmml">1</mn><mo id="S3.E1.m1.1.1.3.3.2.2.1" xref="S3.E1.m1.1.1.3.3.2.2.1.cmml">−</mo><msub id="S3.E1.m1.1.1.3.3.2.2.3" xref="S3.E1.m1.1.1.3.3.2.2.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2.3.2" xref="S3.E1.m1.1.1.3.3.2.2.3.2.cmml">α</mi><mi id="S3.E1.m1.1.1.3.3.2.2.3.3" xref="S3.E1.m1.1.1.3.3.2.2.3.3.cmml">t</mi></msub></mrow></msqrt><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">ϵ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><plus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><times id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"></times><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><root id="S3.E1.m1.1.1.3.2.2a.cmml" xref="S3.E1.m1.1.1.3.2.2"></root><apply id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2.2">𝛼</ci><ci id="S3.E1.m1.1.1.3.2.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.2.3">𝑡</ci></apply></apply><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝑥</ci><cn id="S3.E1.m1.1.1.3.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.3.3">0</cn></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><root id="S3.E1.m1.1.1.3.3.2a.cmml" xref="S3.E1.m1.1.1.3.3.2"></root><apply id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2"><minus id="S3.E1.m1.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.2.1"></minus><cn id="S3.E1.m1.1.1.3.3.2.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.2.2.2">1</cn><apply id="S3.E1.m1.1.1.3.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.3.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2.3.2">𝛼</ci><ci id="S3.E1.m1.1.1.3.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.3.3.2.2.3.3">𝑡</ci></apply></apply></apply><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">x_{t}=\sqrt{\alpha_{t}}x_{0}+\sqrt{1-\alpha_{t}}\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG italic_ϵ</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.15"><math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.p1.7.m1.1"><semantics id="S3.p1.7.m1.1a"><msub id="S3.p1.7.m1.1.1" xref="S3.p1.7.m1.1.1.cmml"><mi id="S3.p1.7.m1.1.1.2" xref="S3.p1.7.m1.1.1.2.cmml">x</mi><mi id="S3.p1.7.m1.1.1.3" xref="S3.p1.7.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.7.m1.1b"><apply id="S3.p1.7.m1.1.1.cmml" xref="S3.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m1.1.1.1.cmml" xref="S3.p1.7.m1.1.1">subscript</csymbol><ci id="S3.p1.7.m1.1.1.2.cmml" xref="S3.p1.7.m1.1.1.2">𝑥</ci><ci id="S3.p1.7.m1.1.1.3.cmml" xref="S3.p1.7.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m1.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m1.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the noise feature at timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.p1.8.m2.1"><semantics id="S3.p1.8.m2.1a"><mi id="S3.p1.8.m2.1.1" xref="S3.p1.8.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m2.1b"><ci id="S3.p1.8.m2.1.1.cmml" xref="S3.p1.8.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m2.1d">italic_t</annotation></semantics></math> and <math alttext="\alpha_{t}" class="ltx_Math" display="inline" id="S3.p1.9.m3.1"><semantics id="S3.p1.9.m3.1a"><msub id="S3.p1.9.m3.1.1" xref="S3.p1.9.m3.1.1.cmml"><mi id="S3.p1.9.m3.1.1.2" xref="S3.p1.9.m3.1.1.2.cmml">α</mi><mi id="S3.p1.9.m3.1.1.3" xref="S3.p1.9.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.9.m3.1b"><apply id="S3.p1.9.m3.1.1.cmml" xref="S3.p1.9.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m3.1.1.1.cmml" xref="S3.p1.9.m3.1.1">subscript</csymbol><ci id="S3.p1.9.m3.1.1.2.cmml" xref="S3.p1.9.m3.1.1.2">𝛼</ci><ci id="S3.p1.9.m3.1.1.3.cmml" xref="S3.p1.9.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m3.1c">\alpha_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m3.1d">italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is a predetermined hyperparameter for sampling <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.p1.10.m4.1"><semantics id="S3.p1.10.m4.1a"><msub id="S3.p1.10.m4.1.1" xref="S3.p1.10.m4.1.1.cmml"><mi id="S3.p1.10.m4.1.1.2" xref="S3.p1.10.m4.1.1.2.cmml">x</mi><mi id="S3.p1.10.m4.1.1.3" xref="S3.p1.10.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.10.m4.1b"><apply id="S3.p1.10.m4.1.1.cmml" xref="S3.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.10.m4.1.1.1.cmml" xref="S3.p1.10.m4.1.1">subscript</csymbol><ci id="S3.p1.10.m4.1.1.2.cmml" xref="S3.p1.10.m4.1.1.2">𝑥</ci><ci id="S3.p1.10.m4.1.1.3.cmml" xref="S3.p1.10.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m4.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m4.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with a given noise scheduler <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib72" title="">2020</a>)</cite>.
In the backward process of denoising, given input noise <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.p1.11.m5.1"><semantics id="S3.p1.11.m5.1a"><msub id="S3.p1.11.m5.1.1" xref="S3.p1.11.m5.1.1.cmml"><mi id="S3.p1.11.m5.1.1.2" xref="S3.p1.11.m5.1.1.2.cmml">x</mi><mi id="S3.p1.11.m5.1.1.3" xref="S3.p1.11.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.11.m5.1b"><apply id="S3.p1.11.m5.1.1.cmml" xref="S3.p1.11.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m5.1.1.1.cmml" xref="S3.p1.11.m5.1.1">subscript</csymbol><ci id="S3.p1.11.m5.1.1.2.cmml" xref="S3.p1.11.m5.1.1.2">𝑥</ci><ci id="S3.p1.11.m5.1.1.3.cmml" xref="S3.p1.11.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m5.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m5.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> sampled from a Gaussian distribution, a learnable network <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.p1.12.m6.1"><semantics id="S3.p1.12.m6.1a"><msub id="S3.p1.12.m6.1.1" xref="S3.p1.12.m6.1.1.cmml"><mi id="S3.p1.12.m6.1.1.2" xref="S3.p1.12.m6.1.1.2.cmml">ϵ</mi><mi id="S3.p1.12.m6.1.1.3" xref="S3.p1.12.m6.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.12.m6.1b"><apply id="S3.p1.12.m6.1.1.cmml" xref="S3.p1.12.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.12.m6.1.1.1.cmml" xref="S3.p1.12.m6.1.1">subscript</csymbol><ci id="S3.p1.12.m6.1.1.2.cmml" xref="S3.p1.12.m6.1.1.2">italic-ϵ</ci><ci id="S3.p1.12.m6.1.1.3.cmml" xref="S3.p1.12.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m6.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m6.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> estimates the noise at each timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.p1.13.m7.1"><semantics id="S3.p1.13.m7.1a"><mi id="S3.p1.13.m7.1.1" xref="S3.p1.13.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.13.m7.1b"><ci id="S3.p1.13.m7.1.1.cmml" xref="S3.p1.13.m7.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m7.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p1.13.m7.1d">italic_t</annotation></semantics></math> with condition <math alttext="c" class="ltx_Math" display="inline" id="S3.p1.14.m8.1"><semantics id="S3.p1.14.m8.1a"><mi id="S3.p1.14.m8.1.1" xref="S3.p1.14.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p1.14.m8.1b"><ci id="S3.p1.14.m8.1.1.cmml" xref="S3.p1.14.m8.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.14.m8.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p1.14.m8.1d">italic_c</annotation></semantics></math>. <math alttext="x_{t-1}" class="ltx_Math" display="inline" id="S3.p1.15.m9.1"><semantics id="S3.p1.15.m9.1a"><msub id="S3.p1.15.m9.1.1" xref="S3.p1.15.m9.1.1.cmml"><mi id="S3.p1.15.m9.1.1.2" xref="S3.p1.15.m9.1.1.2.cmml">x</mi><mrow id="S3.p1.15.m9.1.1.3" xref="S3.p1.15.m9.1.1.3.cmml"><mi id="S3.p1.15.m9.1.1.3.2" xref="S3.p1.15.m9.1.1.3.2.cmml">t</mi><mo id="S3.p1.15.m9.1.1.3.1" xref="S3.p1.15.m9.1.1.3.1.cmml">−</mo><mn id="S3.p1.15.m9.1.1.3.3" xref="S3.p1.15.m9.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.15.m9.1b"><apply id="S3.p1.15.m9.1.1.cmml" xref="S3.p1.15.m9.1.1"><csymbol cd="ambiguous" id="S3.p1.15.m9.1.1.1.cmml" xref="S3.p1.15.m9.1.1">subscript</csymbol><ci id="S3.p1.15.m9.1.1.2.cmml" xref="S3.p1.15.m9.1.1.2">𝑥</ci><apply id="S3.p1.15.m9.1.1.3.cmml" xref="S3.p1.15.m9.1.1.3"><minus id="S3.p1.15.m9.1.1.3.1.cmml" xref="S3.p1.15.m9.1.1.3.1"></minus><ci id="S3.p1.15.m9.1.1.3.2.cmml" xref="S3.p1.15.m9.1.1.3.2">𝑡</ci><cn id="S3.p1.15.m9.1.1.3.3.cmml" type="integer" xref="S3.p1.15.m9.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.15.m9.1c">x_{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.15.m9.1d">italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the feature at the previous timestep, is then derived as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{t-1}=\frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_{t}}}x_{t}+\sqrt{\alpha_{t-1}}%
(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_{t}}-1})\epsilon_{%
\theta}(x_{t},t,c)" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msub id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.2" xref="S3.E2.m1.4.4.4.2.cmml">x</mi><mrow id="S3.E2.m1.4.4.4.3" xref="S3.E2.m1.4.4.4.3.cmml"><mi id="S3.E2.m1.4.4.4.3.2" xref="S3.E2.m1.4.4.4.3.2.cmml">t</mi><mo id="S3.E2.m1.4.4.4.3.1" xref="S3.E2.m1.4.4.4.3.1.cmml">−</mo><mn id="S3.E2.m1.4.4.4.3.3" xref="S3.E2.m1.4.4.4.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><mrow id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.4.cmml"><mfrac id="S3.E2.m1.4.4.2.4.2" xref="S3.E2.m1.4.4.2.4.2.cmml"><msqrt id="S3.E2.m1.4.4.2.4.2.2" xref="S3.E2.m1.4.4.2.4.2.2.cmml"><msub id="S3.E2.m1.4.4.2.4.2.2.2" xref="S3.E2.m1.4.4.2.4.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.4.2.2.2.2" xref="S3.E2.m1.4.4.2.4.2.2.2.2.cmml">α</mi><mrow id="S3.E2.m1.4.4.2.4.2.2.2.3" xref="S3.E2.m1.4.4.2.4.2.2.2.3.cmml"><mi id="S3.E2.m1.4.4.2.4.2.2.2.3.2" xref="S3.E2.m1.4.4.2.4.2.2.2.3.2.cmml">t</mi><mo id="S3.E2.m1.4.4.2.4.2.2.2.3.1" xref="S3.E2.m1.4.4.2.4.2.2.2.3.1.cmml">−</mo><mn id="S3.E2.m1.4.4.2.4.2.2.2.3.3" xref="S3.E2.m1.4.4.2.4.2.2.2.3.3.cmml">1</mn></mrow></msub></msqrt><msqrt id="S3.E2.m1.4.4.2.4.2.3" xref="S3.E2.m1.4.4.2.4.2.3.cmml"><msub id="S3.E2.m1.4.4.2.4.2.3.2" xref="S3.E2.m1.4.4.2.4.2.3.2.cmml"><mi id="S3.E2.m1.4.4.2.4.2.3.2.2" xref="S3.E2.m1.4.4.2.4.2.3.2.2.cmml">α</mi><mi id="S3.E2.m1.4.4.2.4.2.3.2.3" xref="S3.E2.m1.4.4.2.4.2.3.2.3.cmml">t</mi></msub></msqrt></mfrac><mo id="S3.E2.m1.4.4.2.4.1" xref="S3.E2.m1.4.4.2.4.1.cmml">⁢</mo><msub id="S3.E2.m1.4.4.2.4.3" xref="S3.E2.m1.4.4.2.4.3.cmml"><mi id="S3.E2.m1.4.4.2.4.3.2" xref="S3.E2.m1.4.4.2.4.3.2.cmml">x</mi><mi id="S3.E2.m1.4.4.2.4.3.3" xref="S3.E2.m1.4.4.2.4.3.3.cmml">t</mi></msub></mrow><mo id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml">+</mo><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml"><msqrt id="S3.E2.m1.4.4.2.2.4" xref="S3.E2.m1.4.4.2.2.4.cmml"><msub id="S3.E2.m1.4.4.2.2.4.2" xref="S3.E2.m1.4.4.2.2.4.2.cmml"><mi id="S3.E2.m1.4.4.2.2.4.2.2" xref="S3.E2.m1.4.4.2.2.4.2.2.cmml">α</mi><mrow id="S3.E2.m1.4.4.2.2.4.2.3" xref="S3.E2.m1.4.4.2.2.4.2.3.cmml"><mi id="S3.E2.m1.4.4.2.2.4.2.3.2" xref="S3.E2.m1.4.4.2.2.4.2.3.2.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.4.2.3.1" xref="S3.E2.m1.4.4.2.2.4.2.3.1.cmml">−</mo><mn id="S3.E2.m1.4.4.2.2.4.2.3.3" xref="S3.E2.m1.4.4.2.2.4.2.3.3.cmml">1</mn></mrow></msub></msqrt><mo id="S3.E2.m1.4.4.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml"><msqrt id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.cmml"><mfrac id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml"><mn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.2.cmml">1</mn><msub id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.2.cmml">α</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.2.cmml">t</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.1.cmml">−</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></msub></mfrac><mo id="S3.E2.m1.3.3.1.1.1.1.1.2.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.1.cmml">−</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml">1</mn></mrow></msqrt><mo id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">−</mo><msqrt id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml"><mfrac id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml"><mn id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.2.cmml">1</mn><msub id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.2.cmml">α</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.3.cmml">t</mi></msub></mfrac><mo id="S3.E2.m1.3.3.1.1.1.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.1.cmml">−</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.3.cmml">1</mn></mrow></msqrt></mrow><mo id="S3.E2.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E2.m1.4.4.2.2.3a" xref="S3.E2.m1.4.4.2.2.3.cmml">⁢</mo><msub id="S3.E2.m1.4.4.2.2.5" xref="S3.E2.m1.4.4.2.2.5.cmml"><mi id="S3.E2.m1.4.4.2.2.5.2" xref="S3.E2.m1.4.4.2.2.5.2.cmml">ϵ</mi><mi id="S3.E2.m1.4.4.2.2.5.3" xref="S3.E2.m1.4.4.2.2.5.3.cmml">θ</mi></msub><mo id="S3.E2.m1.4.4.2.2.3b" xref="S3.E2.m1.4.4.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.2.2.2.1" xref="S3.E2.m1.4.4.2.2.2.2.cmml"><mo id="S3.E2.m1.4.4.2.2.2.1.2" stretchy="false" xref="S3.E2.m1.4.4.2.2.2.2.cmml">(</mo><msub id="S3.E2.m1.4.4.2.2.2.1.1" xref="S3.E2.m1.4.4.2.2.2.1.1.cmml"><mi id="S3.E2.m1.4.4.2.2.2.1.1.2" xref="S3.E2.m1.4.4.2.2.2.1.1.2.cmml">x</mi><mi id="S3.E2.m1.4.4.2.2.2.1.1.3" xref="S3.E2.m1.4.4.2.2.2.1.1.3.cmml">t</mi></msub><mo id="S3.E2.m1.4.4.2.2.2.1.3" xref="S3.E2.m1.4.4.2.2.2.2.cmml">,</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">t</mi><mo id="S3.E2.m1.4.4.2.2.2.1.4" xref="S3.E2.m1.4.4.2.2.2.2.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">c</mi><mo id="S3.E2.m1.4.4.2.2.2.1.5" stretchy="false" xref="S3.E2.m1.4.4.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"></eq><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.2">𝑥</ci><apply id="S3.E2.m1.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.3"><minus id="S3.E2.m1.4.4.4.3.1.cmml" xref="S3.E2.m1.4.4.4.3.1"></minus><ci id="S3.E2.m1.4.4.4.3.2.cmml" xref="S3.E2.m1.4.4.4.3.2">𝑡</ci><cn id="S3.E2.m1.4.4.4.3.3.cmml" type="integer" xref="S3.E2.m1.4.4.4.3.3">1</cn></apply></apply><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><plus id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"></plus><apply id="S3.E2.m1.4.4.2.4.cmml" xref="S3.E2.m1.4.4.2.4"><times id="S3.E2.m1.4.4.2.4.1.cmml" xref="S3.E2.m1.4.4.2.4.1"></times><apply id="S3.E2.m1.4.4.2.4.2.cmml" xref="S3.E2.m1.4.4.2.4.2"><divide id="S3.E2.m1.4.4.2.4.2.1.cmml" xref="S3.E2.m1.4.4.2.4.2"></divide><apply id="S3.E2.m1.4.4.2.4.2.2.cmml" xref="S3.E2.m1.4.4.2.4.2.2"><root id="S3.E2.m1.4.4.2.4.2.2a.cmml" xref="S3.E2.m1.4.4.2.4.2.2"></root><apply id="S3.E2.m1.4.4.2.4.2.2.2.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.4.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2.2">𝛼</ci><apply id="S3.E2.m1.4.4.2.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2.3"><minus id="S3.E2.m1.4.4.2.4.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2.3.1"></minus><ci id="S3.E2.m1.4.4.2.4.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.4.2.2.2.3.2">𝑡</ci><cn id="S3.E2.m1.4.4.2.4.2.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.4.4.2.4.2.2.2.3.3">1</cn></apply></apply></apply><apply id="S3.E2.m1.4.4.2.4.2.3.cmml" xref="S3.E2.m1.4.4.2.4.2.3"><root id="S3.E2.m1.4.4.2.4.2.3a.cmml" xref="S3.E2.m1.4.4.2.4.2.3"></root><apply id="S3.E2.m1.4.4.2.4.2.3.2.cmml" xref="S3.E2.m1.4.4.2.4.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.4.2.3.2.1.cmml" xref="S3.E2.m1.4.4.2.4.2.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.4.2.3.2.2.cmml" xref="S3.E2.m1.4.4.2.4.2.3.2.2">𝛼</ci><ci id="S3.E2.m1.4.4.2.4.2.3.2.3.cmml" xref="S3.E2.m1.4.4.2.4.2.3.2.3">𝑡</ci></apply></apply></apply><apply id="S3.E2.m1.4.4.2.4.3.cmml" xref="S3.E2.m1.4.4.2.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.4.3.1.cmml" xref="S3.E2.m1.4.4.2.4.3">subscript</csymbol><ci id="S3.E2.m1.4.4.2.4.3.2.cmml" xref="S3.E2.m1.4.4.2.4.3.2">𝑥</ci><ci id="S3.E2.m1.4.4.2.4.3.3.cmml" xref="S3.E2.m1.4.4.2.4.3.3">𝑡</ci></apply></apply><apply id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2"><times id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3"></times><apply id="S3.E2.m1.4.4.2.2.4.cmml" xref="S3.E2.m1.4.4.2.2.4"><root id="S3.E2.m1.4.4.2.2.4a.cmml" xref="S3.E2.m1.4.4.2.2.4"></root><apply id="S3.E2.m1.4.4.2.2.4.2.cmml" xref="S3.E2.m1.4.4.2.2.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.4.2.1.cmml" xref="S3.E2.m1.4.4.2.2.4.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2.4.2.2">𝛼</ci><apply id="S3.E2.m1.4.4.2.2.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2.4.2.3"><minus id="S3.E2.m1.4.4.2.2.4.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.4.2.3.1"></minus><ci id="S3.E2.m1.4.4.2.2.4.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.4.2.3.2">𝑡</ci><cn id="S3.E2.m1.4.4.2.2.4.2.3.3.cmml" type="integer" xref="S3.E2.m1.4.4.2.2.4.2.3.3">1</cn></apply></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"><root id="S3.E2.m1.3.3.1.1.1.1.1.2a.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2"></root><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2"><minus id="S3.E2.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2"><divide id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2"></divide><cn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.2">1</cn><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.2">𝛼</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3"><minus id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.1"></minus><ci id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.2">𝑡</ci><cn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply><cn id="S3.E2.m1.3.3.1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.2.2.3">1</cn></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3"><root id="S3.E2.m1.3.3.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3"></root><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2"><minus id="S3.E2.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.1"></minus><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2"><divide id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2"></divide><cn id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.2.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.2">1</cn><apply id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.2">𝛼</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.2.3.3">𝑡</ci></apply></apply><cn id="S3.E2.m1.3.3.1.1.1.1.1.3.2.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.1.1.1.3.2.3">1</cn></apply></apply></apply><apply id="S3.E2.m1.4.4.2.2.5.cmml" xref="S3.E2.m1.4.4.2.2.5"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.5.1.cmml" xref="S3.E2.m1.4.4.2.2.5">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.5.2.cmml" xref="S3.E2.m1.4.4.2.2.5.2">italic-ϵ</ci><ci id="S3.E2.m1.4.4.2.2.5.3.cmml" xref="S3.E2.m1.4.4.2.2.5.3">𝜃</ci></apply><vector id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.1"><apply id="S3.E2.m1.4.4.2.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.1.1.2.cmml" xref="S3.E2.m1.4.4.2.2.2.1.1.2">𝑥</ci><ci id="S3.E2.m1.4.4.2.2.2.1.1.3.cmml" xref="S3.E2.m1.4.4.2.2.2.1.1.3">𝑡</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑡</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑐</ci></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">x_{t-1}=\frac{\sqrt{\alpha_{t-1}}}{\sqrt{\alpha_{t}}}x_{t}+\sqrt{\alpha_{t-1}}%
(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_{t}}-1})\epsilon_{%
\theta}(x_{t},t,c)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = divide start_ARG square-root start_ARG italic_α start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG end_ARG start_ARG square-root start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + square-root start_ARG italic_α start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG ( square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG - 1 end_ARG - square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG - 1 end_ARG ) italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.17">During training, the noise estimation network <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.p1.16.m1.1"><semantics id="S3.p1.16.m1.1a"><msub id="S3.p1.16.m1.1.1" xref="S3.p1.16.m1.1.1.cmml"><mi id="S3.p1.16.m1.1.1.2" xref="S3.p1.16.m1.1.1.2.cmml">ϵ</mi><mi id="S3.p1.16.m1.1.1.3" xref="S3.p1.16.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.16.m1.1b"><apply id="S3.p1.16.m1.1.1.cmml" xref="S3.p1.16.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.16.m1.1.1.1.cmml" xref="S3.p1.16.m1.1.1">subscript</csymbol><ci id="S3.p1.16.m1.1.1.2.cmml" xref="S3.p1.16.m1.1.1.2">italic-ϵ</ci><ci id="S3.p1.16.m1.1.1.3.cmml" xref="S3.p1.16.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.16.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.16.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is guided to conduct denoising with condition <math alttext="c" class="ltx_Math" display="inline" id="S3.p1.17.m2.1"><semantics id="S3.p1.17.m2.1a"><mi id="S3.p1.17.m2.1.1" xref="S3.p1.17.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p1.17.m2.1b"><ci id="S3.p1.17.m2.1.1.cmml" xref="S3.p1.17.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.17.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p1.17.m2.1d">italic_c</annotation></semantics></math> by the learning objective:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min_{\theta}\mathbb{E}_{x_{0},\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),%
c,t}\|\epsilon-\epsilon_{\theta}(x_{t},c,t)\|_{2}^{2}," class="ltx_Math" display="block" id="S3.E3.m1.10"><semantics id="S3.E3.m1.10a"><mrow id="S3.E3.m1.10.10.1" xref="S3.E3.m1.10.10.1.1.cmml"><mrow id="S3.E3.m1.10.10.1.1" xref="S3.E3.m1.10.10.1.1.cmml"><munder id="S3.E3.m1.10.10.1.1.2" xref="S3.E3.m1.10.10.1.1.2.cmml"><mi id="S3.E3.m1.10.10.1.1.2.2" xref="S3.E3.m1.10.10.1.1.2.2.cmml">min</mi><mi id="S3.E3.m1.10.10.1.1.2.3" xref="S3.E3.m1.10.10.1.1.2.3.cmml">θ</mi></munder><mo id="S3.E3.m1.10.10.1.1a" lspace="0.167em" xref="S3.E3.m1.10.10.1.1.cmml">⁡</mo><mrow id="S3.E3.m1.10.10.1.1.1" xref="S3.E3.m1.10.10.1.1.1.cmml"><msub id="S3.E3.m1.10.10.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.3.cmml"><mi id="S3.E3.m1.10.10.1.1.1.3.2" xref="S3.E3.m1.10.10.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E3.m1.7.7.7.7" xref="S3.E3.m1.7.7.7.8.cmml"><mrow id="S3.E3.m1.6.6.6.6.1" xref="S3.E3.m1.6.6.6.6.1.cmml"><mrow id="S3.E3.m1.6.6.6.6.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml"><msub id="S3.E3.m1.6.6.6.6.1.1.1.1" xref="S3.E3.m1.6.6.6.6.1.1.1.1.cmml"><mi id="S3.E3.m1.6.6.6.6.1.1.1.1.2" xref="S3.E3.m1.6.6.6.6.1.1.1.1.2.cmml">x</mi><mn id="S3.E3.m1.6.6.6.6.1.1.1.1.3" xref="S3.E3.m1.6.6.6.6.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.E3.m1.6.6.6.6.1.1.1.2" xref="S3.E3.m1.6.6.6.6.1.1.2.cmml">,</mo><mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">ϵ</mi></mrow><mo id="S3.E3.m1.6.6.6.6.1.2" xref="S3.E3.m1.6.6.6.6.1.2.cmml">∼</mo><mrow id="S3.E3.m1.6.6.6.6.1.3" xref="S3.E3.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.6.6.6.6.1.3.2" xref="S3.E3.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo id="S3.E3.m1.6.6.6.6.1.3.1" xref="S3.E3.m1.6.6.6.6.1.3.1.cmml">⁢</mo><mrow id="S3.E3.m1.6.6.6.6.1.3.3.2" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml"><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">𝟎</mn><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.2" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">𝐈</mi><mo id="S3.E3.m1.6.6.6.6.1.3.3.2.3" stretchy="false" xref="S3.E3.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.7.7.7.7.3" xref="S3.E3.m1.7.7.7.8a.cmml">,</mo><mrow id="S3.E3.m1.7.7.7.7.2.2" xref="S3.E3.m1.7.7.7.7.2.1.cmml"><mi id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">c</mi><mo id="S3.E3.m1.7.7.7.7.2.2.1" xref="S3.E3.m1.7.7.7.7.2.1.cmml">,</mo><mi id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.5.cmml">t</mi></mrow></mrow></msub><mo id="S3.E3.m1.10.10.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.2.cmml">⁢</mo><msubsup id="S3.E3.m1.10.10.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.cmml"><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.3.cmml">ϵ</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.2.cmml">ϵ</mi><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.3.cmml">θ</mi></msub><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.8.8" xref="S3.E3.m1.8.8.cmml">c</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.9.9" xref="S3.E3.m1.9.9.cmml">t</mi><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.5" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.10.10.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.10.10.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E3.m1.10.10.1.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E3.m1.10.10.1.1.1.1.3" xref="S3.E3.m1.10.10.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S3.E3.m1.10.10.1.2" xref="S3.E3.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.10b"><apply id="S3.E3.m1.10.10.1.1.cmml" xref="S3.E3.m1.10.10.1"><apply id="S3.E3.m1.10.10.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.2.1.cmml" xref="S3.E3.m1.10.10.1.1.2">subscript</csymbol><min id="S3.E3.m1.10.10.1.1.2.2.cmml" xref="S3.E3.m1.10.10.1.1.2.2"></min><ci id="S3.E3.m1.10.10.1.1.2.3.cmml" xref="S3.E3.m1.10.10.1.1.2.3">𝜃</ci></apply><apply id="S3.E3.m1.10.10.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1"><times id="S3.E3.m1.10.10.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.2"></times><apply id="S3.E3.m1.10.10.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.3.1.cmml" xref="S3.E3.m1.10.10.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.3.2.cmml" xref="S3.E3.m1.10.10.1.1.1.3.2">𝔼</ci><apply id="S3.E3.m1.7.7.7.8.cmml" xref="S3.E3.m1.7.7.7.7"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.7.8a.cmml" xref="S3.E3.m1.7.7.7.7.3">formulae-sequence</csymbol><apply id="S3.E3.m1.6.6.6.6.1.cmml" xref="S3.E3.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E3.m1.6.6.6.6.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.2">similar-to</csymbol><list id="S3.E3.m1.6.6.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1"><apply id="S3.E3.m1.6.6.6.6.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.6.6.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.6.6.6.6.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.6.6.1.1.1.1.2">𝑥</ci><cn id="S3.E3.m1.6.6.6.6.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.6.6.6.6.1.1.1.1.3">0</cn></apply><ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">italic-ϵ</ci></list><apply id="S3.E3.m1.6.6.6.6.1.3.cmml" xref="S3.E3.m1.6.6.6.6.1.3"><times id="S3.E3.m1.6.6.6.6.1.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.3.1"></times><ci id="S3.E3.m1.6.6.6.6.1.3.2.cmml" xref="S3.E3.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S3.E3.m1.6.6.6.6.1.3.3.1.cmml" xref="S3.E3.m1.6.6.6.6.1.3.3.2"><cn id="S3.E3.m1.1.1.1.1.cmml" type="integer" xref="S3.E3.m1.1.1.1.1">0</cn><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">𝐈</ci></interval></apply></apply><list id="S3.E3.m1.7.7.7.7.2.1.cmml" xref="S3.E3.m1.7.7.7.7.2.2"><ci id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">𝑐</ci><ci id="S3.E3.m1.5.5.5.5.cmml" xref="S3.E3.m1.5.5.5.5">𝑡</ci></list></apply></apply><apply id="S3.E3.m1.10.10.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.10.10.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1"><minus id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.2"></minus><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.3">italic-ϵ</ci><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1"><times id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.2">italic-ϵ</ci><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.3.3">𝜃</ci></apply><vector id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.10.10.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E3.m1.8.8.cmml" xref="S3.E3.m1.8.8">𝑐</ci><ci id="S3.E3.m1.9.9.cmml" xref="S3.E3.m1.9.9">𝑡</ci></vector></apply></apply></apply><cn id="S3.E3.m1.10.10.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.10.10.1.1.1.1.1.3">2</cn></apply><cn id="S3.E3.m1.10.10.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.10.10.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.10c">\min_{\theta}\mathbb{E}_{x_{0},\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),%
c,t}\|\epsilon-\epsilon_{\theta}(x_{t},c,t)\|_{2}^{2},</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.10d">roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_ϵ ∼ caligraphic_N ( bold_0 , bold_I ) , italic_c , italic_t end_POSTSUBSCRIPT ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_c , italic_t ) ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p1.18">With its powerful capability to model complex data distributions, the diffusion model serves as the foundation for generating high-quality FER data. Our SynFER framework is the pioneering work that explores the use of diffusion models to synthesize affective modalities.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We begin by introducing i) the overall synthetic pipeline for generating facial expression image-label pairs. Next, we detail ii) our approach for producing high-fidelity facial expression images, which are controlled through high-level text descriptions (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS1" title="4.2.1 FEText Data Construction ‣ 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>), fine-grained facial action units corresponding to localized facial muscles (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS3" title="4.2.3 Explicit Control Signals via Facial Action Units ‣ 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>), and a semantic guidance technique (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS3" title="4.3 Semantic Guidance for Precise Expression Control ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Finally, we introduce iii) the FER annotation crafter (FERAnno), a crucial component that thoroughly understands the synthetic facial expression data and automatically generates accurate annotations accordingly(Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS4" title="4.4 Diffusion-based Label Calibrator (FERAnno) ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.4</span></a>). This pipeline ensures both precision and reliability in facial expression generation and labeling.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S4.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall pipeline of our FER data synthesis process.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overall Pipeline for FER Data Synthesis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We introduce the overall pipeline for FER data synthesis (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.F2" title="Figure 2 ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>). The process starts with a coarse human portrait description assigned to a specific facial expression. ChatGPT enriches this description with details such as facial appearance, subtle facial muscle movements, and contextual cues. Simultaneously, facial action unit annotations are generated based on prior FAU-FE knowledge <cite class="ltx_cite ltx_citemacro_cite">Ekman &amp; Friesen (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib23" title="">1978</a>)</cite>, aligning them with emotion categories to serve as explicit control signals for guiding the facial expression image synthesis.
Once the facial expression label, facial action unit labels, and expanded textual prompt are prepared, these inputs condition our diffusion model to generate high-fidelity FER images, guided by semantic guidance to ensure accurate FER semantic.
During the denoising process, FERAnno automatically produces pseudo labels for the generated images.
To further improve labeling accuracy, we ensemble our FERAnno with existing FER models, which collaborate to vote on the accuracy of the predefined FER labels. In cases where discrepancies arise, the predefined label is refined by averaging the predictions from the ensemble experts. This mechanism effectively reduces the risk of inconsistent or uncertain annotations, ensuring that the final synthesis data is precise and dependable for downstream applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Diffusion Model Training for FER Data</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>FEText Data Construction</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">To address the lack of facial expression image-text pairs for diffusion model training, we introduce FEText (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.F3" title="Figure 3 ‣ 4.2.2 Diffusion Model Fine-tuning ‣ 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>), the first hybrid image-text dataset for FER. It combines face images from FFHQ <cite class="ltx_cite ltx_citemacro_cite">Karras et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib38" title="">2019</a>)</cite>, CelebA-HQ <cite class="ltx_cite ltx_citemacro_cite">Karras et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib37" title="">2017</a>)</cite>, AffectNet <cite class="ltx_cite ltx_citemacro_cite">Mollahosseini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib61" title="">2017</a>)</cite> and SFEW <cite class="ltx_cite ltx_citemacro_cite">Dhall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib20" title="">2011</a>)</cite>, each paired with captions generated by a multi-modal large language model (MLLM). FEText includes 400,000 curated pairs tailored for facial expression tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">Resolution Alignment.</span> Due to variations in image resolution across different datasets, we first utilize a super-resolution model <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib51" title="">2023</a>)</cite> to standardize the resolutions of images from AffectNet and SFEW. Specifically, we incorporate high-resolution images from FFHQ and CelebA-HQ datasets to preserve the model’s capacity for high-fidelity image generation. This dual approach allows the model to not only maintain the fidelity of the generated images but also to learn and incorporate the facial expression semantics from AffectNet and SFEW.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">Textual Caption Annotation.</span> To generate a textural caption for each face image, we employ the open-source multi-modal language model ShareGPT-4V <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib12" title="">2023b</a>)</cite>, by guiding it with carefully crafted instructions. To ensure that the generated captions are both context-aware and expressive, we clearly define the model’s role and provide examples of detailed facial expression descriptions within the prompts. This approach enables the model to generate precise, emotion-reflective captions for the input images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Diffusion Model Fine-tuning</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">To facilitate our diffusion model to generate high-fidelity facial expressions, a straightforward approach is to fine-tune the model directly on the proposed FEText using the diffusion loss in Eq.  <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S3.E3" title="In 3 Preliminaries ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>. However, since FEText contains images processed through a super-resolution model, this direct fine-tuning strategy may lead to over-smoothing in the generated images. To address this, we introduce a two-stage fine-tuning paradigm.
In the first stage, the diffusion model is trained on the entire FEText dataset to capture facial expression-related semantics. Then, the second stage mitigates over-smoothing by specifically fine-tuning our diffusion model on the CelebA-HQ and FFHQ subsets of FEText, which consist of native high-resolution images. This two-step approach ensures that our model learns expressive facial details while preserving image sharpness. The fine-tuned model then serves as the foundation for controllable facial expression generation, incorporating facial action unit injection (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS3" title="4.2.3 Explicit Control Signals via Facial Action Units ‣ 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>) and semantic guidance (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS3" title="4.3 Semantic Guidance for Precise Expression Control ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S4.F3.g1" src="x3.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our FEText data construction pipeline.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Explicit Control Signals via Facial Action Units</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">While fine-tuning the diffusion model using facial expression captions provides general language-based guidance for facial expression generation, it lacks the precision needed to capture fine-grained facial details, such as localized muscle movements. To overcome this limitation, we propose to incorporate more explicit control signals through Facial Action Units (FAUs), each of which represents a specific facial muscle movement.
Inspired by IP-Adapter <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib84" title="">2023</a>)</cite>, we apply a decoupled cross-attention module to integrate FAU embeddings with the diffusion model’s generation process. These embeddings are derived by mapping discrete FAU labels into high-dimensional representations using a Multi-Layer Perceptron, referred to as the AU adapter. FAU labels for each image in the FEText dataset are annotated using the widely adopted FAU detection model, OpenGraphAU <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib57" title="">2022</a>)</cite>.
With the diffusion model’s parameters frozen, we train the AU adapter to guide the model in recovering facial images based on the annotated FAU labels, using the objective in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S3.E3" title="In 3 Preliminaries ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Semantic Guidance for Precise Expression Control</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Due to the imbalanced distribution of FER labels in the training data and the potential ambiguity between certain facial expressions <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib90" title="">2024b</a>)</cite>, such as <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">disgust</span>, relying solely on textual and FAU conditions might not guarantee the faithful generation of these expressions.
To address this issue, we propose incorporating semantic guidance on the textual embeddings <math alttext="c^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><msup id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">c</mi><mtext id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3a.cmml">text</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑐</ci><ci id="S4.SS3.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><mtext id="S4.SS3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p1.1.m1.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">c^{\text{text}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT</annotation></semantics></math>, during the later stages of the denoising process. We leverage external knowledge from open-source FER models to steer the generation process, ensuring a more accurate and faithful synthesis of hard-to-distinguish facial expressions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.2.1">Layout Initialization.</span>
During inference, we select a random face image <math alttext="x^{s}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">x</mi><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">s</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑥</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">x^{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT</annotation></semantics></math> from FEText and invert it to initialize the noise sample <math alttext="x_{T}^{s}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><msubsup id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2.2" xref="S4.SS3.p2.2.m2.1.1.2.2.cmml">x</mi><mi id="S4.SS3.p2.2.m2.1.1.2.3" xref="S4.SS3.p2.2.m2.1.1.2.3.cmml">T</mi><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">s</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">superscript</csymbol><apply id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.2.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2.2">𝑥</ci><ci id="S4.SS3.p2.2.m2.1.1.2.3.cmml" xref="S4.SS3.p2.2.m2.1.1.2.3">𝑇</ci></apply><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">x_{T}^{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT</annotation></semantics></math>(Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S3.E1" title="In 3 Preliminaries ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>). Since early diffusion stages shape the global layout of the image <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib91" title="">2023</a>); Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib64" title="">2023</a>); Mao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib58" title="">2023</a>)</cite>, this strategy helps preserve the natural facial structure, ensuring the generated images are coherent, high-quality, and visually consistent with real-world expressions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.3"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.3.1">Semantic Guidance.</span> In the early steps of the diffusion process, the generation process is conditioned on the original textual condition <math alttext="c^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msup id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">c</mi><mtext id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3a.cmml">text</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝑐</ci><ci id="S4.SS3.p3.1.m1.1.1.3a.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><mtext id="S4.SS3.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p3.1.m1.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">c^{\text{text}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT</annotation></semantics></math>. To further induce the generation of facial expression images corresponding to their FER labels <math alttext="y" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">italic_y</annotation></semantics></math>, we iteratively update the textual condition in the subsequent time steps. Specifically, a facial expression classifier <math alttext="f(\cdot)" class="ltx_Math" display="inline" id="S4.SS3.p3.3.m3.1"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.2" xref="S4.SS3.p3.3.m3.1.2.cmml"><mi id="S4.SS3.p3.3.m3.1.2.2" xref="S4.SS3.p3.3.m3.1.2.2.cmml">f</mi><mo id="S4.SS3.p3.3.m3.1.2.1" xref="S4.SS3.p3.3.m3.1.2.1.cmml">⁢</mo><mrow id="S4.SS3.p3.3.m3.1.2.3.2" xref="S4.SS3.p3.3.m3.1.2.cmml"><mo id="S4.SS3.p3.3.m3.1.2.3.2.1" stretchy="false" xref="S4.SS3.p3.3.m3.1.2.cmml">(</mo><mo id="S4.SS3.p3.3.m3.1.1" lspace="0em" rspace="0em" xref="S4.SS3.p3.3.m3.1.1.cmml">⋅</mo><mo id="S4.SS3.p3.3.m3.1.2.3.2.2" stretchy="false" xref="S4.SS3.p3.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.2.cmml" xref="S4.SS3.p3.3.m3.1.2"><times id="S4.SS3.p3.3.m3.1.2.1.cmml" xref="S4.SS3.p3.3.m3.1.2.1"></times><ci id="S4.SS3.p3.3.m3.1.2.2.cmml" xref="S4.SS3.p3.3.m3.1.2.2">𝑓</ci><ci id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">f(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.3.m3.1d">italic_f ( ⋅ )</annotation></semantics></math> is utilized for the injection of complex semantics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.4">To guide the generated images towards the specific class <math alttext="y" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><mi id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><ci id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">italic_y</annotation></semantics></math>, we propose to do so by updating the textual embeddings.
Given an intermediate denoised sample <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS3.p4.2.m2.1"><semantics id="S4.SS3.p4.2.m2.1a"><msub id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml"><mi id="S4.SS3.p4.2.m2.1.1.2" xref="S4.SS3.p4.2.m2.1.1.2.cmml">x</mi><mi id="S4.SS3.p4.2.m2.1.1.3" xref="S4.SS3.p4.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><apply id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.2.m2.1.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p4.2.m2.1.1.2.cmml" xref="S4.SS3.p4.2.m2.1.1.2">𝑥</ci><ci id="S4.SS3.p4.2.m2.1.1.3.cmml" xref="S4.SS3.p4.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> at timestep <math alttext="t" class="ltx_Math" display="inline" id="S4.SS3.p4.3.m3.1"><semantics id="S4.SS3.p4.3.m3.1a"><mi id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><ci id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.3.m3.1d">italic_t</annotation></semantics></math>, following Eq. 15 in DDPM <cite class="ltx_cite ltx_citemacro_cite">Ho et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib35" title="">2020</a>)</cite>, we first estimate the one-step prediction of the original image <math alttext="\hat{x}_{0}" class="ltx_Math" display="inline" id="S4.SS3.p4.4.m4.1"><semantics id="S4.SS3.p4.4.m4.1a"><msub id="S4.SS3.p4.4.m4.1.1" xref="S4.SS3.p4.4.m4.1.1.cmml"><mover accent="true" id="S4.SS3.p4.4.m4.1.1.2" xref="S4.SS3.p4.4.m4.1.1.2.cmml"><mi id="S4.SS3.p4.4.m4.1.1.2.2" xref="S4.SS3.p4.4.m4.1.1.2.2.cmml">x</mi><mo id="S4.SS3.p4.4.m4.1.1.2.1" xref="S4.SS3.p4.4.m4.1.1.2.1.cmml">^</mo></mover><mn id="S4.SS3.p4.4.m4.1.1.3" xref="S4.SS3.p4.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.4.m4.1b"><apply id="S4.SS3.p4.4.m4.1.1.cmml" xref="S4.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.4.m4.1.1.1.cmml" xref="S4.SS3.p4.4.m4.1.1">subscript</csymbol><apply id="S4.SS3.p4.4.m4.1.1.2.cmml" xref="S4.SS3.p4.4.m4.1.1.2"><ci id="S4.SS3.p4.4.m4.1.1.2.1.cmml" xref="S4.SS3.p4.4.m4.1.1.2.1">^</ci><ci id="S4.SS3.p4.4.m4.1.1.2.2.cmml" xref="S4.SS3.p4.4.m4.1.1.2.2">𝑥</ci></apply><cn id="S4.SS3.p4.4.m4.1.1.3.cmml" type="integer" xref="S4.SS3.p4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.4.m4.1c">\hat{x}_{0}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.4.m4.1d">over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{x}_{0}=(x_{t}-\sqrt{1-\bar{\alpha}_{t}})\epsilon_{\theta}(x_{t},t,c^{%
\text{text}},c^{\text{au}})/\sqrt{\bar{\alpha}_{t}}" class="ltx_Math" display="block" id="S4.E4.m1.5"><semantics id="S4.E4.m1.5a"><mrow id="S4.E4.m1.5.5" xref="S4.E4.m1.5.5.cmml"><msub id="S4.E4.m1.5.5.6" xref="S4.E4.m1.5.5.6.cmml"><mover accent="true" id="S4.E4.m1.5.5.6.2" xref="S4.E4.m1.5.5.6.2.cmml"><mi id="S4.E4.m1.5.5.6.2.2" xref="S4.E4.m1.5.5.6.2.2.cmml">x</mi><mo id="S4.E4.m1.5.5.6.2.1" xref="S4.E4.m1.5.5.6.2.1.cmml">^</mo></mover><mn id="S4.E4.m1.5.5.6.3" xref="S4.E4.m1.5.5.6.3.cmml">0</mn></msub><mo id="S4.E4.m1.5.5.5" xref="S4.E4.m1.5.5.5.cmml">=</mo><mrow id="S4.E4.m1.5.5.4" xref="S4.E4.m1.5.5.4.cmml"><mrow id="S4.E4.m1.5.5.4.4" xref="S4.E4.m1.5.5.4.4.cmml"><mrow id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.E4.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml"><msub id="S4.E4.m1.2.2.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.2.cmml">x</mi><mi id="S4.E4.m1.2.2.1.1.1.1.1.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S4.E4.m1.2.2.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.cmml">−</mo><msqrt id="S4.E4.m1.2.2.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.3.cmml"><mrow id="S4.E4.m1.2.2.1.1.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.cmml"><mn id="S4.E4.m1.2.2.1.1.1.1.1.3.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.2.cmml">1</mn><mo id="S4.E4.m1.2.2.1.1.1.1.1.3.2.1" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.1.cmml">−</mo><msub id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.cmml"><mover accent="true" id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.2.cmml">α</mi><mo id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.1" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.1.cmml">¯</mo></mover><mi id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.3" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.3.cmml">t</mi></msub></mrow></msqrt></mrow><mo id="S4.E4.m1.2.2.1.1.1.1.3" stretchy="false" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E4.m1.5.5.4.4.5" xref="S4.E4.m1.5.5.4.4.5.cmml">⁢</mo><msub id="S4.E4.m1.5.5.4.4.6" xref="S4.E4.m1.5.5.4.4.6.cmml"><mi id="S4.E4.m1.5.5.4.4.6.2" xref="S4.E4.m1.5.5.4.4.6.2.cmml">ϵ</mi><mi id="S4.E4.m1.5.5.4.4.6.3" xref="S4.E4.m1.5.5.4.4.6.3.cmml">θ</mi></msub><mo id="S4.E4.m1.5.5.4.4.5a" xref="S4.E4.m1.5.5.4.4.5.cmml">⁢</mo><mrow id="S4.E4.m1.5.5.4.4.4.3" xref="S4.E4.m1.5.5.4.4.4.4.cmml"><mo id="S4.E4.m1.5.5.4.4.4.3.4" stretchy="false" xref="S4.E4.m1.5.5.4.4.4.4.cmml">(</mo><msub id="S4.E4.m1.3.3.2.2.2.1.1" xref="S4.E4.m1.3.3.2.2.2.1.1.cmml"><mi id="S4.E4.m1.3.3.2.2.2.1.1.2" xref="S4.E4.m1.3.3.2.2.2.1.1.2.cmml">x</mi><mi id="S4.E4.m1.3.3.2.2.2.1.1.3" xref="S4.E4.m1.3.3.2.2.2.1.1.3.cmml">t</mi></msub><mo id="S4.E4.m1.5.5.4.4.4.3.5" xref="S4.E4.m1.5.5.4.4.4.4.cmml">,</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">t</mi><mo id="S4.E4.m1.5.5.4.4.4.3.6" xref="S4.E4.m1.5.5.4.4.4.4.cmml">,</mo><msup id="S4.E4.m1.4.4.3.3.3.2.2" xref="S4.E4.m1.4.4.3.3.3.2.2.cmml"><mi id="S4.E4.m1.4.4.3.3.3.2.2.2" xref="S4.E4.m1.4.4.3.3.3.2.2.2.cmml">c</mi><mtext id="S4.E4.m1.4.4.3.3.3.2.2.3" xref="S4.E4.m1.4.4.3.3.3.2.2.3a.cmml">text</mtext></msup><mo id="S4.E4.m1.5.5.4.4.4.3.7" xref="S4.E4.m1.5.5.4.4.4.4.cmml">,</mo><msup id="S4.E4.m1.5.5.4.4.4.3.3" xref="S4.E4.m1.5.5.4.4.4.3.3.cmml"><mi id="S4.E4.m1.5.5.4.4.4.3.3.2" xref="S4.E4.m1.5.5.4.4.4.3.3.2.cmml">c</mi><mtext id="S4.E4.m1.5.5.4.4.4.3.3.3" xref="S4.E4.m1.5.5.4.4.4.3.3.3a.cmml">au</mtext></msup><mo id="S4.E4.m1.5.5.4.4.4.3.8" stretchy="false" xref="S4.E4.m1.5.5.4.4.4.4.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.5.5.4.5" xref="S4.E4.m1.5.5.4.5.cmml">/</mo><msqrt id="S4.E4.m1.5.5.4.6" xref="S4.E4.m1.5.5.4.6.cmml"><msub id="S4.E4.m1.5.5.4.6.2" xref="S4.E4.m1.5.5.4.6.2.cmml"><mover accent="true" id="S4.E4.m1.5.5.4.6.2.2" xref="S4.E4.m1.5.5.4.6.2.2.cmml"><mi id="S4.E4.m1.5.5.4.6.2.2.2" xref="S4.E4.m1.5.5.4.6.2.2.2.cmml">α</mi><mo id="S4.E4.m1.5.5.4.6.2.2.1" xref="S4.E4.m1.5.5.4.6.2.2.1.cmml">¯</mo></mover><mi id="S4.E4.m1.5.5.4.6.2.3" xref="S4.E4.m1.5.5.4.6.2.3.cmml">t</mi></msub></msqrt></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.5b"><apply id="S4.E4.m1.5.5.cmml" xref="S4.E4.m1.5.5"><eq id="S4.E4.m1.5.5.5.cmml" xref="S4.E4.m1.5.5.5"></eq><apply id="S4.E4.m1.5.5.6.cmml" xref="S4.E4.m1.5.5.6"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.6.1.cmml" xref="S4.E4.m1.5.5.6">subscript</csymbol><apply id="S4.E4.m1.5.5.6.2.cmml" xref="S4.E4.m1.5.5.6.2"><ci id="S4.E4.m1.5.5.6.2.1.cmml" xref="S4.E4.m1.5.5.6.2.1">^</ci><ci id="S4.E4.m1.5.5.6.2.2.cmml" xref="S4.E4.m1.5.5.6.2.2">𝑥</ci></apply><cn id="S4.E4.m1.5.5.6.3.cmml" type="integer" xref="S4.E4.m1.5.5.6.3">0</cn></apply><apply id="S4.E4.m1.5.5.4.cmml" xref="S4.E4.m1.5.5.4"><divide id="S4.E4.m1.5.5.4.5.cmml" xref="S4.E4.m1.5.5.4.5"></divide><apply id="S4.E4.m1.5.5.4.4.cmml" xref="S4.E4.m1.5.5.4.4"><times id="S4.E4.m1.5.5.4.4.5.cmml" xref="S4.E4.m1.5.5.4.4.5"></times><apply id="S4.E4.m1.2.2.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1"><minus id="S4.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1"></minus><apply id="S4.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.2">𝑥</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3"><root id="S4.E4.m1.2.2.1.1.1.1.1.3a.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3"></root><apply id="S4.E4.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2"><minus id="S4.E4.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.1"></minus><cn id="S4.E4.m1.2.2.1.1.1.1.1.3.2.2.cmml" type="integer" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.2">1</cn><apply id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3">subscript</csymbol><apply id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2"><ci id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.1">¯</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.2.2">𝛼</ci></apply><ci id="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.3.2.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S4.E4.m1.5.5.4.4.6.cmml" xref="S4.E4.m1.5.5.4.4.6"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.4.4.6.1.cmml" xref="S4.E4.m1.5.5.4.4.6">subscript</csymbol><ci id="S4.E4.m1.5.5.4.4.6.2.cmml" xref="S4.E4.m1.5.5.4.4.6.2">italic-ϵ</ci><ci id="S4.E4.m1.5.5.4.4.6.3.cmml" xref="S4.E4.m1.5.5.4.4.6.3">𝜃</ci></apply><vector id="S4.E4.m1.5.5.4.4.4.4.cmml" xref="S4.E4.m1.5.5.4.4.4.3"><apply id="S4.E4.m1.3.3.2.2.2.1.1.cmml" xref="S4.E4.m1.3.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.2.2.1.1.1.cmml" xref="S4.E4.m1.3.3.2.2.2.1.1">subscript</csymbol><ci id="S4.E4.m1.3.3.2.2.2.1.1.2.cmml" xref="S4.E4.m1.3.3.2.2.2.1.1.2">𝑥</ci><ci id="S4.E4.m1.3.3.2.2.2.1.1.3.cmml" xref="S4.E4.m1.3.3.2.2.2.1.1.3">𝑡</ci></apply><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">𝑡</ci><apply id="S4.E4.m1.4.4.3.3.3.2.2.cmml" xref="S4.E4.m1.4.4.3.3.3.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.3.3.3.2.2.1.cmml" xref="S4.E4.m1.4.4.3.3.3.2.2">superscript</csymbol><ci id="S4.E4.m1.4.4.3.3.3.2.2.2.cmml" xref="S4.E4.m1.4.4.3.3.3.2.2.2">𝑐</ci><ci id="S4.E4.m1.4.4.3.3.3.2.2.3a.cmml" xref="S4.E4.m1.4.4.3.3.3.2.2.3"><mtext id="S4.E4.m1.4.4.3.3.3.2.2.3.cmml" mathsize="70%" xref="S4.E4.m1.4.4.3.3.3.2.2.3">text</mtext></ci></apply><apply id="S4.E4.m1.5.5.4.4.4.3.3.cmml" xref="S4.E4.m1.5.5.4.4.4.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.4.4.4.3.3.1.cmml" xref="S4.E4.m1.5.5.4.4.4.3.3">superscript</csymbol><ci id="S4.E4.m1.5.5.4.4.4.3.3.2.cmml" xref="S4.E4.m1.5.5.4.4.4.3.3.2">𝑐</ci><ci id="S4.E4.m1.5.5.4.4.4.3.3.3a.cmml" xref="S4.E4.m1.5.5.4.4.4.3.3.3"><mtext id="S4.E4.m1.5.5.4.4.4.3.3.3.cmml" mathsize="70%" xref="S4.E4.m1.5.5.4.4.4.3.3.3">au</mtext></ci></apply></vector></apply><apply id="S4.E4.m1.5.5.4.6.cmml" xref="S4.E4.m1.5.5.4.6"><root id="S4.E4.m1.5.5.4.6a.cmml" xref="S4.E4.m1.5.5.4.6"></root><apply id="S4.E4.m1.5.5.4.6.2.cmml" xref="S4.E4.m1.5.5.4.6.2"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.4.6.2.1.cmml" xref="S4.E4.m1.5.5.4.6.2">subscript</csymbol><apply id="S4.E4.m1.5.5.4.6.2.2.cmml" xref="S4.E4.m1.5.5.4.6.2.2"><ci id="S4.E4.m1.5.5.4.6.2.2.1.cmml" xref="S4.E4.m1.5.5.4.6.2.2.1">¯</ci><ci id="S4.E4.m1.5.5.4.6.2.2.2.cmml" xref="S4.E4.m1.5.5.4.6.2.2.2">𝛼</ci></apply><ci id="S4.E4.m1.5.5.4.6.2.3.cmml" xref="S4.E4.m1.5.5.4.6.2.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.5c">\hat{x}_{0}=(x_{t}-\sqrt{1-\bar{\alpha}_{t}})\epsilon_{\theta}(x_{t},t,c^{%
\text{text}},c^{\text{au}})/\sqrt{\bar{\alpha}_{t}}</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.5d">over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - square-root start_ARG 1 - over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT , italic_c start_POSTSUPERSCRIPT au end_POSTSUPERSCRIPT ) / square-root start_ARG over¯ start_ARG italic_α end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p4.10">We then calculate the classification loss with:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{g}=-y\log(h(f(\hat{x}_{0}))_{i})" class="ltx_Math" display="block" id="S4.E5.m1.2"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml"><msub id="S4.E5.m1.2.2.3" xref="S4.E5.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.2.2.3.2" xref="S4.E5.m1.2.2.3.2.cmml">ℒ</mi><mi id="S4.E5.m1.2.2.3.3" xref="S4.E5.m1.2.2.3.3.cmml">g</mi></msub><mo id="S4.E5.m1.2.2.2" xref="S4.E5.m1.2.2.2.cmml">=</mo><mrow id="S4.E5.m1.2.2.1" xref="S4.E5.m1.2.2.1.cmml"><mo id="S4.E5.m1.2.2.1a" xref="S4.E5.m1.2.2.1.cmml">−</mo><mrow id="S4.E5.m1.2.2.1.1" xref="S4.E5.m1.2.2.1.1.cmml"><mi id="S4.E5.m1.2.2.1.1.3" xref="S4.E5.m1.2.2.1.1.3.cmml">y</mi><mo id="S4.E5.m1.2.2.1.1.2" lspace="0.167em" xref="S4.E5.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S4.E5.m1.2.2.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.2.cmml"><mi id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml">log</mi><mo id="S4.E5.m1.2.2.1.1.1.1a" xref="S4.E5.m1.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S4.E5.m1.2.2.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.2.cmml"><mo id="S4.E5.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S4.E5.m1.2.2.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.2.2.1.1.1.1.1.1.3" xref="S4.E5.m1.2.2.1.1.1.1.1.1.3.cmml">h</mi><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.2" xref="S4.E5.m1.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><msub id="S4.E5.m1.2.2.1.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">f</mi><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mn id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S4.E5.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub></mrow><mo id="S4.E5.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E5.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2"><eq id="S4.E5.m1.2.2.2.cmml" xref="S4.E5.m1.2.2.2"></eq><apply id="S4.E5.m1.2.2.3.cmml" xref="S4.E5.m1.2.2.3"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.3.1.cmml" xref="S4.E5.m1.2.2.3">subscript</csymbol><ci id="S4.E5.m1.2.2.3.2.cmml" xref="S4.E5.m1.2.2.3.2">ℒ</ci><ci id="S4.E5.m1.2.2.3.3.cmml" xref="S4.E5.m1.2.2.3.3">𝑔</ci></apply><apply id="S4.E5.m1.2.2.1.cmml" xref="S4.E5.m1.2.2.1"><minus id="S4.E5.m1.2.2.1.2.cmml" xref="S4.E5.m1.2.2.1"></minus><apply id="S4.E5.m1.2.2.1.1.cmml" xref="S4.E5.m1.2.2.1.1"><times id="S4.E5.m1.2.2.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.2"></times><ci id="S4.E5.m1.2.2.1.1.3.cmml" xref="S4.E5.m1.2.2.1.1.3">𝑦</ci><apply id="S4.E5.m1.2.2.1.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1"><log id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"></log><apply id="S4.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1"><times id="S4.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S4.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.3">ℎ</ci><apply id="S4.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1"><times id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">𝑓</ci><apply id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci></apply><cn id="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3">0</cn></apply></apply><ci id="S4.E5.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\mathcal{L}_{g}=-y\log(h(f(\hat{x}_{0}))_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = - italic_y roman_log ( italic_h ( italic_f ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p4.5">Given the guidance loss <math alttext="\mathcal{L}_{g}" class="ltx_Math" display="inline" id="S4.SS3.p4.5.m1.1"><semantics id="S4.SS3.p4.5.m1.1a"><msub id="S4.SS3.p4.5.m1.1.1" xref="S4.SS3.p4.5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p4.5.m1.1.1.2" xref="S4.SS3.p4.5.m1.1.1.2.cmml">ℒ</mi><mi id="S4.SS3.p4.5.m1.1.1.3" xref="S4.SS3.p4.5.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.5.m1.1b"><apply id="S4.SS3.p4.5.m1.1.1.cmml" xref="S4.SS3.p4.5.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.5.m1.1.1.1.cmml" xref="S4.SS3.p4.5.m1.1.1">subscript</csymbol><ci id="S4.SS3.p4.5.m1.1.1.2.cmml" xref="S4.SS3.p4.5.m1.1.1.2">ℒ</ci><ci id="S4.SS3.p4.5.m1.1.1.3.cmml" xref="S4.SS3.p4.5.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.5.m1.1c">\mathcal{L}_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.5.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>, the textual embedding is updated with the corresponding gradient:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="c^{\text{text}}_{t-1}=c^{\text{text}}_{t}+\lambda_{g}\frac{\nabla_{c^{\text{%
text}}_{t}}\mathcal{L}_{g}}{||\nabla_{c^{\text{text}}_{t}}\mathcal{L}_{g}||_{2}}" class="ltx_Math" display="block" id="S4.E6.m1.1"><semantics id="S4.E6.m1.1a"><mrow id="S4.E6.m1.1.2" xref="S4.E6.m1.1.2.cmml"><msubsup id="S4.E6.m1.1.2.2" xref="S4.E6.m1.1.2.2.cmml"><mi id="S4.E6.m1.1.2.2.2.2" xref="S4.E6.m1.1.2.2.2.2.cmml">c</mi><mrow id="S4.E6.m1.1.2.2.3" xref="S4.E6.m1.1.2.2.3.cmml"><mi id="S4.E6.m1.1.2.2.3.2" xref="S4.E6.m1.1.2.2.3.2.cmml">t</mi><mo id="S4.E6.m1.1.2.2.3.1" xref="S4.E6.m1.1.2.2.3.1.cmml">−</mo><mn id="S4.E6.m1.1.2.2.3.3" xref="S4.E6.m1.1.2.2.3.3.cmml">1</mn></mrow><mtext id="S4.E6.m1.1.2.2.2.3" xref="S4.E6.m1.1.2.2.2.3a.cmml">text</mtext></msubsup><mo id="S4.E6.m1.1.2.1" xref="S4.E6.m1.1.2.1.cmml">=</mo><mrow id="S4.E6.m1.1.2.3" xref="S4.E6.m1.1.2.3.cmml"><msubsup id="S4.E6.m1.1.2.3.2" xref="S4.E6.m1.1.2.3.2.cmml"><mi id="S4.E6.m1.1.2.3.2.2.2" xref="S4.E6.m1.1.2.3.2.2.2.cmml">c</mi><mi id="S4.E6.m1.1.2.3.2.3" xref="S4.E6.m1.1.2.3.2.3.cmml">t</mi><mtext id="S4.E6.m1.1.2.3.2.2.3" xref="S4.E6.m1.1.2.3.2.2.3a.cmml">text</mtext></msubsup><mo id="S4.E6.m1.1.2.3.1" xref="S4.E6.m1.1.2.3.1.cmml">+</mo><mrow id="S4.E6.m1.1.2.3.3" xref="S4.E6.m1.1.2.3.3.cmml"><msub id="S4.E6.m1.1.2.3.3.2" xref="S4.E6.m1.1.2.3.3.2.cmml"><mi id="S4.E6.m1.1.2.3.3.2.2" xref="S4.E6.m1.1.2.3.3.2.2.cmml">λ</mi><mi id="S4.E6.m1.1.2.3.3.2.3" xref="S4.E6.m1.1.2.3.3.2.3.cmml">g</mi></msub><mo id="S4.E6.m1.1.2.3.3.1" xref="S4.E6.m1.1.2.3.3.1.cmml">⁢</mo><mfrac id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mrow id="S4.E6.m1.1.1.3" xref="S4.E6.m1.1.1.3.cmml"><msub id="S4.E6.m1.1.1.3.1" xref="S4.E6.m1.1.1.3.1.cmml"><mo id="S4.E6.m1.1.1.3.1.2" xref="S4.E6.m1.1.1.3.1.2.cmml">∇</mo><msubsup id="S4.E6.m1.1.1.3.1.3" xref="S4.E6.m1.1.1.3.1.3.cmml"><mi id="S4.E6.m1.1.1.3.1.3.2.2" xref="S4.E6.m1.1.1.3.1.3.2.2.cmml">c</mi><mi id="S4.E6.m1.1.1.3.1.3.3" xref="S4.E6.m1.1.1.3.1.3.3.cmml">t</mi><mtext id="S4.E6.m1.1.1.3.1.3.2.3" xref="S4.E6.m1.1.1.3.1.3.2.3a.cmml">text</mtext></msubsup></msub><msub id="S4.E6.m1.1.1.3.2" xref="S4.E6.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.1.1.3.2.2" xref="S4.E6.m1.1.1.3.2.2.cmml">ℒ</mi><mi id="S4.E6.m1.1.1.3.2.3" xref="S4.E6.m1.1.1.3.2.3.cmml">g</mi></msub></mrow><msub id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.2.cmml"><mo id="S4.E6.m1.1.1.1.1.1.2" stretchy="false" xref="S4.E6.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E6.m1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.cmml"><msub id="S4.E6.m1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E6.m1.1.1.1.1.1.1.1.2" rspace="0.167em" xref="S4.E6.m1.1.1.1.1.1.1.1.2.cmml">∇</mo><msubsup id="S4.E6.m1.1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.1.3.2.2" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.2.cmml">c</mi><mi id="S4.E6.m1.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.1.1.1.1.1.1.1.3.3.cmml">t</mi><mtext id="S4.E6.m1.1.1.1.1.1.1.1.3.2.3" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.3a.cmml">text</mtext></msubsup></msub><msub id="S4.E6.m1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.1.1.1.1.1.1.2.2.cmml">ℒ</mi><mi id="S4.E6.m1.1.1.1.1.1.1.2.3" xref="S4.E6.m1.1.1.1.1.1.1.2.3.cmml">g</mi></msub></mrow><mo id="S4.E6.m1.1.1.1.1.1.3" stretchy="false" xref="S4.E6.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E6.m1.1.1.1.3" xref="S4.E6.m1.1.1.1.3.cmml">2</mn></msub></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.1b"><apply id="S4.E6.m1.1.2.cmml" xref="S4.E6.m1.1.2"><eq id="S4.E6.m1.1.2.1.cmml" xref="S4.E6.m1.1.2.1"></eq><apply id="S4.E6.m1.1.2.2.cmml" xref="S4.E6.m1.1.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.2.1.cmml" xref="S4.E6.m1.1.2.2">subscript</csymbol><apply id="S4.E6.m1.1.2.2.2.cmml" xref="S4.E6.m1.1.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.2.2.1.cmml" xref="S4.E6.m1.1.2.2">superscript</csymbol><ci id="S4.E6.m1.1.2.2.2.2.cmml" xref="S4.E6.m1.1.2.2.2.2">𝑐</ci><ci id="S4.E6.m1.1.2.2.2.3a.cmml" xref="S4.E6.m1.1.2.2.2.3"><mtext id="S4.E6.m1.1.2.2.2.3.cmml" mathsize="70%" xref="S4.E6.m1.1.2.2.2.3">text</mtext></ci></apply><apply id="S4.E6.m1.1.2.2.3.cmml" xref="S4.E6.m1.1.2.2.3"><minus id="S4.E6.m1.1.2.2.3.1.cmml" xref="S4.E6.m1.1.2.2.3.1"></minus><ci id="S4.E6.m1.1.2.2.3.2.cmml" xref="S4.E6.m1.1.2.2.3.2">𝑡</ci><cn id="S4.E6.m1.1.2.2.3.3.cmml" type="integer" xref="S4.E6.m1.1.2.2.3.3">1</cn></apply></apply><apply id="S4.E6.m1.1.2.3.cmml" xref="S4.E6.m1.1.2.3"><plus id="S4.E6.m1.1.2.3.1.cmml" xref="S4.E6.m1.1.2.3.1"></plus><apply id="S4.E6.m1.1.2.3.2.cmml" xref="S4.E6.m1.1.2.3.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.3.2.1.cmml" xref="S4.E6.m1.1.2.3.2">subscript</csymbol><apply id="S4.E6.m1.1.2.3.2.2.cmml" xref="S4.E6.m1.1.2.3.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.3.2.2.1.cmml" xref="S4.E6.m1.1.2.3.2">superscript</csymbol><ci id="S4.E6.m1.1.2.3.2.2.2.cmml" xref="S4.E6.m1.1.2.3.2.2.2">𝑐</ci><ci id="S4.E6.m1.1.2.3.2.2.3a.cmml" xref="S4.E6.m1.1.2.3.2.2.3"><mtext id="S4.E6.m1.1.2.3.2.2.3.cmml" mathsize="70%" xref="S4.E6.m1.1.2.3.2.2.3">text</mtext></ci></apply><ci id="S4.E6.m1.1.2.3.2.3.cmml" xref="S4.E6.m1.1.2.3.2.3">𝑡</ci></apply><apply id="S4.E6.m1.1.2.3.3.cmml" xref="S4.E6.m1.1.2.3.3"><times id="S4.E6.m1.1.2.3.3.1.cmml" xref="S4.E6.m1.1.2.3.3.1"></times><apply id="S4.E6.m1.1.2.3.3.2.cmml" xref="S4.E6.m1.1.2.3.3.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.2.3.3.2.1.cmml" xref="S4.E6.m1.1.2.3.3.2">subscript</csymbol><ci id="S4.E6.m1.1.2.3.3.2.2.cmml" xref="S4.E6.m1.1.2.3.3.2.2">𝜆</ci><ci id="S4.E6.m1.1.2.3.3.2.3.cmml" xref="S4.E6.m1.1.2.3.3.2.3">𝑔</ci></apply><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"><divide id="S4.E6.m1.1.1.2.cmml" xref="S4.E6.m1.1.1"></divide><apply id="S4.E6.m1.1.1.3.cmml" xref="S4.E6.m1.1.1.3"><apply id="S4.E6.m1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.3.1.1.cmml" xref="S4.E6.m1.1.1.3.1">subscript</csymbol><ci id="S4.E6.m1.1.1.3.1.2.cmml" xref="S4.E6.m1.1.1.3.1.2">∇</ci><apply id="S4.E6.m1.1.1.3.1.3.cmml" xref="S4.E6.m1.1.1.3.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.3.1.3.1.cmml" xref="S4.E6.m1.1.1.3.1.3">subscript</csymbol><apply id="S4.E6.m1.1.1.3.1.3.2.cmml" xref="S4.E6.m1.1.1.3.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.3.1.3.2.1.cmml" xref="S4.E6.m1.1.1.3.1.3">superscript</csymbol><ci id="S4.E6.m1.1.1.3.1.3.2.2.cmml" xref="S4.E6.m1.1.1.3.1.3.2.2">𝑐</ci><ci id="S4.E6.m1.1.1.3.1.3.2.3a.cmml" xref="S4.E6.m1.1.1.3.1.3.2.3"><mtext id="S4.E6.m1.1.1.3.1.3.2.3.cmml" mathsize="50%" xref="S4.E6.m1.1.1.3.1.3.2.3">text</mtext></ci></apply><ci id="S4.E6.m1.1.1.3.1.3.3.cmml" xref="S4.E6.m1.1.1.3.1.3.3">𝑡</ci></apply></apply><apply id="S4.E6.m1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.3.2.1.cmml" xref="S4.E6.m1.1.1.3.2">subscript</csymbol><ci id="S4.E6.m1.1.1.3.2.2.cmml" xref="S4.E6.m1.1.1.3.2.2">ℒ</ci><ci id="S4.E6.m1.1.1.3.2.3.cmml" xref="S4.E6.m1.1.1.3.2.3">𝑔</ci></apply></apply><apply id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E6.m1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E6.m1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1"><apply id="S4.E6.m1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2">∇</ci><apply id="S4.E6.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E6.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.2">𝑐</ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.3"><mtext id="S4.E6.m1.1.1.1.1.1.1.1.3.2.3.cmml" mathsize="50%" xref="S4.E6.m1.1.1.1.1.1.1.1.3.2.3">text</mtext></ci></apply><ci id="S4.E6.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S4.E6.m1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2.2">ℒ</ci><ci id="S4.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2.3">𝑔</ci></apply></apply></apply><cn id="S4.E6.m1.1.1.1.3.cmml" type="integer" xref="S4.E6.m1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.1c">c^{\text{text}}_{t-1}=c^{\text{text}}_{t}+\lambda_{g}\frac{\nabla_{c^{\text{%
text}}_{t}}\mathcal{L}_{g}}{||\nabla_{c^{\text{text}}_{t}}\mathcal{L}_{g}||_{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.1d">italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT divide start_ARG ∇ start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT end_ARG start_ARG | | ∇ start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p4.9">where <math alttext="\lambda_{g}" class="ltx_Math" display="inline" id="S4.SS3.p4.6.m1.1"><semantics id="S4.SS3.p4.6.m1.1a"><msub id="S4.SS3.p4.6.m1.1.1" xref="S4.SS3.p4.6.m1.1.1.cmml"><mi id="S4.SS3.p4.6.m1.1.1.2" xref="S4.SS3.p4.6.m1.1.1.2.cmml">λ</mi><mi id="S4.SS3.p4.6.m1.1.1.3" xref="S4.SS3.p4.6.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.6.m1.1b"><apply id="S4.SS3.p4.6.m1.1.1.cmml" xref="S4.SS3.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.6.m1.1.1.1.cmml" xref="S4.SS3.p4.6.m1.1.1">subscript</csymbol><ci id="S4.SS3.p4.6.m1.1.1.2.cmml" xref="S4.SS3.p4.6.m1.1.1.2">𝜆</ci><ci id="S4.SS3.p4.6.m1.1.1.3.cmml" xref="S4.SS3.p4.6.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.6.m1.1c">\lambda_{g}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.6.m1.1d">italic_λ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="c_{t-1}^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS3.p4.7.m2.1"><semantics id="S4.SS3.p4.7.m2.1a"><msubsup id="S4.SS3.p4.7.m2.1.1" xref="S4.SS3.p4.7.m2.1.1.cmml"><mi id="S4.SS3.p4.7.m2.1.1.2.2" xref="S4.SS3.p4.7.m2.1.1.2.2.cmml">c</mi><mrow id="S4.SS3.p4.7.m2.1.1.2.3" xref="S4.SS3.p4.7.m2.1.1.2.3.cmml"><mi id="S4.SS3.p4.7.m2.1.1.2.3.2" xref="S4.SS3.p4.7.m2.1.1.2.3.2.cmml">t</mi><mo id="S4.SS3.p4.7.m2.1.1.2.3.1" xref="S4.SS3.p4.7.m2.1.1.2.3.1.cmml">−</mo><mn id="S4.SS3.p4.7.m2.1.1.2.3.3" xref="S4.SS3.p4.7.m2.1.1.2.3.3.cmml">1</mn></mrow><mtext id="S4.SS3.p4.7.m2.1.1.3" xref="S4.SS3.p4.7.m2.1.1.3a.cmml">text</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.7.m2.1b"><apply id="S4.SS3.p4.7.m2.1.1.cmml" xref="S4.SS3.p4.7.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.7.m2.1.1.1.cmml" xref="S4.SS3.p4.7.m2.1.1">superscript</csymbol><apply id="S4.SS3.p4.7.m2.1.1.2.cmml" xref="S4.SS3.p4.7.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.7.m2.1.1.2.1.cmml" xref="S4.SS3.p4.7.m2.1.1">subscript</csymbol><ci id="S4.SS3.p4.7.m2.1.1.2.2.cmml" xref="S4.SS3.p4.7.m2.1.1.2.2">𝑐</ci><apply id="S4.SS3.p4.7.m2.1.1.2.3.cmml" xref="S4.SS3.p4.7.m2.1.1.2.3"><minus id="S4.SS3.p4.7.m2.1.1.2.3.1.cmml" xref="S4.SS3.p4.7.m2.1.1.2.3.1"></minus><ci id="S4.SS3.p4.7.m2.1.1.2.3.2.cmml" xref="S4.SS3.p4.7.m2.1.1.2.3.2">𝑡</ci><cn id="S4.SS3.p4.7.m2.1.1.2.3.3.cmml" type="integer" xref="S4.SS3.p4.7.m2.1.1.2.3.3">1</cn></apply></apply><ci id="S4.SS3.p4.7.m2.1.1.3a.cmml" xref="S4.SS3.p4.7.m2.1.1.3"><mtext id="S4.SS3.p4.7.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p4.7.m2.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.7.m2.1c">c_{t-1}^{\text{text}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.7.m2.1d">italic_c start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT</annotation></semantics></math> denote the step size and the updated textual embedding at timestep <math alttext="t-1" class="ltx_Math" display="inline" id="S4.SS3.p4.8.m3.1"><semantics id="S4.SS3.p4.8.m3.1a"><mrow id="S4.SS3.p4.8.m3.1.1" xref="S4.SS3.p4.8.m3.1.1.cmml"><mi id="S4.SS3.p4.8.m3.1.1.2" xref="S4.SS3.p4.8.m3.1.1.2.cmml">t</mi><mo id="S4.SS3.p4.8.m3.1.1.1" xref="S4.SS3.p4.8.m3.1.1.1.cmml">−</mo><mn id="S4.SS3.p4.8.m3.1.1.3" xref="S4.SS3.p4.8.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.8.m3.1b"><apply id="S4.SS3.p4.8.m3.1.1.cmml" xref="S4.SS3.p4.8.m3.1.1"><minus id="S4.SS3.p4.8.m3.1.1.1.cmml" xref="S4.SS3.p4.8.m3.1.1.1"></minus><ci id="S4.SS3.p4.8.m3.1.1.2.cmml" xref="S4.SS3.p4.8.m3.1.1.2">𝑡</ci><cn id="S4.SS3.p4.8.m3.1.1.3.cmml" type="integer" xref="S4.SS3.p4.8.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.8.m3.1c">t-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.8.m3.1d">italic_t - 1</annotation></semantics></math>, respectively.
In the latter steps of the diffusion process, the noise estimator network <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS3.p4.9.m4.1"><semantics id="S4.SS3.p4.9.m4.1a"><msub id="S4.SS3.p4.9.m4.1.1" xref="S4.SS3.p4.9.m4.1.1.cmml"><mi id="S4.SS3.p4.9.m4.1.1.2" xref="S4.SS3.p4.9.m4.1.1.2.cmml">ϵ</mi><mi id="S4.SS3.p4.9.m4.1.1.3" xref="S4.SS3.p4.9.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.9.m4.1b"><apply id="S4.SS3.p4.9.m4.1.1.cmml" xref="S4.SS3.p4.9.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.9.m4.1.1.1.cmml" xref="S4.SS3.p4.9.m4.1.1">subscript</csymbol><ci id="S4.SS3.p4.9.m4.1.1.2.cmml" xref="S4.SS3.p4.9.m4.1.1.2">italic-ϵ</ci><ci id="S4.SS3.p4.9.m4.1.1.3.cmml" xref="S4.SS3.p4.9.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.9.m4.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.9.m4.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is conditioned on the updated textual embeddings rather than the original one.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Diffusion-based Label Calibrator (FERAnno)</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="371" id="S4.F4.g1" src="x4.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overview of our FERAnno pseudo-label generator.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To ensure semantic alignment between each synthesized face image and its assigned facial expression label, we introduce FERAnno, a label calibration framework designed to validate the consistency of the generated data. By analyzing the facial patterns of each synthesized image, FERAnno categorizes them and compares the post-categorized labels with their pre-assigned facial expression labels. This verification process helps identify and filter out samples with mismatched labels, preventing them from negatively impacting downstream FER model training.
Specifically, FERAnno is a diffusion-based label calibrator equipped with a deep understanding of facial semantics. It leverages the multi-scale intermediate features and cross-attention maps inherent in the diffusion model to predict accurate FER labels, as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.F4" title="Figure 4 ‣ 4.4 Diffusion-based Label Calibrator (FERAnno) ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a>. This ensures only high-quality, correctly labeled samples are included in the training pipeline, leading to more reliable model performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.6"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.6.1">Image Inversion.</span> To extract facial features and cross-attention maps with the diffusion model <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><msub id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">italic-ϵ</ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we first inverse the generated image <math alttext="x_{0}" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><msub id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">x</mi><mn id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">𝑥</ci><cn id="S4.SS4.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS4.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> back to the noise sample <math alttext="x_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><msub id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">x</mi><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">𝑥</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> at a denoising timestep <math alttext="t" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m4.1"><semantics id="S4.SS4.p2.4.m4.1a"><mi id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><ci id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.4.m4.1d">italic_t</annotation></semantics></math>, following a predefined scheduler, as described in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S3.E1" title="In 3 Preliminaries ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>.
To preserve facial details, we set <math alttext="t=1" class="ltx_Math" display="inline" id="S4.SS4.p2.5.m5.1"><semantics id="S4.SS4.p2.5.m5.1a"><mrow id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mi id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">t</mi><mo id="S4.SS4.p2.5.m5.1.1.1" xref="S4.SS4.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS4.p2.5.m5.1.1.3" xref="S4.SS4.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><eq id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1.1"></eq><ci id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">𝑡</ci><cn id="S4.SS4.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS4.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">t=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.5.m5.1d">italic_t = 1</annotation></semantics></math> during the inversion process, ensuring that the facial features remain as close as possible to the original generated image <math alttext="x_{0}" class="ltx_Math" display="inline" id="S4.SS4.p2.6.m6.1"><semantics id="S4.SS4.p2.6.m6.1a"><msub id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml"><mi id="S4.SS4.p2.6.m6.1.1.2" xref="S4.SS4.p2.6.m6.1.1.2.cmml">x</mi><mn id="S4.SS4.p2.6.m6.1.1.3" xref="S4.SS4.p2.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><apply id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.6.m6.1.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p2.6.m6.1.1.2.cmml" xref="S4.SS4.p2.6.m6.1.1.2">𝑥</ci><cn id="S4.SS4.p2.6.m6.1.1.3.cmml" type="integer" xref="S4.SS4.p2.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.6.m6.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. This partially denoised sample is then passed through the trained denoising network, allowing us to extract rich facial features and cross-attention maps from intermediate layers, which are critical for capturing detailed facial patterns.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.13"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.13.1">Feature Extraction.</span> Given the inverted noise sample <math alttext="x_{1}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">x</mi><mn id="S4.SS4.p3.1.m1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">𝑥</ci><cn id="S4.SS4.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">x_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and the corresponding textual condition <math alttext="c^{\text{text}}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><msup id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">c</mi><mtext id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3a.cmml">text</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">superscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">𝑐</ci><ci id="S4.SS4.p3.2.m2.1.1.3a.cmml" xref="S4.SS4.p3.2.m2.1.1.3"><mtext id="S4.SS4.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p3.2.m2.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">c^{\text{text}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT</annotation></semantics></math> and AU condition <math alttext="c^{\text{au}}" class="ltx_Math" display="inline" id="S4.SS4.p3.3.m3.1"><semantics id="S4.SS4.p3.3.m3.1a"><msup id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">c</mi><mtext id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3a.cmml">au</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">superscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">𝑐</ci><ci id="S4.SS4.p3.3.m3.1.1.3a.cmml" xref="S4.SS4.p3.3.m3.1.1.3"><mtext id="S4.SS4.p3.3.m3.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p3.3.m3.1.1.3">au</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">c^{\text{au}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.m3.1d">italic_c start_POSTSUPERSCRIPT au end_POSTSUPERSCRIPT</annotation></semantics></math>, we can extract the multi-scale feature representations and textual cross-attention maps from the U-Net <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.p3.4.m4.1"><semantics id="S4.SS4.p3.4.m4.1a"><msub id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mi id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.p3.4.m4.1.1.3" xref="S4.SS4.p3.4.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">italic-ϵ</ci><ci id="S4.SS4.p3.4.m4.1.1.3.cmml" xref="S4.SS4.p3.4.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.4.m4.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> as <math alttext="\left\{\mathcal{F},\mathcal{A}\right\}=\epsilon_{\theta}(x_{1},t_{1},c^{\text{%
text}},c^{\text{au}})" class="ltx_Math" display="inline" id="S4.SS4.p3.5.m5.6"><semantics id="S4.SS4.p3.5.m5.6a"><mrow id="S4.SS4.p3.5.m5.6.6" xref="S4.SS4.p3.5.m5.6.6.cmml"><mrow id="S4.SS4.p3.5.m5.6.6.6.2" xref="S4.SS4.p3.5.m5.6.6.6.1.cmml"><mo id="S4.SS4.p3.5.m5.6.6.6.2.1" xref="S4.SS4.p3.5.m5.6.6.6.1.cmml">{</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.5.m5.1.1" xref="S4.SS4.p3.5.m5.1.1.cmml">ℱ</mi><mo id="S4.SS4.p3.5.m5.6.6.6.2.2" xref="S4.SS4.p3.5.m5.6.6.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.5.m5.2.2" xref="S4.SS4.p3.5.m5.2.2.cmml">𝒜</mi><mo id="S4.SS4.p3.5.m5.6.6.6.2.3" xref="S4.SS4.p3.5.m5.6.6.6.1.cmml">}</mo></mrow><mo id="S4.SS4.p3.5.m5.6.6.5" xref="S4.SS4.p3.5.m5.6.6.5.cmml">=</mo><mrow id="S4.SS4.p3.5.m5.6.6.4" xref="S4.SS4.p3.5.m5.6.6.4.cmml"><msub id="S4.SS4.p3.5.m5.6.6.4.6" xref="S4.SS4.p3.5.m5.6.6.4.6.cmml"><mi id="S4.SS4.p3.5.m5.6.6.4.6.2" xref="S4.SS4.p3.5.m5.6.6.4.6.2.cmml">ϵ</mi><mi id="S4.SS4.p3.5.m5.6.6.4.6.3" xref="S4.SS4.p3.5.m5.6.6.4.6.3.cmml">θ</mi></msub><mo id="S4.SS4.p3.5.m5.6.6.4.5" xref="S4.SS4.p3.5.m5.6.6.4.5.cmml">⁢</mo><mrow id="S4.SS4.p3.5.m5.6.6.4.4.4" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml"><mo id="S4.SS4.p3.5.m5.6.6.4.4.4.5" stretchy="false" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml">(</mo><msub id="S4.SS4.p3.5.m5.3.3.1.1.1.1" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1.cmml"><mi id="S4.SS4.p3.5.m5.3.3.1.1.1.1.2" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1.2.cmml">x</mi><mn id="S4.SS4.p3.5.m5.3.3.1.1.1.1.3" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS4.p3.5.m5.6.6.4.4.4.6" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml">,</mo><msub id="S4.SS4.p3.5.m5.4.4.2.2.2.2" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2.cmml"><mi id="S4.SS4.p3.5.m5.4.4.2.2.2.2.2" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2.2.cmml">t</mi><mn id="S4.SS4.p3.5.m5.4.4.2.2.2.2.3" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2.3.cmml">1</mn></msub><mo id="S4.SS4.p3.5.m5.6.6.4.4.4.7" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml">,</mo><msup id="S4.SS4.p3.5.m5.5.5.3.3.3.3" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.cmml"><mi id="S4.SS4.p3.5.m5.5.5.3.3.3.3.2" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.2.cmml">c</mi><mtext id="S4.SS4.p3.5.m5.5.5.3.3.3.3.3" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.3a.cmml">text</mtext></msup><mo id="S4.SS4.p3.5.m5.6.6.4.4.4.8" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml">,</mo><msup id="S4.SS4.p3.5.m5.6.6.4.4.4.4" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.cmml"><mi id="S4.SS4.p3.5.m5.6.6.4.4.4.4.2" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.2.cmml">c</mi><mtext id="S4.SS4.p3.5.m5.6.6.4.4.4.4.3" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.3a.cmml">au</mtext></msup><mo id="S4.SS4.p3.5.m5.6.6.4.4.4.9" stretchy="false" xref="S4.SS4.p3.5.m5.6.6.4.4.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.5.m5.6b"><apply id="S4.SS4.p3.5.m5.6.6.cmml" xref="S4.SS4.p3.5.m5.6.6"><eq id="S4.SS4.p3.5.m5.6.6.5.cmml" xref="S4.SS4.p3.5.m5.6.6.5"></eq><set id="S4.SS4.p3.5.m5.6.6.6.1.cmml" xref="S4.SS4.p3.5.m5.6.6.6.2"><ci id="S4.SS4.p3.5.m5.1.1.cmml" xref="S4.SS4.p3.5.m5.1.1">ℱ</ci><ci id="S4.SS4.p3.5.m5.2.2.cmml" xref="S4.SS4.p3.5.m5.2.2">𝒜</ci></set><apply id="S4.SS4.p3.5.m5.6.6.4.cmml" xref="S4.SS4.p3.5.m5.6.6.4"><times id="S4.SS4.p3.5.m5.6.6.4.5.cmml" xref="S4.SS4.p3.5.m5.6.6.4.5"></times><apply id="S4.SS4.p3.5.m5.6.6.4.6.cmml" xref="S4.SS4.p3.5.m5.6.6.4.6"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.6.6.4.6.1.cmml" xref="S4.SS4.p3.5.m5.6.6.4.6">subscript</csymbol><ci id="S4.SS4.p3.5.m5.6.6.4.6.2.cmml" xref="S4.SS4.p3.5.m5.6.6.4.6.2">italic-ϵ</ci><ci id="S4.SS4.p3.5.m5.6.6.4.6.3.cmml" xref="S4.SS4.p3.5.m5.6.6.4.6.3">𝜃</ci></apply><vector id="S4.SS4.p3.5.m5.6.6.4.4.5.cmml" xref="S4.SS4.p3.5.m5.6.6.4.4.4"><apply id="S4.SS4.p3.5.m5.3.3.1.1.1.1.cmml" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.3.3.1.1.1.1.1.cmml" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1">subscript</csymbol><ci id="S4.SS4.p3.5.m5.3.3.1.1.1.1.2.cmml" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1.2">𝑥</ci><cn id="S4.SS4.p3.5.m5.3.3.1.1.1.1.3.cmml" type="integer" xref="S4.SS4.p3.5.m5.3.3.1.1.1.1.3">1</cn></apply><apply id="S4.SS4.p3.5.m5.4.4.2.2.2.2.cmml" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.4.4.2.2.2.2.1.cmml" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2">subscript</csymbol><ci id="S4.SS4.p3.5.m5.4.4.2.2.2.2.2.cmml" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2.2">𝑡</ci><cn id="S4.SS4.p3.5.m5.4.4.2.2.2.2.3.cmml" type="integer" xref="S4.SS4.p3.5.m5.4.4.2.2.2.2.3">1</cn></apply><apply id="S4.SS4.p3.5.m5.5.5.3.3.3.3.cmml" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.5.5.3.3.3.3.1.cmml" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3">superscript</csymbol><ci id="S4.SS4.p3.5.m5.5.5.3.3.3.3.2.cmml" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.2">𝑐</ci><ci id="S4.SS4.p3.5.m5.5.5.3.3.3.3.3a.cmml" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.3"><mtext id="S4.SS4.p3.5.m5.5.5.3.3.3.3.3.cmml" mathsize="70%" xref="S4.SS4.p3.5.m5.5.5.3.3.3.3.3">text</mtext></ci></apply><apply id="S4.SS4.p3.5.m5.6.6.4.4.4.4.cmml" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4"><csymbol cd="ambiguous" id="S4.SS4.p3.5.m5.6.6.4.4.4.4.1.cmml" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4">superscript</csymbol><ci id="S4.SS4.p3.5.m5.6.6.4.4.4.4.2.cmml" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.2">𝑐</ci><ci id="S4.SS4.p3.5.m5.6.6.4.4.4.4.3a.cmml" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.3"><mtext id="S4.SS4.p3.5.m5.6.6.4.4.4.4.3.cmml" mathsize="70%" xref="S4.SS4.p3.5.m5.6.6.4.4.4.4.3">au</mtext></ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.5.m5.6c">\left\{\mathcal{F},\mathcal{A}\right\}=\epsilon_{\theta}(x_{1},t_{1},c^{\text{%
text}},c^{\text{au}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.5.m5.6d">{ caligraphic_F , caligraphic_A } = italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPT , italic_c start_POSTSUPERSCRIPT au end_POSTSUPERSCRIPT )</annotation></semantics></math>,
where <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S4.SS4.p3.6.m6.1"><semantics id="S4.SS4.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.6.m6.1.1" xref="S4.SS4.p3.6.m6.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.6.m6.1b"><ci id="S4.SS4.p3.6.m6.1.1.cmml" xref="S4.SS4.p3.6.m6.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.6.m6.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.6.m6.1d">caligraphic_F</annotation></semantics></math> and <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS4.p3.7.m7.1"><semantics id="S4.SS4.p3.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.7.m7.1.1" xref="S4.SS4.p3.7.m7.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.7.m7.1b"><ci id="S4.SS4.p3.7.m7.1.1.cmml" xref="S4.SS4.p3.7.m7.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.7.m7.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.7.m7.1d">caligraphic_A</annotation></semantics></math> denote the multi-scale feature representations and the cross-attention maps, respectively. <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S4.SS4.p3.8.m8.1"><semantics id="S4.SS4.p3.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.8.m8.1.1" xref="S4.SS4.p3.8.m8.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.8.m8.1b"><ci id="S4.SS4.p3.8.m8.1.1.cmml" xref="S4.SS4.p3.8.m8.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.8.m8.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.8.m8.1d">caligraphic_F</annotation></semantics></math> contains multi-scale feature maps from different layers of the U-Net <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.p3.9.m9.1"><semantics id="S4.SS4.p3.9.m9.1a"><msub id="S4.SS4.p3.9.m9.1.1" xref="S4.SS4.p3.9.m9.1.1.cmml"><mi id="S4.SS4.p3.9.m9.1.1.2" xref="S4.SS4.p3.9.m9.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.p3.9.m9.1.1.3" xref="S4.SS4.p3.9.m9.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.9.m9.1b"><apply id="S4.SS4.p3.9.m9.1.1.cmml" xref="S4.SS4.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.9.m9.1.1.1.cmml" xref="S4.SS4.p3.9.m9.1.1">subscript</csymbol><ci id="S4.SS4.p3.9.m9.1.1.2.cmml" xref="S4.SS4.p3.9.m9.1.1.2">italic-ϵ</ci><ci id="S4.SS4.p3.9.m9.1.1.3.cmml" xref="S4.SS4.p3.9.m9.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.9.m9.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.9.m9.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> with four different resolutions. <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS4.p3.10.m10.1"><semantics id="S4.SS4.p3.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.10.m10.1.1" xref="S4.SS4.p3.10.m10.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.10.m10.1b"><ci id="S4.SS4.p3.10.m10.1.1.cmml" xref="S4.SS4.p3.10.m10.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.10.m10.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.10.m10.1d">caligraphic_A</annotation></semantics></math> contains the cross-attention maps drawn from the 16 cross-attention blocks in <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S4.SS4.p3.11.m11.1"><semantics id="S4.SS4.p3.11.m11.1a"><msub id="S4.SS4.p3.11.m11.1.1" xref="S4.SS4.p3.11.m11.1.1.cmml"><mi id="S4.SS4.p3.11.m11.1.1.2" xref="S4.SS4.p3.11.m11.1.1.2.cmml">ϵ</mi><mi id="S4.SS4.p3.11.m11.1.1.3" xref="S4.SS4.p3.11.m11.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.11.m11.1b"><apply id="S4.SS4.p3.11.m11.1.1.cmml" xref="S4.SS4.p3.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS4.p3.11.m11.1.1.1.cmml" xref="S4.SS4.p3.11.m11.1.1">subscript</csymbol><ci id="S4.SS4.p3.11.m11.1.1.2.cmml" xref="S4.SS4.p3.11.m11.1.1.2">italic-ϵ</ci><ci id="S4.SS4.p3.11.m11.1.1.3.cmml" xref="S4.SS4.p3.11.m11.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.11.m11.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.11.m11.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>. Both the feature representation <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S4.SS4.p3.12.m12.1"><semantics id="S4.SS4.p3.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.12.m12.1.1" xref="S4.SS4.p3.12.m12.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.12.m12.1b"><ci id="S4.SS4.p3.12.m12.1.1.cmml" xref="S4.SS4.p3.12.m12.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.12.m12.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.12.m12.1d">caligraphic_F</annotation></semantics></math> and the cross-attention maps <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS4.p3.13.m13.1"><semantics id="S4.SS4.p3.13.m13.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p3.13.m13.1.1" xref="S4.SS4.p3.13.m13.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.13.m13.1b"><ci id="S4.SS4.p3.13.m13.1.1.cmml" xref="S4.SS4.p3.13.m13.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.13.m13.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.13.m13.1d">caligraphic_A</annotation></semantics></math> are regrouped according to their resolutions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Multi-scale Features and Attention Maps Fusion.</span>
Given that the multi-scale feature maps <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S4.SS4.p4.1.m1.1"><semantics id="S4.SS4.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><ci id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.1.m1.1d">caligraphic_F</annotation></semantics></math> capture global information essential for image generation, and the cross-attention maps provide class-discriminative information as well as relationships between object locations <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib73" title="">2022</a>); Caron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib9" title="">2021</a>)</cite>, FERAnno fuses both features and attention maps within a dual-branch encoder architecture for pseudo-label annotation. An overview of this architecture is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.F4" title="Figure 4 ‣ 4.4 Diffusion-based Label Calibrator (FERAnno) ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.5">We first compute the mean of the regrouped attention maps, denoted as <math alttext="\mathcal{A}_{\text{reg}}" class="ltx_Math" display="inline" id="S4.SS4.p5.1.m1.1"><semantics id="S4.SS4.p5.1.m1.1a"><msub id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.1.m1.1.1.2" xref="S4.SS4.p5.1.m1.1.1.2.cmml">𝒜</mi><mtext id="S4.SS4.p5.1.m1.1.1.3" xref="S4.SS4.p5.1.m1.1.1.3a.cmml">reg</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><apply id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p5.1.m1.1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p5.1.m1.1.1.2.cmml" xref="S4.SS4.p5.1.m1.1.1.2">𝒜</ci><ci id="S4.SS4.p5.1.m1.1.1.3a.cmml" xref="S4.SS4.p5.1.m1.1.1.3"><mtext id="S4.SS4.p5.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p5.1.m1.1.1.3">reg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">\mathcal{A}_{\text{reg}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.1.m1.1d">caligraphic_A start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT</annotation></semantics></math>, yielding a set of averaged attention maps <math alttext="\bar{\mathcal{A}}" class="ltx_Math" display="inline" id="S4.SS4.p5.2.m2.1"><semantics id="S4.SS4.p5.2.m2.1a"><mover accent="true" id="S4.SS4.p5.2.m2.1.1" xref="S4.SS4.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.2.m2.1.1.2" xref="S4.SS4.p5.2.m2.1.1.2.cmml">𝒜</mi><mo id="S4.SS4.p5.2.m2.1.1.1" xref="S4.SS4.p5.2.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.2.m2.1b"><apply id="S4.SS4.p5.2.m2.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1"><ci id="S4.SS4.p5.2.m2.1.1.1.cmml" xref="S4.SS4.p5.2.m2.1.1.1">¯</ci><ci id="S4.SS4.p5.2.m2.1.1.2.cmml" xref="S4.SS4.p5.2.m2.1.1.2">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.2.m2.1c">\bar{\mathcal{A}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.2.m2.1d">over¯ start_ARG caligraphic_A end_ARG</annotation></semantics></math>. Both the feature maps <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S4.SS4.p5.3.m3.1"><semantics id="S4.SS4.p5.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.3.m3.1.1" xref="S4.SS4.p5.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.3.m3.1b"><ci id="S4.SS4.p5.3.m3.1.1.cmml" xref="S4.SS4.p5.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.3.m3.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.3.m3.1d">caligraphic_F</annotation></semantics></math> and the averaged attention maps <math alttext="\bar{\mathcal{A}}" class="ltx_Math" display="inline" id="S4.SS4.p5.4.m4.1"><semantics id="S4.SS4.p5.4.m4.1a"><mover accent="true" id="S4.SS4.p5.4.m4.1.1" xref="S4.SS4.p5.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p5.4.m4.1.1.2" xref="S4.SS4.p5.4.m4.1.1.2.cmml">𝒜</mi><mo id="S4.SS4.p5.4.m4.1.1.1" xref="S4.SS4.p5.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.4.m4.1b"><apply id="S4.SS4.p5.4.m4.1.1.cmml" xref="S4.SS4.p5.4.m4.1.1"><ci id="S4.SS4.p5.4.m4.1.1.1.cmml" xref="S4.SS4.p5.4.m4.1.1.1">¯</ci><ci id="S4.SS4.p5.4.m4.1.1.2.cmml" xref="S4.SS4.p5.4.m4.1.1.2">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.4.m4.1c">\bar{\mathcal{A}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.4.m4.1d">over¯ start_ARG caligraphic_A end_ARG</annotation></semantics></math> are then passed through a residual convolution block to prepare them for further processing. To effectively integrate information at different scales, we introduce a bi-directional cross-attention block to fuse the features and attention maps. 1<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.p5.5.m5.1"><semantics id="S4.SS4.p5.5.m5.1a"><mo id="S4.SS4.p5.5.m5.1.1" xref="S4.SS4.p5.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.5.m5.1b"><times id="S4.SS4.p5.5.m5.1.1.cmml" xref="S4.SS4.p5.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.5.m5.1d">×</annotation></semantics></math>1 convolutions are employed at various stages to adapt the fusion across multiple resolution layers. Finally, the fused feature maps and attention maps are concatenated and passed through a linear layer, which outputs a probability vector for predicting facial expression classes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We conduct extensive experiments to evaluate both the generation quality of our synthetic data (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.SS1" title="5.1 Generation Quality ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5.1</span></a>) and its effectiveness in FER tasks (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.SS2" title="5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5.2</span></a>). For more details on experimental setup, implementation details are provided in the appendix.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Generation Quality</h3>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.8" style="width:433.6pt;height:112.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.0pt,7.4pt) scale(0.882186602589491,0.882186602589491) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.8.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.8.8.9.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.9.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="6" id="S5.T1.8.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.9.1.2.1">Objective Metrics</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S5.T1.8.8.9.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.9.1.3.1">User study (Ours vs. )(%)</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.1">FID (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.2.2.2.2">HPSv2(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.3.3.3.3">FS(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.3.m1.1a"><mo id="S5.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4">MPS (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.5.5.5.5">FER Acc.(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.5.5.5.5.m1.1"><semantics id="S5.T1.5.5.5.5.m1.1a"><mo id="S5.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S5.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.m1.1b"><ci id="S5.T1.5.5.5.5.m1.1.1.cmml" xref="S5.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.5.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.6.6.6.6">FAU Acc.(<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.6.6.6.6.m1.1"><semantics id="S5.T1.6.6.6.6.m1.1a"><mo id="S5.T1.6.6.6.6.m1.1.1" stretchy="false" xref="S5.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.6.m1.1b"><ci id="S5.T1.6.6.6.6.m1.1.1.cmml" xref="S5.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.6.6.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.7.7.7.7">EA (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.7.7.7.7.m1.1"><semantics id="S5.T1.7.7.7.7.m1.1a"><mo id="S5.T1.7.7.7.7.m1.1.1" stretchy="false" xref="S5.T1.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.7.m1.1b"><ci id="S5.T1.7.7.7.7.m1.1.1.cmml" xref="S5.T1.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.7.7.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.8.8.8.8">FF (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.8.8.8.8.m1.1"><semantics id="S5.T1.8.8.8.8.m1.1a"><mo id="S5.T1.8.8.8.8.m1.1.1" stretchy="false" xref="S5.T1.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.8.m1.1b"><ci id="S5.T1.8.8.8.8.m1.1.1.cmml" xref="S5.T1.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.8.8.m1.1d">↑</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.8.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.8.8.10.1.1">Stable Diffusion</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.2">88.40</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.3">0.263</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.4">2.01</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.5">2.00</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.6">20.06</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.7">87.72</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.8">2.86</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.8.8.10.1.9">1.79</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.8.11.2.1">PixelArt</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.2">145.23</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.3">0.271</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.4">3.79</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.5">5.26</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.6">15.52</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.7">84.57</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.8">24.26</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.11.2.9">10.00</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.8.12.3.1">PlayGround</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.2">81.76</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.3">0.265</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.4">2.86</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.5">3.73</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.6">21.56</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.7">87.28</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.8">7.50</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.12.3.9">5.00</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.8.8.13.4.1">FineFace</th>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.2">74.61</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.3">0.268</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.4">3.29</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.5">1.48</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.6">38.05</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.7">89.68</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.8">5.73</td>
<td class="ltx_td ltx_align_center" id="S5.T1.8.8.13.4.9">6.41</td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.14.5" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.1"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.1.1" style="background-color:#E6E6E6;">SynFER</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.2"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.2.1" style="background-color:#E6E6E6;">16.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.3"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.3.1" style="background-color:#E6E6E6;">0.280</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.4"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.4.1" style="background-color:#E6E6E6;">4.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.5"><span class="ltx_text" id="S5.T1.8.8.14.5.5.1" style="background-color:#E6E6E6;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.6"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.6.1" style="background-color:#E6E6E6;">55.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.7"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.7.1" style="background-color:#E6E6E6;">93.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.8"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.8.1" style="background-color:#E6E6E6;">59.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.8.8.14.5.9"><span class="ltx_text ltx_font_bold" id="S5.T1.8.8.14.5.9.1" style="background-color:#E6E6E6;">76.79</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ours vs.’ shows the proportion of users who prefer our method over the alternative. An MPS above 1.00 and results above 50% in the user study indicate our method outplays the counterpart. FS, FER Acc., FAU Acc., EA and FF denote FaceScore <cite class="ltx_cite ltx_citemacro_cite">Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib50" title="">2024</a>)</cite>, FER accuracy, facial action unit accuracy, expression alignment and face fidelity, respectively.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We present both objective metrics and subjective user studies, comparing our method to existing state-of-the-art (SOTA) diffusion models <cite class="ltx_cite ltx_citemacro_cite">Rombach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib67" title="">2022a</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib11" title="">2023a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib43" title="">2024</a>)</cite> and the latest facial expression generation technique, FineFace <cite class="ltx_cite ltx_citemacro_cite">Varanka et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib75" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We compute FID between the synthesis images and the test set of the AffectNet <cite class="ltx_cite ltx_citemacro_cite">Mollahosseini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib61" title="">2017</a>)</cite>.
HPSv2 <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib80" title="">2023b</a>)</cite> and MPS <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib88" title="">2024a</a>)</cite> evaluate the human preferences of the overall synthesis images, while FaceScore <cite class="ltx_cite ltx_citemacro_cite">Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib50" title="">2024</a>)</cite> measures the quality of the generated faces.
Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T1" title="Table 1 ‣ 5.1 Generation Quality ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a> shows that our method outperforms popular diffusion models and the SOTA facial expression generation method FineFace <cite class="ltx_cite ltx_citemacro_cite">Varanka et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib75" title="">2024</a>)</cite>, across all metrics of image quality, human preference and facial expression accuracy. Notably, the advantages of SynFER in both FE Acc. and AU Acc. indicate its outstanding controllability in facial expression generation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Effectiveness of Synthetic Dataset</h3>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T3.fig1" style="width:230.5pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.fig1.1" style="width:234.4pt;height:166.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.4pt,42.8pt) scale(0.66,0.66) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.fig1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.fig1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.fig1.1.1.1.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.fig1.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.1.1.2.1">Pre-train Data</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T3.fig1.1.1.1.1.3" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.1.1.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.1.1.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.1.1.3.1.1.1">RAF-DB</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T3.fig1.1.1.1.1.4" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.1.1.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.1.1.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.1.1.4.1.1.1">AffectNet</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S5.T3.fig1.1.1.1.1.5" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.1.1.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.1.1.5.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.1.1.5.1.1.1">SFEW</span></span>
</span>
</th>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.2.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S5.T3.fig1.1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.2.2.1.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.2.2.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.2.2.1.1.1.1">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S5.T3.fig1.1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.2.2.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.2.2.2.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.2.2.2.1.1.1">Scale</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.fig1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.fig1.1.1.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">MCF</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.fig1.1.1.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.3.1.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.3.1.2.1.1" style="width:71.1pt;">Laion-Face</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.fig1.1.1.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.3.1.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.3.1.3.1.1" style="width:42.7pt;">20M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.fig1.1.1.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.3.1.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.3.1.4.1.1" style="width:56.9pt;">65.22</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.fig1.1.1.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.3.1.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.3.1.5.1.1" style="width:56.9pt;">65.28</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S5.T3.fig1.1.1.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.3.1.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.3.1.6.1.1" style="width:56.9pt;">32.61</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">FRA</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.4.2.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.4.2.2.1.1" style="width:71.1pt;">VGGFace2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.4.2.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.4.2.3.1.1" style="width:42.7pt;">3.3M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.4.2.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.4.2.4.1.1" style="width:56.9pt;">73.89</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.4.2.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.4.2.5.1.1" style="width:56.9pt;">57.38</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.4.2.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.4.2.6.1.1" style="width:56.9pt;">-</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">PCL</th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.5.3.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.5.3.2.1.1" style="width:71.1pt;">VoxCeleb</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.5.3.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.5.3.3.1.1" style="width:42.7pt;">1.8M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.5.3.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.5.3.4.1.1" style="width:56.9pt;">74.47</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.5.3.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.5.3.5.1.1" style="width:56.9pt;">68.35</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.5.3.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.5.3.6.1.1" style="width:56.9pt;">39.68</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.fig1.1.1.6.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">SimCLR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.6.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.6.4.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.6.4.2.1.1" style="width:71.1pt;">AffectNet</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.6.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.6.4.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.6.4.3.1.1" style="width:42.7pt;">0.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.6.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.6.4.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.6.4.4.1.1" style="width:56.9pt;">78.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.6.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.6.4.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.6.4.5.1.1" style="width:56.9pt;">74.16</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.6.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.6.4.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.6.4.6.1.1" style="width:56.9pt;">46.79</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.7.5" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.7.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.7.5.1.1" style="background-color:#E6E6E6;">SimCLR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.7.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.7.5.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.7.5.2.1.1" style="width:71.1pt;">Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.7.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.7.5.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.7.5.3.1.1" style="width:42.7pt;">1.0M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.7.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.7.5.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.7.5.4.1.1" style="width:56.9pt;">80.24 <span class="ltx_text" id="S5.T3.fig1.1.1.7.5.4.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+1.59)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.7.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.7.5.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.7.5.5.1.1" style="width:56.9pt;">75.36 <span class="ltx_text" id="S5.T3.fig1.1.1.7.5.5.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+1.20)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.7.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.7.5.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.7.5.6.1.1" style="width:56.9pt;">47.62 <span class="ltx_text" id="S5.T3.fig1.1.1.7.5.6.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+0.83)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.8.6" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.8.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.8.6.1.1" style="background-color:#E6E6E6;">SimCLR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.8.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.8.6.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.8.6.2.1.1" style="width:71.1pt;">AffectNet+Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.8.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.8.6.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.8.6.3.1.1" style="width:42.7pt;">1.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.8.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.8.6.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.8.6.4.1.1" style="width:56.9pt;">81.52 <span class="ltx_text" id="S5.T3.fig1.1.1.8.6.4.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+2.87)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.8.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.8.6.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.8.6.5.1.1" style="width:56.9pt;">75.64 <span class="ltx_text" id="S5.T3.fig1.1.1.8.6.5.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+1.48)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.8.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.8.6.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.8.6.6.1.1" style="width:56.9pt;">48.52 <span class="ltx_text" id="S5.T3.fig1.1.1.8.6.6.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+1.73)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.fig1.1.1.9.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">BYOL</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.9.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.9.7.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.9.7.2.1.1" style="width:71.1pt;">AffectNet</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.9.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.9.7.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.9.7.3.1.1" style="width:42.7pt;">0.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.9.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.9.7.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.9.7.4.1.1" style="width:56.9pt;">78.24</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.9.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.9.7.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.9.7.5.1.1" style="width:56.9pt;">73.26</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.9.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.9.7.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.9.7.6.1.1" style="width:56.9pt;">48.70</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.10.8" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.10.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.10.8.1.1" style="background-color:#E6E6E6;">BYOL</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.10.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.10.8.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.10.8.2.1.1" style="width:71.1pt;">Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.10.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.10.8.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.10.8.3.1.1" style="width:42.7pt;">1.0M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.10.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.10.8.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.10.8.4.1.1" style="width:56.9pt;">80.96 <span class="ltx_text" id="S5.T3.fig1.1.1.10.8.4.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+2.72)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.10.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.10.8.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.10.8.5.1.1" style="width:56.9pt;">75.27 <span class="ltx_text" id="S5.T3.fig1.1.1.10.8.5.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+2.01)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.10.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.10.8.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.10.8.6.1.1" style="width:56.9pt;">51.35 <span class="ltx_text" id="S5.T3.fig1.1.1.10.8.6.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+2.65)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.11.9" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.11.9.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.11.9.1.1" style="background-color:#E6E6E6;">BYOL</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.11.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.11.9.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.11.9.2.1.1" style="width:71.1pt;">AffectNet+Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.11.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.11.9.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.11.9.3.1.1" style="width:42.7pt;">1.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.11.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.11.9.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.11.9.4.1.1" style="width:56.9pt;">81.25 <span class="ltx_text" id="S5.T3.fig1.1.1.11.9.4.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+3.01)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.11.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.11.9.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.11.9.5.1.1" style="width:56.9pt;">75.83 <span class="ltx_text" id="S5.T3.fig1.1.1.11.9.5.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+2.57)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.11.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.11.9.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.11.9.6.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.11.9.6.1.1.1">51.70</span> <span class="ltx_text" id="S5.T3.fig1.1.1.11.9.6.1.1.2" style="color:#0000FF;background-color:#E6E6E6;">(+3.00)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.fig1.1.1.12.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">MoCo v3</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.12.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.12.10.2.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.12.10.2.1.1" style="width:71.1pt;">AffectNet</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.12.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.12.10.3.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.12.10.3.1.1" style="width:42.7pt;">0.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.12.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.12.10.4.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.12.10.4.1.1" style="width:56.9pt;">79.05</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.12.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.12.10.5.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.12.10.5.1.1" style="width:56.9pt;">74.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S5.T3.fig1.1.1.12.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.12.10.6.1">
<span class="ltx_p" id="S5.T3.fig1.1.1.12.10.6.1.1" style="width:56.9pt;">49.34</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.13.11" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig1.1.1.13.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.13.11.1.1" style="background-color:#E6E6E6;">MoCo v3</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.13.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.13.11.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.13.11.2.1.1" style="width:71.1pt;">Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.13.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.13.11.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.13.11.3.1.1" style="width:42.7pt;">1.0M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.13.11.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.13.11.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.13.11.4.1.1" style="width:56.9pt;">81.17 <span class="ltx_text" id="S5.T3.fig1.1.1.13.11.4.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+2.12)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.13.11.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.13.11.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.13.11.5.1.1" style="width:56.9pt;">77.43 <span class="ltx_text" id="S5.T3.fig1.1.1.13.11.5.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+2.90)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S5.T3.fig1.1.1.13.11.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.13.11.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.13.11.6.1.1" style="width:56.9pt;">50.78 <span class="ltx_text" id="S5.T3.fig1.1.1.13.11.6.1.1.1" style="color:#FF0000;background-color:#E6E6E6;">(+1.44)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig1.1.1.14.12" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.fig1.1.1.14.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig1.1.1.14.12.1.1" style="background-color:#E6E6E6;">MoCo v3</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T3.fig1.1.1.14.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.14.12.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.14.12.2.1.1" style="width:71.1pt;">AffectNet+Ours</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T3.fig1.1.1.14.12.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.14.12.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.14.12.3.1.1" style="width:42.7pt;">1.2M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T3.fig1.1.1.14.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.14.12.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.14.12.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.14.12.4.1.1.1">81.68</span> <span class="ltx_text" id="S5.T3.fig1.1.1.14.12.4.1.1.2" style="color:#0000FF;background-color:#E6E6E6;">(+2.63)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T3.fig1.1.1.14.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.14.12.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.14.12.5.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig1.1.1.14.12.5.1.1.1">77.82</span> <span class="ltx_text" id="S5.T3.fig1.1.1.14.12.5.1.1.2" style="color:#0000FF;background-color:#E6E6E6;">(+3.29)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S5.T3.fig1.1.1.14.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.fig1.1.1.14.12.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="S5.T3.fig1.1.1.14.12.6.1.1" style="width:56.9pt;">51.26 <span class="ltx_text" id="S5.T3.fig1.1.1.14.12.6.1.1.1" style="color:#0000FF;background-color:#E6E6E6;">(+1.92)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Linear probe performance comparisons of SSL models on three FER datasets.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T3.fig2" style="width:151.0pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.fig2.1" style="width:144.8pt;height:135pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.1pt,22.5pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.fig2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.fig2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.fig2.1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig2.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.fig2.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig2.1.1.1.1.2.1">RAF-DB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.fig2.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig2.1.1.1.1.3.1">AffectNet</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.fig2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.fig2.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">ResNet-18</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.fig2.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">87.48</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.fig2.1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">50.32</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.3.2" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.3.2.1.1" style="background-color:#E6E6E6;">ResNet-18 + Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.3.2.2.1" style="background-color:#E6E6E6;">87.97</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.3.2.3.1" style="background-color:#E6E6E6;">51.65</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.4.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">Ada-DF</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">90.94</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.4.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">65.34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.5.4" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.5.4.1.1" style="background-color:#E6E6E6;">Ada-DF + Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.5.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.5.4.2.1" style="background-color:#E6E6E6;">91.21</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.5.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.5.4.3.1" style="background-color:#E6E6E6;">66.82</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">POSTER++</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.6.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">91.59</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.6.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">67.49</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.7.6" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.7.6.1.1" style="background-color:#E6E6E6;">POSTER++ + Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.7.6.2.1" style="background-color:#E6E6E6;">91.95</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.7.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.7.6.3.1" style="background-color:#E6E6E6;">69.04</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">APViT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">91.78</td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.8.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">66.94</td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.9.8" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.fig2.1.1.9.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.9.8.1.1" style="background-color:#E6E6E6;">APViT + Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.fig2.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.9.8.2.1" style="background-color:#E6E6E6;">92.05</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.fig2.1.1.9.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.9.8.3.1" style="background-color:#E6E6E6;">67.26</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.fig2.1.1.10.9" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.fig2.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T3.fig2.1.1.10.9.1.1" style="background-color:#E6E6E6;">FERAnno</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.fig2.1.1.10.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig2.1.1.10.9.2.1" style="background-color:#E6E6E6;">92.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.fig2.1.1.10.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.fig2.1.1.10.9.3.1" style="background-color:#E6E6E6;">70.38</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Comparison of supervised learning models (with and without our synthetic data) and the label calibrator FERAnno.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Self-supervised Representation Learning.</span>
We trained self-supervised learning (SSL) models, including BYOL <cite class="ltx_cite ltx_citemacro_cite">Grill et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib31" title="">2020</a>)</cite>, MoCo v3 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib14" title="">2021</a>)</cite>, and SimCLR <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib13" title="">2020</a>)</cite>, using real-world data, our synthetic data, and a combination of both. The linear probe performance of these models was evaluated on three widely used facial expression recognition (FER) datasets: RAF-DB <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib46" title="">2017</a>)</cite>, AffectNet <cite class="ltx_cite ltx_citemacro_cite">Mollahosseini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib61" title="">2017</a>)</cite>, and SFEW <cite class="ltx_cite ltx_citemacro_cite">Dhall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib20" title="">2011</a>)</cite>, with results reported in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T3" title="Table 3 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>. All SSL models were trained with a ResNet-50 architecture <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib34" title="">2016</a>)</cite>.
Notably, state-of-the-art methods in self-supervised facial representation learning, such as MCF <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib77" title="">2023</a>)</cite>, FRA <cite class="ltx_cite ltx_citemacro_cite">Gao &amp; Patras (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib29" title="">2024</a>)</cite>, and PCL <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib55" title="">2023c</a>)</cite>, were pre-trained on much larger face datasets like LAION-Face <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib93" title="">2022</a>)</cite>, VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib8" title="">2018</a>)</cite>, and VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">Nagrani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib62" title="">2020</a>)</cite>. However, these models underperformed on FER tasks compared to ours, highlighting that existing large-scale face datasets may lack the high-quality and diverse facial expression patterns required for accurate FER.
Results demonstrate that combining real-world and synthetic data consistently boosts SSL baselines. Remarkably, even when MoCo v3 was trained solely on our synthetic data, it achieved a 2.12% improvement on RAF-DB, underscoring the effectiveness of our approach in capturing critical facial expression details that are essential for FER.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Supervised Representation Learning.</span></p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T5.fig1" style="width:178.9pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T5.fig1.1" style="width:185.6pt;height:135pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.9pt,22.5pt) scale(0.75,0.75) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.fig1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.fig1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig1.1.1.1.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.fig1.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.1.1.2.1">CFEE_C</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.fig1.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.1.1.3.1">EmotionNet_C</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.fig1.1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.1.1.4.1">RAF_C</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.1.1">1-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.2.1">5-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.3.1">1-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.4.1">5-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.5.1">1-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.2.2.6.1">5-shot</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig1.1.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">InfoPatch</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.1.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">54.19</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T5.fig1.1.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">67.29</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.1.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">48.14</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T5.fig1.1.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">59.84</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.1.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">41.02</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.fig1.1.1.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">57.98</th>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.4.4" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig1.1.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.1.1" style="background-color:#E6E6E6;">InfoPatch*</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.2.1" style="background-color:#E6E6E6;">55.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.3.1" style="background-color:#E6E6E6;">68.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.4.1" style="background-color:#E6E6E6;">48.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.5.1" style="background-color:#E6E6E6;">61.16</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.6.1" style="background-color:#E6E6E6;">41.88</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.4.4.7.1" style="background-color:#E6E6E6;">59.54</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">LR+DC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">53.20</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">64.18</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">52.09</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">60.12</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">42.90</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">56.74</th>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.6.6" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig1.1.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.1.1" style="background-color:#E6E6E6;">LR+DC*</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.2.1" style="background-color:#E6E6E6;">54.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.3.1" style="background-color:#E6E6E6;">65.28</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.4.1" style="background-color:#E6E6E6;">51.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.5.1" style="background-color:#E6E6E6;">60.14</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.6.1" style="background-color:#E6E6E6;">43.87</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.6.6.7.1" style="background-color:#E6E6E6;">57.90</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">STARTUP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">54.89</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">67.79</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">52.61</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.7.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">61.95</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.7.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">43.97</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">59.14</th>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.8.8" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig1.1.1.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.1.1" style="background-color:#E6E6E6;">STARTUP*</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.2.1" style="background-color:#E6E6E6;">56.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.3.1" style="background-color:#E6E6E6;">69.93</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.4.1" style="background-color:#E6E6E6;">52.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig1.1.1.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.5.1" style="background-color:#E6E6E6;">62.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.6.1" style="background-color:#E6E6E6;">45.18</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig1.1.1.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.8.8.7.1" style="background-color:#E6E6E6;">61.23</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">CDNet</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.9.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">56.99</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">68.98</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.9.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">55.16</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.fig1.1.1.9.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.03</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.9.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">46.07</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.fig1.1.1.9.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">63.03</th>
</tr>
<tr class="ltx_tr" id="S5.T5.fig1.1.1.10.10" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.fig1.1.1.10.10.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig1.1.1.10.10.1.1" style="background-color:#E6E6E6;">CDNet*</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.1.1.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.2.1" style="background-color:#E6E6E6;">57.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.fig1.1.1.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.3.1" style="background-color:#E6E6E6;">70.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.1.1.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.4.1" style="background-color:#E6E6E6;">56.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.fig1.1.1.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.5.1" style="background-color:#E6E6E6;">65.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.1.1.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.6.1" style="background-color:#E6E6E6;">46.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig1.1.1.10.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig1.1.1.10.10.7.1" style="background-color:#E6E6E6;">64.34</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 4: </span>Performance comparisons with SOTA few-shot learning methods on 5-way few-shot FER tasks with a 95% confidence interval. (*) indicates training with both real-world data and our synthesis data.
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T5.fig2" style="width:198.7pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T5.fig2.1" style="width:217.0pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.1pt,18.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.fig2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.fig2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig2.1.1.1.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.1.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.fig2.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.1.1.2.1">RAF-DB</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.fig2.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.1.1.3.1">AffectNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T5.fig2.1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.1.1.4.1">FERPlus</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.1.1">UAR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.2.1">WAR</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.3.1">UAR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.4.1">WAR</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.5.1">UAR</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.2.2.6.1">WAR</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.fig2.1.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">FaRL</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.fig2.1.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">24.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.fig2.1.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">38.53</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.fig2.1.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">26.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.fig2.1.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">26.95</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.fig2.1.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">24.27</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.fig2.1.1.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">35.26</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig2.1.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">FLAVA</th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">14.35</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">38.69</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">14.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">14.26</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">12.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">28.43</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">CLIP ViT-B/16</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">38.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">36.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">34.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">34.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">34.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">45.14</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig2.1.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">Exp-CLIP ViT-B/16</th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">48.96</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">54.50</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">39.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">39.98</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">40.81</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">53.02</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.7.7" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig2.1.1.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.1.1" style="background-color:#E6E6E6;">Exp-CLIP* ViT-B/16</span></th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.2.1" style="background-color:#E6E6E6;">51.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.3.1" style="background-color:#E6E6E6;">56.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.4.1" style="background-color:#E6E6E6;">41.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.7.7.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.5.1" style="background-color:#E6E6E6;">41.68</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.7.7.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.6.1" style="background-color:#E6E6E6;">42.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.7.7.7.1" style="background-color:#E6E6E6;">56.02</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">CLIP ViT-L/14</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">47.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">41.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">34.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.fig2.1.1.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">34.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">33.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.fig2.1.1.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">46.67</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.fig2.1.1.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">Exp-CLIP ViT-L/14</th>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.9.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">58.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">65.37</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.9.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">44.27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.fig2.1.1.9.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">44.27</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.9.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">48.28</td>
<td class="ltx_td ltx_align_center" id="S5.T5.fig2.1.1.9.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">55.42</td>
</tr>
<tr class="ltx_tr" id="S5.T5.fig2.1.1.10.10" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.fig2.1.1.10.10.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S5.T5.fig2.1.1.10.10.1.1" style="background-color:#E6E6E6;">Exp-CLIP* ViT-L/14</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.1.1.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.2.1" style="background-color:#E6E6E6;">60.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.fig2.1.1.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.3.1" style="background-color:#E6E6E6;">68.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.1.1.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.4.1" style="background-color:#E6E6E6;">46.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.fig2.1.1.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.5.1" style="background-color:#E6E6E6;">46.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.1.1.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.6.1" style="background-color:#E6E6E6;">50.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.fig2.1.1.10.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.fig2.1.1.10.10.7.1" style="background-color:#E6E6E6;">57.98</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Zero-shot performance comparison of CLIP for FER, reporting both Weighted Average Recall (WAR) and Unweighted Average Recall (UAR) as in previous works <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib92" title="">2024</a>)</cite>. (*) indicates models trained with both real-world and synthetic data.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We validate the effectiveness of SynFER for supervised representation learning by evaluating its performance on RAF-DB and AffectNet (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T3" title="Table 3 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>). We compare with SOTA FER models, including Ada-DF <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib53" title="">2023a</a>)</cite>, POSTER++ <cite class="ltx_cite ltx_citemacro_cite">Mao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib59" title="">2024</a>)</cite>, and APViT <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib81" title="">2022</a>)</cite>. The results demonstrate that incorporating synthetic data consistently enhances both baseline models and the latest SOTAs in supervised facial expression recognition. Notably, APViT benefits from the synthetic data with improvements of 0.27% on RAF-DB and 0.32% on AffectNet.
While the improvements in supervised learning are more modest compared to self-supervised learning, they remain consistent. This is likely due to the stricter distribution alignment required in supervised learning between synthetic training data and real-world test data. In the following section on scaling behavior analysis, we provide further insights, showcasing the use of the distribution alignment technique, Real-Fake <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib86" title="">2023</a>)</cite>, to alleviate this problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Few-shot Learning.</span> Addressing the challenge of limited labeled FER data across different scenarios, we explore the potential of synthetic data to enhance few-shot learning, as presented in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T5" title="Table 5 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>. Following the protocol established by CDNet <cite class="ltx_cite ltx_citemacro_cite">Zou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib96" title="">2022</a>)</cite>, we train models on five basic expression datasets and evaluate them on three compound expression datasets: CFEE_C <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib22" title="">2014</a>)</cite>, EmotionNet_C <cite class="ltx_cite ltx_citemacro_cite">Fabian Benitez-Quiroz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib24" title="">2016</a>)</cite>, and RAF_C <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib46" title="">2017</a>)</cite>. To benchmark our approach, we compare it against SOTA few-shot learning methods, including InfoPatch <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib52" title="">2021</a>)</cite>, LR+DC <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib83" title="">2021</a>)</cite>, and STARTUP <cite class="ltx_cite ltx_citemacro_cite">Phoo &amp; Hariharan (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib65" title="">2021</a>)</cite>.
The results clearly demonstrate that integrating synthetic data consistently enhances few-shot FER performance across key metrics. This highlights the ability of synthetic data, with its broader range of FER patterns, to bridge the gap in data-limited scenarios, allowing models to better generalize to complex, real-world expressions in few-shot tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Multi-modal Fine-tuning.</span> The synthesis data encompasses multiple modalities, including generated images, textual prompts, and FER labels.
To assess its impact on multi-modal fine-tuning for FER, we focus on fine-tuning the vision-language foundation model CLIP <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib76" title="">2022</a>)</cite>, as its performance on face-related tasks is widely regarded as sub-optimal <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib33" title="">2023</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib10" title="">2024</a>)</cite>.
Building on Exp-CLIP <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib92" title="">2024</a>)</cite>, we fine-tune the models on CAER-S <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib41" title="">2019</a>)</cite> and evaluate their zero-shot performances on the datasets outlined in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T5" title="Table 5 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>.
Our results show that the inclusion of detailed textual prompts and a larger training image set significantly enhances the generalization ability of Exp-CLIP in understanding facial expressions, achieving significant improvements such as +2.58% UAR on the AffectNet dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Study</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Effectiveness of FAU Control.</span> We validate the effectiveness of SynFER by examining how Facial Action Units (FAUs) enhance the generation process and refine facial details. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F5" title="Figure 5 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>, samples generated with FAU control (third column) exhibit facial expressions that more accurately match their assigned labels compared to those generated with only text guidance (second column). For example, the ’fear’ expression, driven by FAUs like Inner Brow Raiser and Lip Stretcher, becomes more distinct (third column, second row), making it easier to differentiate from other emotions such as ’surprise.’ Similarly, ’disgust’ is more pronounced with FAUs like Lid Tightener. Without FAU control, facial expressions (second column) tend to blur, as different categories show overlapping features.
Quantitative results in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T6" title="Table 6 ‣ Figure 5 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a> highlight the impact of FAU control: FER accuracy increases from 34.62% to 48.74%, and FAU detection accuracy rises from 88.91% to 92.37%. This also translates into improved downstream performance on RAF-DB and AffectNet.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S5.F5.1" style="width:198.7pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="409" id="S5.F5.1.g1" src="x5.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Generated samples. The first and second rows are fear and disgust, respectively.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S5.F5.fig1" style="width:178.9pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S5.F5.fig1.1" style="width:190.5pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-63.5pt,21.6pt) scale(0.6,0.6) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.F5.fig1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F5.fig1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F5.fig1.1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.2.1">HPSv2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.3.1">FE Acc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F5.fig1.1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.4.1">AU Acc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.5.1">RAF-DB</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.1.1.6.1">AffectNet</span></td>
</tr>
<tr class="ltx_tr" id="S5.F5.fig1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F5.fig1.1.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Real-world Data</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F5.fig1.1.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">91.59</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F5.fig1.1.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">67.49</td>
</tr>
<tr class="ltx_tr" id="S5.F5.fig1.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F5.fig1.1.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">SD</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">0.263</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">20.06</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F5.fig1.1.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">87.72</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">89.42</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">65.36</td>
</tr>
<tr class="ltx_tr" id="S5.F5.fig1.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.F5.fig1.1.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">w/ FEText</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.fig1.1.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">0.267</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.fig1.1.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">34.62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.F5.fig1.1.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">88.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.fig1.1.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">90.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F5.fig1.1.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">66.62</td>
</tr>
<tr class="ltx_tr" id="S5.F5.fig1.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F5.fig1.1.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">w/ FEText+FAUs</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">0.275</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">48.74</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F5.fig1.1.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">92.37</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">91.68</td>
<td class="ltx_td ltx_align_center" id="S5.F5.fig1.1.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">67.68</td>
</tr>
<tr class="ltx_tr" id="S5.F5.fig1.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.F5.fig1.1.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">w/ FEText+FAUs+SG</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F5.fig1.1.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.6.6.2.1">0.280</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F5.fig1.1.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.6.6.3.1">55.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.F5.fig1.1.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.6.6.4.1">93.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F5.fig1.1.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.6.6.5.1">91.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F5.fig1.1.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.F5.fig1.1.1.6.6.6.1">68.13</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation study on the influence of AU injection and semantic guidance (SG) on both the generation quality and supervised representation learning. SD denotes Stable Diffusion , which is used as a baseline.</figcaption>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Effectiveness of Semantic Guidance.</span> We further explore the impact of semantic guidance (SG) on both generation quality and supervised representation learning, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F5" title="Figure 5 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a> and Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T6" title="Table 6 ‣ Figure 5 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>. By updating text embeddings to better align with the target facial expression category, SG improves the accuracy of the generated expressions by 6.4%, compared to static text and FAUs. The samples in the last column of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F5" title="Figure 5 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a> show more exaggerated facial expressions than those in the third column, with SG enhancing the intensity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Reliability of FERAnno.</span> We assess the reliability of FERAnno as a label calibrator by evaluating its performance on two FER datasets and visualizing its attention maps in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T3" title="Table 3 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F6" title="Figure 6 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>.
FERAnno significantly outperforms previous SOTAs, achieving a +0.51% improvement on RAF-DB and a +1.34% improvement on AffectNet over the second-best models.
The attention maps in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F6" title="Figure 6 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a> further demonstrate FERAnno’s ability to accurately locate facial expression-related facial features, such as jaw-dropping and furrowed eyebrows, highlighting the diffusion model’s great semantic understanding and fine-grained facial expression recognition.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="197" id="S5.F6.g1" src="extracted/5922699/images/attnmap-.png" width="148"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Synthesis images and attention maps in the fine-tuned diffusion model.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Synthetic Data Scaling Analysis.</span> Following <cite class="ltx_cite ltx_citemacro_cite">Tian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib74" title="">2024</a>); Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib25" title="">2024</a>)</cite>, we investigate the scaling behavior of synthetic data in both self-supervised and supervised learning paradigms.
To highlight the potential of synthetic FER data, we train models exclusively on synthetic images, without combining real-world data. The results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F7" title="Figure 7 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a> (a)-(b) show a stronger scaling effect in self-supervised learning compared to supervised learning, where performance improves significantly with more data.
This difference is likely due to the need for better distribution alignment in supervised learning <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib86" title="">2023</a>)</cite>. While SynFER focuses on addressing FER data scarcity, aligning the synthetic data distribution with real-world data is crucial for supervised tasks. To further explore this, we apply the Real-Fake technique <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib86" title="">2023</a>)</cite> for real and synthetic data distribution alignment, and present the results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.F7" title="Figure 7 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a> (c). Compared to standard supervised learning, Real-Fake demonstrates a clear performance boost.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="178" id="S5.F7.sf1.g1" src="x6.png" width="236"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="180" id="S5.F7.sf2.g1" src="x7.png" width="236"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="176" id="S5.F7.sf3.g1" src="x8.png" width="236"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Scaling up the synthetic FER dataset. MoCo v3 (ResNet-50) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib14" title="">2021</a>)</cite> is used for SSL pre-training, and linear probe performance is evaluated on AffectNet and RAF-DB. The SOTA FER model, POSTER++ <cite class="ltx_cite ltx_citemacro_cite">Mao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib59" title="">2024</a>)</cite>, is trained using supervised learning (with and without the Real-Fake technique <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib86" title="">2023</a>)</cite>) on our synthetic dataset and evaluated on the same two target FER datasets. ★ is model’s performance trained on corresponding real data.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we propose a synthesis data framework SynFER for facial expression recognition to address the data shortage in the field. By consolidating existing FER data and annotating with multi-modal large language models, we introduce the first facial expression-related image-text pair hybrid dataset FEText. We further propose to inject facial action unit information and external knowledge from existing FER models to ensure both fine-grained control and faithful generation of the facial expression images.
To incorporate the generated images into training, we propose a diffusion-based label calibrator to help rectify the robust facial expression annotations for the synthesized images. After constructing the data synthesis methodology, we investigate the effectiveness of the synthesis data across different learning paradigms, demonstrating consistent and superior performances. We further study the scaling behavior of the synthesis data for FER.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdat et al. (2011)</span>
<span class="ltx_bibblock">
Faiza Abdat, Choubeila Maaoui, and Alain Pruski.

</span>
<span class="ltx_bibblock">Human-computer interaction using emotion recognition from facial expression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2011 UKSim 5th European Symposium on Computer Modeling and Simulation</em>, pp.  196–201. IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Atapour-Abarghouei &amp; Breckon (2018)</span>
<span class="ltx_bibblock">
Amir Atapour-Abarghouei and Toby P Breckon.

</span>
<span class="ltx_bibblock">Real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2800–2810, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et al. (2023)</span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2304.08466</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baranchuk et al. (2021)</span>
<span class="ltx_bibblock">
Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko.

</span>
<span class="ltx_bibblock">Label-efficient semantic segmentation with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2112.03126</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barsoum et al. (2016)</span>
<span class="ltx_bibblock">
Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and Zhengyou Zhang.

</span>
<span class="ltx_bibblock">Training deep networks for facial expression recognition with crowd-sourced label distribution.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 18th ACM international conference on multimodal interaction</em>, pp.  279–283, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Betker et al. (2023)</span>
<span class="ltx_bibblock">
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.

</span>
<span class="ltx_bibblock">Improving image generation with better captions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf</em>, 2(3):8, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boutros et al. (2023)</span>
<span class="ltx_bibblock">
Fadi Boutros, Jonas Henry Grebe, Arjan Kuijper, and Naser Damer.

</span>
<span class="ltx_bibblock">Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  19650–19661, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2018)</span>
<span class="ltx_bibblock">
Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Vggface2: A dataset for recognising faces across pose and age.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2018 13th IEEE international conference on automatic face &amp; gesture recognition (FG 2018)</em>, pp.  67–74. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron et al. (2021)</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  9650–9660, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao.

</span>
<span class="ltx_bibblock">Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2407.02157</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al.

</span>
<span class="ltx_bibblock">Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2310.00426</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2311.12793</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International conference on machine learning</em>, pp.  1597–1607. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Xinlei Chen, Saining Xie, and Kaiming He.

</span>
<span class="ltx_bibblock">An empirical study of training self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  9640–9649, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  1841–1850, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2020)</span>
<span class="ltx_bibblock">
Bin Cheng, Inderjot Singh Saggu, Raunak Shah, Gaurav Bansal, and Dinesh Bharadia.

</span>
<span class="ltx_bibblock">S 3 net: Semantic-aware self-supervised depth estimation with monocular videos and synthetic data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">European Conference on Computer Vision</em>, pp.  52–69. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2024)</span>
<span class="ltx_bibblock">
Yunseong Cho, Chanwoo Kim, Hoseong Cho, Yunhoe Ku, Eunseo Kim, Muhammadjon Boboev, Joonseok Lee, and Seungryul Baek.

</span>
<span class="ltx_bibblock">Rmfer: Semi-supervised contrastive learning for facial expression recognition with reaction mashup video.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.  5913–5922, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Da Costa et al. (2022)</span>
<span class="ltx_bibblock">
Victor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci.

</span>
<span class="ltx_bibblock">solo-learn: A library of self-supervised methods for visual representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Journal of Machine Learning Research</em>, 23(56):1–6, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2009 IEEE conference on computer vision and pattern recognition</em>, pp.  248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhall et al. (2011)</span>
<span class="ltx_bibblock">
Abhinav Dhall, Roland Goecke, Simon Lucey, and Tom Gedeon.

</span>
<span class="ltx_bibblock">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2011 IEEE international conference on computer vision workshops (ICCV workshops)</em>, pp.  2106–2112. IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhall et al. (2017)</span>
<span class="ltx_bibblock">
Abhinav Dhall, Roland Goecke, Shreya Ghosh, Jyoti Joshi, Jesse Hoey, and Tom Gedeon.

</span>
<span class="ltx_bibblock">From individual to group-level emotion recognition: Emotiw 5.0.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 19th ACM international conference on multimodal interaction</em>, pp.  524–528, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2014)</span>
<span class="ltx_bibblock">
Shichuan Du, Yong Tao, and Aleix M Martinez.

</span>
<span class="ltx_bibblock">Compound facial expressions of emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the national academy of sciences</em>, 111(15):E1454–E1462, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman &amp; Friesen (1978)</span>
<span class="ltx_bibblock">
Paul Ekman and Wallace V Friesen.

</span>
<span class="ltx_bibblock">Facial action coding system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Environmental Psychology &amp; Nonverbal Behavior</em>, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabian Benitez-Quiroz et al. (2016)</span>
<span class="ltx_bibblock">
C Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M Martinez.

</span>
<span class="ltx_bibblock">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  5562–5570, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2024)</span>
<span class="ltx_bibblock">
Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian.

</span>
<span class="ltx_bibblock">Scaling laws of synthetic images for model training … for now.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  7382–7392, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farzaneh &amp; Qi (2021)</span>
<span class="ltx_bibblock">
Amir Hossein Farzaneh and Xiaojun Qi.

</span>
<span class="ltx_bibblock">Facial expression recognition in the wild via deep attentive center loss.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, pp.  2402–2411, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2023)</span>
<span class="ltx_bibblock">
Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma, and Hyung Jin Chang.

</span>
<span class="ltx_bibblock">Diffpose: Spatiotemporal diffusion model for video-based human pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  14861–14872, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frid-Adar et al. (2018)</span>
<span class="ltx_bibblock">
Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan.

</span>
<span class="ltx_bibblock">Synthetic data augmentation using gan for improved liver lesion classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)</em>, pp.  289–293. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao &amp; Patras (2024)</span>
<span class="ltx_bibblock">
Zheng Gao and Ioannis Patras.

</span>
<span class="ltx_bibblock">Self-supervised facial representation learning with facial region awareness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  2081–2092, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2020)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grill et al. (2020)</span>
<span class="ltx_bibblock">
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al.

</span>
<span class="ltx_bibblock">Bootstrap your own latent-a new approach to self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in neural information processing systems</em>, 33:21271–21284, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guizilini et al. (2022)</span>
<span class="ltx_bibblock">
Vitor Guizilini, Kuan-Hui Lee, Rareş Ambruş, and Adrien Gaidon.

</span>
<span class="ltx_bibblock">Learning optical flow, depth, and scene flow without real-world labels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Robotics and Automation Letters</em>, 7(2):3491–3498, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Jiayi Guo, Hayk Manukyan, Chenyu Yang, Chaofei Wang, Levon Khachatryan, Shant Navasardyan, Shiji Song, Humphrey Shi, and Gao Huang.

</span>
<span class="ltx_bibblock">Faceclip: Facial image-to-video translation via a brief text description.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">IEEE Transactions on Circuits and Systems for Video Technology</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</em>, 33:6840–6851, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Yuhao Huang, Jay Gopal, Bina Kakusa, Alice H Li, Weichen Huang, Jeffrey B Wang, Amit Persad, Ashwin Ramayya, Josef Parvizi, Vivek P Buch, et al.

</span>
<span class="ltx_bibblock">Naturalistic acute pain states decoded from neural and facial dynamics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">bioRxiv</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2017)</span>
<span class="ltx_bibblock">
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.

</span>
<span class="ltx_bibblock">Progressive growing of gans for improved quality, stability, and variation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:1710.10196</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2019)</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, and Timo Aila.

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  4401–4410, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu.

</span>
<span class="ltx_bibblock">Dcface: Synthetic face generation with dual condition diffusion model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  12715–12725, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2023)</span>
<span class="ltx_bibblock">
Nhat Le, Khanh Nguyen, Quang Tran, Erman Tjiputra, Bac Le, and Anh Nguyen.

</span>
<span class="ltx_bibblock">Uncertainty-aware label distribution learning for facial expression recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, pp.  6088–6097, January 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2019)</span>
<span class="ltx_bibblock">
Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, and Kwanghoon Sohn.

</span>
<span class="ltx_bibblock">Context-aware emotion recognition networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  10143–10152, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  21330–21340, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.

</span>
<span class="ltx_bibblock">Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2402.17245</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
Hangyu Li, Nannan Wang, Xi Yang, Xiaoyu Wang, and Xinbo Gao.

</span>
<span class="ltx_bibblock">Towards semi-supervised deep facial expression recognition with an adaptive confidence margin.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  4166–4175, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Deng (2020)</span>
<span class="ltx_bibblock">
Shan Li and Weihong Deng.

</span>
<span class="ltx_bibblock">Deep facial expression recognition: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE transactions on affective computing</em>, 13(3):1195–1215, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017)</span>
<span class="ltx_bibblock">
Shan Li, Weihong Deng, and JunPing Du.

</span>
<span class="ltx_bibblock">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2852–2861, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Ximan Li, Weihong Deng, Shan Li, and Yong Li.

</span>
<span class="ltx_bibblock">Compound expression recognition in-the-wild with au-assisted meta multi-task learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  5734–5743, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022c)</span>
<span class="ltx_bibblock">
Yingjian Li, Zheng Zhang, Bingzhi Chen, Guangming Lu, and David Zhang.

</span>
<span class="ltx_bibblock">Deep margin-sensitive representation learning for cross-domain facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">IEEE Transactions on Multimedia</em>, 25:1359–1373, 2022c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Open-vocabulary object segmentation with diffusion models.

</span>
<span class="ltx_bibblock">2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2024)</span>
<span class="ltx_bibblock">
Zhenyi Liao, Qingsong Xie, Chen Chen, Hannan Lu, and Zhijie Deng.

</span>
<span class="ltx_bibblock">Facescore: Benchmarking and enhancing face quality in human generation.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:270711518" title="">https://api.semanticscholar.org/CorpusID:270711518</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong.

</span>
<span class="ltx_bibblock">Diffbir: Towards blind image restoration with generative diffusion prior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2308.15070</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning a few-shot embedding model with contrastive learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume 35, pp.  8635–8643, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Shu Liu, Yan Xu, Tongming Wan, and Xiaoyan Kui.

</span>
<span class="ltx_bibblock">A dual-branch adaptive distribution fusion framework for real-world facial expression recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5. IEEE, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Yang Liu, Xingming Zhang, Janne Kauttonen, and Guoying Zhao.

</span>
<span class="ltx_bibblock">Uncertain facial expression recognition via multi-task assisted correction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">IEEE Transactions on Multimedia</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Yuanyuan Liu, Wenbin Wang, Yibing Zhan, Shaoze Feng, Kejun Liu, and Zhe Chen.

</span>
<span class="ltx_bibblock">Pose-disentangled contrastive learning for self-supervised facial representation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9717–9728, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucey et al. (2010)</span>
<span class="ltx_bibblock">
Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, and Iain Matthews.

</span>
<span class="ltx_bibblock">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">2010 ieee computer society conference on computer vision and pattern recognition-workshops</em>, pp.  94–101. IEEE, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2022)</span>
<span class="ltx_bibblock">
Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, and Hatice Gunes.

</span>
<span class="ltx_bibblock">Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</em>, pp.  1239–1246, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2023)</span>
<span class="ltx_bibblock">
Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa.

</span>
<span class="ltx_bibblock">Guided image synthesis via initial image editing in diffusion model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 31st ACM International Conference on Multimedia</em>, pp.  5321–5329, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Mao, Rui Xu, Xuesong Yin, Yuanqi Chang, Binling Nie, Aibin Huang, and Yigang Wang.

</span>
<span class="ltx_bibblock">Poster++: A simpler and stronger facial expression recognition network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Pattern Recognition</em>, pp.  110951, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moin et al. (2023)</span>
<span class="ltx_bibblock">
Anam Moin, Farhan Aadil, Zeeshan Ali, and Dongwann Kang.

</span>
<span class="ltx_bibblock">Emotion recognition framework using multiple modalities for an effective human–computer interaction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">The Journal of Supercomputing</em>, 79(8):9320–9349, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mollahosseini et al. (2017)</span>
<span class="ltx_bibblock">
Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor.

</span>
<span class="ltx_bibblock">Affectnet: A database for facial expression, valence, and arousal computing in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">IEEE Transactions on Affective Computing</em>, 10(1):18–31, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagrani et al. (2020)</span>
<span class="ltx_bibblock">
Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Voxceleb: Large-scale speaker verification in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Computer Speech &amp; Language</em>, 60:101027, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2023)</span>
<span class="ltx_bibblock">
Quang Nguyen, Truong Vu, Anh Tran, and Khoi Nguyen.

</span>
<span class="ltx_bibblock">Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pp.  76872–76892. Curran Associates, Inc., 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/f2957e48240c1d90e62b303574871b47-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/f2957e48240c1d90e62b303574871b47-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang.

</span>
<span class="ltx_bibblock">Effective real image editing with accelerated iterative diffusion inversion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  15912–15921, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phoo &amp; Hariharan (2021)</span>
<span class="ltx_bibblock">
Cheng Perng Phoo and Bharath Hariharan.

</span>
<span class="ltx_bibblock">Self-training for few-shot transfer across extreme task differences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=O3Y56aqpChA" title="">https://openreview.net/forum?id=O3Y56aqpChA</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ringeval et al. (2019)</span>
<span class="ltx_bibblock">
Fabien Ringeval, Björn Schuller, Michel Valstar, Nicholas Cummins, Roddy Cowie, Leili Tavabi, Maximilian Schmitt, Sina Alisamir, Shahin Amiriparian, Eva-Maria Messner, et al.

</span>
<span class="ltx_bibblock">Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop</em>, pp.  3–12, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022a)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022b)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al. (2022)</span>
<span class="ltx_bibblock">
Delian Ruan, Rongyun Mo, Yan Yan, Si Chen, Jing-Hao Xue, and Hanzi Wang.

</span>
<span class="ltx_bibblock">Adaptive deep disturbance-disentangled learning for facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">International Journal of Computer Vision</em>, 130(2):455–477, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sajjad et al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Sajjad, Fath U Min Ullah, Mohib Ullah, Georgia Christodoulou, Faouzi Alaya Cheikh, Mohammad Hijji, Khan Muhammad, and Joel JPC Rodrigues.

</span>
<span class="ltx_bibblock">A comprehensive survey on deep facial expression recognition: challenges, applications, and future guidelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Alexandria Engineering Journal</em>, 68:817–840, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Advances in Neural Information Processing Systems</em>, 35:25278–25294, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2020)</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Denoising diffusion implicit models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2010.02502</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture.

</span>
<span class="ltx_bibblock">What the daam: Interpreting stable diffusion using cross attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2210.04885</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2024)</span>
<span class="ltx_bibblock">
Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan.

</span>
<span class="ltx_bibblock">Stablerep: Synthetic images from text-to-image models make strong visual representation learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varanka et al. (2024)</span>
<span class="ltx_bibblock">
Tuomas Varanka, Huai-Qian Khor, Yante Li, Mengting Wei, Hanwei Kung, Nicu Sebe, and Guoying Zhao.

</span>
<span class="ltx_bibblock">Towards localized fine-grained control for facial expression generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2407.20175</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Suchen Wang, Yueqi Duan, Henghui Ding, Yap-Peng Tan, Kim-Hui Yap, and Junsong Yuan.

</span>
<span class="ltx_bibblock">Learning transferable human-object interaction detector with natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  939–948, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Liang Liu, Yabiao Wang, and Chengjie Wang.

</span>
<span class="ltx_bibblock">Toward high quality facial representation learning.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023a)</span>
<span class="ltx_bibblock">
Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  1206–1217, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Datasetdm: Synthesizing data with perception annotations using diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023b)</span>
<span class="ltx_bibblock">
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2306.09341</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2022)</span>
<span class="ltx_bibblock">
Fanglei Xue, Qiangchang Wang, Zichang Tan, Zhongsong Ma, and Guodong Guo.

</span>
<span class="ltx_bibblock">Vision transformer with attentive pooling for robust facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">IEEE Transactions on Affective Computing</em>, 14(4):3244–3256, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2022)</span>
<span class="ltx_bibblock">
Huan Yan, Yu Gu, Xiang Zhang, Yantong Wang, Yusheng Ji, and Fuji Ren.

</span>
<span class="ltx_bibblock">Mitigating label-noise for facial expression recognition in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">2022 IEEE International Conference on Multimedia and Expo (ICME)</em>, pp.  1–6, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICME52920.2022.9859818</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Shuo Yang, Lu Liu, and Min Xu.

</span>
<span class="ltx_bibblock">Free lunch for few-shot learning: Distribution calibration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=JWOiYxMG92s" title="">https://openreview.net/forum?id=JWOiYxMG92s</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.

</span>
<span class="ltx_bibblock">Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2308.06721</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Jun Yu, Zhongpeng Cai, Renda Li, Gongpeng Zhao, Guochen Xie, Jichao Zhu, Wangyuan Zhu, Qiang Ling, Lei Wang, Cong Wang, Luyu Qiu, and Wei Zheng.

</span>
<span class="ltx_bibblock">Exploring large-scale unlabeled faces to enhance facial expression recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, pp.  5803–5810, June 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2023)</span>
<span class="ltx_bibblock">
Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, and Bo Zhao.

</span>
<span class="ltx_bibblock">Real-fake: Effective training data synthesis through distribution matching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2310.10402</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2022)</span>
<span class="ltx_bibblock">
Dan Zeng, Zhiyuan Lin, Xiao Yan, Yuting Liu, Fei Wang, and Bo Tang.

</span>
<span class="ltx_bibblock">Face2exp: Combating data biases for facial expression recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  20291–20300, June 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang.

</span>
<span class="ltx_bibblock">Learning multi-dimensional human preference for text-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  8018–8027, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Yuhang Zhang, Chengrui Wang, and Weihong Deng.

</span>
<span class="ltx_bibblock">Relative uncertainty learning for facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Advances in Neural Information Processing Systems</em>, 34:17616–17627, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Yuhang Zhang, Yaqi Li, Xuannan Liu, Weihong Deng, et al.

</span>
<span class="ltx_bibblock">Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu.

</span>
<span class="ltx_bibblock">Inversion-based style transfer with diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10146–10156, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Zengqun Zhao, Yu Cao, Shaogang Gong, and Ioannis Patras.

</span>
<span class="ltx_bibblock">Enhancing zero-shot facial expression recognition by llm knowledge transfer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2022)</span>
<span class="ltx_bibblock">
Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen.

</span>
<span class="ltx_bibblock">General facial representation learning in a visual-linguistic manner.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  18697–18709, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Jieming Zhou, Tong Zhang, Zeeshan Hayder, Lars Petersson, and Mehrtash Harandi.

</span>
<span class="ltx_bibblock">Diff3dhpe: A diffusion model for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  2092–2102, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu &amp; Luo (2023)</span>
<span class="ltx_bibblock">
Qihao Zhu and Jianxi Luo.

</span>
<span class="ltx_bibblock">Toward artificial empathy for human-centered design: A framework.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</em>, volume 87318, pp.  V03BT03A072. American Society of Mechanical Engineers, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. (2022)</span>
<span class="ltx_bibblock">
Xinyi Zou, Yan Yan, Jing-Hao Xue, Si Chen, and Hanzi Wang.

</span>
<span class="ltx_bibblock">Learn-to-decompose: cascaded decomposition network for cross-domain few-shot facial expression recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">European Conference on Computer Vision</em>, pp.  683–700. Springer, 2022.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Hyper-parameter Studies</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.1">Step Size <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS1.p1.1.1.m1.1"><semantics id="A1.SS1.p1.1.1.m1.1a"><mi id="A1.SS1.p1.1.1.m1.1.1" xref="A1.SS1.p1.1.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.1.m1.1b"><ci id="A1.SS1.p1.1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.1.m1.1d">italic_λ</annotation></semantics></math> in Semantic Guidance.</span> For hyper-parameter analysis, we consider five configurations of the step size <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m1.1"><semantics id="A1.SS1.p1.2.m1.1a"><mi id="A1.SS1.p1.2.m1.1.1" xref="A1.SS1.p1.2.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m1.1b"><ci id="A1.SS1.p1.2.m1.1.1.cmml" xref="A1.SS1.p1.2.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.2.m1.1d">italic_λ</annotation></semantics></math> in semantic guidance.
Due to computational resource constraints, we provide results of self-supervised learning with MoCo v3 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib14" title="">2021</a>)</cite> on 0.2M synthetic data for pre-training and report the linear probe performances on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib46" title="">2017</a>)</cite>. Experiment results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.F8" title="Figure 8 ‣ A.1 Hyper-parameter Studies ‣ Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">8</span></a>.
It can be seen that when <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m2.1"><semantics id="A1.SS1.p1.3.m2.1a"><mi id="A1.SS1.p1.3.m2.1.1" xref="A1.SS1.p1.3.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m2.1b"><ci id="A1.SS1.p1.3.m2.1.1.cmml" xref="A1.SS1.p1.3.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.3.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.3.m2.1d">italic_λ</annotation></semantics></math> is relatively small, its influence on the performance is relatively small. However, as <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS1.p1.4.m3.1"><semantics id="A1.SS1.p1.4.m3.1a"><mi id="A1.SS1.p1.4.m3.1.1" xref="A1.SS1.p1.4.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.4.m3.1b"><ci id="A1.SS1.p1.4.m3.1.1.cmml" xref="A1.SS1.p1.4.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.4.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.4.m3.1d">italic_λ</annotation></semantics></math> continues to increase, the downstream performance is severely degraded. This is because an excessive <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS1.p1.5.m4.1"><semantics id="A1.SS1.p1.5.m4.1a"><mi id="A1.SS1.p1.5.m4.1.1" xref="A1.SS1.p1.5.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.5.m4.1b"><ci id="A1.SS1.p1.5.m4.1.1.cmml" xref="A1.SS1.p1.5.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.5.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.5.m4.1d">italic_λ</annotation></semantics></math> would lead to severely disrupted images, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.F9" title="Figure 9 ‣ A.1 Hyper-parameter Studies ‣ Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="345" id="A1.F8.g1" src="x9.png" width="456"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Hyper-parameter study on the <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.F8.2.m1.1"><semantics id="A1.F8.2.m1.1b"><mi id="A1.F8.2.m1.1.1" xref="A1.F8.2.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.F8.2.m1.1c"><ci id="A1.F8.2.m1.1.1.cmml" xref="A1.F8.2.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F8.2.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.F8.2.m1.1e">italic_λ</annotation></semantics></math> in semantic guidance. We report the linear probe performances of MoCo v3 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib14" title="">2021</a>)</cite> pre-trained with 0.2M synthetic data on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib46" title="">2017</a>)</cite>.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="A1.F9.g1" src="x10.png" width="457"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Visualizations of images generated with different values of <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.F9.2.m1.1"><semantics id="A1.F9.2.m1.1b"><mi id="A1.F9.2.m1.1.1" xref="A1.F9.2.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A1.F9.2.m1.1c"><ci id="A1.F9.2.m1.1.1.cmml" xref="A1.F9.2.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F9.2.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.F9.2.m1.1e">italic_λ</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Experiment Setting and Implementation Details</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p1.1.1">Self-Supervised Learning.</span>
We use the widely adopted self-supervised learning library solo-learn <cite class="ltx_cite ltx_citemacro_cite">Da Costa et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#bib.bib18" title="">2022</a>)</cite> for experiments and follow the default settings in solo-learn for various methods. Detailed settings are shown in the tables below:</p>
</div>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.1" style="width:433.6pt;height:131pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.9pt,15.9pt) scale(0.803950002745588,0.803950002745588) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T7.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.1.1">Config</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="A1.T7.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.2.1">Pre-Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.3.1">Linear Probe</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.2.2">
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.1.1">SimCLR</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.2.1">BYOL</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.3.1">MoCo v3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.4.1">SimCLR</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.5.1">BYOL</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.2.2.6.1">MoCo v3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T7.1.1.3.3.1">batch size</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.3.3.2">64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.3.3.3">64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.3.3.4">64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.3.3.5">32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.3.3.6">32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.3.3.7">32</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.1.1.4.4.1">optimizer</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.4.4.2">Lars</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.4.4.3">Lars</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.4.4.4">Lars</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.4.4.5">SGD</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.4.4.6">SGD</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.4.4.7">SGD</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.1.1.5.5.1">base learning rate</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.2">0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.3">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.5.5.4">0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.5">1e-3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.6">1e-3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.7">1e-3</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.1.1.6.6.1">weight decay</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.2">1e-4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.3">1e-6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.6.6.4">1e-6</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.5">1e-4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.6">1e-4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.7">1e-4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.1.1.7.7.1">learning rate schedule</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.7.7.2">warmup cosine</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.7.7.3">warmup cosine</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.7.7.4">warmup cosine</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.7.7.5">step (60,80)</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.7.7.6">steps (60,80)</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.7.7.7">steps (60,80)</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.1.1.8.8.1">epochs</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.8.8.2">200</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.8.8.3">200</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.8.8.4">200</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.8.8.5">100</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.8.8.6">100</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.8.8.7">100</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T7.1.1.9.9.1">augmentation</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.9.9.2">RRC</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.9.9.3">RRC</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.1.1.9.9.4">RRC</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.9.9.5">RRC+RHF</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.9.9.6">RRC+RHF</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.9.9.7">RRC+RHF</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Implementation details on self-supervised pre-training. RRC and RHF denote random resize crop and random horizontal flip, respectively.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">Others.</span> As all the methods for comparisons in supervised learning (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T3" title="Table 3 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>), few-shot learning (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T5" title="Table 5 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>) and multi-modal fine-tuning (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S5.T5" title="Table 5 ‣ 5.2 Effectiveness of Synthetic Dataset ‣ 5 Experiments ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a>) are open-source, we thus only need to rewrite the corresponding code for dataset reading to incorporate the synthetic data. We follow the default setting in each open-source code of the compared methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.1.1">Facial Action Unit Setting.</span>
We use pre-defined facial action unit (FAU) labels to generate images corresponding to specific facial expressions as shown below:</p>
</div>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.1">Facial Expression</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.2.1">FAU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T8.1.2.1.1">Happy</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.2.1.2">AU6 + AU12</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T8.1.3.2.1">Sad</th>
<td class="ltx_td ltx_align_center" id="A1.T8.1.3.2.2">AU1 + AU4 + AU15</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T8.1.4.3.1">Surprise</th>
<td class="ltx_td ltx_align_center" id="A1.T8.1.4.3.2">AU1 + AU2 + AU5 + AU26</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T8.1.5.4.1">Fear</th>
<td class="ltx_td ltx_align_center" id="A1.T8.1.5.4.2">AU1 + AU2 + AU4 + AU5 + AU7 + AU20 + AU26</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T8.1.6.5.1">Angry</th>
<td class="ltx_td ltx_align_center" id="A1.T8.1.6.5.2">AU4 + AU5 + AU7 + AU23</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T8.1.7.6.1">Disgust</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.7.6.2">AU9 + AU15 + AU16</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>FAU annotations to generate specific classes of facial expression images.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p4.1.1">Step of Performing Semantic Guidance.</span> During the synthesis process, the total denoising steps of the diffusion model are set as 50. Semantic guidance requires backward gradient computation, which would cost a large amount of GPU hours. Thereby, we only perform semantic guidance in the latter steps, which is set as the last 5 steps of the denoising.
Another reason to perform semantic guidance at the latter steps is that estimated results at early steps tend to be blurry and degraded facial images, performing semantic guidance on such images might to incorrect results.</p>
</div>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="396" id="A1.F10.g1" src="x11.png" width="609"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Synthetic images comparison between the over-smoothing images and the natural images.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Over-smoothing of Super Resolution Training Data</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.F10" title="Figure 10 ‣ A.2 Experiment Setting and Implementation Details ‣ Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">10</span></a>, we provide comparisons between the over-smoothing synthetic images and the more natural ones. Due to the large amount of super-resolution data in FEText, it can be seen that solely performing fine-tuning on the entire FEText significantly degrades the realism of the images, while the proposed two-stage fine-tuning strategies in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#S4.SS2.SSS2" title="4.2.2 Diffusion Model Fine-tuning ‣ 4.2 Diffusion Model Training for FER Data ‣ 4 Methodology ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2.2</span></a> could effectively prevent over-smoothing.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Limitations and future work</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">While the effectiveness of the proposed synthetic data framework has been demonstrated through extensive experiments, its current use is limited to augmenting the training set. A more efficient and optimized approach for leveraging synthetic data remains unexplored and warrants further investigation. Additionally, the generation process remains relatively slow, particularly when incorporating semantic guidance, which is crucial for ensuring accurate and faithful data generation. Moreover, this work focuses exclusively on facial expression recognition. However, it is important to note that the synthetic data framework has potential applications in other areas of facial affective computing, such as facial action unit detection and affective valence and arousal recognition. These avenues are left for future exploration.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>FEText</h3>
<div class="ltx_para ltx_noindent" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1">More examples from FEText are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09865v1#A1.F11" title="Figure 11 ‣ A.5 FEText ‣ Appendix A Appendix ‣ SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="639" id="A1.F11.g1" src="x12.png" width="532"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Examples from FEText.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct 13 14:54:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
