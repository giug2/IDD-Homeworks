<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised</title>
<!--Generated on Tue Sep 10 03:52:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.06197v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S1" title="In UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2" title="In UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS1" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Road Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS2" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Strong baseline: PLARD</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS3" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Depth Anything</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS4" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Meta Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS5" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Encoder Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S2.SS6" title="In 2 Related Work ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Intern Image</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S3" title="In UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>METHOD</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S3.SS1" title="In 3 METHOD ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S3.SS2" title="In 3 METHOD ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Multi-sources Encoder-Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S3.SS3" title="In 3 METHOD ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Meta Pseudo Labels</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4" title="In UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4.SS1" title="In 4 Experiments ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4.SS2" title="In 4 Experiments ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4.SS3" title="In 4 Experiments ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Connection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4.SS4" title="In 4 Experiments ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Quantitive Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S4.SS5" title="In 4 Experiments ‣ UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06197v1#S5" title="In UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\version</span>
<p class="ltx_p" id="p1.2">Technical Report

</p>
</div>
<h1 class="ltx_title ltx_title_document">UdeerLID+: Integrating LiDAR, Image, and Relative Depth with Semi-Supervised</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Ni 
<br class="ltx_break"/>UDeer.ai 
<br class="ltx_break"/>nitao@udeer.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin Zhan 
<br class="ltx_break"/>UDeer.ai 
<br class="ltx_break"/>zhanxin@udeer.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Luo 
<br class="ltx_break"/>UDeer.ai 
<br class="ltx_break"/>luotao@udeer.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenbin Liu 
<br class="ltx_break"/>isep 
<br class="ltx_break"/>wenbin@udeer.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhan Shi 
<br class="ltx_break"/>Zhejiang University 
<br class="ltx_break"/>shizhan@udeer.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">JunBo Chen 
<br class="ltx_break"/>UDeer.ai 
<br class="ltx_break"/>junbo@udeer.ai
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Road segmentation is a critical task for autonomous driving systems, requiring accurate and robust methods to classify road surfaces from various environmental data. Our work introduces an innovative approach that integrates LiDAR point cloud data, visual image, and relative depth maps derived from images.
The integration of multiple data sources in road segmentation presents both opportunities and challenges. One of the primary challenges is the scarcity of large-scale, accurately labeled datasets that are necessary for training robust deep learning models. To address this, we have developed the [UdeerLID+] framework under a semi-supervised learning paradigm.
Experiments results on KITTI datasets validate the superior performance.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Road segmentation, Multi sources, Semi-supervised.
</div>
<section class="ltx_section ltx_indent_first" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The accurate identification of road areas in urban environments is a crucial capability for autonomous driving systems. Reliable road detection is essential for enabling autonomous vehicles to navigate safely and efficiently in complex real-world scenarios. While traditional vision-based road detection methods have seen substantial progress, they often face significant challenges such as varying illumination, shadow occlusion, and motion blur, which can compromise segmentation accuracy and reliability.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To overcome these challenges, we propose a novel method that integrates LiDAR data, relative depth, and visual imagery. LiDAR is particularly advantageous due to its resilience against visual noise and its ability to provide precise spatial information. Meanwhile, relative depth complements sparse LiDAR point clouds by offering additional spatial context. The fusion of these modalities—LiDAR, depth, and visual data—offers a comprehensive framework to improve road detection by leveraging their individual strengths.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our method, Integrating LiDAR, Image, and Relative Depth with Semi-Supervised (UdeerLID+), introduces a two-step adaptation process to seamlessly integrate LiDAR information and Relative Depth into the visual domain.
Like PLARD[1], the first step involves Data Space Adaptation, where raw LiDAR data is transformed into a format that aligns with the 2D perspective of visual data. This is achieved by utilizing an altitude difference-based transformation that preserves the distinguishing characteristics of road surfaces within the LiDAR data.
The robust monocular depth is generated by Depth Anything[2].</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the semi-supervised learning framework of [UdeerLID+], the model is trained using a combination of labeled and unlabeled data. The labeled data provides direct supervision, helping the model learn the mapping from input features to road segmentation labels. The unlabeled data, on the other hand, is used to learn generalizable representations that can improve the model’s ability to generalize to new, unseen data.
In [3], the incorporation of meta-learning into the detection domain has inspired innovative frameworks for road segmentation tasks.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Extensive experiments on the KITTI road detection benchmark demonstrate that [UdeerLID+] surpasses state-of-the-art methods, significantly improving road detection accuracy across diverse urban conditions, including those characterized by challenging illumination.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In conclusion, [UdeerLID+] represents a substantial advancement in autonomous driving, offering a robust and precise solution for road detection by effectively integrating the complementary strengths of LiDAR, depth, and visual data.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection ltx_indent_first" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Road Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Road segmentation is a pivotal component for autonomous driving systems, focusing on the accurate identification and delineation of drivable road surfaces from environmental sensor data. This task is crucial for enabling vehicles to perceive their surroundings, make informed navigation decisions, and ensure safe path planning.
As PLARD[1], road segmentation is addressed by integrating and adapting LiDAR data with visual imagery to enhance the detection performance. The core objective of the road segmentation task is to assign binary labels to each pixel on the 2D image plane, distinguishing whether it corresponds to a road surface or a non-road area.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Strong baseline: PLARD</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">PLARD (Progressive LiDAR Adaptation-aided Road Detection[1] is designed for robust road segmentation in autonomous driving applications. It innovatively combines LiDAR data with visual imagery to enhance road detection accuracy and reliability.
The method employs a two-step adaptation process: first, it adapts the LiDAR data into the visual data space using an altitude difference-based transformation to align the perspectives and preserve road features; second, it adapts the LiDAR features to the visual features through a cascaded fusion structure within a deep learning framework, allowing for a more comprehensive and accurate representation of the road environment. This dual-modal integration and progressive adaptation approach enable PLARD to achieve superior performance in road detection tasks, even under challenging conditions such as varying lighting or occlusions.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Depth Anything</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Depth information, derived from various sensors such as LiDAR or stereo cameras, plays a crucial role in numerous applications across fields like autonomous driving, robotics, and augmented reality.
DepthAnything[2] provides a quantitative measurement of the distance between the sensor and objects in the environment, enabling systems to perceive the three-dimensional structure of their surroundings.
This understanding is vital for tasks such as obstacle detection, navigation, and scene reconstruction, where the precise estimation of depth helps in making informed decisions and actions based on the spatial relationships within the scene.
Depth data also enhances the capabilities of machine learning models by offering an additional layer of insight that can improve the accuracy and reliability of predictive and analytical tasks.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Meta Learning</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Meta learning[3], also known as ”learning to learn” is a subfield of machine learning that focuses on developing algorithms that can efficiently learn from small amounts of data by leveraging prior knowledge or experience gained from training on related tasks.
This approach enables models to quickly adapt to new problems with minimal data through techniques such as model agnostic meta learning (MAML), gradient-based meta learning, and reinforcement-based methods, thereby improving their generalization and flexibility across diverse tasks.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Encoder Decoder</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Encoder-Decoder based segmentation models are a class of deep learning architectures designed for pixel-wise image segmentation tasks, where an encoder captures and compresses spatial information from the input image into a latent feature representation, and a decoder subsequently up-samples and maps this representation back to the original image resolution to produce a segmentation mask.
These models[4][5][6] have been widely used in various segmentation domains, including semantic segmentation and instance segmentation, due to their ability to effectively model long-range dependencies in images.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Intern Image</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">InternImage[7] is a novel large-scale convolutional neural network (CNN) foundation model that leverages deformable convolutions as its core operator, allowing it to dynamically adapt its receptive field and perform adaptive spatial aggregation. This model not only captures long-range dependencies essential for tasks like detection and segmentation but also reduces the strict inductive bias of traditional CNNs. By scaling up the model parameters and training data similar to Vision Transformers (ViTs), InternImage achieves state-of-the-art performance on various challenging benchmarks, including ImageNet, COCO, and ADE20K, demonstrating the potential of CNN-based models in the era of large-scale vision foundation models.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>METHOD</h2>
<section class="ltx_subsection ltx_indent_first" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our UdeerLID+ is mainly construsted by two main components, multi-sources Encoder-Decoder based segmentator and Meta Pseudo Labels in the field of semantic segmentation. Both methods can effectively enhance performance metrics, and when employed in conjunction, they can lead to further improvements.
Below section would descript them in detail. The pipline of UdeerLID+ is shown in Fig. 1.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="276" id="S3.F1.g1" src="extracted/5844421/fig_1.png" width="548"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>UdeerLID+ Pipline</figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-sources Encoder-Decoder</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Images offer rich texture and color information that is crucial for recognizing and distinguishing various objects and scenes, providing visual cues that are invaluable for tasks such as classification and object detection.
LiDAR point clouds deliver precise three-dimensional spatial data with accurate distance measurements, making them robust against lighting variations and capable of capturing the geometric structure of the environment, which is essential for tasks like 3D object detection and segmentation.
Relative depth maps, derived from images, provide an additional layer of depth perception that complements monocular vision by estimating the distance of objects within the scene, thus enhancing the understanding of the scene’s layout and facilitating tasks such as depth estimation and semantic segmentation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">In our framework, image data, LiDAR point clouds, and relative depth maps synergistically contribute their unique strengths.
Like Fig. 1, each modality has its dedicated encoder, which processes the input data and extracts distinctive features. Furthermore, an auxiliary loss function is applied at the output layer of each encoder to ensure that each modality can independently perform the road segmentation task (<math alttext="Loss_{image}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">o</mi><mo id="S3.SS2.p2.1.m1.1.1.1a" xref="S3.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.4" xref="S3.SS2.p2.1.m1.1.1.4.cmml">s</mi><mo id="S3.SS2.p2.1.m1.1.1.1b" xref="S3.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.1.m1.1.1.5" xref="S3.SS2.p2.1.m1.1.1.5.cmml"><mi id="S3.SS2.p2.1.m1.1.1.5.2" xref="S3.SS2.p2.1.m1.1.1.5.2.cmml">s</mi><mrow id="S3.SS2.p2.1.m1.1.1.5.3" xref="S3.SS2.p2.1.m1.1.1.5.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.5.3.2" xref="S3.SS2.p2.1.m1.1.1.5.3.2.cmml">i</mi><mo id="S3.SS2.p2.1.m1.1.1.5.3.1" xref="S3.SS2.p2.1.m1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.5.3.3" xref="S3.SS2.p2.1.m1.1.1.5.3.3.cmml">m</mi><mo id="S3.SS2.p2.1.m1.1.1.5.3.1a" xref="S3.SS2.p2.1.m1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.5.3.4" xref="S3.SS2.p2.1.m1.1.1.5.3.4.cmml">a</mi><mo id="S3.SS2.p2.1.m1.1.1.5.3.1b" xref="S3.SS2.p2.1.m1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.5.3.5" xref="S3.SS2.p2.1.m1.1.1.5.3.5.cmml">g</mi><mo id="S3.SS2.p2.1.m1.1.1.5.3.1c" xref="S3.SS2.p2.1.m1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.5.3.6" xref="S3.SS2.p2.1.m1.1.1.5.3.6.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐿</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑜</ci><ci id="S3.SS2.p2.1.m1.1.1.4.cmml" xref="S3.SS2.p2.1.m1.1.1.4">𝑠</ci><apply id="S3.SS2.p2.1.m1.1.1.5.cmml" xref="S3.SS2.p2.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.5.1.cmml" xref="S3.SS2.p2.1.m1.1.1.5">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.5.2.cmml" xref="S3.SS2.p2.1.m1.1.1.5.2">𝑠</ci><apply id="S3.SS2.p2.1.m1.1.1.5.3.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3"><times id="S3.SS2.p2.1.m1.1.1.5.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.5.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.2">𝑖</ci><ci id="S3.SS2.p2.1.m1.1.1.5.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.3">𝑚</ci><ci id="S3.SS2.p2.1.m1.1.1.5.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.4">𝑎</ci><ci id="S3.SS2.p2.1.m1.1.1.5.3.5.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.5">𝑔</ci><ci id="S3.SS2.p2.1.m1.1.1.5.3.6.cmml" xref="S3.SS2.p2.1.m1.1.1.5.3.6">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">Loss_{image}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_i italic_m italic_a italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="Loss_{LiDAR}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">o</mi><mo id="S3.SS2.p2.2.m2.1.1.1a" xref="S3.SS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.4" xref="S3.SS2.p2.2.m2.1.1.4.cmml">s</mi><mo id="S3.SS2.p2.2.m2.1.1.1b" xref="S3.SS2.p2.2.m2.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.2.m2.1.1.5" xref="S3.SS2.p2.2.m2.1.1.5.cmml"><mi id="S3.SS2.p2.2.m2.1.1.5.2" xref="S3.SS2.p2.2.m2.1.1.5.2.cmml">s</mi><mrow id="S3.SS2.p2.2.m2.1.1.5.3" xref="S3.SS2.p2.2.m2.1.1.5.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.5.3.2" xref="S3.SS2.p2.2.m2.1.1.5.3.2.cmml">L</mi><mo id="S3.SS2.p2.2.m2.1.1.5.3.1" xref="S3.SS2.p2.2.m2.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.5.3.3" xref="S3.SS2.p2.2.m2.1.1.5.3.3.cmml">i</mi><mo id="S3.SS2.p2.2.m2.1.1.5.3.1a" xref="S3.SS2.p2.2.m2.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.5.3.4" xref="S3.SS2.p2.2.m2.1.1.5.3.4.cmml">D</mi><mo id="S3.SS2.p2.2.m2.1.1.5.3.1b" xref="S3.SS2.p2.2.m2.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.5.3.5" xref="S3.SS2.p2.2.m2.1.1.5.3.5.cmml">A</mi><mo id="S3.SS2.p2.2.m2.1.1.5.3.1c" xref="S3.SS2.p2.2.m2.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.5.3.6" xref="S3.SS2.p2.2.m2.1.1.5.3.6.cmml">R</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝐿</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑜</ci><ci id="S3.SS2.p2.2.m2.1.1.4.cmml" xref="S3.SS2.p2.2.m2.1.1.4">𝑠</ci><apply id="S3.SS2.p2.2.m2.1.1.5.cmml" xref="S3.SS2.p2.2.m2.1.1.5"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.5.1.cmml" xref="S3.SS2.p2.2.m2.1.1.5">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.5.2.cmml" xref="S3.SS2.p2.2.m2.1.1.5.2">𝑠</ci><apply id="S3.SS2.p2.2.m2.1.1.5.3.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3"><times id="S3.SS2.p2.2.m2.1.1.5.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.5.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.2">𝐿</ci><ci id="S3.SS2.p2.2.m2.1.1.5.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.3">𝑖</ci><ci id="S3.SS2.p2.2.m2.1.1.5.3.4.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.4">𝐷</ci><ci id="S3.SS2.p2.2.m2.1.1.5.3.5.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.5">𝐴</ci><ci id="S3.SS2.p2.2.m2.1.1.5.3.6.cmml" xref="S3.SS2.p2.2.m2.1.1.5.3.6">𝑅</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">Loss_{LiDAR}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_L italic_i italic_D italic_A italic_R end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="Loss_{depth}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">o</mi><mo id="S3.SS2.p2.3.m3.1.1.1a" xref="S3.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.4" xref="S3.SS2.p2.3.m3.1.1.4.cmml">s</mi><mo id="S3.SS2.p2.3.m3.1.1.1b" xref="S3.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.3.m3.1.1.5" xref="S3.SS2.p2.3.m3.1.1.5.cmml"><mi id="S3.SS2.p2.3.m3.1.1.5.2" xref="S3.SS2.p2.3.m3.1.1.5.2.cmml">s</mi><mrow id="S3.SS2.p2.3.m3.1.1.5.3" xref="S3.SS2.p2.3.m3.1.1.5.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.5.3.2" xref="S3.SS2.p2.3.m3.1.1.5.3.2.cmml">d</mi><mo id="S3.SS2.p2.3.m3.1.1.5.3.1" xref="S3.SS2.p2.3.m3.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.5.3.3" xref="S3.SS2.p2.3.m3.1.1.5.3.3.cmml">e</mi><mo id="S3.SS2.p2.3.m3.1.1.5.3.1a" xref="S3.SS2.p2.3.m3.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.5.3.4" xref="S3.SS2.p2.3.m3.1.1.5.3.4.cmml">p</mi><mo id="S3.SS2.p2.3.m3.1.1.5.3.1b" xref="S3.SS2.p2.3.m3.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.5.3.5" xref="S3.SS2.p2.3.m3.1.1.5.3.5.cmml">t</mi><mo id="S3.SS2.p2.3.m3.1.1.5.3.1c" xref="S3.SS2.p2.3.m3.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.1.1.5.3.6" xref="S3.SS2.p2.3.m3.1.1.5.3.6.cmml">h</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝐿</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">𝑜</ci><ci id="S3.SS2.p2.3.m3.1.1.4.cmml" xref="S3.SS2.p2.3.m3.1.1.4">𝑠</ci><apply id="S3.SS2.p2.3.m3.1.1.5.cmml" xref="S3.SS2.p2.3.m3.1.1.5"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.5.1.cmml" xref="S3.SS2.p2.3.m3.1.1.5">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.5.2.cmml" xref="S3.SS2.p2.3.m3.1.1.5.2">𝑠</ci><apply id="S3.SS2.p2.3.m3.1.1.5.3.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3"><times id="S3.SS2.p2.3.m3.1.1.5.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.1"></times><ci id="S3.SS2.p2.3.m3.1.1.5.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.2">𝑑</ci><ci id="S3.SS2.p2.3.m3.1.1.5.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.3">𝑒</ci><ci id="S3.SS2.p2.3.m3.1.1.5.3.4.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.4">𝑝</ci><ci id="S3.SS2.p2.3.m3.1.1.5.3.5.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.5">𝑡</ci><ci id="S3.SS2.p2.3.m3.1.1.5.3.6.cmml" xref="S3.SS2.p2.3.m3.1.1.5.3.6">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">Loss_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math>), thereby enhancing the overall performance and robustness of our multi-modal framework.
In the decoder phase of our framework, the image features take the central role, which are then augmented by integrating the upsampled features from LiDAR and depth data. This fusion process amalgamates the complementary strengths of all three sensor modalities, culminating in an enriched feature representation that encapsulates the multi-sensory information.
The final segmentation <math alttext="Loss_{fine}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">o</mi><mo id="S3.SS2.p2.4.m4.1.1.1a" xref="S3.SS2.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.1.1.4" xref="S3.SS2.p2.4.m4.1.1.4.cmml">s</mi><mo id="S3.SS2.p2.4.m4.1.1.1b" xref="S3.SS2.p2.4.m4.1.1.1.cmml">⁢</mo><msub id="S3.SS2.p2.4.m4.1.1.5" xref="S3.SS2.p2.4.m4.1.1.5.cmml"><mi id="S3.SS2.p2.4.m4.1.1.5.2" xref="S3.SS2.p2.4.m4.1.1.5.2.cmml">s</mi><mrow id="S3.SS2.p2.4.m4.1.1.5.3" xref="S3.SS2.p2.4.m4.1.1.5.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.5.3.2" xref="S3.SS2.p2.4.m4.1.1.5.3.2.cmml">f</mi><mo id="S3.SS2.p2.4.m4.1.1.5.3.1" xref="S3.SS2.p2.4.m4.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.1.1.5.3.3" xref="S3.SS2.p2.4.m4.1.1.5.3.3.cmml">i</mi><mo id="S3.SS2.p2.4.m4.1.1.5.3.1a" xref="S3.SS2.p2.4.m4.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.1.1.5.3.4" xref="S3.SS2.p2.4.m4.1.1.5.3.4.cmml">n</mi><mo id="S3.SS2.p2.4.m4.1.1.5.3.1b" xref="S3.SS2.p2.4.m4.1.1.5.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.1.1.5.3.5" xref="S3.SS2.p2.4.m4.1.1.5.3.5.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝐿</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑜</ci><ci id="S3.SS2.p2.4.m4.1.1.4.cmml" xref="S3.SS2.p2.4.m4.1.1.4">𝑠</ci><apply id="S3.SS2.p2.4.m4.1.1.5.cmml" xref="S3.SS2.p2.4.m4.1.1.5"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.5.1.cmml" xref="S3.SS2.p2.4.m4.1.1.5">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.5.2.cmml" xref="S3.SS2.p2.4.m4.1.1.5.2">𝑠</ci><apply id="S3.SS2.p2.4.m4.1.1.5.3.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3"><times id="S3.SS2.p2.4.m4.1.1.5.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3.1"></times><ci id="S3.SS2.p2.4.m4.1.1.5.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3.2">𝑓</ci><ci id="S3.SS2.p2.4.m4.1.1.5.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3.3">𝑖</ci><ci id="S3.SS2.p2.4.m4.1.1.5.3.4.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3.4">𝑛</ci><ci id="S3.SS2.p2.4.m4.1.1.5.3.5.cmml" xref="S3.SS2.p2.4.m4.1.1.5.3.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">Loss_{fine}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_f italic_i italic_n italic_e end_POSTSUBSCRIPT</annotation></semantics></math> is computed on the merged features, and the total loss is formulated as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Loss=Loss_{fine}+\alpha\cdot Loss_{image}+\beta\cdot Loss_{LiDAR}+\gamma\cdot
Loss%
_{depth}" class="ltx_Math" display="block" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.2.2" xref="S3.Ex1.m1.1.1.2.2.cmml">L</mi><mo id="S3.Ex1.m1.1.1.2.1" xref="S3.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.2.3" xref="S3.Ex1.m1.1.1.2.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.2.1a" xref="S3.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.2.4" xref="S3.Ex1.m1.1.1.2.4.cmml">s</mi><mo id="S3.Ex1.m1.1.1.2.1b" xref="S3.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.2.5" xref="S3.Ex1.m1.1.1.2.5.cmml">s</mi></mrow><mo id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mrow id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml"><mi id="S3.Ex1.m1.1.1.3.2.2" xref="S3.Ex1.m1.1.1.3.2.2.cmml">L</mi><mo id="S3.Ex1.m1.1.1.3.2.1" xref="S3.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.3" xref="S3.Ex1.m1.1.1.3.2.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.3.2.1a" xref="S3.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.4" xref="S3.Ex1.m1.1.1.3.2.4.cmml">s</mi><mo id="S3.Ex1.m1.1.1.3.2.1b" xref="S3.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.3.2.5" xref="S3.Ex1.m1.1.1.3.2.5.cmml"><mi id="S3.Ex1.m1.1.1.3.2.5.2" xref="S3.Ex1.m1.1.1.3.2.5.2.cmml">s</mi><mrow id="S3.Ex1.m1.1.1.3.2.5.3" xref="S3.Ex1.m1.1.1.3.2.5.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2.5.3.2" xref="S3.Ex1.m1.1.1.3.2.5.3.2.cmml">f</mi><mo id="S3.Ex1.m1.1.1.3.2.5.3.1" xref="S3.Ex1.m1.1.1.3.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.5.3.3" xref="S3.Ex1.m1.1.1.3.2.5.3.3.cmml">i</mi><mo id="S3.Ex1.m1.1.1.3.2.5.3.1a" xref="S3.Ex1.m1.1.1.3.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.5.3.4" xref="S3.Ex1.m1.1.1.3.2.5.3.4.cmml">n</mi><mo id="S3.Ex1.m1.1.1.3.2.5.3.1b" xref="S3.Ex1.m1.1.1.3.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.5.3.5" xref="S3.Ex1.m1.1.1.3.2.5.3.5.cmml">e</mi></mrow></msub></mrow><mo id="S3.Ex1.m1.1.1.3.1" xref="S3.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml"><mrow id="S3.Ex1.m1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.3.3.2.cmml"><mi id="S3.Ex1.m1.1.1.3.3.2.2" xref="S3.Ex1.m1.1.1.3.3.2.2.cmml">α</mi><mo id="S3.Ex1.m1.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.3.3.2.1.cmml">⋅</mo><mi id="S3.Ex1.m1.1.1.3.3.2.3" xref="S3.Ex1.m1.1.1.3.3.2.3.cmml">L</mi></mrow><mo id="S3.Ex1.m1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.3.3.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.3.3.1a" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.4" xref="S3.Ex1.m1.1.1.3.3.4.cmml">s</mi><mo id="S3.Ex1.m1.1.1.3.3.1b" xref="S3.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.3.3.5" xref="S3.Ex1.m1.1.1.3.3.5.cmml"><mi id="S3.Ex1.m1.1.1.3.3.5.2" xref="S3.Ex1.m1.1.1.3.3.5.2.cmml">s</mi><mrow id="S3.Ex1.m1.1.1.3.3.5.3" xref="S3.Ex1.m1.1.1.3.3.5.3.cmml"><mi id="S3.Ex1.m1.1.1.3.3.5.3.2" xref="S3.Ex1.m1.1.1.3.3.5.3.2.cmml">i</mi><mo id="S3.Ex1.m1.1.1.3.3.5.3.1" xref="S3.Ex1.m1.1.1.3.3.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.5.3.3" xref="S3.Ex1.m1.1.1.3.3.5.3.3.cmml">m</mi><mo id="S3.Ex1.m1.1.1.3.3.5.3.1a" xref="S3.Ex1.m1.1.1.3.3.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.5.3.4" xref="S3.Ex1.m1.1.1.3.3.5.3.4.cmml">a</mi><mo id="S3.Ex1.m1.1.1.3.3.5.3.1b" xref="S3.Ex1.m1.1.1.3.3.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.5.3.5" xref="S3.Ex1.m1.1.1.3.3.5.3.5.cmml">g</mi><mo id="S3.Ex1.m1.1.1.3.3.5.3.1c" xref="S3.Ex1.m1.1.1.3.3.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.3.5.3.6" xref="S3.Ex1.m1.1.1.3.3.5.3.6.cmml">e</mi></mrow></msub></mrow><mo id="S3.Ex1.m1.1.1.3.1a" xref="S3.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.3.4" xref="S3.Ex1.m1.1.1.3.4.cmml"><mrow id="S3.Ex1.m1.1.1.3.4.2" xref="S3.Ex1.m1.1.1.3.4.2.cmml"><mi id="S3.Ex1.m1.1.1.3.4.2.2" xref="S3.Ex1.m1.1.1.3.4.2.2.cmml">β</mi><mo id="S3.Ex1.m1.1.1.3.4.2.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.3.4.2.1.cmml">⋅</mo><mi id="S3.Ex1.m1.1.1.3.4.2.3" xref="S3.Ex1.m1.1.1.3.4.2.3.cmml">L</mi></mrow><mo id="S3.Ex1.m1.1.1.3.4.1" xref="S3.Ex1.m1.1.1.3.4.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.3" xref="S3.Ex1.m1.1.1.3.4.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.3.4.1a" xref="S3.Ex1.m1.1.1.3.4.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.4" xref="S3.Ex1.m1.1.1.3.4.4.cmml">s</mi><mo id="S3.Ex1.m1.1.1.3.4.1b" xref="S3.Ex1.m1.1.1.3.4.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.3.4.5" xref="S3.Ex1.m1.1.1.3.4.5.cmml"><mi id="S3.Ex1.m1.1.1.3.4.5.2" xref="S3.Ex1.m1.1.1.3.4.5.2.cmml">s</mi><mrow id="S3.Ex1.m1.1.1.3.4.5.3" xref="S3.Ex1.m1.1.1.3.4.5.3.cmml"><mi id="S3.Ex1.m1.1.1.3.4.5.3.2" xref="S3.Ex1.m1.1.1.3.4.5.3.2.cmml">L</mi><mo id="S3.Ex1.m1.1.1.3.4.5.3.1" xref="S3.Ex1.m1.1.1.3.4.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.5.3.3" xref="S3.Ex1.m1.1.1.3.4.5.3.3.cmml">i</mi><mo id="S3.Ex1.m1.1.1.3.4.5.3.1a" xref="S3.Ex1.m1.1.1.3.4.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.5.3.4" xref="S3.Ex1.m1.1.1.3.4.5.3.4.cmml">D</mi><mo id="S3.Ex1.m1.1.1.3.4.5.3.1b" xref="S3.Ex1.m1.1.1.3.4.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.5.3.5" xref="S3.Ex1.m1.1.1.3.4.5.3.5.cmml">A</mi><mo id="S3.Ex1.m1.1.1.3.4.5.3.1c" xref="S3.Ex1.m1.1.1.3.4.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.4.5.3.6" xref="S3.Ex1.m1.1.1.3.4.5.3.6.cmml">R</mi></mrow></msub></mrow><mo id="S3.Ex1.m1.1.1.3.1b" xref="S3.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.3.5" xref="S3.Ex1.m1.1.1.3.5.cmml"><mrow id="S3.Ex1.m1.1.1.3.5.2" xref="S3.Ex1.m1.1.1.3.5.2.cmml"><mi id="S3.Ex1.m1.1.1.3.5.2.2" xref="S3.Ex1.m1.1.1.3.5.2.2.cmml">γ</mi><mo id="S3.Ex1.m1.1.1.3.5.2.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex1.m1.1.1.3.5.2.1.cmml">⋅</mo><mi id="S3.Ex1.m1.1.1.3.5.2.3" xref="S3.Ex1.m1.1.1.3.5.2.3.cmml">L</mi></mrow><mo id="S3.Ex1.m1.1.1.3.5.1" xref="S3.Ex1.m1.1.1.3.5.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.3" xref="S3.Ex1.m1.1.1.3.5.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.3.5.1a" xref="S3.Ex1.m1.1.1.3.5.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.4" xref="S3.Ex1.m1.1.1.3.5.4.cmml">s</mi><mo id="S3.Ex1.m1.1.1.3.5.1b" xref="S3.Ex1.m1.1.1.3.5.1.cmml">⁢</mo><msub id="S3.Ex1.m1.1.1.3.5.5" xref="S3.Ex1.m1.1.1.3.5.5.cmml"><mi id="S3.Ex1.m1.1.1.3.5.5.2" xref="S3.Ex1.m1.1.1.3.5.5.2.cmml">s</mi><mrow id="S3.Ex1.m1.1.1.3.5.5.3" xref="S3.Ex1.m1.1.1.3.5.5.3.cmml"><mi id="S3.Ex1.m1.1.1.3.5.5.3.2" xref="S3.Ex1.m1.1.1.3.5.5.3.2.cmml">d</mi><mo id="S3.Ex1.m1.1.1.3.5.5.3.1" xref="S3.Ex1.m1.1.1.3.5.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.5.3.3" xref="S3.Ex1.m1.1.1.3.5.5.3.3.cmml">e</mi><mo id="S3.Ex1.m1.1.1.3.5.5.3.1a" xref="S3.Ex1.m1.1.1.3.5.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.5.3.4" xref="S3.Ex1.m1.1.1.3.5.5.3.4.cmml">p</mi><mo id="S3.Ex1.m1.1.1.3.5.5.3.1b" xref="S3.Ex1.m1.1.1.3.5.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.5.3.5" xref="S3.Ex1.m1.1.1.3.5.5.3.5.cmml">t</mi><mo id="S3.Ex1.m1.1.1.3.5.5.3.1c" xref="S3.Ex1.m1.1.1.3.5.5.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.5.5.3.6" xref="S3.Ex1.m1.1.1.3.5.5.3.6.cmml">h</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"></eq><apply id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"><times id="S3.Ex1.m1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.2.1"></times><ci id="S3.Ex1.m1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.2.2">𝐿</ci><ci id="S3.Ex1.m1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.2.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.2.4.cmml" xref="S3.Ex1.m1.1.1.2.4">𝑠</ci><ci id="S3.Ex1.m1.1.1.2.5.cmml" xref="S3.Ex1.m1.1.1.2.5">𝑠</ci></apply><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><plus id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3.1"></plus><apply id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2"><times id="S3.Ex1.m1.1.1.3.2.1.cmml" xref="S3.Ex1.m1.1.1.3.2.1"></times><ci id="S3.Ex1.m1.1.1.3.2.2.cmml" xref="S3.Ex1.m1.1.1.3.2.2">𝐿</ci><ci id="S3.Ex1.m1.1.1.3.2.3.cmml" xref="S3.Ex1.m1.1.1.3.2.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.3.2.4.cmml" xref="S3.Ex1.m1.1.1.3.2.4">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.2.5.cmml" xref="S3.Ex1.m1.1.1.3.2.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.2.5.1.cmml" xref="S3.Ex1.m1.1.1.3.2.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.5.2.cmml" xref="S3.Ex1.m1.1.1.3.2.5.2">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.2.5.3.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3"><times id="S3.Ex1.m1.1.1.3.2.5.3.1.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3.1"></times><ci id="S3.Ex1.m1.1.1.3.2.5.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3.2">𝑓</ci><ci id="S3.Ex1.m1.1.1.3.2.5.3.3.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3.3">𝑖</ci><ci id="S3.Ex1.m1.1.1.3.2.5.3.4.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3.4">𝑛</ci><ci id="S3.Ex1.m1.1.1.3.2.5.3.5.cmml" xref="S3.Ex1.m1.1.1.3.2.5.3.5">𝑒</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3"><times id="S3.Ex1.m1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.3.3.1"></times><apply id="S3.Ex1.m1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.3.3.2"><ci id="S3.Ex1.m1.1.1.3.3.2.1.cmml" xref="S3.Ex1.m1.1.1.3.3.2.1">⋅</ci><ci id="S3.Ex1.m1.1.1.3.3.2.2.cmml" xref="S3.Ex1.m1.1.1.3.3.2.2">𝛼</ci><ci id="S3.Ex1.m1.1.1.3.3.2.3.cmml" xref="S3.Ex1.m1.1.1.3.3.2.3">𝐿</ci></apply><ci id="S3.Ex1.m1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.3.3.4.cmml" xref="S3.Ex1.m1.1.1.3.3.4">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.3.5.cmml" xref="S3.Ex1.m1.1.1.3.3.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.3.5.1.cmml" xref="S3.Ex1.m1.1.1.3.3.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.3.5.2.cmml" xref="S3.Ex1.m1.1.1.3.3.5.2">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.3.5.3.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3"><times id="S3.Ex1.m1.1.1.3.3.5.3.1.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.1"></times><ci id="S3.Ex1.m1.1.1.3.3.5.3.2.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.2">𝑖</ci><ci id="S3.Ex1.m1.1.1.3.3.5.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.3">𝑚</ci><ci id="S3.Ex1.m1.1.1.3.3.5.3.4.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.4">𝑎</ci><ci id="S3.Ex1.m1.1.1.3.3.5.3.5.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.5">𝑔</ci><ci id="S3.Ex1.m1.1.1.3.3.5.3.6.cmml" xref="S3.Ex1.m1.1.1.3.3.5.3.6">𝑒</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.3.4.cmml" xref="S3.Ex1.m1.1.1.3.4"><times id="S3.Ex1.m1.1.1.3.4.1.cmml" xref="S3.Ex1.m1.1.1.3.4.1"></times><apply id="S3.Ex1.m1.1.1.3.4.2.cmml" xref="S3.Ex1.m1.1.1.3.4.2"><ci id="S3.Ex1.m1.1.1.3.4.2.1.cmml" xref="S3.Ex1.m1.1.1.3.4.2.1">⋅</ci><ci id="S3.Ex1.m1.1.1.3.4.2.2.cmml" xref="S3.Ex1.m1.1.1.3.4.2.2">𝛽</ci><ci id="S3.Ex1.m1.1.1.3.4.2.3.cmml" xref="S3.Ex1.m1.1.1.3.4.2.3">𝐿</ci></apply><ci id="S3.Ex1.m1.1.1.3.4.3.cmml" xref="S3.Ex1.m1.1.1.3.4.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.3.4.4.cmml" xref="S3.Ex1.m1.1.1.3.4.4">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.4.5.cmml" xref="S3.Ex1.m1.1.1.3.4.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.4.5.1.cmml" xref="S3.Ex1.m1.1.1.3.4.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.4.5.2.cmml" xref="S3.Ex1.m1.1.1.3.4.5.2">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.4.5.3.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3"><times id="S3.Ex1.m1.1.1.3.4.5.3.1.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.1"></times><ci id="S3.Ex1.m1.1.1.3.4.5.3.2.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.2">𝐿</ci><ci id="S3.Ex1.m1.1.1.3.4.5.3.3.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.3">𝑖</ci><ci id="S3.Ex1.m1.1.1.3.4.5.3.4.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.4">𝐷</ci><ci id="S3.Ex1.m1.1.1.3.4.5.3.5.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.5">𝐴</ci><ci id="S3.Ex1.m1.1.1.3.4.5.3.6.cmml" xref="S3.Ex1.m1.1.1.3.4.5.3.6">𝑅</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.3.5.cmml" xref="S3.Ex1.m1.1.1.3.5"><times id="S3.Ex1.m1.1.1.3.5.1.cmml" xref="S3.Ex1.m1.1.1.3.5.1"></times><apply id="S3.Ex1.m1.1.1.3.5.2.cmml" xref="S3.Ex1.m1.1.1.3.5.2"><ci id="S3.Ex1.m1.1.1.3.5.2.1.cmml" xref="S3.Ex1.m1.1.1.3.5.2.1">⋅</ci><ci id="S3.Ex1.m1.1.1.3.5.2.2.cmml" xref="S3.Ex1.m1.1.1.3.5.2.2">𝛾</ci><ci id="S3.Ex1.m1.1.1.3.5.2.3.cmml" xref="S3.Ex1.m1.1.1.3.5.2.3">𝐿</ci></apply><ci id="S3.Ex1.m1.1.1.3.5.3.cmml" xref="S3.Ex1.m1.1.1.3.5.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.3.5.4.cmml" xref="S3.Ex1.m1.1.1.3.5.4">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.5.5.cmml" xref="S3.Ex1.m1.1.1.3.5.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.5.5.1.cmml" xref="S3.Ex1.m1.1.1.3.5.5">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.5.5.2.cmml" xref="S3.Ex1.m1.1.1.3.5.5.2">𝑠</ci><apply id="S3.Ex1.m1.1.1.3.5.5.3.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3"><times id="S3.Ex1.m1.1.1.3.5.5.3.1.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.1"></times><ci id="S3.Ex1.m1.1.1.3.5.5.3.2.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.2">𝑑</ci><ci id="S3.Ex1.m1.1.1.3.5.5.3.3.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.3">𝑒</ci><ci id="S3.Ex1.m1.1.1.3.5.5.3.4.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.4">𝑝</ci><ci id="S3.Ex1.m1.1.1.3.5.5.3.5.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.5">𝑡</ci><ci id="S3.Ex1.m1.1.1.3.5.5.3.6.cmml" xref="S3.Ex1.m1.1.1.3.5.5.3.6">ℎ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">Loss=Loss_{fine}+\alpha\cdot Loss_{image}+\beta\cdot Loss_{LiDAR}+\gamma\cdot
Loss%
_{depth}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">italic_L italic_o italic_s italic_s = italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_f italic_i italic_n italic_e end_POSTSUBSCRIPT + italic_α ⋅ italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_i italic_m italic_a italic_g italic_e end_POSTSUBSCRIPT + italic_β ⋅ italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_L italic_i italic_D italic_A italic_R end_POSTSUBSCRIPT + italic_γ ⋅ italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Dettailed experimental result will be shown in next section for multi-modality road estimation.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Meta Pseudo Labels</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The KITTI road segmentation dataset, while limited to just over 200 training images, is complemented by a substantial collection of unlabeled yet thematically consistent data within the KITTI detection dataset.
This extensive repository of similar unlabeled images serves as a rich source for semi-supervised learning, allowing models to generalize better and improve their segmentation capabilities without the need for extensive manual annotation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">In the KITTI road segmentation task, we employ a semi-supervised learning approach utilizing meta learning techniques. The process begins with the training of a fully supervised model on the complete dataset, which is represented as <math alttext="Model_{Supervised}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">M</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">o</mi><mo id="S3.SS3.p2.1.m1.1.1.1a" xref="S3.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.4" xref="S3.SS3.p2.1.m1.1.1.4.cmml">d</mi><mo id="S3.SS3.p2.1.m1.1.1.1b" xref="S3.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.5" xref="S3.SS3.p2.1.m1.1.1.5.cmml">e</mi><mo id="S3.SS3.p2.1.m1.1.1.1c" xref="S3.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS3.p2.1.m1.1.1.6" xref="S3.SS3.p2.1.m1.1.1.6.cmml"><mi id="S3.SS3.p2.1.m1.1.1.6.2" xref="S3.SS3.p2.1.m1.1.1.6.2.cmml">l</mi><mrow id="S3.SS3.p2.1.m1.1.1.6.3" xref="S3.SS3.p2.1.m1.1.1.6.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.6.3.2" xref="S3.SS3.p2.1.m1.1.1.6.3.2.cmml">S</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.3" xref="S3.SS3.p2.1.m1.1.1.6.3.3.cmml">u</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1a" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.4" xref="S3.SS3.p2.1.m1.1.1.6.3.4.cmml">p</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1b" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.5" xref="S3.SS3.p2.1.m1.1.1.6.3.5.cmml">e</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1c" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.6" xref="S3.SS3.p2.1.m1.1.1.6.3.6.cmml">r</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1d" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.7" xref="S3.SS3.p2.1.m1.1.1.6.3.7.cmml">v</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1e" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.8" xref="S3.SS3.p2.1.m1.1.1.6.3.8.cmml">i</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1f" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.9" xref="S3.SS3.p2.1.m1.1.1.6.3.9.cmml">s</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1g" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.10" xref="S3.SS3.p2.1.m1.1.1.6.3.10.cmml">e</mi><mo id="S3.SS3.p2.1.m1.1.1.6.3.1h" xref="S3.SS3.p2.1.m1.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS3.p2.1.m1.1.1.6.3.11" xref="S3.SS3.p2.1.m1.1.1.6.3.11.cmml">d</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑀</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑜</ci><ci id="S3.SS3.p2.1.m1.1.1.4.cmml" xref="S3.SS3.p2.1.m1.1.1.4">𝑑</ci><ci id="S3.SS3.p2.1.m1.1.1.5.cmml" xref="S3.SS3.p2.1.m1.1.1.5">𝑒</ci><apply id="S3.SS3.p2.1.m1.1.1.6.cmml" xref="S3.SS3.p2.1.m1.1.1.6"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.6.1.cmml" xref="S3.SS3.p2.1.m1.1.1.6">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.6.2.cmml" xref="S3.SS3.p2.1.m1.1.1.6.2">𝑙</ci><apply id="S3.SS3.p2.1.m1.1.1.6.3.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3"><times id="S3.SS3.p2.1.m1.1.1.6.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.6.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.2">𝑆</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.3">𝑢</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.4.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.4">𝑝</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.5.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.5">𝑒</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.6.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.6">𝑟</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.7.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.7">𝑣</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.8.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.8">𝑖</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.9.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.9">𝑠</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.10.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.10">𝑒</ci><ci id="S3.SS3.p2.1.m1.1.1.6.3.11.cmml" xref="S3.SS3.p2.1.m1.1.1.6.3.11">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">Model_{Supervised}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_M italic_o italic_d italic_e italic_l start_POSTSUBSCRIPT italic_S italic_u italic_p italic_e italic_r italic_v italic_i italic_s italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Following the initial training, we commence the semi-supervised learning phase with an iterative algorithm. For each iteration, we define a confidence threshold <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_τ</annotation></semantics></math> for the model’s predictions, and only pixels with confidence above this threshold contribute to the loss function, while others are masked. This can be mathematically expressed as:</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Semi-Supervised Update}=arg_{min}{\theta}\sum_{i\in S}\mathcal{L}(y_{i},%
\hat{y}_{i};\theta)," class="ltx_Math" display="block" id="S3.Ex2.m1.2"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mrow id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mtext id="S3.Ex2.m1.2.2.1.1.4" xref="S3.Ex2.m1.2.2.1.1.4a.cmml">Semi-Supervised Update</mtext><mo id="S3.Ex2.m1.2.2.1.1.3" xref="S3.Ex2.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.1.1.2" xref="S3.Ex2.m1.2.2.1.1.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.4" xref="S3.Ex2.m1.2.2.1.1.2.4.cmml">a</mi><mo id="S3.Ex2.m1.2.2.1.1.2.3" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.1.1.2.5" xref="S3.Ex2.m1.2.2.1.1.2.5.cmml">r</mi><mo id="S3.Ex2.m1.2.2.1.1.2.3a" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">⁢</mo><msub id="S3.Ex2.m1.2.2.1.1.2.6" xref="S3.Ex2.m1.2.2.1.1.2.6.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.6.2" xref="S3.Ex2.m1.2.2.1.1.2.6.2.cmml">g</mi><mrow id="S3.Ex2.m1.2.2.1.1.2.6.3" xref="S3.Ex2.m1.2.2.1.1.2.6.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.6.3.2" xref="S3.Ex2.m1.2.2.1.1.2.6.3.2.cmml">m</mi><mo id="S3.Ex2.m1.2.2.1.1.2.6.3.1" xref="S3.Ex2.m1.2.2.1.1.2.6.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.1.1.2.6.3.3" xref="S3.Ex2.m1.2.2.1.1.2.6.3.3.cmml">i</mi><mo id="S3.Ex2.m1.2.2.1.1.2.6.3.1a" xref="S3.Ex2.m1.2.2.1.1.2.6.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.1.1.2.6.3.4" xref="S3.Ex2.m1.2.2.1.1.2.6.3.4.cmml">n</mi></mrow></msub><mo id="S3.Ex2.m1.2.2.1.1.2.3b" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">⁢</mo><mi id="S3.Ex2.m1.2.2.1.1.2.7" xref="S3.Ex2.m1.2.2.1.1.2.7.cmml">θ</mi><mo id="S3.Ex2.m1.2.2.1.1.2.3c" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">⁢</mo><mrow id="S3.Ex2.m1.2.2.1.1.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.cmml"><munder id="S3.Ex2.m1.2.2.1.1.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.Ex2.m1.2.2.1.1.2.2.3.2" movablelimits="false" xref="S3.Ex2.m1.2.2.1.1.2.2.3.2.cmml">∑</mo><mrow id="S3.Ex2.m1.2.2.1.1.2.2.3.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2.3.3.2" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.2.cmml">i</mi><mo id="S3.Ex2.m1.2.2.1.1.2.2.3.3.1" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.2.2.1.1.2.2.3.3.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.3.cmml">S</mi></mrow></munder><mrow id="S3.Ex2.m1.2.2.1.1.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.2.2.1.1.2.2.2.4" xref="S3.Ex2.m1.2.2.1.1.2.2.2.4.cmml">ℒ</mi><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml"><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.4" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml">y</mi><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.1" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.5" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml">;</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">θ</mi><mo id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.6" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.2.2.1.2" xref="S3.Ex2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1"><eq id="S3.Ex2.m1.2.2.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3"></eq><ci id="S3.Ex2.m1.2.2.1.1.4a.cmml" xref="S3.Ex2.m1.2.2.1.1.4"><mtext id="S3.Ex2.m1.2.2.1.1.4.cmml" xref="S3.Ex2.m1.2.2.1.1.4">Semi-Supervised Update</mtext></ci><apply id="S3.Ex2.m1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2"><times id="S3.Ex2.m1.2.2.1.1.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.3"></times><ci id="S3.Ex2.m1.2.2.1.1.2.4.cmml" xref="S3.Ex2.m1.2.2.1.1.2.4">𝑎</ci><ci id="S3.Ex2.m1.2.2.1.1.2.5.cmml" xref="S3.Ex2.m1.2.2.1.1.2.5">𝑟</ci><apply id="S3.Ex2.m1.2.2.1.1.2.6.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.6.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.2.6.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.2">𝑔</ci><apply id="S3.Ex2.m1.2.2.1.1.2.6.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.3"><times id="S3.Ex2.m1.2.2.1.1.2.6.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.3.1"></times><ci id="S3.Ex2.m1.2.2.1.1.2.6.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.3.2">𝑚</ci><ci id="S3.Ex2.m1.2.2.1.1.2.6.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.3.3">𝑖</ci><ci id="S3.Ex2.m1.2.2.1.1.2.6.3.4.cmml" xref="S3.Ex2.m1.2.2.1.1.2.6.3.4">𝑛</ci></apply></apply><ci id="S3.Ex2.m1.2.2.1.1.2.7.cmml" xref="S3.Ex2.m1.2.2.1.1.2.7">𝜃</ci><apply id="S3.Ex2.m1.2.2.1.1.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2"><apply id="S3.Ex2.m1.2.2.1.1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3">subscript</csymbol><sum id="S3.Ex2.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.2"></sum><apply id="S3.Ex2.m1.2.2.1.1.2.2.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3"><in id="S3.Ex2.m1.2.2.1.1.2.2.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.1"></in><ci id="S3.Ex2.m1.2.2.1.1.2.2.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.2">𝑖</ci><ci id="S3.Ex2.m1.2.2.1.1.2.2.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.3">𝑆</ci></apply></apply><apply id="S3.Ex2.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2"><times id="S3.Ex2.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.3"></times><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.4.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.4">ℒ</ci><vector id="S3.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2"><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2"><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.1">^</ci><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.2.2">𝑦</ci></apply><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝜃</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\text{Semi-Supervised Update}=arg_{min}{\theta}\sum_{i\in S}\mathcal{L}(y_{i},%
\hat{y}_{i};\theta),</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.2d">Semi-Supervised Update = italic_a italic_r italic_g start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT italic_θ ∑ start_POSTSUBSCRIPT italic_i ∈ italic_S end_POSTSUBSCRIPT caligraphic_L ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; italic_θ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.6">where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.1d">italic_θ</annotation></semantics></math> denotes the model parameters, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1"><semantics id="S3.SS3.p5.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.1d">caligraphic_L</annotation></semantics></math> is the loss function, <math alttext="y_{i}" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.1"><semantics id="S3.SS3.p5.3.m3.1a"><msub id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.2" xref="S3.SS3.p5.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">𝑦</ci><ci id="S3.SS3.p5.3.m3.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the pseudo labels, and <math alttext="\hat{y}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p5.4.m4.1"><semantics id="S3.SS3.p5.4.m4.1a"><msub id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml"><mover accent="true" id="S3.SS3.p5.4.m4.1.1.2" xref="S3.SS3.p5.4.m4.1.1.2.cmml"><mi id="S3.SS3.p5.4.m4.1.1.2.2" xref="S3.SS3.p5.4.m4.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p5.4.m4.1.1.2.1" xref="S3.SS3.p5.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p5.4.m4.1.1.3" xref="S3.SS3.p5.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><apply id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.4.m4.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">subscript</csymbol><apply id="S3.SS3.p5.4.m4.1.1.2.cmml" xref="S3.SS3.p5.4.m4.1.1.2"><ci id="S3.SS3.p5.4.m4.1.1.2.1.cmml" xref="S3.SS3.p5.4.m4.1.1.2.1">^</ci><ci id="S3.SS3.p5.4.m4.1.1.2.2.cmml" xref="S3.SS3.p5.4.m4.1.1.2.2">𝑦</ci></apply><ci id="S3.SS3.p5.4.m4.1.1.3.cmml" xref="S3.SS3.p5.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\hat{y}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.4.m4.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the predicted labels.
<math alttext="S" class="ltx_Math" display="inline" id="S3.SS3.p5.5.m5.1"><semantics id="S3.SS3.p5.5.m5.1a"><mi id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><ci id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.5.m5.1d">italic_S</annotation></semantics></math> is the subset of pixels with confidence scores above the threshold <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS3.p5.6.m6.1"><semantics id="S3.SS3.p5.6.m6.1a"><mi id="S3.SS3.p5.6.m6.1.1" xref="S3.SS3.p5.6.m6.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m6.1b"><ci id="S3.SS3.p5.6.m6.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m6.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.6.m6.1d">italic_τ</annotation></semantics></math>, and the remaining pixels are masked to prevent them from influencing the learning process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">This method allows the model to progressively refine its understanding of the road segmentation task by focusing on high-confidence predictions and incorporating unlabeled data in a controlled manner.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection ltx_indent_first" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We validate the proposed UdeerLID+ on KITTI dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The KITTI road dataset, is a comprehensive benchmark designed for evaluating road segmentation algorithms in the context of autonomous driving.
It features a diverse set of real-world driving scenarios, captured by high-resolution cameras and LiDAR sensors mounted on a moving vehicle.
The dataset consists of over 200 training images and 200 test images, each paired with accurate ground truth annotations that delineate the drivable road regions.
The KITTI road dataset serves not only as a testbed for algorithm development but also as a valuable resource for advancing the state-of-the-art in road detection technology.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the implementation, an encoder-decoder architecture is utilized, specifically using the InternImage-B model[7] pretrained on the ADE20K Semantic Segmentation dataset.
This pretrained model serves as the backbone for the image encoder, effectively capturing and encoding the semantic information from the input images.
The decoder then upscales these encoded features to generate high-resolution segmentation maps, providing detailed and accurate delineation of road regions.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Connection</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In the integration of images, point clouds, and depth information, our approach draws inspiration from PLARD[1], employing a multi-level fusion and upsampling strategy.
This method involves the synergistic combination of these diverse data types at various stages of the network. By conducting feature fusion at multiple hierarchical levels, we enhance the representational power of the model, which is crucial for tasks such as semantic segmentation and object detection that require detailed understanding of the scene’s geometry and appearance.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Quantitive Evaluation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The quantitive results on KITTI dataset are shown in Table 1. PLARD’s result is based on its released model on the Github; UdeerLID is the version without Meta Pseudo Labels.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Um Road Estimation On The Test Set Of KITTI Dataset</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.1.1.1.2">method</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.3">MaxF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">PLARD[1]</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.3">96.32</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.3.2.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.3.2.2">UdeerLID</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">96.94</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.4.3.1">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.3.2">UdeerLID+</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.3">97.26</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Discussion</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The effectiveness of relative depth in the task of road recognition has been demonstrated, showcasing its potential to enhance the accuracy and robustness of road detection models.
Additionally, the integration of meta-learning techniques in semantic segmentation has been observed to significantly contribute to the growth of model performance. By leveraging the unique advantages of each data modality and employing advanced learning strategies, we can achieve more reliable and efficient models for autonomous driving and computer vision applications.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This endeavor aims to optimize model performance by exploring various combinations of visual imagery, LiDAR point clouds, relative depth maps, and other relevant sensor data. The goal is to identify the optimal data fusion architectures and algorithms that can best capture the complexities of the driving environment, ultimately leading to more reliable and intelligent autonomous systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Chen Z, Zhang J, Tao D. Progressive lidar adaptation for road detection[J]. IEEE/CAA Journal of Automatica Sinica, 2019, 6(3): 693-702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Yang L, Kang B, Huang Z, et al. Depth anything: Unleashing the power of large-scale unlabeled data[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 10371-10381.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Pham H, Dai Z, Xie Q, et al. Meta pseudo labels[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 11557-11568.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Yu C, Gao C, Wang J, et al. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation[J]. International journal of computer vision, 2021, 129: 3051-3068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Chen J, Lu Y, Yu Q, et al. Transunet: Transformers make strong encoders for medical image segmentation[J]. arXiv preprint arXiv:2102.04306, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Wang W, Dai J, Chen Z, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 14408-14419.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\addappheadtotoc</span>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 03:52:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
