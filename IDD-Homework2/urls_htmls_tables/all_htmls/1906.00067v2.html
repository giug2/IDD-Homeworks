<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1906.00067] OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</title><meta property="og:description" content="Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1906.00067">

<!--Generated on Fri Mar  1 20:07:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">OK-VQA: A Visual Question Answering Benchmark Requiring 
<br class="ltx_break">External Knowledge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kenneth Marino
</span><span class="ltx_author_notes">Work done during internship at Allen Institute for AI
<span class="ltx_contact ltx_role_affiliation">Carnegie Mellon University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Rastegari
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">PRIOR @ Allen Institute for AI
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Farhadi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">PRIOR @ Allen Institute for AI
</span>
<span class="ltx_contact ltx_role_affiliation">University of Washington
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roozbeh Mottaghi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">PRIOR @ Allen Institute for AI
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See <a target="_blank" href="http://okvqa.allenai.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://okvqa.allenai.org</a> to download and browse the dataset.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/1906.00067/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="400" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">We propose a novel dataset for visual question answering, where the questions require external knowledge resources to be answered. In this example, the visual content of the image is not sufficient to answer the question. A set of facts about teddy bears makes the connection between teddy bear and the American president, which enables answering the question.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The field of Visual Question Answering (VQA) has made amazing strides in recent years, achieving record numbers on standard VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. As originally conceived, VQA is not only a fertile ground for vision and language research, but is also a proxy to evaluate AI models for the task of open-ended scene understanding. In its ideal form, VQA would require not only visual recognition, but also logical reasoning and incorporating knowledge about the world. However, current VQA datasets (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>) are focused mainly on recognition, and most questions are about simple counting, colors, and other visual detection tasks, so do not require much logical reasoning or association with external knowledge. The most difficult and interesting questions ideally require knowing more than what the question entails or what information is contained in the images.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Consider the question in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which asks about the relation between the teddy bear and an American president. The information in the image here is not complete for answering the question. We need to link the image content to external knowledge sources, such as the sentences at the bottom of the figure taken from Wikipedia. Given the question, image, and Wikipedia sentences, there is now enough information to answer the question: Teddy Roosevelt!</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">More recent research has started to look at how to incorporate knowledge-based methods into VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. These methods have investigated incorporating knowledge bases and retrieval methods into VQA datasets with a set of associated facts for each question. In this work, we go one step forward and design a VQA dataset which requires VQA to perform reasoning using unstructured knowledge.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To enable research in this exciting direction, we introduce a novel dataset, named Outside Knowledge VQA (OK-VQA), which includes only questions that require external resources for answering them. On our dataset, we can start to evaluate the reasoning capabilities of models in scenarios where the answer cannot be obtained by only looking at the image. Answering <span id="S1.p4.1.1" class="ltx_text">OK-VQA</span> questions is a challenging task since, in addition to understanding the question and the image, the model needs to: (1) learn what knowledge is necessary to answer the questions, (2) determine what query to do to retrieve the necessary knowledge from an outside source of knowledge, and (3) incorporate the knowledge from its original representation to answer the question.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The <span id="S1.p5.1.1" class="ltx_text">OK-VQA</span> dataset consists of more than 14,000 questions that cover a variety of knowledge categories such as science &amp; technology, history, and sports. We provide category breakdowns of our dataset, as well as other relevant statistics to examine its properties. We also analyze state-of-the-art models and show their performance degrades on this new dataset. Furthermore, we provide results for a set of baseline approaches that are based on simple knowledge retrieval. Our dataset is diverse, difficult, and to date the largest VQA dataset focused on knowledge-based VQA in natural images.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions are: (a) we introduce the OK-VQA dataset, which includes only questions that require external resources to answer; (b) we benchmark some state-of-the-art VQA models on our new dataset and show the performance of these models degrades drastically; (c) we propose a set of baselines that exploit unstructured knowledge.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1906.00067/assets/figs/Dataset_Example_Categories_v2.jpg" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="292" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset examples.<span id="S1.F2.5.2.1" class="ltx_text ltx_font_medium"> Some example questions and their corresponding images and answers have been shown. We show one example question for each knowledge category.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering (VQA).</span> Visual question answering (VQA) has been one of the most popular topics in the computer vision community over the past few years. Early approaches to VQA combined recurrent networks with CNNs to integrate textual and visual data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Attention-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> better guide the model in answering the questions by highlighting image regions that are relevant to the question. Modular networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> leverage the compositional nature of the language in deep neural networks. These approaches have been extended to the video domain as well <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> address the problem of question answering in an interactive environment. None of these approaches, however, is designed for leveraging external knowledge so they cannot handle the cases that the image does not represent the full knowledge to answer the question.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The problem of using external knowledge for answering questions has been tackled by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. These methods only handle the knowledge that is represented by subject-relation-object or visual concept-relation-attribute triplets, and rely on supervision to do the retrieval of facts. In contrast, answering questions in our dataset requires handling unstructured knowledge resources.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">VQA datasets.</span> In the past few years several datasets have been proposed for visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. The DAQUAR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> includes template-based and natural questions for a set of indoor scenes. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed the VQA dataset, which is two orders of magnitude larger than DAQUAR and includes more diverse images and less constrained answers. FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is another dataset that includes multi-lingual questions and answers. Visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, constructs fill-in-the-blank templates for natural language descriptions. COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> is constructed automatically by converting image descriptions to questions. The idea of Visual 7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> is to provide object-level grounding for question-answer pairs as opposed to image-level associations between images and QA pairs. Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> provides dense annotations for image regions, attributes, relationships, etc. and provide free-form and region-based QA pairs for each image. MovieQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> is a movie-based QA dataset, where the QAs are based on information in the video clips, subtitles, scripts, etc. CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is a synthetic VQA dataset that mainly targets visual reasoning abilities. In contrast to all these datasets, we focus on questions that cannot be answered by the information in the associated image and require external knowledge to be answered.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Most similar to our dataset is FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. While that work also tackles the difficult problem of creating a VQA dataset requiring outside knowledge, their method annotates questions by selecting a fact (a knowledge triplet such as “dog is mammal") from a fixed knowledge base. While this dataset is still quite useful for testing methods’ ability to incorporate a knowledge base into a VQA system, our dataset tests methods’ ability to retrieve relevant facts from the web, from a database, or some other source of knowledge that was not used to create the questions. Another issue is that triplets are not sufficient to represent general knowledge.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Building knowledge bases &amp; Knowledge-based reasoning.</span> Several knowledge bases have been created using visual data or for visual reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. These knowledge bases are potentially helpful resources for answering questions in our dataset. Knowledge-based question answering has received much more attention in the NLP community (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text">OK-VQA</span> Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we explain how we collect a dataset which better measures performance of VQA systems requiring external knowledge. The common VQA datasets such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> do not require much knowledge to answer a large majority of the questions. The dataset mostly contains questions such as “How many apples are there?”, “What animal is this?”, and “What color is the bowl?”. While these are perfectly reasonable tasks for open-ended visual recognition, they do not test our algorithms’ ability to reason about a scene or draw on information outside of the image. Thus, for our goal of combining visual recognition with information extraction from sources outside the image, we would not be able to evaluate knowledge-based systems as most questions do not require outside knowledge.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To see this specifically, we examine the “age annotations” that are provided for 10,000 questions in the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. For each question and image pair, an MTurk worker was asked how old someone would need to be to answer the question. While this is not a perfect metric, it is a reasonable approximation of the difficulty of a question and how much a person would have to know to answer a question. The analysis shows that more than 78% of the questions can be answered by people who are 10 years old or younger. This suggests that very little background knowledge is actually required to answer the vast majority of these questions.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<td id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.2.1" class="ltx_text">
<span id="S3.T1.2.1.1.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:45.5pt;">
<span id="S3.T1.2.1.1.2.1.1.1" class="ltx_p">Number of questions</span>
</span></span></td>
<td id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.3.1" class="ltx_text">
<span id="S3.T1.2.1.1.3.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S3.T1.2.1.1.3.1.1.1" class="ltx_p">Number of images</span>
</span></span></td>
<td id="S3.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.4.1" class="ltx_text">
<span id="S3.T1.2.1.1.4.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S3.T1.2.1.1.4.1.1.1" class="ltx_p">Knowledge based?</span>
</span></span></td>
<td id="S3.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.5.1" class="ltx_text">
<span id="S3.T1.2.1.1.5.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T1.2.1.1.5.1.1.1" class="ltx_p">Goal</span>
</span></span></td>
<td id="S3.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.6.1" class="ltx_text">
<span id="S3.T1.2.1.1.6.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S3.T1.2.1.1.6.1.1.1" class="ltx_p">Answer type</span>
</span></span></td>
<td id="S3.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.7.1" class="ltx_text">
<span id="S3.T1.2.1.1.7.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:34.1pt;">
<span id="S3.T1.2.1.1.7.1.1.1" class="ltx_p">Avg. A length</span>
</span></span></td>
<td id="S3.T1.2.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S3.T1.2.1.1.8.1" class="ltx_text">
<span id="S3.T1.2.1.1.8.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:34.1pt;">
<span id="S3.T1.2.1.1.8.1.1.1" class="ltx_p">Avg. Q length</span>
</span></span></td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">12,468</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1,449</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">visual: counts, colors, objects</td>
<td id="S3.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Open</td>
<td id="S3.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.1</td>
<td id="S3.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">11.5</td>
</tr>
<tr id="S3.T1.2.3.3" class="ltx_tr">
<th id="S3.T1.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</th>
<td id="S3.T1.2.3.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">360,001</td>
<td id="S3.T1.2.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">10,738</td>
<td id="S3.T1.2.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">visual: scene, objects, person</td>
<td id="S3.T1.2.3.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">FITB/MC</td>
<td id="S3.T1.2.3.3.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.8</td>
<td id="S3.T1.2.3.3.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.9</td>
</tr>
<tr id="S3.T1.2.4.4" class="ltx_tr">
<th id="S3.T1.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Visual 7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</th>
<td id="S3.T1.2.4.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">327,939</td>
<td id="S3.T1.2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47,300</td>
<td id="S3.T1.2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">visual: object-grounded questions</td>
<td id="S3.T1.2.4.4.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">MC</td>
<td id="S3.T1.2.4.4.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.0</td>
<td id="S3.T1.2.4.4.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.9</td>
</tr>
<tr id="S3.T1.2.5.5" class="ltx_tr">
<th id="S3.T1.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">VQA (v2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S3.T1.2.5.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.1M</td>
<td id="S3.T1.2.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">200K</td>
<td id="S3.T1.2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">visual understanding</td>
<td id="S3.T1.2.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Open/MC</td>
<td id="S3.T1.2.5.5.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.2</td>
<td id="S3.T1.2.5.5.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.1</td>
</tr>
<tr id="S3.T1.2.6.6" class="ltx_tr">
<th id="S3.T1.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">MovieQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</th>
<td id="S3.T1.2.6.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14,944</td>
<td id="S3.T1.2.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">408V</td>
<td id="S3.T1.2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">text+visual story comprehension</td>
<td id="S3.T1.2.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">MC</td>
<td id="S3.T1.2.6.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.3</td>
<td id="S3.T1.2.6.6.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">9.3</td>
</tr>
<tr id="S3.T1.2.7.7" class="ltx_tr">
<th id="S3.T1.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<td id="S3.T1.2.7.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">999,968</td>
<td id="S3.T1.2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">100,000</td>
<td id="S3.T1.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✘</td>
<td id="S3.T1.2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">logical reasoning</td>
<td id="S3.T1.2.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Open</td>
<td id="S3.T1.2.7.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.0</td>
<td id="S3.T1.2.7.7.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.4</td>
</tr>
<tr id="S3.T1.2.8.8" class="ltx_tr">
<th id="S3.T1.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</th>
<td id="S3.T1.2.8.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2,402</td>
<td id="S3.T1.2.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">700</td>
<td id="S3.T1.2.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S3.T1.2.8.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">visual reasoning with given KB</td>
<td id="S3.T1.2.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Open</td>
<td id="S3.T1.2.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">2.0</td>
<td id="S3.T1.2.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">6.8</td>
</tr>
<tr id="S3.T1.2.9.9" class="ltx_tr">
<th id="S3.T1.2.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S3.T1.2.9.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5,826</td>
<td id="S3.T1.2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2,190</td>
<td id="S3.T1.2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S3.T1.2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">visual reasoning with given KB</td>
<td id="S3.T1.2.9.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Open</td>
<td id="S3.T1.2.9.9.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.2</td>
<td id="S3.T1.2.9.9.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">9.5</td>
</tr>
<tr id="S3.T1.2.10.10" class="ltx_tr">
<th id="S3.T1.2.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">OK-VQA (ours)</th>
<td id="S3.T1.2.10.10.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">14,055</td>
<td id="S3.T1.2.10.10.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">14,031</td>
<td id="S3.T1.2.10.10.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">✓</td>
<td id="S3.T1.2.10.10.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">visual reasoning with open knowledge</td>
<td id="S3.T1.2.10.10.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">Open</td>
<td id="S3.T1.2.10.10.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">1.3</td>
<td id="S3.T1.2.10.10.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">8.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison of various visual QA datasets.<span id="S3.T1.5.2.1" class="ltx_text ltx_font_medium"> We compare OK-VQA with some other VQA datasets. The bottom three rows correspond to knowledge-based VQA datasets. A length: answer length; Q length: question length; MC: multiple choice; FITB: fill in the blanks; KB: knowledge base.</span></span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Given that current VQA datasets do not test exactly what we are looking for, we collect a new dataset. We use random images from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, using the original 80k-40k training and validation splits for our train and test splits. The visual complexity of these images compared to other datasets make them ideal for labeling knowledge-based questions.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">In the first round of labeling, we asked MTurk workers to write a question given an image. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, we prompt users to come up with questions to fool a “smart robot.” We also ask in the instructions that the question should be related to the image content. In addition, we prompt users not to ask what is in an image, or how many of something there is, and specify that the question should require some outside knowledge. In a second round of labeling, we asked <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.p4.1.m1.1a"><mn id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><cn type="integer" id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">5</annotation></semantics></math> different MTurk workers to label each question-image pair with an answer.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Although this prompt yielded many high-quality questions, it also yielded a lot of low quality questions, for example, ones that asked basic questions such as counting, did not require looking at the image, or were nonsensical. To ensure that the dataset asked these difficult knowledge-requiring questions, the MTurk provided questions were manually filtered to get only questions requiring knowledge. From a pool of 86,700 questions, we filtered down to 34,921 questions.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">One more factor to consider was the potential bias in the dataset. As discussed in many works, including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the VQAv1 dataset had a lot of bias. Famously, questions beginning with “Is there a …” had a very strong bias towards “Yes.” Similarly, in our unfiltered dataset, there were a lot of questions with a bias towards certain answers. For instance, in a lot of images where there is snowfall, the question would ask “What season is it?” Although there were other images (such as ones with deciduous trees with multi-colored leaves) with different answers, there was a clear bias towards “winter.” To alleviate this problem, for train and test, we removed questions so that the answer distribution was uniform; specifically, we removed questions if there were more than 5 instances of that answer as the most common answer. This had the effect of removing a lot of the answer bias. It also had the effect of making the dataset harder by limiting the number of times VQA algorithms would see questions with a particular answer, making outside information more important. We also removed questions which had no inter-annotator agreement on the answer. Performing this filtering brought us down to 9,009 questions in train and 5,046 questions in test for a total of 14,055 questions.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows some of the collected questions, images, and answers from our dataset. More are provided in the Appendix <a href="#A5" title="Appendix E Additional Dataset Examples ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. You can see that these questions require at least one piece of background knowledge to answer. For instance, in the bottom left question, the system needs to recognize that the image is of a christian church and know that those churches hold religious services on Sundays. That latter piece of knowledge should be obtained from external knowledge resources, and it cannot be inferred from the image and question alone.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Statistics</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we explore the statistical properties of our dataset, and compare to other visual question answering datasets to show that our dataset is diverse, difficult, and, to the best of our knowledge, the largest VQA dataset specifically targeted for knowledge-based VQA on natural scenes.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Knowledge category.</span>
Requiring knowledge for VQA is a good start, but there are many different types of knowledge that humans have about the world that could come into play. There is common-sense knowledge: water is wet, couches are found in living rooms. There is geographical knowledge: the Eiffel Tower is in Paris, scientific knowledge: humans have 23 chromosomes, and historical knowledge: George Washington is the first U.S. president. To get a better understanding of the kinds of knowledge our dataset requires, we asked five MTurk workers to annotate each question as belonging to one of ten categories of knowledge that we specified: Vehicles and Transportation; Brands, Companies and Products; Objects, Materials and Clothing; Sports and Recreation; Cooking and Food; Geography, History, Language and Culture; People and Everyday Life, Plants and Animals; Science and Technology; and Weather and Climate. If no one category had a plurality of workers, it was categorized as “Other". This also ensured that the final category labels are mutually exclusive. We show the distribution of questions across categories in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1906.00067/assets/x2.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="334" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Breakdown of questions in terms of knowledge categories.<span id="S4.F3.5.2.1" class="ltx_text ltx_font_medium"> We show the percentage of questions falling into each of our 10 knowledge categories.</span></span></figcaption>
</figure>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Comparison with other VQA datasets.</span>
In Table <a href="#S3.T1" title="Table 1 ‣ 3 OK-VQA Dataset ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we look at a number of other visual question answering datasets and compare them to our dataset in a number of different ways. In the top section, we look at a number of datasets which do not explicitly try to include a knowledge component including the ubiquitous VQAv2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the first version of which was one of the first datasets to investigate visual question answering. Compared to these datasets, we have a comparable number of questions to DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> as well as MovieQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and many more questions than knowledge-based datasets KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. We have fewer questions compared to CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> where the images, questions and answers are automatically generated, as well compared to more large-scale human annotated visual datasets such as VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and Visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. Since we manually filtered our dataset to avoid the pitfalls of other datasets and to ensure our questions are knowledge-based and because we filtered down common answers to emphasize the long tail of answers, our dataset is more time-intensive and expensive to collect. We trade off size in this case for knowledge and difficulty.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We can see from the average question lengths and average answer lengths that our questions and answers are about comparable to KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and longer than the other VQA datasets with the exception of DAQUAR and CLEVR (which are partially and fully automated from templates respectively). This makes sense since we would expect knowledge-based questions to be longer as they are typically not able to be as short as common questions in other datasets such as “How many objects are in the image?” or “What color is the couch?”.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">OK-VQA</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_tt">
<span id="S4.T2.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.1.1.3.1.1" class="ltx_p"><span id="S4.T2.2.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">VT</span></span>
</span>
</th>
<td id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.4.1" class="ltx_text" style="font-size:90%;">BCP</span></td>
<td id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.5.1" class="ltx_text" style="font-size:90%;">OMC</span></td>
<td id="S4.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.6.1" class="ltx_text" style="font-size:90%;">SR</span></td>
<td id="S4.T2.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.7.1" class="ltx_text" style="font-size:90%;">CF</span></td>
<td id="S4.T2.2.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.8.1" class="ltx_text" style="font-size:90%;">GHLC</span></td>
<td id="S4.T2.2.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.9.1" class="ltx_text" style="font-size:90%;">PEL</span></td>
<td id="S4.T2.2.1.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.10.1" class="ltx_text" style="font-size:90%;">PA</span></td>
<td id="S4.T2.2.1.1.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.11.1" class="ltx_text" style="font-size:90%;">ST</span></td>
<td id="S4.T2.2.1.1.12" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.12.1" class="ltx_text" style="font-size:90%;">WC</span></td>
<td id="S4.T2.2.1.1.13" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.1.1.13.1" class="ltx_text" style="font-size:90%;">Other</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Q-Only</span></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">14.93</span></th>
<th id="S4.T2.2.2.2.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.2.2.3.1.1" class="ltx_p"><span id="S4.T2.2.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">14.64</span></span>
</span>
</th>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:90%;">14.19</span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.5.1" class="ltx_text" style="font-size:90%;">11.78</span></td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.6.1" class="ltx_text" style="font-size:90%;">15.94</span></td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.7.1" class="ltx_text" style="font-size:90%;">16.92</span></td>
<td id="S4.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.8.1" class="ltx_text" style="font-size:90%;">11.91</span></td>
<td id="S4.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.9.1" class="ltx_text" style="font-size:90%;">14.02</span></td>
<td id="S4.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.10.1" class="ltx_text" style="font-size:90%;">14.28</span></td>
<td id="S4.T2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.11.1" class="ltx_text" style="font-size:90%;">19.76</span></td>
<td id="S4.T2.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.12.1" class="ltx_text" style="font-size:90%;">25.74</span></td>
<td id="S4.T2.2.2.2.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.13.1" class="ltx_text" style="font-size:90%;">13.51</span></td>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.3.3.1.1" class="ltx_text" style="font-size:90%;">MLP</span></th>
<th id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.3.3.2.1" class="ltx_text" style="font-size:90%;">20.67</span></th>
<th id="S4.T2.2.3.3.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.3.3.3.1.1" class="ltx_p"><span id="S4.T2.2.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">21.33</span></span>
</span>
</th>
<td id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.4.1" class="ltx_text" style="font-size:90%;">15.81</span></td>
<td id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.5.1" class="ltx_text" style="font-size:90%;">17.76</span></td>
<td id="S4.T2.2.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.6.1" class="ltx_text" style="font-size:90%;">24.69</span></td>
<td id="S4.T2.2.3.3.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.7.1" class="ltx_text" style="font-size:90%;">21.81</span></td>
<td id="S4.T2.2.3.3.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.8.1" class="ltx_text" style="font-size:90%;">11.91</span></td>
<td id="S4.T2.2.3.3.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.9.1" class="ltx_text" style="font-size:90%;">17.15</span></td>
<td id="S4.T2.2.3.3.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.10.1" class="ltx_text" style="font-size:90%;">21.33</span></td>
<td id="S4.T2.2.3.3.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.11.1" class="ltx_text" style="font-size:90%;">19.29</span></td>
<td id="S4.T2.2.3.3.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.12.1" class="ltx_text" style="font-size:90%;">29.92</span></td>
<td id="S4.T2.2.3.3.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.3.13.1" class="ltx_text" style="font-size:90%;">19.81</span></td>
</tr>
<tr id="S4.T2.2.4.4" class="ltx_tr">
<th id="S4.T2.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.4.4.1.1" class="ltx_text" style="font-size:90%;">ArticleNet (AN)</span></th>
<th id="S4.T2.2.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.4.4.2.1" class="ltx_text" style="font-size:90%;">5.28</span></th>
<th id="S4.T2.2.4.4.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.4.4.3.1.1" class="ltx_p"><span id="S4.T2.2.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">4.48</span></span>
</span>
</th>
<td id="S4.T2.2.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.4.1" class="ltx_text" style="font-size:90%;">0.93</span></td>
<td id="S4.T2.2.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.5.1" class="ltx_text" style="font-size:90%;">5.09</span></td>
<td id="S4.T2.2.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.6.1" class="ltx_text" style="font-size:90%;">5.11</span></td>
<td id="S4.T2.2.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.7.1" class="ltx_text" style="font-size:90%;">5.69</span></td>
<td id="S4.T2.2.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.8.1" class="ltx_text" style="font-size:90%;">6.24</span></td>
<td id="S4.T2.2.4.4.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.9.1" class="ltx_text" style="font-size:90%;">3.13</span></td>
<td id="S4.T2.2.4.4.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.10.1" class="ltx_text" style="font-size:90%;">6.95</span></td>
<td id="S4.T2.2.4.4.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.11.1" class="ltx_text" style="font-size:90%;">5.00</span></td>
<td id="S4.T2.2.4.4.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.12.1" class="ltx_text" style="font-size:90%;">9.92</span></td>
<td id="S4.T2.2.4.4.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.13.1" class="ltx_text" style="font-size:90%;">5.33</span></td>
</tr>
<tr id="S4.T2.2.5.5" class="ltx_tr">
<th id="S4.T2.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.2.5.5.1.1" class="ltx_text" style="font-size:90%;">BAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.2.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T2.2.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.5.5.2.1" class="ltx_text" style="font-size:90%;">25.17</span></th>
<th id="S4.T2.2.5.5.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.5.5.3.1.1" class="ltx_p"><span id="S4.T2.2.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">23.79</span></span>
</span>
</th>
<td id="S4.T2.2.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.4.1" class="ltx_text" style="font-size:90%;">17.67</span></td>
<td id="S4.T2.2.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.5.1" class="ltx_text" style="font-size:90%;">22.43</span></td>
<td id="S4.T2.2.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.6.1" class="ltx_text" style="font-size:90%;">30.58</span></td>
<td id="S4.T2.2.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.7.1" class="ltx_text" style="font-size:90%;">27.90</span></td>
<td id="S4.T2.2.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.96</span></td>
<td id="S4.T2.2.5.5.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.9.1" class="ltx_text" style="font-size:90%;">20.33</span></td>
<td id="S4.T2.2.5.5.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.10.1" class="ltx_text" style="font-size:90%;">25.60</span></td>
<td id="S4.T2.2.5.5.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.11.1" class="ltx_text" style="font-size:90%;">20.95</span></td>
<td id="S4.T2.2.5.5.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.12.1" class="ltx_text ltx_font_bold" style="font-size:90%;">40.16</span></td>
<td id="S4.T2.2.5.5.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.13.1" class="ltx_text" style="font-size:90%;">22.46</span></td>
</tr>
<tr id="S4.T2.2.6.6" class="ltx_tr">
<th id="S4.T2.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.2.6.6.1.1" class="ltx_text" style="font-size:90%;">MUTAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.2.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S4.T2.2.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S4.T2.2.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.6.6.2.1" class="ltx_text" style="font-size:90%;">26.41</span></th>
<th id="S4.T2.2.6.6.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.6.6.3.1.1" class="ltx_p"><span id="S4.T2.2.6.6.3.1.1.1" class="ltx_text" style="font-size:90%;">25.36</span></span>
</span>
</th>
<td id="S4.T2.2.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.4.1" class="ltx_text" style="font-size:90%;">18.95</span></td>
<td id="S4.T2.2.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.5.1" class="ltx_text" style="font-size:90%;">24.02</span></td>
<td id="S4.T2.2.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.6.1" class="ltx_text" style="font-size:90%;">33.23</span></td>
<td id="S4.T2.2.6.6.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.7.1" class="ltx_text" style="font-size:90%;">27.73</span></td>
<td id="S4.T2.2.6.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.8.1" class="ltx_text" style="font-size:90%;">17.59</span></td>
<td id="S4.T2.2.6.6.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.9.1" class="ltx_text" style="font-size:90%;">20.09</span></td>
<td id="S4.T2.2.6.6.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">30.44</span></td>
<td id="S4.T2.2.6.6.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.11.1" class="ltx_text" style="font-size:90%;">20.48</span></td>
<td id="S4.T2.2.6.6.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.12.1" class="ltx_text" style="font-size:90%;">39.38</span></td>
<td id="S4.T2.2.6.6.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.6.6.13.1" class="ltx_text" style="font-size:90%;">22.46</span></td>
</tr>
<tr id="S4.T2.2.7.7" class="ltx_tr">
<th id="S4.T2.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.7.7.1.1" class="ltx_text" style="font-size:90%;">BAN + AN</span></th>
<th id="S4.T2.2.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.7.7.2.1" class="ltx_text" style="font-size:90%;">25.61</span></th>
<th id="S4.T2.2.7.7.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.7.7.3.1.1" class="ltx_p"><span id="S4.T2.2.7.7.3.1.1.1" class="ltx_text" style="font-size:90%;">24.45</span></span>
</span>
</th>
<td id="S4.T2.2.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.4.1" class="ltx_text" style="font-size:90%;">19.88</span></td>
<td id="S4.T2.2.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.5.1" class="ltx_text" style="font-size:90%;">21.59</span></td>
<td id="S4.T2.2.7.7.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.6.1" class="ltx_text" style="font-size:90%;">30.79</span></td>
<td id="S4.T2.2.7.7.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.7.1" class="ltx_text" style="font-size:90%;">29.12</span></td>
<td id="S4.T2.2.7.7.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.8.1" class="ltx_text" style="font-size:90%;">20.57</span></td>
<td id="S4.T2.2.7.7.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.9.1" class="ltx_text" style="font-size:90%;">21.54</span></td>
<td id="S4.T2.2.7.7.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.10.1" class="ltx_text" style="font-size:90%;">26.42</span></td>
<td id="S4.T2.2.7.7.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">27.14</span></td>
<td id="S4.T2.2.7.7.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.12.1" class="ltx_text" style="font-size:90%;">38.29</span></td>
<td id="S4.T2.2.7.7.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.13.1" class="ltx_text" style="font-size:90%;">22.16</span></td>
</tr>
<tr id="S4.T2.2.8.8" class="ltx_tr">
<th id="S4.T2.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.8.8.1.1" class="ltx_text" style="font-size:90%;">MUTAN + AN</span></th>
<th id="S4.T2.2.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">27.84</span></th>
<th id="S4.T2.2.8.8.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T2.2.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.8.8.3.1.1" class="ltx_p"><span id="S4.T2.2.8.8.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.56</span></span>
</span>
</th>
<td id="S4.T2.2.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">23.95</span></td>
<td id="S4.T2.2.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">26.87</span></td>
<td id="S4.T2.2.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">33.44</span></td>
<td id="S4.T2.2.8.8.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">29.94</span></td>
<td id="S4.T2.2.8.8.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.8.1" class="ltx_text" style="font-size:90%;">20.71</span></td>
<td id="S4.T2.2.8.8.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.05</span></td>
<td id="S4.T2.2.8.8.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.10.1" class="ltx_text" style="font-size:90%;">29.70</span></td>
<td id="S4.T2.2.8.8.11" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.11.1" class="ltx_text" style="font-size:90%;">24.76</span></td>
<td id="S4.T2.2.8.8.12" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.12.1" class="ltx_text" style="font-size:90%;">39.84</span></td>
<td id="S4.T2.2.8.8.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.8.8.13.1" class="ltx_text ltx_font_bold" style="font-size:90%;">23.62</span></td>
</tr>
<tr id="S4.T2.2.9.9" class="ltx_tr">
<th id="S4.T2.2.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.9.9.1.1" class="ltx_text" style="font-size:90%;">BAN/AN oracle</span></th>
<th id="S4.T2.2.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.9.9.2.1" class="ltx_text" style="font-size:90%;">27.59</span></th>
<th id="S4.T2.2.9.9.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.2.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.9.9.3.1.1" class="ltx_p"><span id="S4.T2.2.9.9.3.1.1.1" class="ltx_text" style="font-size:90%;">26.35</span></span>
</span>
</th>
<td id="S4.T2.2.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.4.1" class="ltx_text" style="font-size:90%;">18.26</span></td>
<td id="S4.T2.2.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.5.1" class="ltx_text" style="font-size:90%;">24.35</span></td>
<td id="S4.T2.2.9.9.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.6.1" class="ltx_text" style="font-size:90%;">33.12</span></td>
<td id="S4.T2.2.9.9.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.7.1" class="ltx_text" style="font-size:90%;">30.46</span></td>
<td id="S4.T2.2.9.9.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.8.1" class="ltx_text" style="font-size:90%;">28.51</span></td>
<td id="S4.T2.2.9.9.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.9.1" class="ltx_text" style="font-size:90%;">21.54</span></td>
<td id="S4.T2.2.9.9.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.10.1" class="ltx_text" style="font-size:90%;">28.79</span></td>
<td id="S4.T2.2.9.9.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.11.1" class="ltx_text" style="font-size:90%;">24.52</span></td>
<td id="S4.T2.2.9.9.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.12.1" class="ltx_text" style="font-size:90%;">41.4</span></td>
<td id="S4.T2.2.9.9.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.9.9.13.1" class="ltx_text" style="font-size:90%;">25.07</span></td>
</tr>
<tr id="S4.T2.2.10.10" class="ltx_tr">
<th id="S4.T2.2.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T2.2.10.10.1.1" class="ltx_text" style="font-size:90%;">MUTAN/AN oracle</span></th>
<th id="S4.T2.2.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T2.2.10.10.2.1" class="ltx_text" style="font-size:90%;">28.47</span></th>
<th id="S4.T2.2.10.10.3" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T2.2.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.10.10.3.1.1" class="ltx_p"><span id="S4.T2.2.10.10.3.1.1.1" class="ltx_text" style="font-size:90%;">27.28</span></span>
</span>
</th>
<td id="S4.T2.2.10.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.4.1" class="ltx_text" style="font-size:90%;">19.53</span></td>
<td id="S4.T2.2.10.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.5.1" class="ltx_text" style="font-size:90%;">25.28</span></td>
<td id="S4.T2.2.10.10.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.6.1" class="ltx_text" style="font-size:90%;">35.13</span></td>
<td id="S4.T2.2.10.10.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.7.1" class="ltx_text" style="font-size:90%;">30.53</span></td>
<td id="S4.T2.2.10.10.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.8.1" class="ltx_text" style="font-size:90%;">21.56</span></td>
<td id="S4.T2.2.10.10.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.9.1" class="ltx_text" style="font-size:90%;">21.68</span></td>
<td id="S4.T2.2.10.10.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.10.1" class="ltx_text" style="font-size:90%;">32.16</span></td>
<td id="S4.T2.2.10.10.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.11.1" class="ltx_text" style="font-size:90%;">24.76</span></td>
<td id="S4.T2.2.10.10.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.12.1" class="ltx_text" style="font-size:90%;">41.4</span></td>
<td id="S4.T2.2.10.10.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.10.10.13.1" class="ltx_text" style="font-size:90%;">24.85</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark results on <span id="S4.T2.5.2.1" class="ltx_text">OK-VQA</span>.<span id="S4.T2.5.2.2" class="ltx_text ltx_font_medium"> We show the results for the full OK-VQA dataset and for each knowledge category: Vehicles and Transportation (VT); Brands, Companies and Products (BCP); Objects, Material and Clothing (OMC); Sports and Recreation (SR); Cooking and Food (CF); Geography, History, Language and Culture (GHLC); People and Everyday Life (PEL); Plants and Animals (PA); Science and Technology (ST); Weather and Climate (WC); and Other.</span></span></figcaption>
</figure>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Question statistics.</span>
We also collected statistics for our dataset by looking at the number of questions, and by looking at which were most frequent for each knowledge category. OK-VQA has 12,591 unique questions out of 14,055 total, and 7,178 unique question words. This indicates that we get a variety of different questions and answers in our dataset.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1906.00067/assets/x3.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="622" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">For each category we show the question words and answers that have the highest relative frequency across our knowledge categories (i.e. frequency in category divided by overall frequency).</span></figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">We also looked at the variety of images in our dataset. As we stated earlier, our images come from the COCO image dataset, so our dataset contains the same basic distribution of images. However, we only use a subset of COCO images, so we want to see if we still get a wide distribution of images. For this, we ran a Places2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> classifier on our images and looked at the top-1 scene class for each image and compared that to COCO overall. Out of 365 scenes, our dataset contains all but 5 classes: hunting lodge, mansion, movie theater, ruin and volcano. These classes appear infrequently in the overall COCO dataset (10, 22, 28, 37 and 25 times respectively), so overall, we still captured quite a lot of the variation in scenes.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Finally, we show in Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> the question words and answers in each category that are the most “unique” to get a better idea of what types of questions we have in each of these categories. We calculate these for each knowledge category by looking at the number of appearances within the category over the total number in the dataset to see which question words and answers had the highest relative frequency in their category. When looking at the question words, we see words specific to categories such as bus in Vehicles and Transportation, sandwich in Cooking and Food, and clouds in Weather and Climate. We also see that the answers are also extremely related to each category, such as herbivore in Plants and Animals, and umpire in Sports and Recreation. In Appendix <a href="#A1" title="Appendix A More Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, we also show the most common question words and answers.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Benchmarking</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we evaluate current state-of-the-art VQA approaches and provide results for some baselines, including knowledge-based ones.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">MUTAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>:
Multimodal Tucker Fusion (MUTAN) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, a recent state-of-the-art tensor-based method for VQA. Specifically, we use the attention version of MUTAN, and choose the parameters to match the single best performing model of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">BAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>:
Bilinear Attention Networks for VQA. A recent state-of-the art VQA method that uses a co-attention mechanism between the question features and the bottom-up detection features of the image. We modify some hyperparameters to improve performance on our dataset (see Appendix <a href="#A3" title="Appendix C MUTAN+AN and BAN+AN Details ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">MLP</span>:
The MLP has 3 hidden layers with ReLU activations and hidden size <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S5.p4.1.m1.1a"><mn id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><cn type="integer" id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">2048</annotation></semantics></math> that concatenates the image and question features after a skip-thought GRU after one fully connected layer each. Like MUTAN, it uses fc7 features from ResNet-152.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Q-Only</span>:
The same model as MLP, but only takes the question features.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">ArticleNet (AN)</span>:
We consider a simple knowledge-based baseline that we refer to as ArticleNet. The idea is to retrieve some articles from Wikipedia for each question-image pair and then train a network to find the answer in the retrieved articles.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">Retrieving articles is composed of three steps. First, we collect possible search queries for each question-image pair. We come up with all possible queries for each question by combining words from the question and words that are identified by pre-trained image and scene classifiers. Second, we use the Wikipedia search API to get the top retrieved article for each query. Third, for each query and article, we extract a small subset of each article that is most relevant for the query by selecting the sentences within the article that best correspond to our query based on the frequency of those query words in the sentence.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.3" class="ltx_p">Once the sentences have been retrieved, the next step is to filter and encode them for use in VQA. Specifically, we train ArticleNet to predict whether and where the ground truth answers appear in the article and in each sentence. The architecture is shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. To find the answer to a question, we pick the top scoring word among the retrieved sentences. More specifically, we take the highest value of <math id="S5.p8.1.m1.2" class="ltx_Math" alttext="a_{w_{i}}.a_{sent}" display="inline"><semantics id="S5.p8.1.m1.2a"><mrow id="S5.p8.1.m1.2.2.2" xref="S5.p8.1.m1.2.2.3.cmml"><msub id="S5.p8.1.m1.1.1.1.1" xref="S5.p8.1.m1.1.1.1.1.cmml"><mi id="S5.p8.1.m1.1.1.1.1.2" xref="S5.p8.1.m1.1.1.1.1.2.cmml">a</mi><msub id="S5.p8.1.m1.1.1.1.1.3" xref="S5.p8.1.m1.1.1.1.1.3.cmml"><mi id="S5.p8.1.m1.1.1.1.1.3.2" xref="S5.p8.1.m1.1.1.1.1.3.2.cmml">w</mi><mi id="S5.p8.1.m1.1.1.1.1.3.3" xref="S5.p8.1.m1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo lspace="0em" rspace="0.167em" id="S5.p8.1.m1.2.2.2.3" xref="S5.p8.1.m1.2.2.3a.cmml">.</mo><msub id="S5.p8.1.m1.2.2.2.2" xref="S5.p8.1.m1.2.2.2.2.cmml"><mi id="S5.p8.1.m1.2.2.2.2.2" xref="S5.p8.1.m1.2.2.2.2.2.cmml">a</mi><mrow id="S5.p8.1.m1.2.2.2.2.3" xref="S5.p8.1.m1.2.2.2.2.3.cmml"><mi id="S5.p8.1.m1.2.2.2.2.3.2" xref="S5.p8.1.m1.2.2.2.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.2.2.2.2.3.1" xref="S5.p8.1.m1.2.2.2.2.3.1.cmml">​</mo><mi id="S5.p8.1.m1.2.2.2.2.3.3" xref="S5.p8.1.m1.2.2.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.2.2.2.2.3.1a" xref="S5.p8.1.m1.2.2.2.2.3.1.cmml">​</mo><mi id="S5.p8.1.m1.2.2.2.2.3.4" xref="S5.p8.1.m1.2.2.2.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p8.1.m1.2.2.2.2.3.1b" xref="S5.p8.1.m1.2.2.2.2.3.1.cmml">​</mo><mi id="S5.p8.1.m1.2.2.2.2.3.5" xref="S5.p8.1.m1.2.2.2.2.3.5.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p8.1.m1.2b"><apply id="S5.p8.1.m1.2.2.3.cmml" xref="S5.p8.1.m1.2.2.2"><csymbol cd="ambiguous" id="S5.p8.1.m1.2.2.3a.cmml" xref="S5.p8.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S5.p8.1.m1.1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p8.1.m1.1.1.1.1.1.cmml" xref="S5.p8.1.m1.1.1.1.1">subscript</csymbol><ci id="S5.p8.1.m1.1.1.1.1.2.cmml" xref="S5.p8.1.m1.1.1.1.1.2">𝑎</ci><apply id="S5.p8.1.m1.1.1.1.1.3.cmml" xref="S5.p8.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.p8.1.m1.1.1.1.1.3.1.cmml" xref="S5.p8.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.p8.1.m1.1.1.1.1.3.2.cmml" xref="S5.p8.1.m1.1.1.1.1.3.2">𝑤</ci><ci id="S5.p8.1.m1.1.1.1.1.3.3.cmml" xref="S5.p8.1.m1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S5.p8.1.m1.2.2.2.2.cmml" xref="S5.p8.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.p8.1.m1.2.2.2.2.1.cmml" xref="S5.p8.1.m1.2.2.2.2">subscript</csymbol><ci id="S5.p8.1.m1.2.2.2.2.2.cmml" xref="S5.p8.1.m1.2.2.2.2.2">𝑎</ci><apply id="S5.p8.1.m1.2.2.2.2.3.cmml" xref="S5.p8.1.m1.2.2.2.2.3"><times id="S5.p8.1.m1.2.2.2.2.3.1.cmml" xref="S5.p8.1.m1.2.2.2.2.3.1"></times><ci id="S5.p8.1.m1.2.2.2.2.3.2.cmml" xref="S5.p8.1.m1.2.2.2.2.3.2">𝑠</ci><ci id="S5.p8.1.m1.2.2.2.2.3.3.cmml" xref="S5.p8.1.m1.2.2.2.2.3.3">𝑒</ci><ci id="S5.p8.1.m1.2.2.2.2.3.4.cmml" xref="S5.p8.1.m1.2.2.2.2.3.4">𝑛</ci><ci id="S5.p8.1.m1.2.2.2.2.3.5.cmml" xref="S5.p8.1.m1.2.2.2.2.3.5">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.1.m1.2c">a_{w_{i}}.a_{sent}</annotation></semantics></math>, where <math id="S5.p8.2.m2.1" class="ltx_Math" alttext="a_{w_{i}}" display="inline"><semantics id="S5.p8.2.m2.1a"><msub id="S5.p8.2.m2.1.1" xref="S5.p8.2.m2.1.1.cmml"><mi id="S5.p8.2.m2.1.1.2" xref="S5.p8.2.m2.1.1.2.cmml">a</mi><msub id="S5.p8.2.m2.1.1.3" xref="S5.p8.2.m2.1.1.3.cmml"><mi id="S5.p8.2.m2.1.1.3.2" xref="S5.p8.2.m2.1.1.3.2.cmml">w</mi><mi id="S5.p8.2.m2.1.1.3.3" xref="S5.p8.2.m2.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S5.p8.2.m2.1b"><apply id="S5.p8.2.m2.1.1.cmml" xref="S5.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p8.2.m2.1.1.1.cmml" xref="S5.p8.2.m2.1.1">subscript</csymbol><ci id="S5.p8.2.m2.1.1.2.cmml" xref="S5.p8.2.m2.1.1.2">𝑎</ci><apply id="S5.p8.2.m2.1.1.3.cmml" xref="S5.p8.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p8.2.m2.1.1.3.1.cmml" xref="S5.p8.2.m2.1.1.3">subscript</csymbol><ci id="S5.p8.2.m2.1.1.3.2.cmml" xref="S5.p8.2.m2.1.1.3.2">𝑤</ci><ci id="S5.p8.2.m2.1.1.3.3.cmml" xref="S5.p8.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.2.m2.1c">a_{w_{i}}</annotation></semantics></math> is the score for the word being the answer and <math id="S5.p8.3.m3.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="S5.p8.3.m3.1a"><msub id="S5.p8.3.m3.1.1" xref="S5.p8.3.m3.1.1.cmml"><mi id="S5.p8.3.m3.1.1.2" xref="S5.p8.3.m3.1.1.2.cmml">a</mi><mrow id="S5.p8.3.m3.1.1.3" xref="S5.p8.3.m3.1.1.3.cmml"><mi id="S5.p8.3.m3.1.1.3.2" xref="S5.p8.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.p8.3.m3.1.1.3.1" xref="S5.p8.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.p8.3.m3.1.1.3.3" xref="S5.p8.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.p8.3.m3.1.1.3.1a" xref="S5.p8.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.p8.3.m3.1.1.3.4" xref="S5.p8.3.m3.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p8.3.m3.1.1.3.1b" xref="S5.p8.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.p8.3.m3.1.1.3.5" xref="S5.p8.3.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p8.3.m3.1b"><apply id="S5.p8.3.m3.1.1.cmml" xref="S5.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p8.3.m3.1.1.1.cmml" xref="S5.p8.3.m3.1.1">subscript</csymbol><ci id="S5.p8.3.m3.1.1.2.cmml" xref="S5.p8.3.m3.1.1.2">𝑎</ci><apply id="S5.p8.3.m3.1.1.3.cmml" xref="S5.p8.3.m3.1.1.3"><times id="S5.p8.3.m3.1.1.3.1.cmml" xref="S5.p8.3.m3.1.1.3.1"></times><ci id="S5.p8.3.m3.1.1.3.2.cmml" xref="S5.p8.3.m3.1.1.3.2">𝑠</ci><ci id="S5.p8.3.m3.1.1.3.3.cmml" xref="S5.p8.3.m3.1.1.3.3">𝑒</ci><ci id="S5.p8.3.m3.1.1.3.4.cmml" xref="S5.p8.3.m3.1.1.3.4">𝑛</ci><ci id="S5.p8.3.m3.1.1.3.5.cmml" xref="S5.p8.3.m3.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.3.m3.1c">a_{sent}</annotation></semantics></math> is the score for the sentence including the answer.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1906.00067/assets/x4.png" id="S5.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.20.9.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.17.8" class="ltx_text ltx_font_bold" style="font-size:90%;">ArticleNet architecture.<span id="S5.F5.17.8.8" class="ltx_text ltx_font_medium"> ArticleNet takes in the question <math id="S5.F5.10.1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.F5.10.1.1.m1.1b"><mi id="S5.F5.10.1.1.m1.1.1" xref="S5.F5.10.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.F5.10.1.1.m1.1c"><ci id="S5.F5.10.1.1.m1.1.1.cmml" xref="S5.F5.10.1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.10.1.1.m1.1d">Q</annotation></semantics></math> and visual features <math id="S5.F5.11.2.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S5.F5.11.2.2.m2.1b"><mi id="S5.F5.11.2.2.m2.1.1" xref="S5.F5.11.2.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.F5.11.2.2.m2.1c"><ci id="S5.F5.11.2.2.m2.1.1.cmml" xref="S5.F5.11.2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.11.2.2.m2.1d">V</annotation></semantics></math>. All modules within the dotted line box share weights. The output of the GRUs is used to classify each word as the answer or not <math id="S5.F5.12.3.3.m3.1" class="ltx_Math" alttext="a_{w_{i}}" display="inline"><semantics id="S5.F5.12.3.3.m3.1b"><msub id="S5.F5.12.3.3.m3.1.1" xref="S5.F5.12.3.3.m3.1.1.cmml"><mi id="S5.F5.12.3.3.m3.1.1.2" xref="S5.F5.12.3.3.m3.1.1.2.cmml">a</mi><msub id="S5.F5.12.3.3.m3.1.1.3" xref="S5.F5.12.3.3.m3.1.1.3.cmml"><mi id="S5.F5.12.3.3.m3.1.1.3.2" xref="S5.F5.12.3.3.m3.1.1.3.2.cmml">w</mi><mi id="S5.F5.12.3.3.m3.1.1.3.3" xref="S5.F5.12.3.3.m3.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S5.F5.12.3.3.m3.1c"><apply id="S5.F5.12.3.3.m3.1.1.cmml" xref="S5.F5.12.3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.F5.12.3.3.m3.1.1.1.cmml" xref="S5.F5.12.3.3.m3.1.1">subscript</csymbol><ci id="S5.F5.12.3.3.m3.1.1.2.cmml" xref="S5.F5.12.3.3.m3.1.1.2">𝑎</ci><apply id="S5.F5.12.3.3.m3.1.1.3.cmml" xref="S5.F5.12.3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.F5.12.3.3.m3.1.1.3.1.cmml" xref="S5.F5.12.3.3.m3.1.1.3">subscript</csymbol><ci id="S5.F5.12.3.3.m3.1.1.3.2.cmml" xref="S5.F5.12.3.3.m3.1.1.3.2">𝑤</ci><ci id="S5.F5.12.3.3.m3.1.1.3.3.cmml" xref="S5.F5.12.3.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.12.3.3.m3.1d">a_{w_{i}}</annotation></semantics></math>. The final GRU hidden states <math id="S5.F5.13.4.4.m4.1" class="ltx_Math" alttext="h_{title}" display="inline"><semantics id="S5.F5.13.4.4.m4.1b"><msub id="S5.F5.13.4.4.m4.1.1" xref="S5.F5.13.4.4.m4.1.1.cmml"><mi id="S5.F5.13.4.4.m4.1.1.2" xref="S5.F5.13.4.4.m4.1.1.2.cmml">h</mi><mrow id="S5.F5.13.4.4.m4.1.1.3" xref="S5.F5.13.4.4.m4.1.1.3.cmml"><mi id="S5.F5.13.4.4.m4.1.1.3.2" xref="S5.F5.13.4.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.F5.13.4.4.m4.1.1.3.1" xref="S5.F5.13.4.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.F5.13.4.4.m4.1.1.3.3" xref="S5.F5.13.4.4.m4.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.F5.13.4.4.m4.1.1.3.1b" xref="S5.F5.13.4.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.F5.13.4.4.m4.1.1.3.4" xref="S5.F5.13.4.4.m4.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.F5.13.4.4.m4.1.1.3.1c" xref="S5.F5.13.4.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.F5.13.4.4.m4.1.1.3.5" xref="S5.F5.13.4.4.m4.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.F5.13.4.4.m4.1.1.3.1d" xref="S5.F5.13.4.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.F5.13.4.4.m4.1.1.3.6" xref="S5.F5.13.4.4.m4.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F5.13.4.4.m4.1c"><apply id="S5.F5.13.4.4.m4.1.1.cmml" xref="S5.F5.13.4.4.m4.1.1"><csymbol cd="ambiguous" id="S5.F5.13.4.4.m4.1.1.1.cmml" xref="S5.F5.13.4.4.m4.1.1">subscript</csymbol><ci id="S5.F5.13.4.4.m4.1.1.2.cmml" xref="S5.F5.13.4.4.m4.1.1.2">ℎ</ci><apply id="S5.F5.13.4.4.m4.1.1.3.cmml" xref="S5.F5.13.4.4.m4.1.1.3"><times id="S5.F5.13.4.4.m4.1.1.3.1.cmml" xref="S5.F5.13.4.4.m4.1.1.3.1"></times><ci id="S5.F5.13.4.4.m4.1.1.3.2.cmml" xref="S5.F5.13.4.4.m4.1.1.3.2">𝑡</ci><ci id="S5.F5.13.4.4.m4.1.1.3.3.cmml" xref="S5.F5.13.4.4.m4.1.1.3.3">𝑖</ci><ci id="S5.F5.13.4.4.m4.1.1.3.4.cmml" xref="S5.F5.13.4.4.m4.1.1.3.4">𝑡</ci><ci id="S5.F5.13.4.4.m4.1.1.3.5.cmml" xref="S5.F5.13.4.4.m4.1.1.3.5">𝑙</ci><ci id="S5.F5.13.4.4.m4.1.1.3.6.cmml" xref="S5.F5.13.4.4.m4.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.13.4.4.m4.1d">h_{title}</annotation></semantics></math> and <math id="S5.F5.14.5.5.m5.1" class="ltx_Math" alttext="h_{sent}" display="inline"><semantics id="S5.F5.14.5.5.m5.1b"><msub id="S5.F5.14.5.5.m5.1.1" xref="S5.F5.14.5.5.m5.1.1.cmml"><mi id="S5.F5.14.5.5.m5.1.1.2" xref="S5.F5.14.5.5.m5.1.1.2.cmml">h</mi><mrow id="S5.F5.14.5.5.m5.1.1.3" xref="S5.F5.14.5.5.m5.1.1.3.cmml"><mi id="S5.F5.14.5.5.m5.1.1.3.2" xref="S5.F5.14.5.5.m5.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.F5.14.5.5.m5.1.1.3.1" xref="S5.F5.14.5.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.F5.14.5.5.m5.1.1.3.3" xref="S5.F5.14.5.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.F5.14.5.5.m5.1.1.3.1b" xref="S5.F5.14.5.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.F5.14.5.5.m5.1.1.3.4" xref="S5.F5.14.5.5.m5.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.F5.14.5.5.m5.1.1.3.1c" xref="S5.F5.14.5.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.F5.14.5.5.m5.1.1.3.5" xref="S5.F5.14.5.5.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F5.14.5.5.m5.1c"><apply id="S5.F5.14.5.5.m5.1.1.cmml" xref="S5.F5.14.5.5.m5.1.1"><csymbol cd="ambiguous" id="S5.F5.14.5.5.m5.1.1.1.cmml" xref="S5.F5.14.5.5.m5.1.1">subscript</csymbol><ci id="S5.F5.14.5.5.m5.1.1.2.cmml" xref="S5.F5.14.5.5.m5.1.1.2">ℎ</ci><apply id="S5.F5.14.5.5.m5.1.1.3.cmml" xref="S5.F5.14.5.5.m5.1.1.3"><times id="S5.F5.14.5.5.m5.1.1.3.1.cmml" xref="S5.F5.14.5.5.m5.1.1.3.1"></times><ci id="S5.F5.14.5.5.m5.1.1.3.2.cmml" xref="S5.F5.14.5.5.m5.1.1.3.2">𝑠</ci><ci id="S5.F5.14.5.5.m5.1.1.3.3.cmml" xref="S5.F5.14.5.5.m5.1.1.3.3">𝑒</ci><ci id="S5.F5.14.5.5.m5.1.1.3.4.cmml" xref="S5.F5.14.5.5.m5.1.1.3.4">𝑛</ci><ci id="S5.F5.14.5.5.m5.1.1.3.5.cmml" xref="S5.F5.14.5.5.m5.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.14.5.5.m5.1d">h_{sent}</annotation></semantics></math> are put through fully connected layers to predict if the answer is in the sentence <math id="S5.F5.15.6.6.m6.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="S5.F5.15.6.6.m6.1b"><msub id="S5.F5.15.6.6.m6.1.1" xref="S5.F5.15.6.6.m6.1.1.cmml"><mi id="S5.F5.15.6.6.m6.1.1.2" xref="S5.F5.15.6.6.m6.1.1.2.cmml">a</mi><mrow id="S5.F5.15.6.6.m6.1.1.3" xref="S5.F5.15.6.6.m6.1.1.3.cmml"><mi id="S5.F5.15.6.6.m6.1.1.3.2" xref="S5.F5.15.6.6.m6.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.F5.15.6.6.m6.1.1.3.1" xref="S5.F5.15.6.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.F5.15.6.6.m6.1.1.3.3" xref="S5.F5.15.6.6.m6.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.F5.15.6.6.m6.1.1.3.1b" xref="S5.F5.15.6.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.F5.15.6.6.m6.1.1.3.4" xref="S5.F5.15.6.6.m6.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.F5.15.6.6.m6.1.1.3.1c" xref="S5.F5.15.6.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.F5.15.6.6.m6.1.1.3.5" xref="S5.F5.15.6.6.m6.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F5.15.6.6.m6.1c"><apply id="S5.F5.15.6.6.m6.1.1.cmml" xref="S5.F5.15.6.6.m6.1.1"><csymbol cd="ambiguous" id="S5.F5.15.6.6.m6.1.1.1.cmml" xref="S5.F5.15.6.6.m6.1.1">subscript</csymbol><ci id="S5.F5.15.6.6.m6.1.1.2.cmml" xref="S5.F5.15.6.6.m6.1.1.2">𝑎</ci><apply id="S5.F5.15.6.6.m6.1.1.3.cmml" xref="S5.F5.15.6.6.m6.1.1.3"><times id="S5.F5.15.6.6.m6.1.1.3.1.cmml" xref="S5.F5.15.6.6.m6.1.1.3.1"></times><ci id="S5.F5.15.6.6.m6.1.1.3.2.cmml" xref="S5.F5.15.6.6.m6.1.1.3.2">𝑠</ci><ci id="S5.F5.15.6.6.m6.1.1.3.3.cmml" xref="S5.F5.15.6.6.m6.1.1.3.3">𝑒</ci><ci id="S5.F5.15.6.6.m6.1.1.3.4.cmml" xref="S5.F5.15.6.6.m6.1.1.3.4">𝑛</ci><ci id="S5.F5.15.6.6.m6.1.1.3.5.cmml" xref="S5.F5.15.6.6.m6.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.15.6.6.m6.1d">a_{sent}</annotation></semantics></math> or title <math id="S5.F5.16.7.7.m7.1" class="ltx_Math" alttext="a_{title}" display="inline"><semantics id="S5.F5.16.7.7.m7.1b"><msub id="S5.F5.16.7.7.m7.1.1" xref="S5.F5.16.7.7.m7.1.1.cmml"><mi id="S5.F5.16.7.7.m7.1.1.2" xref="S5.F5.16.7.7.m7.1.1.2.cmml">a</mi><mrow id="S5.F5.16.7.7.m7.1.1.3" xref="S5.F5.16.7.7.m7.1.1.3.cmml"><mi id="S5.F5.16.7.7.m7.1.1.3.2" xref="S5.F5.16.7.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.F5.16.7.7.m7.1.1.3.1" xref="S5.F5.16.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.F5.16.7.7.m7.1.1.3.3" xref="S5.F5.16.7.7.m7.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.F5.16.7.7.m7.1.1.3.1b" xref="S5.F5.16.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.F5.16.7.7.m7.1.1.3.4" xref="S5.F5.16.7.7.m7.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.F5.16.7.7.m7.1.1.3.1c" xref="S5.F5.16.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.F5.16.7.7.m7.1.1.3.5" xref="S5.F5.16.7.7.m7.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.F5.16.7.7.m7.1.1.3.1d" xref="S5.F5.16.7.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.F5.16.7.7.m7.1.1.3.6" xref="S5.F5.16.7.7.m7.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F5.16.7.7.m7.1c"><apply id="S5.F5.16.7.7.m7.1.1.cmml" xref="S5.F5.16.7.7.m7.1.1"><csymbol cd="ambiguous" id="S5.F5.16.7.7.m7.1.1.1.cmml" xref="S5.F5.16.7.7.m7.1.1">subscript</csymbol><ci id="S5.F5.16.7.7.m7.1.1.2.cmml" xref="S5.F5.16.7.7.m7.1.1.2">𝑎</ci><apply id="S5.F5.16.7.7.m7.1.1.3.cmml" xref="S5.F5.16.7.7.m7.1.1.3"><times id="S5.F5.16.7.7.m7.1.1.3.1.cmml" xref="S5.F5.16.7.7.m7.1.1.3.1"></times><ci id="S5.F5.16.7.7.m7.1.1.3.2.cmml" xref="S5.F5.16.7.7.m7.1.1.3.2">𝑡</ci><ci id="S5.F5.16.7.7.m7.1.1.3.3.cmml" xref="S5.F5.16.7.7.m7.1.1.3.3">𝑖</ci><ci id="S5.F5.16.7.7.m7.1.1.3.4.cmml" xref="S5.F5.16.7.7.m7.1.1.3.4">𝑡</ci><ci id="S5.F5.16.7.7.m7.1.1.3.5.cmml" xref="S5.F5.16.7.7.m7.1.1.3.5">𝑙</ci><ci id="S5.F5.16.7.7.m7.1.1.3.6.cmml" xref="S5.F5.16.7.7.m7.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.16.7.7.m7.1d">a_{title}</annotation></semantics></math>, and then are combined together and used to classify if the answer is in the article <math id="S5.F5.17.8.8.m8.1" class="ltx_Math" alttext="a_{art}" display="inline"><semantics id="S5.F5.17.8.8.m8.1b"><msub id="S5.F5.17.8.8.m8.1.1" xref="S5.F5.17.8.8.m8.1.1.cmml"><mi id="S5.F5.17.8.8.m8.1.1.2" xref="S5.F5.17.8.8.m8.1.1.2.cmml">a</mi><mrow id="S5.F5.17.8.8.m8.1.1.3" xref="S5.F5.17.8.8.m8.1.1.3.cmml"><mi id="S5.F5.17.8.8.m8.1.1.3.2" xref="S5.F5.17.8.8.m8.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.F5.17.8.8.m8.1.1.3.1" xref="S5.F5.17.8.8.m8.1.1.3.1.cmml">​</mo><mi id="S5.F5.17.8.8.m8.1.1.3.3" xref="S5.F5.17.8.8.m8.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.F5.17.8.8.m8.1.1.3.1b" xref="S5.F5.17.8.8.m8.1.1.3.1.cmml">​</mo><mi id="S5.F5.17.8.8.m8.1.1.3.4" xref="S5.F5.17.8.8.m8.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.F5.17.8.8.m8.1c"><apply id="S5.F5.17.8.8.m8.1.1.cmml" xref="S5.F5.17.8.8.m8.1.1"><csymbol cd="ambiguous" id="S5.F5.17.8.8.m8.1.1.1.cmml" xref="S5.F5.17.8.8.m8.1.1">subscript</csymbol><ci id="S5.F5.17.8.8.m8.1.1.2.cmml" xref="S5.F5.17.8.8.m8.1.1.2">𝑎</ci><apply id="S5.F5.17.8.8.m8.1.1.3.cmml" xref="S5.F5.17.8.8.m8.1.1.3"><times id="S5.F5.17.8.8.m8.1.1.3.1.cmml" xref="S5.F5.17.8.8.m8.1.1.3.1"></times><ci id="S5.F5.17.8.8.m8.1.1.3.2.cmml" xref="S5.F5.17.8.8.m8.1.1.3.2">𝑎</ci><ci id="S5.F5.17.8.8.m8.1.1.3.3.cmml" xref="S5.F5.17.8.8.m8.1.1.3.3">𝑟</ci><ci id="S5.F5.17.8.8.m8.1.1.3.4.cmml" xref="S5.F5.17.8.8.m8.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.17.8.8.m8.1d">a_{art}</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">For a more detailed description of ArticleNet see Appendix <a href="#A2.SS2" title="B.2 ArticleNet Overview ‣ Appendix B ArticleNet Details ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.1" class="ltx_p"><span id="S5.p10.1.1" class="ltx_text ltx_font_bold">MUTAN + AN</span>:
We augment MUTAN with the top sentence hidden states (<math id="S5.p10.1.m1.1" class="ltx_Math" alttext="h_{sent}" display="inline"><semantics id="S5.p10.1.m1.1a"><msub id="S5.p10.1.m1.1.1" xref="S5.p10.1.m1.1.1.cmml"><mi id="S5.p10.1.m1.1.1.2" xref="S5.p10.1.m1.1.1.2.cmml">h</mi><mrow id="S5.p10.1.m1.1.1.3" xref="S5.p10.1.m1.1.1.3.cmml"><mi id="S5.p10.1.m1.1.1.3.2" xref="S5.p10.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.p10.1.m1.1.1.3.1" xref="S5.p10.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.p10.1.m1.1.1.3.3" xref="S5.p10.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.p10.1.m1.1.1.3.1a" xref="S5.p10.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.p10.1.m1.1.1.3.4" xref="S5.p10.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p10.1.m1.1.1.3.1b" xref="S5.p10.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.p10.1.m1.1.1.3.5" xref="S5.p10.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.p10.1.m1.1b"><apply id="S5.p10.1.m1.1.1.cmml" xref="S5.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p10.1.m1.1.1.1.cmml" xref="S5.p10.1.m1.1.1">subscript</csymbol><ci id="S5.p10.1.m1.1.1.2.cmml" xref="S5.p10.1.m1.1.1.2">ℎ</ci><apply id="S5.p10.1.m1.1.1.3.cmml" xref="S5.p10.1.m1.1.1.3"><times id="S5.p10.1.m1.1.1.3.1.cmml" xref="S5.p10.1.m1.1.1.3.1"></times><ci id="S5.p10.1.m1.1.1.3.2.cmml" xref="S5.p10.1.m1.1.1.3.2">𝑠</ci><ci id="S5.p10.1.m1.1.1.3.3.cmml" xref="S5.p10.1.m1.1.1.3.3">𝑒</ci><ci id="S5.p10.1.m1.1.1.3.4.cmml" xref="S5.p10.1.m1.1.1.3.4">𝑛</ci><ci id="S5.p10.1.m1.1.1.3.5.cmml" xref="S5.p10.1.m1.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p10.1.m1.1c">h_{sent}</annotation></semantics></math> in Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) from ArticleNet (AN). During VQA training and testing, we take the top predicted sentences (ignoring duplicate sentences), and feed them in the memory of an end-to-end memory network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The output of the memory network is concatenated with the output of the first MUTAN fusion layer.</p>
</div>
<div id="S5.p11" class="ltx_para">
<p id="S5.p11.1" class="ltx_p"><span id="S5.p11.1.1" class="ltx_text ltx_font_bold">BAN + AN</span>:
Similarly, we incorporate the ArticleNet hidden states into BAN and incorporate it into VQA pipeline with another memory network. We concatenate output of the memory network with the BAN hidden state right before the final classification network. See Appendix <a href="#A3" title="Appendix C MUTAN+AN and BAN+AN Details ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for details.</p>
</div>
<div id="S5.p12" class="ltx_para">
<p id="S5.p12.1" class="ltx_p"><span id="S5.p12.1.1" class="ltx_text ltx_font_bold">MUTAN/AN oracle</span>:
As an upper bound check, and to see potentially how much VQA models could benefit from the knowledge retrieved using ArticleNet, we also provide results on an oracle, which simply takes the raw ArticleNet and MUTAN predictions, taking the best answer (comparing to ground truth) from either.</p>
</div>
<div id="S5.p13" class="ltx_para">
<p id="S5.p13.1" class="ltx_p"><span id="S5.p13.1.1" class="ltx_text ltx_font_bold">BAN/AN oracle</span>:
Similar to the MUTAN/AN oracle, but we take the best answer from the raw ArticleNet and BAN instead, again taking the best answer for each question.</p>
</div>
<div id="S5.p14" class="ltx_para ltx_noindent">
<p id="S5.p14.1" class="ltx_p"><span id="S5.p14.1.1" class="ltx_text ltx_font_bold">Benchmark results.</span> We report the results using the common VQA evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, but use each of our answer annotations twice, since we have 5 answer annotations versus 10 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. We also stem the answers using Porter stemming to consolidate answers that are identical except for pluralization and conjugation as in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. We also show the breakdowns for each of our knowledge categories. The results are reported in Table <a href="#S4.T2" title="Table 2 ‣ 4 Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.p15" class="ltx_para">
<p id="S5.p15.1" class="ltx_p">The first observation is that no method gets close to numbers on standard VQA dataset such as VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (where the best real open-ended result for the 2018 competition is 72.41). Moreover, state-of-the-art models such as MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which are specifically designed for VQA to learn high-level associations between the image and question, get far worse numbers on our dataset. This suggests that <span id="S5.p15.1.1" class="ltx_text">OK-VQA</span> cannot be solved simply by coming up with a clever model, but actually requires methods that incorporate information from outside the image.</p>
</div>
<div id="S5.p16" class="ltx_para">
<p id="S5.p16.1" class="ltx_p">It is interesting to note that although the performance of the raw ArticleNet is low, it provides improvement when combined with the state-of-the-art models (MUTAN + AN and BAN + AN). From the oracle numbers, we can see that the knowledge retrieved by ArticleNet provides complementary information to the state-of-the-art VQA models. These oracles are optimistic upper bounds using ArticleNet, but they show that smarter knowledge-retrieval approaches could have stronger performance on our dataset. Note that ArticleNet is not directly trained on VQA and can only predict answers within the articles it has retrieved. So the relatively low performance on VQA is not surprising.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/1906.00067/assets/x5.png" id="S5.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="160" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative results.<span id="S5.F6.5.2.1" class="ltx_text ltx_font_medium"> We show the result of MUTAN+AN compared to the MUTAN baseline answer and the ground truth answer (‘GT Ans’). We show the query words that were used by ArticleNet (pink boxes) and the corresponding most relevant sentences (blue boxes).</span></span></figcaption>
</figure>
<div id="S5.p17" class="ltx_para">
<p id="S5.p17.1" class="ltx_p">Looking at the category breakdowns, we see that ArticleNet is particularly helpful for brands, science, and cooking categories, perhaps suggesting that these categories are better represented in Wikipedia. It should be noted that the major portion of our dataset requires knowledge outside Wikipedia such as commonsense or visual knowledge.</p>
</div>
<div id="S5.p18" class="ltx_para">
<p id="S5.p18.1" class="ltx_p">The Q-Only baseline performs significantly worse than the other VQA baselines, suggesting that visual features are indeed necessary and our procedure for reducing answer bias was effective.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA score on <span id="S5.T3.2.1.1.2.1" class="ltx_text">OK-VQA</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<td id="S5.T3.2.2.1.1" class="ltx_td ltx_align_center ltx_border_t">ResNet152</td>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">26.41</td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<td id="S5.T3.2.3.2.1" class="ltx_td ltx_align_center">ResNet50</td>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_center">24.74</td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<td id="S5.T3.2.4.3.1" class="ltx_td ltx_align_center">ResNet18</td>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_center">23.64</td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<td id="S5.T3.2.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">Q-Only</td>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">14.93</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.5.2" class="ltx_text" style="font-size:90%;">Results on <span id="S5.T3.5.2.1" class="ltx_text">OK-VQA</span> with different visual features.</span></figcaption>
</figure>
<div id="S5.p19" class="ltx_para ltx_noindent">
<p id="S5.p19.1" class="ltx_p"><span id="S5.p19.1.1" class="ltx_text ltx_font_bold">Visual feature ablation.</span> We also want to demonstrate the difficulty of the dataset from the perspective of visual features, so we show MUTAN results using different ResNet architectures. The previously reported result for MUTAN is based on ResNet152. We also show the results using extracted features from ResNet50 and ResNet18 in Table <a href="#S5.T3" title="Table 3 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. From this table it can be seen that going from ResNet50 to ResNet152 features only has a marginal improvement, and similarly going from ResNet18 to ResNet50. However, going from ResNet18 to no image (Q-Only) causes a large drop in performance. This suggests that our dataset is indeed visually grounded, but better image features do not hugely improve the results, suggesting the difficulty lies in the retrieving the relevant knowledge and reasoning required to answer the questions.</p>
</div>
<div id="S5.p20" class="ltx_para ltx_noindent">
<p id="S5.p20.1" class="ltx_p"><span id="S5.p20.1.1" class="ltx_text ltx_font_bold">Scale ablation.</span> Finally, we investigate the degree to which the size of our dataset relates to its difficulty as opposed to the nature of the questions themselves. We first take a random subdivision of our training set and train MUTAN on progressively smaller subsets of the training data and evaluate on our original test set. Figure <a href="#S5.F7" title="Figure 7 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the results.</p>
</div>
<div id="S5.p21" class="ltx_para ltx_noindent">
<p id="S5.p21.1" class="ltx_p"><span id="S5.p21.1.1" class="ltx_text ltx_font_bold">Qualitative examples.</span> We show some qualitative examples in Figure <a href="#S5.F6" title="Figure 6 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> to see how outside knowledge helps VQA systems in a few examples. We compare MUTAN+AN method with MUTAN. The left example asks what “fruit family” the fruit in the image (oranges) comes from. We see that two sentences that directly contain the information that oranges are citrus fruits are retrieved —“The orange … is a fruit of the citrus species” and “The citrus sinensis is subdivided into four classes [including] common oranges”.</p>
</div>
<div id="S5.p22" class="ltx_para">
<p id="S5.p22.1" class="ltx_p">The middle example asks what liquid the animal (cow) produces. The first and third sentences tell us that cows produce milk, and the second sentence tells us that milk is a liquid. This gives the combined MUTAN+AN method enough information to correctly answer milk.</p>
</div>
<div id="S5.p23" class="ltx_para">
<p id="S5.p23.1" class="ltx_p">The example on the right asks how many chromosomes humans have. It is somewhat ambiguous whether it means how many individual chromosomes or how many pairs, so workers labeled both as answers. The retrieved articles are helpful here, retrieving two different articles referring to 23 pairs of chromosomes and 46 chromosomes total. The combined MUTAN+AN method correctly answers 23, while MUTAN guesses 3.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/1906.00067/assets/x6.png" id="S5.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="333" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.4.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.5.2" class="ltx_text" style="font-size:90%;">Results on <span id="S5.F7.5.2.1" class="ltx_text">OK-VQA</span> using different sizes of the training set.</span></figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We address the task of knowledge-based visual question answering. We introduce a novel benchmark called <span id="S6.p1.1.1" class="ltx_text">OK-VQA</span> for this task. Unlike the common VQA benchmarks, the information provided in the question and the corresponding images of <span id="S6.p1.1.2" class="ltx_text">OK-VQA</span> is not sufficient to answer the questions, and answering the questions requires reasoning on external knowledge resources. We show that the performance of state-of-the-art VQA models significantly drops on <span id="S6.p1.1.3" class="ltx_text">OK-VQA</span>. We analyze the properties and statistics of the dataset and show that background knowledge can improve results on our dataset. Our experimental evaluations show that the proposed benchmark is quite challenging and that there is a large room for improvement.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acknowledgements<span id="S6.p2.1.1.1" class="ltx_text ltx_font_medium">: We would like to thank everyone who took time to review this work and provide helpful comments. This work is in part supported by NSF IIS-165205, NSF IIS-1637479, NSF IIS-1703166, Sloan Fellowship, NVIDIA Artificial Intelligence Lab, and Allen Institute for artificial intelligence. Thanks to Aishwarya Agrawal, Gunnar Sigurdsson, Victoria Donley, Achal Dave, and Eric Kolve who provided valuable assistance, advice and feedback. Kenneth Marino is supported by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program. This research is sponsored by Army Research Office and accomplished under Grant Number W911NF-18-1-0019.</span></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence
Zitnick, Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">VQA: visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Hedi Ben-younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Semantic parsing on freebase from question-answer pairs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Antoine Bordes, Sumit Chopra, and Jason Weston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Question answering with subgraph embeddings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Reading wikipedia to answer open-domain questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">An implementation of faster rcnn with study for region sampling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Neil: Extracting visual knowledge from web data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and
Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Embodied question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Santosh Divvala, Ali Farhadi, and Carlos Guestrin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Learning everything about anything: Webly-supervised visual concept
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Are you talking to a machine? dataset and methods for multilingual
image question.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Fast r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter
Fox, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">IQA: Visual question answering in interactive environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Learning to reason: End-to-end module networks for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Sergey Ioffe and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">TGIF-QA: Toward spatio-temporal reasoning in visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Pythia v0. 1: the winning entry to the vqa challenge 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.09956</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li
Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Inferring and executing programs for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Bilinear Attention Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1805.07932</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Skip-thought vectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan
Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Ask me anything: Dynamic memory networks for natural language
processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Guohao Li, Hang Su, and Wenwu Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Incorporating external knowledge to answer open-domain visual
questions with dynamic memory networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B.
Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Hierarchical question-image co-attention for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Mateusz Malinowski and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Marioqa: Answering questions by watching gameplay videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Medhini Narasimhan, Svetlana Lazebnik, and Alexander G Schwing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Out of the box: Reasoning with graph convolution nets for factual
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Medhini Narasimhan and Alexander G. Schwing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Straight to the facts: Learning knowledge base retrieval for factual
visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Mengye Ren, Ryan Kiros, and Richard S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Imagenet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Fereshteh Sadeghi, Santosh K Divvala, and Ali Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Viske: Visual knowledge extraction and question answering by visual
verification of relation phrases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">End-to-end memory networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel
Urtasun, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Movieqa: Understanding stories in movies through question-answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and Song-Chun Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Joint video and text parsing for understanding events and answering
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE MultiMedia</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Explicit knowledge-based reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, and Anthony R. Dick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Fvqa: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Qi Wu, Peng Wang, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Ask me anything: Free-form visual question answering based on
knowledge from external sources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Caiming Xiong, Stephen Merity, and Richard Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Dynamic memory networks for visual and textual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Huijuan Xu and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Xuchen Yao and Benjamin Van Durme.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Information extraction over structured data: Question answering with
freebase.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Semantic parsing via staged query graph generation: Question
answering with knowledge base.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL-IJCNLP</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Eunbyung Park, Alexander C. Berg, and Tamara L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Visual madlibs: Fill in the blank description generation and question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Places: A 10 million image database for scene recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Alireza Fathi, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Reasoning about object affordances in a knowledge base
representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Visual7w: Grounded question answering in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Joseph J. Lim, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Knowledge acquisition for visual question answering via iterative
querying.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Ce Zhang, Christopher Ré, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Building a large-scale multimodal knowledge base for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More Dataset Statistics</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In Figure <a href="#A1.F8" title="Figure 8 ‣ Appendix A More Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> we show the distribution of the lengths of the answers in the dataset. In Figure <a href="#A1.F9" title="Figure 9 ‣ Appendix A More Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we show the distribution of answer frequency for each of the unique answers in the dataset.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">In Figure <a href="#A1.F10" title="Figure 10 ‣ Appendix A More Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> we show the most common and the highest “relative frequency” question words and answers in each category.</p>
</div>
<figure id="A1.F8" class="ltx_figure"><img src="/html/1906.00067/assets/x7.png" id="A1.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.4.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A1.F8.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer length distribution.<span id="A1.F8.5.2.1" class="ltx_text ltx_font_medium"> Histogram of the answer lengths in the dataset.</span></span></figcaption>
</figure>
<figure id="A1.F9" class="ltx_figure"><img src="/html/1906.00067/assets/x8.png" id="A1.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.4.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A1.F9.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer frequency distribution.<span id="A1.F9.5.2.1" class="ltx_text ltx_font_medium"> Histogram of the frequency of answers in the dataset. All 5 answers for each question are considered to compute the histogram. This shows for instance that answers that appear between 6 and 10 times in the dataset make up about 10% of all answers.</span></span></figcaption>
</figure>
<figure id="A1.F10" class="ltx_figure"><img src="/html/1906.00067/assets/x9.png" id="A1.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A1.F10.4.2" class="ltx_text" style="font-size:90%;">For each category we show the question words and answers that have the highest frequency and relative frequency across our knowledge categories (i.e. frequency in category divided by overall frequency).</span></figcaption>
</figure>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">Some other relevant dataset statistics for <span id="A1.p3.1.1" class="ltx_text">OK-VQA</span> can be found in Table <a href="#A1.T4" title="Table 4 ‣ Appendix A More Dataset Statistics ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<table id="A1.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.2.1.1" class="ltx_tr">
<td id="A1.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">Number of unique answers</td>
<td id="A1.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">14,454</td>
</tr>
<tr id="A1.T4.2.2.2" class="ltx_tr">
<td id="A1.T4.2.2.2.1" class="ltx_td ltx_align_center">Test set covered by top 2000 answers</td>
<td id="A1.T4.2.2.2.2" class="ltx_td ltx_align_center">88.5057%</td>
</tr>
<tr id="A1.T4.2.3.3" class="ltx_tr">
<td id="A1.T4.2.3.3.1" class="ltx_td ltx_align_center">Number of unique questions</td>
<td id="A1.T4.2.3.3.2" class="ltx_td ltx_align_center">12,591</td>
</tr>
<tr id="A1.T4.2.4.4" class="ltx_tr">
<td id="A1.T4.2.4.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">Number of unique question words</td>
<td id="A1.T4.2.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">7,178</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.5.2" class="ltx_text" style="font-size:90%;">More <span id="A1.T4.5.2.1" class="ltx_text">OK-VQA</span> dataset statistics.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>ArticleNet Details</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Article Collection</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Extracting the articles is composed of three steps: collecting possible search queries, using the Wikipedia search API to get the top retrieved article for each query and extracting a small subset of each article that is most relevant for the query.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">To perform the search query step, first we need to come up with possible queries for each question. We extract all non-stop words (i.e. remove “the”, “a”, “what”, etc.) from the question itself. Next we extract visual entities from the images by taking the top classifications from trained classifiers. We take the top classifications from an object classifier (trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>), a place classifier (trained on Places2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>) and an object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (trained on COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>). Figure <a href="#A2.F11" title="Figure 11 ‣ B.1 Article Collection ‣ Appendix B ArticleNet Details ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows some example images and their corresponding questions and classifications and shows some example queries that can be generated for that question.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<p id="A2.SS1.p3.1" class="ltx_p">Once the query words are selected, we compute possible queries. We choose every query word by itself and every two word combination of the query words as possible queries. We then retrieve the top article for each query from the Wikipedia search. Using the retrieval model from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to achieve a consistent snapshot, we retrieve the raw text.</p>
</div>
<div id="A2.SS1.p4" class="ltx_para">
<p id="A2.SS1.p4.1" class="ltx_p">Finally, we use the original query and the retrieved Wikipedia article to extract the most relevant sentences from the article for the query. Essentially, we perform another step of retrieval. The sentence priority is determined by three hierarchical metrics: (1) the number of unique query words in the sentence, (2) the total number of query words in the sentence, counting repeats, (3) the order of the sentence in the article. The priority is determined by factor (1). If two sentences tie on this metric, we use metric (2) as a tie breaker, and similarly we use metric (3) to break ties for metric (2).</p>
</div>
<div id="A2.SS1.p5" class="ltx_para">
<p id="A2.SS1.p5.3" class="ltx_p">After these steps, we have our final “article” for each query consisting of the title, and <math id="A2.SS1.p5.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="A2.SS1.p5.1.m1.1a"><mi id="A2.SS1.p5.1.m1.1.1" xref="A2.SS1.p5.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.1.m1.1b"><ci id="A2.SS1.p5.1.m1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.1.m1.1c">T</annotation></semantics></math> most relevant sentences (in our case <math id="A2.SS1.p5.2.m2.1" class="ltx_Math" alttext="T=5" display="inline"><semantics id="A2.SS1.p5.2.m2.1a"><mrow id="A2.SS1.p5.2.m2.1.1" xref="A2.SS1.p5.2.m2.1.1.cmml"><mi id="A2.SS1.p5.2.m2.1.1.2" xref="A2.SS1.p5.2.m2.1.1.2.cmml">T</mi><mo id="A2.SS1.p5.2.m2.1.1.1" xref="A2.SS1.p5.2.m2.1.1.1.cmml">=</mo><mn id="A2.SS1.p5.2.m2.1.1.3" xref="A2.SS1.p5.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.2.m2.1b"><apply id="A2.SS1.p5.2.m2.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1"><eq id="A2.SS1.p5.2.m2.1.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1.1"></eq><ci id="A2.SS1.p5.2.m2.1.1.2.cmml" xref="A2.SS1.p5.2.m2.1.1.2">𝑇</ci><cn type="integer" id="A2.SS1.p5.2.m2.1.1.3.cmml" xref="A2.SS1.p5.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.2.m2.1c">T=5</annotation></semantics></math>). In our experiments, we retrieve on the order of <math id="A2.SS1.p5.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A2.SS1.p5.3.m3.1a"><mn id="A2.SS1.p5.3.m3.1.1" xref="A2.SS1.p5.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.3.m3.1b"><cn type="integer" id="A2.SS1.p5.3.m3.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.3.m3.1c">100</annotation></semantics></math> articles for each question at this step.</p>
</div>
<figure id="A2.F11" class="ltx_figure"><img src="/html/1906.00067/assets/x10.png" id="A2.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="331" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F11.4.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A2.F11.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Example generated queries.<span id="A2.F11.5.2.1" class="ltx_text ltx_font_medium"> For some example questions, we show the image, question and top classification results from the trained models. In the rightmost column, we show some example queries that can be constructed for each question.</span></span></figcaption>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>ArticleNet Overview</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">Once the Wikipedia articles have been retrieved, the next step is to filter and encode them for use in VQA. Simple encodings such as an average word2vec encoding, or with skip-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> are not suitable for encoding long articles. Hence, we train an encoding specific to our data and useful for our eventual task. Taking inspiration from the fact that a hidden layer of a network trained on ImageNet is a good representation for images, we train a network on the retrieved articles on a proxy task to get a good representation. Specifically, we train ArticleNet to predict whether and where the ground truth answers appear in the article and each sentence. This also gives a way to narrow down the hundreds of articles for each question-image pair to a handful for the final VQA training.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.5" class="ltx_p">For each of the Wikipedia articles, each word and series of words in the sentence are compared to the ground truth answer for that question to see if they match (using Porter stemming). Hence, a label <math id="A2.SS2.p2.1.m1.1" class="ltx_Math" alttext="l_{art}" display="inline"><semantics id="A2.SS2.p2.1.m1.1a"><msub id="A2.SS2.p2.1.m1.1.1" xref="A2.SS2.p2.1.m1.1.1.cmml"><mi id="A2.SS2.p2.1.m1.1.1.2" xref="A2.SS2.p2.1.m1.1.1.2.cmml">l</mi><mrow id="A2.SS2.p2.1.m1.1.1.3" xref="A2.SS2.p2.1.m1.1.1.3.cmml"><mi id="A2.SS2.p2.1.m1.1.1.3.2" xref="A2.SS2.p2.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.1.m1.1.1.3.1" xref="A2.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.1.m1.1.1.3.3" xref="A2.SS2.p2.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.1.m1.1.1.3.1a" xref="A2.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.1.m1.1.1.3.4" xref="A2.SS2.p2.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.1.m1.1b"><apply id="A2.SS2.p2.1.m1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p2.1.m1.1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="A2.SS2.p2.1.m1.1.1.2.cmml" xref="A2.SS2.p2.1.m1.1.1.2">𝑙</ci><apply id="A2.SS2.p2.1.m1.1.1.3.cmml" xref="A2.SS2.p2.1.m1.1.1.3"><times id="A2.SS2.p2.1.m1.1.1.3.1.cmml" xref="A2.SS2.p2.1.m1.1.1.3.1"></times><ci id="A2.SS2.p2.1.m1.1.1.3.2.cmml" xref="A2.SS2.p2.1.m1.1.1.3.2">𝑎</ci><ci id="A2.SS2.p2.1.m1.1.1.3.3.cmml" xref="A2.SS2.p2.1.m1.1.1.3.3">𝑟</ci><ci id="A2.SS2.p2.1.m1.1.1.3.4.cmml" xref="A2.SS2.p2.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.1.m1.1c">l_{art}</annotation></semantics></math> is obtained if the answer appears in the article, and also a label <math id="A2.SS2.p2.2.m2.1" class="ltx_Math" alttext="l_{title}" display="inline"><semantics id="A2.SS2.p2.2.m2.1a"><msub id="A2.SS2.p2.2.m2.1.1" xref="A2.SS2.p2.2.m2.1.1.cmml"><mi id="A2.SS2.p2.2.m2.1.1.2" xref="A2.SS2.p2.2.m2.1.1.2.cmml">l</mi><mrow id="A2.SS2.p2.2.m2.1.1.3" xref="A2.SS2.p2.2.m2.1.1.3.cmml"><mi id="A2.SS2.p2.2.m2.1.1.3.2" xref="A2.SS2.p2.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.1.1.3.1" xref="A2.SS2.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.2.m2.1.1.3.3" xref="A2.SS2.p2.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.1.1.3.1a" xref="A2.SS2.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.2.m2.1.1.3.4" xref="A2.SS2.p2.2.m2.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.1.1.3.1b" xref="A2.SS2.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.2.m2.1.1.3.5" xref="A2.SS2.p2.2.m2.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.1.1.3.1c" xref="A2.SS2.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.2.m2.1.1.3.6" xref="A2.SS2.p2.2.m2.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.2.m2.1b"><apply id="A2.SS2.p2.2.m2.1.1.cmml" xref="A2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.1.1.1.cmml" xref="A2.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="A2.SS2.p2.2.m2.1.1.2.cmml" xref="A2.SS2.p2.2.m2.1.1.2">𝑙</ci><apply id="A2.SS2.p2.2.m2.1.1.3.cmml" xref="A2.SS2.p2.2.m2.1.1.3"><times id="A2.SS2.p2.2.m2.1.1.3.1.cmml" xref="A2.SS2.p2.2.m2.1.1.3.1"></times><ci id="A2.SS2.p2.2.m2.1.1.3.2.cmml" xref="A2.SS2.p2.2.m2.1.1.3.2">𝑡</ci><ci id="A2.SS2.p2.2.m2.1.1.3.3.cmml" xref="A2.SS2.p2.2.m2.1.1.3.3">𝑖</ci><ci id="A2.SS2.p2.2.m2.1.1.3.4.cmml" xref="A2.SS2.p2.2.m2.1.1.3.4">𝑡</ci><ci id="A2.SS2.p2.2.m2.1.1.3.5.cmml" xref="A2.SS2.p2.2.m2.1.1.3.5">𝑙</ci><ci id="A2.SS2.p2.2.m2.1.1.3.6.cmml" xref="A2.SS2.p2.2.m2.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.2.m2.1c">l_{title}</annotation></semantics></math> and <math id="A2.SS2.p2.3.m3.1" class="ltx_Math" alttext="l_{sent_{i}}" display="inline"><semantics id="A2.SS2.p2.3.m3.1a"><msub id="A2.SS2.p2.3.m3.1.1" xref="A2.SS2.p2.3.m3.1.1.cmml"><mi id="A2.SS2.p2.3.m3.1.1.2" xref="A2.SS2.p2.3.m3.1.1.2.cmml">l</mi><mrow id="A2.SS2.p2.3.m3.1.1.3" xref="A2.SS2.p2.3.m3.1.1.3.cmml"><mi id="A2.SS2.p2.3.m3.1.1.3.2" xref="A2.SS2.p2.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.3.m3.1.1.3.1" xref="A2.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.3.m3.1.1.3.3" xref="A2.SS2.p2.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.3.m3.1.1.3.1a" xref="A2.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.3.m3.1.1.3.4" xref="A2.SS2.p2.3.m3.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.3.m3.1.1.3.1b" xref="A2.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><msub id="A2.SS2.p2.3.m3.1.1.3.5" xref="A2.SS2.p2.3.m3.1.1.3.5.cmml"><mi id="A2.SS2.p2.3.m3.1.1.3.5.2" xref="A2.SS2.p2.3.m3.1.1.3.5.2.cmml">t</mi><mi id="A2.SS2.p2.3.m3.1.1.3.5.3" xref="A2.SS2.p2.3.m3.1.1.3.5.3.cmml">i</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.3.m3.1b"><apply id="A2.SS2.p2.3.m3.1.1.cmml" xref="A2.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS2.p2.3.m3.1.1.1.cmml" xref="A2.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="A2.SS2.p2.3.m3.1.1.2.cmml" xref="A2.SS2.p2.3.m3.1.1.2">𝑙</ci><apply id="A2.SS2.p2.3.m3.1.1.3.cmml" xref="A2.SS2.p2.3.m3.1.1.3"><times id="A2.SS2.p2.3.m3.1.1.3.1.cmml" xref="A2.SS2.p2.3.m3.1.1.3.1"></times><ci id="A2.SS2.p2.3.m3.1.1.3.2.cmml" xref="A2.SS2.p2.3.m3.1.1.3.2">𝑠</ci><ci id="A2.SS2.p2.3.m3.1.1.3.3.cmml" xref="A2.SS2.p2.3.m3.1.1.3.3">𝑒</ci><ci id="A2.SS2.p2.3.m3.1.1.3.4.cmml" xref="A2.SS2.p2.3.m3.1.1.3.4">𝑛</ci><apply id="A2.SS2.p2.3.m3.1.1.3.5.cmml" xref="A2.SS2.p2.3.m3.1.1.3.5"><csymbol cd="ambiguous" id="A2.SS2.p2.3.m3.1.1.3.5.1.cmml" xref="A2.SS2.p2.3.m3.1.1.3.5">subscript</csymbol><ci id="A2.SS2.p2.3.m3.1.1.3.5.2.cmml" xref="A2.SS2.p2.3.m3.1.1.3.5.2">𝑡</ci><ci id="A2.SS2.p2.3.m3.1.1.3.5.3.cmml" xref="A2.SS2.p2.3.m3.1.1.3.5.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.3.m3.1c">l_{sent_{i}}</annotation></semantics></math> if the answer appears in the title or sentence <math id="A2.SS2.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A2.SS2.p2.4.m4.1a"><mi id="A2.SS2.p2.4.m4.1.1" xref="A2.SS2.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.4.m4.1b"><ci id="A2.SS2.p2.4.m4.1.1.cmml" xref="A2.SS2.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.4.m4.1c">i</annotation></semantics></math>, and a label <math id="A2.SS2.p2.5.m5.1" class="ltx_Math" alttext="l_{word_{j}}" display="inline"><semantics id="A2.SS2.p2.5.m5.1a"><msub id="A2.SS2.p2.5.m5.1.1" xref="A2.SS2.p2.5.m5.1.1.cmml"><mi id="A2.SS2.p2.5.m5.1.1.2" xref="A2.SS2.p2.5.m5.1.1.2.cmml">l</mi><mrow id="A2.SS2.p2.5.m5.1.1.3" xref="A2.SS2.p2.5.m5.1.1.3.cmml"><mi id="A2.SS2.p2.5.m5.1.1.3.2" xref="A2.SS2.p2.5.m5.1.1.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.5.m5.1.1.3.1" xref="A2.SS2.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.5.m5.1.1.3.3" xref="A2.SS2.p2.5.m5.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.5.m5.1.1.3.1a" xref="A2.SS2.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p2.5.m5.1.1.3.4" xref="A2.SS2.p2.5.m5.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p2.5.m5.1.1.3.1b" xref="A2.SS2.p2.5.m5.1.1.3.1.cmml">​</mo><msub id="A2.SS2.p2.5.m5.1.1.3.5" xref="A2.SS2.p2.5.m5.1.1.3.5.cmml"><mi id="A2.SS2.p2.5.m5.1.1.3.5.2" xref="A2.SS2.p2.5.m5.1.1.3.5.2.cmml">d</mi><mi id="A2.SS2.p2.5.m5.1.1.3.5.3" xref="A2.SS2.p2.5.m5.1.1.3.5.3.cmml">j</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.5.m5.1b"><apply id="A2.SS2.p2.5.m5.1.1.cmml" xref="A2.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS2.p2.5.m5.1.1.1.cmml" xref="A2.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="A2.SS2.p2.5.m5.1.1.2.cmml" xref="A2.SS2.p2.5.m5.1.1.2">𝑙</ci><apply id="A2.SS2.p2.5.m5.1.1.3.cmml" xref="A2.SS2.p2.5.m5.1.1.3"><times id="A2.SS2.p2.5.m5.1.1.3.1.cmml" xref="A2.SS2.p2.5.m5.1.1.3.1"></times><ci id="A2.SS2.p2.5.m5.1.1.3.2.cmml" xref="A2.SS2.p2.5.m5.1.1.3.2">𝑤</ci><ci id="A2.SS2.p2.5.m5.1.1.3.3.cmml" xref="A2.SS2.p2.5.m5.1.1.3.3">𝑜</ci><ci id="A2.SS2.p2.5.m5.1.1.3.4.cmml" xref="A2.SS2.p2.5.m5.1.1.3.4">𝑟</ci><apply id="A2.SS2.p2.5.m5.1.1.3.5.cmml" xref="A2.SS2.p2.5.m5.1.1.3.5"><csymbol cd="ambiguous" id="A2.SS2.p2.5.m5.1.1.3.5.1.cmml" xref="A2.SS2.p2.5.m5.1.1.3.5">subscript</csymbol><ci id="A2.SS2.p2.5.m5.1.1.3.5.2.cmml" xref="A2.SS2.p2.5.m5.1.1.3.5.2">𝑑</ci><ci id="A2.SS2.p2.5.m5.1.1.3.5.3.cmml" xref="A2.SS2.p2.5.m5.1.1.3.5.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.5.m5.1c">l_{word_{j}}</annotation></semantics></math> for each word in the title and sentence.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.6" class="ltx_p">The architecture of the ArticleNet is shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Benchmarking ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The inputs to the network are the question <math id="A2.SS2.p3.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A2.SS2.p3.1.m1.1a"><mi id="A2.SS2.p3.1.m1.1.1" xref="A2.SS2.p3.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.1.m1.1b"><ci id="A2.SS2.p3.1.m1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.1.m1.1c">Q</annotation></semantics></math>, the visual features <math id="A2.SS2.p3.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="A2.SS2.p3.2.m2.1a"><mi id="A2.SS2.p3.2.m2.1.1" xref="A2.SS2.p3.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.2.m2.1b"><ci id="A2.SS2.p3.2.m2.1.1.cmml" xref="A2.SS2.p3.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.2.m2.1c">V</annotation></semantics></math> taken from an ImageNet trained ResNet152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the title of the Wikipedia article, and the <math id="A2.SS2.p3.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="A2.SS2.p3.3.m3.1a"><mi id="A2.SS2.p3.3.m3.1.1" xref="A2.SS2.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.3.m3.1b"><ci id="A2.SS2.p3.3.m3.1.1.cmml" xref="A2.SS2.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.3.m3.1c">T</annotation></semantics></math> sentences of the article (retrieved by the method explained in the previous section). From these inputs, it predicts whether the answer is in the title <math id="A2.SS2.p3.4.m4.1" class="ltx_Math" alttext="a_{title}" display="inline"><semantics id="A2.SS2.p3.4.m4.1a"><msub id="A2.SS2.p3.4.m4.1.1" xref="A2.SS2.p3.4.m4.1.1.cmml"><mi id="A2.SS2.p3.4.m4.1.1.2" xref="A2.SS2.p3.4.m4.1.1.2.cmml">a</mi><mrow id="A2.SS2.p3.4.m4.1.1.3" xref="A2.SS2.p3.4.m4.1.1.3.cmml"><mi id="A2.SS2.p3.4.m4.1.1.3.2" xref="A2.SS2.p3.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.4.m4.1.1.3.1" xref="A2.SS2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.4.m4.1.1.3.3" xref="A2.SS2.p3.4.m4.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.4.m4.1.1.3.1a" xref="A2.SS2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.4.m4.1.1.3.4" xref="A2.SS2.p3.4.m4.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.4.m4.1.1.3.1b" xref="A2.SS2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.4.m4.1.1.3.5" xref="A2.SS2.p3.4.m4.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.4.m4.1.1.3.1c" xref="A2.SS2.p3.4.m4.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.4.m4.1.1.3.6" xref="A2.SS2.p3.4.m4.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.4.m4.1b"><apply id="A2.SS2.p3.4.m4.1.1.cmml" xref="A2.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.4.m4.1.1.1.cmml" xref="A2.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="A2.SS2.p3.4.m4.1.1.2.cmml" xref="A2.SS2.p3.4.m4.1.1.2">𝑎</ci><apply id="A2.SS2.p3.4.m4.1.1.3.cmml" xref="A2.SS2.p3.4.m4.1.1.3"><times id="A2.SS2.p3.4.m4.1.1.3.1.cmml" xref="A2.SS2.p3.4.m4.1.1.3.1"></times><ci id="A2.SS2.p3.4.m4.1.1.3.2.cmml" xref="A2.SS2.p3.4.m4.1.1.3.2">𝑡</ci><ci id="A2.SS2.p3.4.m4.1.1.3.3.cmml" xref="A2.SS2.p3.4.m4.1.1.3.3">𝑖</ci><ci id="A2.SS2.p3.4.m4.1.1.3.4.cmml" xref="A2.SS2.p3.4.m4.1.1.3.4">𝑡</ci><ci id="A2.SS2.p3.4.m4.1.1.3.5.cmml" xref="A2.SS2.p3.4.m4.1.1.3.5">𝑙</ci><ci id="A2.SS2.p3.4.m4.1.1.3.6.cmml" xref="A2.SS2.p3.4.m4.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.4.m4.1c">a_{title}</annotation></semantics></math>, any of the sentences <math id="A2.SS2.p3.5.m5.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="A2.SS2.p3.5.m5.1a"><msub id="A2.SS2.p3.5.m5.1.1" xref="A2.SS2.p3.5.m5.1.1.cmml"><mi id="A2.SS2.p3.5.m5.1.1.2" xref="A2.SS2.p3.5.m5.1.1.2.cmml">a</mi><mrow id="A2.SS2.p3.5.m5.1.1.3" xref="A2.SS2.p3.5.m5.1.1.3.cmml"><mi id="A2.SS2.p3.5.m5.1.1.3.2" xref="A2.SS2.p3.5.m5.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.5.m5.1.1.3.1" xref="A2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.5.m5.1.1.3.3" xref="A2.SS2.p3.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.5.m5.1.1.3.1a" xref="A2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.5.m5.1.1.3.4" xref="A2.SS2.p3.5.m5.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.5.m5.1.1.3.1b" xref="A2.SS2.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.5.m5.1.1.3.5" xref="A2.SS2.p3.5.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.5.m5.1b"><apply id="A2.SS2.p3.5.m5.1.1.cmml" xref="A2.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.5.m5.1.1.1.cmml" xref="A2.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="A2.SS2.p3.5.m5.1.1.2.cmml" xref="A2.SS2.p3.5.m5.1.1.2">𝑎</ci><apply id="A2.SS2.p3.5.m5.1.1.3.cmml" xref="A2.SS2.p3.5.m5.1.1.3"><times id="A2.SS2.p3.5.m5.1.1.3.1.cmml" xref="A2.SS2.p3.5.m5.1.1.3.1"></times><ci id="A2.SS2.p3.5.m5.1.1.3.2.cmml" xref="A2.SS2.p3.5.m5.1.1.3.2">𝑠</ci><ci id="A2.SS2.p3.5.m5.1.1.3.3.cmml" xref="A2.SS2.p3.5.m5.1.1.3.3">𝑒</ci><ci id="A2.SS2.p3.5.m5.1.1.3.4.cmml" xref="A2.SS2.p3.5.m5.1.1.3.4">𝑛</ci><ci id="A2.SS2.p3.5.m5.1.1.3.5.cmml" xref="A2.SS2.p3.5.m5.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.5.m5.1c">a_{sent}</annotation></semantics></math> or the entire article <math id="A2.SS2.p3.6.m6.1" class="ltx_Math" alttext="a_{art}" display="inline"><semantics id="A2.SS2.p3.6.m6.1a"><msub id="A2.SS2.p3.6.m6.1.1" xref="A2.SS2.p3.6.m6.1.1.cmml"><mi id="A2.SS2.p3.6.m6.1.1.2" xref="A2.SS2.p3.6.m6.1.1.2.cmml">a</mi><mrow id="A2.SS2.p3.6.m6.1.1.3" xref="A2.SS2.p3.6.m6.1.1.3.cmml"><mi id="A2.SS2.p3.6.m6.1.1.3.2" xref="A2.SS2.p3.6.m6.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.6.m6.1.1.3.1" xref="A2.SS2.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.6.m6.1.1.3.3" xref="A2.SS2.p3.6.m6.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p3.6.m6.1.1.3.1a" xref="A2.SS2.p3.6.m6.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p3.6.m6.1.1.3.4" xref="A2.SS2.p3.6.m6.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.6.m6.1b"><apply id="A2.SS2.p3.6.m6.1.1.cmml" xref="A2.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.6.m6.1.1.1.cmml" xref="A2.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="A2.SS2.p3.6.m6.1.1.2.cmml" xref="A2.SS2.p3.6.m6.1.1.2">𝑎</ci><apply id="A2.SS2.p3.6.m6.1.1.3.cmml" xref="A2.SS2.p3.6.m6.1.1.3"><times id="A2.SS2.p3.6.m6.1.1.3.1.cmml" xref="A2.SS2.p3.6.m6.1.1.3.1"></times><ci id="A2.SS2.p3.6.m6.1.1.3.2.cmml" xref="A2.SS2.p3.6.m6.1.1.3.2">𝑎</ci><ci id="A2.SS2.p3.6.m6.1.1.3.3.cmml" xref="A2.SS2.p3.6.m6.1.1.3.3">𝑟</ci><ci id="A2.SS2.p3.6.m6.1.1.3.4.cmml" xref="A2.SS2.p3.6.m6.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.6.m6.1c">a_{art}</annotation></semantics></math>. The hidden states of this network are used later in the VQA pipeline to encode the sentences.</p>
</div>
<div id="A2.SS2.p4" class="ltx_para">
<p id="A2.SS2.p4.1" class="ltx_p">After training, the network is evaluated on the articles for each question, the sentences that have the highest prediction score <math id="A2.SS2.p4.1.m1.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="A2.SS2.p4.1.m1.1a"><msub id="A2.SS2.p4.1.m1.1.1" xref="A2.SS2.p4.1.m1.1.1.cmml"><mi id="A2.SS2.p4.1.m1.1.1.2" xref="A2.SS2.p4.1.m1.1.1.2.cmml">a</mi><mrow id="A2.SS2.p4.1.m1.1.1.3" xref="A2.SS2.p4.1.m1.1.1.3.cmml"><mi id="A2.SS2.p4.1.m1.1.1.3.2" xref="A2.SS2.p4.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p4.1.m1.1.1.3.1" xref="A2.SS2.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p4.1.m1.1.1.3.3" xref="A2.SS2.p4.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p4.1.m1.1.1.3.1a" xref="A2.SS2.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p4.1.m1.1.1.3.4" xref="A2.SS2.p4.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.SS2.p4.1.m1.1.1.3.1b" xref="A2.SS2.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS2.p4.1.m1.1.1.3.5" xref="A2.SS2.p4.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p4.1.m1.1b"><apply id="A2.SS2.p4.1.m1.1.1.cmml" xref="A2.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p4.1.m1.1.1.1.cmml" xref="A2.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="A2.SS2.p4.1.m1.1.1.2.cmml" xref="A2.SS2.p4.1.m1.1.1.2">𝑎</ci><apply id="A2.SS2.p4.1.m1.1.1.3.cmml" xref="A2.SS2.p4.1.m1.1.1.3"><times id="A2.SS2.p4.1.m1.1.1.3.1.cmml" xref="A2.SS2.p4.1.m1.1.1.3.1"></times><ci id="A2.SS2.p4.1.m1.1.1.3.2.cmml" xref="A2.SS2.p4.1.m1.1.1.3.2">𝑠</ci><ci id="A2.SS2.p4.1.m1.1.1.3.3.cmml" xref="A2.SS2.p4.1.m1.1.1.3.3">𝑒</ci><ci id="A2.SS2.p4.1.m1.1.1.3.4.cmml" xref="A2.SS2.p4.1.m1.1.1.3.4">𝑛</ci><ci id="A2.SS2.p4.1.m1.1.1.3.5.cmml" xref="A2.SS2.p4.1.m1.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p4.1.m1.1c">a_{sent}</annotation></semantics></math> are used in our VQA training.</p>
</div>
<figure id="A2.F12" class="ltx_figure"><img src="/html/1906.00067/assets/x11.png" id="A2.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="306" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A2.F12.3.2" class="ltx_text" style="font-size:90%;">Retrieval@K curve for words and sentences.</span></figcaption>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>ArticleNet Performance</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.5" class="ltx_p">We rank each sentence during evaluation by the sentence score <math id="A2.SS3.p1.1.m1.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="A2.SS3.p1.1.m1.1a"><msub id="A2.SS3.p1.1.m1.1.1" xref="A2.SS3.p1.1.m1.1.1.cmml"><mi id="A2.SS3.p1.1.m1.1.1.2" xref="A2.SS3.p1.1.m1.1.1.2.cmml">a</mi><mrow id="A2.SS3.p1.1.m1.1.1.3" xref="A2.SS3.p1.1.m1.1.1.3.cmml"><mi id="A2.SS3.p1.1.m1.1.1.3.2" xref="A2.SS3.p1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.1.m1.1.1.3.1" xref="A2.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.1.m1.1.1.3.3" xref="A2.SS3.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.1.m1.1.1.3.1a" xref="A2.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.1.m1.1.1.3.4" xref="A2.SS3.p1.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.1.m1.1.1.3.1b" xref="A2.SS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.1.m1.1.1.3.5" xref="A2.SS3.p1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.1.m1.1b"><apply id="A2.SS3.p1.1.m1.1.1.cmml" xref="A2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.1.m1.1.1.1.cmml" xref="A2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS3.p1.1.m1.1.1.2.cmml" xref="A2.SS3.p1.1.m1.1.1.2">𝑎</ci><apply id="A2.SS3.p1.1.m1.1.1.3.cmml" xref="A2.SS3.p1.1.m1.1.1.3"><times id="A2.SS3.p1.1.m1.1.1.3.1.cmml" xref="A2.SS3.p1.1.m1.1.1.3.1"></times><ci id="A2.SS3.p1.1.m1.1.1.3.2.cmml" xref="A2.SS3.p1.1.m1.1.1.3.2">𝑠</ci><ci id="A2.SS3.p1.1.m1.1.1.3.3.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3">𝑒</ci><ci id="A2.SS3.p1.1.m1.1.1.3.4.cmml" xref="A2.SS3.p1.1.m1.1.1.3.4">𝑛</ci><ci id="A2.SS3.p1.1.m1.1.1.3.5.cmml" xref="A2.SS3.p1.1.m1.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.1.m1.1c">a_{sent}</annotation></semantics></math>, and then plot on average how many sentences should be retrieved to find one including the answer. We compute the same curve for words where the ranking is based on the word score <math id="A2.SS3.p1.2.m2.1" class="ltx_Math" alttext="a_{w_{i}}" display="inline"><semantics id="A2.SS3.p1.2.m2.1a"><msub id="A2.SS3.p1.2.m2.1.1" xref="A2.SS3.p1.2.m2.1.1.cmml"><mi id="A2.SS3.p1.2.m2.1.1.2" xref="A2.SS3.p1.2.m2.1.1.2.cmml">a</mi><msub id="A2.SS3.p1.2.m2.1.1.3" xref="A2.SS3.p1.2.m2.1.1.3.cmml"><mi id="A2.SS3.p1.2.m2.1.1.3.2" xref="A2.SS3.p1.2.m2.1.1.3.2.cmml">w</mi><mi id="A2.SS3.p1.2.m2.1.1.3.3" xref="A2.SS3.p1.2.m2.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.2.m2.1b"><apply id="A2.SS3.p1.2.m2.1.1.cmml" xref="A2.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.2.m2.1.1.1.cmml" xref="A2.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="A2.SS3.p1.2.m2.1.1.2.cmml" xref="A2.SS3.p1.2.m2.1.1.2">𝑎</ci><apply id="A2.SS3.p1.2.m2.1.1.3.cmml" xref="A2.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A2.SS3.p1.2.m2.1.1.3.1.cmml" xref="A2.SS3.p1.2.m2.1.1.3">subscript</csymbol><ci id="A2.SS3.p1.2.m2.1.1.3.2.cmml" xref="A2.SS3.p1.2.m2.1.1.3.2">𝑤</ci><ci id="A2.SS3.p1.2.m2.1.1.3.3.cmml" xref="A2.SS3.p1.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.2.m2.1c">a_{w_{i}}</annotation></semantics></math> multiplied by the sentence score <math id="A2.SS3.p1.3.m3.1" class="ltx_Math" alttext="a_{sent}" display="inline"><semantics id="A2.SS3.p1.3.m3.1a"><msub id="A2.SS3.p1.3.m3.1.1" xref="A2.SS3.p1.3.m3.1.1.cmml"><mi id="A2.SS3.p1.3.m3.1.1.2" xref="A2.SS3.p1.3.m3.1.1.2.cmml">a</mi><mrow id="A2.SS3.p1.3.m3.1.1.3" xref="A2.SS3.p1.3.m3.1.1.3.cmml"><mi id="A2.SS3.p1.3.m3.1.1.3.2" xref="A2.SS3.p1.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.3.m3.1.1.3.1" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.3.m3.1.1.3.3" xref="A2.SS3.p1.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.3.m3.1.1.3.1a" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.3.m3.1.1.3.4" xref="A2.SS3.p1.3.m3.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.SS3.p1.3.m3.1.1.3.1b" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.SS3.p1.3.m3.1.1.3.5" xref="A2.SS3.p1.3.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.3.m3.1b"><apply id="A2.SS3.p1.3.m3.1.1.cmml" xref="A2.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.3.m3.1.1.1.cmml" xref="A2.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="A2.SS3.p1.3.m3.1.1.2.cmml" xref="A2.SS3.p1.3.m3.1.1.2">𝑎</ci><apply id="A2.SS3.p1.3.m3.1.1.3.cmml" xref="A2.SS3.p1.3.m3.1.1.3"><times id="A2.SS3.p1.3.m3.1.1.3.1.cmml" xref="A2.SS3.p1.3.m3.1.1.3.1"></times><ci id="A2.SS3.p1.3.m3.1.1.3.2.cmml" xref="A2.SS3.p1.3.m3.1.1.3.2">𝑠</ci><ci id="A2.SS3.p1.3.m3.1.1.3.3.cmml" xref="A2.SS3.p1.3.m3.1.1.3.3">𝑒</ci><ci id="A2.SS3.p1.3.m3.1.1.3.4.cmml" xref="A2.SS3.p1.3.m3.1.1.3.4">𝑛</ci><ci id="A2.SS3.p1.3.m3.1.1.3.5.cmml" xref="A2.SS3.p1.3.m3.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.3.m3.1c">a_{sent}</annotation></semantics></math>. Product of these scores results in a higher retrieval than <math id="A2.SS3.p1.4.m4.1" class="ltx_Math" alttext="a_{w_{i}}" display="inline"><semantics id="A2.SS3.p1.4.m4.1a"><msub id="A2.SS3.p1.4.m4.1.1" xref="A2.SS3.p1.4.m4.1.1.cmml"><mi id="A2.SS3.p1.4.m4.1.1.2" xref="A2.SS3.p1.4.m4.1.1.2.cmml">a</mi><msub id="A2.SS3.p1.4.m4.1.1.3" xref="A2.SS3.p1.4.m4.1.1.3.cmml"><mi id="A2.SS3.p1.4.m4.1.1.3.2" xref="A2.SS3.p1.4.m4.1.1.3.2.cmml">w</mi><mi id="A2.SS3.p1.4.m4.1.1.3.3" xref="A2.SS3.p1.4.m4.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.4.m4.1b"><apply id="A2.SS3.p1.4.m4.1.1.cmml" xref="A2.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.4.m4.1.1.1.cmml" xref="A2.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="A2.SS3.p1.4.m4.1.1.2.cmml" xref="A2.SS3.p1.4.m4.1.1.2">𝑎</ci><apply id="A2.SS3.p1.4.m4.1.1.3.cmml" xref="A2.SS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A2.SS3.p1.4.m4.1.1.3.1.cmml" xref="A2.SS3.p1.4.m4.1.1.3">subscript</csymbol><ci id="A2.SS3.p1.4.m4.1.1.3.2.cmml" xref="A2.SS3.p1.4.m4.1.1.3.2">𝑤</ci><ci id="A2.SS3.p1.4.m4.1.1.3.3.cmml" xref="A2.SS3.p1.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.4.m4.1c">a_{w_{i}}</annotation></semantics></math> by itself. These results show that ArticleNet is able to retrieve relevant sentences and words from the articles with reasonable accuracy. The plots that show Recall for top <math id="A2.SS3.p1.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A2.SS3.p1.5.m5.1a"><mi id="A2.SS3.p1.5.m5.1.1" xref="A2.SS3.p1.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.5.m5.1b"><ci id="A2.SS3.p1.5.m5.1.1.cmml" xref="A2.SS3.p1.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.5.m5.1c">K</annotation></semantics></math> sentences or words are shown in Figure <a href="#A2.F12" title="Figure 12 ‣ B.2 ArticleNet Overview ‣ Appendix B ArticleNet Details ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>MUTAN+AN and BAN+AN Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We provide more details for the MUTAN+AN and BAN+AN models in this section. The MUTAN model is the Multimodal Tucker Fusion (MUTAN) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Specifically, we use the attention version of MUTAN, and choose the parameters to match the single best performing model of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">The BAN model is the single model version of Bilinear Attention Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We use the single model version, and we use faster-rcnn features trained on COCO train (to avoid overlap with our test set). For both BAN and MUTAN, we use the top 2000 answers in train as our answer vocabulary.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">We incorporate hidden states of ArticleNet for the top retrieved sentences into MUTAN and BAN. During VQA training and testing, we take the hidden states for the top <math id="A3.p3.1.m1.1" class="ltx_Math" alttext="N_{art}" display="inline"><semantics id="A3.p3.1.m1.1a"><msub id="A3.p3.1.m1.1.1" xref="A3.p3.1.m1.1.1.cmml"><mi id="A3.p3.1.m1.1.1.2" xref="A3.p3.1.m1.1.1.2.cmml">N</mi><mrow id="A3.p3.1.m1.1.1.3" xref="A3.p3.1.m1.1.1.3.cmml"><mi id="A3.p3.1.m1.1.1.3.2" xref="A3.p3.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A3.p3.1.m1.1.1.3.1" xref="A3.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="A3.p3.1.m1.1.1.3.3" xref="A3.p3.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A3.p3.1.m1.1.1.3.1a" xref="A3.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="A3.p3.1.m1.1.1.3.4" xref="A3.p3.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p3.1.m1.1b"><apply id="A3.p3.1.m1.1.1.cmml" xref="A3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p3.1.m1.1.1.1.cmml" xref="A3.p3.1.m1.1.1">subscript</csymbol><ci id="A3.p3.1.m1.1.1.2.cmml" xref="A3.p3.1.m1.1.1.2">𝑁</ci><apply id="A3.p3.1.m1.1.1.3.cmml" xref="A3.p3.1.m1.1.1.3"><times id="A3.p3.1.m1.1.1.3.1.cmml" xref="A3.p3.1.m1.1.1.3.1"></times><ci id="A3.p3.1.m1.1.1.3.2.cmml" xref="A3.p3.1.m1.1.1.3.2">𝑎</ci><ci id="A3.p3.1.m1.1.1.3.3.cmml" xref="A3.p3.1.m1.1.1.3.3">𝑟</ci><ci id="A3.p3.1.m1.1.1.3.4.cmml" xref="A3.p3.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.1.m1.1c">N_{art}</annotation></semantics></math> predicted sentences (ignoring duplicate sentences), and feed them in the memory in an end-to-end memory network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.2" class="ltx_p">We use the visual features <math id="A3.p4.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="A3.p4.1.m1.1a"><mi id="A3.p4.1.m1.1.1" xref="A3.p4.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="A3.p4.1.m1.1b"><ci id="A3.p4.1.m1.1.1.cmml" xref="A3.p4.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.1.m1.1c">V</annotation></semantics></math> and encoded question <math id="A3.p4.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A3.p4.2.m2.1a"><mi id="A3.p4.2.m2.1.1" xref="A3.p4.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A3.p4.2.m2.1b"><ci id="A3.p4.2.m2.1.1.cmml" xref="A3.p4.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.2.m2.1c">Q</annotation></semantics></math> passed through a hidden layer as the key to the memory network. To incorporate the memory network into the VQA system, we concatenate the output of the memory network to the hidden layer of the MUTAN after the attention MUTAN fusion and before the final MUTAN fusion. For the BAN model, we feed the output of the question embedding as the key to the memory network, and concatenate the output of the memory network to BAN right before the final classification layers.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Training and Model Details</h2>

<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>ArticleNet</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.3" class="ltx_p">The question is encoded using a pre-trained skip-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> encoder. All fully connected layers (except at output layers) have batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and ReLU activations. All output layers have Sigmoid before the final output. We train ArticleNet for <math id="A4.SS1.p1.1.m1.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="A4.SS1.p1.1.m1.2a"><mrow id="A4.SS1.p1.1.m1.2.3.2" xref="A4.SS1.p1.1.m1.2.3.1.cmml"><mn id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml">10</mn><mo id="A4.SS1.p1.1.m1.2.3.2.1" xref="A4.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="A4.SS1.p1.1.m1.2.2" xref="A4.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.2b"><list id="A4.SS1.p1.1.m1.2.3.1.cmml" xref="A4.SS1.p1.1.m1.2.3.2"><cn type="integer" id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1">10</cn><cn type="integer" id="A4.SS1.p1.1.m1.2.2.cmml" xref="A4.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.2c">10,000</annotation></semantics></math> iterations with a batch size of <math id="A4.SS1.p1.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="A4.SS1.p1.2.m2.1a"><mn id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><cn type="integer" id="A4.SS1.p1.2.m2.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">64</annotation></semantics></math> using ADAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with a learning rate of <math id="A4.SS1.p1.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="A4.SS1.p1.3.m3.1a"><msup id="A4.SS1.p1.3.m3.1.1" xref="A4.SS1.p1.3.m3.1.1.cmml"><mn id="A4.SS1.p1.3.m3.1.1.2" xref="A4.SS1.p1.3.m3.1.1.2.cmml">10</mn><mrow id="A4.SS1.p1.3.m3.1.1.3" xref="A4.SS1.p1.3.m3.1.1.3.cmml"><mo id="A4.SS1.p1.3.m3.1.1.3a" xref="A4.SS1.p1.3.m3.1.1.3.cmml">−</mo><mn id="A4.SS1.p1.3.m3.1.1.3.2" xref="A4.SS1.p1.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.3.m3.1b"><apply id="A4.SS1.p1.3.m3.1.1.cmml" xref="A4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A4.SS1.p1.3.m3.1.1.1.cmml" xref="A4.SS1.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="A4.SS1.p1.3.m3.1.1.2.cmml" xref="A4.SS1.p1.3.m3.1.1.2">10</cn><apply id="A4.SS1.p1.3.m3.1.1.3.cmml" xref="A4.SS1.p1.3.m3.1.1.3"><minus id="A4.SS1.p1.3.m3.1.1.3.1.cmml" xref="A4.SS1.p1.3.m3.1.1.3"></minus><cn type="integer" id="A4.SS1.p1.3.m3.1.1.3.2.cmml" xref="A4.SS1.p1.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.3.m3.1c">10^{-4}</annotation></semantics></math>, and using a balanced training set of “positive” and “negative” articles, meaning that with equal probability, an input article will contain the answer somewhere.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>VQA Models</h3>

<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.3" class="ltx_p">The MUTAN models as well as the MLP and Q-Only models were trained for <math id="A4.SS2.p1.1.m1.1" class="ltx_Math" alttext="500" display="inline"><semantics id="A4.SS2.p1.1.m1.1a"><mn id="A4.SS2.p1.1.m1.1.1" xref="A4.SS2.p1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.1.m1.1b"><cn type="integer" id="A4.SS2.p1.1.m1.1.1.cmml" xref="A4.SS2.p1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.1.m1.1c">500</annotation></semantics></math> epochs. All use batch size of <math id="A4.SS2.p1.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="A4.SS2.p1.2.m2.1a"><mn id="A4.SS2.p1.2.m2.1.1" xref="A4.SS2.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.2.m2.1b"><cn type="integer" id="A4.SS2.p1.2.m2.1.1.cmml" xref="A4.SS2.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.2.m2.1c">128</annotation></semantics></math> using ADAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with learning rate <math id="A4.SS2.p1.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="A4.SS2.p1.3.m3.1a"><msup id="A4.SS2.p1.3.m3.1.1" xref="A4.SS2.p1.3.m3.1.1.cmml"><mn id="A4.SS2.p1.3.m3.1.1.2" xref="A4.SS2.p1.3.m3.1.1.2.cmml">10</mn><mrow id="A4.SS2.p1.3.m3.1.1.3" xref="A4.SS2.p1.3.m3.1.1.3.cmml"><mo id="A4.SS2.p1.3.m3.1.1.3a" xref="A4.SS2.p1.3.m3.1.1.3.cmml">−</mo><mn id="A4.SS2.p1.3.m3.1.1.3.2" xref="A4.SS2.p1.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.3.m3.1b"><apply id="A4.SS2.p1.3.m3.1.1.cmml" xref="A4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A4.SS2.p1.3.m3.1.1.1.cmml" xref="A4.SS2.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="A4.SS2.p1.3.m3.1.1.2.cmml" xref="A4.SS2.p1.3.m3.1.1.2">10</cn><apply id="A4.SS2.p1.3.m3.1.1.3.cmml" xref="A4.SS2.p1.3.m3.1.1.3"><minus id="A4.SS2.p1.3.m3.1.1.3.1.cmml" xref="A4.SS2.p1.3.m3.1.1.3"></minus><cn type="integer" id="A4.SS2.p1.3.m3.1.1.3.2.cmml" xref="A4.SS2.p1.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.3.m3.1c">10^{-4}</annotation></semantics></math>.</p>
</div>
<div id="A4.SS2.p2" class="ltx_para">
<p id="A4.SS2.p2.3" class="ltx_p">The BAN models were trained for <math id="A4.SS2.p2.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.SS2.p2.1.m1.1a"><mn id="A4.SS2.p2.1.m1.1.1" xref="A4.SS2.p2.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p2.1.m1.1b"><cn type="integer" id="A4.SS2.p2.1.m1.1.1.cmml" xref="A4.SS2.p2.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p2.1.m1.1c">200</annotation></semantics></math> epochs. We found that setting <math id="A4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="A4.SS2.p2.2.m2.1a"><mi id="A4.SS2.p2.2.m2.1.1" xref="A4.SS2.p2.2.m2.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="A4.SS2.p2.2.m2.1b"><ci id="A4.SS2.p2.2.m2.1.1.cmml" xref="A4.SS2.p2.2.m2.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p2.2.m2.1c">\gamma</annotation></semantics></math> (number of glimpses) to 2 and the hidden feature size to <math id="A4.SS2.p2.3.m3.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A4.SS2.p2.3.m3.1a"><mn id="A4.SS2.p2.3.m3.1.1" xref="A4.SS2.p2.3.m3.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p2.3.m3.1b"><cn type="integer" id="A4.SS2.p2.3.m3.1.1.cmml" xref="A4.SS2.p2.3.m3.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p2.3.m3.1c">512</annotation></semantics></math> yielded much better performance on our dataset than the default parameter options used for VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="A4.SS2.p3" class="ltx_para">
<p id="A4.SS2.p3.4" class="ltx_p">We choose <math id="A4.SS2.p3.1.m1.1" class="ltx_Math" alttext="N_{art}" display="inline"><semantics id="A4.SS2.p3.1.m1.1a"><msub id="A4.SS2.p3.1.m1.1.1" xref="A4.SS2.p3.1.m1.1.1.cmml"><mi id="A4.SS2.p3.1.m1.1.1.2" xref="A4.SS2.p3.1.m1.1.1.2.cmml">N</mi><mrow id="A4.SS2.p3.1.m1.1.1.3" xref="A4.SS2.p3.1.m1.1.1.3.cmml"><mi id="A4.SS2.p3.1.m1.1.1.3.2" xref="A4.SS2.p3.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A4.SS2.p3.1.m1.1.1.3.1" xref="A4.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="A4.SS2.p3.1.m1.1.1.3.3" xref="A4.SS2.p3.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="A4.SS2.p3.1.m1.1.1.3.1a" xref="A4.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="A4.SS2.p3.1.m1.1.1.3.4" xref="A4.SS2.p3.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.1.m1.1b"><apply id="A4.SS2.p3.1.m1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS2.p3.1.m1.1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="A4.SS2.p3.1.m1.1.1.2.cmml" xref="A4.SS2.p3.1.m1.1.1.2">𝑁</ci><apply id="A4.SS2.p3.1.m1.1.1.3.cmml" xref="A4.SS2.p3.1.m1.1.1.3"><times id="A4.SS2.p3.1.m1.1.1.3.1.cmml" xref="A4.SS2.p3.1.m1.1.1.3.1"></times><ci id="A4.SS2.p3.1.m1.1.1.3.2.cmml" xref="A4.SS2.p3.1.m1.1.1.3.2">𝑎</ci><ci id="A4.SS2.p3.1.m1.1.1.3.3.cmml" xref="A4.SS2.p3.1.m1.1.1.3.3">𝑟</ci><ci id="A4.SS2.p3.1.m1.1.1.3.4.cmml" xref="A4.SS2.p3.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.1.m1.1c">N_{art}</annotation></semantics></math> to be <math id="A4.SS2.p3.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.SS2.p3.2.m2.1a"><mn id="A4.SS2.p3.2.m2.1.1" xref="A4.SS2.p3.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.2.m2.1b"><cn type="integer" id="A4.SS2.p3.2.m2.1.1.cmml" xref="A4.SS2.p3.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.2.m2.1c">20</annotation></semantics></math>, number of hops in the memory network as <math id="A4.SS2.p3.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.SS2.p3.3.m3.1a"><mn id="A4.SS2.p3.3.m3.1.1" xref="A4.SS2.p3.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.3.m3.1b"><cn type="integer" id="A4.SS2.p3.3.m3.1.1.cmml" xref="A4.SS2.p3.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.3.m3.1c">2</annotation></semantics></math>, and the hidden size of the memory network as <math id="A4.SS2.p3.4.m4.1" class="ltx_Math" alttext="300" display="inline"><semantics id="A4.SS2.p3.4.m4.1a"><mn id="A4.SS2.p3.4.m4.1.1" xref="A4.SS2.p3.4.m4.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.4.m4.1b"><cn type="integer" id="A4.SS2.p3.4.m4.1.1.cmml" xref="A4.SS2.p3.4.m4.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.4.m4.1c">300</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional Dataset Examples</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In Figures <a href="#A5.F13" title="Figure 13 ‣ Appendix E Additional Dataset Examples ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <a href="#A5.F14" title="Figure 14 ‣ Appendix E Additional Dataset Examples ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, <a href="#A5.F15" title="Figure 15 ‣ Appendix E Additional Dataset Examples ‣ OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> we provide additional examples of <span id="A5.p1.1.1" class="ltx_text">Outside Knowledge VQA (OK-VQA)</span>.</p>
</div>
<figure id="A5.F13" class="ltx_figure"><img src="/html/1906.00067/assets/figs/Supp1.jpg" id="A5.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="812" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F13.5.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A5.F13.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset examples.<span id="A5.F13.6.2.1" class="ltx_text ltx_font_medium"> Some more sample questions of <span id="A5.F13.6.2.1.1" class="ltx_text">OK-VQA</span>.</span></span></figcaption>
</figure>
<figure id="A5.F14" class="ltx_figure"><img src="/html/1906.00067/assets/figs/Supp2.jpg" id="A5.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="812" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F14.5.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A5.F14.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset examples.<span id="A5.F14.6.2.1" class="ltx_text ltx_font_medium"> Some more sample questions of <span id="A5.F14.6.2.1.1" class="ltx_text">OK-VQA</span>.</span></span></figcaption>
</figure>
<figure id="A5.F15" class="ltx_figure"><img src="/html/1906.00067/assets/figs/Supp3.jpg" id="A5.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="812" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F15.5.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A5.F15.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset examples.<span id="A5.F15.6.2.1" class="ltx_text ltx_font_medium"> Some more sample questions of <span id="A5.F15.6.2.1.1" class="ltx_text">OK-VQA</span>.</span></span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1906.00066" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1906.00067" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1906.00067">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1906.00067" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1906.00068" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 20:07:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
