<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Depth Any Video with Scalable Synthetic Data</title>
<!--Generated on Sun Oct 13 16:47:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.10815v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S1" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S2" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Synthetic Data Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Generative Video Depth Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS1" title="In 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS2" title="In 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Mixed-duration Training Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS3" title="In 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Long Video Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS1" title="In 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets and Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS2" title="In 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS3" title="In 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Zero-shot Depth Estimation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS4" title="In 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S5" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S6" title="In Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Depth Any Video with Scalable Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Honghui Yang<sup class="ltx_sup" id="id7.5.id1">∗</sup>, Di Huang<sup class="ltx_sup" id="id8.6.id2">∗</sup>, Wei Yin, Chunhua Shen, Haifeng Liu
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id4.4.2">Xiaofei He, Binbin Lin<sup class="ltx_sup" id="id4.4.2.1"><span class="ltx_text ltx_font_medium" id="id4.4.2.1.1">†</span></sup>, Wanli Ouyang, Tong He<sup class="ltx_sup" id="id4.4.2.2"><span class="ltx_text ltx_font_medium" id="id4.4.2.2.1">†</span></sup>
<br class="ltx_break"/></span>
Shanghai AI Laboratory  Zhejiang University  The University of Sydney
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce <span class="ltx_text ltx_font_bold" id="id9.id1.1">Depth Any Video</span>, a model that tackles the challenge through two key innovations. First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse synthetic environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations.
Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency.
Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates—even on single frames. At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames.
Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency.
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup class="ltx_sup" id="footnote1.1">∗</sup>Equal contribution.</span></span></span>
<span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup class="ltx_sup" id="footnote1a.1"><span class="ltx_text ltx_font_italic" id="footnote1a.1.1">†</span></sup>Corresponding author.</span></span></span></p>
</div>
<div class="ltx_para" id="id6">
<p class="ltx_p ltx_align_center" id="id6.2"><a class="ltx_ref ltx_href" href="https://depthanyvideo.github.io" title=""><math alttext="\tt https" class="ltx_Math" display="inline" id="id5.1.1.m1.1"><semantics id="id5.1.1.m1.1a"><mi id="id5.1.1.m1.1.1" xref="id5.1.1.m1.1.1.cmml">𝚑𝚝𝚝𝚙𝚜</mi><annotation-xml encoding="MathML-Content" id="id5.1.1.m1.1b"><ci id="id5.1.1.m1.1.1.cmml" xref="id5.1.1.m1.1.1">𝚑𝚝𝚝𝚙𝚜</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.1.1.m1.1c">\tt https</annotation><annotation encoding="application/x-llamapun" id="id5.1.1.m1.1d">typewriter_https</annotation></semantics></math>://<math alttext="\tt depthanyvideo.github.io" class="ltx_Math" display="inline" id="id6.2.2.m2.3"><semantics id="id6.2.2.m2.3a"><mrow id="id6.2.2.m2.3.4.2" xref="id6.2.2.m2.3.4.1.cmml"><mi id="id6.2.2.m2.1.1" xref="id6.2.2.m2.1.1.cmml">𝚍𝚎𝚙𝚝𝚑𝚊𝚗𝚢𝚟𝚒𝚍𝚎𝚘</mi><mo id="id6.2.2.m2.3.4.2.1" lspace="0em" rspace="0.167em" xref="id6.2.2.m2.3.4.1a.cmml">.</mo><mi id="id6.2.2.m2.2.2" xref="id6.2.2.m2.2.2.cmml">𝚐𝚒𝚝𝚑𝚞𝚋</mi><mo id="id6.2.2.m2.3.4.2.2" lspace="0em" rspace="0.167em" xref="id6.2.2.m2.3.4.1a.cmml">.</mo><mi id="id6.2.2.m2.3.3" xref="id6.2.2.m2.3.3.cmml">𝚒𝚘</mi></mrow><annotation-xml encoding="MathML-Content" id="id6.2.2.m2.3b"><apply id="id6.2.2.m2.3.4.1.cmml" xref="id6.2.2.m2.3.4.2"><csymbol cd="ambiguous" id="id6.2.2.m2.3.4.1a.cmml" xref="id6.2.2.m2.3.4.2.1">formulae-sequence</csymbol><ci id="id6.2.2.m2.1.1.cmml" xref="id6.2.2.m2.1.1">𝚍𝚎𝚙𝚝𝚑𝚊𝚗𝚢𝚟𝚒𝚍𝚎𝚘</ci><ci id="id6.2.2.m2.2.2.cmml" xref="id6.2.2.m2.2.2">𝚐𝚒𝚝𝚑𝚞𝚋</ci><ci id="id6.2.2.m2.3.3.cmml" xref="id6.2.2.m2.3.3">𝚒𝚘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.2.2.m2.3c">\tt depthanyvideo.github.io</annotation><annotation encoding="application/x-llamapun" id="id6.2.2.m2.3d">typewriter_depthanyvideo . typewriter_github . typewriter_io</annotation></semantics></math></a></p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="763" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We present <span class="ltx_text ltx_font_bold" id="S0.F1.2.1">Depth Any Video</span>, a versatile foundation model supporting both image (top half) and video (bottom half) depth estimation. Derived from Stable Video Diffusion and fine-tuned with diverse and high-quality synthetic data, our model achieves remarkably robust generalization across various real and synthetic unseen scenarios. Additionally, it faithfully captures intricate fine-grained details while ensuring temporal consistency throughout the video.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Depth estimation is a foundational problem in understanding the 3D structure of the real world.
The ability to accurately perceive and represent depth in video sequences is crucial for a broad range of applications, including autonomous navigation <cite class="ltx_cite ltx_citemacro_citep">(Borghi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib5" title="">2017</a>)</cite>, augmented reality <cite class="ltx_cite ltx_citemacro_citep">(Holynski &amp; Kopf, <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib20" title="">2018</a>)</cite>, and advanced video editing <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib60" title="">2024</a>; Peng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib35" title="">2024</a>)</cite>.
Although recent advancements in single-image depth estimation <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib51" title="">2024a</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib13" title="">2024</a>; Ranftl et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib37" title="">2021</a>)</cite> have led to significant improvements in spatial accuracy, ensuring temporal consistency across video frames remains a substantial challenge.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A major bottleneck in existing video depth estimation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>; Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib42" title="">2024</a>)</cite> is the lack of diverse and large-scale video depth data that capture the complexity of real-world environments.
Existing datasets <cite class="ltx_cite ltx_citemacro_citep">(Geiger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib14" title="">2012</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib29" title="">2023</a>; Karaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib25" title="">2023</a>)</cite> are often limited in terms of scale, diversity, and scene variation, making it difficult for models to generalize effectively across different scenarios.
From a hardware perspective, depth sensors like LiDAR, structured light systems, and time-of-flight cameras can provide accurate depth measurements but are often costly, limited in range or resolution, and struggle under specific lighting conditions or when dealing with reflective surfaces.
Another common approach <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib22" title="">2024b</a>)</cite> is to rely on unlabeled stereo video datasets <cite class="ltx_cite ltx_citemacro_citep">(Alahari et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib1" title="">2013</a>)</cite> and state-of-the-art stereo-matching methods <cite class="ltx_cite ltx_citemacro_citep">(Jing et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib24" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib50" title="">2023</a>)</cite>; however, such methods are complex, computationally intensive, and often fail in areas with weak textures. These limitations hinder the development of robust models that can ensure both spatial precision and temporal consistency in dynamic scenes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To tackle the challenge, in this paper, we propose a solution from two complementary perspectives: (1) constructing a large-scale synthetic video depth dataset and (2) designing a novel framework that leverages powerful visual priors of generative models to effectively handle various real-world videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Synthetic Data: </span>
Modern video games offer highly realistic graphics and simulate diverse real-world scenarios. For example, car racing games realistically replicate driving environments, while open-world games simulate various complex scenes.
Given that modern rendering pipelines often include depth buffers, it becomes possible to extract large-scale, highly accurate video depth data from synthetic environments, which is scalable and cost-effective.
In light of this, we construct DA-V, a synthetic dataset comprising 40,000 video clips.
DA-V captures a wide range of scenarios, covering various lighting conditions, dynamic camera movements, and intricate object interactions in both indoor and outdoor environments, providing the opportunity for models to generalize effectively to real-world environments.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Framework: </span>
To complement the dataset, we propose a novel framework for video depth estimation that leverages the rich prior knowledge embedded in video generation models.
Drawing from recent advancements <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite>, we build upon SVD <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite> and introduce two key innovations to enhance generalization and efficiency.
First, a mixed-duration training strategy is introduced to simulate videos with varying frame rates and lengths by randomly dropping frames.
To handle videos of different lengths, those with the same duration are grouped into the same batch, and batch sizes are adjusted accordingly, thus optimizing memory usage and improving training efficiency.
Second, a depth interpolation module is proposed, generating intermediate frames conditioned on globally consistent depth estimates from key frames, allowing for high-resolution and coherent inference of long videos under limited computational constraints.
Additionally, we refine the pipeline by introducing a flow-matching approach <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib30" title="">2023</a>)</cite> and rotary position encoding <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib46" title="">2021</a>)</cite> to further improve inference efficiency and flexibility.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions can be summarized as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a large-scale synthetic dataset of 40,000 video depth clips collected from diverse rendering engines. This is the first dataset to leverage a variety of high-fidelity synthetic environments and to verify its generalization capability in real-world scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a new training and inference framework that integrates a mixed-duration training strategy and a long-video inference module, enabling the model to handle varying video lengths while ensuring spatial accuracy and temporal consistency.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our method achieves state-of-the-art performance among generative depth models, setting a new benchmark for accuracy and robustness in video depth estimation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic Data Workflow</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Real-time Data Collection.</span>
To address the challenges of depth data, we collect a large-scale synthetic dataset comprising approximately 40,000 video clips.
A significant portion of this dataset is derived from state-of-the-art synthetic engines, leveraging their ability to generate photorealistic environments with accurate depth information.
We extract depth data from a diverse set of virtual environments, carefully selected to encompass a wide range of scene conditions.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S2.T1" title="Table 1 ‣ 2 Synthetic Data Workflow ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare our dataset with previous public synthetic datasets. To the best of our knowledge, ours is the largest synthetic video dataset covering a wide range of realistic scenes.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S2.T1.2.1">Comparisons of synthetic datasets</span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.3" style="width:216.8pt;height:59.2pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-157.2pt,42.6pt) scale(0.408072042642556,0.408072042642556) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.3.1">
<tr class="ltx_tr" id="S2.T1.3.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S2.T1.3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">Outdoor</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Indoor</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">Dynamic</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">Video</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.1.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"># Frame</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.3.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Hypersim <cite class="ltx_cite ltx_citemacro_citep">(Roberts et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib38" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.1.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">68K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.3.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">MVS-Synth <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib23" title="">2018</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">12K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.3.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">VKITTI <cite class="ltx_cite ltx_citemacro_citep">(Cabon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib7" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">25K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.3.1.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">MatrixCity <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib29" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">519K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.3.1.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">Sintel <cite class="ltx_cite ltx_citemacro_citep">(Butler et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib6" title="">2012</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">1.6K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.3.1.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">DynamicReplica <cite class="ltx_cite ltx_citemacro_citep">(Karaev et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib25" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.1.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">169K</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S2.T1.3.1.8.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.8.1.1">DA-V (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.1.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.1.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.1.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.1.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.1.8.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.8.6.1">6M</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Data Filtering.</span>
After collecting initial synthetic video data, occasional misalignments between the image and depth are observed.
To filter these frames, we first employ a scene cut method<span class="ltx_note ltx_role_footnote" id="footnote1b"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/Breakthrough/PySceneDetect</span></span></span> to detect scene transitions based on significant color changes.
Then, the depth model (detailed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS1" title="3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3.1</span></a>), trained on a hand-picked subset of the data, is used to filter out splited video sequences with low depth metric scores.
However, this straightforward approach can lead to excessive filtering of unseen data.
Therefore, we further use a CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib36" title="">2021</a>)</cite> model to compute semantic similarity between the actual and predicted depth, both colorized from a single channel to three.
Finally, we uniformly sample 10 frames from each video segment.
If both the median semantic and depth metric scores fall below predefined thresholds, the segment is removed.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Generative Video Depth Model</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce Depth Any Video, a generative model designed for robust and consistent video depth estimation.
The model builds upon prior video foundation models, framing video depth estimation as a conditional denoising process (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS1" title="3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3.1</span></a>). A mixed-duration training strategy is then presented to improve model generalization and training efficiency (Sec.<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS2" title="3.2 Mixed-duration Training Strategy ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Finally, we extend the model to estimate high-resolution depth in long videos (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS3" title="3.3 Long Video Inference ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Design</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">Our approach builds upon the video foundation model, Stable Video Diffusion (SVD) <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite>, and reformulates monocular video depth estimation as a generative denoising process.
The overall framework is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F2" title="Figure 2 ‣ 3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>.
The training flow consists of a forward process that gradually corrupts the ground truth video depth <math alttext="x_{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">x_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> by adding Gaussian noise <math alttext="\epsilon\sim\mathcal{N}(0,I)" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.2"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3" xref="S3.SS1.p1.2.m2.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.2.cmml">ϵ</mi><mo id="S3.SS1.p1.2.m2.2.3.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">∼</mo><mrow id="S3.SS1.p1.2.m2.2.3.3" xref="S3.SS1.p1.2.m2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.2.3.3.2" xref="S3.SS1.p1.2.m2.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS1.p1.2.m2.2.3.3.1" xref="S3.SS1.p1.2.m2.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS1.p1.2.m2.2.3.3.3.2" xref="S3.SS1.p1.2.m2.2.3.3.3.1.cmml"><mo id="S3.SS1.p1.2.m2.2.3.3.3.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS1.p1.2.m2.2.3.3.3.2.2" xref="S3.SS1.p1.2.m2.2.3.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">I</mi><mo id="S3.SS1.p1.2.m2.2.3.3.3.2.3" stretchy="false" xref="S3.SS1.p1.2.m2.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.3"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.1">similar-to</csymbol><ci id="S3.SS1.p1.2.m2.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.3.2">italic-ϵ</ci><apply id="S3.SS1.p1.2.m2.2.3.3.cmml" xref="S3.SS1.p1.2.m2.2.3.3"><times id="S3.SS1.p1.2.m2.2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3.1"></times><ci id="S3.SS1.p1.2.m2.2.3.3.2.cmml" xref="S3.SS1.p1.2.m2.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p1.2.m2.2.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3.3.2"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">0</cn><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">𝐼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">\epsilon\sim\mathcal{N}(0,I)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.2d">italic_ϵ ∼ caligraphic_N ( 0 , italic_I )</annotation></semantics></math> and a reverse process that uses a denoising model <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑣</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, conditioned on the input video <math alttext="x_{c}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑥</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">x_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, to remove the noise.
Once <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">v</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑣</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is trained, the inference flow begins with pure noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_ϵ</annotation></semantics></math> and progressively denoises it, moving towards a cleaner result with each step.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.2.1">Latent Video Condition.</span>
Following prior latent diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib39" title="">2022</a>; Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib12" title="">2024</a>)</cite>, the generation process operates within the latent space of a pre-trained variational autoencoder (VAE), allowing the model to handle high-resolution input without sacrificing computational efficiency.
Specifically, given a video depth <math alttext="x_{d}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">x_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, we first apply a normalization as in <cite class="ltx_cite ltx_citemacro_cite">Ke et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite> to ensure that depth values fall primarily within the VAE’s input range of <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.2"><semantics id="S3.SS1.p2.2.m2.2a"><mrow id="S3.SS1.p2.2.m2.2.2.1" xref="S3.SS1.p2.2.m2.2.2.2.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.2.cmml">[</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1a" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml">−</mo><mn id="S3.SS1.p2.2.m2.2.2.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p2.2.m2.2.2.1.3" xref="S3.SS1.p2.2.m2.2.2.2.cmml">,</mo><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.p2.2.m2.2.2.1.4" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.2b"><interval closure="closed" id="S3.SS1.p2.2.m2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1"><apply id="S3.SS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1"><minus id="S3.SS1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1"></minus><cn id="S3.SS1.p2.2.m2.2.2.1.1.2.cmml" type="integer" xref="S3.SS1.p2.2.m2.2.2.1.1.2">1</cn></apply><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.2d">[ - 1 , 1 ]</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{x}_{d}=\left(\frac{x_{d}-d_{2}}{d_{98}-d_{2}}-0.5\right)\times 2," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">x</mi><mo id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">d</mi></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">d</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">d</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">2</mn></msub></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">d</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">98</mn></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">d</mi><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">2</mn></msub></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">0.5</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.3" rspace="0.055em" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.1.2" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.1.2.cmml">×</mo><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">2</mn></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1">~</ci><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝑥</ci></apply><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑑</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"></divide><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.3">𝑑</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.2">𝑑</ci><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.3.3">2</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2">𝑑</ci><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3">98</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.2">𝑑</ci><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.3">2</cn></apply></apply></apply><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" type="float" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">0.5</cn></apply><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\tilde{x}_{d}=\left(\frac{x_{d}-d_{2}}{d_{98}-d_{2}}-0.5\right)\times 2,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = ( divide start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT - italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_d start_POSTSUBSCRIPT 98 end_POSTSUBSCRIPT - italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG - 0.5 ) × 2 ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.9">where <math alttext="d_{2}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m1.1"><semantics id="S3.SS1.p2.3.m1.1a"><msub id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml"><mi id="S3.SS1.p2.3.m1.1.1.2" xref="S3.SS1.p2.3.m1.1.1.2.cmml">d</mi><mn id="S3.SS1.p2.3.m1.1.1.3" xref="S3.SS1.p2.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.1b"><apply id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m1.1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m1.1.1.2.cmml" xref="S3.SS1.p2.3.m1.1.1.2">𝑑</ci><cn id="S3.SS1.p2.3.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.1c">d_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m1.1d">italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="d_{98}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m2.1"><semantics id="S3.SS1.p2.4.m2.1a"><msub id="S3.SS1.p2.4.m2.1.1" xref="S3.SS1.p2.4.m2.1.1.cmml"><mi id="S3.SS1.p2.4.m2.1.1.2" xref="S3.SS1.p2.4.m2.1.1.2.cmml">d</mi><mn id="S3.SS1.p2.4.m2.1.1.3" xref="S3.SS1.p2.4.m2.1.1.3.cmml">98</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m2.1b"><apply id="S3.SS1.p2.4.m2.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m2.1.1.1.cmml" xref="S3.SS1.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m2.1.1.2.cmml" xref="S3.SS1.p2.4.m2.1.1.2">𝑑</ci><cn id="S3.SS1.p2.4.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.4.m2.1.1.3">98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m2.1c">d_{98}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m2.1d">italic_d start_POSTSUBSCRIPT 98 end_POSTSUBSCRIPT</annotation></semantics></math> represent the 2% and 98% percentiles of <math alttext="x_{d}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m3.1"><semantics id="S3.SS1.p2.5.m3.1a"><msub id="S3.SS1.p2.5.m3.1.1" xref="S3.SS1.p2.5.m3.1.1.cmml"><mi id="S3.SS1.p2.5.m3.1.1.2" xref="S3.SS1.p2.5.m3.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.5.m3.1.1.3" xref="S3.SS1.p2.5.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m3.1b"><apply id="S3.SS1.p2.5.m3.1.1.cmml" xref="S3.SS1.p2.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m3.1.1.1.cmml" xref="S3.SS1.p2.5.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m3.1.1.2.cmml" xref="S3.SS1.p2.5.m3.1.1.2">𝑥</ci><ci id="S3.SS1.p2.5.m3.1.1.3.cmml" xref="S3.SS1.p2.5.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m3.1c">x_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m3.1d">italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
Then, the corresponding latent code is obtained using the encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m4.1"><semantics id="S3.SS1.p2.6.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m4.1.1" xref="S3.SS1.p2.6.m4.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m4.1b"><ci id="S3.SS1.p2.6.m4.1.1.cmml" xref="S3.SS1.p2.6.m4.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m4.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m4.1d">caligraphic_E</annotation></semantics></math>: <math alttext="z_{d}=\mathcal{E}(\tilde{x}_{d})" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m5.1"><semantics id="S3.SS1.p2.7.m5.1a"><mrow id="S3.SS1.p2.7.m5.1.1" xref="S3.SS1.p2.7.m5.1.1.cmml"><msub id="S3.SS1.p2.7.m5.1.1.3" xref="S3.SS1.p2.7.m5.1.1.3.cmml"><mi id="S3.SS1.p2.7.m5.1.1.3.2" xref="S3.SS1.p2.7.m5.1.1.3.2.cmml">z</mi><mi id="S3.SS1.p2.7.m5.1.1.3.3" xref="S3.SS1.p2.7.m5.1.1.3.3.cmml">d</mi></msub><mo id="S3.SS1.p2.7.m5.1.1.2" xref="S3.SS1.p2.7.m5.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.7.m5.1.1.1" xref="S3.SS1.p2.7.m5.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m5.1.1.1.3" xref="S3.SS1.p2.7.m5.1.1.1.3.cmml">ℰ</mi><mo id="S3.SS1.p2.7.m5.1.1.1.2" xref="S3.SS1.p2.7.m5.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.7.m5.1.1.1.1.1" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.7.m5.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.7.m5.1.1.1.1.1.1" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.1" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS1.p2.7.m5.1.1.1.1.1.1.3" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.SS1.p2.7.m5.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m5.1b"><apply id="S3.SS1.p2.7.m5.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1"><eq id="S3.SS1.p2.7.m5.1.1.2.cmml" xref="S3.SS1.p2.7.m5.1.1.2"></eq><apply id="S3.SS1.p2.7.m5.1.1.3.cmml" xref="S3.SS1.p2.7.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m5.1.1.3.1.cmml" xref="S3.SS1.p2.7.m5.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.7.m5.1.1.3.2.cmml" xref="S3.SS1.p2.7.m5.1.1.3.2">𝑧</ci><ci id="S3.SS1.p2.7.m5.1.1.3.3.cmml" xref="S3.SS1.p2.7.m5.1.1.3.3">𝑑</ci></apply><apply id="S3.SS1.p2.7.m5.1.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1.1"><times id="S3.SS1.p2.7.m5.1.1.1.2.cmml" xref="S3.SS1.p2.7.m5.1.1.1.2"></times><ci id="S3.SS1.p2.7.m5.1.1.1.3.cmml" xref="S3.SS1.p2.7.m5.1.1.1.3">ℰ</ci><apply id="S3.SS1.p2.7.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2"><ci id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.1">~</ci><ci id="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.2.2">𝑥</ci></apply><ci id="S3.SS1.p2.7.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.7.m5.1.1.1.1.1.1.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m5.1c">z_{d}=\mathcal{E}(\tilde{x}_{d})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m5.1d">italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = caligraphic_E ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )</annotation></semantics></math>.
From this latent code, the normalized video depth can then be recovered by the decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m6.1"><semantics id="S3.SS1.p2.8.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m6.1.1" xref="S3.SS1.p2.8.m6.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m6.1b"><ci id="S3.SS1.p2.8.m6.1.1.cmml" xref="S3.SS1.p2.8.m6.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m6.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m6.1d">caligraphic_D</annotation></semantics></math>: <math alttext="\hat{x}_{d}=\mathcal{D}(z_{d})" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m7.1"><semantics id="S3.SS1.p2.9.m7.1a"><mrow id="S3.SS1.p2.9.m7.1.1" xref="S3.SS1.p2.9.m7.1.1.cmml"><msub id="S3.SS1.p2.9.m7.1.1.3" xref="S3.SS1.p2.9.m7.1.1.3.cmml"><mover accent="true" id="S3.SS1.p2.9.m7.1.1.3.2" xref="S3.SS1.p2.9.m7.1.1.3.2.cmml"><mi id="S3.SS1.p2.9.m7.1.1.3.2.2" xref="S3.SS1.p2.9.m7.1.1.3.2.2.cmml">x</mi><mo id="S3.SS1.p2.9.m7.1.1.3.2.1" xref="S3.SS1.p2.9.m7.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.SS1.p2.9.m7.1.1.3.3" xref="S3.SS1.p2.9.m7.1.1.3.3.cmml">d</mi></msub><mo id="S3.SS1.p2.9.m7.1.1.2" xref="S3.SS1.p2.9.m7.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.9.m7.1.1.1" xref="S3.SS1.p2.9.m7.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m7.1.1.1.3" xref="S3.SS1.p2.9.m7.1.1.1.3.cmml">𝒟</mi><mo id="S3.SS1.p2.9.m7.1.1.1.2" xref="S3.SS1.p2.9.m7.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.9.m7.1.1.1.1.1" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.9.m7.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.9.m7.1.1.1.1.1.1" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.9.m7.1.1.1.1.1.1.2" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS1.p2.9.m7.1.1.1.1.1.1.3" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.SS1.p2.9.m7.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m7.1b"><apply id="S3.SS1.p2.9.m7.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1"><eq id="S3.SS1.p2.9.m7.1.1.2.cmml" xref="S3.SS1.p2.9.m7.1.1.2"></eq><apply id="S3.SS1.p2.9.m7.1.1.3.cmml" xref="S3.SS1.p2.9.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m7.1.1.3.1.cmml" xref="S3.SS1.p2.9.m7.1.1.3">subscript</csymbol><apply id="S3.SS1.p2.9.m7.1.1.3.2.cmml" xref="S3.SS1.p2.9.m7.1.1.3.2"><ci id="S3.SS1.p2.9.m7.1.1.3.2.1.cmml" xref="S3.SS1.p2.9.m7.1.1.3.2.1">^</ci><ci id="S3.SS1.p2.9.m7.1.1.3.2.2.cmml" xref="S3.SS1.p2.9.m7.1.1.3.2.2">𝑥</ci></apply><ci id="S3.SS1.p2.9.m7.1.1.3.3.cmml" xref="S3.SS1.p2.9.m7.1.1.3.3">𝑑</ci></apply><apply id="S3.SS1.p2.9.m7.1.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1.1"><times id="S3.SS1.p2.9.m7.1.1.1.2.cmml" xref="S3.SS1.p2.9.m7.1.1.1.2"></times><ci id="S3.SS1.p2.9.m7.1.1.1.3.cmml" xref="S3.SS1.p2.9.m7.1.1.1.3">𝒟</ci><apply id="S3.SS1.p2.9.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m7.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.p2.9.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.9.m7.1.1.1.1.1.1.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m7.1c">\hat{x}_{d}=\mathcal{D}(z_{d})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m7.1d">over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = caligraphic_D ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )</annotation></semantics></math>.
Unlike the recent advanced 3D VAE <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib53" title="">2024c</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib34" title="">2024</a>)</cite>, which compresses the input across both temporal and spatial dimensions into the latent code, we focus on compressing only the spatial dimension, as in <cite class="ltx_cite ltx_citemacro_citet">Blattmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite>.
This is because temporal compression potentially causes motion blur artifacts when decoding latent depth codes, especially in videos with fast motion (detailed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS4" title="4.4 Ablation Studies ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.5">To condition the denoiser <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">v</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑣</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> on the input video, we first transform the video <math alttext="x_{c}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝑥</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">x_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> into latent space as <math alttext="z_{c}=\mathcal{E}(x_{c})" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><msub id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml"><mi id="S3.SS1.p3.3.m3.1.1.3.2" xref="S3.SS1.p3.3.m3.1.1.3.2.cmml">z</mi><mi id="S3.SS1.p3.3.m3.1.1.3.3" xref="S3.SS1.p3.3.m3.1.1.3.3.cmml">c</mi></msub><mo id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.3.m3.1.1.1.3" xref="S3.SS1.p3.3.m3.1.1.1.3.cmml">ℰ</mi><mo id="S3.SS1.p3.3.m3.1.1.1.2" xref="S3.SS1.p3.3.m3.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.3.m3.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.3.m3.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.3.cmml">c</mi></msub><mo id="S3.SS1.p3.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><eq id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2"></eq><apply id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.1.1.3.2">𝑧</ci><ci id="S3.SS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3.3">𝑐</ci></apply><apply id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"><times id="S3.SS1.p3.3.m3.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.2"></times><ci id="S3.SS1.p3.3.m3.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.3">ℰ</ci><apply id="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">z_{c}=\mathcal{E}(x_{c})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = caligraphic_E ( italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT )</annotation></semantics></math>.
Then, <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">z</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝑧</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">z_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is concatenated with the latent depth code <math alttext="z_{d}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">z</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">𝑧</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">z_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> frame by frame to form the input for the denoiser.
Unlike SVD, we remove the CLIP embedding condition and replace it with a zero embedding, as we find it has minimal impact on performance.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S3.F2.g1" src="extracted/5923385/figure/struct.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.20.1">The overall architecture</span>. The input video <math alttext="x_{c}" class="ltx_Math" display="inline" id="S3.F2.9.m1.1"><semantics id="S3.F2.9.m1.1b"><msub id="S3.F2.9.m1.1.1" xref="S3.F2.9.m1.1.1.cmml"><mi id="S3.F2.9.m1.1.1.2" xref="S3.F2.9.m1.1.1.2.cmml">x</mi><mi id="S3.F2.9.m1.1.1.3" xref="S3.F2.9.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.9.m1.1c"><apply id="S3.F2.9.m1.1.1.cmml" xref="S3.F2.9.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.9.m1.1.1.1.cmml" xref="S3.F2.9.m1.1.1">subscript</csymbol><ci id="S3.F2.9.m1.1.1.2.cmml" xref="S3.F2.9.m1.1.1.2">𝑥</ci><ci id="S3.F2.9.m1.1.1.3.cmml" xref="S3.F2.9.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.m1.1d">x_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.9.m1.1e">italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and depth <math alttext="x_{d}" class="ltx_Math" display="inline" id="S3.F2.10.m2.1"><semantics id="S3.F2.10.m2.1b"><msub id="S3.F2.10.m2.1.1" xref="S3.F2.10.m2.1.1.cmml"><mi id="S3.F2.10.m2.1.1.2" xref="S3.F2.10.m2.1.1.2.cmml">x</mi><mi id="S3.F2.10.m2.1.1.3" xref="S3.F2.10.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.10.m2.1c"><apply id="S3.F2.10.m2.1.1.cmml" xref="S3.F2.10.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.10.m2.1.1.1.cmml" xref="S3.F2.10.m2.1.1">subscript</csymbol><ci id="S3.F2.10.m2.1.1.2.cmml" xref="S3.F2.10.m2.1.1.2">𝑥</ci><ci id="S3.F2.10.m2.1.1.3.cmml" xref="S3.F2.10.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.m2.1d">x_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.10.m2.1e">italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> are first encoded into latent space using a pretrained latent encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.F2.11.m3.1"><semantics id="S3.F2.11.m3.1b"><mi class="ltx_font_mathcaligraphic" id="S3.F2.11.m3.1.1" xref="S3.F2.11.m3.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.11.m3.1c"><ci id="S3.F2.11.m3.1.1.cmml" xref="S3.F2.11.m3.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.m3.1d">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.11.m3.1e">caligraphic_E</annotation></semantics></math>. During training, Gaussian noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.F2.12.m4.1"><semantics id="S3.F2.12.m4.1b"><mi id="S3.F2.12.m4.1.1" xref="S3.F2.12.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.12.m4.1c"><ci id="S3.F2.12.m4.1.1.cmml" xref="S3.F2.12.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.m4.1d">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.F2.12.m4.1e">italic_ϵ</annotation></semantics></math> is added to the latent depth in a forward process, while a denoising model <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.F2.13.m5.1"><semantics id="S3.F2.13.m5.1b"><msub id="S3.F2.13.m5.1.1" xref="S3.F2.13.m5.1.1.cmml"><mi id="S3.F2.13.m5.1.1.2" xref="S3.F2.13.m5.1.1.2.cmml">v</mi><mi id="S3.F2.13.m5.1.1.3" xref="S3.F2.13.m5.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.13.m5.1c"><apply id="S3.F2.13.m5.1.1.cmml" xref="S3.F2.13.m5.1.1"><csymbol cd="ambiguous" id="S3.F2.13.m5.1.1.1.cmml" xref="S3.F2.13.m5.1.1">subscript</csymbol><ci id="S3.F2.13.m5.1.1.2.cmml" xref="S3.F2.13.m5.1.1.2">𝑣</ci><ci id="S3.F2.13.m5.1.1.3.cmml" xref="S3.F2.13.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.m5.1d">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.13.m5.1e">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, conditioned on the latent video <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.F2.14.m6.1"><semantics id="S3.F2.14.m6.1b"><msub id="S3.F2.14.m6.1.1" xref="S3.F2.14.m6.1.1.cmml"><mi id="S3.F2.14.m6.1.1.2" xref="S3.F2.14.m6.1.1.2.cmml">z</mi><mi id="S3.F2.14.m6.1.1.3" xref="S3.F2.14.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.14.m6.1c"><apply id="S3.F2.14.m6.1.1.cmml" xref="S3.F2.14.m6.1.1"><csymbol cd="ambiguous" id="S3.F2.14.m6.1.1.1.cmml" xref="S3.F2.14.m6.1.1">subscript</csymbol><ci id="S3.F2.14.m6.1.1.2.cmml" xref="S3.F2.14.m6.1.1.2">𝑧</ci><ci id="S3.F2.14.m6.1.1.3.cmml" xref="S3.F2.14.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.m6.1d">z_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.14.m6.1e">italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, removes the noise in a reverse process. After training, the inference flow begins with pure noise, progressively denoises it, and then uses a latent decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.F2.15.m7.1"><semantics id="S3.F2.15.m7.1b"><mi class="ltx_font_mathcaligraphic" id="S3.F2.15.m7.1.1" xref="S3.F2.15.m7.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.F2.15.m7.1c"><ci id="S3.F2.15.m7.1.1.cmml" xref="S3.F2.15.m7.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.15.m7.1d">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.15.m7.1e">caligraphic_D</annotation></semantics></math> to transform it into the prediction depth <math alttext="\hat{x}_{d}" class="ltx_Math" display="inline" id="S3.F2.16.m8.1"><semantics id="S3.F2.16.m8.1b"><msub id="S3.F2.16.m8.1.1" xref="S3.F2.16.m8.1.1.cmml"><mover accent="true" id="S3.F2.16.m8.1.1.2" xref="S3.F2.16.m8.1.1.2.cmml"><mi id="S3.F2.16.m8.1.1.2.2" xref="S3.F2.16.m8.1.1.2.2.cmml">x</mi><mo id="S3.F2.16.m8.1.1.2.1" xref="S3.F2.16.m8.1.1.2.1.cmml">^</mo></mover><mi id="S3.F2.16.m8.1.1.3" xref="S3.F2.16.m8.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.16.m8.1c"><apply id="S3.F2.16.m8.1.1.cmml" xref="S3.F2.16.m8.1.1"><csymbol cd="ambiguous" id="S3.F2.16.m8.1.1.1.cmml" xref="S3.F2.16.m8.1.1">subscript</csymbol><apply id="S3.F2.16.m8.1.1.2.cmml" xref="S3.F2.16.m8.1.1.2"><ci id="S3.F2.16.m8.1.1.2.1.cmml" xref="S3.F2.16.m8.1.1.2.1">^</ci><ci id="S3.F2.16.m8.1.1.2.2.cmml" xref="S3.F2.16.m8.1.1.2.2">𝑥</ci></apply><ci id="S3.F2.16.m8.1.1.3.cmml" xref="S3.F2.16.m8.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.16.m8.1d">\hat{x}_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.16.m8.1e">over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> with the original resolution.
Besides, the pipeline incorporates a mixed-duration training strategy: <span class="ltx_text ltx_font_bold" id="S3.F2.21.2">(a) frame dropout</span> and <span class="ltx_text ltx_font_bold" id="S3.F2.22.3">(b) video packing</span> to enhance model generalization and training efficiency.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.2"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.2.1">Conditional Flow Matching.</span>
To accelerate the denoising process, we replace the original EDM framework <cite class="ltx_cite ltx_citemacro_citep">(Karras et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib26" title="">2022</a>)</cite> in SVD with conditional flow matching <cite class="ltx_cite ltx_citemacro_citep">(Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib30" title="">2023</a>)</cite>, which achieves satisfactory results in just 1 step, compared to the original 25 steps.
Concretely, the data corruption in our framework is formulated as a linear interpolation between Gaussian noise <math alttext="\epsilon\sim\mathcal{N}(0,I)" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.2"><semantics id="S3.SS1.p4.1.m1.2a"><mrow id="S3.SS1.p4.1.m1.2.3" xref="S3.SS1.p4.1.m1.2.3.cmml"><mi id="S3.SS1.p4.1.m1.2.3.2" xref="S3.SS1.p4.1.m1.2.3.2.cmml">ϵ</mi><mo id="S3.SS1.p4.1.m1.2.3.1" xref="S3.SS1.p4.1.m1.2.3.1.cmml">∼</mo><mrow id="S3.SS1.p4.1.m1.2.3.3" xref="S3.SS1.p4.1.m1.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.2.3.3.2" xref="S3.SS1.p4.1.m1.2.3.3.2.cmml">𝒩</mi><mo id="S3.SS1.p4.1.m1.2.3.3.1" xref="S3.SS1.p4.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS1.p4.1.m1.2.3.3.3.2" xref="S3.SS1.p4.1.m1.2.3.3.3.1.cmml"><mo id="S3.SS1.p4.1.m1.2.3.3.3.2.1" stretchy="false" xref="S3.SS1.p4.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">0</mn><mo id="S3.SS1.p4.1.m1.2.3.3.3.2.2" xref="S3.SS1.p4.1.m1.2.3.3.3.1.cmml">,</mo><mi id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml">I</mi><mo id="S3.SS1.p4.1.m1.2.3.3.3.2.3" stretchy="false" xref="S3.SS1.p4.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.3.cmml" xref="S3.SS1.p4.1.m1.2.3"><csymbol cd="latexml" id="S3.SS1.p4.1.m1.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3.1">similar-to</csymbol><ci id="S3.SS1.p4.1.m1.2.3.2.cmml" xref="S3.SS1.p4.1.m1.2.3.2">italic-ϵ</ci><apply id="S3.SS1.p4.1.m1.2.3.3.cmml" xref="S3.SS1.p4.1.m1.2.3.3"><times id="S3.SS1.p4.1.m1.2.3.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3.3.1"></times><ci id="S3.SS1.p4.1.m1.2.3.3.2.cmml" xref="S3.SS1.p4.1.m1.2.3.3.2">𝒩</ci><interval closure="open" id="S3.SS1.p4.1.m1.2.3.3.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3.3.3.2"><cn id="S3.SS1.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p4.1.m1.1.1">0</cn><ci id="S3.SS1.p4.1.m1.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2">𝐼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">\epsilon\sim\mathcal{N}(0,I)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.2d">italic_ϵ ∼ caligraphic_N ( 0 , italic_I )</annotation></semantics></math> and data <math alttext="x\sim p(x)" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.2" xref="S3.SS1.p4.2.m2.1.2.cmml"><mi id="S3.SS1.p4.2.m2.1.2.2" xref="S3.SS1.p4.2.m2.1.2.2.cmml">x</mi><mo id="S3.SS1.p4.2.m2.1.2.1" xref="S3.SS1.p4.2.m2.1.2.1.cmml">∼</mo><mrow id="S3.SS1.p4.2.m2.1.2.3" xref="S3.SS1.p4.2.m2.1.2.3.cmml"><mi id="S3.SS1.p4.2.m2.1.2.3.2" xref="S3.SS1.p4.2.m2.1.2.3.2.cmml">p</mi><mo id="S3.SS1.p4.2.m2.1.2.3.1" xref="S3.SS1.p4.2.m2.1.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p4.2.m2.1.2.3.3.2" xref="S3.SS1.p4.2.m2.1.2.3.cmml"><mo id="S3.SS1.p4.2.m2.1.2.3.3.2.1" stretchy="false" xref="S3.SS1.p4.2.m2.1.2.3.cmml">(</mo><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">x</mi><mo id="S3.SS1.p4.2.m2.1.2.3.3.2.2" stretchy="false" xref="S3.SS1.p4.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.2.cmml" xref="S3.SS1.p4.2.m2.1.2"><csymbol cd="latexml" id="S3.SS1.p4.2.m2.1.2.1.cmml" xref="S3.SS1.p4.2.m2.1.2.1">similar-to</csymbol><ci id="S3.SS1.p4.2.m2.1.2.2.cmml" xref="S3.SS1.p4.2.m2.1.2.2">𝑥</ci><apply id="S3.SS1.p4.2.m2.1.2.3.cmml" xref="S3.SS1.p4.2.m2.1.2.3"><times id="S3.SS1.p4.2.m2.1.2.3.1.cmml" xref="S3.SS1.p4.2.m2.1.2.3.1"></times><ci id="S3.SS1.p4.2.m2.1.2.3.2.cmml" xref="S3.SS1.p4.2.m2.1.2.3.2">𝑝</ci><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">x\sim p(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_x ∼ italic_p ( italic_x )</annotation></semantics></math> along a straight line:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\phi_{t}(x)=tx+\left(1-t\right)\epsilon," class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.cmml">ϕ</mi><mi id="S3.E2.m1.2.2.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.cmml"><mo id="S3.E2.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.3.2.cmml">t</mi><mo id="S3.E2.m1.2.2.1.1.1.3.1" xref="S3.E2.m1.2.2.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.3.3.cmml">x</mi></mrow><mo id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">ϵ</mi></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></times><apply id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2">italic-ϕ</ci><ci id="S3.E2.m1.2.2.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3">𝑡</ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑥</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><plus id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></plus><apply id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3"><times id="S3.E2.m1.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.3.2">𝑡</ci><ci id="S3.E2.m1.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3.3">𝑥</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"></minus><cn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\phi_{t}(x)=tx+\left(1-t\right)\epsilon,</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) = italic_t italic_x + ( 1 - italic_t ) italic_ϵ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p4.4">where <math alttext="\phi_{t}(x)" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m1.1"><semantics id="S3.SS1.p4.3.m1.1a"><mrow id="S3.SS1.p4.3.m1.1.2" xref="S3.SS1.p4.3.m1.1.2.cmml"><msub id="S3.SS1.p4.3.m1.1.2.2" xref="S3.SS1.p4.3.m1.1.2.2.cmml"><mi id="S3.SS1.p4.3.m1.1.2.2.2" xref="S3.SS1.p4.3.m1.1.2.2.2.cmml">ϕ</mi><mi id="S3.SS1.p4.3.m1.1.2.2.3" xref="S3.SS1.p4.3.m1.1.2.2.3.cmml">t</mi></msub><mo id="S3.SS1.p4.3.m1.1.2.1" xref="S3.SS1.p4.3.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.p4.3.m1.1.2.3.2" xref="S3.SS1.p4.3.m1.1.2.cmml"><mo id="S3.SS1.p4.3.m1.1.2.3.2.1" stretchy="false" xref="S3.SS1.p4.3.m1.1.2.cmml">(</mo><mi id="S3.SS1.p4.3.m1.1.1" xref="S3.SS1.p4.3.m1.1.1.cmml">x</mi><mo id="S3.SS1.p4.3.m1.1.2.3.2.2" stretchy="false" xref="S3.SS1.p4.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m1.1b"><apply id="S3.SS1.p4.3.m1.1.2.cmml" xref="S3.SS1.p4.3.m1.1.2"><times id="S3.SS1.p4.3.m1.1.2.1.cmml" xref="S3.SS1.p4.3.m1.1.2.1"></times><apply id="S3.SS1.p4.3.m1.1.2.2.cmml" xref="S3.SS1.p4.3.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m1.1.2.2.1.cmml" xref="S3.SS1.p4.3.m1.1.2.2">subscript</csymbol><ci id="S3.SS1.p4.3.m1.1.2.2.2.cmml" xref="S3.SS1.p4.3.m1.1.2.2.2">italic-ϕ</ci><ci id="S3.SS1.p4.3.m1.1.2.2.3.cmml" xref="S3.SS1.p4.3.m1.1.2.2.3">𝑡</ci></apply><ci id="S3.SS1.p4.3.m1.1.1.cmml" xref="S3.SS1.p4.3.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m1.1c">\phi_{t}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m1.1d">italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math> represents the corrupted data, with <math alttext="t\in[0,1]" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m2.2"><semantics id="S3.SS1.p4.4.m2.2a"><mrow id="S3.SS1.p4.4.m2.2.3" xref="S3.SS1.p4.4.m2.2.3.cmml"><mi id="S3.SS1.p4.4.m2.2.3.2" xref="S3.SS1.p4.4.m2.2.3.2.cmml">t</mi><mo id="S3.SS1.p4.4.m2.2.3.1" xref="S3.SS1.p4.4.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p4.4.m2.2.3.3.2" xref="S3.SS1.p4.4.m2.2.3.3.1.cmml"><mo id="S3.SS1.p4.4.m2.2.3.3.2.1" stretchy="false" xref="S3.SS1.p4.4.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p4.4.m2.1.1" xref="S3.SS1.p4.4.m2.1.1.cmml">0</mn><mo id="S3.SS1.p4.4.m2.2.3.3.2.2" xref="S3.SS1.p4.4.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p4.4.m2.2.2" xref="S3.SS1.p4.4.m2.2.2.cmml">1</mn><mo id="S3.SS1.p4.4.m2.2.3.3.2.3" stretchy="false" xref="S3.SS1.p4.4.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m2.2b"><apply id="S3.SS1.p4.4.m2.2.3.cmml" xref="S3.SS1.p4.4.m2.2.3"><in id="S3.SS1.p4.4.m2.2.3.1.cmml" xref="S3.SS1.p4.4.m2.2.3.1"></in><ci id="S3.SS1.p4.4.m2.2.3.2.cmml" xref="S3.SS1.p4.4.m2.2.3.2">𝑡</ci><interval closure="closed" id="S3.SS1.p4.4.m2.2.3.3.1.cmml" xref="S3.SS1.p4.4.m2.2.3.3.2"><cn id="S3.SS1.p4.4.m2.1.1.cmml" type="integer" xref="S3.SS1.p4.4.m2.1.1">0</cn><cn id="S3.SS1.p4.4.m2.2.2.cmml" type="integer" xref="S3.SS1.p4.4.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m2.2c">t\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m2.2d">italic_t ∈ [ 0 , 1 ]</annotation></semantics></math> as the time-dependent interpolation factor.
This formulation implies a uniform transformation with constant velocity between data and noise.
The corresponding time-dependent velocity field, moving from noise to data, is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="v_{t}(x)=x-\epsilon." class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><msub id="S3.E3.m1.2.2.1.1.2.2" xref="S3.E3.m1.2.2.1.1.2.2.cmml"><mi id="S3.E3.m1.2.2.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.cmml">v</mi><mi id="S3.E3.m1.2.2.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.3.cmml">t</mi></msub><mo id="S3.E3.m1.2.2.1.1.2.1" xref="S3.E3.m1.2.2.1.1.2.1.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><mo id="S3.E3.m1.2.2.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">x</mi><mo id="S3.E3.m1.2.2.1.1.2.3.2.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml">x</mi><mo id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">−</mo><mi id="S3.E3.m1.2.2.1.1.3.3" xref="S3.E3.m1.2.2.1.1.3.3.cmml">ϵ</mi></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" lspace="0em" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"></eq><apply id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><times id="S3.E3.m1.2.2.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.1"></times><apply id="S3.E3.m1.2.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2">𝑣</ci><ci id="S3.E3.m1.2.2.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3">𝑡</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑥</ci></apply><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><minus id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1"></minus><ci id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2">𝑥</ci><ci id="S3.E3.m1.2.2.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3">italic-ϵ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">v_{t}(x)=x-\epsilon.</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) = italic_x - italic_ϵ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p4.5">The velocity field <math alttext="v_{t}:[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m1.2"><semantics id="S3.SS1.p4.5.m1.2a"><mrow id="S3.SS1.p4.5.m1.2.3" xref="S3.SS1.p4.5.m1.2.3.cmml"><msub id="S3.SS1.p4.5.m1.2.3.2" xref="S3.SS1.p4.5.m1.2.3.2.cmml"><mi id="S3.SS1.p4.5.m1.2.3.2.2" xref="S3.SS1.p4.5.m1.2.3.2.2.cmml">v</mi><mi id="S3.SS1.p4.5.m1.2.3.2.3" xref="S3.SS1.p4.5.m1.2.3.2.3.cmml">t</mi></msub><mo id="S3.SS1.p4.5.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS1.p4.5.m1.2.3.1.cmml">:</mo><mrow id="S3.SS1.p4.5.m1.2.3.3" xref="S3.SS1.p4.5.m1.2.3.3.cmml"><mrow id="S3.SS1.p4.5.m1.2.3.3.2" xref="S3.SS1.p4.5.m1.2.3.3.2.cmml"><mrow id="S3.SS1.p4.5.m1.2.3.3.2.2.2" xref="S3.SS1.p4.5.m1.2.3.3.2.2.1.cmml"><mo id="S3.SS1.p4.5.m1.2.3.3.2.2.2.1" stretchy="false" xref="S3.SS1.p4.5.m1.2.3.3.2.2.1.cmml">[</mo><mn id="S3.SS1.p4.5.m1.1.1" xref="S3.SS1.p4.5.m1.1.1.cmml">0</mn><mo id="S3.SS1.p4.5.m1.2.3.3.2.2.2.2" xref="S3.SS1.p4.5.m1.2.3.3.2.2.1.cmml">,</mo><mn id="S3.SS1.p4.5.m1.2.2" xref="S3.SS1.p4.5.m1.2.2.cmml">1</mn><mo id="S3.SS1.p4.5.m1.2.3.3.2.2.2.3" rspace="0.055em" stretchy="false" xref="S3.SS1.p4.5.m1.2.3.3.2.2.1.cmml">]</mo></mrow><mo id="S3.SS1.p4.5.m1.2.3.3.2.1" rspace="0.222em" xref="S3.SS1.p4.5.m1.2.3.3.2.1.cmml">×</mo><msup id="S3.SS1.p4.5.m1.2.3.3.2.3" xref="S3.SS1.p4.5.m1.2.3.3.2.3.cmml"><mi id="S3.SS1.p4.5.m1.2.3.3.2.3.2" xref="S3.SS1.p4.5.m1.2.3.3.2.3.2.cmml">ℝ</mi><mi id="S3.SS1.p4.5.m1.2.3.3.2.3.3" xref="S3.SS1.p4.5.m1.2.3.3.2.3.3.cmml">d</mi></msup></mrow><mo id="S3.SS1.p4.5.m1.2.3.3.1" stretchy="false" xref="S3.SS1.p4.5.m1.2.3.3.1.cmml">→</mo><msup id="S3.SS1.p4.5.m1.2.3.3.3" xref="S3.SS1.p4.5.m1.2.3.3.3.cmml"><mi id="S3.SS1.p4.5.m1.2.3.3.3.2" xref="S3.SS1.p4.5.m1.2.3.3.3.2.cmml">ℝ</mi><mi id="S3.SS1.p4.5.m1.2.3.3.3.3" xref="S3.SS1.p4.5.m1.2.3.3.3.3.cmml">d</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m1.2b"><apply id="S3.SS1.p4.5.m1.2.3.cmml" xref="S3.SS1.p4.5.m1.2.3"><ci id="S3.SS1.p4.5.m1.2.3.1.cmml" xref="S3.SS1.p4.5.m1.2.3.1">:</ci><apply id="S3.SS1.p4.5.m1.2.3.2.cmml" xref="S3.SS1.p4.5.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m1.2.3.2.1.cmml" xref="S3.SS1.p4.5.m1.2.3.2">subscript</csymbol><ci id="S3.SS1.p4.5.m1.2.3.2.2.cmml" xref="S3.SS1.p4.5.m1.2.3.2.2">𝑣</ci><ci id="S3.SS1.p4.5.m1.2.3.2.3.cmml" xref="S3.SS1.p4.5.m1.2.3.2.3">𝑡</ci></apply><apply id="S3.SS1.p4.5.m1.2.3.3.cmml" xref="S3.SS1.p4.5.m1.2.3.3"><ci id="S3.SS1.p4.5.m1.2.3.3.1.cmml" xref="S3.SS1.p4.5.m1.2.3.3.1">→</ci><apply id="S3.SS1.p4.5.m1.2.3.3.2.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2"><times id="S3.SS1.p4.5.m1.2.3.3.2.1.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.1"></times><interval closure="closed" id="S3.SS1.p4.5.m1.2.3.3.2.2.1.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.2.2"><cn id="S3.SS1.p4.5.m1.1.1.cmml" type="integer" xref="S3.SS1.p4.5.m1.1.1">0</cn><cn id="S3.SS1.p4.5.m1.2.2.cmml" type="integer" xref="S3.SS1.p4.5.m1.2.2">1</cn></interval><apply id="S3.SS1.p4.5.m1.2.3.3.2.3.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m1.2.3.3.2.3.1.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.3">superscript</csymbol><ci id="S3.SS1.p4.5.m1.2.3.3.2.3.2.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.3.2">ℝ</ci><ci id="S3.SS1.p4.5.m1.2.3.3.2.3.3.cmml" xref="S3.SS1.p4.5.m1.2.3.3.2.3.3">𝑑</ci></apply></apply><apply id="S3.SS1.p4.5.m1.2.3.3.3.cmml" xref="S3.SS1.p4.5.m1.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m1.2.3.3.3.1.cmml" xref="S3.SS1.p4.5.m1.2.3.3.3">superscript</csymbol><ci id="S3.SS1.p4.5.m1.2.3.3.3.2.cmml" xref="S3.SS1.p4.5.m1.2.3.3.3.2">ℝ</ci><ci id="S3.SS1.p4.5.m1.2.3.3.3.3.cmml" xref="S3.SS1.p4.5.m1.2.3.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m1.2c">v_{t}:[0,1]\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m1.2d">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : [ 0 , 1 ] × blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> defines an ordinary differential equation (ODE):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d\phi_{t}(x)=v_{t}\left(\phi_{t}(x)\right)dt." class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.3.2" xref="S3.E4.m1.3.3.1.1.3.2.cmml">d</mi><mo id="S3.E4.m1.3.3.1.1.3.1" xref="S3.E4.m1.3.3.1.1.3.1.cmml">⁢</mo><msub id="S3.E4.m1.3.3.1.1.3.3" xref="S3.E4.m1.3.3.1.1.3.3.cmml"><mi id="S3.E4.m1.3.3.1.1.3.3.2" xref="S3.E4.m1.3.3.1.1.3.3.2.cmml">ϕ</mi><mi id="S3.E4.m1.3.3.1.1.3.3.3" xref="S3.E4.m1.3.3.1.1.3.3.3.cmml">t</mi></msub><mo id="S3.E4.m1.3.3.1.1.3.1a" xref="S3.E4.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.3.4.2" xref="S3.E4.m1.3.3.1.1.3.cmml"><mo id="S3.E4.m1.3.3.1.1.3.4.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">x</mi><mo id="S3.E4.m1.3.3.1.1.3.4.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.3.2.cmml">v</mi><mi id="S3.E4.m1.3.3.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml">ϕ</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">x</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E4.m1.3.3.1.1.1.2a" xref="S3.E4.m1.3.3.1.1.1.2.cmml">⁢</mo><mi id="S3.E4.m1.3.3.1.1.1.4" xref="S3.E4.m1.3.3.1.1.1.4.cmml">d</mi><mo id="S3.E4.m1.3.3.1.1.1.2b" xref="S3.E4.m1.3.3.1.1.1.2.cmml">⁢</mo><mi id="S3.E4.m1.3.3.1.1.1.5" xref="S3.E4.m1.3.3.1.1.1.5.cmml">t</mi></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" lspace="0em" xref="S3.E4.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><eq id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"></eq><apply id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"><times id="S3.E4.m1.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.1"></times><ci id="S3.E4.m1.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2">𝑑</ci><apply id="S3.E4.m1.3.3.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.3.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.3.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.3.2">italic-ϕ</ci><ci id="S3.E4.m1.3.3.1.1.3.3.3.cmml" xref="S3.E4.m1.3.3.1.1.3.3.3">𝑡</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑥</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2"></times><apply id="S3.E4.m1.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.3.2">𝑣</ci><ci id="S3.E4.m1.3.3.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1"></times><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.2">italic-ϕ</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑥</ci></apply><ci id="S3.E4.m1.3.3.1.1.1.4.cmml" xref="S3.E4.m1.3.3.1.1.1.4">𝑑</ci><ci id="S3.E4.m1.3.3.1.1.1.5.cmml" xref="S3.E4.m1.3.3.1.1.1.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">d\phi_{t}(x)=v_{t}\left(\phi_{t}(x)\right)dt.</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_d italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) = italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ) ) italic_d italic_t .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p4.8">By solving this ODE from <math alttext="t=0" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m1.1"><semantics id="S3.SS1.p4.6.m1.1a"><mrow id="S3.SS1.p4.6.m1.1.1" xref="S3.SS1.p4.6.m1.1.1.cmml"><mi id="S3.SS1.p4.6.m1.1.1.2" xref="S3.SS1.p4.6.m1.1.1.2.cmml">t</mi><mo id="S3.SS1.p4.6.m1.1.1.1" xref="S3.SS1.p4.6.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.6.m1.1.1.3" xref="S3.SS1.p4.6.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m1.1b"><apply id="S3.SS1.p4.6.m1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1"><eq id="S3.SS1.p4.6.m1.1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1.1"></eq><ci id="S3.SS1.p4.6.m1.1.1.2.cmml" xref="S3.SS1.p4.6.m1.1.1.2">𝑡</ci><cn id="S3.SS1.p4.6.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p4.6.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m1.1c">t=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.6.m1.1d">italic_t = 0</annotation></semantics></math> to <math alttext="t=1" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m2.1"><semantics id="S3.SS1.p4.7.m2.1a"><mrow id="S3.SS1.p4.7.m2.1.1" xref="S3.SS1.p4.7.m2.1.1.cmml"><mi id="S3.SS1.p4.7.m2.1.1.2" xref="S3.SS1.p4.7.m2.1.1.2.cmml">t</mi><mo id="S3.SS1.p4.7.m2.1.1.1" xref="S3.SS1.p4.7.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.7.m2.1.1.3" xref="S3.SS1.p4.7.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m2.1b"><apply id="S3.SS1.p4.7.m2.1.1.cmml" xref="S3.SS1.p4.7.m2.1.1"><eq id="S3.SS1.p4.7.m2.1.1.1.cmml" xref="S3.SS1.p4.7.m2.1.1.1"></eq><ci id="S3.SS1.p4.7.m2.1.1.2.cmml" xref="S3.SS1.p4.7.m2.1.1.2">𝑡</ci><cn id="S3.SS1.p4.7.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p4.7.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m2.1c">t=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.7.m2.1d">italic_t = 1</annotation></semantics></math>, we can transform noise into a data sample using the approximated velocity field <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p4.8.m3.1"><semantics id="S3.SS1.p4.8.m3.1a"><msub id="S3.SS1.p4.8.m3.1.1" xref="S3.SS1.p4.8.m3.1.1.cmml"><mi id="S3.SS1.p4.8.m3.1.1.2" xref="S3.SS1.p4.8.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.p4.8.m3.1.1.3" xref="S3.SS1.p4.8.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m3.1b"><apply id="S3.SS1.p4.8.m3.1.1.cmml" xref="S3.SS1.p4.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m3.1.1.1.cmml" xref="S3.SS1.p4.8.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.8.m3.1.1.2.cmml" xref="S3.SS1.p4.8.m3.1.1.2">𝑣</ci><ci id="S3.SS1.p4.8.m3.1.1.3.cmml" xref="S3.SS1.p4.8.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m3.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.8.m3.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.
During training, the flow matching objective directly predicts the target velocity to generate the desired probability trajectory:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\theta}=\mathbb{E}_{t}\left\|v_{\theta}\left(\phi_{t}\left(z_{d}%
\right),z_{c},t\right)-v_{t}\left(z_{d}\right)\right\|^{2}," class="ltx_Math" display="block" id="S3.E5.m1.2"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.2.2.1.1.3.2" xref="S3.E5.m1.2.2.1.1.3.2.cmml">ℒ</mi><mi id="S3.E5.m1.2.2.1.1.3.3" xref="S3.E5.m1.2.2.1.1.3.3.cmml">θ</mi></msub><mo id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.3.2.cmml">𝔼</mi><mi id="S3.E5.m1.2.2.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml">⁢</mo><msup id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.2.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.2.cmml">v</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.3.cmml">θ</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ϕ</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.cmml">z</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.3.cmml">c</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">t</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.6" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.4.cmml">−</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml">v</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">⁢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.cmml">(</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.2.cmml">z</mi><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.3.cmml">d</mi></msub><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E5.m1.2.2.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><apply id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2">ℒ</ci><ci id="S3.E5.m1.2.2.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.3.3">𝜃</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"></times><apply id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2">𝔼</ci><ci id="S3.E5.m1.2.2.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1">superscript</csymbol><apply id="S3.E5.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.E5.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.4"></minus><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.3"></times><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.2">𝑣</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.4.3">𝜃</ci></apply><vector id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2">italic-ϕ</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑑</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.2">𝑧</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.3">𝑐</ci></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝑡</ci></vector></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2"></times><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.2">𝑣</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.2">𝑧</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.1.1.1.3">𝑑</ci></apply></apply></apply></apply><cn id="S3.E5.m1.2.2.1.1.1.1.3.cmml" type="integer" xref="S3.E5.m1.2.2.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\mathcal{L}_{\theta}=\mathbb{E}_{t}\left\|v_{\theta}\left(\phi_{t}\left(z_{d}%
\right),z_{c},t\right)-v_{t}\left(z_{d}\right)\right\|^{2},</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_t ) - italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p4.10">where <math alttext="z_{d}" class="ltx_Math" display="inline" id="S3.SS1.p4.9.m1.1"><semantics id="S3.SS1.p4.9.m1.1a"><msub id="S3.SS1.p4.9.m1.1.1" xref="S3.SS1.p4.9.m1.1.1.cmml"><mi id="S3.SS1.p4.9.m1.1.1.2" xref="S3.SS1.p4.9.m1.1.1.2.cmml">z</mi><mi id="S3.SS1.p4.9.m1.1.1.3" xref="S3.SS1.p4.9.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m1.1b"><apply id="S3.SS1.p4.9.m1.1.1.cmml" xref="S3.SS1.p4.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m1.1.1.1.cmml" xref="S3.SS1.p4.9.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.9.m1.1.1.2.cmml" xref="S3.SS1.p4.9.m1.1.1.2">𝑧</ci><ci id="S3.SS1.p4.9.m1.1.1.3.cmml" xref="S3.SS1.p4.9.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m1.1c">z_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.9.m1.1d">italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS1.p4.10.m2.1"><semantics id="S3.SS1.p4.10.m2.1a"><msub id="S3.SS1.p4.10.m2.1.1" xref="S3.SS1.p4.10.m2.1.1.cmml"><mi id="S3.SS1.p4.10.m2.1.1.2" xref="S3.SS1.p4.10.m2.1.1.2.cmml">z</mi><mi id="S3.SS1.p4.10.m2.1.1.3" xref="S3.SS1.p4.10.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m2.1b"><apply id="S3.SS1.p4.10.m2.1.1.cmml" xref="S3.SS1.p4.10.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m2.1.1.1.cmml" xref="S3.SS1.p4.10.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.10.m2.1.1.2.cmml" xref="S3.SS1.p4.10.m2.1.1.2">𝑧</ci><ci id="S3.SS1.p4.10.m2.1.1.3.cmml" xref="S3.SS1.p4.10.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m2.1c">z_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.10.m2.1d">italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> represent the latent depth code and video code, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Mixed-duration Training Strategy</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Real-world applications often encounter data in various formats, including images and variable-length videos.
To enhance the model’s generalization across tasks like image and video depth estimation, we implement a mixed-duration training strategy to ensure robustness across various inputs.
This strategy includes frame dropout augmentation, which preserves training efficiency when handling long video sequences, and a video packing technique that optimizes memory usage for variable-length videos, enabling our model to scale efficiently across different input formats.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.3.1">Frame Dropout.</span>
Directly training long-frame videos is computationally expensive, requiring substantial training time and GPU resources.
Inspired by context extension techniques <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib10" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib8" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib31" title="">2024</a>)</cite> in large language models, we propose frame dropout augmentation with rotary position encoding (RoPE) <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib46" title="">2021</a>)</cite> to enhance training efficiency while maintaining adaptability for long videos.
Concretely, in each temporal transformer block of the 3D UNet used in SVD, we replace the original sinusoidal absolute position encoding for fixed-frame videos with RoPE to support variable frames.
However, training on a short video with RoPE still struggles to generalize to longer ones with unlearned frame positions, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F2" title="Figure 2 ‣ 3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>(a).
To mitigate this, we retain the original frame position indices <math alttext="i=[0,\cdots,T-1]" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.3"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3" xref="S3.SS2.p2.1.m1.3.3.cmml"><mi id="S3.SS2.p2.1.m1.3.3.3" xref="S3.SS2.p2.1.m1.3.3.3.cmml">i</mi><mo id="S3.SS2.p2.1.m1.3.3.2" xref="S3.SS2.p2.1.m1.3.3.2.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.3.3.1.1" xref="S3.SS2.p2.1.m1.3.3.1.2.cmml"><mo id="S3.SS2.p2.1.m1.3.3.1.1.2" stretchy="false" xref="S3.SS2.p2.1.m1.3.3.1.2.cmml">[</mo><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p2.1.m1.3.3.1.1.3" xref="S3.SS2.p2.1.m1.3.3.1.2.cmml">,</mo><mi id="S3.SS2.p2.1.m1.2.2" mathvariant="normal" xref="S3.SS2.p2.1.m1.2.2.cmml">⋯</mi><mo id="S3.SS2.p2.1.m1.3.3.1.1.4" xref="S3.SS2.p2.1.m1.3.3.1.2.cmml">,</mo><mrow id="S3.SS2.p2.1.m1.3.3.1.1.1" xref="S3.SS2.p2.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.3.3.1.1.1.2" xref="S3.SS2.p2.1.m1.3.3.1.1.1.2.cmml">T</mi><mo id="S3.SS2.p2.1.m1.3.3.1.1.1.1" xref="S3.SS2.p2.1.m1.3.3.1.1.1.1.cmml">−</mo><mn id="S3.SS2.p2.1.m1.3.3.1.1.1.3" xref="S3.SS2.p2.1.m1.3.3.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p2.1.m1.3.3.1.1.5" stretchy="false" xref="S3.SS2.p2.1.m1.3.3.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><apply id="S3.SS2.p2.1.m1.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3"><eq id="S3.SS2.p2.1.m1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2"></eq><ci id="S3.SS2.p2.1.m1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.3">𝑖</ci><list id="S3.SS2.p2.1.m1.3.3.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1">0</cn><ci id="S3.SS2.p2.1.m1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2">⋯</ci><apply id="S3.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1.1"><minus id="S3.SS2.p2.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1.1.1"></minus><ci id="S3.SS2.p2.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1.1.2">𝑇</ci><cn id="S3.SS2.p2.1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.3.3.1.1.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">i=[0,\cdots,T-1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.3d">italic_i = [ 0 , ⋯ , italic_T - 1 ]</annotation></semantics></math> of the long video with <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_T</annotation></semantics></math> frames, and randomly sample <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_K</annotation></semantics></math> frames with their original indices for training.
This simple strategy helps the temporal layer generalize effectively across variable frame lengths.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Video Packing.</span>
To train videos of varying lengths, an intuitive way is to use only one sample per batch, as all data within a batch must maintain a consistent shape.
However, this leads to inefficient memory usage for shorter videos.
To solve this, we first group videos by similar resolution and crop them to a fixed size.
For each batch, we then sample examples from the same group and apply the same frame dropout parameter <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_K</annotation></semantics></math>.
The process is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F2" title="Figure 2 ‣ 3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>(b).
In particular, we increase the batch size for small-resolution and short-duration videos to improve training efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Long Video Inference</h3>
<figure class="ltx_figure ltx_align_floatright" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S3.F3.g1" src="extracted/5923385/figure/interp_net.png" width="234"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the <span class="ltx_text ltx_font_bold" id="S3.F3.2.1">frame interpolation network</span>, conditioned on key frames to produce coherent predictions.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Using the trained model, we can process up to 32 frames at <math alttext="960\times 540" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">960</mn><mo id="S3.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">540</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.2">960</cn><cn id="S3.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.3">540</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">960\times 540</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">960 × 540</annotation></semantics></math> resolution in one forward pass on a single 80GB A100 GPU.
To handle longer high-resolution videos, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>)</cite> applies a sliding window to process short segments independently and concatenate the results.
However, this would lead to temporal inconsistencies and flickering artifacts between windows.
Thus, we first predict consistent key frames, and then each window generates intermediate frames using a frame interpolation network conditioned on these key frames to align the scale and shift of the depth distributions, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F3" title="Figure 3 ‣ 3.3 Long Video Inference ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>.
Specifically, the interpolation network is finetuned from the video depth model <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">v</mi><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑣</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.SS1" title="3.1 Model Design ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
Instead of conditioning solely on the video, the first and last key frames of each window are also used, with a masking map indicating which frames are known.
The frame interpolation is formulated as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{z}_{d}=v_{\theta}\left(\phi_{t}\left(z_{d}\right),z_{c},\hat{z}_{d},m,t%
\right)," class="ltx_Math" display="block" id="S3.E6.m1.3"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3.1" xref="S3.E6.m1.3.3.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1" xref="S3.E6.m1.3.3.1.1.cmml"><msub id="S3.E6.m1.3.3.1.1.5" xref="S3.E6.m1.3.3.1.1.5.cmml"><mover accent="true" id="S3.E6.m1.3.3.1.1.5.2" xref="S3.E6.m1.3.3.1.1.5.2.cmml"><mi id="S3.E6.m1.3.3.1.1.5.2.2" xref="S3.E6.m1.3.3.1.1.5.2.2.cmml">z</mi><mo id="S3.E6.m1.3.3.1.1.5.2.1" xref="S3.E6.m1.3.3.1.1.5.2.1.cmml">~</mo></mover><mi id="S3.E6.m1.3.3.1.1.5.3" xref="S3.E6.m1.3.3.1.1.5.3.cmml">d</mi></msub><mo id="S3.E6.m1.3.3.1.1.4" xref="S3.E6.m1.3.3.1.1.4.cmml">=</mo><mrow id="S3.E6.m1.3.3.1.1.3" xref="S3.E6.m1.3.3.1.1.3.cmml"><msub id="S3.E6.m1.3.3.1.1.3.5" xref="S3.E6.m1.3.3.1.1.3.5.cmml"><mi id="S3.E6.m1.3.3.1.1.3.5.2" xref="S3.E6.m1.3.3.1.1.3.5.2.cmml">v</mi><mi id="S3.E6.m1.3.3.1.1.3.5.3" xref="S3.E6.m1.3.3.1.1.3.5.3.cmml">θ</mi></msub><mo id="S3.E6.m1.3.3.1.1.3.4" xref="S3.E6.m1.3.3.1.1.3.4.cmml">⁢</mo><mrow id="S3.E6.m1.3.3.1.1.3.3.3" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml"><mo id="S3.E6.m1.3.3.1.1.3.3.3.4" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">(</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.3.3.1.1.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.cmml">ϕ</mi><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">d</mi></msub><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.3.3.3.5" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">,</mo><msub id="S3.E6.m1.3.3.1.1.2.2.2.2" xref="S3.E6.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E6.m1.3.3.1.1.2.2.2.2.2" xref="S3.E6.m1.3.3.1.1.2.2.2.2.2.cmml">z</mi><mi id="S3.E6.m1.3.3.1.1.2.2.2.2.3" xref="S3.E6.m1.3.3.1.1.2.2.2.2.3.cmml">c</mi></msub><mo id="S3.E6.m1.3.3.1.1.3.3.3.6" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">,</mo><msub id="S3.E6.m1.3.3.1.1.3.3.3.3" xref="S3.E6.m1.3.3.1.1.3.3.3.3.cmml"><mover accent="true" id="S3.E6.m1.3.3.1.1.3.3.3.3.2" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2.cmml"><mi id="S3.E6.m1.3.3.1.1.3.3.3.3.2.2" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2.2.cmml">z</mi><mo id="S3.E6.m1.3.3.1.1.3.3.3.3.2.1" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2.1.cmml">^</mo></mover><mi id="S3.E6.m1.3.3.1.1.3.3.3.3.3" xref="S3.E6.m1.3.3.1.1.3.3.3.3.3.cmml">d</mi></msub><mo id="S3.E6.m1.3.3.1.1.3.3.3.7" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">,</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">m</mi><mo id="S3.E6.m1.3.3.1.1.3.3.3.8" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">,</mo><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">t</mi><mo id="S3.E6.m1.3.3.1.1.3.3.3.9" xref="S3.E6.m1.3.3.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.3.3.1.2" xref="S3.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.1.1.cmml" xref="S3.E6.m1.3.3.1"><eq id="S3.E6.m1.3.3.1.1.4.cmml" xref="S3.E6.m1.3.3.1.1.4"></eq><apply id="S3.E6.m1.3.3.1.1.5.cmml" xref="S3.E6.m1.3.3.1.1.5"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.5.1.cmml" xref="S3.E6.m1.3.3.1.1.5">subscript</csymbol><apply id="S3.E6.m1.3.3.1.1.5.2.cmml" xref="S3.E6.m1.3.3.1.1.5.2"><ci id="S3.E6.m1.3.3.1.1.5.2.1.cmml" xref="S3.E6.m1.3.3.1.1.5.2.1">~</ci><ci id="S3.E6.m1.3.3.1.1.5.2.2.cmml" xref="S3.E6.m1.3.3.1.1.5.2.2">𝑧</ci></apply><ci id="S3.E6.m1.3.3.1.1.5.3.cmml" xref="S3.E6.m1.3.3.1.1.5.3">𝑑</ci></apply><apply id="S3.E6.m1.3.3.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.3"><times id="S3.E6.m1.3.3.1.1.3.4.cmml" xref="S3.E6.m1.3.3.1.1.3.4"></times><apply id="S3.E6.m1.3.3.1.1.3.5.cmml" xref="S3.E6.m1.3.3.1.1.3.5"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.5.1.cmml" xref="S3.E6.m1.3.3.1.1.3.5">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.5.2.cmml" xref="S3.E6.m1.3.3.1.1.3.5.2">𝑣</ci><ci id="S3.E6.m1.3.3.1.1.3.5.3.cmml" xref="S3.E6.m1.3.3.1.1.3.5.3">𝜃</ci></apply><vector id="S3.E6.m1.3.3.1.1.3.3.4.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3"><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1"><times id="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2">italic-ϕ</ci><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑑</ci></apply></apply><apply id="S3.E6.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2.2">𝑧</ci><ci id="S3.E6.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2.3">𝑐</ci></apply><apply id="S3.E6.m1.3.3.1.1.3.3.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.3.3.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3">subscript</csymbol><apply id="S3.E6.m1.3.3.1.1.3.3.3.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2"><ci id="S3.E6.m1.3.3.1.1.3.3.3.3.2.1.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2.1">^</ci><ci id="S3.E6.m1.3.3.1.1.3.3.3.3.2.2.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3.2.2">𝑧</ci></apply><ci id="S3.E6.m1.3.3.1.1.3.3.3.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3.3">𝑑</ci></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑚</ci><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝑡</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">\tilde{z}_{d}=v_{\theta}\left(\phi_{t}\left(z_{d}\right),z_{c},\hat{z}_{d},m,t%
\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.3d">over~ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_ϕ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_m , italic_t ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.7">where <math alttext="\hat{z}_{d}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m1.1"><semantics id="S3.SS3.p1.3.m1.1a"><msub id="S3.SS3.p1.3.m1.1.1" xref="S3.SS3.p1.3.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p1.3.m1.1.1.2" xref="S3.SS3.p1.3.m1.1.1.2.cmml"><mi id="S3.SS3.p1.3.m1.1.1.2.2" xref="S3.SS3.p1.3.m1.1.1.2.2.cmml">z</mi><mo id="S3.SS3.p1.3.m1.1.1.2.1" xref="S3.SS3.p1.3.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p1.3.m1.1.1.3" xref="S3.SS3.p1.3.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m1.1b"><apply id="S3.SS3.p1.3.m1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m1.1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1">subscript</csymbol><apply id="S3.SS3.p1.3.m1.1.1.2.cmml" xref="S3.SS3.p1.3.m1.1.1.2"><ci id="S3.SS3.p1.3.m1.1.1.2.1.cmml" xref="S3.SS3.p1.3.m1.1.1.2.1">^</ci><ci id="S3.SS3.p1.3.m1.1.1.2.2.cmml" xref="S3.SS3.p1.3.m1.1.1.2.2">𝑧</ci></apply><ci id="S3.SS3.p1.3.m1.1.1.3.cmml" xref="S3.SS3.p1.3.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m1.1c">\hat{z}_{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m1.1d">over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> represents the predicted key frames, with non-key frames padded with zeros. The masking map <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m2.1"><semantics id="S3.SS3.p1.4.m2.1a"><mi id="S3.SS3.p1.4.m2.1.1" xref="S3.SS3.p1.4.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m2.1b"><ci id="S3.SS3.p1.4.m2.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m2.1d">italic_m</annotation></semantics></math> is used to indicate known key frames, which are set to <math alttext="1" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m3.1"><semantics id="S3.SS3.p1.5.m3.1a"><mn id="S3.SS3.p1.5.m3.1.1" xref="S3.SS3.p1.5.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m3.1b"><cn id="S3.SS3.p1.5.m3.1.1.cmml" type="integer" xref="S3.SS3.p1.5.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m3.1d">1</annotation></semantics></math>, while other frames are set to <math alttext="0" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m4.1"><semantics id="S3.SS3.p1.6.m4.1a"><mn id="S3.SS3.p1.6.m4.1.1" xref="S3.SS3.p1.6.m4.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m4.1b"><cn id="S3.SS3.p1.6.m4.1.1.cmml" type="integer" xref="S3.SS3.p1.6.m4.1.1">0</cn></annotation-xml></semantics></math>. The masking map is replicated four times to align with the latent feature dimensions.
To preserve the pre-trained structure and accommodate the expanded input, we duplicate input channels of <math alttext="v_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m5.1"><semantics id="S3.SS3.p1.7.m5.1a"><msub id="S3.SS3.p1.7.m5.1.1" xref="S3.SS3.p1.7.m5.1.1.cmml"><mi id="S3.SS3.p1.7.m5.1.1.2" xref="S3.SS3.p1.7.m5.1.1.2.cmml">v</mi><mi id="S3.SS3.p1.7.m5.1.1.3" xref="S3.SS3.p1.7.m5.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m5.1b"><apply id="S3.SS3.p1.7.m5.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m5.1.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m5.1.1.2.cmml" xref="S3.SS3.p1.7.m5.1.1.2">𝑣</ci><ci id="S3.SS3.p1.7.m5.1.1.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m5.1c">v_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m5.1d">italic_v start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and halve the input layer’s weight tensor as initialization.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="528" id="S3.F4.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.2.1">Qualitative comparisons</span> of monocular depth estimation methods across
different datasets. We are able to capture fine-grained details and generalize effectively on in-the-wild data.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Evaluation Metrics</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Training Datasets.</span>
In addition to the collected DA-V dataset, we follow <cite class="ltx_cite ltx_citemacro_citet">Ke et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite> by incorporating two single-frame synthetic datasets, Hypersim <cite class="ltx_cite ltx_citemacro_citep">(Roberts et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib38" title="">2021</a>)</cite> and Virtual KITTI 2 <cite class="ltx_cite ltx_citemacro_citep">(Cabon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib7" title="">2020</a>)</cite>.
Hypersim is a photorealistic synthetic dataset featuring 461 indoor scenes, from which we use the official <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">train</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">val</span> split, totaling approximately 68K samples.
Virtual KITTI 2 is a synthetic urban dataset comprising 5 scenes with variations in weather and camera configurations, contributing about 25K samples to our training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Evaluation Datasets.</span>
For monocular depth estimation, we conduct a series of experiments to evaluate our model’s performance on four widely used benchmarks.
NYUv2 <cite class="ltx_cite ltx_citemacro_citep">(Silberman et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib43" title="">2012</a>)</cite> and ScanNet <cite class="ltx_cite ltx_citemacro_citep">(Yeshwanth et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib54" title="">2023</a>)</cite> provide RGB-D data from indoor environments captured using Kinect cameras. ETH3D <cite class="ltx_cite ltx_citemacro_citep">(Schops et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib41" title="">2017</a>)</cite> features both indoor and outdoor scenes, with depth data collected by a laser scanner.
KITTI <cite class="ltx_cite ltx_citemacro_citep">(Geiger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib14" title="">2012</a>)</cite> comprises outdoor driving scenes captured by cameras and LiDAR sensors.
For video depth estimation, we sample 98 video clips from ScanNet++ <cite class="ltx_cite ltx_citemacro_citep">(Yeshwanth et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib54" title="">2023</a>)</cite>, with each clip containing 32 frames.
The overlap ratio between adjacent frames in each clip exceeds 40%, ensuring sufficient continuity for video depth estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.4"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.4.1">Evaluation Metrics.</span>
All evaluations are conducted in the zero-shot setting.
Following prior methods <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib13" title="">2024</a>)</cite>, we evaluate affine-invariant depth predictions by optimizing for scale and shift between the predicted depth and the ground truth.
The quantitative comparisons are conducted with metrics AbsRel (absolute relative error: <math alttext="\frac{1}{N}\sum_{k=0}^{N-1}\frac{\left|\hat{x}_{d}-x_{d}\right|}{x_{d}}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.2" xref="S4.SS1.p3.1.m1.1.2.cmml"><mfrac id="S4.SS1.p3.1.m1.1.2.2" xref="S4.SS1.p3.1.m1.1.2.2.cmml"><mn id="S4.SS1.p3.1.m1.1.2.2.2" xref="S4.SS1.p3.1.m1.1.2.2.2.cmml">1</mn><mi id="S4.SS1.p3.1.m1.1.2.2.3" xref="S4.SS1.p3.1.m1.1.2.2.3.cmml">N</mi></mfrac><mo id="S4.SS1.p3.1.m1.1.2.1" xref="S4.SS1.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.SS1.p3.1.m1.1.2.3" xref="S4.SS1.p3.1.m1.1.2.3.cmml"><msubsup id="S4.SS1.p3.1.m1.1.2.3.1" xref="S4.SS1.p3.1.m1.1.2.3.1.cmml"><mo id="S4.SS1.p3.1.m1.1.2.3.1.2.2" xref="S4.SS1.p3.1.m1.1.2.3.1.2.2.cmml">∑</mo><mrow id="S4.SS1.p3.1.m1.1.2.3.1.2.3" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.cmml"><mi id="S4.SS1.p3.1.m1.1.2.3.1.2.3.2" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.2.cmml">k</mi><mo id="S4.SS1.p3.1.m1.1.2.3.1.2.3.1" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.2.3.1.2.3.3" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.3.cmml">0</mn></mrow><mrow id="S4.SS1.p3.1.m1.1.2.3.1.3" xref="S4.SS1.p3.1.m1.1.2.3.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.2.3.1.3.2" xref="S4.SS1.p3.1.m1.1.2.3.1.3.2.cmml">N</mi><mo id="S4.SS1.p3.1.m1.1.2.3.1.3.1" xref="S4.SS1.p3.1.m1.1.2.3.1.3.1.cmml">−</mo><mn id="S4.SS1.p3.1.m1.1.2.3.1.3.3" xref="S4.SS1.p3.1.m1.1.2.3.1.3.3.cmml">1</mn></mrow></msubsup><mfrac id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.2.cmml"><mo id="S4.SS1.p3.1.m1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS1.p3.1.m1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS1.p3.1.m1.1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.cmml"><mover accent="true" id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.2.cmml">x</mi><mo id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.2.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.3.cmml">d</mi></msub><mo id="S4.SS1.p3.1.m1.1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.SS1.p3.1.m1.1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml">d</mi></msub></mrow><mo id="S4.SS1.p3.1.m1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.2.1.cmml">|</mo></mrow><msub id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml">x</mi><mi id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml">d</mi></msub></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.2"><times id="S4.SS1.p3.1.m1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.2.1"></times><apply id="S4.SS1.p3.1.m1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.2.2"><divide id="S4.SS1.p3.1.m1.1.2.2.1.cmml" xref="S4.SS1.p3.1.m1.1.2.2"></divide><cn id="S4.SS1.p3.1.m1.1.2.2.2.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.2.2.2">1</cn><ci id="S4.SS1.p3.1.m1.1.2.2.3.cmml" xref="S4.SS1.p3.1.m1.1.2.2.3">𝑁</ci></apply><apply id="S4.SS1.p3.1.m1.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.2.3"><apply id="S4.SS1.p3.1.m1.1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.2.3.1.1.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1">superscript</csymbol><apply id="S4.SS1.p3.1.m1.1.2.3.1.2.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.2.3.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1">subscript</csymbol><sum id="S4.SS1.p3.1.m1.1.2.3.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.2.2"></sum><apply id="S4.SS1.p3.1.m1.1.2.3.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3"><eq id="S4.SS1.p3.1.m1.1.2.3.1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.1"></eq><ci id="S4.SS1.p3.1.m1.1.2.3.1.2.3.2.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.2">𝑘</ci><cn id="S4.SS1.p3.1.m1.1.2.3.1.2.3.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.2.3.1.2.3.3">0</cn></apply></apply><apply id="S4.SS1.p3.1.m1.1.2.3.1.3.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.3"><minus id="S4.SS1.p3.1.m1.1.2.3.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.3.1"></minus><ci id="S4.SS1.p3.1.m1.1.2.3.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.2.3.1.3.2">𝑁</ci><cn id="S4.SS1.p3.1.m1.1.2.3.1.3.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.2.3.1.3.3">1</cn></apply></apply><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><divide id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1"></divide><apply id="S4.SS1.p3.1.m1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1"><abs id="S4.SS1.p3.1.m1.1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.2"></abs><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1"><minus id="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1"></minus><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2">subscript</csymbol><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2"><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.1">^</ci><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.2.2">𝑥</ci></apply><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.2.3">𝑑</ci></apply><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.2">𝑥</ci><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.3.3">𝑑</ci></apply></apply></apply><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2">𝑥</ci><ci id="S4.SS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\frac{1}{N}\sum_{k=0}^{N-1}\frac{\left|\hat{x}_{d}-x_{d}\right|}{x_{d}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT divide start_ARG | over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | end_ARG start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_N</annotation></semantics></math> denoting the number of pixels) and and <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">δ</mi><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mn id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><times id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></times><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">𝛿</ci><cn id="S4.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">italic_δ 1</annotation></semantics></math> accuracy (percentage of <math alttext="\frac{1}{N}\sum_{k=0}^{N-1}\max(\frac{\hat{x}_{d}}{x_{d}},\frac{x_{d}}{\hat{x}%
_{d}})&lt;1.25" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.3"><semantics id="S4.SS1.p3.4.m4.3a"><mrow id="S4.SS1.p3.4.m4.3.4" xref="S4.SS1.p3.4.m4.3.4.cmml"><mrow id="S4.SS1.p3.4.m4.3.4.2" xref="S4.SS1.p3.4.m4.3.4.2.cmml"><mfrac id="S4.SS1.p3.4.m4.3.4.2.2" xref="S4.SS1.p3.4.m4.3.4.2.2.cmml"><mn id="S4.SS1.p3.4.m4.3.4.2.2.2" xref="S4.SS1.p3.4.m4.3.4.2.2.2.cmml">1</mn><mi id="S4.SS1.p3.4.m4.3.4.2.2.3" xref="S4.SS1.p3.4.m4.3.4.2.2.3.cmml">N</mi></mfrac><mo id="S4.SS1.p3.4.m4.3.4.2.1" xref="S4.SS1.p3.4.m4.3.4.2.1.cmml">⁢</mo><mrow id="S4.SS1.p3.4.m4.3.4.2.3" xref="S4.SS1.p3.4.m4.3.4.2.3.cmml"><msubsup id="S4.SS1.p3.4.m4.3.4.2.3.1" xref="S4.SS1.p3.4.m4.3.4.2.3.1.cmml"><mo id="S4.SS1.p3.4.m4.3.4.2.3.1.2.2" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.2.cmml">∑</mo><mrow id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.cmml"><mi id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.2" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.2.cmml">k</mi><mo id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.1" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.1.cmml">=</mo><mn id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.3" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.3.cmml">0</mn></mrow><mrow id="S4.SS1.p3.4.m4.3.4.2.3.1.3" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.cmml"><mi id="S4.SS1.p3.4.m4.3.4.2.3.1.3.2" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.2.cmml">N</mi><mo id="S4.SS1.p3.4.m4.3.4.2.3.1.3.1" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.1.cmml">−</mo><mn id="S4.SS1.p3.4.m4.3.4.2.3.1.3.3" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.3.cmml">1</mn></mrow></msubsup><mrow id="S4.SS1.p3.4.m4.3.4.2.3.2.2" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml"><mi id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">max</mi><mo id="S4.SS1.p3.4.m4.3.4.2.3.2.2a" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml">⁡</mo><mrow id="S4.SS1.p3.4.m4.3.4.2.3.2.2.1" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml"><mo id="S4.SS1.p3.4.m4.3.4.2.3.2.2.1.1" stretchy="false" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml">(</mo><mfrac id="S4.SS1.p3.4.m4.2.2" xref="S4.SS1.p3.4.m4.2.2.cmml"><msub id="S4.SS1.p3.4.m4.2.2.2" xref="S4.SS1.p3.4.m4.2.2.2.cmml"><mover accent="true" id="S4.SS1.p3.4.m4.2.2.2.2" xref="S4.SS1.p3.4.m4.2.2.2.2.cmml"><mi id="S4.SS1.p3.4.m4.2.2.2.2.2" xref="S4.SS1.p3.4.m4.2.2.2.2.2.cmml">x</mi><mo id="S4.SS1.p3.4.m4.2.2.2.2.1" xref="S4.SS1.p3.4.m4.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.4.m4.2.2.2.3" xref="S4.SS1.p3.4.m4.2.2.2.3.cmml">d</mi></msub><msub id="S4.SS1.p3.4.m4.2.2.3" xref="S4.SS1.p3.4.m4.2.2.3.cmml"><mi id="S4.SS1.p3.4.m4.2.2.3.2" xref="S4.SS1.p3.4.m4.2.2.3.2.cmml">x</mi><mi id="S4.SS1.p3.4.m4.2.2.3.3" xref="S4.SS1.p3.4.m4.2.2.3.3.cmml">d</mi></msub></mfrac><mo id="S4.SS1.p3.4.m4.3.4.2.3.2.2.1.2" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml">,</mo><mfrac id="S4.SS1.p3.4.m4.3.3" xref="S4.SS1.p3.4.m4.3.3.cmml"><msub id="S4.SS1.p3.4.m4.3.3.2" xref="S4.SS1.p3.4.m4.3.3.2.cmml"><mi id="S4.SS1.p3.4.m4.3.3.2.2" xref="S4.SS1.p3.4.m4.3.3.2.2.cmml">x</mi><mi id="S4.SS1.p3.4.m4.3.3.2.3" xref="S4.SS1.p3.4.m4.3.3.2.3.cmml">d</mi></msub><msub id="S4.SS1.p3.4.m4.3.3.3" xref="S4.SS1.p3.4.m4.3.3.3.cmml"><mover accent="true" id="S4.SS1.p3.4.m4.3.3.3.2" xref="S4.SS1.p3.4.m4.3.3.3.2.cmml"><mi id="S4.SS1.p3.4.m4.3.3.3.2.2" xref="S4.SS1.p3.4.m4.3.3.3.2.2.cmml">x</mi><mo id="S4.SS1.p3.4.m4.3.3.3.2.1" xref="S4.SS1.p3.4.m4.3.3.3.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.4.m4.3.3.3.3" xref="S4.SS1.p3.4.m4.3.3.3.3.cmml">d</mi></msub></mfrac><mo id="S4.SS1.p3.4.m4.3.4.2.3.2.2.1.3" stretchy="false" xref="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.SS1.p3.4.m4.3.4.1" xref="S4.SS1.p3.4.m4.3.4.1.cmml">&lt;</mo><mn id="S4.SS1.p3.4.m4.3.4.3" xref="S4.SS1.p3.4.m4.3.4.3.cmml">1.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.3b"><apply id="S4.SS1.p3.4.m4.3.4.cmml" xref="S4.SS1.p3.4.m4.3.4"><lt id="S4.SS1.p3.4.m4.3.4.1.cmml" xref="S4.SS1.p3.4.m4.3.4.1"></lt><apply id="S4.SS1.p3.4.m4.3.4.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2"><times id="S4.SS1.p3.4.m4.3.4.2.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.1"></times><apply id="S4.SS1.p3.4.m4.3.4.2.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2.2"><divide id="S4.SS1.p3.4.m4.3.4.2.2.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.2"></divide><cn id="S4.SS1.p3.4.m4.3.4.2.2.2.cmml" type="integer" xref="S4.SS1.p3.4.m4.3.4.2.2.2">1</cn><ci id="S4.SS1.p3.4.m4.3.4.2.2.3.cmml" xref="S4.SS1.p3.4.m4.3.4.2.2.3">𝑁</ci></apply><apply id="S4.SS1.p3.4.m4.3.4.2.3.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3"><apply id="S4.SS1.p3.4.m4.3.4.2.3.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.3.4.2.3.1.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1">superscript</csymbol><apply id="S4.SS1.p3.4.m4.3.4.2.3.1.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.3.4.2.3.1.2.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1">subscript</csymbol><sum id="S4.SS1.p3.4.m4.3.4.2.3.1.2.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.2"></sum><apply id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3"><eq id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.1"></eq><ci id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.2">𝑘</ci><cn id="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.3.cmml" type="integer" xref="S4.SS1.p3.4.m4.3.4.2.3.1.2.3.3">0</cn></apply></apply><apply id="S4.SS1.p3.4.m4.3.4.2.3.1.3.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3"><minus id="S4.SS1.p3.4.m4.3.4.2.3.1.3.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.1"></minus><ci id="S4.SS1.p3.4.m4.3.4.2.3.1.3.2.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.2">𝑁</ci><cn id="S4.SS1.p3.4.m4.3.4.2.3.1.3.3.cmml" type="integer" xref="S4.SS1.p3.4.m4.3.4.2.3.1.3.3">1</cn></apply></apply><apply id="S4.SS1.p3.4.m4.3.4.2.3.2.1.cmml" xref="S4.SS1.p3.4.m4.3.4.2.3.2.2"><max id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"></max><apply id="S4.SS1.p3.4.m4.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2"><divide id="S4.SS1.p3.4.m4.2.2.1.cmml" xref="S4.SS1.p3.4.m4.2.2"></divide><apply id="S4.SS1.p3.4.m4.2.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.2.2.2.1.cmml" xref="S4.SS1.p3.4.m4.2.2.2">subscript</csymbol><apply id="S4.SS1.p3.4.m4.2.2.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2.2.2"><ci id="S4.SS1.p3.4.m4.2.2.2.2.1.cmml" xref="S4.SS1.p3.4.m4.2.2.2.2.1">^</ci><ci id="S4.SS1.p3.4.m4.2.2.2.2.2.cmml" xref="S4.SS1.p3.4.m4.2.2.2.2.2">𝑥</ci></apply><ci id="S4.SS1.p3.4.m4.2.2.2.3.cmml" xref="S4.SS1.p3.4.m4.2.2.2.3">𝑑</ci></apply><apply id="S4.SS1.p3.4.m4.2.2.3.cmml" xref="S4.SS1.p3.4.m4.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.2.2.3.1.cmml" xref="S4.SS1.p3.4.m4.2.2.3">subscript</csymbol><ci id="S4.SS1.p3.4.m4.2.2.3.2.cmml" xref="S4.SS1.p3.4.m4.2.2.3.2">𝑥</ci><ci id="S4.SS1.p3.4.m4.2.2.3.3.cmml" xref="S4.SS1.p3.4.m4.2.2.3.3">𝑑</ci></apply></apply><apply id="S4.SS1.p3.4.m4.3.3.cmml" xref="S4.SS1.p3.4.m4.3.3"><divide id="S4.SS1.p3.4.m4.3.3.1.cmml" xref="S4.SS1.p3.4.m4.3.3"></divide><apply id="S4.SS1.p3.4.m4.3.3.2.cmml" xref="S4.SS1.p3.4.m4.3.3.2"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.3.3.2.1.cmml" xref="S4.SS1.p3.4.m4.3.3.2">subscript</csymbol><ci id="S4.SS1.p3.4.m4.3.3.2.2.cmml" xref="S4.SS1.p3.4.m4.3.3.2.2">𝑥</ci><ci id="S4.SS1.p3.4.m4.3.3.2.3.cmml" xref="S4.SS1.p3.4.m4.3.3.2.3">𝑑</ci></apply><apply id="S4.SS1.p3.4.m4.3.3.3.cmml" xref="S4.SS1.p3.4.m4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.3.3.3.1.cmml" xref="S4.SS1.p3.4.m4.3.3.3">subscript</csymbol><apply id="S4.SS1.p3.4.m4.3.3.3.2.cmml" xref="S4.SS1.p3.4.m4.3.3.3.2"><ci id="S4.SS1.p3.4.m4.3.3.3.2.1.cmml" xref="S4.SS1.p3.4.m4.3.3.3.2.1">^</ci><ci id="S4.SS1.p3.4.m4.3.3.3.2.2.cmml" xref="S4.SS1.p3.4.m4.3.3.3.2.2">𝑥</ci></apply><ci id="S4.SS1.p3.4.m4.3.3.3.3.cmml" xref="S4.SS1.p3.4.m4.3.3.3.3">𝑑</ci></apply></apply></apply></apply></apply><cn id="S4.SS1.p3.4.m4.3.4.3.cmml" type="float" xref="S4.SS1.p3.4.m4.3.4.3">1.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.3c">\frac{1}{N}\sum_{k=0}^{N-1}\max(\frac{\hat{x}_{d}}{x_{d}},\frac{x_{d}}{\hat{x}%
_{d}})&lt;1.25</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.3d">divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT roman_max ( divide start_ARG over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG , divide start_ARG italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG start_ARG over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG ) &lt; 1.25</annotation></semantics></math>).
To assess the temporal consistency of video depth, we further introduce the temporal alignment error (TAE):</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{TAE}=\frac{1}{2(T-2)}\sum_{k=0}^{T-1}\text{AbsRel}\left(f(\hat{x}_{d}^{k%
},p^{k}),\hat{x}_{d}^{k+1}\right)+\text{AbsRel}\left(f(\hat{x}_{d}^{k+1},p_{-}%
^{k+1}),\hat{x}_{d}^{k}\right)," class="ltx_Math" display="block" id="S4.E7.m1.2"><semantics id="S4.E7.m1.2a"><mrow id="S4.E7.m1.2.2.1" xref="S4.E7.m1.2.2.1.1.cmml"><mrow id="S4.E7.m1.2.2.1.1" xref="S4.E7.m1.2.2.1.1.cmml"><mtext id="S4.E7.m1.2.2.1.1.6" xref="S4.E7.m1.2.2.1.1.6a.cmml">TAE</mtext><mo id="S4.E7.m1.2.2.1.1.5" xref="S4.E7.m1.2.2.1.1.5.cmml">=</mo><mrow id="S4.E7.m1.2.2.1.1.4" xref="S4.E7.m1.2.2.1.1.4.cmml"><mrow id="S4.E7.m1.2.2.1.1.2.2" xref="S4.E7.m1.2.2.1.1.2.2.cmml"><mfrac id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml"><mn id="S4.E7.m1.1.1.3" xref="S4.E7.m1.1.1.3.cmml">1</mn><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.cmml"><mn id="S4.E7.m1.1.1.1.3" xref="S4.E7.m1.1.1.1.3.cmml">2</mn><mo id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E7.m1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mo id="S4.E7.m1.1.1.1.1.1.2" stretchy="false" xref="S4.E7.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.2.cmml">T</mi><mo id="S4.E7.m1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S4.E7.m1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="S4.E7.m1.1.1.1.1.1.3" stretchy="false" xref="S4.E7.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S4.E7.m1.2.2.1.1.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.3.cmml">⁢</mo><mrow id="S4.E7.m1.2.2.1.1.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.cmml"><munderover id="S4.E7.m1.2.2.1.1.2.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.3.cmml"><mo id="S4.E7.m1.2.2.1.1.2.2.2.3.2.2" movablelimits="false" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.2.cmml">∑</mo><mrow id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.cmml"><mi id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.2" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.2.cmml">k</mi><mo id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.1" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.1.cmml">=</mo><mn id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.3" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.3.cmml">0</mn></mrow><mrow id="S4.E7.m1.2.2.1.1.2.2.2.3.3" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.cmml"><mi id="S4.E7.m1.2.2.1.1.2.2.2.3.3.2" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.2.cmml">T</mi><mo id="S4.E7.m1.2.2.1.1.2.2.2.3.3.1" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.1.cmml">−</mo><mn id="S4.E7.m1.2.2.1.1.2.2.2.3.3.3" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></munderover><mrow id="S4.E7.m1.2.2.1.1.2.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.cmml"><mtext id="S4.E7.m1.2.2.1.1.2.2.2.2.4" xref="S4.E7.m1.2.2.1.1.2.2.2.2.4a.cmml">AbsRel</mtext><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.2.3.cmml">⁢</mo><mrow id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml">(</mo><mrow id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.4.cmml">f</mi><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msubsup id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">x</mi><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">d</mi><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.4" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">p</mi><mi id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">k</mi></msup><mo id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.4" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml">,</mo><msubsup id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mover accent="true" id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.2.cmml">x</mi><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.1" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml">d</mi><mrow id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml"><mi id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.2" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.2.cmml">k</mi><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.1" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.1.cmml">+</mo><mn id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.3" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.5" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E7.m1.2.2.1.1.4.5" xref="S4.E7.m1.2.2.1.1.4.5.cmml">+</mo><mrow id="S4.E7.m1.2.2.1.1.4.4" xref="S4.E7.m1.2.2.1.1.4.4.cmml"><mtext id="S4.E7.m1.2.2.1.1.4.4.4" xref="S4.E7.m1.2.2.1.1.4.4.4a.cmml">AbsRel</mtext><mo id="S4.E7.m1.2.2.1.1.4.4.3" xref="S4.E7.m1.2.2.1.1.4.4.3.cmml">⁢</mo><mrow id="S4.E7.m1.2.2.1.1.4.4.2.2" xref="S4.E7.m1.2.2.1.1.4.4.2.3.cmml"><mo id="S4.E7.m1.2.2.1.1.4.4.2.2.3" xref="S4.E7.m1.2.2.1.1.4.4.2.3.cmml">(</mo><mrow id="S4.E7.m1.2.2.1.1.3.3.1.1.1" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.cmml"><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.4" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.4.cmml">f</mi><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.3.cmml">⁢</mo><mrow id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.3.cmml"><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.3" stretchy="false" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.3.cmml">(</mo><msubsup id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.2.cmml">x</mi><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.1" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.3.cmml">d</mi><mrow id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.2.cmml">k</mi><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.1" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.4" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.3.cmml">,</mo><msubsup id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.2.cmml">p</mi><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.3.cmml">−</mo><mrow id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.cmml"><mi id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.2" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.2.cmml">k</mi><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.1" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.1.cmml">+</mo><mn id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.3" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.5" stretchy="false" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.2.2.1.1.4.4.2.2.4" xref="S4.E7.m1.2.2.1.1.4.4.2.3.cmml">,</mo><msubsup id="S4.E7.m1.2.2.1.1.4.4.2.2.2" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.cmml"><mover accent="true" id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.cmml"><mi id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.2" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.2.cmml">x</mi><mo id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.1" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.3" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.3.cmml">d</mi><mi id="S4.E7.m1.2.2.1.1.4.4.2.2.2.3" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.3.cmml">k</mi></msubsup><mo id="S4.E7.m1.2.2.1.1.4.4.2.2.5" xref="S4.E7.m1.2.2.1.1.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E7.m1.2.2.1.2" xref="S4.E7.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.2b"><apply id="S4.E7.m1.2.2.1.1.cmml" xref="S4.E7.m1.2.2.1"><eq id="S4.E7.m1.2.2.1.1.5.cmml" xref="S4.E7.m1.2.2.1.1.5"></eq><ci id="S4.E7.m1.2.2.1.1.6a.cmml" xref="S4.E7.m1.2.2.1.1.6"><mtext id="S4.E7.m1.2.2.1.1.6.cmml" xref="S4.E7.m1.2.2.1.1.6">TAE</mtext></ci><apply id="S4.E7.m1.2.2.1.1.4.cmml" xref="S4.E7.m1.2.2.1.1.4"><plus id="S4.E7.m1.2.2.1.1.4.5.cmml" xref="S4.E7.m1.2.2.1.1.4.5"></plus><apply id="S4.E7.m1.2.2.1.1.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2"><times id="S4.E7.m1.2.2.1.1.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.3"></times><apply id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"><divide id="S4.E7.m1.1.1.2.cmml" xref="S4.E7.m1.1.1"></divide><cn id="S4.E7.m1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.3">1</cn><apply id="S4.E7.m1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><times id="S4.E7.m1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.2"></times><cn id="S4.E7.m1.1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.3">2</cn><apply id="S4.E7.m1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1"><minus id="S4.E7.m1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1"></minus><ci id="S4.E7.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2">𝑇</ci><cn id="S4.E7.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E7.m1.1.1.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S4.E7.m1.2.2.1.1.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2"><apply id="S4.E7.m1.2.2.1.1.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.2.2.2.3.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.2.2.2.3.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.2.2.2.3.2.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3">subscript</csymbol><sum id="S4.E7.m1.2.2.1.1.2.2.2.3.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.2"></sum><apply id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3"><eq id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.1"></eq><ci id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.2">𝑘</ci><cn id="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.3.cmml" type="integer" xref="S4.E7.m1.2.2.1.1.2.2.2.3.2.3.3">0</cn></apply></apply><apply id="S4.E7.m1.2.2.1.1.2.2.2.3.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3"><minus id="S4.E7.m1.2.2.1.1.2.2.2.3.3.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.1"></minus><ci id="S4.E7.m1.2.2.1.1.2.2.2.3.3.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.2">𝑇</ci><cn id="S4.E7.m1.2.2.1.1.2.2.2.3.3.3.cmml" type="integer" xref="S4.E7.m1.2.2.1.1.2.2.2.3.3.3">1</cn></apply></apply><apply id="S4.E7.m1.2.2.1.1.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2"><times id="S4.E7.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.3"></times><ci id="S4.E7.m1.2.2.1.1.2.2.2.2.4a.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.4"><mtext id="S4.E7.m1.2.2.1.1.2.2.2.2.4.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.4">AbsRel</mtext></ci><interval closure="open" id="S4.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2"><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1"><times id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.3"></times><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.4">𝑓</ci><interval closure="open" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑥</ci></apply><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑑</ci></apply><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑝</ci><ci id="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑘</ci></apply></interval></apply><apply id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2"><ci id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.1">^</ci><ci id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.2">𝑥</ci></apply><ci id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.2.3">𝑑</ci></apply><apply id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3"><plus id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.1.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.1"></plus><ci id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.2">𝑘</ci><cn id="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.3.cmml" type="integer" xref="S4.E7.m1.2.2.1.1.2.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></apply><apply id="S4.E7.m1.2.2.1.1.4.4.cmml" xref="S4.E7.m1.2.2.1.1.4.4"><times id="S4.E7.m1.2.2.1.1.4.4.3.cmml" xref="S4.E7.m1.2.2.1.1.4.4.3"></times><ci id="S4.E7.m1.2.2.1.1.4.4.4a.cmml" xref="S4.E7.m1.2.2.1.1.4.4.4"><mtext id="S4.E7.m1.2.2.1.1.4.4.4.cmml" xref="S4.E7.m1.2.2.1.1.4.4.4">AbsRel</mtext></ci><interval closure="open" id="S4.E7.m1.2.2.1.1.4.4.2.3.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2"><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1"><times id="S4.E7.m1.2.2.1.1.3.3.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.3"></times><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.4.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.4">𝑓</ci><interval closure="open" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2"><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2"><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.1">^</ci><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.2.2">𝑥</ci></apply><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.2.3">𝑑</ci></apply><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3"><plus id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.1"></plus><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.2">𝑘</ci><cn id="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.2">𝑝</ci><minus id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.2.3"></minus></apply><apply id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3"><plus id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.1.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.1"></plus><ci id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.2.cmml" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.2">𝑘</ci><cn id="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S4.E7.m1.2.2.1.1.3.3.1.1.1.2.2.2.3.3">1</cn></apply></apply></interval></apply><apply id="S4.E7.m1.2.2.1.1.4.4.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.4.4.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2">superscript</csymbol><apply id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2">subscript</csymbol><apply id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2"><ci id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.1">^</ci><ci id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.2.2">𝑥</ci></apply><ci id="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.2.3">𝑑</ci></apply><ci id="S4.E7.m1.2.2.1.1.4.4.2.2.2.3.cmml" xref="S4.E7.m1.2.2.1.1.4.4.2.2.2.3">𝑘</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.2c">\text{TAE}=\frac{1}{2(T-2)}\sum_{k=0}^{T-1}\text{AbsRel}\left(f(\hat{x}_{d}^{k%
},p^{k}),\hat{x}_{d}^{k+1}\right)+\text{AbsRel}\left(f(\hat{x}_{d}^{k+1},p_{-}%
^{k+1}),\hat{x}_{d}^{k}\right),</annotation><annotation encoding="application/x-llamapun" id="S4.E7.m1.2d">TAE = divide start_ARG 1 end_ARG start_ARG 2 ( italic_T - 2 ) end_ARG ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT AbsRel ( italic_f ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_p start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) , over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ) + AbsRel ( italic_f ( over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT , italic_p start_POSTSUBSCRIPT - end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ) , over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p3.11">where <math alttext="T" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m1.1"><semantics id="S4.SS1.p3.5.m1.1a"><mi id="S4.SS1.p3.5.m1.1.1" xref="S4.SS1.p3.5.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m1.1b"><ci id="S4.SS1.p3.5.m1.1.1.cmml" xref="S4.SS1.p3.5.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.5.m1.1d">italic_T</annotation></semantics></math> is the number of frames, <math alttext="f" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m2.1"><semantics id="S4.SS1.p3.6.m2.1a"><mi id="S4.SS1.p3.6.m2.1.1" xref="S4.SS1.p3.6.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m2.1b"><ci id="S4.SS1.p3.6.m2.1.1.cmml" xref="S4.SS1.p3.6.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.6.m2.1d">italic_f</annotation></semantics></math> is the projection function that uses transformation matrix <math alttext="p^{k}" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m3.1"><semantics id="S4.SS1.p3.7.m3.1a"><msup id="S4.SS1.p3.7.m3.1.1" xref="S4.SS1.p3.7.m3.1.1.cmml"><mi id="S4.SS1.p3.7.m3.1.1.2" xref="S4.SS1.p3.7.m3.1.1.2.cmml">p</mi><mi id="S4.SS1.p3.7.m3.1.1.3" xref="S4.SS1.p3.7.m3.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m3.1b"><apply id="S4.SS1.p3.7.m3.1.1.cmml" xref="S4.SS1.p3.7.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.7.m3.1.1.1.cmml" xref="S4.SS1.p3.7.m3.1.1">superscript</csymbol><ci id="S4.SS1.p3.7.m3.1.1.2.cmml" xref="S4.SS1.p3.7.m3.1.1.2">𝑝</ci><ci id="S4.SS1.p3.7.m3.1.1.3.cmml" xref="S4.SS1.p3.7.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m3.1c">p^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.7.m3.1d">italic_p start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> to map depth <math alttext="\hat{x}_{d}^{k}" class="ltx_Math" display="inline" id="S4.SS1.p3.8.m4.1"><semantics id="S4.SS1.p3.8.m4.1a"><msubsup id="S4.SS1.p3.8.m4.1.1" xref="S4.SS1.p3.8.m4.1.1.cmml"><mover accent="true" id="S4.SS1.p3.8.m4.1.1.2.2" xref="S4.SS1.p3.8.m4.1.1.2.2.cmml"><mi id="S4.SS1.p3.8.m4.1.1.2.2.2" xref="S4.SS1.p3.8.m4.1.1.2.2.2.cmml">x</mi><mo id="S4.SS1.p3.8.m4.1.1.2.2.1" xref="S4.SS1.p3.8.m4.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.8.m4.1.1.2.3" xref="S4.SS1.p3.8.m4.1.1.2.3.cmml">d</mi><mi id="S4.SS1.p3.8.m4.1.1.3" xref="S4.SS1.p3.8.m4.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m4.1b"><apply id="S4.SS1.p3.8.m4.1.1.cmml" xref="S4.SS1.p3.8.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m4.1.1.1.cmml" xref="S4.SS1.p3.8.m4.1.1">superscript</csymbol><apply id="S4.SS1.p3.8.m4.1.1.2.cmml" xref="S4.SS1.p3.8.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m4.1.1.2.1.cmml" xref="S4.SS1.p3.8.m4.1.1">subscript</csymbol><apply id="S4.SS1.p3.8.m4.1.1.2.2.cmml" xref="S4.SS1.p3.8.m4.1.1.2.2"><ci id="S4.SS1.p3.8.m4.1.1.2.2.1.cmml" xref="S4.SS1.p3.8.m4.1.1.2.2.1">^</ci><ci id="S4.SS1.p3.8.m4.1.1.2.2.2.cmml" xref="S4.SS1.p3.8.m4.1.1.2.2.2">𝑥</ci></apply><ci id="S4.SS1.p3.8.m4.1.1.2.3.cmml" xref="S4.SS1.p3.8.m4.1.1.2.3">𝑑</ci></apply><ci id="S4.SS1.p3.8.m4.1.1.3.cmml" xref="S4.SS1.p3.8.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m4.1c">\hat{x}_{d}^{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.8.m4.1d">over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> from the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.p3.9.m5.1"><semantics id="S4.SS1.p3.9.m5.1a"><mi id="S4.SS1.p3.9.m5.1.1" xref="S4.SS1.p3.9.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.9.m5.1b"><ci id="S4.SS1.p3.9.m5.1.1.cmml" xref="S4.SS1.p3.9.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.9.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.9.m5.1d">italic_k</annotation></semantics></math>-th frame to the <math alttext="(k+1)" class="ltx_Math" display="inline" id="S4.SS1.p3.10.m6.1"><semantics id="S4.SS1.p3.10.m6.1a"><mrow id="S4.SS1.p3.10.m6.1.1.1" xref="S4.SS1.p3.10.m6.1.1.1.1.cmml"><mo id="S4.SS1.p3.10.m6.1.1.1.2" stretchy="false" xref="S4.SS1.p3.10.m6.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p3.10.m6.1.1.1.1" xref="S4.SS1.p3.10.m6.1.1.1.1.cmml"><mi id="S4.SS1.p3.10.m6.1.1.1.1.2" xref="S4.SS1.p3.10.m6.1.1.1.1.2.cmml">k</mi><mo id="S4.SS1.p3.10.m6.1.1.1.1.1" xref="S4.SS1.p3.10.m6.1.1.1.1.1.cmml">+</mo><mn id="S4.SS1.p3.10.m6.1.1.1.1.3" xref="S4.SS1.p3.10.m6.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS1.p3.10.m6.1.1.1.3" stretchy="false" xref="S4.SS1.p3.10.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.10.m6.1b"><apply id="S4.SS1.p3.10.m6.1.1.1.1.cmml" xref="S4.SS1.p3.10.m6.1.1.1"><plus id="S4.SS1.p3.10.m6.1.1.1.1.1.cmml" xref="S4.SS1.p3.10.m6.1.1.1.1.1"></plus><ci id="S4.SS1.p3.10.m6.1.1.1.1.2.cmml" xref="S4.SS1.p3.10.m6.1.1.1.1.2">𝑘</ci><cn id="S4.SS1.p3.10.m6.1.1.1.1.3.cmml" type="integer" xref="S4.SS1.p3.10.m6.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.10.m6.1c">(k+1)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.10.m6.1d">( italic_k + 1 )</annotation></semantics></math>-th frame, and <math alttext="p_{-}^{k+1}" class="ltx_Math" display="inline" id="S4.SS1.p3.11.m7.1"><semantics id="S4.SS1.p3.11.m7.1a"><msubsup id="S4.SS1.p3.11.m7.1.1" xref="S4.SS1.p3.11.m7.1.1.cmml"><mi id="S4.SS1.p3.11.m7.1.1.2.2" xref="S4.SS1.p3.11.m7.1.1.2.2.cmml">p</mi><mo id="S4.SS1.p3.11.m7.1.1.2.3" xref="S4.SS1.p3.11.m7.1.1.2.3.cmml">−</mo><mrow id="S4.SS1.p3.11.m7.1.1.3" xref="S4.SS1.p3.11.m7.1.1.3.cmml"><mi id="S4.SS1.p3.11.m7.1.1.3.2" xref="S4.SS1.p3.11.m7.1.1.3.2.cmml">k</mi><mo id="S4.SS1.p3.11.m7.1.1.3.1" xref="S4.SS1.p3.11.m7.1.1.3.1.cmml">+</mo><mn id="S4.SS1.p3.11.m7.1.1.3.3" xref="S4.SS1.p3.11.m7.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.11.m7.1b"><apply id="S4.SS1.p3.11.m7.1.1.cmml" xref="S4.SS1.p3.11.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.11.m7.1.1.1.cmml" xref="S4.SS1.p3.11.m7.1.1">superscript</csymbol><apply id="S4.SS1.p3.11.m7.1.1.2.cmml" xref="S4.SS1.p3.11.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.11.m7.1.1.2.1.cmml" xref="S4.SS1.p3.11.m7.1.1">subscript</csymbol><ci id="S4.SS1.p3.11.m7.1.1.2.2.cmml" xref="S4.SS1.p3.11.m7.1.1.2.2">𝑝</ci><minus id="S4.SS1.p3.11.m7.1.1.2.3.cmml" xref="S4.SS1.p3.11.m7.1.1.2.3"></minus></apply><apply id="S4.SS1.p3.11.m7.1.1.3.cmml" xref="S4.SS1.p3.11.m7.1.1.3"><plus id="S4.SS1.p3.11.m7.1.1.3.1.cmml" xref="S4.SS1.p3.11.m7.1.1.3.1"></plus><ci id="S4.SS1.p3.11.m7.1.1.3.2.cmml" xref="S4.SS1.p3.11.m7.1.1.3.2">𝑘</ci><cn id="S4.SS1.p3.11.m7.1.1.3.3.cmml" type="integer" xref="S4.SS1.p3.11.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.11.m7.1c">p_{-}^{k+1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.11.m7.1d">italic_p start_POSTSUBSCRIPT - end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is the inverse matrix for projection in the reverse direction.
The transformation matrix consists of both intrinsic and extrinsic camera parameters, which can be obtained from the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.18">Our implementation is based on SVD <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite>, using the diffusers library <cite class="ltx_cite ltx_citemacro_citep">(von Platen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib48" title="">2022</a>)</cite>.
We employ the AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib32" title="">2019</a>)</cite> with a learning rate of <math alttext="6.4\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">6.4</mn><mo id="S4.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p1.1.m1.1.1.3.3a" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p1.1.m1.1.1.3.3.2" xref="S4.SS2.p1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn id="S4.SS2.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1.2">6.4</cn><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"><minus id="S4.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"></minus><cn id="S4.SS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">6.4\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">6.4 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>. The model is trained at various resolutions: <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn id="S4.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.2">512</cn><cn id="S4.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">512 × 512</annotation></semantics></math>, <math alttext="480\times 640" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">480</mn><mo id="S4.SS2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn id="S4.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.2">480</cn><cn id="S4.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">480\times 640</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">480 × 640</annotation></semantics></math>, <math alttext="707\times 707" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">707</mn><mo id="S4.SS2.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">707</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><times id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></times><cn id="S4.SS2.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.2">707</cn><cn id="S4.SS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.3">707</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">707\times 707</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">707 × 707</annotation></semantics></math>, <math alttext="352\times 1216" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mn id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">352</mn><mo id="S4.SS2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">1216</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><times id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></times><cn id="S4.SS2.p1.5.m5.1.1.2.cmml" type="integer" xref="S4.SS2.p1.5.m5.1.1.2">352</cn><cn id="S4.SS2.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS2.p1.5.m5.1.1.3">1216</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">352\times 1216</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">352 × 1216</annotation></semantics></math>, and <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mn id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">1024</mn><mo id="S4.SS2.p1.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.6.m6.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><times id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></times><cn id="S4.SS2.p1.6.m6.1.1.2.cmml" type="integer" xref="S4.SS2.p1.6.m6.1.1.2">1024</cn><cn id="S4.SS2.p1.6.m6.1.1.3.cmml" type="integer" xref="S4.SS2.p1.6.m6.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">1024 × 1024</annotation></semantics></math>, with corresponding batch sizes of <math alttext="384" class="ltx_Math" display="inline" id="S4.SS2.p1.7.m7.1"><semantics id="S4.SS2.p1.7.m7.1a"><mn id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">384</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><cn id="S4.SS2.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS2.p1.7.m7.1.1">384</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">384</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.7.m7.1d">384</annotation></semantics></math>, <math alttext="256" class="ltx_Math" display="inline" id="S4.SS2.p1.8.m8.1"><semantics id="S4.SS2.p1.8.m8.1a"><mn id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><cn id="S4.SS2.p1.8.m8.1.1.cmml" type="integer" xref="S4.SS2.p1.8.m8.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">256</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.8.m8.1d">256</annotation></semantics></math>, <math alttext="192" class="ltx_Math" display="inline" id="S4.SS2.p1.9.m9.1"><semantics id="S4.SS2.p1.9.m9.1a"><mn id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml">192</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><cn id="S4.SS2.p1.9.m9.1.1.cmml" type="integer" xref="S4.SS2.p1.9.m9.1.1">192</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.9.m9.1d">192</annotation></semantics></math>, <math alttext="128" class="ltx_Math" display="inline" id="S4.SS2.p1.10.m10.1"><semantics id="S4.SS2.p1.10.m10.1a"><mn id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><cn id="S4.SS2.p1.10.m10.1.1.cmml" type="integer" xref="S4.SS2.p1.10.m10.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.10.m10.1d">128</annotation></semantics></math>, and <math alttext="64" class="ltx_Math" display="inline" id="S4.SS2.p1.11.m11.1"><semantics id="S4.SS2.p1.11.m11.1a"><mn id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><cn id="S4.SS2.p1.11.m11.1.1.cmml" type="integer" xref="S4.SS2.p1.11.m11.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.11.m11.1d">64</annotation></semantics></math>.
The video length is sampled from <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.p1.12.m12.1"><semantics id="S4.SS2.p1.12.m12.1a"><mn id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><cn id="S4.SS2.p1.12.m12.1.1.cmml" type="integer" xref="S4.SS2.p1.12.m12.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.12.m12.1d">1</annotation></semantics></math> to <math alttext="6" class="ltx_Math" display="inline" id="S4.SS2.p1.13.m13.1"><semantics id="S4.SS2.p1.13.m13.1a"><mn id="S4.SS2.p1.13.m13.1.1" xref="S4.SS2.p1.13.m13.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m13.1b"><cn id="S4.SS2.p1.13.m13.1.1.cmml" type="integer" xref="S4.SS2.p1.13.m13.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m13.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.13.m13.1d">6</annotation></semantics></math>, with the batch size adjusting correspondingly to meet GPU memory requirements.
Experiments are conducted on 32 NVIDIA A100 GPUs for <math alttext="20" class="ltx_Math" display="inline" id="S4.SS2.p1.14.m14.1"><semantics id="S4.SS2.p1.14.m14.1a"><mn id="S4.SS2.p1.14.m14.1.1" xref="S4.SS2.p1.14.m14.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.14.m14.1b"><cn id="S4.SS2.p1.14.m14.1.1.cmml" type="integer" xref="S4.SS2.p1.14.m14.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.14.m14.1c">20</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.14.m14.1d">20</annotation></semantics></math> epochs, with a total training time of approximately <math alttext="1" class="ltx_Math" display="inline" id="S4.SS2.p1.15.m15.1"><semantics id="S4.SS2.p1.15.m15.1a"><mn id="S4.SS2.p1.15.m15.1.1" xref="S4.SS2.p1.15.m15.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.15.m15.1b"><cn id="S4.SS2.p1.15.m15.1.1.cmml" type="integer" xref="S4.SS2.p1.15.m15.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.15.m15.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.15.m15.1d">1</annotation></semantics></math> day.
For training efficiency, we utilize Fully Sharded Data Parallel (FSDP) with ZeRO Stage 2, gradient checkpointing, and mixed-precision training.
During inference, we set the number of denoising steps to <math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.p1.16.m16.1"><semantics id="S4.SS2.p1.16.m16.1a"><mn id="S4.SS2.p1.16.m16.1.1" xref="S4.SS2.p1.16.m16.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.16.m16.1b"><cn id="S4.SS2.p1.16.m16.1.1.cmml" type="integer" xref="S4.SS2.p1.16.m16.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.16.m16.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.16.m16.1d">3</annotation></semantics></math> and the ensemble size to <math alttext="20" class="ltx_Math" display="inline" id="S4.SS2.p1.17.m17.1"><semantics id="S4.SS2.p1.17.m17.1a"><mn id="S4.SS2.p1.17.m17.1.1" xref="S4.SS2.p1.17.m17.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.17.m17.1b"><cn id="S4.SS2.p1.17.m17.1.1.cmml" type="integer" xref="S4.SS2.p1.17.m17.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.17.m17.1c">20</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.17.m17.1d">20</annotation></semantics></math> for benchmark comparison, following <cite class="ltx_cite ltx_citemacro_citet">Ke et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite>, to ensure optimal performance.
In contrast, the ablation studies do not utilize the ensemble strategy to focus on the isolated impact of individual components.
The runtime evaluation is performed on a single NVIDIA A100 GPU with a resolution of <math alttext="480\times 640" class="ltx_Math" display="inline" id="S4.SS2.p1.18.m18.1"><semantics id="S4.SS2.p1.18.m18.1a"><mrow id="S4.SS2.p1.18.m18.1.1" xref="S4.SS2.p1.18.m18.1.1.cmml"><mn id="S4.SS2.p1.18.m18.1.1.2" xref="S4.SS2.p1.18.m18.1.1.2.cmml">480</mn><mo id="S4.SS2.p1.18.m18.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p1.18.m18.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.18.m18.1.1.3" xref="S4.SS2.p1.18.m18.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.18.m18.1b"><apply id="S4.SS2.p1.18.m18.1.1.cmml" xref="S4.SS2.p1.18.m18.1.1"><times id="S4.SS2.p1.18.m18.1.1.1.cmml" xref="S4.SS2.p1.18.m18.1.1.1"></times><cn id="S4.SS2.p1.18.m18.1.1.2.cmml" type="integer" xref="S4.SS2.p1.18.m18.1.1.2">480</cn><cn id="S4.SS2.p1.18.m18.1.1.3.cmml" type="integer" xref="S4.SS2.p1.18.m18.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.18.m18.1c">480\times 640</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.18.m18.1d">480 × 640</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.10.1">Quantitative comparisons</span> with state-of-the-art depth estimation methods using single-frame input across four zero-shot affine-invariant depth benchmarks.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.8" style="width:433.6pt;height:169.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-152.0pt,59.4pt) scale(0.587806030124394,0.587806030124394) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.8.8">
<tr class="ltx_tr" id="S4.T2.8.8.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.8.8.9.1" rowspan="2"><span class="ltx_text" id="S4.T2.8.8.9.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.8.8.9.2"># Training Data</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.8.8.9.3">NYUv2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.8.8.9.4">KITTI</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.8.8.9.5">ETH3D</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.8.8.9.6">ScanNet</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.8">
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.9">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.8.10">Synthetic</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1.1">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2.2"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mrow id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.2.2" xref="S4.T2.2.2.2.2.m1.1.1.2.2.cmml">δ</mi><mo id="S4.T2.2.2.2.2.m1.1.1.2.1" xref="S4.T2.2.2.2.2.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.T2.2.2.2.2.m1.1.1.2.3" xref="S4.T2.2.2.2.2.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.T2.2.2.2.2.m1.1.1.1" stretchy="false" xref="S4.T2.2.2.2.2.m1.1.1.1.cmml">↑</mo><mi id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><ci id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.1">↑</ci><apply id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2"><times id="S4.T2.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.1"></times><ci id="S4.T2.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2.2">𝛿</ci><cn id="S4.T2.2.2.2.2.m1.1.1.2.3.cmml" type="integer" xref="S4.T2.2.2.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">italic_δ 1 ↑</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.4"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.4.m1.1a"><mrow id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml"><mrow id="S4.T2.4.4.4.4.m1.1.1.2" xref="S4.T2.4.4.4.4.m1.1.1.2.cmml"><mi id="S4.T2.4.4.4.4.m1.1.1.2.2" xref="S4.T2.4.4.4.4.m1.1.1.2.2.cmml">δ</mi><mo id="S4.T2.4.4.4.4.m1.1.1.2.1" xref="S4.T2.4.4.4.4.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.T2.4.4.4.4.m1.1.1.2.3" xref="S4.T2.4.4.4.4.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.T2.4.4.4.4.m1.1.1.1" stretchy="false" xref="S4.T2.4.4.4.4.m1.1.1.1.cmml">↑</mo><mi id="S4.T2.4.4.4.4.m1.1.1.3" xref="S4.T2.4.4.4.4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><apply id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1"><ci id="S4.T2.4.4.4.4.m1.1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1.1">↑</ci><apply id="S4.T2.4.4.4.4.m1.1.1.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2"><times id="S4.T2.4.4.4.4.m1.1.1.2.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.1"></times><ci id="S4.T2.4.4.4.4.m1.1.1.2.2.cmml" xref="S4.T2.4.4.4.4.m1.1.1.2.2">𝛿</ci><cn id="S4.T2.4.4.4.4.m1.1.1.2.3.cmml" type="integer" xref="S4.T2.4.4.4.4.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.T2.4.4.4.4.m1.1.1.3.cmml" xref="S4.T2.4.4.4.4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.4.m1.1d">italic_δ 1 ↑</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.5.5.5.5">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.5.5.5.5.m1.1"><semantics id="S4.T2.5.5.5.5.m1.1a"><mo id="S4.T2.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T2.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.5.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.6"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.T2.6.6.6.6.m1.1"><semantics id="S4.T2.6.6.6.6.m1.1a"><mrow id="S4.T2.6.6.6.6.m1.1.1" xref="S4.T2.6.6.6.6.m1.1.1.cmml"><mrow id="S4.T2.6.6.6.6.m1.1.1.2" xref="S4.T2.6.6.6.6.m1.1.1.2.cmml"><mi id="S4.T2.6.6.6.6.m1.1.1.2.2" xref="S4.T2.6.6.6.6.m1.1.1.2.2.cmml">δ</mi><mo id="S4.T2.6.6.6.6.m1.1.1.2.1" xref="S4.T2.6.6.6.6.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.T2.6.6.6.6.m1.1.1.2.3" xref="S4.T2.6.6.6.6.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.T2.6.6.6.6.m1.1.1.1" stretchy="false" xref="S4.T2.6.6.6.6.m1.1.1.1.cmml">↑</mo><mi id="S4.T2.6.6.6.6.m1.1.1.3" xref="S4.T2.6.6.6.6.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.m1.1b"><apply id="S4.T2.6.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1"><ci id="S4.T2.6.6.6.6.m1.1.1.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1.1">↑</ci><apply id="S4.T2.6.6.6.6.m1.1.1.2.cmml" xref="S4.T2.6.6.6.6.m1.1.1.2"><times id="S4.T2.6.6.6.6.m1.1.1.2.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1.2.1"></times><ci id="S4.T2.6.6.6.6.m1.1.1.2.2.cmml" xref="S4.T2.6.6.6.6.m1.1.1.2.2">𝛿</ci><cn id="S4.T2.6.6.6.6.m1.1.1.2.3.cmml" type="integer" xref="S4.T2.6.6.6.6.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.T2.6.6.6.6.m1.1.1.3.cmml" xref="S4.T2.6.6.6.6.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.6.m1.1d">italic_δ 1 ↑</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.7.7.7">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.7.7.7.7.m1.1"><semantics id="S4.T2.7.7.7.7.m1.1a"><mo id="S4.T2.7.7.7.7.m1.1.1" stretchy="false" xref="S4.T2.7.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.7.m1.1b"><ci id="S4.T2.7.7.7.7.m1.1.1.cmml" xref="S4.T2.7.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.7.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.7.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.8"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.T2.8.8.8.8.m1.1"><semantics id="S4.T2.8.8.8.8.m1.1a"><mrow id="S4.T2.8.8.8.8.m1.1.1" xref="S4.T2.8.8.8.8.m1.1.1.cmml"><mrow id="S4.T2.8.8.8.8.m1.1.1.2" xref="S4.T2.8.8.8.8.m1.1.1.2.cmml"><mi id="S4.T2.8.8.8.8.m1.1.1.2.2" xref="S4.T2.8.8.8.8.m1.1.1.2.2.cmml">δ</mi><mo id="S4.T2.8.8.8.8.m1.1.1.2.1" xref="S4.T2.8.8.8.8.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.T2.8.8.8.8.m1.1.1.2.3" xref="S4.T2.8.8.8.8.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.T2.8.8.8.8.m1.1.1.1" stretchy="false" xref="S4.T2.8.8.8.8.m1.1.1.1.cmml">↑</mo><mi id="S4.T2.8.8.8.8.m1.1.1.3" xref="S4.T2.8.8.8.8.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.8.m1.1b"><apply id="S4.T2.8.8.8.8.m1.1.1.cmml" xref="S4.T2.8.8.8.8.m1.1.1"><ci id="S4.T2.8.8.8.8.m1.1.1.1.cmml" xref="S4.T2.8.8.8.8.m1.1.1.1">↑</ci><apply id="S4.T2.8.8.8.8.m1.1.1.2.cmml" xref="S4.T2.8.8.8.8.m1.1.1.2"><times id="S4.T2.8.8.8.8.m1.1.1.2.1.cmml" xref="S4.T2.8.8.8.8.m1.1.1.2.1"></times><ci id="S4.T2.8.8.8.8.m1.1.1.2.2.cmml" xref="S4.T2.8.8.8.8.m1.1.1.2.2">𝛿</ci><cn id="S4.T2.8.8.8.8.m1.1.1.2.3.cmml" type="integer" xref="S4.T2.8.8.8.8.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.T2.8.8.8.8.m1.1.1.3.cmml" xref="S4.T2.8.8.8.8.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.8.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.8.m1.1d">italic_δ 1 ↑</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.10">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="11" id="S4.T2.8.8.10.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.8.8.10.1.1">Discriminative Model:</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.11.1">DiverseDepth <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib55" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.2">320K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.11.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.4">11.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.5">87.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.6">19.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.7">70.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.8">22.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.9">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.10">10.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.11.11">88.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.12.1">MiDaS <cite class="ltx_cite ltx_citemacro_citep">(Lasinger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib28" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.2">2M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.12.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.4">9.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.5">91.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.6">18.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.7">71.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.8">19.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.9">88.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.10">9.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.12.11">90.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.13.1">LeReS <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib56" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.2">354K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.13.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.4">9.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.5">91.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.6">14.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.7">78.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.8">17.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.9">77.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.10">9.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.13.11">91.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.14.1">Omnidata <cite class="ltx_cite ltx_citemacro_citep">(Eftekhar et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib11" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.2">12.1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.14.3">59K</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.4">7.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.5">94.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.6">14.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.7">83.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.8">16.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.9">77.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.10">7.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.14.11">93.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.15.1">HDN <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib58" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.2">300K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.15.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.4">6.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.5">94.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.6">11.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.7">86.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.8">12.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.9">83.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.10">8.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.15.11">93.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.16">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.16.1">DPT <cite class="ltx_cite ltx_citemacro_citep">(Ranftl et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib37" title="">2021</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.2">1.4M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.16.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.4">9.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.5">91.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.6">11.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.7">88.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.8">11.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.9">92.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.10">8.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.16.11">93.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.17.1">Metric3D <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib57" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.2">8M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.17.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.4">5.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.5">96.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.6">5.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.7">97.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.8">6.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.9">96.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.10">7.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.17.11">94.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.18">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.18.1">Depth Anything <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib51" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.2">63.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.18.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.4">4.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.5">98.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.6">8.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.7">94.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.8">6.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.9">98.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.10">4.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.18.11">98.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.19">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="11" id="S4.T2.8.8.19.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.8.8.19.1.1">Generative Model:</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.20.1">Marigold <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.20.3">74K</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.4">5.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.5">96.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.6">9.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.7">91.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.8">6.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.9">96.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.10">6.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.20.11">95.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.21.1">DepthFM <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib15" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.21.3">63K</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.4">6.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.5">95.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.6">8.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.7">93.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.9">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.10">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.21.11">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.8.8.22.1">GeoWizard <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib13" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.8.8.22.3">0.3M</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.4">5.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.5">96.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.6">9.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.7">92.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.8">6.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.9">96.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.10">6.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.22.11">95.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.23">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T2.8.8.23.1"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.1.1">Depth Any Video (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.8.8.23.3"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.3.1">6M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.4"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.4.1">5.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.5"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.5.1">97.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.6"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.6.1">7.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.7"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.7.1">95.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.8"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.8.1">4.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.9"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.9.1">97.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.10"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.10.1">5.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.23.11"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.23.11.1">96.6</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S4.SS2.7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.SS2.3.3" style="width:188.6pt;">
<span class="ltx_ERROR undefined" id="S4.SS2.3.3.4">\captionof</span>
<p class="ltx_p ltx_align_center" id="S4.SS2.3.3.3">table<span class="ltx_text ltx_font_bold" id="S4.SS2.3.3.3.4">Temporal consistency and spatial accuracy comparisons</span> on ScanNet++.


<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.3.3.3.3" style="width:433.6pt;height:93.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.5pt,-1.8pt) scale(1.04090281425636,1.04090281425636) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.SS2.3.3.3.3.3">
<span class="ltx_tr" id="S4.SS2.3.3.3.3.3.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.SS2.3.3.3.3.3.3.4" style="padding-left:5.5pt;padding-right:5.5pt;">Method</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.1.1.1.1.1.1.1" style="padding-left:5.5pt;padding-right:5.5pt;">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.1.1.1.1.1.1.1.m1.1"><semantics id="S4.SS2.1.1.1.1.1.1.1.m1.1a"><mo id="S4.SS2.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.SS2.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.1.1.1.1.1.1.1.m1.1b"><ci id="S4.SS2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS2.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.1.1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.2.2.2.2.2.2.2" style="padding-left:5.5pt;padding-right:5.5pt;"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.SS2.2.2.2.2.2.2.2.m1.1"><semantics id="S4.SS2.2.2.2.2.2.2.2.m1.1a"><mrow id="S4.SS2.2.2.2.2.2.2.2.m1.1.1" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.cmml"><mrow id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.cmml"><mi id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.2" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.2.cmml">δ</mi><mo id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.1" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.3" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.1" stretchy="false" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.1.cmml">↑</mo><mi id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.3" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.2.2.2.2.2.2.2.m1.1b"><apply id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1"><ci id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.1">↑</ci><apply id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2"><times id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.1"></times><ci id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.2">𝛿</ci><cn id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.3.cmml" type="integer" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.SS2.2.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S4.SS2.2.2.2.2.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.2.2.2.2.2.2.2.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.2.2.2.2.2.2.2.m1.1d">italic_δ 1 ↑</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.3.3.3.3.3.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">TAE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.3.3.3.3.3.3.3.m1.1"><semantics id="S4.SS2.3.3.3.3.3.3.3.m1.1a"><mo id="S4.SS2.3.3.3.3.3.3.3.m1.1.1" stretchy="false" xref="S4.SS2.3.3.3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.3.3.3.3.3.3.3.m1.1b"><ci id="S4.SS2.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.SS2.3.3.3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.3.3.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.3.3.3.3.3.3.3.m1.1d">↓</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.SS2.3.3.3.3.3.4">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.SS2.3.3.3.3.3.4.1" style="padding-left:5.5pt;padding-right:5.5pt;">NVDS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.3.3.3.3.3.4.2" style="padding-left:5.5pt;padding-right:5.5pt;">22.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.3.3.3.3.3.4.3" style="padding-left:5.5pt;padding-right:5.5pt;">61.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.3.3.3.3.3.4.4" style="padding-left:5.5pt;padding-right:5.5pt;">3.7</span></span>
<span class="ltx_tr" id="S4.SS2.3.3.3.3.3.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.3.3.3.3.3.5.1" style="padding-left:5.5pt;padding-right:5.5pt;">ChronoDepth <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib42" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.5.2" style="padding-left:5.5pt;padding-right:5.5pt;">10.4</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.5.3" style="padding-left:5.5pt;padding-right:5.5pt;">90.7</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.5.4" style="padding-left:5.5pt;padding-right:5.5pt;">2.3</span></span>
<span class="ltx_tr" id="S4.SS2.3.3.3.3.3.6">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.3.3.3.3.3.6.1" style="padding-left:5.5pt;padding-right:5.5pt;">DepthCrafter <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib22" title="">2024b</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.6.2" style="padding-left:5.5pt;padding-right:5.5pt;">11.5</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.6.3" style="padding-left:5.5pt;padding-right:5.5pt;">88.1</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.3.3.3.3.3.6.4" style="padding-left:5.5pt;padding-right:5.5pt;">2.2</span></span>
<span class="ltx_tr" id="S4.SS2.3.3.3.3.3.7">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.SS2.3.3.3.3.3.7.1" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.3.3.3.3.3.7.1.1">Depth Any Video (Ours)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.3.3.3.3.3.7.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.3.3.3.3.3.7.2.1">9.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.3.3.3.3.3.7.3" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.3.3.3.3.3.7.3.1">93.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.3.3.3.3.3.7.4" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.3.3.3.3.3.7.4.1">2.1</span></span></span>
</span>
</span></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.SS2.7.7" style="width:233.3pt;">
<span class="ltx_ERROR undefined" id="S4.SS2.7.7.5">\captionof</span>
<p class="ltx_p ltx_align_center" id="S4.SS2.7.7.4">table<span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.5">Performance and inference efficiency comparisons</span> on the ScanNet dataset.


<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.7.7.4.4" style="width:433.6pt;height:79.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,5.1pt) scale(0.887357867322486,0.887357867322486) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.SS2.7.7.4.4.4">
<span class="ltx_tr" id="S4.SS2.7.7.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.SS2.7.7.4.4.4.4.5">Method</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.4.4.1.1.1.1.1">Step <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.4.4.1.1.1.1.1.m1.1"><semantics id="S4.SS2.4.4.1.1.1.1.1.m1.1a"><mo id="S4.SS2.4.4.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.SS2.4.4.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.4.4.1.1.1.1.1.m1.1b"><ci id="S4.SS2.4.4.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS2.4.4.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.4.4.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.4.4.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.5.5.2.2.2.2.2"># Param. <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.5.5.2.2.2.2.2.m1.1"><semantics id="S4.SS2.5.5.2.2.2.2.2.m1.1a"><mo id="S4.SS2.5.5.2.2.2.2.2.m1.1.1" stretchy="false" xref="S4.SS2.5.5.2.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.5.5.2.2.2.2.2.m1.1b"><ci id="S4.SS2.5.5.2.2.2.2.2.m1.1.1.cmml" xref="S4.SS2.5.5.2.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.5.5.2.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.5.5.2.2.2.2.2.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.6.6.3.3.3.3.3">Runtime <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS2.6.6.3.3.3.3.3.m1.1"><semantics id="S4.SS2.6.6.3.3.3.3.3.m1.1a"><mo id="S4.SS2.6.6.3.3.3.3.3.m1.1.1" stretchy="false" xref="S4.SS2.6.6.3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.6.6.3.3.3.3.3.m1.1b"><ci id="S4.SS2.6.6.3.3.3.3.3.m1.1.1.cmml" xref="S4.SS2.6.6.3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.6.6.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.6.6.3.3.3.3.3.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS2.7.7.4.4.4.4.4"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.SS2.7.7.4.4.4.4.4.m1.1"><semantics id="S4.SS2.7.7.4.4.4.4.4.m1.1a"><mrow id="S4.SS2.7.7.4.4.4.4.4.m1.1.1" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.cmml"><mrow id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.cmml"><mi id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.2" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.2.cmml">δ</mi><mo id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.1" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.3" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.1" stretchy="false" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.1.cmml">↑</mo><mi id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.3" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.7.7.4.4.4.4.4.m1.1b"><apply id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1"><ci id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.1.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.1">↑</ci><apply id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2"><times id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.1.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.1"></times><ci id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.2.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.2">𝛿</ci><cn id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.3.cmml" type="integer" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.SS2.7.7.4.4.4.4.4.m1.1.1.3.cmml" xref="S4.SS2.7.7.4.4.4.4.4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.7.7.4.4.4.4.4.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.7.7.4.4.4.4.4.m1.1d">italic_δ 1 ↑</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.SS2.7.7.4.4.4.5">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.SS2.7.7.4.4.4.5.1">Marigold <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.7.7.4.4.4.5.2">50</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.7.7.4.4.4.5.3">865.9M</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.7.7.4.4.4.5.4">2.06s</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.7.7.4.4.4.5.5">94.5</span></span>
<span class="ltx_tr" id="S4.SS2.7.7.4.4.4.6">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.7.7.4.4.4.6.1">ChronoDepth <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib42" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.6.2">10</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.6.3">1524.6M</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.6.4">1.04s</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.6.5">93.4</span></span>
<span class="ltx_tr" id="S4.SS2.7.7.4.4.4.7">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.SS2.7.7.4.4.4.7.1">DepthCrafter <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib22" title="">2024b</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.7.2">25</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.7.3">2156.7M</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.7.4">4.80s</span>
<span class="ltx_td ltx_align_center" id="S4.SS2.7.7.4.4.4.7.5">93.8</span></span>
<span class="ltx_tr" id="S4.SS2.7.7.4.4.4.8">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.SS2.7.7.4.4.4.8.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.4.4.8.1.1">Depth Any Video (Ours)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.7.7.4.4.4.8.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.4.4.8.2.1">3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.7.7.4.4.4.8.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.4.4.8.3.1">1422.8M</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.7.7.4.4.4.8.4"><span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.4.4.8.4.1">0.37s</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS2.7.7.4.4.4.8.5"><span class="ltx_text ltx_font_bold" id="S4.SS2.7.7.4.4.4.8.5.1">96.1</span></span></span>
</span>
</span></span></p>
</div>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Zero-shot Depth Estimation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our model demonstrates exceptional zero-shot generalization in depth estimation across both indoor and outdoor datasets, as well as single-frame and multi-frame datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.5.1">Quantitative Comparisons.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T2" title="Table 2 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a> presents our model’s performance in comparison to state-of-the-art depth estimation models using single-frame inputs.
Our model significantly surpasses all previous generative models across various datasets and achieves results that are comparable to, and in some cases better than, those of top-performing discriminative models.
For example, compared to GeoWizard <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib13" title="">2024</a>)</cite>, our model shows improvements of 0.4 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">δ</mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝛿</ci><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_δ 1</annotation></semantics></math> and 0.1 in AbsRel on the NYUv2 dataset, 3.0 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">δ</mi><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">⁢</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝛿</ci><cn id="S4.SS3.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS3.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_δ 1</annotation></semantics></math> and 2.4 in AbsRel on KITTI, 1.8 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">δ</mi><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">⁢</mo><mn id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><times id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></times><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">𝛿</ci><cn id="S4.SS3.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS3.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">italic_δ 1</annotation></semantics></math> and 1.7 in AbsRel on ETH3D, and 1.3 in <math alttext="\delta_{1}" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><msub id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mi id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">δ</mi><mn id="S4.SS3.p2.4.m4.1.1.3" xref="S4.SS3.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">𝛿</ci><cn id="S4.SS3.p2.4.m4.1.1.3.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">\delta_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">italic_δ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and 0.8 in AbsRel on the ScanNet dataset.
When compared to Depth Anything <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib51" title="">2024a</a>)</cite>, we achieve gains of 0.5 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mi id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml">δ</mi><mo id="S4.SS3.p2.5.m5.1.1.1" xref="S4.SS3.p2.5.m5.1.1.1.cmml">⁢</mo><mn id="S4.SS3.p2.5.m5.1.1.3" xref="S4.SS3.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><times id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1.1"></times><ci id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2">𝛿</ci><cn id="S4.SS3.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">italic_δ 1</annotation></semantics></math> and 0.7 in AbsRel on KITTI, along with a 1.5 improvement in the AbsRel metric on the ETH3D dataset.
The impressive results are primarily attributed to the large-scale synthetic data we collected.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2</span></a> presents a comprehensive comparison of our model against previous video depth models.
All generative models process multi-frame inputs in a single forward pass.
Notably, our model demonstrates improved temporal consistency and spatial accuracy on the ScanNet++ dataset, highlighting its effectiveness in video depth estimation.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">4.2</span></a> presents detailed comparisons with previous generative methods without ensemble techniques.
Our model has fewer parameters than ChronoDepth <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib42" title="">2024</a>)</cite> because we utilize a parameter-free RoPE instead of learnable absolute positional embeddings.
It also reduces complexity compared to DepthCrafter <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib22" title="">2024b</a>)</cite> by removing the clip embedding condition and classifier-free guidance.
Additionally, we achieve lower inference time and fewer denoising steps while attaining better spatial accuracy on the ScanNet dataset compared to Marigold, ChronoDepth, and DepthCrafter.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S4.F5.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.2.1">Qualitative comparisons</span> of depth estimation models on in-the-wild videos. Red boxes show changes in color or depth over time at vertical red lines in videos. Best viewed by zooming in.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S4.T3.8.1">Ablation study</span> of each component. All variants are trained for <math alttext="10" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mn id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><cn id="S4.T3.2.m1.1.1.cmml" type="integer" xref="S4.T3.2.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">10</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">10</annotation></semantics></math> epochs to ensure training efficiency. <span class="ltx_text ltx_font_italic" id="S4.T3.9.2">Memory Util.</span> refers to the minimum GPU memory utilization across all GPUs, while <span class="ltx_text ltx_font_italic" id="S4.T3.10.3">Average Metric</span> represents the average accuracy across four datasets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.4" style="width:433.6pt;height:82.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.7pt,21.5pt) scale(0.657940545231983,0.657940545231983) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.4.2">
<tr class="ltx_tr" id="S4.T3.4.2.3">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.1" style="padding-left:9.0pt;padding-right:9.0pt;">Generative</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.2" style="padding-left:9.0pt;padding-right:9.0pt;">Conditional</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.3" style="padding-left:9.0pt;padding-right:9.0pt;">Synthetic</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.2.3.4" style="padding-left:9.0pt;padding-right:9.0pt;">Mixed-duration</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.5" style="padding-left:9.0pt;padding-right:9.0pt;">Runtime</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.6" style="padding-left:9.0pt;padding-right:9.0pt;">Training Time</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.2.3.7" style="padding-left:9.0pt;padding-right:9.0pt;">Memory Util.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.4.2.3.8" style="padding-left:9.0pt;padding-right:9.0pt;">Average Metric</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.2">
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.3" style="padding-left:9.0pt;padding-right:9.0pt;">Visual Prior</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.4" style="padding-left:9.0pt;padding-right:9.0pt;">Flow Matching</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.5" style="padding-left:9.0pt;padding-right:9.0pt;">Synthetic Data</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.2.2.6" style="padding-left:9.0pt;padding-right:9.0pt;">Training</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.7" style="padding-left:9.0pt;padding-right:9.0pt;">(s)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.8" style="padding-left:9.0pt;padding-right:9.0pt;">(hours)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.9" style="padding-left:9.0pt;padding-right:9.0pt;">(%)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.1.1.1" style="padding-left:9.0pt;padding-right:9.0pt;">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.1.1.1.m1.1"><semantics id="S4.T3.3.1.1.1.m1.1a"><mo id="S4.T3.3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.1.m1.1b"><ci id="S4.T3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.2.2" style="padding-left:9.0pt;padding-right:9.0pt;"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.2.2.2.m1.1"><semantics id="S4.T3.4.2.2.2.m1.1a"><mrow id="S4.T3.4.2.2.2.m1.1.1" xref="S4.T3.4.2.2.2.m1.1.1.cmml"><mrow id="S4.T3.4.2.2.2.m1.1.1.2" xref="S4.T3.4.2.2.2.m1.1.1.2.cmml"><mi id="S4.T3.4.2.2.2.m1.1.1.2.2" xref="S4.T3.4.2.2.2.m1.1.1.2.2.cmml">δ</mi><mo id="S4.T3.4.2.2.2.m1.1.1.2.1" xref="S4.T3.4.2.2.2.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.T3.4.2.2.2.m1.1.1.2.3" xref="S4.T3.4.2.2.2.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.T3.4.2.2.2.m1.1.1.1" stretchy="false" xref="S4.T3.4.2.2.2.m1.1.1.1.cmml">↑</mo><mi id="S4.T3.4.2.2.2.m1.1.1.3" xref="S4.T3.4.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.2.m1.1b"><apply id="S4.T3.4.2.2.2.m1.1.1.cmml" xref="S4.T3.4.2.2.2.m1.1.1"><ci id="S4.T3.4.2.2.2.m1.1.1.1.cmml" xref="S4.T3.4.2.2.2.m1.1.1.1">↑</ci><apply id="S4.T3.4.2.2.2.m1.1.1.2.cmml" xref="S4.T3.4.2.2.2.m1.1.1.2"><times id="S4.T3.4.2.2.2.m1.1.1.2.1.cmml" xref="S4.T3.4.2.2.2.m1.1.1.2.1"></times><ci id="S4.T3.4.2.2.2.m1.1.1.2.2.cmml" xref="S4.T3.4.2.2.2.m1.1.1.2.2">𝛿</ci><cn id="S4.T3.4.2.2.2.m1.1.1.2.3.cmml" type="integer" xref="S4.T3.4.2.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.T3.4.2.2.2.m1.1.1.3.cmml" xref="S4.T3.4.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.2.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.2.2.2.m1.1d">italic_δ 1 ↑</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.1" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.2" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.3" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.4.4" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.5" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.6" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.7" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.8" style="padding-left:9.0pt;padding-right:9.0pt;">21.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.4.9" style="padding-left:9.0pt;padding-right:9.0pt;">65.1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.5">
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.1" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.2" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.3" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.2.5.4" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.5" style="padding-left:9.0pt;padding-right:9.0pt;">2.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.6" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.7" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.8" style="padding-left:9.0pt;padding-right:9.0pt;">7.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.5.9" style="padding-left:9.0pt;padding-right:9.0pt;">93.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.6">
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.1" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.2" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.3" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.2.6.4" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.5" style="padding-left:9.0pt;padding-right:9.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.6.5.1">0.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.6" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.7" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.8" style="padding-left:9.0pt;padding-right:9.0pt;">6.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.6.9" style="padding-left:9.0pt;padding-right:9.0pt;">94.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.7">
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.1" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.2" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.3" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.2.7.4" style="padding-left:9.0pt;padding-right:9.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.5" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.6" style="padding-left:9.0pt;padding-right:9.0pt;">16</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.7" style="padding-left:9.0pt;padding-right:9.0pt;">23</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.8" style="padding-left:9.0pt;padding-right:9.0pt;">6.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.2.7.9" style="padding-left:9.0pt;padding-right:9.0pt;">95.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.2.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.1" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.2" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.3" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.2.8.4" style="padding-left:9.0pt;padding-right:9.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.5" style="padding-left:9.0pt;padding-right:9.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.6" style="padding-left:9.0pt;padding-right:9.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.8.6.1">12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.7" style="padding-left:9.0pt;padding-right:9.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.8.7.1">63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.8" style="padding-left:9.0pt;padding-right:9.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.8.8.1">6.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.2.8.9" style="padding-left:9.0pt;padding-right:9.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.8.9.1">95.8</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Qualitative Comparisons.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F4" title="Figure 4 ‣ 3.3 Long Video Inference ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a> presents qualitative monocular depth estimation results across different datasets. It highlights the ability of our model to capture fine-grained details compared to Depth Anything V2 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib52" title="">2024b</a>)</cite>, such as the cup in the NYUv2 dataset and the ladder in the ETH3D dataset.
Moreover, our model handles objects with similar colors more effectively.
For example, on the KITTI dataset, the person’s head blends with the background, which Depth Anything V2 fails to predict.
Compared to generative methods like Marigold and ChronoDepth, our model generalizes well to in-the-wild data, offering a better distinction between the sky and foreground.
The diversity of DA-V significantly contributes to this, enabling our model to generalize effectively across various environments, particularly in complex, real-world scenarios.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F5" title="Figure 5 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a> further demonstrates qualitative results of depth estimation on open-world videos, covering a wide range of scenarios such as dust and sand, animals, architecture, and human motion presented in both generated and real-world videos.
Following <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>)</cite>, we visualize the changes in estimated depth values over time at the vertical red lines by slicing along the time axis to better capture temporal consistency.
Marigold exhibits zigzag artifacts on a per-frame basis, while ChronoDepth displays similar issues at a per-window level. DepthCrafter, with its large overlap between windows and interpolation of overlap space, achieves smoother transitions across windows but still struggles with window-wise flickering. In contrast, our method ensures global consistency by predicting key frames and interpolating intermediate frames, significantly reducing flicker artifacts between windows.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S4.F6.sf1.g1" src="extracted/5923385/figure/denoise_step.png" width="198"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Impact of denoising step</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="131" id="S4.F6.sf2.g1" src="extracted/5923385/figure/ensemble_size.png" width="198"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Impact of ensemble size</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="127" id="S4.F6.sf3.g1" src="extracted/5923385/figure/training_epoch.png" width="198"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Impact of training epoch</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S4.F6.2.1">Ablation study</span> of hyper-parameters on depth estimation performance. Average accuracy across four datasets is reported to provide a comprehensive evaluation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we evaluate the effectiveness of each component in Depth Any Video. For training efficiency, unless otherwise specified, the model is trained for only <math alttext="10" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn id="S4.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS4.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">10</annotation></semantics></math> epochs during ablation studies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Generative Visual Prior.</span>
We investigate the impact of prior visual knowledge from the stable video diffusion model, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T3" title="Table 3 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>.
The first two rows clearly demonstrate that incorporating this prior significantly boosts the model’s overall performance.
Additionally, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F6" title="Figure 6 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>(c) illustrates that this prior provides a strong initialization, leading to fast training convergence and enabling the model to achieve impressive results with as few as five epochs. This suggests that the generative visual prior not only enhances model performance but also improves training efficiency.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.F7.3" style="width:212.5pt;">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.F7.2" style="width:212.5pt;">
<span class="ltx_ERROR undefined" id="S4.F7.2.3">\captionof</span>
<p class="ltx_p ltx_align_center" id="S4.F7.2.2">table<span class="ltx_text ltx_font_bold" id="S4.F7.2.2.3">Ablation study</span> of the reconstruction quality of different variational autoencoders. We categorize the VAEs into 2D and 3D based on the presence of temporal feature interactions.


<span class="ltx_inline-block ltx_transformed_outer" id="S4.F7.2.2.2" style="width:433.6pt;height:93.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(8.3pt,-1.8pt) scale(1.0399320952562,1.0399320952562) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.F7.2.2.2.2">
<span class="ltx_tr" id="S4.F7.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.F7.2.2.2.2.2.3" style="padding-left:9.0pt;padding-right:9.0pt;">Method</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.F7.2.2.2.2.2.4" style="padding-left:9.0pt;padding-right:9.0pt;">Type</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.F7.1.1.1.1.1.1" style="padding-left:9.0pt;padding-right:9.0pt;">AbsRel <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.F7.1.1.1.1.1.1.m1.1"><semantics id="S4.F7.1.1.1.1.1.1.m1.1a"><mo id="S4.F7.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.F7.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.F7.1.1.1.1.1.1.m1.1b"><ci id="S4.F7.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F7.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.1.1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F7.1.1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.F7.2.2.2.2.2.2" style="padding-left:9.0pt;padding-right:9.0pt;"><math alttext="\delta 1\uparrow" class="ltx_Math" display="inline" id="S4.F7.2.2.2.2.2.2.m1.1"><semantics id="S4.F7.2.2.2.2.2.2.m1.1a"><mrow id="S4.F7.2.2.2.2.2.2.m1.1.1" xref="S4.F7.2.2.2.2.2.2.m1.1.1.cmml"><mrow id="S4.F7.2.2.2.2.2.2.m1.1.1.2" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.cmml"><mi id="S4.F7.2.2.2.2.2.2.m1.1.1.2.2" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.2.cmml">δ</mi><mo id="S4.F7.2.2.2.2.2.2.m1.1.1.2.1" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.1.cmml">⁢</mo><mn id="S4.F7.2.2.2.2.2.2.m1.1.1.2.3" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.F7.2.2.2.2.2.2.m1.1.1.1" stretchy="false" xref="S4.F7.2.2.2.2.2.2.m1.1.1.1.cmml">↑</mo><mi id="S4.F7.2.2.2.2.2.2.m1.1.1.3" xref="S4.F7.2.2.2.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.2.2.2.2.2.2.m1.1b"><apply id="S4.F7.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1"><ci id="S4.F7.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1.1">↑</ci><apply id="S4.F7.2.2.2.2.2.2.m1.1.1.2.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2"><times id="S4.F7.2.2.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.1"></times><ci id="S4.F7.2.2.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.2">𝛿</ci><cn id="S4.F7.2.2.2.2.2.2.m1.1.1.2.3.cmml" type="integer" xref="S4.F7.2.2.2.2.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.F7.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S4.F7.2.2.2.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.2.2.2.2.2.m1.1c">\delta 1\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F7.2.2.2.2.2.2.m1.1d">italic_δ 1 ↑</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.F7.2.2.2.2.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.F7.2.2.2.2.3.1" style="padding-left:9.0pt;padding-right:9.0pt;">SD2 <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib39" title="">2022</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.3.2" style="padding-left:9.0pt;padding-right:9.0pt;">2D</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.3.3" style="padding-left:9.0pt;padding-right:9.0pt;">1.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.3.4" style="padding-left:9.0pt;padding-right:9.0pt;">99.0</span></span>
<span class="ltx_tr" id="S4.F7.2.2.2.2.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S4.F7.2.2.2.2.4.1" style="padding-left:9.0pt;padding-right:9.0pt;">SD3 <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib12" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S4.F7.2.2.2.2.4.2" style="padding-left:9.0pt;padding-right:9.0pt;">2D</span>
<span class="ltx_td ltx_align_center" id="S4.F7.2.2.2.2.4.3" style="padding-left:9.0pt;padding-right:9.0pt;">0.6</span>
<span class="ltx_td ltx_align_center" id="S4.F7.2.2.2.2.4.4" style="padding-left:9.0pt;padding-right:9.0pt;">99.7</span></span>
<span class="ltx_tr" id="S4.F7.2.2.2.2.5">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.F7.2.2.2.2.5.1" style="padding-left:9.0pt;padding-right:9.0pt;">CogVideoX <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib53" title="">2024c</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.5.2" style="padding-left:9.0pt;padding-right:9.0pt;">3D</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.5.3" style="padding-left:9.0pt;padding-right:9.0pt;">2.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.F7.2.2.2.2.5.4" style="padding-left:9.0pt;padding-right:9.0pt;">98.6</span></span>
<span class="ltx_tr" id="S4.F7.2.2.2.2.6">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.F7.2.2.2.2.6.1" style="padding-left:9.0pt;padding-right:9.0pt;">SVD <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.F7.2.2.2.2.6.2" style="padding-left:9.0pt;padding-right:9.0pt;">3D</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.F7.2.2.2.2.6.3" style="padding-left:9.0pt;padding-right:9.0pt;">1.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.F7.2.2.2.2.6.4" style="padding-left:9.0pt;padding-right:9.0pt;">98.1</span></span>
</span>
</span></span></p>
</div>
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="376" id="S4.F7.3.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S4.F7.3.5.1">Visualization</span> of reconstruction quality.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.2.1">Conditional Flow Matching.</span>
The second and third rows in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T3" title="Table 3 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a> indicate that flow matching not only reduces inference time, achieving a 6.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><times id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">×</annotation></semantics></math> acceleration compared to the original EDM scheduler in SVD, but also results in improvements of 0.6 in AbsRel and 1.1 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">δ</mi><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">⁢</mo><mn id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><times id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1"></times><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">𝛿</ci><cn id="S4.SS4.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS4.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">italic_δ 1</annotation></semantics></math>. The faster inference is primarily attributed to the ability to achieve strong performance with fewer denoising steps, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F6" title="Figure 6 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>(a).
It demonstrates that even a single denoising step can yield strong results, with the sweet spot identified at three steps for optimal performance.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F6" title="Figure 6 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">6</span></a>(b), we further evaluate the effectiveness of ensembling multiple predictions, as varying noise initializations produce minor variations in outputs. The results show consistent performance gains as the number of predictions increases, with improvements becoming less pronounced after five predictions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Synthetic Data.</span>
In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T3" title="Table 3 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>, the third and fourth rows show that our collected synthetic data brings gains of 0.7 in <math alttext="\delta 1" class="ltx_Math" display="inline" id="S4.SS4.p4.1.m1.1"><semantics id="S4.SS4.p4.1.m1.1a"><mrow id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml"><mi id="S4.SS4.p4.1.m1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.2.cmml">δ</mi><mo id="S4.SS4.p4.1.m1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.cmml">⁢</mo><mn id="S4.SS4.p4.1.m1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><apply id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1"><times id="S4.SS4.p4.1.m1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1"></times><ci id="S4.SS4.p4.1.m1.1.1.2.cmml" xref="S4.SS4.p4.1.m1.1.1.2">𝛿</ci><cn id="S4.SS4.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">\delta 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.1.m1.1d">italic_δ 1</annotation></semantics></math> and 0.4 in AbsRel. The accuracy improvement in outdoor scenes is particularly significant, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T2" title="Table 2 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>, likely due to the diverse range of outdoor environments in our dataset.
Additionally, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S3.F4" title="Figure 4 ‣ 3.3 Long Video Inference ‣ 3 Generative Video Depth Model ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F5" title="Figure 5 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrate that models trained on our synthetic data generalize well to in-the-wild scenarios, showcasing both the realism and effectiveness of our synthetic data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p5.1.1">Mixed-duration Training.</span>
The fourth and fifth entries in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.T3" title="Table 3 ‣ 4.3 Zero-shot Depth Estimation ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a> show that the mixed-duration training strategy improves training efficiency while simultaneously enhancing spatial accuracy.
It can save 33% of training time and increase GPU utilization by 40% because different batch sizes can be applied to video sequences of varying lengths, thus optimizing training efficiency.
The accuracy improvement is attributed to the increased proportion of individual frames during training, achieved by randomly dropping out frames, which enhances the single-frame performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.1.1">VAE Variants.</span>
Since our depth is generated in the latent space, the quality of the VAE directly impacts the upper bound of the final result. Therefore, we provide a detailed comparison of the depth reconstruction results across different VAEs in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F7.3" title="Figure 7 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a>.
We find that the 2D VAE surpasses the 3D VAE in reconstruction quality, particularly with the VAE from SD3 <cite class="ltx_cite ltx_citemacro_citep">(Esser et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib12" title="">2024</a>)</cite>. This indicates there is still potential to further enhance our model’s performance.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#S4.F7.3" title="Figure 7 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ Depth Any Video with Scalable Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a>, we present visual comparisons of the reconstructions produced by different 3D VAEs.
Specifically, (a) shows the input ground truth depth, (b) displays the reconstruction results from <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib53" title="">2024c</a>)</cite>, which incorporates temporal compression, and (c) shows the results from <cite class="ltx_cite ltx_citemacro_citet">Blattmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite>.
Although the VAE with temporal compression can reduce the computational complexity of the latent model, it struggles to handle fast motion effectively, as indicated by the red box in (b).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Monocular Depth Estimation.</span>
Existing models for monocular depth estimation can be roughly divided into two categories: discriminative and generative. Discriminative models are trained end-to-end to predict depth from images. For example, MiDaS <cite class="ltx_cite ltx_citemacro_citep">(Lasinger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib28" title="">2019</a>)</cite> focuses on relative depth estimation by factoring out scale, enabling robust training on mixed datasets. Depth Anything <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib51" title="">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib52" title="">b</a>)</cite> builds on this concept, leveraging both labeled and unlabeled images to further enhance generalization. ZoeDepth <cite class="ltx_cite ltx_citemacro_citep">(Bhat et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib2" title="">2023</a>)</cite> and Metric3D <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib57" title="">2023</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib21" title="">2024a</a>)</cite> aim to directly estimate metric depth.
Generative models <cite class="ltx_cite ltx_citemacro_citep">(Saxena et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib40" title="">2023</a>)</cite>, such as Marigold <cite class="ltx_cite ltx_citemacro_citep">(Ke et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib27" title="">2024</a>)</cite> and GeoWizard <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib13" title="">2024</a>)</cite>, leverage powerful priors learned from large-scale real-world data, allowing them to generate depth estimates in a zero-shot manner, even on unseen datasets.
Our work falls into the second category, but focuses on video depth estimation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Video Depth Estimation.</span>
Unlike single-image depth estimation, video depth estimation requires maintaining temporal consistency between frames.
To eliminate flickering effects between consecutive frames, some works <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib33" title="">2020</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib9" title="">2019</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib61" title="">2021</a>)</cite> use an optimization procedure to overfit each video during inference. Other approaches <cite class="ltx_cite ltx_citemacro_citep">(Guizilini et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib16" title="">2022</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib59" title="">2019</a>; Teed &amp; Deng, <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib47" title="">2020</a>)</cite> directly predict depth sequences from videos. For instance, NVDS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib49" title="">2023</a>)</cite> proposes a refinement network to optimize temporal consistency from off-the-shelf depth predictors.
Some concurrent works <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib22" title="">2024b</a>; Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib42" title="">2024</a>)</cite> have focused on leveraging video diffusion models to produce coherent predictions. However, they often face challenges due to a lack of sufficiently high-quality and realistic depth data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Video Generation.</span>
Diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib17" title="">2020</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib45" title="">2021</a>)</cite> have achieved high-fidelity image generation from text descriptions, benefiting from large-scale aligned image-text datasets. Building on this success, VDM <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib19" title="">2022b</a>)</cite> first introduces unconditional video generation in pixel space. Imagen Video <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib18" title="">2022a</a>)</cite> and Make-a-Video <cite class="ltx_cite ltx_citemacro_citep">(Singer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib44" title="">2023</a>)</cite> are cascade models designed for text-to-video generation. Align Your Latent <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib4" title="">2023b</a>)</cite> and SVD <cite class="ltx_cite ltx_citemacro_citep">(Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib3" title="">2023a</a>)</cite> extend <cite class="ltx_cite ltx_citemacro_citet">Rombach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10815v1#bib.bib39" title="">2022</a>)</cite> by modeling videos in the latent space of an autoencoder.
Our model builds upon the generative visual prior of SVD, which is trained on diverse real video data, to maintain robust generalization in real-world scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We present Depth Any Video, a novel approach for versatile image and video depth estimation, powered by generative video diffusion models. Leveraging diverse and high-quality depth data collected from diverse synthetic environments, our model could generate temporally consistent depth sequences with fine-grained details across a broad spectrum of unseen scenarios. Equipped with a mixed-duration training strategy and frame interpolation, it generalizes effectively to videos of various lengths and resolutions. Compared to previous generative depth estimation models, our approach sets a new state-of-the-art in performance while significantly enhancing efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Limitations.</span>
There are still certain issues in our model, such as difficulties in estimating depth for mirror-like reflections on water surfaces and challenges with extremely long videos. Future work will focus on collecting data for these challenging scenarios and improving model efficiency.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alahari et al. (2013)</span>
<span class="ltx_bibblock">
Karteek Alahari, Guillaume Seguin, Josef Sivic, and Ivan Laptev.

</span>
<span class="ltx_bibblock">Pose estimation and segmentation of people in 3d movies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  2112–2119, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhat et al. (2023)</span>
<span class="ltx_bibblock">
Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller.

</span>
<span class="ltx_bibblock">Zoedepth: Zero-shot transfer by combining relative and metric depth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv: Computing Research Repository</em>, abs/2302.12288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et al. (2023a)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach.

</span>
<span class="ltx_bibblock">Stable video diffusion: Scaling latent video diffusion models to large datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv: Computing Research Repository</em>, abs/2311.15127, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et al. (2023b)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.

</span>
<span class="ltx_bibblock">Align your latents: High-resolution video synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  22563–22575, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borghi et al. (2017)</span>
<span class="ltx_bibblock">
Guido Borghi, Marco Venturelli, Roberto Vezzani, and Rita Cucchiara.

</span>
<span class="ltx_bibblock">Poseidon: Face-from-depth for driver pose estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  5494–5503, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Butler et al. (2012)</span>
<span class="ltx_bibblock">
Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black.

</span>
<span class="ltx_bibblock">A naturalistic open source movie for optical flow evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">European Conference on Computer Vision</em>, pp.  611–625, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cabon et al. (2020)</span>
<span class="ltx_bibblock">
Yohann Cabon, Naila Murray, and Martin Humenberger.

</span>
<span class="ltx_bibblock">Virtual KITTI 2.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv: Computing Research Repository</em>, abs/2001.10773, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv: Computing Research Repository</em>, abs/2306.15595, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu.

</span>
<span class="ltx_bibblock">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  7063–7072, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Longlora: Efficient fine-tuning of long-context large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eftekhar et al. (2021)</span>
<span class="ltx_bibblock">
Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir.

</span>
<span class="ltx_bibblock">Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  10786–10796, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et al. (2024)</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2024)</span>
<span class="ltx_bibblock">
Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long.

</span>
<span class="ltx_bibblock">Geowizard: Unleashing the diffusion priors for 3d geometry estimation from a single image.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv: Computing Research Repository</em>, abs/2403.12013, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiger et al. (2012)</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Are we ready for autonomous driving? the kitti vision benchmark suite.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  3354–3361, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gui et al. (2024)</span>
<span class="ltx_bibblock">
Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Björn Ommer.

</span>
<span class="ltx_bibblock">Depthfm: Fast monocular depth estimation with flow matching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv: Computing Research Repository</em>, abs/2403.13788, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guizilini et al. (2022)</span>
<span class="ltx_bibblock">
Vitor Guizilini, Rares Ambrus, Dian Chen, Sergey Zakharov, and Adrien Gaidon.

</span>
<span class="ltx_bibblock">Multi-frame self-supervised depth with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  160–170, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2022a)</span>
<span class="ltx_bibblock">
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans.

</span>
<span class="ltx_bibblock">Imagen video: High definition video generation with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv: Computing Research Repository</em>, abs/2210.02303, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2022b)</span>
<span class="ltx_bibblock">
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.

</span>
<span class="ltx_bibblock">Video diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pp.  8633–8646, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holynski &amp; Kopf (2018)</span>
<span class="ltx_bibblock">
Aleksander Holynski and Johannes Kopf.

</span>
<span class="ltx_bibblock">Fast depth densification for occlusion-aware augmented reality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM Transactions on Graphics</em>, 37(6):1–11, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024a)</span>
<span class="ltx_bibblock">
Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen.

</span>
<span class="ltx_bibblock">Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv: Computing Research Repository</em>, abs/2404.15506, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024b)</span>
<span class="ltx_bibblock">
Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan.

</span>
<span class="ltx_bibblock">Depthcrafter: Generating consistent long depth sequences for open-world videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv: Computing Research Repository</em>, abs/2409.02095, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2018)</span>
<span class="ltx_bibblock">
Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang.

</span>
<span class="ltx_bibblock">Deepmvs: Learning multi-view stereopsis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  2821–2830, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jing et al. (2024)</span>
<span class="ltx_bibblock">
Junpeng Jing, Ye Mao, and Krystian Mikolajczyk.

</span>
<span class="ltx_bibblock">Match-stereo-videos: Bidirectional alignment for consistent dynamic stereo matching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv: Computing Research Repository</em>, abs/2403.10755, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karaev et al. (2023)</span>
<span class="ltx_bibblock">
Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht.

</span>
<span class="ltx_bibblock">Dynamicstereo: Consistent dynamic depth from stereo videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  13229–13239, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et al. (2022)</span>
<span class="ltx_bibblock">
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.

</span>
<span class="ltx_bibblock">Elucidating the design space of diffusion-based generative models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pp.  26565–26577, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. (2024)</span>
<span class="ltx_bibblock">
Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler.

</span>
<span class="ltx_bibblock">Repurposing diffusion-based image generators for monocular depth estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  9492–9502, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lasinger et al. (2019)</span>
<span class="ltx_bibblock">
Katrin Lasinger, René Ranftl, Konrad Schindler, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv: Computing Research Repository</em>, abs/1907.01341, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai.

</span>
<span class="ltx_bibblock">Matrixcity: A large-scale city dataset for city-scale neural rendering and beyond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  3182–3192. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipman et al. (2023)</span>
<span class="ltx_bibblock">
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu, Wenhu Chen, and Bo Zheng.

</span>
<span class="ltx_bibblock">E^2-llm: Efficient and extreme length extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv: Computing Research Repository</em>, abs/2401.06951, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2020)</span>
<span class="ltx_bibblock">
Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf.

</span>
<span class="ltx_bibblock">Consistent video depth estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ACM Transactions on Graphics</em>, 39(4):71–1, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Sora, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/sora/" title="">https://openai.com/index/sora/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2024)</span>
<span class="ltx_bibblock">
Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Controlnext: Powerful and efficient control for image and video generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv: Computing Research Repository</em>, abs/2408.06070, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Conference on Machine Learning</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranftl et al. (2021)</span>
<span class="ltx_bibblock">
René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Vision transformers for dense prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  12179–12188, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. (2021)</span>
<span class="ltx_bibblock">
Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Ángel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind.

</span>
<span class="ltx_bibblock">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  10892–10902, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxena et al. (2023)</span>
<span class="ltx_bibblock">
Saurabh Saxena, Junhwa Hur, Charles Herrmann, Deqing Sun, and David J. Fleet.

</span>
<span class="ltx_bibblock">Zero-shot metric depth with a field-of-view conditioned diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv: Computing Research Repository</em>, abs/2312.13252, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schops et al. (2017)</span>
<span class="ltx_bibblock">
Thomas Schops, Johannes L. Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger.

</span>
<span class="ltx_bibblock">A multi-view stereo benchmark with high-resolution images and multi-camera videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  3260–3269, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao.

</span>
<span class="ltx_bibblock">Learning temporally consistent video depth from video diffusion priors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv: Computing Research Repository</em>, abs/2406.01493, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silberman et al. (2012)</span>
<span class="ltx_bibblock">
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from RGBD images.

</span>
<span class="ltx_bibblock">In Andrew W. Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">European Conference on Computer Vision</em>, pp.  746–760, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer et al. (2023)</span>
<span class="ltx_bibblock">
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.

</span>
<span class="ltx_bibblock">Make-a-video: Text-to-video generation without text-video data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2021)</span>
<span class="ltx_bibblock">
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.

</span>
<span class="ltx_bibblock">Score-based generative modeling through stochastic differential equations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv: Computing Research Repository</em>, abs/2104.09864, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teed &amp; Deng (2020)</span>
<span class="ltx_bibblock">
Zachary Teed and Jia Deng.

</span>
<span class="ltx_bibblock">Deepv2d: Video to depth with differentiable structure from motion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Platen et al. (2022)</span>
<span class="ltx_bibblock">
Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Diffusers: State-of-the-art diffusion models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/diffusers" title="">https://github.com/huggingface/diffusers</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin.

</span>
<span class="ltx_bibblock">Neural video depth stabilizer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  9432–9442, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Unifying flow, stereo and depth estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024a)</span>
<span class="ltx_bibblock">
Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">Depth anything: Unleashing the power of large-scale unlabeled data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  10371–10381, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024b)</span>
<span class="ltx_bibblock">
Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">Depth anything V2.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv: Computing Research Repository</em>, abs/2406.09414, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024c)</span>
<span class="ltx_bibblock">
Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al.

</span>
<span class="ltx_bibblock">Cogvideox: Text-to-video diffusion models with an expert transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv: Computing Research Repository</em>, abs/2408.06072, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeshwanth et al. (2023)</span>
<span class="ltx_bibblock">
Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai.

</span>
<span class="ltx_bibblock">Scannet++: A high-fidelity dataset of 3d indoor scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  12–22, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2020)</span>
<span class="ltx_bibblock">
Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin.

</span>
<span class="ltx_bibblock">Diversedepth: Affine-invariant depth prediction using diverse data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv: Computing Research Repository</em>, abs/2002.00569, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2021)</span>
<span class="ltx_bibblock">
Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Learning to recover 3d scene shape from a single image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  204–213, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2023)</span>
<span class="ltx_bibblock">
Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Metric3d: Towards zero-shot metric 3d prediction from a single image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  9043–9053, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen.

</span>
<span class="ltx_bibblock">Hierarchical normalization for robust monocular depth estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pp.  14128–14139, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Haokui Zhang, Ying Li, Yuanzhouhan Cao, Yu Liu, Chunhua Shen, and Youliang Yan.

</span>
<span class="ltx_bibblock">Exploiting temporal consistency for real-time video depth estimation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  1725–1734, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian.

</span>
<span class="ltx_bibblock">Controlvideo: Training-free controllable text-to-video generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Zhoutong Zhang, Forrester Cole, Richard Tucker, William T Freeman, and Tali Dekel.

</span>
<span class="ltx_bibblock">Consistent depth of moving objects in video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">ACM Transactions on Graphics</em>, 40(4):1–12, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct 13 16:47:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
