<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Making Large Language Models into World Models with Precondition and Effect Knowledge</title>
<!--Generated on Wed Oct  2 23:30:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12278v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S1" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S2" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S2.SS1" title="In 2 Related Work ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>World Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S2.SS2" title="In 2 Related Work ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Precondition and Effect</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>World Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS1" title="In 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Precondition/Effect Inference Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS2" title="In 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Semantic State Matching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS3" title="In 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Applying the World Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS1" title="In 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation of Global-Local Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS2" title="In 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation of Precondition/Effect Inference Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS3" title="In 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation of World Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S5" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis of Search Space</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S6" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S7" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#A1" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#A2" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Prompts for Global-Local Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#A3" title="In Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Prompts for Semantic State Matching</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Making Large Language Models into World Models with Precondition and Effect Knowledge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaige Xie  Ian Yang  John Gunerli  Mark Riedl 
<br class="ltx_break"/>School of Interactive Computing 
<br class="ltx_break"/>Georgia Institute of Technology 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{kaigexie, iyang30, hakancangunerli, riedl}@gatech.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">World models, which encapsulate the dynamics of how actions affect environments, are foundational to the functioning of intelligent agents.
In this work, we explore the potential of Large Language Models (LLMs) to operate as world models.
Although LLMs are not inherently designed to model real-world dynamics, we show that they can be induced to perform two critical world model functions: determining the applicability of an action based on a given world state, and predicting the resulting world state upon action execution.
This is achieved by fine-tuning two separate LLMs—one for precondition prediction and another for effect prediction—while leveraging synthetic data generation techniques.
Through human-participant studies, we validate that the precondition and effect knowledge generated by our models aligns with human understanding of world dynamics.
We also analyze the extent to which the world model trained on our synthetic data results in an inferred state space that supports the creation of action chains, a necessary property for planning.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Making Large Language Models into World Models with Precondition and Effect Knowledge</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Kaige Xie  Ian Yang  John Gunerli  Mark Riedl</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">School of Interactive Computing</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">Georgia Institute of Technology</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.1">{kaigexie, iyang30, hakancangunerli, riedl}@gatech.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="696" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A world model must be able to make valid action prediction and state transition prediction.
Valid action prediction refers to predicting whether an action is valid to be taken at a world state.
State transition prediction refers to predicting after a valid action is taken at an old world state, what the new world state will look like.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.4">Intelligent agents must reason about how actions affect the world.
A <span class="ltx_text ltx_font_italic" id="S1.p1.4.1">world model</span> is a model of the underlying, true dynamics of how an environment can change or be changed.
A world model can also be referred to as a transition model, <math alttext="P(s^{\prime}|s,a)" class="ltx_Math" display="inline" id="S1.p1.1.m1.3"><semantics id="S1.p1.1.m1.3a"><mrow id="S1.p1.1.m1.3.3" xref="S1.p1.1.m1.3.3.cmml"><mi id="S1.p1.1.m1.3.3.3" xref="S1.p1.1.m1.3.3.3.cmml">P</mi><mo id="S1.p1.1.m1.3.3.2" xref="S1.p1.1.m1.3.3.2.cmml">⁢</mo><mrow id="S1.p1.1.m1.3.3.1.1" xref="S1.p1.1.m1.3.3.1.1.1.cmml"><mo id="S1.p1.1.m1.3.3.1.1.2" stretchy="false" xref="S1.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S1.p1.1.m1.3.3.1.1.1" xref="S1.p1.1.m1.3.3.1.1.1.cmml"><msup id="S1.p1.1.m1.3.3.1.1.1.2" xref="S1.p1.1.m1.3.3.1.1.1.2.cmml"><mi id="S1.p1.1.m1.3.3.1.1.1.2.2" xref="S1.p1.1.m1.3.3.1.1.1.2.2.cmml">s</mi><mo id="S1.p1.1.m1.3.3.1.1.1.2.3" xref="S1.p1.1.m1.3.3.1.1.1.2.3.cmml">′</mo></msup><mo fence="false" id="S1.p1.1.m1.3.3.1.1.1.1" xref="S1.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S1.p1.1.m1.3.3.1.1.1.3.2" xref="S1.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">s</mi><mo id="S1.p1.1.m1.3.3.1.1.1.3.2.1" xref="S1.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S1.p1.1.m1.2.2" xref="S1.p1.1.m1.2.2.cmml">a</mi></mrow></mrow><mo id="S1.p1.1.m1.3.3.1.1.3" stretchy="false" xref="S1.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.3b"><apply id="S1.p1.1.m1.3.3.cmml" xref="S1.p1.1.m1.3.3"><times id="S1.p1.1.m1.3.3.2.cmml" xref="S1.p1.1.m1.3.3.2"></times><ci id="S1.p1.1.m1.3.3.3.cmml" xref="S1.p1.1.m1.3.3.3">𝑃</ci><apply id="S1.p1.1.m1.3.3.1.1.1.cmml" xref="S1.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S1.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><apply id="S1.p1.1.m1.3.3.1.1.1.2.cmml" xref="S1.p1.1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S1.p1.1.m1.3.3.1.1.1.2.1.cmml" xref="S1.p1.1.m1.3.3.1.1.1.2">superscript</csymbol><ci id="S1.p1.1.m1.3.3.1.1.1.2.2.cmml" xref="S1.p1.1.m1.3.3.1.1.1.2.2">𝑠</ci><ci id="S1.p1.1.m1.3.3.1.1.1.2.3.cmml" xref="S1.p1.1.m1.3.3.1.1.1.2.3">′</ci></apply><list id="S1.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S1.p1.1.m1.3.3.1.1.1.3.2"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">𝑠</ci><ci id="S1.p1.1.m1.2.2.cmml" xref="S1.p1.1.m1.2.2">𝑎</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.3c">P(s^{\prime}|s,a)</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.3d">italic_P ( italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | italic_s , italic_a )</annotation></semantics></math> because it tells us how an action <math alttext="a" class="ltx_Math" display="inline" id="S1.p1.2.m2.1"><semantics id="S1.p1.2.m2.1a"><mi id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><ci id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">a</annotation><annotation encoding="application/x-llamapun" id="S1.p1.2.m2.1d">italic_a</annotation></semantics></math>, when performed in a state <math alttext="s" class="ltx_Math" display="inline" id="S1.p1.3.m3.1"><semantics id="S1.p1.3.m3.1a"><mi id="S1.p1.3.m3.1.1" xref="S1.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S1.p1.3.m3.1b"><ci id="S1.p1.3.m3.1.1.cmml" xref="S1.p1.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S1.p1.3.m3.1d">italic_s</annotation></semantics></math>, can result in the world transitioning to state <math alttext="s^{\prime}" class="ltx_Math" display="inline" id="S1.p1.4.m4.1"><semantics id="S1.p1.4.m4.1a"><msup id="S1.p1.4.m4.1.1" xref="S1.p1.4.m4.1.1.cmml"><mi id="S1.p1.4.m4.1.1.2" xref="S1.p1.4.m4.1.1.2.cmml">s</mi><mo id="S1.p1.4.m4.1.1.3" xref="S1.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p1.4.m4.1b"><apply id="S1.p1.4.m4.1.1.cmml" xref="S1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S1.p1.4.m4.1.1.1.cmml" xref="S1.p1.4.m4.1.1">superscript</csymbol><ci id="S1.p1.4.m4.1.1.2.cmml" xref="S1.p1.4.m4.1.1.2">𝑠</ci><ci id="S1.p1.4.m4.1.1.3.cmml" xref="S1.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.4.m4.1c">s^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S1.p1.4.m4.1d">italic_s start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>.
World models are an essential aspect of creating <span class="ltx_text ltx_font_italic" id="S1.p1.4.2">agents</span>, which are entities that can perceive the environment and perform actions in order to affect change on the environment with respect to some objective.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Are Large Language Models (LLMs) such as GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib23" title="">2023</a>)</cite>, ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib21" title="">2022</a>)</cite>, and Llama 2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib33" title="">2023</a>)</cite>, capable of expressing themselves as world models?
The answer is generally found to be “no” <cite class="ltx_cite ltx_citemacro_cite">Valmeekam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib34" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib35" title="">2024</a>); Pan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib24" title="">2024</a>); Chambers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib4" title="">2024</a>)</cite>.
However, some nuance is needed.
LLMs express world model-like capabilities because they are trained on language produced by humans who operate in a world dictated by a consistent set of physical rules—water always boils when hot enough, objects always fall due to gravity, activating the brakes on a bike generally causes the bike to come to a stop, etc.
Phenomena of the real world appear in our text corpora and are learned such that when we ask questions about the dynamics of the real world (e.g., “what happens when I apply the brakes?”), we often get plausible responses.
However, the ability to generate a span of text that plausibly explains what happens does not necessarily mean that the LLM understands the phenomenon.
This can be demonstrated when we ask LLMs about novel situations or novel combinations of phenomenon that are too far outside their training distribution.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Counter-intuitively, LLMs trained on sufficiently rich text sources may be induced to behave like a world model.
While an LLM may not be reasoning about the underlying transition dynamics to answer a question about the application of brakes on a bike, the same LLM can be asked questions about the applicability of actions and how the world is changed by actions carried out in the real world.
For example, large language models such as GPT-4, Gemini 1.5 Pro, and Claude 3.5 Sonnet can reliably answer questions about what must be true in the world for an action to be performable (also called <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">preconditions</span>), and what aspects of the world does one desire to affect through the execution of an action (also called <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">effects</span>).
Precondition and effect knowledge about actions can be used to assemble a world model because they tell us whether an action can be performed and what states we will transition to if an action is performed.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we show how to induce large language models to behave like world models.
As depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, a world model must perform two functions.
(1) It must determine action applicability.
We must be able to answer the question: “can this action be performed at this time?”
(2) If the action is applicable, a world model must be able to answer the question: “how the world is different if it were to be performed?”
This is equivalent to asking “what state I will be in?”
We fine-tune two LLMs, one that predicts the preconditions of an action, and another that predicts the effects of an action.
We provide a means for these two models to work together, along with procedures for checking preconditions against a world state and applying effects to alter the world state, thus fully replicating the functionality of a world model.
We also provide a method for using LLMs to generate synthetic data with which to fine-tune the above two models.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We conduct human-participant studies that assess the extent to which people agree that the corpus of preconditions and effects generated by our models matches their own world model understanding;
human understanding of the real world is the best source of ground-truth data available.
Then, having established bounds on the accuracy of our models with respect to human understanding, we conduct automated evaluations showing the effectiveness of our overall approach in terms of precondition/effect inference and world modeling.
We also measure the reliability of our synthetic data generation approach through human evaluations.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">we introduce an approach to inducing an LLM to behave like a world model that is able to perform valid action prediction and state transition prediction;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">we design a technique to create a synthetic precondition/effect corpus from the LLM that is essential for building a world model;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">both human evaluations and automated evaluations show the effectiveness of our method in creating a high-quality precondition/effect corpus and a capable world model.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>World Modeling</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">World modeling has emerged in reinforcement learning as an effective way to learn both the transition function and policy for a given RL task <cite class="ltx_cite ltx_citemacro_cite">Ha and Schmidhuber (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib11" title="">2018</a>)</cite>.
Improvements in world modeling have led to advances in causal reasoning <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib39" title="">2023</a>); Richens and Everitt (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib28" title="">2024</a>)</cite>, world knowledge maintenance <cite class="ltx_cite ltx_citemacro_cite">Freed et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib9" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib41" title="">2023</a>); Samsami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib29" title="">2024</a>)</cite>, and model-based planning <cite class="ltx_cite ltx_citemacro_cite">Guan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib10" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib40" title="">2021</a>); Sekar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib31" title="">2020</a>)</cite>.
For causal reasoning, <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib39" title="">2023</a>)</cite> explore causality in RL and find that learning a causal world model without knowledge of the environment structure can improve explainability without suffering from low accuracy.
<cite class="ltx_cite ltx_citemacro_citet">Richens and Everitt (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib28" title="">2024</a>)</cite> incorporate concepts from causality and decision theory to prove that agents that satisfy a large set of distribution shifts must have necessarily approximated a causal world model of the data generation process.
For world knowledge maintenance, <cite class="ltx_cite ltx_citemacro_citet">Freed et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib9" title="">2023</a>)</cite> explore to what extent language models intrinsically encode relational world knowledge through the training process without a predefined relational schema.
<cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib41" title="">2023</a>)</cite> survey different methods for incorporating world knowledge into LLMs, and distinguish between implicit methods (emergent world knowledge after training on massive corpora, knowledge editing) and explicit methods (external memory, off-the-shelf retrieval, or Internet-enhanced).
<cite class="ltx_cite ltx_citemacro_citet">Samsami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib29" title="">2024</a>)</cite> approach the memory problem for world model-based RL agents by integrating state-space models into the DreamerV3 world model to capture and maintain long-range relational knowledge.
For model-based planning, <cite class="ltx_cite ltx_citemacro_citet">Guan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib10" title="">2023</a>)</cite> argue that LLMs themselves are not sufficient to tackle difficult planning problems, and propose to use pre-trained language models as an interface between planning domain definition language and natural language, so as to construct more capable explicit world domain models.
<cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib40" title="">2021</a>)</cite> propose an algorithm that first groups together goals within the world model embedded in latent space by temporal distance, then learns sparse landmarks that can be used for planning, aiming to overcome the difficulty of long planning horizons which might lead to world models that are far from reality.
<cite class="ltx_cite ltx_citemacro_citet">Sekar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib31" title="">2020</a>)</cite> demonstrate that self-supervised world models can be used to help RL agents more efficiently explore environments via planning, and better generalize to unseen tasks as well.
Recently, world models have started to have different ways of intersecting with LLMs <cite class="ltx_cite ltx_citemacro_cite">Nottingham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib20" title="">2023</a>); Hao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib12" title="">2023</a>); Xiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib38" title="">2024</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Nottingham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib20" title="">2023</a>)</cite> propose to use LLMs to hypothesize a world model that will be utilized for planning &amp; exploration and verified with grounded experience, in order to improve sample efficiency of RL agents.
<cite class="ltx_cite ltx_citemacro_citet">Hao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib12" title="">2023</a>)</cite> introduce Reasoning via Planning which leverages an LLM as a world model and applies Monte Carlo Tree Search to build a reasoning tree that is explored iteratively to provide reasoning guidance.
<cite class="ltx_cite ltx_citemacro_citet">Xiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib38" title="">2024</a>)</cite> propose a paradigm for efficiently fine-tuning LLMs with embodied experiences from world models to improve performance on seen and unseen RL tasks.
In contrast, we focus on developing techniques to induce the LLM to work reliably as a world model.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Precondition and Effect</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Understanding action preconditions and effects in text is a crucial yet challenging task.
<cite class="ltx_cite ltx_citemacro_citet">Branavan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib3" title="">2012</a>)</cite> pioneer work in this area using reinforcement learning to extract high-level planning knowledge from text with the guidance of action preconditions and effects.
<cite class="ltx_cite ltx_citemacro_citet">Dalvi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib7" title="">2018</a>)</cite> develop a dataset and models for paragraph comprehension, and highlight the importance of tracking state changes in procedural text.
<cite class="ltx_cite ltx_citemacro_citet">Hayton et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib13" title="">2020</a>)</cite> propose an automated process for extracting action models from text summaries, though their approach aims primarily to mirror the input narrative.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib16" title="">2024</a>)</cite> introduce a more sophisticated system that addresses complex challenges in narrative texts such as nested event arguments and conditional events.
Their approach combines structured event extraction with predictions of commonsense event relations to get more nuanced action models.
<cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib37" title="">2023</a>)</cite> highlight the importance of extracting pre- and post-conditions from instructional texts and introduce an approach that leverages weak supervision and contextual information to improve action condition inference.
<cite class="ltx_cite ltx_citemacro_citet">Martin (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib19" title="">2021</a>)</cite> designs a commonsense rule engine with preconditions and effects derived from VerbNet for an appropriate selection of events in neurosymbolic story generation to improve the causal consistency of generated stories.
Commonsense reasoning and its connection to preconditions have been a major area of study, particularly in relation to how AI systems infer what makes a statement valid or invalid in different contexts.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Qasemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib27" title="">2022b</a>)</cite> design models that utilize preconditions to understand common actions and statements.
<cite class="ltx_cite ltx_citemacro_citet">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib15" title="">2020</a>)</cite> create a dataset to analyze how preconditions operate in textual contexts, allowing for a deeper understanding of both enabling and disabling conditions that can affect whether a given action or statement is possible.
<cite class="ltx_cite ltx_citemacro_citet">Qasemi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib26" title="">2022a</a>)</cite> formalize the notion of enabling and disabling preconditions in commonsense reasoning, and introduce a framework that forces models to make clear decisions about the preconditions required for a statement to hold true.
This differs from prior work that primarily focuses on probabilistic inferences, such as the ATOMIC dataset <cite class="ltx_cite ltx_citemacro_cite">Sap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib30" title="">2019</a>)</cite>, which explores cause-and-effect relationships but does not make explicit the conditions under which those relationships are valid.
<cite class="ltx_cite ltx_citemacro_citet">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib14" title="">2021</a>)</cite> emphasize the importance of variety in precondition inference and tackle the issue of limited precondition generation by developing methods that can create a broader range of preconditions, which enhances the robustness of commonsense reasoning models.
Diversity in preconditions and effects is crucial for building AI systems that can handle the variability of real-world scenarios.
Our work aims to make accurate precondition and effect inferences by learning from a high-quality synthetic corpus of action preconditions and effects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>World Model</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section introduces our methodological contributions in creating a world model.
We focus on real-world domains where there is significant dependency among actions, meaning that to be able to take one action in an environment, it is generally necessary to take a few other actions beforehand to set up an appropriate world state for that action to be taken.
We call this phenomenon of dependency among actions as <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">action chaining</span>.
Representative real-world domains such as dish cooking with a high degree of action chaining are worth studying because it is technically more challenging to model actions and states in these domains than fictional domains existing works <cite class="ltx_cite ltx_citemacro_cite">Ammanabrolu and Riedl (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib1" title="">2021a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib2" title="">b</a>)</cite> have been focusing on where there is much less action chaining.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">A world model needs to be able to make the following two types of predictions: <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">valid action prediction</span> and <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">state transition prediction</span>.
Valid action prediction means: given a world state at a certain time point, predicting which actions are valid to be taken.
State transition prediction means: given a world state at a certain time point and a valid action that has been taken, predicting what the new world state is, i.e. how the world state transitions from the old one to the new one.
Previous work <cite class="ltx_cite ltx_citemacro_cite">Ammanabrolu and Riedl (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib1" title="">2021a</a>)</cite> tackles these two prediction tasks by directly modeling the mapping relationships between states and actions.
However, by ignoring preconditions and effects, it reduces the possibility of interpretability.
In high-stakes domains with more complicated actions and states, it is usually critical to make interpretable predictions in case developers or users might want to know how/why a certain prediction is made.
This motivates us to use a first principles approach to building the world model by modeling the preconditions and effects and then connecting them back to actions and states.
In this way, two complex prediction tasks (valid action prediction and state transition prediction) are first broken down into the most basic elements (precondition and effect of actions) and then reassembled from the ground up.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Specifically, our world model consists of two main sub-modules: a precondition/effect inference module (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS1" title="3.1 Precondition/Effect Inference Module ‣ 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and a semantic matching module (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS2" title="3.2 Semantic State Matching ‣ 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
The world model performs valid action prediction and state transition prediction by invoking these two modules (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S3.SS3" title="3.3 Applying the World Model ‣ 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Precondition/Effect Inference Module</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This module is designed to be able to take an action and infer the preconditions that this action requires, and the effects this action will cause if it is taken.
We fine-tune one LLM for precondition inference.
The input to this LLM is an action and its output is the corresponding precondition, where both the input and output are in natural language.
We similarly fine-tune another LLM for effect inference with an action as the input and effect as the output.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To train these models, we require a dataset of actions accompanied by their preconditions and effects.
However, to the best of our knowledge there is no existing precondition/effect corpus created for domains with significant action chaining.
Collecting a supervised dataset from the human annotation is an option but it might be a bit costly to carry out.
To fill this gap while saving the cost, we curate an action precondition/effect corpus automatically by prompting GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib23" title="">2023</a>)</cite>.
Through preliminary prompting experiments, we find that GPT-4 possesses the intrinsic knowledge about action precondition/effect but does not necessarily apply that knowledge when generating plan action sequences.
That is, a carefully designed prompting technique is needed to properly induce this knowledge out of GPT-4.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">An additional challenge that needs to be overcome during the data curation is how to ensure that there is a high degree of action chaining.
From the standpoint of preconditions and effects, significant action chaining can be interpreted as follows: in most cases, one action’s preconditions are covered in other actions’ effects, and one action’s effects are covered in other actions’ preconditions as well, suggesting strong dependency among actions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">We propose a <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">global-local</span> prompting technique to induce high-quality action preconditions and effects with significant action chaining.
This technique contains five steps:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Prompt GPT-4 to come up with a full action plan which involves a series of mutually dependent action steps for performing a certain task.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Prompt GPT-4 with few-shot annotated examples to selectively discard the action steps that it believes are isolated from other steps and thus do not have enough potential to produce significant action chaining, and also to optionally add new action steps if they can provide chaining and make the whole action plan more coherent.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Prompt GPT-4 to simultaneously generate preconditions and effects for each action step.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Prompt GPT-4 with few-shot annotated examples to (1) identify action steps whose preconditions and effects are not quite chained with other steps’, (2) and next perform a one-time re-generation on the identified steps to obtain a new version of preconditions and effects, (3) and then determine whether or not to selectively discard some of these action steps due to weak action chaining.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Filter action plan samples according to the percentages of the preconditions/effects that are not chained (i.e., not covered) in the plans: specifically, feed GPT-4 with both a sample’s collection of preconditions and a sample’s collection of effects, and next ask GPT-4 to output the preconditions/effects which are not covered in the effect/precondition collections, and then calculate the percentage number, and finally filter samples of highest percentages (we filter the highest 5%).</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.p4.2">We call this prompting technique as <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.2.1">global-local</span> because it first performs one round of step discard based on a local inspection of individual action steps (steps #2 and #4), and then performs another round of sample filtering based on a global view of the whole action plan (step #5).
Note that since a sample technically contains a series of action steps, sample filtering can also be viewed as step discard in batch.
All the prompts we use are presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#A2" title="Appendix B Prompts for Global-Local Prompting ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.
We run the global-local prompting with GPT-4 thousands of times and obtain more than two thousand action plans, each of them comprising about twenty action steps on average.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Semantic State Matching</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To support the valid action prediction and state transition prediction, we need
(1) to determine whether the inferred preconditions are a subset of a world state, and
(2) to determine how the world state must change to produce a successor world state.
Since world states are natural language descriptions, as are preconditions and effects, we require a means of semantically matching preconditions to states and updating states based on effects.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Specifically, we design two separate GPT-4-based modules for the two prediction tasks respectively.
For the valid action prediction task, its semantic matching module needs to be able to match the inferred action preconditions with the current world state, and then determine whether all the preconditions are covered in the current world state, meaning that all the preconditions are satisfied and thus this action is valid to be taken at this moment.
For the state transition prediction task, its semantic matching module needs to be able to match the inferred action effects with the current world state, and then determine whether there exists any part of the world state that contradicts the effects, meaning that this state transition requires to update the old world state by adding the effects to the old world state and in the meantime deleting the contradicted part from the old world state.
All the prompts we use are presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#A3" title="Appendix C Prompts for Semantic State Matching ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Applying the World Model</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Recall that a world model must do two things: valid action prediction and state transition prediction.
To do valid action prediction, our world model first uses the precondition inference module to infer an action’s preconditions, then uses the semantic matching module to match the inferred preconditions with the current world state, and makes a judgment on whether this action is valid to be taken at the moment.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To do state transition prediction, our world model first uses the effect inference module to infer an action’s effects, then uses the semantic matching module to match the inferred effects with the current world state, and predicts the new world state by making proper addition and/or deletion to the old world state.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We perform multi-faceted evaluations of our world model.
Specifically, we evaluate the following three things: (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS1" title="4.1 Evaluation of Global-Local Prompting ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">4.1</span></a>) the effectiveness of the global-local prompting technique in creating a high-quality action precondition/effect corpus, (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS2" title="4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">4.2</span></a>) the effectiveness of the precondition/effect inference module in making accurate precondition/effect predictions, and (§<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#S4.SS3" title="4.3 Evaluation of World Model ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">4.3</span></a>) the effectiveness of the world model (precondition/effect inference module and semantic matching module working together) in making accurate valid action predictions and state transition predictions.
We choose dish cooking as the domain to evaluate as it is a representative real-world domain example in which there is significant dependency among different actions.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In addition to evaluations based on automatic metrics, we also perform human evaluations to make the assessment more comprehensive.
All the human evaluations are performed using the Prolific crowdsourcing platform<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.prolific.co/" title="">https://www.prolific.co/</a></span></span></span>.
These human evaluations have been approved by our institution’s Institutional Review Board (IRB).
We qualify annotators by first asking them a screening question at the beginning of the questionnaire, and then verifying their answers manually to disregard annotations provided by those who fail the screening.
We require annotators to be physically located in the U.S. and to speak English as a first language.
For each human evaluation, we source a distinct set of annotators without any overlap to avoid potential bias in annotations that could occur from participating in related evaluations in the past.
In each evaluation, we measure the average inter-annotator agreement using Fleiss’s kappa <cite class="ltx_cite ltx_citemacro_cite">Fleiss (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib8" title="">1971</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation of Global-Local Prompting</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We seek to understand whether our global-local prompting technique is effective in creating a high-quality action precondition/effect corpus that is reliable and informative enough for an inference module to learn useful knowledge from.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The reliability of the corpus is determined by how reasonable an action’s preconditions and effects are.
We randomly select 30 action samples (with their preconditions and effects) from the corpus, and ask annotators to inspect each of them to judge if the preconditions and effects are reasonable for the action.
We arrange five different annotators to assess each action sample, and use the majority vote of their annotations as the final judgment.
We find that 93% of action samples are deemed reasonable with a moderate average inter-annotator agreement, suggesting the reliability of the corpus.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The informativeness of the corpus is determined by how significant the action chaining is.
To measure it, we randomly select 30 action plan samples (each of them comprising a series of action steps with preconditions and effects) from the corpus, and ask annotators to inspect each of them to judge if there exist more than two action steps (approximately more than 10% of all action steps in an action plan sample) whose preconditions or effects are never covered in any other action steps’ effects and preconditions (i.e. indicating insignificant action chaining).
We find that 87% of action plan samples are deemed to have significant action chaining with a moderate average inter-annotator agreement, suggesting the informativeness of the corpus.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Since this technique involves multiple steps, we need to confirm its effectiveness by evaluating if the key steps we designed are running properly as we expected.
We specifically examine steps #2, #4, and #5 (global and local step discard), because steps #1 and #3 are foundational steps that have been implicitly measured in the evaluation of the informativeness and reliability of the corpus.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">For steps #2 and #4 (local step discard), we measure how insignificant the action chaining is in the discarded action steps.
We randomly select 30 discarded action step samples (15 from step #2 without preconditions/effects, and 15 from step #4 with preconditions/effects), provide them along with their corresponding original action plans to annotators, and ask annotators to inspect each of them to judge if they are not chained with other steps in the action plans.
We find that 73% of discarded action step samples are deemed not chained with other steps in action plans with a moderate average inter-annotator agreement, suggesting the effectiveness of steps #2 and #4.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">For step #5 (global step discard), we measure how insignificant the action chaining is in the filtered action plans.
We randomly select 30 filtered action plan samples, and also randomly select 30 kept action plan samples from the corpus.
We put the selected samples into 30 filtered-kept sample pairs (one filtered and one kept) and ask annotators to inspect each of them to judge if the filtered sample clearly has less action chaining than the kept sample.
We find that 80% of the time the filtered action plan samples have less action chaining than the kept samples with a moderate average inter-annotator agreement, suggesting the effectiveness of step #5.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation of Precondition/Effect Inference Module</h3>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:224.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(60.3pt,-31.2pt) scale(1.38487340573882,1.38487340573882) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.2">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.3">BLEU-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.4">BLEU-3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.5">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.6">SMS</th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="6" id="S4.T1.1.1.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.1.2.2.1.1" style="background-color:#F2F2F2;">Precondition Inference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.3.1.1">Ablation-Local</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1.2">58.57</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1.3">63.47</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1.4">57.88</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1.5">51.07</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.1.6">17.02</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.4.2.1">Ablation-Global</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.2">60.53</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.3">66.25</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.4">60.06</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.5">52.24</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.6">17.89</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.5.3.1">Full Method</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.2.1">65.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.3.1">70.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.4.1">64.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.5.1">57.96</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.6.1">19.77</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.4" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6" id="S4.T1.1.1.6.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.1.6.4.1.1" style="background-color:#F2F2F2;">Effect Inference</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.7.5.1">Ablation-Local</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.2">55.03</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.3">60.41</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.4">54.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.5">53.17</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.6">16.56</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.8.6.1">Ablation-Global</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.2">58.51</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.3">62.71</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.4">57.92</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.5">55.13</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.6">17.55</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.1.9.7.1">Full Method</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.9.7.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.7.2.1">61.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.9.7.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.7.3.1">65.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.9.7.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.7.4.1">59.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.9.7.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.7.5.1">57.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.9.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.7.6.1">18.25</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results on automatic metrics for precondition inference module and effect inference module.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:89.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(42.9pt,-8.9pt) scale(1.24643026101782,1.24643026101782) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.2">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.3">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.4">BLEU-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.5">BLEU-3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.6">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1.7">SMS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.2.1.1">Ablation-Local</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.2">74.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3">49.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.4">55.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.5">49.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.6">44.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.7">12.96</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.3.2.1">Ablation-Global</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.2">77.00</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.3">53.43</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.4">59.09</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.5">54.30</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.6">49.71</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.7">14.52</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.1.4.3.1">Full Method</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1">81.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.3.1">56.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.4.1">61.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.5.1">56.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.6.1">52.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.7.1">15.19</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on automatic metrics for world model’s valid action prediction and state transition prediction.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this evaluation, we seek to understand whether our precondition/effect inference module is effective in making accurate precondition/effect predictions.
We train two FLAN-T5 models <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib5" title="">2024</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">flan-t5-large<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url" href="https://huggingface.co/google/flan-t5-large" title="">https://huggingface.co/google/flan-t5-large</a></span></span></span></span>, on our action precondition and effect corpus to learn to perform precondition and effect inference separately.
Implementation details can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#A1" title="Appendix A Implementation Details ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>.
We maintain a holdout test set which is composed of 200 action plans with a series of action steps.
We use the following four automatic evaluation metrics to measure the empirical test-set performance: (1) token-level F1 score <cite class="ltx_cite ltx_citemacro_cite">Tjong Kim Sang and
De Meulder (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib32" title="">2003</a>)</cite> (F1); (2) BLEU score <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib25" title="">2002</a>)</cite> (BLEU-2 and BLEU-3); (3) ROUGE score <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib17" title="">2004</a>)</cite> (ROUGE-L); (4) sentence mover’s similarity <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib6" title="">2019</a>)</cite> (SMS).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Since the empirical performance of the precondition/effect inference module is dependent on how good the training data is, we thereby perform ablation studies to investigate how the key steps (global and local step discard) in creating the corpus may impact the precondition/effect inference performance, and also to confirm the key steps’ necessity by making performance comparison between the full version and the ablated versions.
Specifically, we study the following two ablations: training inference modules on the corpus created by the global-local prompting technique while skipping (1) steps #2 and #4 (denoted as Ablation-Local) or (2) step #5 (denoted as Ablation-Global).
Results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S4.T1" title="Table 1 ‣ 4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Table 1</span></a> show that the precondition/effect inference modules trained on our high-quality corpus can make accurate predictions that are very consistent with the ground truth.
The performance comparison between the full version and the ablated versions demonstrates that both global and local step discard are necessary and beneficial for creating a high-quality corpus that LLMs can further learn from to make reliable action precondition/effect inferences.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Automatic evaluation metrics are not always perfect since they only measure how aligned the prediction and the ground truth are.
Sometimes it may be the case that an automatic metric negatively prefers a prediction that a human would prefer positively, and vice versa.
Because of this potential discrepancy, we need human evaluation to assess the prediction more comprehensively.
We randomly select 30 model-predicted action preconditions and 30 model-predicted action effects.
We ask annotators to inspect each of the predicted preconditions and effects to judge if these predictions look reasonable to them and are consistent with their commonsense understanding of the real world.
We find that 77% of the predicted preconditions and 70% of the predicted effects match human’s world model understanding with a moderate average inter-annotator agreement, confirming the effectiveness of the precondition/effect inference module from the human perspective.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation of World Model</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this evaluation, we seek to understand whether our world model is effective in making accurate valid action predictions and state transition predictions.
Before starting the evaluation, we perform a corpus refactoring to adapt the data into appropriate input-output formats for the two prediction tasks.
Since both tasks need the ground-truth world state as part of the input, we refactor the action preconditions and effects in an entire action plan to get the ground-truth world state at each time step.
We automate this refactoring process with the assistance of GPT-4o <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib22" title="">2024</a>)</cite>.
For each action plan, we get the initial world state by combining all the preconditions excluding those that are covered in any of the effects within the plan.
We derive new world states right after taking every single step by using the state addition and deletion at each step to perform step-by-step iterative updates on old world states.
The state addition is straightforward to get as it is just equivalent to the ground truth of the action effects.
The state deletion is obtained by iterating through each item in the old state, and asking GPT-4o to compare them to each item in the state addition to judge if there is a contradiction and thus that item in the old state needs to be deleted.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">For each of the two prediction tasks, we maintain a 200-sample holdout test set respectively.
For valid action prediction, we balance the test set by equalizing the number of valid and invalid action samples, and use the prediction accuracy (Acc.) as the evaluation metric.
For state transition prediction, we follow an existing evaluation setup <cite class="ltx_cite ltx_citemacro_cite">Ammanabrolu and Riedl (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib1" title="">2021a</a>)</cite> by combining the state addition and deletion as the final prediction and using the metrics introduced in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S4.SS2" title="4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">subsection 4.2</span></a> (F1, BLEU-2/3, ROUGE-L, and SMS) to measure the performance.
Results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S4.T2" title="Table 2 ‣ 4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">Table 2</span></a> show that on both tasks our world model can make accurate predictions consistent with the ground truth.
We perform similar ablation studies as <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S4.SS2" title="4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">subsection 4.2</span></a> on Ablation-Local and Ablation-Global, and the results demonstrate that both global and local step discard are necessary and beneficial for establishing a capable world model.
For state transition prediction, we also do a similar human evaluation as <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S4.SS2" title="4.2 Evaluation of Precondition/Effect Inference Module ‣ 4 Evaluation ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">subsection 4.2</span></a> to evaluate our world model more comprehensively. On 30 randomly selected samples, we find that with a moderate average inter-annotator agreement, 63% of the predicted state transitions are consistent with human’s commonsense understanding and reasoning of the real world.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis of Search Space</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">One of the utilities of a world model is to make logically sound plans.
We analyze the search space of our world model to determine if it could be used by a hypothetical planning system.
Specifically, we answer the following two questions.
(1) Given a never-before-seen action, how likely is it for this action to be satisfiable by the search space our world model creates?
This question assesses the likelihood that chaining will occur for any given target, and that valid plans exist for which an action would be the last step in a plan.
An action being satisfiable by a search space means that the preconditions of this action can be met in a certain world state that can be reached by taking some actions from this search space.
In other words, in this search space it is possible to reach a certain world state after taking some actions and applying their action effects such that it now becomes valid to take this action.
(2) Given a never-before-seen action, how many different ways are there to satisfy the action in the search space? This question assesses the versatility of the world model to allow for new combinations of chains to be found by a planner that differ from any one recipe example in the training corpus.
That is, the world model supports creative trajectories through the state space and is not just memorizing what is necessary to recreate known plans.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We maintain a holdout test set containing 200 never-before-seen actions and run automatic experiments to answer these two questions.
The answers are essentially determined by the diversity of our action precondition/effect corpus and the effectiveness of our precondition/effect inference modules.
Specifically, we iterate through all the actions in our corpus and use the effect inference module to infer their effects.
We also iterate through all the actions in the holdout test set and use the precondition inference module to infer their preconditions.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">To answer the first question, for each action in the holdout test set, we use GPT-4o to compare each item in its precondition with each item in the corpus-level collection of all action effects, and to determine if there exists at least one effect item that semantically matches the precondition item.
If all the precondition items of this action are successfully matched with at least one effect item, then such an action is deemed satisfiable.
We find that 83.5% of the actions in the holdout test set are satisfiable, suggesting that the search space created by our world model is large enough to be able to make most of the never-before-seen actions valid to be taken.
To answer the second question, for each action in the holdout test set, we use GPT-4o to compare each item in its precondition with each item in the corpus-level collection of all action effects, and to determine how many effect items there exist that semantically match the precondition item.
We multiply together all # of matched effects for each precondition item and find that on average, a satisfiable action can be satisfied in 9.7 different ways in the search space.
It suggests that if one way to satisfy an action is blocked for some reason, one can reliably find other ways in this search space as alternatives.
It can also be inferred that the system is not simply memorizing dish recipes.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we have demonstrated the potential of Large Language Models (LLMs) to function as world models by predicting valid actions and state transitions, two fundamental aspects of any world model.
Through fine-tuning, we adapt LLMs to infer both the preconditions and the effects of actions, thereby replicating key functions necessary for modeling environmental dynamics.
Our approach hinges on using synthetic data generation to enhance model training, a technique validated by human-participant studies, which confirms that the LLM-generated precondition and effect knowledge aligns with human understanding of real-world phenomena.
Automated evaluations further support the effectiveness and reliability of our method in creating a robust world model.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While our approach demonstrates the ability to induce LLMs to behave as world models, it has certain limitations.
First, the model’s performance is constrained by the quality and diversity of its training data, which may not cover super rare or highly novel scenarios.
Additionally, LLMs do not possess true causal reasoning, and their predictions rely on patterns from textual data rather than an inherent understanding of real-world dynamics.
Finally, the synthetic data generation process, while effective, may introduce biases or inaccuracies that could affect the overall performance in specific edge cases.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammanabrolu and Riedl (2021a)</span>
<span class="ltx_bibblock">
Prithviraj Ammanabrolu and Mark Riedl. 2021a.

</span>
<span class="ltx_bibblock">Learning knowledge graph-based world models of textual environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>,
34:3720–3731.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammanabrolu and Riedl (2021b)</span>
<span class="ltx_bibblock">
Prithviraj Ammanabrolu and Mark Riedl. 2021b.

</span>
<span class="ltx_bibblock">Modeling worlds in text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Workshop on Commonsense Reasoning and Knowledge Bases</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Branavan et al. (2012)</span>
<span class="ltx_bibblock">
S.R.K. Branavan, Nate Kushman, Tao Lei, and Regina Barzilay. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P12-1014" title="">Learning high-level
planning from text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 126–135, Jeju
Island, Korea. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chambers et al. (2024)</span>
<span class="ltx_bibblock">
Rachel Chambers, Naomi Tack, Eliot Pearson, Lara J Martin, and Francis Ferraro.
2024.

</span>
<span class="ltx_bibblock">Berall: Towards generating retrieval-augmented state-based
interactive fiction games.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">The 4th Wordplay: When Language Meets Games Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2024)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Journal of Machine Learning Research</em>, 25(70):1–53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2019)</span>
<span class="ltx_bibblock">
Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1264" title="">Sentence mover’s
similarity: Automatic evaluation for multi-sentence texts</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2748–2760, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dalvi et al. (2018)</span>
<span class="ltx_bibblock">
Bhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau Yih, and Peter Clark. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1144" title="">Tracking state changes
in procedural text: a challenge dataset and models for process paragraph
comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1595–1604, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fleiss (1971)</span>
<span class="ltx_bibblock">
Joseph L Fleiss. 1971.

</span>
<span class="ltx_bibblock">Measuring nominal scale agreement among many raters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Psychological bulletin</em>, 76(5):378.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freed et al. (2023)</span>
<span class="ltx_bibblock">
Benjamin Freed, Siddarth Venkatraman, Guillaume Adrien Sartoretti, Jeff
Schneider, and Howie Choset. 2023.

</span>
<span class="ltx_bibblock">Learning temporally abstractworld models without online
experimentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Machine Learning</em>, pages
10338–10356. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan et al. (2023)</span>
<span class="ltx_bibblock">
Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. 2023.

</span>
<span class="ltx_bibblock">Leveraging pre-trained large language models to construct and utilize
world models for model-based task planning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>,
36:79081–79094.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha and Schmidhuber (2018)</span>
<span class="ltx_bibblock">
David Ha and Jürgen Schmidhuber. 2018.

</span>
<span class="ltx_bibblock">Recurrent world models facilitate policy evolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in neural information processing systems</em>, 31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2023)</span>
<span class="ltx_bibblock">
Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu.
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.507" title="">Reasoning
with language model is planning with world model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing</em>, pages 8154–8173, Singapore. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayton et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Hayton, Julie Porteous, Joao Ferreira, and Alan Lindsay. 2020.

</span>
<span class="ltx_bibblock">Narrative planning model acquisition from text summaries and
descriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 1709–1716.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2021)</span>
<span class="ltx_bibblock">
Heeyoung Kwon, Nathanael Chambers, and Niranjan Balasubramanian. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.starsem-1.15" title="">Toward diverse
precondition generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of *SEM 2021: The Tenth Joint Conference on
Lexical and Computational Semantics</em>, pages 160–172, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2020)</span>
<span class="ltx_bibblock">
Heeyoung Kwon, Mahnaz Koupaee, Pratyush Singh, Gargi Sawhney, Anmol Shukla,
Keerthi Kumar Kallur, Nathanael Chambers, and Niranjan Balasubramanian. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.340" title="">Modeling
preconditions in text with a crowd-sourced dataset</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 3818–3828, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Ruiqi Li, Leyang Cui, Songtuan Lin, and Patrik Haslum. 2024.

</span>
<span class="ltx_bibblock">Naruto: Automatically acquiring planning models from narrative texts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 38, pages 20194–20202.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for
automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled weight
decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin (2021)</span>
<span class="ltx_bibblock">
Lara J Martin. 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Neurosymbolic Automated Story Generation.</em>
</span>
<span class="ltx_bibblock">Ph.D. thesis, Georgia Institute of Technology, Atlanta, GA, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nottingham et al. (2023)</span>
<span class="ltx_bibblock">
Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh
Hajishirzi, Sameer Singh, and Roy Fox. 2023.

</span>
<span class="ltx_bibblock">Do embodied agents dream of pixelated sheep: Embodied decision making
using language guided world modelling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Machine Learning</em>, pages
26311–26325. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/blog/chatgpt/" title="">Chatgpt: A large-scale
opendomain chatbot</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">OpenAI blog</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/index/hello-gpt-4o/" title="">Hello gpt-4o</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">OpenAI blog</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. (2023)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2024)</span>
<span class="ltx_bibblock">
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.
2024.

</span>
<span class="ltx_bibblock">Unifying large language models and knowledge graphs: A roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE Transactions on Knowledge &amp; Data Engineering</em>,
36(07):3580–3599.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qasemi et al. (2022a)</span>
<span class="ltx_bibblock">
Ehsan Qasemi, Filip Ilievski, Muhao Chen, and Pedro Szekely.
2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.505" title="">PaCo: Preconditions attributed to commonsense knowledge</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2022</em>, pages 6781–6796, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qasemi et al. (2022b)</span>
<span class="ltx_bibblock">
Ehsan Qasemi, Piyush Khanna, Qiang Ning, and Muhao Chen. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.aacl-main.26" title="">PInKS:
Preconditioned commonsense inference with minimal supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2nd Conference of the Asia-Pacific
Chapter of the Association for Computational Linguistics and the 12th
International Joint Conference on Natural Language Processing (Volume 1: Long
Papers)</em>, pages 320–336, Online only. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richens and Everitt (2024)</span>
<span class="ltx_bibblock">
Jonathan Richens and Tom Everitt. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=pOoKI3ouv1" title="">Robust agents
learn causal world models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">The Twelfth International Conference on Learning
Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samsami et al. (2024)</span>
<span class="ltx_bibblock">
Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar.
2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=1vDArHJ68h" title="">Mastering memory
tasks with world models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">The Twelfth International Conference on Learning
Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas
Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Atomic: An atlas of machine commonsense for if-then reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume 33, pages 3027–3035.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sekar et al. (2020)</span>
<span class="ltx_bibblock">
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner,
and Deepak Pathak. 2020.

</span>
<span class="ltx_bibblock">Planning to explore via self-supervised world models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">International conference on machine learning</em>, pages
8583–8592. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjong Kim Sang and
De Meulder (2003)</span>
<span class="ltx_bibblock">
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W03-0419" title="">Introduction to the
CoNLL-2003 shared task: Language-independent named entity recognition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003</em>, pages 142–147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valmeekam et al. (2023)</span>
<span class="ltx_bibblock">
Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=gGQfkyb0KL" title="">Investigating the
effectiveness of self-critiquing in LLMs solving planning tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">NeurIPS 2023 Foundation Models for Decision Making
Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté,
Peter Clark, and Peter Jansen. 2024.

</span>
<span class="ltx_bibblock">Can language models serve as text-based world simulators?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2406.06485</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="">Transformers:
State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Te-Lin Wu, Caiqi Zhang, Qingyuan Hu, Alexander Spangher, and Nanyun Peng. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.170" title="">Learning
action conditions from instructional manuals for instruction understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3023–3043,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang et al. (2024)</span>
<span class="ltx_bibblock">
Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and
Zhiting Hu. 2024.

</span>
<span class="ltx_bibblock">Language models meet world models: Embodied experiences enhance
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Zhongwei Yu, Jingqing Ruan, and Dengpeng Xing. 2023.

</span>
<span class="ltx_bibblock">Explainable reinforcement learning via a causal world model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence</em>, pages 4540–4548.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Lunjun Zhang, Ge Yang, and Bradly C Stadie. 2021.

</span>
<span class="ltx_bibblock">World model as a graph: Learning latent landmarks for planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">International conference on machine learning</em>, pages
12611–12620. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, and Jun Wang.
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.516" title="">How do large
language models capture the ever-changing world knowledge? a review of recent
advances</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing</em>, pages 8289–8311, Singapore. Association for
Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We use Hugging Face Transformers<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers" title="">https://github.com/huggingface/transformers</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib36" title="">2020</a>)</cite> during implementation.
We train the FLAN-T5-large models <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib5" title="">2024</a>)</cite> using AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2409.12278v2#bib.bib18" title="">2019</a>)</cite> with the default learning rate linearly decaying from <math alttext="5E-5" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mrow id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mn id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">5</mn><mo id="A1.p1.1.m1.1.1.2.1" xref="A1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3.cmml">E</mi></mrow><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">−</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><minus id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></minus><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><times id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2.1"></times><cn id="A1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2.2">5</cn><ci id="A1.p1.1.m1.1.1.2.3.cmml" xref="A1.p1.1.m1.1.1.2.3">𝐸</ci></apply><cn id="A1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">5E-5</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">5 italic_E - 5</annotation></semantics></math>.
All models are trained for 50 epochs on an NVIDIA TITAN Xp GPU.
During inference, we perform greedy decoding.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompts for Global-Local Prompting</h2>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The prompts we use in global-local prompting (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S3.SS1" title="3.1 Precondition/Effect Inference Module ‣ 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">subsection 3.1</span></a>). We omit the output-format controlling prompts for brevity.</figcaption>
<table class="ltx_tabular" id="A2.T3.1">
<tr class="ltx_tr" id="A2.T3.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.T3.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.1.1">
<span class="ltx_p" id="A2.T3.1.1.1.1.1" style="width:390.3pt;">Step #1: (we denote the model output as $model_output_step_1)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.2.1.1">
<span class="ltx_p" id="A2.T3.1.2.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.2.1.1.1.1">Give me a series of action steps that are generally involved in $domain_and_task_description.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.3.1.1">
<span class="ltx_p" id="A2.T3.1.3.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.3.1.1.1.1">Steps must have a strong dependency on each other.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.4.1.1">
<span class="ltx_p" id="A2.T3.1.4.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.4.1.1.1.1">Steps must be grounded in a specific concrete environment.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.5.1.1">
<span class="ltx_p" id="A2.T3.1.5.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.5.1.1.1.1">A series of action steps:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.6.1.1">
<span class="ltx_p" id="A2.T3.1.6.1.1.1" style="width:390.3pt;">Step #2: (we denote the model output as $model_output_step_2)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.7.1.1">
<span class="ltx_p" id="A2.T3.1.7.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.7.1.1.1.1">You will be given full action steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.8.1.1">
<span class="ltx_p" id="A2.T3.1.8.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.8.1.1.1.1">Identify and discard the action steps that you believe are isolated from and not quite dependent on other steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.9.1.1">
<span class="ltx_p" id="A2.T3.1.9.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.9.1.1.1.1">After discarding steps, if you find that without the discarded steps the full action steps lack coherence, you can optionally add new action steps in the place where you discard action steps, to make the full action steps look more coherent.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.10.1.1">
<span class="ltx_p" id="A2.T3.1.10.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.10.1.1.1.1">Unlike the discarded action steps, the new action steps you add must have a strong dependency on other existing steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.11.1.1">
<span class="ltx_p" id="A2.T3.1.11.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.11.1.1.1.1">Below are some examples of how to identify and discard isolated and independent action steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.12.1.1">
<span class="ltx_p" id="A2.T3.1.12.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.12.1.1.1.1">$few_shot_examples_step_2</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.13.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.13.1.1">
<span class="ltx_p" id="A2.T3.1.13.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.13.1.1.1.1">Full action steps:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.14.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.14.1.1">
<span class="ltx_p" id="A2.T3.1.14.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.14.1.1.1.1">$model_output_step_1</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.15.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.15.1.1">
<span class="ltx_p" id="A2.T3.1.15.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.15.1.1.1.1">Full action steps after discarding and adding steps:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.16.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.16.1.1">
<span class="ltx_p" id="A2.T3.1.16.1.1.1" style="width:390.3pt;">Step #3: (we denote the model output as $model_output_step_3)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.17.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.17.1.1">
<span class="ltx_p" id="A2.T3.1.17.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.17.1.1.1.1">You will be given full action steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.18.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.18.1.1">
<span class="ltx_p" id="A2.T3.1.18.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.18.1.1.1.1">For each action step, independently infer all of its preconditions and effects which might comprise multiple sentences.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.19.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.19.1.1">
<span class="ltx_p" id="A2.T3.1.19.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.19.1.1.1.1">The precondition is the state that must be made true before action execution.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.20.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.20.1.1">
<span class="ltx_p" id="A2.T3.1.20.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.20.1.1.1.1">The effect is the state achieved after action execution.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.21.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.21.1.1">
<span class="ltx_p" id="A2.T3.1.21.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.21.1.1.1.1">Be as accurate and precise as possible.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.22.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.22.1.1">
<span class="ltx_p" id="A2.T3.1.22.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.22.1.1.1.1">Note that there is no need to make the precondition of step N identical to the effect of step N-1.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.23">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.23.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.23.1.1">
<span class="ltx_p" id="A2.T3.1.23.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.23.1.1.1.1">Full action steps:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.24">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.24.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.24.1.1">
<span class="ltx_p" id="A2.T3.1.24.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.24.1.1.1.1">$model_output_step_2</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.25">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.25.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.25.1.1">
<span class="ltx_p" id="A2.T3.1.25.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.25.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.26">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.26.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.26.1.1">
<span class="ltx_p" id="A2.T3.1.26.1.1.1" style="width:390.3pt;">Step #4.1: (we denote the model output as $model_output_step_4.1)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.27">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.27.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.27.1.1">
<span class="ltx_p" id="A2.T3.1.27.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.27.1.1.1.1">You will be given full action steps with preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.28">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.28.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.28.1.1">
<span class="ltx_p" id="A2.T3.1.28.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.28.1.1.1.1">Identify the action steps (along with their preconditions and effects) that you believe have preconditions and effects which are isolated from and not quite dependent on other steps’ preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.29">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.29.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.29.1.1">
<span class="ltx_p" id="A2.T3.1.29.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.29.1.1.1.1">Below are some examples of how to identify isolated and independent action steps.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.30">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.30.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.30.1.1">
<span class="ltx_p" id="A2.T3.1.30.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.30.1.1.1.1">$few_shot_examples_step_4.1</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.31">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.31.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.31.1.1">
<span class="ltx_p" id="A2.T3.1.31.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.31.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.32">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.32.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.32.1.1">
<span class="ltx_p" id="A2.T3.1.32.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.32.1.1.1.1">$model_output_step_3</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.33">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.33.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.33.1.1">
<span class="ltx_p" id="A2.T3.1.33.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.33.1.1.1.1">Identified action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.34">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.34.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.34.1.1">
<span class="ltx_p" id="A2.T3.1.34.1.1.1" style="width:390.3pt;">Step #4.2: (we denote the model output as $model_output_step_4.2)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.35">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.35.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.35.1.1">
<span class="ltx_p" id="A2.T3.1.35.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.35.1.1.1.1">You will be given full action steps with preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.36">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.36.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.36.1.1">
<span class="ltx_p" id="A2.T3.1.36.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.36.1.1.1.1">You will also be given identified action steps with preconditions and effects that are isolated from and not quite dependent on other steps’ preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.37">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.37.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.37.1.1">
<span class="ltx_p" id="A2.T3.1.37.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.37.1.1.1.1">Rethink and regenerate the preconditions and effects of these identified action steps, to make them more dependent on other steps’ preconditions and effects than before.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.38">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.38.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.38.1.1">
<span class="ltx_p" id="A2.T3.1.38.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.38.1.1.1.1">The precondition is the state that must be made true before action execution.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.39">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.39.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.39.1.1">
<span class="ltx_p" id="A2.T3.1.39.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.39.1.1.1.1">The effect is the state achieved after action execution.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.40">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.40.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.40.1.1">
<span class="ltx_p" id="A2.T3.1.40.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.40.1.1.1.1">Be as accurate and precise as possible.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.41">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.41.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.41.1.1">
<span class="ltx_p" id="A2.T3.1.41.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.41.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.42">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.42.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.42.1.1">
<span class="ltx_p" id="A2.T3.1.42.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.42.1.1.1.1">$model_output_step_3</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.43">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.43.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.43.1.1">
<span class="ltx_p" id="A2.T3.1.43.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.43.1.1.1.1">Identified action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.44">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.44.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.44.1.1">
<span class="ltx_p" id="A2.T3.1.44.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.44.1.1.1.1">$model_output_step_4.1</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.45">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.45.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.45.1.1">
<span class="ltx_p" id="A2.T3.1.45.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.45.1.1.1.1">Identified action steps with newly generated preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.46">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.46.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.46.1.1">
<span class="ltx_p" id="A2.T3.1.46.1.1.1" style="width:390.3pt;">Step #4.3: (we denote the model output as $model_output_step_4.3)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.47">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.47.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.47.1.1">
<span class="ltx_p" id="A2.T3.1.47.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.47.1.1.1.1">You will be given full action steps with preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.48">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.48.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.48.1.1">
<span class="ltx_p" id="A2.T3.1.48.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.48.1.1.1.1">You will also be given identified action steps with preconditions and effects that are isolated from and not quite dependent on other steps’ preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.49">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.49.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.49.1.1">
<span class="ltx_p" id="A2.T3.1.49.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.49.1.1.1.1">You will also be given identified action steps with newly generated preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.50">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.50.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.50.1.1">
<span class="ltx_p" id="A2.T3.1.50.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.50.1.1.1.1">Categorize these identified action steps (with newly generated preconditions and effects) into the following two groups: (1) identified action steps whose newly generated preconditions and effects look dependent on other steps’ preconditions and effects; (2) identified action steps whose newly generated preconditions and effects look not quite dependent on other steps’ preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.51">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.51.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.51.1.1">
<span class="ltx_p" id="A2.T3.1.51.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.51.1.1.1.1">Below are some examples of how to do this categorization.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.52">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.52.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.52.1.1">
<span class="ltx_p" id="A2.T3.1.52.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.52.1.1.1.1">$few_shot_examples_step_4.3</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.53">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.53.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.53.1.1">
<span class="ltx_p" id="A2.T3.1.53.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.53.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.54">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.54.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.54.1.1">
<span class="ltx_p" id="A2.T3.1.54.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.54.1.1.1.1">$model_output_step_3</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.55">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.55.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.55.1.1">
<span class="ltx_p" id="A2.T3.1.55.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.55.1.1.1.1">Identified action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.56">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.56.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.56.1.1">
<span class="ltx_p" id="A2.T3.1.56.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.56.1.1.1.1">$model_output_step_4.1</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.57">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.57.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.57.1.1">
<span class="ltx_p" id="A2.T3.1.57.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.57.1.1.1.1">Identified action steps with newly generated preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.58">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.58.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.58.1.1">
<span class="ltx_p" id="A2.T3.1.58.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.58.1.1.1.1">$model_output_step_4.2</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.59">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.59.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.59.1.1">
<span class="ltx_p" id="A2.T3.1.59.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.59.1.1.1.1">Categorization:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.60">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.60.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.60.1.1">
<span class="ltx_p" id="A2.T3.1.60.1.1.1" style="width:390.3pt;">Step #5.1:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.61">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.61.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.61.1.1">
<span class="ltx_p" id="A2.T3.1.61.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.61.1.1.1.1">You will be given full action steps with preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.62">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.62.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.62.1.1">
<span class="ltx_p" id="A2.T3.1.62.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.62.1.1.1.1">Find preconditions that are semantically not covered in any of the effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.63">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.63.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.63.1.1">
<span class="ltx_p" id="A2.T3.1.63.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.63.1.1.1.1">Semantic coverage means there exists at least one effect item that expresses the semantically equivalent meaning as the precondition item.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.64">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.64.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.64.1.1">
<span class="ltx_p" id="A2.T3.1.64.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.64.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.65">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.65.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.65.1.1">
<span class="ltx_p" id="A2.T3.1.65.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.65.1.1.1.1">$model_output_step_4.3_after_post_processing</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.66">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.66.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.66.1.1">
<span class="ltx_p" id="A2.T3.1.66.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.66.1.1.1.1">Preconditions that are semantically not covered in any of the effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.67">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.67.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.67.1.1">
<span class="ltx_p" id="A2.T3.1.67.1.1.1" style="width:390.3pt;">Step #5.2:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.68">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.68.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.68.1.1">
<span class="ltx_p" id="A2.T3.1.68.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.68.1.1.1.1">You will be given full action steps with preconditions and effects.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.69">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.69.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.69.1.1">
<span class="ltx_p" id="A2.T3.1.69.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.69.1.1.1.1">Find effects that are semantically not covered in any of the preconditions.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.70">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.70.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.70.1.1">
<span class="ltx_p" id="A2.T3.1.70.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.70.1.1.1.1">Semantic coverage means there exists at least one precondition item that expresses the semantically equivalent meaning as the effect item.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.71">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.71.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.71.1.1">
<span class="ltx_p" id="A2.T3.1.71.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.71.1.1.1.1">Full action steps with preconditions and effects:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.72">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.72.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.72.1.1">
<span class="ltx_p" id="A2.T3.1.72.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.72.1.1.1.1">$model_output_step_4.3_after_post_processing</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.73">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A2.T3.1.73.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.73.1.1">
<span class="ltx_p" id="A2.T3.1.73.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.73.1.1.1.1">Effects that are semantically not covered in any of the preconditions:</span></span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Prompts for Semantic State Matching</h2>
<figure class="ltx_table" id="A3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The prompts we use in semantic state matching (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.12278v2#S3.SS2" title="3.2 Semantic State Matching ‣ 3 World Model ‣ Making Large Language Models into World Models with Precondition and Effect Knowledge"><span class="ltx_text ltx_ref_tag">subsection 3.2</span></a>). We omit the output-format controlling prompts for brevity.</figcaption>
<table class="ltx_tabular" id="A3.T4.1">
<tr class="ltx_tr" id="A3.T4.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A3.T4.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.1.1.1">
<span class="ltx_p" id="A3.T4.1.1.1.1.1" style="width:390.3pt;">Prompts used in the semantic state matching for valid action prediction:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.2.1.1">
<span class="ltx_p" id="A3.T4.1.2.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.2.1.1.1.1">You will be given some precondition items.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.3.1.1">
<span class="ltx_p" id="A3.T4.1.3.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.3.1.1.1.1">You will also be given some world-state items.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.4.1.1">
<span class="ltx_p" id="A3.T4.1.4.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.4.1.1.1.1">For each precondition item, determine if it is semantically covered by the world-state items.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.5.1.1">
<span class="ltx_p" id="A3.T4.1.5.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.5.1.1.1.1">Semantic coverage means there exists at least one world-state item that expresses the semantically equivalent meaning as the precondition item.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.6.1.1">
<span class="ltx_p" id="A3.T4.1.6.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.6.1.1.1.1">If all the precondition items are semantically covered, return TRUE; otherwise, return FALSE.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.7.1.1">
<span class="ltx_p" id="A3.T4.1.7.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.7.1.1.1.1">Precondition items:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.8.1.1">
<span class="ltx_p" id="A3.T4.1.8.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.8.1.1.1.1">$inferred_action_preconditions</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.9.1.1">
<span class="ltx_p" id="A3.T4.1.9.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.9.1.1.1.1">World-state items:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.10.1.1">
<span class="ltx_p" id="A3.T4.1.10.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.10.1.1.1.1">$current_world_state</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A3.T4.1.11.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.11.1.1">
<span class="ltx_p" id="A3.T4.1.11.1.1.1" style="width:390.3pt;">Prompts used in the semantic state matching for state transition prediction:</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.12.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.12.1.1">
<span class="ltx_p" id="A3.T4.1.12.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.12.1.1.1.1">You will be given some effect items.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.13.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.13.1.1">
<span class="ltx_p" id="A3.T4.1.13.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.13.1.1.1.1">You will also be given some world-state items.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.14.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.14.1.1">
<span class="ltx_p" id="A3.T4.1.14.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.14.1.1.1.1">For each effect item, find all the world-state items that semantically contradict the effect item.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.15.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.15.1.1">
<span class="ltx_p" id="A3.T4.1.15.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.15.1.1.1.1">Effect items:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.16.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.16.1.1">
<span class="ltx_p" id="A3.T4.1.16.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.16.1.1.1.1">$inferred_action_effects</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A3.T4.1.17.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.17.1.1">
<span class="ltx_p" id="A3.T4.1.17.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.17.1.1.1.1">World-state items:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.18">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="A3.T4.1.18.1">
<span class="ltx_inline-block ltx_align_top" id="A3.T4.1.18.1.1">
<span class="ltx_p" id="A3.T4.1.18.1.1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.18.1.1.1.1">$current_world_state</span></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 23:30:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
