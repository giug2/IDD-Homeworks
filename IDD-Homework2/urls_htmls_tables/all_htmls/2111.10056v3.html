<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.10056] Medical Visual Question Answering: A Survey</title><meta property="og:description" content="Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA syst…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Medical Visual Question Answering: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Medical Visual Question Answering: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.10056">

<!--Generated on Sat Mar  9 00:23:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">[orcid=0000-0001-8499-4097]</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\cormark</span>
<p id="p2.2" class="ltx_p">[1]</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\cortext</span>
<p id="p3.2" class="ltx_p">[cor1]Corresponding author</p>
</div>
<h1 class="ltx_title ltx_title_document">Medical Visual Question Answering: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhihong Lin
</span><span class="ltx_author_notes">zhihong.lin@monash.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Donghao Zhang
</span><span class="ltx_author_notes">donghao.zhang@monash.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingyi Tao
</span><span class="ltx_author_notes">qtao002@e.ntu.edu.sg</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danli Shi
</span><span class="ltx_author_notes">shidli@mail2.sysu.edu.cn</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gholamreza Haffari
</span><span class="ltx_author_notes">gholamreza.haffari@monash.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qi Wu
</span><span class="ltx_author_notes">qi.wu01@adelaide.edu.au</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingguang He
</span><span class="ltx_author_notes">mingguang.he@unimelb.edu.au</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zongyuan Ge
</span><span class="ltx_author_notes">zongyuan.ge@monash.edu
<span class="ltx_contact ltx_role_address">Faculty of Engineering, Monash University, Clayton, VIC, 3800 Australia
</span>
<span class="ltx_contact ltx_role_address">eResearch Center, Monash University, Clayton, VIC, 3800 Australia
</span>
<span class="ltx_contact ltx_role_address">NVIDIA AI Technology Center, 038988, Singapore
</span>
<span class="ltx_contact ltx_role_address">State Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun Yat-Sen University, Guangzhou, 510060 China
</span>
<span class="ltx_contact ltx_role_address">Faculty of Information Technology, Monash University, Clayton, 3800, VIC, Australia
</span>
<span class="ltx_contact ltx_role_address">Australian Centre for Robotic Vision, The University of Adelaide, Adelaide, SA 5005, Australia
</span>
<span class="ltx_contact ltx_role_address">Eye Research Australia, Royal Victorian Eye and Ear Hospital, East Melbourne, VIC, 3002 Australia
</span>
<span class="ltx_contact ltx_role_address">Airdoc Research, Melbourne, VIC, 3000 Australia
</span>
<span class="ltx_contact ltx_role_address">Monash-NVIDIA AI Tech Centre, Melbourne, VIC, 3000 Australia
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features.
In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answering field and encourage them to conduct further research in this field.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Visual Question Answering <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>Medical Image Interpretation <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>Computer Vision <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>Natural Language Processing

</div>
<div id="p4" class="ltx_para">
<span id="p4.1" class="ltx_ERROR undefined">{highlights}</span>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">It is the first medical VQA survey paper as the current increasing application demand on medical VQA systems. It describes the history of medical VQA and research directions in the future.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">This survey presents an overview of the publicly available medical VQA datasets. The tasks include previous ImageCLEF VQA-Med challenges and other published datasets. This will help researchers to identify suitable research benchmarks and metrics.</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">This survey gives a comprehensive summary and discussion of the published method papers for the medical VQA. It provides the comparison and discussion for the competition work notes and research papers. These will help researchers to better understand model design principles in common and conduct further research.</p>
</div>
<div id="p8" class="ltx_para">
<p id="p8.1" class="ltx_p">It concludes some current challenges and future research directions. The common core of these challenges is the final application in the clinical scenario. The research topics cover dataset design to human-computer interaction.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Questing Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is a multidisciplinary problem that incorporates computer vision (CV) and natural language processing (NLP). The VQA system is expected to answer an image-related question according to the image content. Inspired by the VQA research in the general domain, the recent exploration of medical VQA has attracted great interest. The medical VQA system is expected to assist in clinical decision-making and improve patient engagement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Unlike other medical AI applications often restricted to pre-defined diseases or organ types, the medical VQA can understand free-form questions in natural language and provide reliable and user-friendly answers.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent research, the medical VQA has been assigned to several “jobs”. The first one is the diagnostic radiologist, who acts as an expert consultant to the referring physician. A workload study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">62</span></a>]</cite> shows that the average radiologist has to interpret one CT or MRI image in 3 to 4 seconds. Besides the long queue of imaging studies, a radiologist must also answer an average of 27 phone calls per day from physicians and patients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, leading to further inefficiencies and disruptions in the workflow. A medical VQA system can potentially answer the physician’s questions and help relieve the burden of the healthcare system and improve medical professionals’ efficiency. Another application matching the advantage of VQA is to act as the pathologists who examine body tissues and help other healthcare providers make diagnoses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In addition to the health professional role, the medical VQA system can also serve as a knowledgeable assistant. For example, the “second opinion” from the VQA system can support the clinicians’ opinion in interpreting medical images and decrease the risk of misdiagnosis at the same time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Ultimately, a mature and complete medical VQA system can directly review patients’ images and answer any kind of questions. In some situations, such as fully automated health examinations, where medical professionals may not be available, a VQA system can provide equivalent consultation. After a hospital visit, patients search for further information online. The irregular and misleading information from the search engine might result in inappropriate answers. Alternatively, a medical VQA can be integrated into an online consultation system to provide reliable answers anytime and anywhere.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Medical VQA is technically more challenging than general-domain VQA because of the following factors. Firstly, creating a large-scale medical VQA dataset is challenging because expert annotation is expensive for its high requirement of professional knowledge, and QA pairs can not be synthetically generated directly from images. Secondly, answering questions according to a medical image also demands a specific design of the VQA model. The task also needs to focus on a fine-grained scale because a lesion is microscopic. Hence, segmentation techniques may be required to locate the region of interest precisely. Finally, a question can be very professional, which requires the model to be trained with medical knowledge base rather than a general language database.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Since the first medical VQA challenge was organized in 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, an increasing number of organizations and researchers have joined to expand the tasks and propose new datasets and approaches, which have made the medical VQA task an active and inspiring field. To provide a comprehensive retrospect of these efforts, we conduct the first survey (to our best knowledge) for medical VQA.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In the first part of this survey, we overview the publicly available medical VQA datasets up-to-date.
To collect the most complete information, we did an exhaustive search for the available medical VQA datasets including sources from relevant papers in google scholar, medical image computing conferences, and top-tier journals, and resulted in a total of 10 papers proposing datasets. Two dataset papers are repetitive and different versions of included datasets, and consequently, 8 datasets are analyzed and discussed in this survey. Among them, three datasets are proposed as ImageCLEF<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_framed ltx_framed_underline">https://www.imageclef.org/</span></span></span></span> competitions. The selected datasets are diverse in image modality and question categories. The imaging modality of those datasets covers chest X-ray, CT, MRI, and pathology. The questions include close-end questions (such as Yes/No questions) and open-end questions on a variety of topics. We also compare the data sources, the question-answer pairs creation methods, and the metrics for evaluation. These will be inspiring for researchers who are interested in designing new tasks.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In the second part of this survey, we review the published approaches to medical VQA. We gather the work notes describing the approaches used in the ImageCLEF VQA-Med competitions and collect 32 papers in total. However, the work notes are mainly simple solutions because of the time-limited situation. We also search for technical papers from conferences or journals that aim at the current pain point and we collect 13 papers. The papers’ sources are from both the community conferences such as Medical Image Computing and Computer Assisted Intervention (MICCAI) and Association for Computing Machinery Multimedia (ACM-MM) and influential journals such as the IEEE Transactions on Medical Imaging. By reviewing those papers, we find that the current approaches are mostly in a framework of four components: image encoder, language encoder, feature fusion module, and answering module. The review of existing approaches will help researchers to identify the key problem in previous research and the potential hypothesis in future research.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Finally, we discuss four medical-specific challenges for the field. The medical-domain VQA is a more application-oriented problem compared with the general-domain VQA. It has a real application scenario that will produce practical challenges. In this work, we analyze the clinical requirements to develop practical and useful applications and raise six significant challenges: the question diversity, extra medical information, interpretability, generalizability, large language models, and integration in the medical workflow. The proposed challenges will inspire researchers and develop mature and accurate medical VQA systems to support the clinical decision-making process.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets and performance metrics</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To the best of our knowledge, there are 8 public-available medical VQA datasets up to date: VQA-MED-2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, VQA-MED-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, RadVisDial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, PathVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, VQA-MED-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, and VQA-MED-2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> (in chronological order). The dataset’s details are summarized in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In the following paragraphs, we provide an overview of the QA pairs collection.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.4.1.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S2.T1.5.2" class="ltx_text" style="font-size:129%;">Overview of the medical VQA datasets and their main characteristics. Visual Genome, VQA 2.0, and OK-VQA are general-domain VQA datasets listed here for comparison. The medical VQA datasets are presented in chronological order.</span></figcaption>
<table id="S2.T1.6" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.6.1" class="ltx_tr">
<td id="S2.T1.6.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td id="S2.T1.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.1.2.1.1.1" class="ltx_text" style="font-size:70%;"># Images</span></span>
</span>
</td>
<td id="S2.T1.6.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.1.3.1.1.1" class="ltx_text" style="font-size:70%;"># QA pairs</span></span>
</span>
</td>
<td id="S2.T1.6.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Source of images</span></span>
<span id="S2.T1.6.1.4.1.2" class="ltx_p"><span id="S2.T1.6.1.4.1.2.1" class="ltx_text" style="font-size:70%;">and content</span></span>
</span>
</td>
<td id="S2.T1.6.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.1.5.1.1.1" class="ltx_text" style="font-size:70%;">QA</span></span>
<span id="S2.T1.6.1.5.1.2" class="ltx_p"><span id="S2.T1.6.1.5.1.2.1" class="ltx_text" style="font-size:70%;">Creation</span></span>
</span>
</td>
<td id="S2.T1.6.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.6.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.1.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.1.6.1.1.1" class="ltx_text" style="font-size:70%;">Question Category</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.2" class="ltx_tr">
<td id="S2.T1.6.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.2.1.1.1.1" class="ltx_text" style="font-size:70%;">Visual Genome </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="S2.T1.6.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.2.2.1.1.1" class="ltx_text" style="font-size:70%;">108K</span></span>
</span>
</td>
<td id="S2.T1.6.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.2.3.1.1.1" class="ltx_text" style="font-size:70%;">1,773K</span></span>
</span>
</td>
<td id="S2.T1.6.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.2.4.1.1.1" class="ltx_text" style="font-size:70%;">YFCC100M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.2.4.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">87</span></a><span id="S2.T1.6.2.4.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
<span id="S2.T1.6.2.4.1.2" class="ltx_p"><span id="S2.T1.6.2.4.1.2.1" class="ltx_text" style="font-size:70%;">Microsoft COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.2.4.1.2.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="S2.T1.6.2.4.1.2.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.2.5.1.1.1" class="ltx_text" style="font-size:70%;">Manual</span></span>
</span>
</td>
<td id="S2.T1.6.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.2.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.2.6.1.1.1" class="ltx_text" style="font-size:70%;">- Object</span></span>
<span id="S2.T1.6.2.6.1.2" class="ltx_p"><span id="S2.T1.6.2.6.1.2.1" class="ltx_text" style="font-size:70%;">- Attibutes</span></span>
<span id="S2.T1.6.2.6.1.3" class="ltx_p"><span id="S2.T1.6.2.6.1.3.1" class="ltx_text" style="font-size:70%;">- Relationships</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.3" class="ltx_tr">
<td id="S2.T1.6.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.3.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA 2.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.T1.6.3.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.3.2.1.1.1" class="ltx_text" style="font-size:70%;">204K</span></span>
</span>
</td>
<td id="S2.T1.6.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.3.3.1.1.1" class="ltx_text" style="font-size:70%;">614K</span></span>
</span>
</td>
<td id="S2.T1.6.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.3.4.1.1.1" class="ltx_text" style="font-size:70%;">Microsoft COCO</span></span>
</span>
</td>
<td id="S2.T1.6.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Manually</span></span>
</span>
</td>
<td id="S2.T1.6.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.3.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.3.6.1.1.1" class="ltx_text" style="font-size:70%;">- Object</span></span>
<span id="S2.T1.6.3.6.1.2" class="ltx_p"><span id="S2.T1.6.3.6.1.2.1" class="ltx_text" style="font-size:70%;">- Color</span></span>
<span id="S2.T1.6.3.6.1.3" class="ltx_p"><span id="S2.T1.6.3.6.1.3.1" class="ltx_text" style="font-size:70%;">- Sport</span></span>
<span id="S2.T1.6.3.6.1.4" class="ltx_p"><span id="S2.T1.6.3.6.1.4.1" class="ltx_text" style="font-size:70%;">- Count</span></span>
<span id="S2.T1.6.3.6.1.5" class="ltx_p"><span id="S2.T1.6.3.6.1.5.1" class="ltx_text" style="font-size:70%;">- etc.</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.4" class="ltx_tr">
<td id="S2.T1.6.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.4.1.1.1.1" class="ltx_text" style="font-size:70%;">OK-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">61</span></a><span id="S2.T1.6.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.4.2.1.1.1" class="ltx_text" style="font-size:70%;">14,031</span></span>
</span>
</td>
<td id="S2.T1.6.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.4.3.1.1.1" class="ltx_text" style="font-size:70%;">14,055</span></span>
</span>
</td>
<td id="S2.T1.6.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.4.4.1.1.1" class="ltx_text" style="font-size:70%;">Microsoft COCO</span></span>
</span>
</td>
<td id="S2.T1.6.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Manual</span></span>
</span>
</td>
<td id="S2.T1.6.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.4.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.4.6.1.1.1" class="ltx_text" style="font-size:70%;">- External knowledge</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.5" class="ltx_tr">
<td id="S2.T1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA-Med-2018 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S2.T1.6.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">2,866</span></span>
</span>
</td>
<td id="S2.T1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.5.3.1.1.1" class="ltx_text" style="font-size:70%;">6,413</span></span>
</span>
</td>
<td id="S2.T1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.5.4.1.1.1" class="ltx_text" style="font-size:70%;">PubMed Central Articles</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnote2.1" class="ltx_text ltx_framed ltx_framed_underline">https://www.ncbi.nlm.nih.gov/pmc/</span></span></span></span></span>
</span>
</td>
<td id="S2.T1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.5.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.5.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.5.6.1.1.1" class="ltx_text" style="font-size:70%;">- Location</span></span>
<span id="S2.T1.6.5.6.1.2" class="ltx_p"><span id="S2.T1.6.5.6.1.2.1" class="ltx_text" style="font-size:70%;">- Finding</span></span>
<span id="S2.T1.6.5.6.1.3" class="ltx_p"><span id="S2.T1.6.5.6.1.3.1" class="ltx_text" style="font-size:70%;">- Yes/No questions</span></span>
<span id="S2.T1.6.5.6.1.4" class="ltx_p"><span id="S2.T1.6.5.6.1.4.1" class="ltx_text" style="font-size:70%;">- Other questions</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.6" class="ltx_tr">
<td id="S2.T1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.6.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA-RAD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S2.T1.6.6.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.6.2.1.1.1" class="ltx_text" style="font-size:70%;">315</span></span>
</span>
</td>
<td id="S2.T1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.6.3.1.1.1" class="ltx_text" style="font-size:70%;">3,515</span></span>
</span>
</td>
<td id="S2.T1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.6.4.1.1.1" class="ltx_text" style="font-size:70%;">MedPix database:</span></span>
<span id="S2.T1.6.6.4.1.2" class="ltx_p"><span id="S2.T1.6.6.4.1.2.1" class="ltx_text" style="font-size:70%;">- Head axial single-slice CTs or MRIs</span></span>
<span id="S2.T1.6.6.4.1.3" class="ltx_p"><span id="S2.T1.6.6.4.1.3.1" class="ltx_text" style="font-size:70%;">- Chest X-rays</span></span>
<span id="S2.T1.6.6.4.1.4" class="ltx_p"><span id="S2.T1.6.6.4.1.4.1" class="ltx_text" style="font-size:70%;">- Abdominal axial CTs</span></span>
</span>
</td>
<td id="S2.T1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.6.5.1.1.1" class="ltx_text" style="font-size:70%;">Manual</span></span>
</span>
</td>
<td id="S2.T1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.6.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.6.6.1.1.1" class="ltx_text" style="font-size:70%;">- Modality</span></span>
<span id="S2.T1.6.6.6.1.2" class="ltx_p"><span id="S2.T1.6.6.6.1.2.1" class="ltx_text" style="font-size:70%;">- Plane</span></span>
<span id="S2.T1.6.6.6.1.3" class="ltx_p"><span id="S2.T1.6.6.6.1.3.1" class="ltx_text" style="font-size:70%;">- Organ System</span></span>
<span id="S2.T1.6.6.6.1.4" class="ltx_p"><span id="S2.T1.6.6.6.1.4.1" class="ltx_text" style="font-size:70%;">- Abnormality</span></span>
<span id="S2.T1.6.6.6.1.5" class="ltx_p"><span id="S2.T1.6.6.6.1.5.1" class="ltx_text" style="font-size:70%;">- Object/Condition Presence</span></span>
<span id="S2.T1.6.6.6.1.6" class="ltx_p"><span id="S2.T1.6.6.6.1.6.1" class="ltx_text" style="font-size:70%;">- Positional Reasoning</span></span>
<span id="S2.T1.6.6.6.1.7" class="ltx_p"><span id="S2.T1.6.6.6.1.7.1" class="ltx_text" style="font-size:70%;">- Color</span></span>
<span id="S2.T1.6.6.6.1.8" class="ltx_p"><span id="S2.T1.6.6.6.1.8.1" class="ltx_text" style="font-size:70%;">- Size</span></span>
<span id="S2.T1.6.6.6.1.9" class="ltx_p"><span id="S2.T1.6.6.6.1.9.1" class="ltx_text" style="font-size:70%;">- Attribute Other</span></span>
<span id="S2.T1.6.6.6.1.10" class="ltx_p"><span id="S2.T1.6.6.6.1.10.1" class="ltx_text" style="font-size:70%;">- Counting</span></span>
<span id="S2.T1.6.6.6.1.11" class="ltx_p"><span id="S2.T1.6.6.6.1.11.1" class="ltx_text" style="font-size:70%;">- Other</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.7" class="ltx_tr">
<td id="S2.T1.6.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.7.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA-Med-2019 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.7.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.T1.6.7.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.7.2.1.1.1" class="ltx_text" style="font-size:70%;">4,200</span></span>
</span>
</td>
<td id="S2.T1.6.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.7.3.1.1.1" class="ltx_text" style="font-size:70%;">15,292</span></span>
</span>
</td>
<td id="S2.T1.6.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.7.4.1.1.1" class="ltx_text" style="font-size:70%;">MedPix database:</span></span>
<span id="S2.T1.6.7.4.1.2" class="ltx_p"><span id="S2.T1.6.7.4.1.2.1" class="ltx_text" style="font-size:70%;">- Various in 36 modalities, 16 planes, and 10 organ systems</span></span>
</span>
</td>
<td id="S2.T1.6.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.7.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.7.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.7.6.1.1.1" class="ltx_text" style="font-size:70%;">- Modality</span></span>
<span id="S2.T1.6.7.6.1.2" class="ltx_p"><span id="S2.T1.6.7.6.1.2.1" class="ltx_text" style="font-size:70%;">- Plane</span></span>
<span id="S2.T1.6.7.6.1.3" class="ltx_p"><span id="S2.T1.6.7.6.1.3.1" class="ltx_text" style="font-size:70%;">- Organ system</span></span>
<span id="S2.T1.6.7.6.1.4" class="ltx_p"><span id="S2.T1.6.7.6.1.4.1" class="ltx_text" style="font-size:70%;">- Abnormality</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.8" class="ltx_tr">
<td id="S2.T1.6.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.8.1.1.1.1" class="ltx_text" style="font-size:70%;">RadVisDial </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.8.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S2.T1.6.8.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
<span id="S2.T1.6.8.1.1.2" class="ltx_p"><span id="S2.T1.6.8.1.1.2.1" class="ltx_text" style="font-size:70%;">(Silver-standard)</span></span>
</span>
</td>
<td id="S2.T1.6.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.8.2.1.1.1" class="ltx_text" style="font-size:70%;">91,060</span></span>
</span>
</td>
<td id="S2.T1.6.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.8.3.1.1.1" class="ltx_text" style="font-size:70%;">455,300</span></span>
</span>
</td>
<td id="S2.T1.6.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.8.4.1.1.1" class="ltx_text" style="font-size:70%;">MIMIC-CXR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.8.4.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S2.T1.6.8.4.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T1.6.8.4.1.1.4" class="ltx_text" style="font-size:70%;">:</span></span>
<span id="S2.T1.6.8.4.1.2" class="ltx_p"><span id="S2.T1.6.8.4.1.2.1" class="ltx_text" style="font-size:70%;">- Chest X-ray posterior-anterior (PA) view</span></span>
</span>
</td>
<td id="S2.T1.6.8.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.8.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.8.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.8.6.1.1.1" class="ltx_text" style="font-size:70%;">Abnormality</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.9" class="ltx_tr">
<td id="S2.T1.6.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.9.1.1.1.1" class="ltx_text" style="font-size:70%;">RadVisDial </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.9.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S2.T1.6.9.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
<span id="S2.T1.6.9.1.1.2" class="ltx_p"><span id="S2.T1.6.9.1.1.2.1" class="ltx_text" style="font-size:70%;">(Gold-standard)</span></span>
</span>
</td>
<td id="S2.T1.6.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.9.2.1.1.1" class="ltx_text" style="font-size:70%;">100</span></span>
</span>
</td>
<td id="S2.T1.6.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.9.3.1.1.1" class="ltx_text" style="font-size:70%;">500</span></span>
</span>
</td>
<td id="S2.T1.6.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.9.4.1.1.1" class="ltx_text" style="font-size:70%;">MIMIC-CXR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.9.4.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S2.T1.6.9.4.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T1.6.9.4.1.1.4" class="ltx_text" style="font-size:70%;">:</span></span>
<span id="S2.T1.6.9.4.1.2" class="ltx_p"><span id="S2.T1.6.9.4.1.2.1" class="ltx_text" style="font-size:70%;">- Chest X-ray posterior-anterior (PA) view</span></span>
</span>
</td>
<td id="S2.T1.6.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.9.5.1.1.1" class="ltx_text" style="font-size:70%;">Manual</span></span>
</span>
</td>
<td id="S2.T1.6.9.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.9.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.9.6.1.1.1" class="ltx_text" style="font-size:70%;">Abnormality</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.10" class="ltx_tr">
<td id="S2.T1.6.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.10.1.1.1.1" class="ltx_text" style="font-size:70%;">PathVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.10.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.T1.6.10.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.10.2.1.1.1" class="ltx_text" style="font-size:70%;">4,998</span></span>
</span>
</td>
<td id="S2.T1.6.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.10.3.1.1.1" class="ltx_text" style="font-size:70%;">32,799</span></span>
</span>
</td>
<td id="S2.T1.6.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.10.4.1.1.1" class="ltx_text" style="font-size:70%;">Electronic pathology textbooks</span></span>
<span id="S2.T1.6.10.4.1.2" class="ltx_p"><span id="S2.T1.6.10.4.1.2.1" class="ltx_text" style="font-size:70%;">PEIR Digital Library</span></span>
</span>
</td>
<td id="S2.T1.6.10.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.10.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.10.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.10.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.10.6.1.1.1" class="ltx_text" style="font-size:70%;">- Color</span></span>
<span id="S2.T1.6.10.6.1.2" class="ltx_p"><span id="S2.T1.6.10.6.1.2.1" class="ltx_text" style="font-size:70%;">- Location</span></span>
<span id="S2.T1.6.10.6.1.3" class="ltx_p"><span id="S2.T1.6.10.6.1.3.1" class="ltx_text" style="font-size:70%;">- Appearance</span></span>
<span id="S2.T1.6.10.6.1.4" class="ltx_p"><span id="S2.T1.6.10.6.1.4.1" class="ltx_text" style="font-size:70%;">- Shape</span></span>
<span id="S2.T1.6.10.6.1.5" class="ltx_p"><span id="S2.T1.6.10.6.1.5.1" class="ltx_text" style="font-size:70%;">- etc.</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.11" class="ltx_tr">
<td id="S2.T1.6.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.11.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA-Med-2020 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.11.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.T1.6.11.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.11.2.1.1.1" class="ltx_text" style="font-size:70%;">5,000</span></span>
</span>
</td>
<td id="S2.T1.6.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.11.3.1.1.1" class="ltx_text" style="font-size:70%;">5,000</span></span>
</span>
</td>
<td id="S2.T1.6.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.11.4.1.1.1" class="ltx_text" style="font-size:70%;">MedPix database</span></span>
</span>
</td>
<td id="S2.T1.6.11.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.11.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.11.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.11.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.11.6.1.1.1" class="ltx_text" style="font-size:70%;">- Abnormality</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.12" class="ltx_tr">
<td id="S2.T1.6.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.12.1.1.1.1" class="ltx_text" style="font-size:70%;">SLAKE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.12.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S2.T1.6.12.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.12.2.1.1.1" class="ltx_text" style="font-size:70%;">642</span></span>
</span>
</td>
<td id="S2.T1.6.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.12.3.1.1.1" class="ltx_text" style="font-size:70%;">14K</span></span>
</span>
</td>
<td id="S2.T1.6.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.12.4.1.1.1" class="ltx_text" style="font-size:70%;">Medical Segmentation Decathlon</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.12.4.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">83</span></a><span id="S2.T1.6.12.4.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T1.6.12.4.1.1.4" class="ltx_text" style="font-size:70%;">,</span></span>
<span id="S2.T1.6.12.4.1.2" class="ltx_p"><span id="S2.T1.6.12.4.1.2.1" class="ltx_text" style="font-size:70%;">NIH Chest X-ray</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.12.4.1.2.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">99</span></a><span id="S2.T1.6.12.4.1.2.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T1.6.12.4.1.2.4" class="ltx_text" style="font-size:70%;">,</span></span>
<span id="S2.T1.6.12.4.1.3" class="ltx_p"><span id="S2.T1.6.12.4.1.3.1" class="ltx_text" style="font-size:70%;">CHAOS</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.12.4.1.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S2.T1.6.12.4.1.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S2.T1.6.12.4.1.3.4" class="ltx_text" style="font-size:70%;">:</span></span>
<span id="S2.T1.6.12.4.1.4" class="ltx_p"><span id="S2.T1.6.12.4.1.4.1" class="ltx_text" style="font-size:70%;">- Chest X-rays/CTs</span></span>
<span id="S2.T1.6.12.4.1.5" class="ltx_p"><span id="S2.T1.6.12.4.1.5.1" class="ltx_text" style="font-size:70%;">- Abdomen CTs/MRIs</span></span>
<span id="S2.T1.6.12.4.1.6" class="ltx_p"><span id="S2.T1.6.12.4.1.6.1" class="ltx_text" style="font-size:70%;">- Head CTs/MRIs</span></span>
<span id="S2.T1.6.12.4.1.7" class="ltx_p"><span id="S2.T1.6.12.4.1.7.1" class="ltx_text" style="font-size:70%;">- Neck CTs</span></span>
<span id="S2.T1.6.12.4.1.8" class="ltx_p"><span id="S2.T1.6.12.4.1.8.1" class="ltx_text" style="font-size:70%;">- Pelvic cavity CTs</span></span>
</span>
</td>
<td id="S2.T1.6.12.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.12.5.1.1.1" class="ltx_text" style="font-size:70%;">Manual</span></span>
</span>
</td>
<td id="S2.T1.6.12.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.6.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.12.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.12.6.1.1.1" class="ltx_text" style="font-size:70%;">- Organ</span></span>
<span id="S2.T1.6.12.6.1.2" class="ltx_p"><span id="S2.T1.6.12.6.1.2.1" class="ltx_text" style="font-size:70%;">- Position</span></span>
<span id="S2.T1.6.12.6.1.3" class="ltx_p"><span id="S2.T1.6.12.6.1.3.1" class="ltx_text" style="font-size:70%;">- Knowledge Graph</span></span>
<span id="S2.T1.6.12.6.1.4" class="ltx_p"><span id="S2.T1.6.12.6.1.4.1" class="ltx_text" style="font-size:70%;">- Abnormality</span></span>
<span id="S2.T1.6.12.6.1.5" class="ltx_p"><span id="S2.T1.6.12.6.1.5.1" class="ltx_text" style="font-size:70%;">- Modality</span></span>
<span id="S2.T1.6.12.6.1.6" class="ltx_p"><span id="S2.T1.6.12.6.1.6.1" class="ltx_text" style="font-size:70%;">- Plane</span></span>
<span id="S2.T1.6.12.6.1.7" class="ltx_p"><span id="S2.T1.6.12.6.1.7.1" class="ltx_text" style="font-size:70%;">- Quality</span></span>
<span id="S2.T1.6.12.6.1.8" class="ltx_p"><span id="S2.T1.6.12.6.1.8.1" class="ltx_text" style="font-size:70%;">- Color</span></span>
<span id="S2.T1.6.12.6.1.9" class="ltx_p"><span id="S2.T1.6.12.6.1.9.1" class="ltx_text" style="font-size:70%;">- Size</span></span>
<span id="S2.T1.6.12.6.1.10" class="ltx_p"><span id="S2.T1.6.12.6.1.10.1" class="ltx_text" style="font-size:70%;">- Shape</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.6.13" class="ltx_tr">
<td id="S2.T1.6.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.1.1.1" class="ltx_p" style="width:80.0pt;"><span id="S2.T1.6.13.1.1.1.1" class="ltx_text" style="font-size:70%;">VQA-Med-2021 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.13.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.T1.6.13.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S2.T1.6.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.2.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.13.2.1.1.1" class="ltx_text" style="font-size:70%;">5,000</span></span>
</span>
</td>
<td id="S2.T1.6.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.13.3.1.1.1" class="ltx_text" style="font-size:70%;">5,000</span></span>
</span>
</td>
<td id="S2.T1.6.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.4.1.1" class="ltx_p" style="width:100.0pt;"><span id="S2.T1.6.13.4.1.1.1" class="ltx_text" style="font-size:70%;">MedPix database</span></span>
</span>
</td>
<td id="S2.T1.6.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S2.T1.6.13.5.1.1.1" class="ltx_text" style="font-size:70%;">Synthetical</span></span>
</span>
</td>
<td id="S2.T1.6.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.6.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.6.13.6.1.1" class="ltx_p" style="width:110.0pt;"><span id="S2.T1.6.13.6.1.1.1" class="ltx_text" style="font-size:70%;">- Abnormality</span></span>
</span>
</td>
</tr>
</table>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>VQA-Med-2018</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">VQA-Med-2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a dataset proposed in the ImageCLEF 2018<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text ltx_framed ltx_framed_underline">https://www.imageclef.org/2018</span></span></span></span>, and it is the first publicly available dataset in the medical domain. The QA pairs were generated from captions by a semi-automatic approach. First, a rule-based question generation (QG) system<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span id="footnote4.1" class="ltx_text ltx_framed ltx_framed_underline">http://www.cs.cmu.edu/ãrk/mheilman/questions/</span></span></span></span> automatically generated possible QA pairs by sentence simplification, answer phrase identification, question generation, and candidate questions ranking. Then, two expert human annotators (including one expert in clinical medicine) manually checked all generated QA pairs in two passes. Respectively, one pass ensures semantic correctness, and another ensures clinical relevance to associated medical images.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>VQA-RAD</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> is a radiology-specific dataset proposed in 2018. The image set is a balanced one containing samples by the head, chest, and abdomen from MedPix<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span id="footnote5.1" class="ltx_text ltx_framed ltx_framed_underline">https://medpix.nlm.nih.gov/home</span></span></span></span>. To investigate the question in a realistic scene, the author presented the images to clinicians to collect unguided questions. The clinicians are required to produce questions in both free-from and template structures. Afterward, the QA pairs are validated and classified manually to analyze the clinical focus. The answer types are either close-ended or open-ended. Although without a large quantity, the VQA-RAD dataset has acquired essential information about what a medical VQA system should be able to answer as an AI radiologist.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.15.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.16.2" class="ltx_text" style="font-size:90%;">Samples of images and question-answer pairs from the mentioned Datasets. Q = Question, A = Answer. The datasets are presented in chronological order.</span></figcaption>
<table id="S2.T2.13" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.13.14" class="ltx_tr">
<td id="S2.T2.13.14.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:80.0pt;">
<span id="S2.T2.13.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.13.14.1.1.1" class="ltx_p">Dataset</span>
</span>
</td>
<td id="S2.T2.13.14.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_tt" colspan="4">Samples</td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S2.T2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.3.1.1" class="ltx_p">VQA-Med-2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span>
</span>
</td>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S2.T2.1.1.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x1.png" id="S2.T2.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="85" height="77" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S2.T2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.4.1.1" class="ltx_p"><span id="S2.T2.2.2.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> What does the ct scan of thorax show?</span>
<span id="S2.T2.2.2.4.1.2" class="ltx_p"><span id="S2.T2.2.2.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> bilateral multiple pulmonary nodules</span>
</span>
</td>
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S2.T2.2.2.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x2.png" id="S2.T2.2.2.2.1.g1" class="ltx_graphics ltx_img_portrait" width="66" height="85" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S2.T2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.5.1.1" class="ltx_p"><span id="S2.T2.2.2.5.1.1.1" class="ltx_text ltx_font_bold">Q:</span> Is the lesion associated with a mass effect?</span>
<span id="S2.T2.2.2.5.1.2" class="ltx_p"><span id="S2.T2.2.2.5.1.2.1" class="ltx_text ltx_font_bold">A:</span> no</span>
</span>
</td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<td id="S2.T2.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.2.1.1" class="ltx_p">VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></span>
</span>
</td>
<td id="S2.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.3.3.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x3.png" id="S2.T2.3.3.1.1.g1" class="ltx_graphics ltx_img_square" width="85" height="85" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.3.1.1" class="ltx_p ltx_align_center"><span id="S2.T2.3.3.3.1.1.1" class="ltx_text ltx_font_bold">Organ System</span></span>
<span id="S2.T2.3.3.3.1.2" class="ltx_p"><span id="S2.T2.3.3.3.1.2.1" class="ltx_text ltx_font_bold">Q:</span> What is the organ system?</span>
<span id="S2.T2.3.3.3.1.3" class="ltx_p"><span id="S2.T2.3.3.3.1.3.1" class="ltx_text ltx_font_bold">A:</span> Gastrointestinal</span>
</span>
</td>
<td id="S2.T2.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.4.1.1" class="ltx_p ltx_align_center"><span id="S2.T2.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Object/Condition Presence</span></span>
<span id="S2.T2.3.3.4.1.2" class="ltx_p ltx_align_center"><span id="S2.T2.3.3.4.1.2.1" class="ltx_text ltx_font_bold">Q:</span> Is there gastric fullness?</span>
<span id="S2.T2.3.3.4.1.3" class="ltx_p ltx_align_center"><span id="S2.T2.3.3.4.1.3.1" class="ltx_text ltx_font_bold">A:</span> yes</span>
</span>
</td>
<td id="S2.T2.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.5.1.1" class="ltx_p ltx_align_center"><span id="S2.T2.3.3.5.1.1.1" class="ltx_text ltx_font_bold">Positional</span></span>
<span id="S2.T2.3.3.5.1.2" class="ltx_p"><span id="S2.T2.3.3.5.1.2.1" class="ltx_text ltx_font_bold">Q:</span> What is the location of the mass?</span>
<span id="S2.T2.3.3.5.1.3" class="ltx_p"><span id="S2.T2.3.3.5.1.3.1" class="ltx_text ltx_font_bold">A:</span> head of the pancreas</span>
</span>
</td>
</tr>
<tr id="S2.T2.5.5" class="ltx_tr">
<td id="S2.T2.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.3.1.1" class="ltx_p">VQA-Med-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span>
</span>
</td>
<td id="S2.T2.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.4.4.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x4.png" id="S2.T2.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="51" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.4.1.1" class="ltx_p ltx_align_center"><span id="S2.T2.5.5.4.1.1.1" class="ltx_text ltx_font_bold">Modality</span></span>
<span id="S2.T2.5.5.4.1.2" class="ltx_p"><span id="S2.T2.5.5.4.1.2.1" class="ltx_text ltx_font_bold">Q:</span> what imaging method was used?</span>
<span id="S2.T2.5.5.4.1.3" class="ltx_p"><span id="S2.T2.5.5.4.1.3.1" class="ltx_text ltx_font_bold">A:</span> us-d - doppler ultrasound</span>
</span>
</td>
<td id="S2.T2.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.5.5.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x5.png" id="S2.T2.5.5.2.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="43" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.5.1.1" class="ltx_p ltx_align_center"><span id="S2.T2.5.5.5.1.1.1" class="ltx_text ltx_font_bold">Plane</span></span>
<span id="S2.T2.5.5.5.1.2" class="ltx_p"><span id="S2.T2.5.5.5.1.2.1" class="ltx_text ltx_font_bold">Q:</span> which plane is the image shown in?</span>
<span id="S2.T2.5.5.5.1.3" class="ltx_p"><span id="S2.T2.5.5.5.1.3.1" class="ltx_text ltx_font_bold">A:</span> axial</span>
</span>
</td>
</tr>
<tr id="S2.T2.6.6" class="ltx_tr">
<td id="S2.T2.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.6.6.2.1.1" class="ltx_p">RadVisDial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite></span>
</span>
</td>
<td id="S2.T2.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.6.6.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x6.png" id="S2.T2.6.6.1.1.g1" class="ltx_graphics ltx_img_square" width="85" height="70" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.6.6.3.1.1" class="ltx_p"><span id="S2.T2.6.6.3.1.1.1" class="ltx_text ltx_font_bold">Q:</span> Airspace opacity?</span>
<span id="S2.T2.6.6.3.1.2" class="ltx_p"><span id="S2.T2.6.6.3.1.2.1" class="ltx_text ltx_font_bold">A:</span> Yes</span>
<span id="S2.T2.6.6.3.1.3" class="ltx_p"><span id="S2.T2.6.6.3.1.3.1" class="ltx_text ltx_font_bold">Q:</span> Fracture?</span>
<span id="S2.T2.6.6.3.1.4" class="ltx_p"><span id="S2.T2.6.6.3.1.4.1" class="ltx_text ltx_font_bold">A:</span> Not in report</span>
</span>
</td>
<td id="S2.T2.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.6.6.4.1.1" class="ltx_p ltx_align_left"><span id="S2.T2.6.6.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> Lung lesion?</span>
<span id="S2.T2.6.6.4.1.2" class="ltx_p ltx_align_center"><span id="S2.T2.6.6.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> No</span>
<span id="S2.T2.6.6.4.1.3" class="ltx_p ltx_align_center"><span id="S2.T2.6.6.4.1.3.1" class="ltx_text ltx_font_bold">Q:</span> Pneumonia?</span>
<span id="S2.T2.6.6.4.1.4" class="ltx_p ltx_align_center"><span id="S2.T2.6.6.4.1.4.1" class="ltx_text ltx_font_bold">A:</span> Yes</span>
</span>
</td>
<td id="S2.T2.6.6.5" class="ltx_td ltx_align_middle" style="width:80.0pt;"></td>
</tr>
<tr id="S2.T2.8.8" class="ltx_tr">
<td id="S2.T2.8.8.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.3.1.1" class="ltx_p">PathVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite></span>
</span>
</td>
<td id="S2.T2.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.7.7.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x7.png" id="S2.T2.7.7.1.1.g1" class="ltx_graphics ltx_img_portrait" width="56" height="85" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.8.8.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.4.1.1" class="ltx_p"><span id="S2.T2.8.8.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> What have been stripped from the bottom half of each specimen to show the surface of the brain?</span>
<span id="S2.T2.8.8.4.1.2" class="ltx_p"><span id="S2.T2.8.8.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> meninges</span>
</span>
</td>
<td id="S2.T2.8.8.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.8.8.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x8.png" id="S2.T2.8.8.2.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="53" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.8.8.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.5.1.1" class="ltx_p"><span id="S2.T2.8.8.5.1.1.1" class="ltx_text ltx_font_bold">Q:</span> Is remote kidney infarct replaced by a large fibrotic scar?</span>
<span id="S2.T2.8.8.5.1.2" class="ltx_p"><span id="S2.T2.8.8.5.1.2.1" class="ltx_text ltx_font_bold">A:</span> yes</span>
</span>
</td>
</tr>
<tr id="S2.T2.10.10" class="ltx_tr">
<td id="S2.T2.10.10.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.10.10.3.1.1" class="ltx_p">VQA-Med-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span>
</span>
</td>
<td id="S2.T2.9.9.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.9.9.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x9.png" id="S2.T2.9.9.1.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="53" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.10.10.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.10.10.4.1.1" class="ltx_p"><span id="S2.T2.10.10.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> what abnormality is seen in the image?</span>
<span id="S2.T2.10.10.4.1.2" class="ltx_p"><span id="S2.T2.10.10.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> ovarian torsion</span>
</span>
</td>
<td id="S2.T2.10.10.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.10.10.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x10.png" id="S2.T2.10.10.2.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="66" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.10.10.5" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.10.10.5.1.1" class="ltx_p"><span id="S2.T2.10.10.5.1.1.1" class="ltx_text ltx_font_bold">Q:</span> what is abnormal in the ct scan?</span>
<span id="S2.T2.10.10.5.1.2" class="ltx_p"><span id="S2.T2.10.10.5.1.2.1" class="ltx_text ltx_font_bold">A:</span> partial anomalous pulmonary venous return</span>
</span>
</td>
</tr>
<tr id="S2.T2.11.11" class="ltx_tr">
<td id="S2.T2.11.11.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.11.11.2.1.1" class="ltx_p">SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite></span>
</span>
</td>
<td id="S2.T2.11.11.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.11.11.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x11.png" id="S2.T2.11.11.1.1.g1" class="ltx_graphics ltx_img_landscape" width="85" height="60" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.11.11.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.11.11.3.1.1" class="ltx_p"><span id="S2.T2.11.11.3.1.1.1" class="ltx_text ltx_font_bold">Q:</span> Does the image contain left lung?</span>
<span id="S2.T2.11.11.3.1.2" class="ltx_p"><span id="S2.T2.11.11.3.1.2.1" class="ltx_text ltx_font_bold">A:</span> Yes</span>
</span>
</td>
<td id="S2.T2.11.11.4" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S2.T2.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.11.11.4.1.1" class="ltx_p ltx_align_left"><span id="S2.T2.11.11.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> What is the function of the rightmost organ in this picture?</span>
<span id="S2.T2.11.11.4.1.2" class="ltx_p ltx_align_center"><span id="S2.T2.11.11.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> Breathe</span>
</span>
</td>
<td id="S2.T2.11.11.5" class="ltx_td ltx_align_middle" style="width:80.0pt;"></td>
</tr>
<tr id="S2.T2.13.13" class="ltx_tr">
<td id="S2.T2.13.13.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:80.0pt;">
<span id="S2.T2.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.13.13.3.1.1" class="ltx_p">VQA-Med-2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span>
</span>
</td>
<td id="S2.T2.12.12.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:80.0pt;">
<span id="S2.T2.12.12.1.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x12.png" id="S2.T2.12.12.1.1.g1" class="ltx_graphics ltx_img_portrait" width="68" height="85" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.13.13.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:80.0pt;">
<span id="S2.T2.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.13.13.4.1.1" class="ltx_p"><span id="S2.T2.13.13.4.1.1.1" class="ltx_text ltx_font_bold">Q:</span> What is most alarming about this mri?</span>
<span id="S2.T2.13.13.4.1.2" class="ltx_p"><span id="S2.T2.13.13.4.1.2.1" class="ltx_text ltx_font_bold">A:</span> focal nodular hyperplasia</span>
</span>
</td>
<td id="S2.T2.13.13.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:80.0pt;">
<span id="S2.T2.13.13.2.1" class="ltx_inline-block ltx_align_top"><img src="/html/2111.10056/assets/x13.png" id="S2.T2.13.13.2.1.g1" class="ltx_graphics ltx_img_square" width="85" height="80" alt="[Uncaptioned image]">
</span>
</td>
<td id="S2.T2.13.13.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" style="width:80.0pt;">
<span id="S2.T2.13.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.13.13.5.1.1" class="ltx_p"><span id="S2.T2.13.13.5.1.1.1" class="ltx_text ltx_font_bold">Q:</span> What abnormality is seen in the image?</span>
<span id="S2.T2.13.13.5.1.2" class="ltx_p"><span id="S2.T2.13.13.5.1.2.1" class="ltx_text ltx_font_bold">A:</span> Enhancing lesion right parietal lobe with surrounding edema</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>VQA-Med-2019</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">VQA-Med-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is the second edition of the VQA-Med and was published during the ImageCLEF 2019 challenge. Inspired by the VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, VQA-Med-2019 has addressed the four most frequent question categories: modality, plane, organ system, and abnormality. For each category, the questions follow the patterns from hundreds of questions naturally asked and validated in the VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. The first three categories (modality, plane, and organ system) can be tackled as classification tasks, while the fourth category (abnormality) presents an answer generation problem.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>RadVisDial</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">RadVisDial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> is the first publicly available dataset for visual dialog in radiology. The visual dialogue consists of multiple QA pairs and is considered a more practical and complicated task for a radiology AI system than VQA. The images are selected from MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. For each image, the MIMIC-CXR has provided a well-structured relevant report with annotations for 14 labels (13 abnormalities and one No Findings label). The RadVisDial consists of two datasets: a silver-standard dataset and a gold-standard dataset. In the silver-standard group, the dialogues are synthetically created using the plain text reports associated with each image. Each dialogue contains five questions randomly sampled from 13 possible questions. The corresponding answer is automatically extracted from the source data and limited to four choices (yes, no, maybe, or not mentioned in the report).
In the gold-standard group, the dialogues are collected from two expert radiologists’ conversations following detailed annotation guidelines to ensure consistency. Only 100 random images are labeled with gold-standard. The RadVisDial dataset explored a real-world scene task of AI in the medical domain. Moreover, the team compared the synthetical dialogue to the real-world dialogue and conducted experiments to reflect the importance of context information. The medical history of the patient was introduced and led to better accuracy.</p>
</div>
</section>
<section id="S2.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>PathVQA</h4>

<div id="S2.SS1.SSS5.p1" class="ltx_para">
<p id="S2.SS1.SSS5.p1.1" class="ltx_p">PathVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is a dataset exploring VQA for pathology. The images with captions are extracted from digital resources (electronic textbooks and online libraries). The author developed a semi-automated pipeline to transfer the captions into QA pairs, and the generated QA pairs are manually checked and revised. The question can be divided into seven categories: what, where, when, whose, how, how much/how many, and yes/no. The open-ended questions account for 50.2% of all questions. For the close-ended “yes/no” questions, the answers are balanced with 8,145 “yes” and 8,189 “no”. The questions are designed according to the pathologist certification examination of the American Board of Pathology (ABP). Therefore it is an exam to verify the “AI Pathologist” in decision support. The PathVQA dataset demonstrates that medical VQA can be applied to various scenes.</p>
</div>
</section>
<section id="S2.SS1.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.6 </span>VQA-Med-2020</h4>

<div id="S2.SS1.SSS6.p1" class="ltx_para">
<p id="S2.SS1.SSS6.p1.1" class="ltx_p">VQA-Med-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is the third edition of the VQA-Med and was published in the ImageCLEF 2020 challenge. The images are selected with the limitation that the diagnosis was made according to the image content. The questions are specifically addressing on abnormality. A list of 330 abnormality problems is selected, and each problem needs to occur at least ten times in the dataset. The QA pairs are generated by patterns created.</p>
</div>
<div id="S2.SS1.SSS6.p2" class="ltx_para">
<p id="S2.SS1.SSS6.p2.1" class="ltx_p">In VQA-Med-2020, the visual question generation (VQG) task is first introduced to the medical domain. The VQG task is to generate natural language questions relative to the image content. The medical VQG dataset includes 1,001 radiology images and 2,400 associated questions. The ground truth questions are generated with a rule-based approach according to the image captions and manually revised.</p>
</div>
</section>
<section id="S2.SS1.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.7 </span>SLAKE</h4>

<div id="S2.SS1.SSS7.p1" class="ltx_para">
<p id="S2.SS1.SSS7.p1.1" class="ltx_p">SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> is a comprehensive dataset with both semantic labels and a structural medical knowledge base. The images are selected from three open source datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">83</span></a>, <a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">99</span></a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and annotated by experienced physicians. The semantic labels for images provide masks (segmentation) and bounding boxes (detection) for visual objects.
The medical knowledge base is provided in the form of a knowledge graph. The knowledge graph is extracted from OwnThink and manually reviewed. They are in the form of triplets (e.g., <span id="S2.SS1.SSS7.p1.1.1" class="ltx_text ltx_font_italic">&lt;Heart, Function, Promote blood flow&gt;</span>). The dataset contains 2,603 triplets in English and 2,629 triplets in Chinese. The introduction of a knowledge graph allows external knowledge-based questions such as organ function and disease prevention. The questions are collected from experienced doctors by selecting pre-defined questions or rewriting questions. Then the questions are categorized by their types and balanced to avoid bias.</p>
</div>
</section>
<section id="S2.SS1.SSS8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.8 </span>VQA-Med-2021</h4>

<div id="S2.SS1.SSS8.p1" class="ltx_para">
<p id="S2.SS1.SSS8.p1.1" class="ltx_p">VQA-Med-2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is published in ImageCLEF 2021 challenge. The VQA-Med-2021 is created under the same principles as those in VQA-Med-2020. The training set is the same dataset used in VQA-Med-2020. The validate set and test set are newly collected and manually reviewed by medical professionals.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2111.10056/assets/x14.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">The question category distribution of VQA-RAD and SLAKE</span></figcaption>
</figure>
</section>
<section id="S2.SS1.SSS9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.9 </span>Discussion</h4>

<div id="S2.SS1.SSS9.p1" class="ltx_para">
<p id="S2.SS1.SSS9.p1.1" class="ltx_p">In the above sections, we present 8 medical VQA datasets about their quantity, data source, QA creation, and question categories. As shown in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we also list three general VQA datasets for comparison. The image amounts of medical VQA datasets range from 315 to 91,060, while the number of QA pairs ranges from 1 QA pair per image to 10 QA pairs per image. The imaging modality includes chest X-ray, CT, MRI, and pathology. Except for RadVisDial, the medical VQA datasets are significantly smaller than the general VQA datasets in quantity. For the QA pair creation, the medical VQA uses synthetic creation more frequently than the general domain VQA. The question categories of medical VQA and general VQA are quite different. Besides the common categories of object and attribute, the general VQA research extends their problem to objects’ relationship and external knowledge, while the medical VQA research tends to image findings.</p>
</div>
<div id="S2.SS1.SSS9.p2" class="ltx_para">
<p id="S2.SS1.SSS9.p2.1" class="ltx_p">These differences between general VQA datasets and medical VQA datasets reflect the difficulties in medical dataset establishment. The three listed general VQA datasets have all utilized the Microsoft COCO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which provides 328K images with natural language descriptions. On the medical side, there is no such large-scale data source.
One type of data source is images with a description such as a caption and medical report. The VQA-Med-2018 is the first exploration of the medical VQA dataset. It utilizes the images in articles so that the images have a corresponding textual description. The PathVQA uses images and text from textbooks and the digital library. For these two datasets, the key problem is how to perform an accurate transformation. Another type of data source is the image with categorical attribution. The VQA-RAD, VQA-Med-2019, VQA-Med-2020, and VQA-Med-2021 are all sourced from the MedPix database, which provides images with attributions. Therefore, the key problem becomes how to acquire better questions given images and answers. The VQA-RAD starts from manual creation and collects unguided problems from clinicians. The question patterns collected from VQA-RAD leverage the construction of VQA-Med-2019, VQA-Med-2020, and VQA-Med-2021. The RadVisDial uses the data from the MIMIC-CXR dataset, which provides extracted disease labels and is the only large-scale data source in medical VQA.</p>
</div>
<div id="S2.SS1.SSS9.p3" class="ltx_para">
<p id="S2.SS1.SSS9.p3.1" class="ltx_p">Besides the data source, another difficulty is the professional knowledge required in data annotation. All three general VQA datasets adopt the Amazon Mechanical Turk workers to achieve their large-scale annotation. On the medical side, the QA creation is usually synthetical, and the manual creation is done by medical students or medical experts. The cost of manual data annotation in medical VQA is inevitably higher due to the requirement of professional knowledge.</p>
</div>
<div id="S2.SS1.SSS9.p4" class="ltx_para">
<p id="S2.SS1.SSS9.p4.1" class="ltx_p">The above difficulties in medical VQA dataset establishment have raised future challenges. With the development of technology, the existing difficulties can possibly be solved. For example, the recent Large Language Models (LLMs) have been believed good at understanding and generating natural language. The advantage of LLMs may make them ideal annotators for medical VQA. Especially for existing large-scale medical report datasets, i.e., MIMIC-CXR or FFA-IR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, the LLMs can help with parsing the natural language reports and converting them into medical VQA data.</p>
</div>
<div id="S2.SS1.SSS9.p5" class="ltx_para">
<p id="S2.SS1.SSS9.p5.1" class="ltx_p">Another problem is the question categories. As shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1.8 VQA-Med-2021 ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the VQA-RAD and SLAKE have different question categories distribution as they are created in different ways. Some categories such as modality and plain can help the image viewer to understand the captured information of the image, while the other categories such as abnormality and abnormality attributes can help interpret the image findings. Among all medical VQA datasets, VQA-RAD is the only one collecting the natural questions and representing a question categories distribution from medical students. In contrast, other datasets are all created with pre-defined question categories and the distribution may not represent any real-world demands. In other words, currently, there is no public dataset representing a question distribution from patients in the clinical scene.</p>
</div>
<div id="S2.SS1.SSS9.p6" class="ltx_para">
<p id="S2.SS1.SSS9.p6.1" class="ltx_p">Furthermore, the task design and mission are also considerable problems. The recently proposed general VQA datasets have shown their special target, such as data balance, knowledge base, etc.
In the medical domain, SLAKE provides more modalities, including segmentation, detection, and knowledge graph. This feature can improve the complexity of tasks and allow more question categories. It also raises a new mission for the approach researchers as it has more modalities to expand the method’s complexity. Despite the manual annotation limit its quantity, the golden standard annotation is prospective to benefit the community and future research.</p>
</div>
<div id="S2.SS1.SSS9.p7" class="ltx_para">
<p id="S2.SS1.SSS9.p7.1" class="ltx_p">As the medical VQA research is still in an early stage, the current datasets are only about radiology and pathology in data subjects. There is more field to discover, such as ophthalmology and dermatology, which are also popular in medical AI research and already has existing databases to create potential VQA task. Besides dataset works, there is also exploration addressing data collection efficiency. The MVQAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> builds an online system providing self-collected and annotation tools to allow users to upload data and semi-automatically generate VQA triplets.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Performance Metrics</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The performance metrics used in the proposed medical VQA tasks can be categorized into classification-based metrics and language-based metrics. The classification-based metrics are the general metrics in classification tasks such as accuracy and F1 score. They treat the answer as a classification result and calculate the exact match accuracy, precision, recall, and e.t.c. All eight tasks in this paper use classification-based metrics as part of their performance metrics. The Language metrics are the general metrics for sentence evaluation tasks (e.g., translation, image captioning). The tasks using language-based metrics include VQA-Med-2018, VQA-Med-2019, PathVQA, VQA-Med-2020, and VQA-Med-2021. All of those four tasks use the BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">67</span></a>]</cite>, which measures the similarity of the phrases (n-grams) between two sentences. However, the BLEU is originally a metric for machine translation and is also used in medical report generation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. As shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.1.2 VQA-RAD ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the ground truth answers in medical VQA are obviously shorter than those of machine translation or medical report generation tasks. Also, for some questions, the semantically positive or negative is more important than the word match. It suggests that BLEU may be an inappropriate metric for current medical VQA datasets. However, the BLEU can still be useful when the answer corpus of the future medical VQA becomes extensive and comprehensive sentences.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Besides the general metrics, there are also custom metrics designed for the medical VQA. For example, the WBSS (Word-based Semantic Similarity) and CBSS (Concept-based Semantic Similarity) are created in the VQA-Med-2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as new language metrics. However, the metric is not a fixed component of the dataset and the approach. Researchers can alternatively use more suitable metrics to evaluate their results. For example, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">79</span></a>]</cite> introduce the AUC-ROC (Area under the ROC Curve) as their classification metrics to better evaluate the measure of separability.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2111.10056/assets/x15.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="523" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">The number of papers included/excluded at the literature review of conference/journal papers</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To investigate the feature of approaches used in the medical VQA task, we reviewed the published papers evaluated on the datasets. We search for the method papers with two strategies. For the ImageCLEF competitions, we use the corresponding overview papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to identify the participating teams and collect their work notes. This strategy helps us collect a total number of 32 papers. Then we search Google Scholar with the citation search function to find 217 papers that cite medical VQA datasets and collect a total of 12 papers. For a detailed count of the papers included and excluded at each stage, refer to Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Finally, we select 45 published papers, including 32 work notes and 13 conference/journal papers. The 45 papers describe 46 approaches, and the performance and characteristics are shown in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table3</span>. The following sub-sections discuss the medical VQA methods by the framework, components, other techniques, performance comparison, and overall discussion.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Framework</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2111.10056/assets/x16.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">A schematic diagram of the mainstream medical VQA frameworks illustrating main components, including image feature extraction, question feature extraction, feature fusion, and answering component.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Among the 46 approaches, 39 of them can be attributed to a common framework in the general VQA domain, the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">joint embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span>. It is proposed as the baseline method for the VQA v1 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and referred to as the "LSTM Q+I". As illustrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Framework ‣ 3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the framework includes four components: an image encoder, a question encoder, a feature fusing algorithm, and an answering component according to the task requirement. Respectively, the image feature extractor can be the well-developed convolution neural network (CNN) backbones such as VGG Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">82</span></a>]</cite> and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and the question encoder can be the prevalent language encoding models such as LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">92</span></a>]</cite>. The feature encoding models are often initialized with pre-trained weights, and they can be either frozen or fine-tuned in an end-to-end manner during the training of the VQA task. The answering component is usually a neural network classifier or a recurrent neural network language generator. In the LSTM Q+I, the question features and image features are fused via element-wise multiplication. Then, researchers developed innovative fusing algorithms and introduced the popular attention mechanism into the system to further increase the performance.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Besides the joint embedding framework, the other 7 approaches choose not to include the question feature. They find the semantic space of the questions is simply due to the data nature and use only the image feature to produce the answer. The framework has outstanding performance in VQA-Med-2020 and VQA-Med-2021, where the questions are only about abnormality and in only “Yes/No” or “What” types.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Compare to the general VQA domain, the architecture that appears in medical VQA research is less diverse. There is also other architecture in the general VQA domain, such as compositional model such as the Neural Module Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, no compositional model has been adopted in current medical VQA approaches, and it is also potential to introduce other frameworks to the medical VQA.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In terms of the image encoder in the public challenges, the VGG Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">82</span></a>]</cite> is the most popular choice. As shown in Fig.<a href="#S3.F4" title="Figure 4 ‣ 3.4 Fusion Algorithm ‣ 3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the participants using VGG Net as the image encoder represent a significant proportion in all of the three VQA-MED challenges. Meanwhile, the pre-trained image encoder is often used for both public challenges and conference/journal works. According to Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table3</span>, more than half of the teams (29 of 46) directly used a pre-trained model on the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">74</span></a>]</cite>. However, ImageNet has different content compared with medical VQA. Using ImageNet pre-trained is a non-reasonable practice but a workable option when the low data quantity and lack of labels both limit the pre-training on medical datasets.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Finding better pre-training methods is a popular topic in medical VQA research as well as in the medical AI community. The image numbers of most medical VQA datasets are under 5,000. It leads to difficulty in training image representation. The solution proposed includes using other pre-trained models, using the extra dataset, contrastive learning, multi-task pre-training, and meta-learning. The LIST team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> utilized an image encoder pre-trained on the CheXpert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">99</span></a>]</cite> dataset. The MMBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> team uses an auxiliary dataset named ROCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">69</span></a>]</cite> (images and captions) to perform pre-training in a token-masking manner. The CPRD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> team introduces contrastive learning technology to conduct pre-training with unlabeled images in a self-supervised scheme. Self-supervised training has the advantage that it does not need image labels, which are expensive to acquire for medical images. The MTPT-CSMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> team uses an auxiliary dataset of segmentation tasks. On the other hand, researchers also try to further digest the original data. The MEVF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">64</span></a>]</cite> team proposed the Mixture of Enhanced Visual Features (MEVF) component, which utilizes the Model-Agnostic Meta-Learning (MAML) and the Convolutional Denoising Auto-Encoder (CDAE) to initialize the model weights for the image encoder to overcome the data limitation in quantity. The various exploration on image encoder is specific in medical VQA compared that in general VQA.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Notably, the auxiliary datasets selected have three different types: captioning dataset, unlabeled image, and segmentation dataset. It indicates that the pre-training on various extra data can leverage the image decoder’s performance. Hence, exploring the methods to pre-train the image encoder will be a prospective task. Another feature is that all image encoders in the reviewed papers are CNN classification models than detection ones. It restricts the application of detection-based methods such as Up-Down <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which are popular in the general VQA domain.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Language Encoder</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The language encoders in the reviewed works include LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> (18 of 46), Bi-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">77</span></a>]</cite> (5 of 46), GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (3 of 38), the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">92</span></a>]</cite> (including BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and BioBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>) (12 of 46) and the others (8 of 46). Notably, all the top five teams are using The BERT in the VQA-Med-2019 challenge. It is shown in Fig.<a href="#S3.F4" title="Figure 4 ‣ 3.4 Fusion Algorithm ‣ 3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> that participating teams select BERT/BioBERT instead of LSTM/Bi-LSTM/GRU over time. This indicates the advantage of the Transformer model and the BERT pre-training despite the corpus differences between the general domain and medical domain. Meanwhile, the Recurrent Neural Networks (LSTM/Bi-LSTM/GRU) users are also adopting the pre-trained word embedding component (12 of 26). It indicates that the researchers tend to use a pre-training model to process the questions. However, only the MMBERT team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> conducts personalized pre-training with a self-supervised method, the token masking strategy on extra data. Compared with the image encoder, the language encoder does not get much research. It is potential to develop more NLP pre-training methods or vision plus language pre-training methods for medical VQA.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Especially, 7 teams process the questions without a deep learning model but with keyword or template matching. The reason is that keyword or template
matching has been powerful in their tasks. Hence, a light language encoder can be a practical choice in medical VQA applications as the task may not have a large number of question categories.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Fusion Algorithm</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The fusion stage gathers the extracted visual feature and language feature and then models the hidden relationship between the language feature and the visual feature. It is the core component of VQA methods. The typical fusion algorithm includes the attention mechanism and the pooling module.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Attention mechanism</span> is widely used in vision and language tasks. Among the Medical VQA approaches, 23 of 46 apply attention mechanisms in the fusion stage. Stacked Attention Networks (SAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite> is a typical attention algorithm. It uses the question feature as a query to rank the answer-related image regions. With a multiple-layer structure, the SAN can query an image several times to infer the answer progressively. The SAN introduced an incursive attention mechanism and is used as a baseline for many datasets. Besides the SAN, some other works employ the attention mechanism, such as the Bilinear Attention Networks (BAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, the Hierarchical Question-Image Co-Attention (HieCoAtt) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">60</span></a>]</cite>, e.t.c. Notably, the popular multi-head attention methods(e.g., the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">92</span></a>]</cite>, the Modular Co-Attention Network (MCAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">105</span></a>]</cite>) are seldom applied to medical VQA.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>The Results and Characteristics of Approaches in medical VQA Tasks.</figcaption>
<table id="S3.T3.4" class="ltx_tabular">
<thead class="ltx_thead">
<tr id="S3.T3.4.5.1" class="ltx_tr">
<th id="S3.T3.4.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Team/Method</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.5.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Image Encoder</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.5.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained (Image)</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.5.1.4.1.1.1" class="ltx_text" style="font-size:90%;">Language Encoder</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.5.1.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained (Language)</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.5.1.6.1.1.1" class="ltx_text" style="font-size:90%;">Attention (Fusion)</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.5.1.7.1.1.1" class="ltx_text" style="font-size:90%;">Fusion</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.5.1.8.1.1.1" class="ltx_text" style="font-size:90%;">Output Mode</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.5.1.9.1.1.1" class="ltx_text" style="font-size:90%;">Other Technique(s)</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.5.1.10.1.1.1" class="ltx_text" style="font-size:90%;">Language Score(BLEU)</span></span>
</span>
</th>
<th id="S3.T3.4.5.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.4.5.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.5.1.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.5.1.11.1.1.1" class="ltx_text" style="font-size:90%;">Classification Accuracy</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.4.6.1" class="ltx_tr">
<td id="S3.T3.4.6.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQA-MED-2018</span></td>
</tr>
<tr id="S3.T3.4.7.2" class="ltx_tr">
<td id="S3.T3.4.7.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.7.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Chakri* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.7.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S3.T3.4.7.2.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.7.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.7.2.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.7.2.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.7.2.4.1.1.1" class="ltx_text" style="font-size:90%;">GRU</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.7.2.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.7.2.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.7.2.7.1.1.1" class="ltx_text" style="font-size:90%;">Element-wise multiplication</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.7.2.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (GRU)</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.9" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T3.4.7.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.7.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.7.2.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.7.2.10.1.1.1" class="ltx_text" style="font-size:90%;">0.188</span></span>
</span>
</td>
<td id="S3.T3.4.7.2.11" class="ltx_td ltx_align_top ltx_border_t"></td>
</tr>
<tr id="S3.T3.4.8.3" class="ltx_tr">
<td id="S3.T3.4.8.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.8.3.1.1.1.1" class="ltx_text" style="font-size:90%;">UMMS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.8.3.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">70</span></a><span id="S3.T3.4.8.3.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.8.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.8.3.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.8.3.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.8.3.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.8.3.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.8.3.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.8.3.7.1.1.1" class="ltx_text" style="font-size:90%;">MFB with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.8.3.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.8.3.9.1.1.1" class="ltx_text" style="font-size:90%;">Embedding based topic model</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.8.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.8.3.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.8.3.10.1.1.1" class="ltx_text" style="font-size:90%;">0.162</span></span>
</span>
</td>
<td id="S3.T3.4.8.3.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.9.4" class="ltx_tr">
<td id="S3.T3.4.9.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.9.4.1.1.1.1" class="ltx_text" style="font-size:90%;">TU </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.9.4.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">111</span></a><span id="S3.T3.4.9.4.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.9.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.9.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Inception-Resnet-v2</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.9.4.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.9.4.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.9.4.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.9.4.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.9.4.7.1.1.1" class="ltx_text" style="font-size:90%;">Attention mechanism</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.9.4.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.9.4.9.1.1.1" class="ltx_text" style="font-size:90%;">Output rules</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.9.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.9.4.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.9.4.10.1.1.1" class="ltx_text" style="font-size:90%;">0.135</span></span>
</span>
</td>
<td id="S3.T3.4.9.4.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.10.5" class="ltx_tr">
<td id="S3.T3.4.10.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.10.5.1.1.1.1" class="ltx_text" style="font-size:90%;">HQS* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.10.5.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S3.T3.4.10.5.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.10.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.10.5.2.1.1.1" class="ltx_text" style="font-size:90%;">Inception-Resnet-v2</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.10.5.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.10.5.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.10.5.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.10.5.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.10.5.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.10.5.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.10.5.9.1.1.1" class="ltx_text" style="font-size:90%;">Question segregation</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.10.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.10.5.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.10.5.10.1.1.1" class="ltx_text" style="font-size:90%;">0.132</span></span>
</span>
</td>
<td id="S3.T3.4.10.5.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.11.6" class="ltx_tr">
<td id="S3.T3.4.11.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.11.6.1.1.1.1" class="ltx_text" style="font-size:90%;">NLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.11.6.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S3.T3.4.11.6.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.11.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.11.6.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.11.6.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.11.6.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.11.6.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.11.6.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.11.6.7.1.1.1" class="ltx_text" style="font-size:90%;">SAN</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.11.6.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.11.6.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.11.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.11.6.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.11.6.10.1.1.1" class="ltx_text" style="font-size:90%;">0.121</span></span>
</span>
</td>
<td id="S3.T3.4.11.6.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.12.7" class="ltx_tr">
<td id="S3.T3.4.12.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.12.7.1.1.1.1" class="ltx_text" style="font-size:90%;">NLM</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.12.7.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.12.7.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.12.7.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.12.7.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.12.7.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.12.7.7.1.1.1" class="ltx_text" style="font-size:90%;">MCB</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.12.7.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.12.7.9.1.1.1" class="ltx_text" style="font-size:90%;">Pre-training with extra data</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.12.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.12.7.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.12.7.10.1.1.1" class="ltx_text" style="font-size:90%;">0.085</span></span>
</span>
</td>
<td id="S3.T3.4.12.7.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.13.8" class="ltx_tr">
<td id="S3.T3.4.13.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.13.8.1.1.1.1" class="ltx_text" style="font-size:90%;">JUST </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.13.8.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">85</span></a><span id="S3.T3.4.13.8.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.13.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.13.8.2.1.1.1" class="ltx_text" style="font-size:90%;">VGGNet</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.13.8.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.13.8.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.13.8.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.13.8.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.13.8.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.13.8.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.13.8.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.13.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.13.8.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.13.8.10.1.1.1" class="ltx_text" style="font-size:90%;">0.061</span></span>
</span>
</td>
<td id="S3.T3.4.13.8.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.14.9" class="ltx_tr">
<td id="S3.T3.4.14.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.14.9.1.1.1.1" class="ltx_text" style="font-size:90%;">FSTT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.14.9.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S3.T3.4.14.9.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.14.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.14.9.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.14.9.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.14.9.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.14.9.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.14.9.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.14.9.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.14.9.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.14.9.9.1.1.1" class="ltx_text" style="font-size:90%;">Decision Tree Classifier</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.14.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.14.9.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.14.9.10.1.1.1" class="ltx_text" style="font-size:90%;">0.054</span></span>
</span>
</td>
<td id="S3.T3.4.14.9.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.15.10" class="ltx_tr">
<td id="S3.T3.4.15.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.15.10.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQA-MED-2019</span></td>
</tr>
<tr id="S3.T3.4.16.11" class="ltx_tr">
<td id="S3.T3.4.16.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.16.11.1.1.1.1" class="ltx_text" style="font-size:90%;">KEML* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.16.11.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">109</span></a><span id="S3.T3.4.16.11.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.16.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.16.11.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.16.11.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.16.11.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.16.11.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.16.11.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.16.11.7.1.1.1" class="ltx_text" style="font-size:90%;">BLOCK</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.16.11.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.16.11.9.1.1.1" class="ltx_text" style="font-size:90%;">Global Average Pooling,</span></span>
<span id="S3.T3.4.16.11.9.1.2" class="ltx_p"><span id="S3.T3.4.16.11.9.1.2.1" class="ltx_text" style="font-size:90%;">Meta-learning,</span></span>
<span id="S3.T3.4.16.11.9.1.3" class="ltx_p"><span id="S3.T3.4.16.11.9.1.3.1" class="ltx_text" style="font-size:90%;">Gated Graph Neural Networks,</span></span>
<span id="S3.T3.4.16.11.9.1.4" class="ltx_p"><span id="S3.T3.4.16.11.9.1.4.1" class="ltx_text" style="font-size:90%;">Knowledge-Based Representation Learning</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.16.11.10.1.1.1" class="ltx_text" style="font-size:90%;">0.912</span></span>
</span>
</td>
<td id="S3.T3.4.16.11.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.16.11.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.16.11.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.16.11.11.1.1.1" class="ltx_text" style="font-size:90%;">0.938</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.17.12" class="ltx_tr">
<td id="S3.T3.4.17.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.17.12.1.1.1.1" class="ltx_text" style="font-size:90%;">MedFuseNet* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.17.12.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">79</span></a><span id="S3.T3.4.17.12.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.17.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.17.12.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.17.12.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.17.12.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.17.12.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.17.12.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.17.12.7.1.1.1" class="ltx_text" style="font-size:90%;">MedFuseNet</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.17.12.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification/ Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.17.12.9.1.1.1" class="ltx_text" style="font-size:90%;">Image Attention,</span></span>
<span id="S3.T3.4.17.12.9.1.2" class="ltx_p"><span id="S3.T3.4.17.12.9.1.2.1" class="ltx_text" style="font-size:90%;">Image-Question Co-Attention</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.17.12.10.1.1.1" class="ltx_text" style="font-size:90%;">0.27</span></span>
<span id="S3.T3.4.17.12.10.1.2" class="ltx_p"><span id="S3.T3.4.17.12.10.1.2.1" class="ltx_text" style="font-size:90%;">(Subset)</span></span>
</span>
</td>
<td id="S3.T3.4.17.12.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.17.12.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.17.12.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.17.12.11.1.1.1" class="ltx_text" style="font-size:90%;">0.789</span></span>
<span id="S3.T3.4.17.12.11.1.2" class="ltx_p"><span id="S3.T3.4.17.12.11.1.2.1" class="ltx_text" style="font-size:90%;">(Subset)</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.18.13" class="ltx_tr">
<td id="S3.T3.4.18.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.18.13.1.1.1.1" class="ltx_text" style="font-size:90%;">MMBERT* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.18.13.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S3.T3.4.18.13.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.18.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.18.13.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.18.13.3.1.1.1" class="ltx_text" style="font-size:90%;">ROCO</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.18.13.3.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">69</span></a><span id="S3.T3.4.18.13.3.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.18.13.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.18.13.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.18.13.5.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.18.13.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.18.13.7.1.1.1" class="ltx_text" style="font-size:90%;">Multi-head attention (Transformer)</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.18.13.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (Transformer)</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.18.13.9.1.1.1" class="ltx_text" style="font-size:90%;">Pre-training with extra data</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.18.13.10.1.1.1" class="ltx_text" style="font-size:90%;">0.69</span></span>
</span>
</td>
<td id="S3.T3.4.18.13.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.18.13.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.18.13.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.18.13.11.1.1.1" class="ltx_text" style="font-size:90%;">0.672</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.19.14" class="ltx_tr">
<td id="S3.T3.4.19.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.19.14.1.1.1.1" class="ltx_text" style="font-size:90%;">CGMVQA* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.19.14.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">73</span></a><span id="S3.T3.4.19.14.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.19.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.19.14.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.19.14.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.19.14.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.19.14.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.19.14.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.19.14.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.19.14.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification Generation (Transformer)</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.19.14.9.1.1.1" class="ltx_text" style="font-size:90%;">Global Average Pooling</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.19.14.10.1.1.1" class="ltx_text" style="font-size:90%;">0.659</span></span>
</span>
</td>
<td id="S3.T3.4.19.14.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.19.14.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.19.14.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.19.14.11.1.1.1" class="ltx_text" style="font-size:90%;">0.64</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.20.15" class="ltx_tr">
<td id="S3.T3.4.20.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.20.15.1.1.1.1" class="ltx_text" style="font-size:90%;">Hanlin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.20.15.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib102" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">102</span></a><span id="S3.T3.4.20.15.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.20.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.20.15.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.20.15.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.20.15.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.20.15.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.20.15.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.20.15.7.1.1.1" class="ltx_text" style="font-size:90%;">MFB with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.20.15.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.20.15.9.1.1.1" class="ltx_text" style="font-size:90%;">Global Average Pooling</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.20.15.10.1.1.1" class="ltx_text" style="font-size:90%;">0.644</span></span>
</span>
</td>
<td id="S3.T3.4.20.15.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.20.15.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.20.15.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.20.15.11.1.1.1" class="ltx_text" style="font-size:90%;">0.62</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.21.16" class="ltx_tr">
<td id="S3.T3.4.21.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.21.16.1.1.1.1" class="ltx_text" style="font-size:90%;">minhvu </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.21.16.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">93</span></a><span id="S3.T3.4.21.16.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.21.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.21.16.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.21.16.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.21.16.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.21.16.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.21.16.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.21.16.7.1.1.1" class="ltx_text" style="font-size:90%;">MLB, MUTAN with attention</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.21.16.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.21.16.9.1.1.1" class="ltx_text" style="font-size:90%;">Ensemble,</span></span>
<span id="S3.T3.4.21.16.9.1.2" class="ltx_p"><span id="S3.T3.4.21.16.9.1.2.1" class="ltx_text" style="font-size:90%;">Skip-thought</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.21.16.10.1.1.1" class="ltx_text" style="font-size:90%;">0.634</span></span>
</span>
</td>
<td id="S3.T3.4.21.16.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.21.16.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.21.16.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.21.16.11.1.1.1" class="ltx_text" style="font-size:90%;">0.616</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.22.17" class="ltx_tr">
<td id="S3.T3.4.22.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.22.17.1.1.1.1" class="ltx_text" style="font-size:90%;">TUA1 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.22.17.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">112</span></a><span id="S3.T3.4.22.17.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.22.17.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.22.17.2.1.1.1" class="ltx_text" style="font-size:90%;">Inception-Resnet-v2</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.22.17.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.22.17.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.22.17.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.22.17.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.22.17.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.22.17.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.22.17.9.1.1.1" class="ltx_text" style="font-size:90%;">Sub-models,</span></span>
<span id="S3.T3.4.22.17.9.1.2" class="ltx_p"><span id="S3.T3.4.22.17.9.1.2.1" class="ltx_text" style="font-size:90%;">Question classifier</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.22.17.10.1.1.1" class="ltx_text" style="font-size:90%;">0.633</span></span>
</span>
</td>
<td id="S3.T3.4.22.17.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.22.17.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.22.17.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.22.17.11.1.1.1" class="ltx_text" style="font-size:90%;">0.606</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.23.18" class="ltx_tr">
<td id="S3.T3.4.23.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.23.18.1.1.1.1" class="ltx_text" style="font-size:90%;">QC-MLB* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.23.18.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">94</span></a><span id="S3.T3.4.23.18.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.23.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.23.18.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.23.18.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.23.18.4.1.1.1" class="ltx_text" style="font-size:90%;">Skip-thought vectors</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.23.18.5.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.23.18.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.23.18.7.1.1.1" class="ltx_text" style="font-size:90%;">QC-MLB</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.23.18.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.23.18.9.1.1.1" class="ltx_text" style="font-size:90%;">Question-centric fusion</span></span>
</span>
</td>
<td id="S3.T3.4.23.18.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.23.18.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.23.18.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.23.18.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.23.18.11.1.1.1" class="ltx_text" style="font-size:90%;">0.603</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.24.19" class="ltx_tr">
<td id="S3.T3.4.24.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.24.19.1.1.1.1" class="ltx_text" style="font-size:90%;">UMMS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.24.19.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">80</span></a><span id="S3.T3.4.24.19.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.24.19.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.24.19.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.24.19.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.24.19.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.24.19.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.24.19.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.24.19.7.1.1.1" class="ltx_text" style="font-size:90%;">MFH with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.24.19.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.24.19.9.1.1.1" class="ltx_text" style="font-size:90%;">SVM Question classifier</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.24.19.10.1.1.1" class="ltx_text" style="font-size:90%;">0.593</span></span>
</span>
</td>
<td id="S3.T3.4.24.19.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.24.19.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.24.19.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.24.19.11.1.1.1" class="ltx_text" style="font-size:90%;">0.566</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.25.20" class="ltx_tr">
<td id="S3.T3.4.25.20.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.25.20.1.1.1.1" class="ltx_text" style="font-size:90%;">IBM Research AI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.25.20.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S3.T3.4.25.20.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.25.20.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.25.20.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.25.20.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.25.20.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.25.20.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.25.20.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.25.20.7.1.1.1" class="ltx_text" style="font-size:90%;">Attention mechanism Classification</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.25.20.8.1.1.1" class="ltx_text" style="font-size:90%;">Question classifier</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.25.20.9.1.1.1" class="ltx_text" style="font-size:90%;">Image size encoder</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.25.20.10.1.1.1" class="ltx_text" style="font-size:90%;">0.582</span></span>
</span>
</td>
<td id="S3.T3.4.25.20.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.25.20.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.25.20.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.25.20.11.1.1.1" class="ltx_text" style="font-size:90%;">0.558</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.26.21" class="ltx_tr">
<td id="S3.T3.4.26.21.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.26.21.1.1.1.1" class="ltx_text" style="font-size:90%;">LIST </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.26.21.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.T3.4.26.21.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.26.21.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.26.21.2.1.1.1" class="ltx_text" style="font-size:90%;">DenseNet-121</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.26.21.3.1.1.1" class="ltx_text" style="font-size:90%;">CheXpert</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.26.21.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.26.21.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.26.21.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.26.21.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.26.21.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.26.21.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.26.21.10.1.1.1" class="ltx_text" style="font-size:90%;">0.583</span></span>
</span>
</td>
<td id="S3.T3.4.26.21.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.26.21.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.26.21.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.26.21.11.1.1.1" class="ltx_text" style="font-size:90%;">0.556</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.27.22" class="ltx_tr">
<td id="S3.T3.4.27.22.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.27.22.1.1.1.1" class="ltx_text" style="font-size:90%;">Turner.JCE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.27.22.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">90</span></a><span id="S3.T3.4.27.22.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.27.22.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.27.22.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG19</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.27.22.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.27.22.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.27.22.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.27.22.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.27.22.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.27.22.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.27.22.9.1.1.1" class="ltx_text" style="font-size:90%;">Sub-models,</span></span>
<span id="S3.T3.4.27.22.9.1.2" class="ltx_p"><span id="S3.T3.4.27.22.9.1.2.1" class="ltx_text" style="font-size:90%;">Question classifier</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.27.22.10.1.1.1" class="ltx_text" style="font-size:90%;">0.572</span></span>
</span>
</td>
<td id="S3.T3.4.27.22.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.27.22.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.27.22.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.27.22.11.1.1.1" class="ltx_text" style="font-size:90%;">0.536</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.28.23" class="ltx_tr">
<td id="S3.T3.4.28.23.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.28.23.1.1.1.1" class="ltx_text" style="font-size:90%;">JUST19 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.28.23.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S3.T3.4.28.23.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.28.23.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.28.23.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.28.23.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.28.23.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.28.23.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.28.23.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.28.23.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.28.23.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification/ Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.28.23.9.1.1.1" class="ltx_text" style="font-size:90%;">Sub-models,</span></span>
<span id="S3.T3.4.28.23.9.1.2" class="ltx_p"><span id="S3.T3.4.28.23.9.1.2.1" class="ltx_text" style="font-size:90%;">Question classifier</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.28.23.10.1.1.1" class="ltx_text" style="font-size:90%;">0.591</span></span>
</span>
</td>
<td id="S3.T3.4.28.23.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.28.23.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.28.23.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.28.23.11.1.1.1" class="ltx_text" style="font-size:90%;">0.534</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.2.2.2" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Team</span><math id="S3.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\_" display="inline"><semantics id="S3.T3.1.1.1.1.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S3.T3.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.m1.1c">\_</annotation></semantics></math><span id="S3.T3.2.2.2.2.2.2" class="ltx_text" style="font-size:90%;">PwC</span><math id="S3.T3.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\_" display="inline"><semantics id="S3.T3.2.2.2.2.2.m2.1a"><mi mathsize="90%" mathvariant="normal" id="S3.T3.2.2.2.2.2.m2.1.1" xref="S3.T3.2.2.2.2.2.m2.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.2.m2.1b"><ci id="S3.T3.2.2.2.2.2.m2.1.1.cmml" xref="S3.T3.2.2.2.2.2.m2.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.2.m2.1c">\_</annotation></semantics></math><span id="S3.T3.2.2.2.2.2.3" class="ltx_text" style="font-size:90%;">-</span></span>
<span id="S3.T3.2.2.2.2.3" class="ltx_p"><span id="S3.T3.2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Med </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.2.2.2.2.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S3.T3.2.2.2.2.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.3.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-50</span></span>
</span>
</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.6.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.2.2.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.2.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.8.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.2.2.8.1.1.1" class="ltx_text" style="font-size:90%;">Attention mechanism</span></span>
</span>
</td>
<td id="S3.T3.2.2.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.9.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.2.2.9.1.1.1" class="ltx_text" style="font-size:90%;">Classification/ Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.2.2.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.10.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.2.2.10.1.1.1" class="ltx_text" style="font-size:90%;">Sub-models,</span></span>
<span id="S3.T3.2.2.10.1.2" class="ltx_p"><span id="S3.T3.2.2.10.1.2.1" class="ltx_text" style="font-size:90%;">Question classifier</span></span>
</span>
</td>
<td id="S3.T3.2.2.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.2.2.11.1.1.1" class="ltx_text" style="font-size:90%;">0.534</span></span>
</span>
</td>
<td id="S3.T3.2.2.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.2.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.12.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.2.2.12.1.1.1" class="ltx_text" style="font-size:90%;">0.488</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.29.24" class="ltx_tr">
<td id="S3.T3.4.29.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.29.24.1.1.1.1" class="ltx_text" style="font-size:90%;">Techno </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.29.24.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.T3.4.29.24.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.29.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.29.24.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.29.24.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.29.24.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.29.24.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.29.24.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.29.24.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.29.24.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.29.24.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.29.24.10.1.1.1" class="ltx_text" style="font-size:90%;">0.486</span></span>
</span>
</td>
<td id="S3.T3.4.29.24.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.29.24.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.29.24.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.29.24.11.1.1.1" class="ltx_text" style="font-size:90%;">0.462</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.30.25" class="ltx_tr">
<td id="S3.T3.4.30.25.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.30.25.1.1.1.1" class="ltx_text" style="font-size:90%;">Gasmi* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.30.25.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.T3.4.30.25.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.30.25.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.30.25.2.1.1.1" class="ltx_text" style="font-size:90%;">EfficientNet</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.30.25.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.30.25.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.30.25.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.30.25.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.30.25.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.30.25.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.30.25.9.1.1.1" class="ltx_text" style="font-size:90%;">Optimal parameter selection</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.30.25.10.1.1.1" class="ltx_text" style="font-size:90%;">0.42</span></span>
</span>
</td>
<td id="S3.T3.4.30.25.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.30.25.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.30.25.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.30.25.11.1.1.1" class="ltx_text" style="font-size:90%;">0.391</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.31.26" class="ltx_tr">
<td id="S3.T3.4.31.26.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.31.26.1.1.1.1" class="ltx_text" style="font-size:90%;">Dear stranger </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.31.26.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">59</span></a><span id="S3.T3.4.31.26.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.31.26.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.31.26.2.1.1.1" class="ltx_text" style="font-size:90%;">Xception</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.31.26.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.31.26.4.1.1.1" class="ltx_text" style="font-size:90%;">GRU</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.31.26.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.31.26.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.31.26.7.1.1.1" class="ltx_text" style="font-size:90%;">Attention mechanism</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.31.26.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.31.26.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.31.26.10.1.1.1" class="ltx_text" style="font-size:90%;">0.393</span></span>
</span>
</td>
<td id="S3.T3.4.31.26.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.31.26.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.31.26.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.31.26.11.1.1.1" class="ltx_text" style="font-size:90%;">0.21</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.32.27" class="ltx_tr">
<td id="S3.T3.4.32.27.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.32.27.1.1.1.1" class="ltx_text" style="font-size:90%;">abhishek-</span></span>
<span id="S3.T3.4.32.27.1.1.2" class="ltx_p"><span id="S3.T3.4.32.27.1.1.2.1" class="ltx_text" style="font-size:90%;">thanki </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.32.27.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">86</span></a><span id="S3.T3.4.32.27.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.32.27.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.32.27.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG19</span></span>
<span id="S3.T3.4.32.27.2.1.2" class="ltx_p"><span id="S3.T3.4.32.27.2.1.2.1" class="ltx_text" style="font-size:90%;">DenseNet-121</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.32.27.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.32.27.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.32.27.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.32.27.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.32.27.7.1.1.1" class="ltx_text" style="font-size:90%;">Element-wise multiplication</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.32.27.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.32.27.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.32.27.10.1.1.1" class="ltx_text" style="font-size:90%;">0.462</span></span>
</span>
</td>
<td id="S3.T3.4.32.27.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.32.27.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.32.27.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.32.27.11.1.1.1" class="ltx_text" style="font-size:90%;">0.16</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.33.28" class="ltx_tr">
<td id="S3.T3.4.33.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.33.28.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQA-MED-2020</span></td>
</tr>
<tr id="S3.T3.4.34.29" class="ltx_tr">
<td id="S3.T3.4.34.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.34.29.1.1.1.1" class="ltx_text" style="font-size:90%;">AIML </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.34.29.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S3.T3.4.34.29.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.34.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.34.29.2.1.1.1" class="ltx_text" style="font-size:90%;">Ensemble CNNs</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.34.29.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.34.29.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.34.29.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.34.29.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.34.29.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.34.29.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.34.29.9.1.1.1" class="ltx_text" style="font-size:90%;">Multi-scale and multi-architecture ensemble</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.34.29.10.1.1.1" class="ltx_text" style="font-size:90%;">0.542</span></span>
</span>
</td>
<td id="S3.T3.4.34.29.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.34.29.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.34.29.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.34.29.11.1.1.1" class="ltx_text" style="font-size:90%;">0.496</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.35.30" class="ltx_tr">
<td id="S3.T3.4.35.30.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.35.30.1.1.1.1" class="ltx_text" style="font-size:90%;">TheInception-</span></span>
<span id="S3.T3.4.35.30.1.1.2" class="ltx_p"><span id="S3.T3.4.35.30.1.1.2.1" class="ltx_text" style="font-size:90%;">Team </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.35.30.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.T3.4.35.30.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.35.30.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.35.30.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.35.30.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.35.30.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.35.30.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.35.30.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.35.30.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.35.30.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.35.30.9.1.1.1" class="ltx_text" style="font-size:90%;">Sub-models</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.35.30.10.1.1.1" class="ltx_text" style="font-size:90%;">0.511</span></span>
</span>
</td>
<td id="S3.T3.4.35.30.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.35.30.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.35.30.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.35.30.11.1.1.1" class="ltx_text" style="font-size:90%;">0.48</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">bumjun</span><math id="S3.T3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\_" display="inline"><semantics id="S3.T3.3.3.1.1.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S3.T3.3.3.1.1.1.m1.1.1" xref="S3.T3.3.3.1.1.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.1.1.m1.1b"><ci id="S3.T3.3.3.1.1.1.m1.1.1.cmml" xref="S3.T3.3.3.1.1.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.1.1.m1.1c">\_</annotation></semantics></math><span id="S3.T3.3.3.1.1.1.2" class="ltx_text" style="font-size:90%;">-</span></span>
<span id="S3.T3.3.3.1.1.2" class="ltx_p"><span id="S3.T3.3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">jung </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.3.3.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S3.T3.3.3.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.3.3.5.1.1.1" class="ltx_text" style="font-size:90%;">BioBERT</span></span>
</span>
</td>
<td id="S3.T3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.3.3.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.3.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.3.3.7.1.1.1" class="ltx_text" style="font-size:90%;">MFH with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.3.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.3.3.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.3.3.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.3.3.9.1.1.1" class="ltx_text" style="font-size:90%;">Global Average Pooling</span></span>
</span>
</td>
<td id="S3.T3.3.3.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.3.3.10.1.1.1" class="ltx_text" style="font-size:90%;">0.502</span></span>
</span>
</td>
<td id="S3.T3.3.3.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.3.3.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.3.3.11.1.1.1" class="ltx_text" style="font-size:90%;">0.466</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.36.31" class="ltx_tr">
<td id="S3.T3.4.36.31.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.36.31.1.1.1.1" class="ltx_text" style="font-size:90%;">HCP-MIC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.36.31.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.T3.4.36.31.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.36.31.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.36.31.2.1.1.1" class="ltx_text" style="font-size:90%;">BBN-ResNeSt-50</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.36.31.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.36.31.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.36.31.5.1.1.1" class="ltx_text" style="font-size:90%;">BioBERT</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.36.31.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.36.31.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.36.31.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.36.31.9.1.1.1" class="ltx_text" style="font-size:90%;">Bilateral-Branch Network</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.36.31.10.1.1.1" class="ltx_text" style="font-size:90%;">0.462</span></span>
</span>
</td>
<td id="S3.T3.4.36.31.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.36.31.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.36.31.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.36.31.11.1.1.1" class="ltx_text" style="font-size:90%;">0.426</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.37.32" class="ltx_tr">
<td id="S3.T3.4.37.32.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.37.32.1.1.1.1" class="ltx_text" style="font-size:90%;">NLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.37.32.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">75</span></a><span id="S3.T3.4.37.32.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.37.32.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.37.32.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-50</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.37.32.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.37.32.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.37.32.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.37.32.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.37.32.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.37.32.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.37.32.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.37.32.10.1.1.1" class="ltx_text" style="font-size:90%;">0.441</span></span>
</span>
</td>
<td id="S3.T3.4.37.32.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.37.32.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.37.32.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.37.32.11.1.1.1" class="ltx_text" style="font-size:90%;">0.4</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.38.33" class="ltx_tr">
<td id="S3.T3.4.38.33.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.38.33.1.1.1.1" class="ltx_text" style="font-size:90%;">HARENDRA-</span></span>
<span id="S3.T3.4.38.33.1.1.2" class="ltx_p"><span id="S3.T3.4.38.33.1.1.2.1" class="ltx_text" style="font-size:90%;">KV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.38.33.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S3.T3.4.38.33.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.38.33.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.38.33.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.38.33.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.38.33.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.38.33.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.38.33.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.38.33.7.1.1.1" class="ltx_text" style="font-size:90%;">MFB</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.38.33.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.38.33.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.38.33.10.1.1.1" class="ltx_text" style="font-size:90%;">0.439</span></span>
</span>
</td>
<td id="S3.T3.4.38.33.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.38.33.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.38.33.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.38.33.11.1.1.1" class="ltx_text" style="font-size:90%;">0.378</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.39.34" class="ltx_tr">
<td id="S3.T3.4.39.34.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.39.34.1.1.1.1" class="ltx_text" style="font-size:90%;">Shengyan </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.39.34.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">58</span></a><span id="S3.T3.4.39.34.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.39.34.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.39.34.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.39.34.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.39.34.4.1.1.1" class="ltx_text" style="font-size:90%;">GRU</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.39.34.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.39.34.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.39.34.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.39.34.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (GRU)</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.39.34.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.39.34.10.1.1.1" class="ltx_text" style="font-size:90%;">0.412</span></span>
</span>
</td>
<td id="S3.T3.4.39.34.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.39.34.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.39.34.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.39.34.11.1.1.1" class="ltx_text" style="font-size:90%;">0.376</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.40.35" class="ltx_tr">
<td id="S3.T3.4.40.35.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.40.35.1.1.1.1" class="ltx_text" style="font-size:90%;">kdevqa </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.40.35.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">91</span></a><span id="S3.T3.4.40.35.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.40.35.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.40.35.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.40.35.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.40.35.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.40.35.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.40.35.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.40.35.7.1.1.1" class="ltx_text" style="font-size:90%;">GLU</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.40.35.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.40.35.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.40.35.10.1.1.1" class="ltx_text" style="font-size:90%;">0.35</span></span>
</span>
</td>
<td id="S3.T3.4.40.35.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.40.35.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.40.35.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.40.35.11.1.1.1" class="ltx_text" style="font-size:90%;">0.314</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.41.36" class="ltx_tr">
<td id="S3.T3.4.41.36.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.41.36.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQA-MED-2021</span></td>
</tr>
<tr id="S3.T3.4.42.37" class="ltx_tr">
<td id="S3.T3.4.42.37.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.42.37.1.1.1.1" class="ltx_text" style="font-size:90%;">SYSU-HCP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.42.37.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S3.T3.4.42.37.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.42.37.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.42.37.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNets, VGGNet, and plus HAGAP</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.42.37.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.42.37.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.42.37.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.42.37.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.42.37.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.42.37.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.42.37.9.1.1.1" class="ltx_text" style="font-size:90%;">Hierarchically adaptive global average pooling,</span></span>
<span id="S3.T3.4.42.37.9.1.2" class="ltx_p"><span id="S3.T3.4.42.37.9.1.2.1" class="ltx_text" style="font-size:90%;">Model ensemble,</span></span>
<span id="S3.T3.4.42.37.9.1.3" class="ltx_p"><span id="S3.T3.4.42.37.9.1.3.1" class="ltx_text" style="font-size:90%;">Mixup Augment,</span></span>
<span id="S3.T3.4.42.37.9.1.4" class="ltx_p"><span id="S3.T3.4.42.37.9.1.4.1" class="ltx_text" style="font-size:90%;">Curriculum learning,</span></span>
<span id="S3.T3.4.42.37.9.1.5" class="ltx_p"><span id="S3.T3.4.42.37.9.1.5.1" class="ltx_text" style="font-size:90%;">Label smoothing</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.42.37.10.1.1.1" class="ltx_text" style="font-size:90%;">0.416</span></span>
</span>
</td>
<td id="S3.T3.4.42.37.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.42.37.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.42.37.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.42.37.11.1.1.1" class="ltx_text" style="font-size:90%;">0.382</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.43.38" class="ltx_tr">
<td id="S3.T3.4.43.38.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.43.38.1.1.1.1" class="ltx_text" style="font-size:90%;">Yunnan University </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.43.38.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">101</span></a><span id="S3.T3.4.43.38.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.43.38.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.43.38.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG16</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.43.38.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.43.38.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.43.38.5.1.1.1" class="ltx_text" style="font-size:90%;">BioBERT</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.43.38.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.43.38.7.1.1.1" class="ltx_text" style="font-size:90%;">MFH with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.43.38.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.43.38.9.1.1.1" class="ltx_text" style="font-size:90%;">Global Average Pooling</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.43.38.10.1.1.1" class="ltx_text" style="font-size:90%;">0.402</span></span>
</span>
</td>
<td id="S3.T3.4.43.38.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.43.38.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.43.38.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.43.38.11.1.1.1" class="ltx_text" style="font-size:90%;">0.362</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.44.39" class="ltx_tr">
<td id="S3.T3.4.44.39.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.44.39.1.1.1.1" class="ltx_text" style="font-size:90%;">TeamS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.44.39.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.T3.4.44.39.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.44.39.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.44.39.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNeSt50</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.44.39.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.44.39.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.44.39.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.44.39.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.44.39.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.44.39.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.44.39.9.1.1.1" class="ltx_text" style="font-size:90%;">Bilateral-Branch Networks</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.44.39.10.1.1.1" class="ltx_text" style="font-size:90%;">0.391</span></span>
</span>
</td>
<td id="S3.T3.4.44.39.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.44.39.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.44.39.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.44.39.11.1.1.1" class="ltx_text" style="font-size:90%;">0.348</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.45.40" class="ltx_tr">
<td id="S3.T3.4.45.40.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.45.40.1.1.1.1" class="ltx_text" style="font-size:90%;">Lijie </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.45.40.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S3.T3.4.45.40.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.45.40.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.45.40.2.1.1.1" class="ltx_text" style="font-size:90%;">VGG8</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.45.40.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.45.40.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.45.40.5.1.1.1" class="ltx_text" style="font-size:90%;">BioBERT</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.45.40.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.45.40.7.1.1.1" class="ltx_text" style="font-size:90%;">MFH with Co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.45.40.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.45.40.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.45.40.10.1.1.1" class="ltx_text" style="font-size:90%;">0.352</span></span>
</span>
</td>
<td id="S3.T3.4.45.40.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.45.40.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.45.40.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.45.40.11.1.1.1" class="ltx_text" style="font-size:90%;">0.316</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">IALab</span><math id="S3.T3.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\_" display="inline"><semantics id="S3.T3.4.4.1.1.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S3.T3.4.4.1.1.1.m1.1.1" xref="S3.T3.4.4.1.1.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.1.1.m1.1b"><ci id="S3.T3.4.4.1.1.1.m1.1.1.cmml" xref="S3.T3.4.4.1.1.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.1.1.m1.1c">\_</annotation></semantics></math><span id="S3.T3.4.4.1.1.1.2" class="ltx_text" style="font-size:90%;">PUC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.4.1.1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">76</span></a><span id="S3.T3.4.4.1.1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.4.2.1.1.1" class="ltx_text" style="font-size:90%;">DenseNet-121</span></span>
</span>
</td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.4.5.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.4.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.4.7.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.4.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.4.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.4.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.4.10.1.1.1" class="ltx_text" style="font-size:90%;">0.276</span></span>
</span>
</td>
<td id="S3.T3.4.4.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.4.11.1.1.1" class="ltx_text" style="font-size:90%;">0.236</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.46.41" class="ltx_tr">
<td id="S3.T3.4.46.41.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.46.41.1.1.1.1" class="ltx_text" style="font-size:90%;">TAM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.46.41.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="S3.T3.4.46.41.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.46.41.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.46.41.2.1.1.1" class="ltx_text" style="font-size:90%;">Modified ResNet-34</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.46.41.3.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.46.41.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.46.41.5.1.1.1" class="ltx_text" style="font-size:90%;">GLOVE word embeddings</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.46.41.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.46.41.7.1.1.1" class="ltx_text" style="font-size:90%;">MFB with a co-attention</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.46.41.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.46.41.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.46.41.10.1.1.1" class="ltx_text" style="font-size:90%;">0.255</span></span>
</span>
</td>
<td id="S3.T3.4.46.41.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.46.41.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.46.41.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.46.41.11.1.1.1" class="ltx_text" style="font-size:90%;">0.222</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.47.42" class="ltx_tr">
<td id="S3.T3.4.47.42.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.47.42.1.1.1.1" class="ltx_text" style="font-size:90%;">Sheerin </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.47.42.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">84</span></a><span id="S3.T3.4.47.42.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.47.42.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.47.42.2.1.1.1" class="ltx_text" style="font-size:90%;">VGGNet</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.47.42.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.47.42.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.47.42.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.47.42.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.47.42.7.1.1.1" class="ltx_text" style="font-size:90%;">Element-wise multiplication</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.47.42.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation(LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.9" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.47.42.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.47.42.10.1.1.1" class="ltx_text" style="font-size:90%;">0.227</span></span>
</span>
</td>
<td id="S3.T3.4.47.42.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.47.42.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.47.42.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.47.42.11.1.1.1" class="ltx_text" style="font-size:90%;">0.196</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.48.43" class="ltx_tr">
<td id="S3.T3.4.48.43.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.48.43.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">VQA-RAD</span></td>
</tr>
<tr id="S3.T3.4.49.44" class="ltx_tr">
<td id="S3.T3.4.49.44.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.49.44.1.1.1.1" class="ltx_text" style="font-size:90%;">MTPT-CMSA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.49.44.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S3.T3.4.49.44.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.49.44.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.49.44.2.1.1.1" class="ltx_text" style="font-size:90%;">Multi-ResNet-34</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.49.44.3.1.1.1" class="ltx_text" style="font-size:90%;">Multi-task</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.49.44.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.49.44.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.49.44.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.49.44.7.1.1.1" class="ltx_text" style="font-size:90%;">CSMA</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.49.44.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.49.44.9.1.1.1" class="ltx_text" style="font-size:90%;">Cross-modal self-attention,</span></span>
<span id="S3.T3.4.49.44.9.1.2" class="ltx_p"><span id="S3.T3.4.49.44.9.1.2.1" class="ltx_text" style="font-size:90%;">Multi-task pre-training with extra data</span></span>
</span>
</td>
<td id="S3.T3.4.49.44.10" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T3.4.49.44.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.49.44.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.49.44.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.49.44.11.1.1.1" class="ltx_text" style="font-size:90%;">0.732</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.50.45" class="ltx_tr">
<td id="S3.T3.4.50.45.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.50.45.1.1.1.1" class="ltx_text" style="font-size:90%;">CPRD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.50.45.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S3.T3.4.50.45.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.50.45.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.50.45.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-8</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.50.45.3.1.1.1" class="ltx_text" style="font-size:90%;">Contrastive</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.50.45.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.50.45.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.50.45.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.50.45.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.50.45.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.50.45.9.1.1.1" class="ltx_text" style="font-size:90%;">Constrastive Learning,</span></span>
<span id="S3.T3.4.50.45.9.1.2" class="ltx_p"><span id="S3.T3.4.50.45.9.1.2.1" class="ltx_text" style="font-size:90%;">Knowledge Distillation</span></span>
</span>
</td>
<td id="S3.T3.4.50.45.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.50.45.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.50.45.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.50.45.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.50.45.11.1.1.1" class="ltx_text" style="font-size:90%;">0.727</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.51.46" class="ltx_tr">
<td id="S3.T3.4.51.46.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.51.46.1.1.1.1" class="ltx_text" style="font-size:90%;">MMBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.51.46.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S3.T3.4.51.46.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.51.46.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.51.46.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.51.46.3.1.1.1" class="ltx_text" style="font-size:90%;">ROCO</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.51.46.4.1.1.1" class="ltx_text" style="font-size:90%;">Transformer</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.51.46.5.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.51.46.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.51.46.7.1.1.1" class="ltx_text" style="font-size:90%;">Multi-head attention (Transformer)</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.51.46.8.1.1.1" class="ltx_text" style="font-size:90%;">Generation (Transformer)</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.51.46.9.1.1.1" class="ltx_text" style="font-size:90%;">Pretraining with extra data</span></span>
</span>
</td>
<td id="S3.T3.4.51.46.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.51.46.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.51.46.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.51.46.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.51.46.11.1.1.1" class="ltx_text" style="font-size:90%;">0.72</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.52.47" class="ltx_tr">
<td id="S3.T3.4.52.47.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.52.47.1.1.1.1" class="ltx_text" style="font-size:90%;">QCR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.52.47.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">108</span></a><span id="S3.T3.4.52.47.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.52.47.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.52.47.2.1.1.1" class="ltx_text" style="font-size:90%;">MEVF</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.52.47.3.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.52.47.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.52.47.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.52.47.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.52.47.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN/SAN</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.52.47.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.52.47.9.1.1.1" class="ltx_text" style="font-size:90%;">Question-Conditioned Reasoning,</span></span>
<span id="S3.T3.4.52.47.9.1.2" class="ltx_p"><span id="S3.T3.4.52.47.9.1.2.1" class="ltx_text" style="font-size:90%;">Type-Conditioned Reasoning</span></span>
</span>
</td>
<td id="S3.T3.4.52.47.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.52.47.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.52.47.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.52.47.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.52.47.11.1.1.1" class="ltx_text" style="font-size:90%;">0.716</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.53.48" class="ltx_tr">
<td id="S3.T3.4.53.48.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.53.48.1.1.1.1" class="ltx_text" style="font-size:90%;">MMQ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.53.48.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.T3.4.53.48.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.53.48.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.53.48.2.1.1.1" class="ltx_text" style="font-size:90%;">MMQ</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.53.48.3.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.53.48.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.53.48.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.53.48.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.53.48.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN/SAN</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.53.48.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.53.48.9.1.1.1" class="ltx_text" style="font-size:90%;">Multiple Meta-model Quantifying</span></span>
</span>
</td>
<td id="S3.T3.4.53.48.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.53.48.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.53.48.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.53.48.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.53.48.11.1.1.1" class="ltx_text" style="font-size:90%;">0.67</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.54.49" class="ltx_tr">
<td id="S3.T3.4.54.49.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.54.49.1.1.1.1" class="ltx_text" style="font-size:90%;">MEVF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.54.49.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">64</span></a><span id="S3.T3.4.54.49.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.54.49.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.54.49.2.1.1.1" class="ltx_text" style="font-size:90%;">MEVF</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.54.49.3.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.54.49.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.54.49.5.1.1.1" class="ltx_text" style="font-size:90%;">Not mentioned</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.54.49.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.54.49.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN/SAN</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.54.49.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.54.49.9.1.1.1" class="ltx_text" style="font-size:90%;">Model-Agnostic Meta-Learning,</span></span>
<span id="S3.T3.4.54.49.9.1.2" class="ltx_p"><span id="S3.T3.4.54.49.9.1.2.1" class="ltx_text" style="font-size:90%;">Convolutional Denoising Auto-Encoder</span></span>
</span>
</td>
<td id="S3.T3.4.54.49.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.54.49.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.54.49.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.54.49.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.54.49.11.1.1.1" class="ltx_text" style="font-size:90%;">0.439/0.751 (0.627)</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.55.50" class="ltx_tr">
<td id="S3.T3.4.55.50.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.55.50.1.1.1.1" class="ltx_text" style="font-size:90%;">HQS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.55.50.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S3.T3.4.55.50.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.55.50.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.55.50.2.1.1.1" class="ltx_text" style="font-size:90%;">Inception-Resnet-v2</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.55.50.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.55.50.4.1.1.1" class="ltx_text" style="font-size:90%;">Bi-LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.55.50.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.55.50.6.1.1.1" class="ltx_text" style="font-size:90%;">No</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.55.50.7.1.1.1" class="ltx_text" style="font-size:90%;">Concatenation</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.55.50.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.55.50.9.1.1.1" class="ltx_text" style="font-size:90%;">Question Segregation</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.55.50.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.55.50.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.55.50.10.1.1.1" class="ltx_text" style="font-size:90%;">0.411</span></span>
</span>
</td>
<td id="S3.T3.4.55.50.11" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T3.4.56.51" class="ltx_tr">
<td id="S3.T3.4.56.51.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.56.51.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">PathVQA</span></td>
</tr>
<tr id="S3.T3.4.57.52" class="ltx_tr">
<td id="S3.T3.4.57.52.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.57.52.1.1.1.1" class="ltx_text" style="font-size:90%;">MedFuseNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.57.52.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">79</span></a><span id="S3.T3.4.57.52.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.57.52.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.57.52.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-152</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.57.52.3.1.1.1" class="ltx_text" style="font-size:90%;">ImageNet</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.57.52.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.57.52.5.1.1.1" class="ltx_text" style="font-size:90%;">BERT Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.57.52.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.57.52.7.1.1.1" class="ltx_text" style="font-size:90%;">MedFuseNet</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.57.52.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification/ Generation (LSTM)</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.57.52.9.1.1.1" class="ltx_text" style="font-size:90%;">Image Attention,</span></span>
<span id="S3.T3.4.57.52.9.1.2" class="ltx_p"><span id="S3.T3.4.57.52.9.1.2.1" class="ltx_text" style="font-size:90%;">Image-Question Co-Attention</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.10.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.57.52.10.1.1.1" class="ltx_text" style="font-size:90%;">0.605 (Subset)</span></span>
</span>
</td>
<td id="S3.T3.4.57.52.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.57.52.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.57.52.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.57.52.11.1.1.1" class="ltx_text" style="font-size:90%;">0.636 (Subset)</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.58.53" class="ltx_tr">
<td id="S3.T3.4.58.53.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.58.53.1.1.1.1" class="ltx_text" style="font-size:90%;">MMQ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.58.53.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.T3.4.58.53.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.58.53.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.58.53.2.1.1.1" class="ltx_text" style="font-size:90%;">MMQ</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.58.53.3.1.1.1" class="ltx_text" style="font-size:90%;">None</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.58.53.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.58.53.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.58.53.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.58.53.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN/SAN</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.58.53.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.58.53.9.1.1.1" class="ltx_text" style="font-size:90%;">Multiple Meta-model Quantifying</span></span>
</span>
</td>
<td id="S3.T3.4.58.53.10" class="ltx_td ltx_align_top"></td>
<td id="S3.T3.4.58.53.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.4.58.53.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.58.53.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.58.53.11.1.1.1" class="ltx_text" style="font-size:90%;">0.488</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.59.54" class="ltx_tr">
<td id="S3.T3.4.59.54.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" colspan="11"><span id="S3.T3.4.59.54.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SLAKE</span></td>
</tr>
<tr id="S3.T3.4.60.55" class="ltx_tr">
<td id="S3.T3.4.60.55.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.1.1.1" class="ltx_p" style="width:55.0pt;"><span id="S3.T3.4.60.55.1.1.1.1" class="ltx_text" style="font-size:90%;">CPRD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.4.60.55.1.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S3.T3.4.60.55.1.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite></span>
</span>
</td>
<td id="S3.T3.4.60.55.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.60.55.2.1.1.1" class="ltx_text" style="font-size:90%;">ResNet-8</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.60.55.3.1.1.1" class="ltx_text" style="font-size:90%;">Contrastive</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.4.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.60.55.4.1.1.1" class="ltx_text" style="font-size:90%;">LSTM</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.5.1.1" class="ltx_p" style="width:45.0pt;"><span id="S3.T3.4.60.55.5.1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained Word Embedding</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S3.T3.4.60.55.6.1.1.1" class="ltx_text" style="font-size:90%;">Yes</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.7.1.1" class="ltx_p" style="width:50.0pt;"><span id="S3.T3.4.60.55.7.1.1.1" class="ltx_text" style="font-size:90%;">BAN</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.8.1.1" class="ltx_p" style="width:60.0pt;"><span id="S3.T3.4.60.55.8.1.1.1" class="ltx_text" style="font-size:90%;">Classification</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.9.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T3.4.60.55.9.1.1.1" class="ltx_text" style="font-size:90%;">Constrastive Learning,</span></span>
<span id="S3.T3.4.60.55.9.1.2" class="ltx_p"><span id="S3.T3.4.60.55.9.1.2.1" class="ltx_text" style="font-size:90%;">Pre-training with extra data,</span></span>
<span id="S3.T3.4.60.55.9.1.3" class="ltx_p"><span id="S3.T3.4.60.55.9.1.3.1" class="ltx_text" style="font-size:90%;">Knowledge Distillation</span></span>
</span>
</td>
<td id="S3.T3.4.60.55.10" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S3.T3.4.60.55.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.4.60.55.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.60.55.11.1.1" class="ltx_p" style="width:40.0pt;"><span id="S3.T3.4.60.55.11.1.1.1" class="ltx_text" style="font-size:90%;">0.821</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.4.61.56" class="ltx_tr">
<td id="S3.T3.4.61.56.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="11"><span id="S3.T3.4.61.56.1.1" class="ltx_text" style="font-size:90%;">The “*” means the approach is not proposed during the public challenge period.</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.10056/assets/x17.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Composition of various network architectures as image encoders.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2111.10056/assets/x18.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_align_left ltx_img_landscape" width="461" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_align_left"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Composition of various network architectures as language encoders.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The encoders used in VQA-Med challenges.</span></figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Multi-modal pooling</span> is another important technique used for fusing visual and language features. The basic practice includes concatenation, sum, and element-wise product. In the reviewed papers, direct concatenation is the most widely used fusion method (10 of 46) but shows average performance.
As the outer product may be computation-cost expensive when input vectors are with high dimensionality, researchers have proposed more efficient pooling methods. Multi-modal Compact Bilinear (MCB) pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is a typical method for pooling. It aggregates visual and text features by embedding the image and text features into higher-dimensional vectors and then convolving them with multiplications in the Fourier space for efficiency. The attention mechanism can also be used in the pooling module. Regarding the multi-modal pooling family, there are some other works such as the Multi-modal Factorized High-Order (MFH) pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">107</span></a>]</cite>, the Multi-modal Factorized Bilinear (MFB) pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">106</span></a>]</cite>, e.t.c. The pooling with attention method is the second adopted method (8 of 46). Both the winners in VQA-Med-2018 and VQA-Med-2019 are using MFB pooling with attention solutions.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Among the 46 approaches, the QC-MLB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite> and MedFuseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">79</span></a>]</cite> are the only two works that proposed an innovative fusion algorithm. The QC-MLB uses a multi-glimpse attention mechanism to ensure that the question feature selects the proper image regions to answer. The MedFuseNet uses two attention modules: Image Attention and Image-Question Co-Attention to let the image feature and question feature interact twice. The QC-MLB and MedFuseNet all show performance improvement. However, the medical VQA research focusing on fusion schemes is still insufficient at this stage.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Answering Component</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Among the 46 reviewed approaches, 33 approaches choose the classification mode for output, and 8 methods choose the generation mode. Some methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">73</span></a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">112</span></a>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">79</span></a>]</cite> use a switching strategy to adopt both classification and generation. The selection of the classification method or generation method reflects the length distribution of ground-truth answers belonging to a single phrase. The classification mode will have an advantage within a small answer space. However, it can become difficult if the answer candidates are longer and with more complex information, such as lesion descriptions.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Other Techniques</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Other than research in the basic components, some task-specific strategies are also applied to improve performance. Sub-task strategy is the approach that divides the overall task into several sub-tasks and assigns branch models. In these cases, an extra task classification module is applied to select the corresponding model by a particular question category or a type of image modality. It is often used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">112</span></a>, <a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">90</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">80</span></a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> especially in VQA-Med-2019 (questions are only in four categories) and VQA-Med-2020(questions are all in one category but in two types). The reason is that the questions have only distinct categories to be divided easily. These multiple model approaches may improve the single model effectiveness but lead to another risk that they may suffer from inappropriate model allocation due to the erroneous results from the sub-task classification module. Another problem is that these approaches cannot be generalized to other tasks if the question categories are different.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Another frequently applied technique is Global Average Pooling(5 of 46). Global Average Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> is using the average of feature maps to replace the last fully connected layers. It is believed to produce better image representation. Other techniques include Embedding-based Topic Model, Question-Conditioned Reasoning, and Image Size encoder.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Performance Comparison</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">As the performance shown in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table3</span>, the keys of state-of-the-art approaches on each dataset are quite different. Among the VQA-Med-2018 approaches, the top team Chakri <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is the only team that uses generative output. For the VQA-Med-2019, the key factor is the language encoder model. The BERT users significantly achieve a higher performance than the LSTM users. In VQA-Med-2020 and VQA-Med-2021, the top teams AIML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and SYSU-HCP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> all removed the language encoders and adopted ensemble strategy for image encoder, as the question categories were reduced. For VQA-RAD, the top three teams all have extra pre-training on their image encoders. The above comparison shows the enhancement of the image encoder is the essential ingredient of achieving state-of-the-art performance in most medical VQA datasets.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">Another finding is that over most datasets, the approaches with attention-based fusion algorithms are averagely outperforming the ones without attention. For example in VQA-Med-2019, the approaches with attention-based fusion have an average accuracy of 0.576, while the others have an average accuracy of 0.522. A similar conclusion can also be drawn for other datasets. It suggests that attention-based fusion algorithms are suitable for medical VQA datasets.</p>
</div>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>Overall Discussion</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">In the method survey, we reviewed 45 papers on medical VQA approaches, including 32 challenges work notes and 13 conference/journal papers. Although medical VQA research just started in 2018, there have already been various methods proposed and explored. Since the challenges only have restricted time for problem-solving, the work notes tend to make direct applications of well-verified deep learning models. They have a high proportion of using pre-trained components. On the other hand, they also develop some task-oriented techniques according to the intrinsic data property. Notably, there are two teams <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> observing the long-tailed distribution in their tasks and introducing the Bilateral-Branch Network (BBN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">110</span></a>]</cite> to deal with the imbalanced data. Despite insufficient innovations, the challenge teams’ observations and strategies are valuable and inspiring.</p>
</div>
<div id="S3.SS8.p2" class="ltx_para">
<p id="S3.SS8.p2.1" class="ltx_p">The conference/journal papers explore further problem solutions. According to the 13 conference/journal papers, the current research is more focused on the image encoder than the other component, which is quite different from the general VQA research. Researchers introduce popular CV field ideas such as meta-learning and contrastive learning to enhance the image encoder. Pre-training is commonly applied in both the image encoder and the language decoder for answering. Auxiliary data is also a simple but efficient solution. Current research shows that acquiring a generalized image encoder is a high-priority and specific task in medical VQA research.</p>
</div>
<div id="S3.SS8.p3" class="ltx_para">
<p id="S3.SS8.p3.1" class="ltx_p">Another research topic that appeared in those technical papers is the generalizability of proposed methods. Among the 13 papers, only 5 papers are aware of showing their generalizability and evaluating their approaches on multiple datasets. And only one team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">94</span></a>]</cite> evaluate their approach not only on a medical VQA dataset but also on general VQA datasets. It will be a future standard that an approach should be evaluated on multiple datasets.</p>
</div>
<div id="S3.SS8.p4" class="ltx_para">
<p id="S3.SS8.p4.1" class="ltx_p">Interpretability is also a demand in medical AI research. Among the 13 papers, 5 papers have illustrated the model visualization with techniques such as GradCAM and attention visualization. Although the SLAKE dataset has the annotation for semantic segmentation and object bounding box, none of them use the SLAKE to evaluate their visualization.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Medical VQA v.s. General VQA</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Medical VQA and general VQA are two approaches that harness the power of visual content to answer questions but in distinct domains and contexts. While both aim to interpret and provide meaningful responses based on visual information, they differ significantly in their applications, objective, datasets, methods, domain knowledge, and evaluation. By understanding the unique characteristics and requirements of each approach, we can gain insights into their respective contributions and impact in their respective fields. In this section, we undertake a comparative analysis of medical VQA and general VQA across various dimensions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Application</span> Commonly, general VQA and medical VQA have potential applications of human-computer interaction such as image interpreting and education. The difference is the general VQA can be embedded into information retrieval such as search engines and virtual assistants. Also, it has potential applications in further human-computer interaction such as navigation and robotics. Medical VQA focuses on specific areas such as clinical decision support, telemedicine, and patient empowerment. Overall, the modes of interaction can be more diverse in general VQA applications.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Objective</span> While both medical VQA and general VQA aim to bridge the gap between visual perception and natural language understanding, they differ in their functions and the specific information they focus on. medical VQA is designed to assist in medical diagnosis, treatment, and decision-making by leveraging visual medical data and providing accurate answers to medical-related questions. Hence, the objective of medical VQA systems is to gain ability in medical image understanding, abnormality locating, and terminology communication.
On the other hand, the objective of general VQA is to enable machines to understand and respond to questions about general visual content, such as everyday images or videos. General VQA systems aim to comprehend the visual scene, recognize objects, infer relationships and attributes, and generate accurate answers in natural language.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Datasets</span> As discussed in Sec. <a href="#S2.SS1.SSS9" title="2.1.9 Discussion ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.9</span></a>, the medical VQA datasets have a smaller quantity because of the limit of data source and high knowledge requirement for annotation. Also, most of the current medical VQA datasets are less diverse in the question category. Besides, each dataset is designed with specific objectives in mind, which can vary depending on the dataset’s focus and intended applications. The general VQA datasets have already focused on specific problems such as answer balance, while most of the medical VQA datasets are at the stage of expanding topics and application scenes.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Method</span> The development of methods has been hindered by the complexity of the dataset. General VQA methods have expanded to encompass various topics, such as multi-task learning, logical reasoning, and interaction with the environment. In the general domain, the research on image and language understanding has primarily been undertaken by the upstream community, namely computer vision and NLP. General methods often rely on universal pre-trained models from the upstream community, with a primary focus on multi-modal fusion and reasoning. Conversely, the medical community lacks such universal resources, resulting in medical VQA works placing less emphasis on fusion algorithms and instead prioritizing image encoder pre-training as discussed in Sec. <a href="#S3.SS8" title="3.8 Overall Discussion ‣ 3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.8</span></a>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Domain knowledge</span> In general VQA, domain knowledge is instrumental in designing effective models and algorithms. For example, different strategies may be needed for object recognition, scene understanding, or reasoning about relationships between objects in images. In the case of medical VQA, domain knowledge is crucial for understanding the specific medical concepts, terminology, and context involved in the questions and images. Understanding the unique characteristics and challenges of medical imaging data, such as different modalities (e.g., X-ray, MRI), image quality variations, and anatomical complexities, is essential for developing robust and accurate medical VQA systems. It is challenging to integrate medical domain knowledge into medical VQA design.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">Evaluation</span> Due to the critical nature of medical information, medical VQA systems are expected to provide highly accurate and reliable answers. Evaluation metrics for Medical VQA might emphasize medical relevance, precision, and recall, considering the importance of accurate medical information. General VQA systems may prioritize a wider range of plausible answers rather than strictly aiming for accuracy. General VQA evaluation metrics may prioritize overall answer correctness, language understanding, and image understanding. Especially, medical datasets are naturally imbalanced distribution and long-tail distribution. Traditional metrics may not fully capture the performance and provide misleading results. The medical VQA can introduce metrics such as Average Precision (AP) for bias problems, while the general VQA pursues balanced distribution.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Challenge and Future Works</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">After reviewing medical VQA datasets and approaches, we identify some existing problems. Compared to the general domain VQA, the specific requirement and practical implementation scene also lead to unique and new challenges. Besides, the works on general-domain VQA provide some inspiration for future research direction.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Question Diversity</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Question diversity is one of the most significant challenges of medical VQA. The VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> investigates the natural questions in clinical conversation. The questions can be categorized into modality, plane, organ system, abnormality, object/condition presence, positional reasoning, color, size, attribute other, counting, and other. In other datasets such as VQA-Med-2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and PathVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, the question categories tend to be less diverse than the VQA-RAD dataset. In the RadVisDial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, VQA-Med-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and VQA-Med-2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the question category is reduced to the abnormality presence only. However, most questions about an abnormality in existing datasets are about presence without further inquiry, like the location of tumors or tumor size. Therefore, questions remain to be added to diversify the medical VQA dataset.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To create better medical VQA and benefit clinical workflow integration, future research should be conducted about identifying the useful question categories in practical requirements. For example, the imaging modality and examined organ are totally not required because they are already in the study record information. This kind of information may be delivered to the end users outside the VQA system. First, the data source can be expanded. The synthesized QA pairs in current medical VQA datasets are usually created from image captions and medical reports. However, the information in captions and reports is restricted to specific topics. Therefore, more data sources, such as the textbook, should be considered to provide more textural corpus. Second, collecting real-world clinical conversations, especially with patients, will help researchers better understand the practical requirements. Third, the restriction that the question must be relevant to the image content should be broken. However, in a realistic clinical scene, the conversation content often exceeds the presented image content, for example, explaining the future risks of the abnormality or predicting disease progression. Overall, the question diversity is an essential consideration in dataset design to empower medical VQA with comprehensive coverage, real-world relevance, and user engagement.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Besides dataset design, question diversity raises a challenge in method development. To answer the diverse questions, the medical VQA systems require various reasoning abilities. For example, as a sample shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.1.2 VQA-RAD ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, to answer question <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">“What is the function of the rightmost organ in this picture?”</span>, the model should understand the region described, identify the organ in the region of interest, and finally answer the function of the organ. Besides the basic image and language understanding, medical domain knowledge is a critical ability required, which includes knowledge of anatomical structures, medical procedures, diseases, medical imaging modalities, treatment options, and clinical practices. More specially, for the question categories in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Datasets ‣ 2 Datasets and performance metrics ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">Modality, Plane</span> need knowledge about radiology examination; <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">Organ, System, Abnormality, Object/Condition Presence</span> need knowledge about human anatomy and medicine; <span id="S5.SS1.p3.1.4" class="ltx_text ltx_font_italic">Positional Reasoning, Color, Size, Attribute Other, Counting</span> need general knowledge and reasoning; <span id="S5.SS1.p3.1.5" class="ltx_text ltx_font_italic">Knowledge Graph</span> need to combine the upon knowledge with the knowledge triplets given in dataset. Therefore, to address question categories correspondingly, the future medical VQA should be equipped with computer-aid diagnosis, general language understanding, reasoning, knowledge integration, and contextual understanding.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Finally, the evaluation of the model performance should also consider the question of diversity. The imbalanced question category distribution can provide misleading results under overall evaluation. To mitigate the impact of imbalanced data on overall evaluation, it is important to consider specialized evaluation strategies. This can involve using category-specific metrics or weighted overall metrics. Furthermore, incorporating language-based evaluation metrics will be a challenge with the presence of verbose answers. Both the correctness of the answers and the quality of the language used should be evaluated to encourage a more comprehensive medical VQA system.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Integrating the Extra Medical Information</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Another challenge for the medical VQA is to integrate the extra information into the inference procedure. For example, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib46" title="" class="ltx_ref">Kovaleva et al.</a></cite> found that incorporating the medical history of the patient leads to better performance in answering questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>EHR</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">An electronic health record (EHR) contains a patient’s medical history, diagnoses, medications, treatment plans, etc. In the medical AI domain, the EHR has been proven useful for disease prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">81</span></a>]</cite>. Hence it should also be helpful in medical VQA. To support the research of EHR in medical VQA, a new dataset containing the original EHR or synthetical EHR is required. Furthermore, the VQA model should also be modified because the EHR contains a lot of metric variables that should be treated as numbers other than natural language. As far as we know, no previous research has investigated combining EHR and computer vision. Using numeric variables as VQA input and the corresponding fusion algorithm also has not been researched. Therefore encoding EHR in VQA is meaningful and worth studying.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Multiple Images</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">In the medical scene, especially radiology, it is general that the study is based on multiple images other than a single image. Multiple images can contain different scan planes and sequential slices to support the decision-making of medical professionals. For example, in MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, a radiology report is often associated with two images of postero-anterior view and lateral view. However, in RadVisDial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the authors keep only the postero-anterior view, although the abandoned images are also informative.
The reason may be that the common VQA or VisDial models only support single-image input. Therefore, the VQA datasets with multiple input images are required to support the model research. Future research should also be devoted to developing the VQA model by taking multiple images as input.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Interpretability and Reliability</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Interpretability is a long-standing problem of deep learning. As the VQA models are commonly based on deep learning methods, the medical VQA has to deal with interpretability. To the medical VQA system, interpretability determines the reliability of the predicted answer, and it is more important than that in the general domain because the wrong decision may lead to catastrophic consequences. The general-domain VQA researchers have addressed this problem and investigated several directions to evaluate the inference ability of a model.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Unimodal Bias of VQA Models</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">In the VQA field, the unimodal bias means that the model may answer the question based on statistical regularities from one modality without considering the other modality. Particularly, the bias to language input is also called language prior. The researchers have noticed bias since the first VQA dataset was proposed. They tested the question-only model “LSTM Q”, which has only a slightly lower performance compared with the standard model “LSTM Q+I” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and indicates the language prior. To better measure the effect of language prior problem of VQA models, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib2" title="" class="ltx_ref">Agrawal et al.</a></cite> presented new splits of the VQA v1 and VQA v2 datasets with changing priors (respectively VQA-CP v1 and VQA-CP v2). They also proposed a Grounded Visual Question Answering model (GVQA) to prevent the model from “cheating” by primarily relying on priors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Alternatively, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Ramakrishnan et al.</span></a></cite> proposed a novel regularization scheme that poses training as an adversarial game between the VQA model and a question-only model to discourage the VQA model from capturing language biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">72</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib19" title="" class="ltx_ref">Cadene et al.</a></cite> proposed the RUBi to reduce biases in VQA models. It minimizes the importance of the most biased examples and implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">The unimodal bias in general-domain VQA has been discussed in many aspects, such as benchmark and training schemes. These provide a good starting point for further work in medical VQA. Benchmarks similar to VQA-CP are required to investigate the unimodal bias problem of models. As the quantity of medical VQA datasets is not so large as general-domain VQA datasets, the solution for unimodal bias also needs research.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>External Knowledge</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">In VQA research, external knowledge means the model may need to incorporate an external knowledge base to infer the answer besides the image and the question representation. In general-domain VQA, an “adult-level common sense” is required to support the cognition and inference in question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">100</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Wang et al.</span></a></cite> proposed an approach named “Ahab” to provide explicit knowledge-base reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">95</span></a>]</cite>. They also provide a dataset KB-VQA and a protocol to evaluate the knowledge-based methods. Furthermore, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Wang et al.</span></a></cite> proposed the FVQA dataset providing a supporting fact that is critical for answering each visual questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">96</span></a>]</cite>. On the other hand, to test the VQA methods’ ability to retrieve relevant facts without a structured knowledge base, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Marino et al.</span></a></cite> proposed the OK-VQA dataset, including only questions that require external knowledge resources. They also proposed a knowledge-based baseline named “ArticleNet”, which retrieved some articles from Wikipedia for each question image pair and then trained a network to find the answer in the retrieved articles.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">The challenge in medical VQA is that the knowledge requirement is much higher than “adult-level common sense”. The future research includes identifying questions that required external knowledge, exploiting the existing medical knowledge base such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">63</span></a>]</cite>, and building a medical VQA dataset with a structured or unstructured knowledge base. There are also additional studies required to determine whether the medical knowledge-based VQA needs specific approaches.</p>
</div>
<div id="S5.SS3.SSS2.p3" class="ltx_para">
<p id="S5.SS3.SSS2.p3.1" class="ltx_p">Moreover, external knowledge can also expand the function of medical VQA in clinic workflow. In the practical environment, the questions from the patient can start from image findings and move further to topics like possible disease, potential risk, and disease control. To answer the questions of those topics, an external knowledge base is required. A medical VQA system equipped with external knowledge will have a strong topic capability and become a powerful tool in clinic workflow.</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Evidence Verification</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">Evidence verification is a common method to verify model interpretability. In the medical domain, illustrating evidence of the abnormality will gain the trust of medical professionals and patients. A typical method to collect model evidence is through visualization, such as feature attribution or saliency map. In terms of the multi-modal models with attention mechanism, an attention map can be visualized to show the image regions that lead to the answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">104</span></a>]</cite>. By comparing the attention map with annotated human visual attention, the researchers can verify whether the VQA methods use the correct evidence to get the answer. In general-domain VQA, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib23" title="" class="ltx_ref">Das et al.</a></cite> proposed the VQA-HAT dataset containing annotated human visual attention to evaluate the attention maps both qualitatively (via visualizations) and quantitatively (via rank-order correlation) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. To extend the verification to textual evidence, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Park et al.</span></a></cite> proposed the VQA-X dataset containing human-annotated both textual explanations and visual explanations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">68</span></a>]</cite>. To address text-based question-answer pairs in the VQA task, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Wang et al.</span></a></cite> proposed the EST-VQA dataset annotating the bounding boxes of correct text clues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">98</span></a>]</cite>. Alternatively, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib38" title="" class="ltx_ref">Jiang et al.</a></cite> proposed the IQVA dataset annotating the eye-tracking data as human visual attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S5.SS3.SSS3.p2" class="ltx_para">
<p id="S5.SS3.SSS3.p2.1" class="ltx_p">Although those studies have established a complete scheme, including the annotated evidence, reasoning model, and evaluation metrics, several problems are to be explored when applying evidence verification in medical VQA. First, the previous studies showed different annotation modes such as bounding box and eye-tracking. It is unknown what is the most suitable mode for medical VQA. For some question categories, such as the modality, the answer evidence can be hard to explain with a certain region and may need a text-based explanation. Second, manual annotation for a medical task is expensive because it requires the professional skill of radiologists. In this situation, the existing medical multi-task datasets such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">89</span></a>]</cite> can be beneficial by providing comprehensive knowledge of medical terms. Therefore, the evidence verification approaches for medical VQA will need to consider generating a text-based explanation and utilizing the existing annotated medical datasets.</p>
</div>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4 </span>Summarization</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p id="S5.SS3.SSS4.p1.1" class="ltx_p">These three directions examine the different reasoning procedures of VQA models and provide performance evaluation methods. The VQA models have to look into the image, learn the extra knowledge, and find the correct evidence to pass the “exams”. To our best knowledge, there are only a few works considering interpretability in medical VQA, such as the SLAKE providing lesion annotation and the CPRD, MedFuseNet providing visualization. However, there is no work really performing quantified measurement of interpretability. Some currently feasible ways include giving the attribution of images against problems and using the SLAKE dataset to calculate the overlapping rate between saliency maps and the annotations. It remains to be done building more quantified benchmarks to evaluate medical VQA interpretability.</p>
</div>
<div id="S5.SS3.SSS4.p2" class="ltx_para">
<p id="S5.SS3.SSS4.p2.1" class="ltx_p">If the medical VQA system can be verified with its inference ability, it will become a more convincing and reliable tool. It is also helpful to present the knowledge and evidence used in inference explicitly. Hence the evaluation of inference ability should be regarded as a more important benchmark than the answer accuracy. Hopefully, the medical VQA will answer the question “why” in the nearest future.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Generalizability</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Generalizability is a universal topic in medical AI research and an inevitable issue for applications running in practical scenarios. The cause of the Generalizability problem is the practical input can be out of the distribution (OOD) of the training data. The factors can be various, such as the patient race and imaging devices. The gap between different data distributions is named <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">domain shift</span>. For downstream tasks like VQA, the generalizability problem is two-fold and more comprehensive. Firstly, as discussed in Sec. <a href="#S3" title="3 Methods ‣ Medical Visual Question Answering: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the VQA models usually consist of several sub-models which may have a pre-trained weight. At this stage, a domain shift between pre-train data and current training data is introduced. Secondly, after a VQA model is developed and deployed, there will be a domain shift between training data and practical data, which is usually evaluated by cross-dataset validation.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Among the reviewed methods, several approaches have considered the domain shift in sub-model pre-training and acquiring medical pre-trained models as image encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. However, there is no study considering the potential domain shift in language encoders. Also, the domain shift crossing medical VQA datasets has not yet been considered and studied. With the growing number of medical VQA datasets works, there has been sufficient material for transferring learning studies such as domain adaptation and domain generalization. Measuring and improving model generalizability will be a feasible and meaningful research topic.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Large Language Models</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Large Language Models (LLMs) are highly complex artificial intelligence systems that have the capability to learn from the vast amounts of available text data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">71</span></a>]</cite>. Discriminative LLMs (e.g., BERT) have been well-studied for answering questions in a classification manner. However, the generative capability of LLMs enables them to answer diverse questions more flexibly and effectively with human-like responses. Generative LLMs have already started to show their potential and remarkable results in the field of the medical domain, where models like Open AI’s GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> have successfully passed a part of the US medical licensing exam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite>. Thus, it is interesting to explore the potential approaches of how these LLMs could be of aid in the task of VQA. As the task of medical VQA involves understanding the spatial relationships in a medical image, it is challenging for LLMs that have been primarily trained on text data. However, LLMs have the potential to be integrated into improving the QA capability of a model. Below we describe certain directions of how LLMs could be beneficial to the task of medical VQA.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">As a straightforward way, LLMs can be used to process the questions and generate responses based on the extracted features from the images provided by an image-based model. For example in the recent ChatCAD framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">97</span></a>]</cite>, the authors converted the outputs provided by different image-based models (classification, segmentation) into textual information and provided it to ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">78</span></a>]</cite> for refinement and understanding. Then, follow-up queries were asked by the authors about the disease condition of the patient and ChatGPT provided comprehensive answers by combining the given outputs from the image-based models and its learned knowledge from the huge corpus of data. Another recent framework Prophet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">103</span></a>]</cite> improved the answer generation capability of a trained VQA model by encoding answer heuristics generated by the VQA model into a prompt for GPT-3 to better comprehend the task thus making better use of the potential of GPT-3. Prophet framework was able to achieve a new state-of-the-art performance on two challenging knowledge-based VQA datasets. Some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">66</span></a>]</cite> have also employed LLMs to improve the interpretability of the existing image-based models by generating specific attributes that could be analyzed in an image. In addition to the above directions, LLMs can also adapt to multiple modalities, such as patient history, demographics, lab results, etc. which if combined with the current medical VQA models could lead to a more comprehensive analysis and better answering of the related questions.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">Although LLMs have a huge potential to be integrated into the task of medical VQA, there are some associated potential pitfalls. First, although LLMs are trained on vast amounts of text corpus data, they might not possess the same domain-specific knowledge expertise as medical professionals in specific domains. Second, LLMs have learned biases or misinformation inherently present in their training data, which could lead to incorrect conclusions. Third, LLMs are not able to express uncertainty for their answers, and thus LLMs could confidently generate incorrect responses or hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">65</span></a>]</cite>, which could be fatal in medical decision-making. So, careful attention and high-level collaboration are required to mitigate the above issues. Specifically, involving medical experts in the framework development and fine-tuning process can help to tailor the model responses corresponding to specific medical domains. Moreover, diverse, accurate, and representative real-world medical training data could help to minimize the above biases and generate factually correct responses.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Integration in Medical Workflow</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">The community has a long history of trying to integrate AI decision-supporting systems into the clinical flow. Integrating the medical VQA system into clinical practice will provide efficient communication and serve as an effective assistant.
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib36" title="" class="ltx_ref">Hekler et al.</a></cite> found that the combination of human and artificial intelligence can achieve superior results over the independent results of both of these systems in skin cancer classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
Furthermore, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Tschandl et al.</span></a></cite> found that physicians with AI-based support outperformed either AI or physicians. Meanwhile, the least experienced clinicians gain the most from AI-based support <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">Tschandl et al.</span></a></cite> also experimented with the accuracy improvements of human-computer collaboration in skin cancer recognition and found multiclass probabilities outperformed either content-based image retrival (CBIR) or malignancy probability in the mobile technolog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">88</span></a>]</cite>. These findings indicate that many factors must be considered in developing successful outcomes for medical AI-supporting system integration, such as clinicians’ cognitive style, cognitive error, personality, experience, and acceptance of AI.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">Moreover, most AI decision support systems used in the clinical setting are good at addressing prepared questions only. In contrast, the advantage of medical VQA is that it understands free-form questions and provides real-time communication. To address this advantage, the workflow can remove redundant querying and get simplified. For example, the presence of abnormality may be always asked, and it can be given before the QA session. Hence, selecting useful question categories and keeping online learning may help to maintain a functional question database. The ultimate goal of a medical VQA system is answering open-ended questions (what, which, e.t.c.).
This goal will introduce more complexity and uncontrolled factors under the “human-in-the-loop” hypothesis. For example, the medical VQA system may need to resolve a dispute when its opinion is different from the clinician’s. Hence, the training corpus may also prepare for a negotiation. More efforts and studies need to be conducted in this field for a successful deployment of medical VQA into the clinical pipeline. Improving workflow efficiency and service quality is also required to study multiple aspects, such as time spent, collaborative answer accuracy, and user satisfaction.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This article presented a survey of the datasets and approaches of medical VQA. We collect information about 8 medical VQA datasets and 45 papers describing medical VQA approaches. We conduct a comprehensive discussion on dataset creation, approach framework, approach components, and corresponding techniques. In addition to the descriptive review, we identified some challenges worth exploring in future research. The medical VQA system mainly faces the following four challenges: first, how to get the system answers a range of more comprehensive question categories; second, how to combine medical features with the task; third, how to verify the evidence of an answer to make it more convincing; forth, how to make the system not bias to any modality; finally, how to maximize the benefit of the medical VQA in the workflow. For future work, we propose the following directions. To investigate potential implementation in a real-world scene is necessary. Furthermore, we should pay attention to the conversation between medical professionals and non-professionals. The advantage of VQA is to understand natural language questions and help non-professionals. We should also introduce meaningful ideas in the general domain, such as evidence verification, bias analysis, external knowledge database, etc. These will help us build a practicable and convincing medical VQA system toward medical AI’s ultimate goal.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Declaration of Interests</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We declare that we have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abacha et al. [2018]</span>
<span class="ltx_bibblock">
Abacha, A.B., Gayen, S.,
Lau, J.J., Rajaraman, S.,
Demner-Fushman, D., 2018.

</span>
<span class="ltx_bibblock">NLM at ImageCLEF 2018 visual question answering
in the medical domain., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. [2018]</span>
<span class="ltx_bibblock">
Agrawal, A., Batra, D.,
Parikh, D., Kembhavi, A.,
2018.

</span>
<span class="ltx_bibblock">Don’t just assume; look and answer: Overcoming priors
for visual question answering, in: 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 4971–4980.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Sadi et al. [2020]</span>
<span class="ltx_bibblock">
Al-Sadi, A., Al-Theiabat, H.,
Al-Ayyoub, M., 2020.

</span>
<span class="ltx_bibblock">The inception team at VQA-Med 2020: Pretrained
VGG with data augmentation for medical vqa and vqg, in:
CLEF 2020 Working Notes.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Sadi et al. [2019]</span>
<span class="ltx_bibblock">
Al-Sadi, A., Talafha, B.,
Al-Ayyoub, M., Jararweh, Y.,
Costen, F., 2019.

</span>
<span class="ltx_bibblock">JUST at ImageCLEF 2019 visual question answering
in the medical domain., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allaouzi and Ahmed [2018]</span>
<span class="ltx_bibblock">
Allaouzi, I., Ahmed, M.B.,
2018.

</span>
<span class="ltx_bibblock">Deep neural networks and decision tree classifier for
visual question answering in the medical domain., in:
CLEF (Working Notes).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allaouzi et al. [2019]</span>
<span class="ltx_bibblock">
Allaouzi, I., Ahmed, M.B.,
Benamrou, B., 2019.

</span>
<span class="ltx_bibblock">An encoder-decoder model for visual question
answering in the medical domain., in: CLEF (Working
Notes).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ambati and Reddy Dudyala [2018]</span>
<span class="ltx_bibblock">
Ambati, R., Reddy Dudyala, C.,
2018.

</span>
<span class="ltx_bibblock">A sequence-to-sequence model approach for imageclef
2018 medical domain visual question answering, in: 2018
15th IEEE India Council International Conference (INDICON), pp.
1–6.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1109/INDICON45594.2018.8987108" title="" class="ltx_ref">10.1109/INDICON45594.2018.8987108</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. [2018]</span>
<span class="ltx_bibblock">
Anderson, P., He, X.,
Buehler, C., Teney, D.,
Johnson, M., Gould, S.,
Zhang, L., 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning
and visual question answering, in: 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 6077–6086.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. [2016]</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M.,
Darrell, T., Klein, D.,
2016.

</span>
<span class="ltx_bibblock">Neural module networks, in: 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 39–48.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A.,
Lu, J., Mitchell, M.,
Batra, D., Zitnick, C.,
Parikh, D., 2015.

</span>
<span class="ltx_bibblock">VQA: Visual question answering, in:
2015 IEEE International Conference on Computer Vision
(ICCV), IEEE Computer Society, Los
Alamitos, CA, USA. pp. 2425–2433.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2021]</span>
<span class="ltx_bibblock">
Bai, H., Shan, X., Huang,
Y., Wang, X., 2021.

</span>
<span class="ltx_bibblock">MVQAS: A Medical Visual Question Answering System.
Association for Computing Machinery,
New York, NY, USA.

</span>
<span class="ltx_bibblock">p. 4675–4679.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al. [2019]</span>
<span class="ltx_bibblock">
Bansal, M., Gadgil, T.,
Shah, R., Verma, P.,
2019.

</span>
<span class="ltx_bibblock">Medical visual question answering at Image CLEF
2019-VQA Med, in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha et al. [2020]</span>
<span class="ltx_bibblock">
Ben Abacha, A., Datla, V.V.,
Hasan, S.A., Demner-Fushman, D.,
Müller, H., 2020.

</span>
<span class="ltx_bibblock">Overview of the VQA-Med task at ImageCLEF 2020:
Visual question answering and generation in the medical domain, in:
CLEF 2020 Working Notes,
CEUR-WS.org, Thessaloniki, Greece.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha et al. [2019]</span>
<span class="ltx_bibblock">
Ben Abacha, A., Hasan, S.A.,
Datla, V.V., Liu, J.,
Demner-Fushman, D., Müller, H.,
2019.

</span>
<span class="ltx_bibblock">VQA-Med: Overview of the medical visual question
answering task at imageclef 2019, in: CLEF2019 Working
Notes, CEUR-WS.org, Lugano,
Switzerland.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha et al. [2021]</span>
<span class="ltx_bibblock">
Ben Abacha, A., Sarrouti, M.,
Demner-Fushman, D., Hasan, S.A.,
Müller, H., 2021.

</span>
<span class="ltx_bibblock">Overview of the VQA-Med task at ImageCLEF 2021:
Visual question answering and generation in the medical domain, in:
CLEF 2021 Working Notes,
CEUR-WS.org, Bucharest, Romania.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bounaama and Abderrahim [2019]</span>
<span class="ltx_bibblock">
Bounaama, R., Abderrahim, M.E.A.,
2019.

</span>
<span class="ltx_bibblock">Tlemcen University at ImageCLEF 2019 visual
question answering task., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B.,
Ryder, N., Subbiah, M.,
Kaplan, J.D., Dhariwal, P.,
Neelakantan, A., Shyam, P.,
Sastry, G., Askell, A., et al.,
2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems
33, 1877–1901.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. [2023]</span>
<span class="ltx_bibblock">
Bubeck, S., Chandrasekaran, V.,
Eldan, R., Gehrke, J.,
Horvitz, E., Kamar, E.,
Lee, P., Lee, Y.T., Li,
Y., Lundberg, S., et al., 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early
experiments with gpt-4.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2303.12712 .

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadene et al. [2019]</span>
<span class="ltx_bibblock">
Cadene, R., Dancette, C.,
Cord, M., Parikh, D., et al.,
2019.

</span>
<span class="ltx_bibblock">RUBi: Reducing unimodal biases for visual question
answering.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems
32, 841–852.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Chen, G., Gong, H., Li,
G., 2020.

</span>
<span class="ltx_bibblock">HCP-MIC at VQA-Med 2020: Effective visual
representation for medical visual question answering, in:
CLEF 2020 Working Notes.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. [2014]</span>
<span class="ltx_bibblock">
Cho, K., van Merriënboer, B.,
Gulcehre, C., Bahdanau, D.,
Bougares, F., Schwenk, H.,
Bengio, Y., 2014.

</span>
<span class="ltx_bibblock">Learning phrase representations using RNN
encoder–decoder for statistical machine translation, in:
Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Association
for Computational Linguistics, Doha, Qatar. pp.
1724–1734.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3115/v1/D14-1179" title="" class="ltx_ref">10.3115/v1/D14-1179</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cross et al. [2020]</span>
<span class="ltx_bibblock">
Cross, N.M., Wildenberg, J.,
Liao, G., Novak, S.,
Bevilacqua, T., Chen, J.,
Siegelman, E., Cook, T.S.,
2020.

</span>
<span class="ltx_bibblock">The voice of the radiologist: Enabling patients to
speak directly to radiologists.

</span>
<span class="ltx_bibblock">Clinical imaging 61,
84–89.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. [2016]</span>
<span class="ltx_bibblock">
Das, A., Agrawal, H.,
Zitnick, C.L., Parikh, D.,
Batra, D., 2016.

</span>
<span class="ltx_bibblock">Human attention in visual question answering: Do
humans and deep networks look at the same regions?, in:
Conference on Empirical Methods in Natural Language
Processing.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2019]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W.,
Lee, K., Toutanova, K.,
2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional
transformers for language understanding, in: Proceedings
of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 4171–4186.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Do et al. [2021]</span>
<span class="ltx_bibblock">
Do, T., Nguyen, B.X.,
Tjiputra, E., Tran, M.,
Tran, Q.D., Nguyen, A.,
2021.

</span>
<span class="ltx_bibblock">Multiple meta-model quantifying for medical visual
question answering, in: de Bruijne, M.,
Cattin, P.C., Cotin, S.,
Padoy, N., Speidel, S.,
Zheng, Y., Essert, C. (Eds.),
Medical Image Computing and Computer Assisted
Intervention – MICCAI 2021, Springer International
Publishing, Cham. pp. 64–74.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eslami et al. [2021]</span>
<span class="ltx_bibblock">
Eslami, S., de Melo, G.,
Meinel, C., 2021.

</span>
<span class="ltx_bibblock">TeamS at VQA-Med 2021: BBN-Orchestra for
long-tailed medical visual question answering.

</span>
<span class="ltx_bibblock">Working Notes of CLEF 201.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al. [2016]</span>
<span class="ltx_bibblock">
Fukui, A., Park, D.H.,
Yang, D., Rohrbach, A.,
Darrell, T., Rohrbach, M.,
2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual
question answering and visual grounding, in: Proceedings
of the 2016 Conference on Empirical Methods in Natural Language Processing,
Association for Computational Linguistics,
Austin, Texas. pp. 457–468.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gasmi et al. [2021]</span>
<span class="ltx_bibblock">
Gasmi, K., Ltaifa, I.B.,
Lejeune, G., Alshammari, H.,
Ammar, L.B., Mahmood, M.A.,
2021.

</span>
<span class="ltx_bibblock">Optimal deep neural network-based model for answering
visual medical question.

</span>
<span class="ltx_bibblock">Cybernetics and Systems 0,
1–22.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1080/01969722.2021.2018543" title="" class="ltx_ref">10.1080/01969722.2021.2018543</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2021a]</span>
<span class="ltx_bibblock">
Gong, H., Chen, G., Liu,
S., Yu, Y., Li, G.,
2021a.

</span>
<span class="ltx_bibblock">Cross-modal self-attention with multi-task
pre-training for medical visual question answering, in:
Proceedings of the 2021 International Conference on
Multimedia Retrieval, Association for Computing
Machinery, New York, NY, USA. p.
456–460.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1145/3460426.3463584" title="" class="ltx_ref">10.1145/3460426.3463584</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2021b]</span>
<span class="ltx_bibblock">
Gong, H., Huang, R., Chen,
G., Li, G., 2021b.

</span>
<span class="ltx_bibblock">SYSU-HCP at VQA-Med 2021: A data-centric model with
efficient training methodology for medical visual question answering.

</span>
<span class="ltx_bibblock">Working Notes of CLEF 201.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017]</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T.,
Summers-Stay, D., Batra, D.,
Parikh, D., 2017.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of
image understanding in Visual Question Answering, in:
Conference on Computer Vision and Pattern Recognition
(CVPR).

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. [2021]</span>
<span class="ltx_bibblock">
Gupta, D., Suman, S.,
Ekbal, A., 2021.

</span>
<span class="ltx_bibblock">Hierarchical deep multi-modal network for medical
visual question answering.

</span>
<span class="ltx_bibblock">Expert Systems with Applications
164, 113993.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.eswa.2020.113993" title="" class="ltx_ref">https://doi.org/10.1016/j.eswa.2020.113993</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al. [2018]</span>
<span class="ltx_bibblock">
Hasan, S.A., Ling, Y.,
Farri, O., Liu, J.,
Müller, H., Lungren, M.P.,
2018.

</span>
<span class="ltx_bibblock">Overview of ImageCLEF 2018 medical domain visual
question answering task., in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren,
S., Sun, J., 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition, in:
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), IEEE Computer Society,
Los Alamitos, CA, USA. pp. 770–778.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
He, X., Zhang, Y., Mou,
L., Xing, E., Xie, P.,
2020.

</span>
<span class="ltx_bibblock">PathVQA: 30000+ questions for medical visual
question answering.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2003.10286 .

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hekler et al. [2019]</span>
<span class="ltx_bibblock">
Hekler, A., Utikal, J.S.,
Enk, A.H., Hauschild, A.,
Weichenthal, M., Maron, R.C.,
Berking, C., Haferkamp, S.,
Klode, J., Schadendorf, D., et al.,
2019.

</span>
<span class="ltx_bibblock">Superior skin cancer classification by the
combination of human and artificial intelligence.

</span>
<span class="ltx_bibblock">European Journal of Cancer 120,
114–121.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber [1997]</span>
<span class="ltx_bibblock">
Hochreiter, S., Schmidhuber, J.,
1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock">Neural Computation 9,
1735–1780.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2020]</span>
<span class="ltx_bibblock">
Jiang, M., Chen, S., Yang,
J., Zhao, Q., 2020.

</span>
<span class="ltx_bibblock">Fantastic answers and where to find them: Immersive
question-directed visual attention, in: 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 2977–2986.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. [2019]</span>
<span class="ltx_bibblock">
Johnson, A.E., Pollard, T.J.,
Greenbaum, N.R., Lungren, M.P.,
Deng, C.y., Peng, Y.,
Lu, Z., Mark, R.G.,
Berkowitz, S.J., Horng, S.,
2019.

</span>
<span class="ltx_bibblock">MIMIC-CXR-JPG, a large publicly available database
of labeled chest radiographs.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1901.07042 .

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al. [2020]</span>
<span class="ltx_bibblock">
Jung, B., Gu, L.,
HaradaAl-Sadi, T., 2020.

</span>
<span class="ltx_bibblock">bumjun<math id="bib.bib40.1.m1.1" class="ltx_Math" alttext="\_" display="inline"><semantics id="bib.bib40.1.m1.1a"><mi mathvariant="normal" id="bib.bib40.1.m1.1.1" xref="bib.bib40.1.m1.1.1.cmml">_</mi><annotation-xml encoding="MathML-Content" id="bib.bib40.1.m1.1b"><ci id="bib.bib40.1.m1.1.1.cmml" xref="bib.bib40.1.m1.1.1">_</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib40.1.m1.1c">\_</annotation></semantics></math>jung at VQA-Med 2020: VQA model based
on feature extraction and multi-modal feature fusion, in:
CLEF 2020 Working Notes.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K. Verma and Ramachandran S. [2020]</span>
<span class="ltx_bibblock">
K. Verma, H., Ramachandran S., S.,
2020.

</span>
<span class="ltx_bibblock">HARENDRAKV at VQA-Med 2020: Sequential VQA with
attention for medical visual question answering, in:
CLEF 2020 Working Notes.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavur et al. [2019]</span>
<span class="ltx_bibblock">
Kavur, A.E., Selver, M.A.,
Dicle, O., Barış, M.,
Gezer, N.S., 2019.

</span>
<span class="ltx_bibblock">CHAOS - Combined (CT-MR) Healthy Abdominal Organ
Segmentation Challenge Data.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.5281/zenodo.3362844" title="" class="ltx_ref">10.5281/zenodo.3362844</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khare et al. [2021]</span>
<span class="ltx_bibblock">
Khare, Y., Bagal, V.,
Mathew, M., Devi, A.,
Priyakumar, U.D., Jawahar, C.,
2021.

</span>
<span class="ltx_bibblock">Mmbert: Multimodal bert pretraining for improved
medical vqa, in: 2021 IEEE 18th International Symposium
on Biomedical Imaging (ISBI), pp. 1033–1036.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1109/ISBI48211.2021.9434063" title="" class="ltx_ref">10.1109/ISBI48211.2021.9434063</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2018]</span>
<span class="ltx_bibblock">
Kim, J., Jun, J., Zhang,
B., 2018.

</span>
<span class="ltx_bibblock">Bilinear attention networks, in:
Bengio, S., Wallach, H.M.,
Larochelle, H., Grauman, K.,
Cesa-Bianchi, N., Garnett, R. (Eds.),
Advances in Neural Information Processing Systems,
Montréal, Canada. pp. 1571–1581.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornuta et al. [2019]</span>
<span class="ltx_bibblock">
Kornuta, T., Rajan, D.,
Shivade, C., Asseman, A.,
Ozcan, A.S., 2019.

</span>
<span class="ltx_bibblock">Leveraging medical visual question answering with
supporting facts, in: CLEF (Working Notes).

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kovaleva et al. [2020]</span>
<span class="ltx_bibblock">
Kovaleva, O., Shivade, C.,
Kashyap, S., Kanjaria, K.,
Wu, J., Ballah, D., Coy,
A., Karargyris, A., Guo, Y.,
Beymer, D.B., et al., 2020.

</span>
<span class="ltx_bibblock">Towards visual dialog for radiology, in:
Proceedings of the 19th SIGBioMed Workshop on Biomedical
Language Processing, pp. 60–69.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. [2017]</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y.,
Groth, O., Johnson, J.,
Hata, K., Kravitz, J.,
Chen, S., Kalantidis, Y.,
Li, L.J., Shamma, D.A., et al.,
2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using
crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock">International journal of computer vision
123, 32–73.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau et al. [2018]</span>
<span class="ltx_bibblock">
Lau, J.J., Gayen, S.,
Abacha, A.B., Demner-Fushman, D.,
2018.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions
and answers about radiology images.

</span>
<span class="ltx_bibblock">Scientific Data 5,
1–10.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2020]</span>
<span class="ltx_bibblock">
Lee, J., Yoon, W., Kim,
S., Kim, D., Kim, S.,
So, C.H., Kang, J., 2020.

</span>
<span class="ltx_bibblock">BioBERT: a pre-trained biomedical language
representation model for biomedical text mining.

</span>
<span class="ltx_bibblock">Bioinformatics 36,
1234–1240.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liu [2021]</span>
<span class="ltx_bibblock">
Li, J., Liu, S., 2021.

</span>
<span class="ltx_bibblock">Lijie at ImageCLEFmed VQA-Med 2021: Attention
model based on efficient interaction between multimodality.

</span>
<span class="ltx_bibblock">Working Notes of CLEF 201.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021a]</span>
<span class="ltx_bibblock">
Li, M., Cai, W., Liu, R.,
Weng, Y., Zhao, X.,
Wang, C., Chen, X., Liu,
Z., Pan, C., Li, M., et al.,
2021a.

</span>
<span class="ltx_bibblock">FFA-IR: Towards an explainable and reliable medical
report generation benchmark, in: Thirty-fifth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track (Round
2).

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021b]</span>
<span class="ltx_bibblock">
Li, Y., Yang, Z., Hao,
T., 2021b.

</span>
<span class="ltx_bibblock">TAM at VQA-Med 2021: A hybrid model with feature
extraction and fusion for medical visual question answering.

</span>
<span class="ltx_bibblock">Working Notes of CLEF 201.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. [2020]</span>
<span class="ltx_bibblock">
Liao, Z., Wu, Q., Shen,
C., van den Hengel, A., Verjans, J.,
2020.

</span>
<span class="ltx_bibblock">AIML at VQA-Med 2020: Knowledge inference via a
skeleton-based sentence mapping approach for medical domain visual question
answering, in: CLEF 2020 Working Notes.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2013]</span>
<span class="ltx_bibblock">
Lin, M., Chen, Q., Yan,
S., 2013.

</span>
<span class="ltx_bibblock">Network in network.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1312.4400 .

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M.,
Belongie, S., Hays, J.,
Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.,
2014.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context, in:
European conference on computer vision,
Springer. pp. 740–755.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021a]</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.M., Wu,
X.M., 2021a.

</span>
<span class="ltx_bibblock">Contrastive pre-training and representation
distillation for medical visual question answering based on radiology
images, in: de Bruijne, M., Cattin,
P.C., Cotin, S., Padoy, N.,
Speidel, S., Zheng, Y.,
Essert, C. (Eds.), Medical Image
Computing and Computer Assisted Intervention – MICCAI 2021,
Springer International Publishing,
Cham. pp. 210–220.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021b]</span>
<span class="ltx_bibblock">
Liu, B., Zhan, L.M., Xu,
L., Ma, L., Yang, Y.,
Wu, X.M., 2021b.

</span>
<span class="ltx_bibblock">Slake: A semantically-labeled knowledge-enhanced
dataset for medical visual question answering.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2102.09542" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2102.09542</a><span id="bib.bib57.1.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text ltx_font_typewriter">Liu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text ltx_font_typewriter">
Liu, S., Ding, H., Zhou,
X., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text ltx_font_typewriter">Shengyan at VQA-Med 2020: An encoder-decoder model
for medical domain visual question answering task, in:
CLEF 2020 Working Notes.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text ltx_font_typewriter">Liu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text ltx_font_typewriter">
Liu, S., Ou, X., Che, J.,
Zhou, X., Ding, H., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text ltx_font_typewriter">An Xception-GRU model for visual question answering
in the medical domain., in: CLEF (Working Notes).
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text ltx_font_typewriter">Lu et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text ltx_font_typewriter">
Lu, J., Yang, J., Batra,
D., Parikh, D., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text ltx_font_typewriter">Hierarchical question-image co-attention for visual
question answering, in: Advances in Neural Information
Processing Systems, pp. 289--297.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text ltx_font_typewriter">Marino et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text ltx_font_typewriter">
Marino, K., Rastegari, M.,
Farhadi, A., Mottaghi, R.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text ltx_font_typewriter">OK-VQA: A visual question answering benchmark
requiring external knowledge, in: 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 3190--3199.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text ltx_font_typewriter">McDonald et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text ltx_font_typewriter">
McDonald, R.J., Schwartz, K.M.,
Eckel, L.J., Diehn, F.E.,
Hunt, C.H., Bartholmai, B.J.,
Erickson, B.J., Kallmes, D.F.,
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text ltx_font_typewriter">The effects of changes in utilization and
technological advancements of cross-sectional imaging on radiologist
workload.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.9.1" class="ltx_text ltx_font_typewriter">Academic radiology 22,
1191--1198.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text ltx_font_typewriter">Müller et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text ltx_font_typewriter">
Müller, L., Gangadharaiah, R.,
Klein, S.C., Perry, J.,
Bernstein, G., Nurkse, D.,
Wailes, D., Graham, R.,
El-Kareh, R., Mehta, S., et al.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text ltx_font_typewriter">An open access medical knowledge base for community
driven diagnostic decision support system development.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.9.1" class="ltx_text ltx_font_typewriter">BMC medical informatics and decision making
19, 93.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text ltx_font_typewriter">Nguyen et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text ltx_font_typewriter">
Nguyen, B.D., Do, T.T.,
Nguyen, B.X., Do, T.,
Tjiputra, E., Tran, Q.D.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text ltx_font_typewriter">Overcoming data limitation in medical visual question
answering, in: Shen, D., Liu, T.,
Peters, T.M., Staib, L.H.,
Essert, C., Zhou, S.,
Yap, P.T., Khan, A. (Eds.),
Medical Image Computing and Computer Assisted
Intervention -- MICCAI 2019, Springer International
Publishing, Cham. pp. 522--530.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text ltx_font_typewriter">Nori et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text ltx_font_typewriter">
Nori, H., King, N.,
McKinney, S.M., Carignan, D.,
Horvitz, E., 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text ltx_font_typewriter">Capabilities of gpt-4 on medical challenge problems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:2303.13375 .
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.4.4.1" class="ltx_text ltx_font_typewriter">[66]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.6.1" class="ltx_text ltx_font_typewriter">
Oikarinen, T., Das, S.,
Nguyen, L.M., Weng, T.W., .
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text ltx_font_typewriter">Label-free concept bottleneck models, in:
International Conference on Learning Representations.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text ltx_font_typewriter">Papineni et al. [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text ltx_font_typewriter">
Papineni, K., Roukos, S.,
Ward, T., Zhu, W.J.,
2002.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text ltx_font_typewriter">BLEU: a method for automatic evaluation of machine
translation, in: Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA. pp.
311--318.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text ltx_font_typewriter">Park et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text ltx_font_typewriter">
Park, D., Hendricks, L.,
Akata, Z., Rohrbach, A.,
Schiele, B., Darrell, T.,
Rohrbach, M., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text ltx_font_typewriter">Multimodal explanations: Justifying decisions and
pointing to the evidence, in: 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), IEEE
Computer Society, Los Alamitos, CA, USA. pp.
8779--8788.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text ltx_font_typewriter">Pelka et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text ltx_font_typewriter">
Pelka, O., Koitka, S.,
Rückert, J., Nensa, F.,
Friedrich, C.M., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text ltx_font_typewriter">Radiology objects in COntext (ROCO): a multimodal
image dataset, in: Intravascular Imaging and Computer
Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert
Label Synthesis. Springer, pp.
180--189.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text ltx_font_typewriter">Peng et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text ltx_font_typewriter">
Peng, Y., Liu, F., Rosen,
M.P., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text ltx_font_typewriter">UMass at ImageCLEF medical visual question
answering (Med-VQA) 2018 task, in: CLEF (Working
Notes).
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text ltx_font_typewriter">Radford et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text ltx_font_typewriter">
Radford, A., Narasimhan, K.,
Salimans, T., Sutskever, I., et al.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text ltx_font_typewriter">Improving language understanding by generative
pre-training .
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text ltx_font_typewriter">Ramakrishnan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text ltx_font_typewriter">
Ramakrishnan, S., Agrawal, A.,
Lee, S., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text ltx_font_typewriter">Overcoming language priors in visual question
answering with adversarial regularization, in: Advances
in Neural Information Processing Systems, pp. 1541--1551.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.4.4.1" class="ltx_text ltx_font_typewriter">Ren and Zhou [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.6.1" class="ltx_text ltx_font_typewriter">
Ren, F., Zhou, Y., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text ltx_font_typewriter">CGMVQA: A new classification and generative model
for medical visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text ltx_font_typewriter">IEEE Access 8,
50626--50636.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text ltx_font_typewriter">Russakovsky et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text ltx_font_typewriter">
Russakovsky, O., Deng, J.,
Su, H., Krause, J.,
Satheesh, S., Ma, S.,
Huang, Z., Karpathy, A.,
Khosla, A., Bernstein, M., et al.,
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text ltx_font_typewriter">ImageNet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.9.1" class="ltx_text ltx_font_typewriter">International Journal of Computer Vision
115, 211--252.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.4.4.1" class="ltx_text ltx_font_typewriter">Sarrouti [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.6.1" class="ltx_text ltx_font_typewriter">
Sarrouti, M., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text ltx_font_typewriter">NLM at VQA-Med 2020: Visual question answering
and generation in the medical domain, in: CLEF 2020
Working Notes.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text ltx_font_typewriter">Schilling et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text ltx_font_typewriter">
Schilling, R., Messina, P.,
Parra, D., Lobel, H.,
2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text ltx_font_typewriter">PUC chile team at VQA-Med 2021: approaching vqa
as a classfication task via fine-tuning a pretrained cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text ltx_font_typewriter">Working Notes of CLEF 201.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.4.4.1" class="ltx_text ltx_font_typewriter">Schuster and Paliwal [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.6.1" class="ltx_text ltx_font_typewriter">
Schuster, M., Paliwal, K.K.,
1997.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text ltx_font_typewriter">Bidirectional recurrent neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text ltx_font_typewriter">IEEE transactions on Signal Processing
45, 2673--2681.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text ltx_font_typewriter">Shao et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text ltx_font_typewriter">
Shao, Z., Yu, Z., Wang,
M., Yu, J., 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text ltx_font_typewriter">Prompting large language models with answer
heuristics for knowledge-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:2303.01903 .
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text ltx_font_typewriter">Sharma et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text ltx_font_typewriter">
Sharma, D., Purushotham, S.,
Reddy, C.K., 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text ltx_font_typewriter">Medfusenet: An attention-based multimodal deep
learning model for visual question answering in the medical domain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.9.1" class="ltx_text ltx_font_typewriter">Scientific Reports 11,
1--18.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text ltx_font_typewriter">Shi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text ltx_font_typewriter">
Shi, L., Liu, F., Rosen,
M.P., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text ltx_font_typewriter">Deep multimodal learning for medical visual question
answering., in: CLEF (Working Notes).
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text ltx_font_typewriter">Shickel et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text ltx_font_typewriter">
Shickel, B., Tighe, P.J.,
Bihorac, A., Rashidi, P.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text ltx_font_typewriter">Deep EHR: A survey of recent advances in deep
learning techniques for electronic health record (EHR) analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text ltx_font_typewriter">IEEE Journal of Biomedical and Health Informatics
22, 1589--1604.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.10.1" class="ltx_text ltx_font_typewriter">doi:</span><a target="_blank" href="https:/doi.org/10.1109/JBHI.2017.2767063" title="" class="ltx_ref ltx_font_typewriter">10.1109/JBHI.2017.2767063</a><span id="bib.bib81.11.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.4.4.1" class="ltx_text ltx_font_typewriter">Simonyan and Zisserman [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.6.1" class="ltx_text ltx_font_typewriter">
Simonyan, K., Zisserman, A.,
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text ltx_font_typewriter">Very deep convolutional networks for large-scale
image recognition, in: Proceedings of the 3rd
International Conference on Learning Representations.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text ltx_font_typewriter">Simpson et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text ltx_font_typewriter">
Simpson, A.L., Antonelli, M.,
Bakas, S., Bilello, M.,
Farahani, K., van Ginneken, B.,
Kopp-Schneider, A., Landman, B.A.,
Litjens, G., Menze, B.,
Ronneberger, O., Summers, R.M.,
Bilic, P., Christ, P.F.,
Do, R.K.G., Gollub, M.,
Golia-Pernicka, J., Heckers, S.H.,
Jarnagin, W.R., McHugo, M.K.,
Napel, S., Vorontsov, E.,
Maier-Hein, L., Cardoso, M.J.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text ltx_font_typewriter">A large annotated medical image dataset for the
development and evaluation of segmentation algorithms.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.09063" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1902.09063</a><span id="bib.bib83.9.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.4.4.1" class="ltx_text ltx_font_typewriter">Sitara and Kavitha [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.6.1" class="ltx_text ltx_font_typewriter">
Sitara, N.M.S., Kavitha, S.,
2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text ltx_font_typewriter">SSN MLRG at VQA-Med 2021: An approach for VQA
to solve abnormality related queries using improved datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text ltx_font_typewriter">Working Notes of CLEF 201.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.4.4.1" class="ltx_text ltx_font_typewriter">Talafha and Al-Ayyoub [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.6.1" class="ltx_text ltx_font_typewriter">
Talafha, B., Al-Ayyoub, M.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text ltx_font_typewriter">JUST at VQA-Med: A VGG-Seq2Seq model, in:
CLEF (Working Notes).
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.4.4.1" class="ltx_text ltx_font_typewriter">Thanki and Makkithaya [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.6.1" class="ltx_text ltx_font_typewriter">
Thanki, A., Makkithaya, K.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text ltx_font_typewriter">MIT Manipal at ImageCLEF 2019 visual question
answering in medical domain, in: CLEF (Working Notes).
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text ltx_font_typewriter">Thomee et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text ltx_font_typewriter">
Thomee, B., Shamma, D.A.,
Friedland, G., Elizalde, B.,
Ni, K., Poland, D.,
Borth, D., Li, L.J.,
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text ltx_font_typewriter">YFCC100M: The new data in multimedia research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.9.1" class="ltx_text ltx_font_typewriter">Communications of the ACM 59,
64--73.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text ltx_font_typewriter">Tschandl et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text ltx_font_typewriter">
Tschandl, P., Rinner, C.,
Apalla, Z., Argenziano, G.,
Codella, N., Halpern, A.,
Janda, M., Lallas, A.,
Longo, C., Malvehy, J., et al.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text ltx_font_typewriter">Human-computer collaboration for skin cancer
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.9.1" class="ltx_text ltx_font_typewriter">Nature Medicine 26,
1229--1234.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text ltx_font_typewriter">Tschandl et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text ltx_font_typewriter">
Tschandl, P., Rosendahl, C.,
Kittler, H., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text ltx_font_typewriter">The HAM10000 dataset, a large collection of
multi-source dermatoscopic images of common pigmented skin lesions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.9.1" class="ltx_text ltx_font_typewriter">Scientific Data 5.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.10.1" class="ltx_text ltx_font_typewriter">doi:</span><a target="_blank" href="https:/doi.org/10.1038/sdata.2018.161" title="" class="ltx_ref ltx_font_typewriter">10.1038/sdata.2018.161</a><span id="bib.bib89.11.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.4.4.1" class="ltx_text ltx_font_typewriter">Turner and Spanier [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.6.1" class="ltx_text ltx_font_typewriter">
Turner, A., Spanier, A.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text ltx_font_typewriter">LSTM in VQA-Med, is it really needed? JCE study
on the ImageCLEF 2019 dataset, in: CLEF (Working
Notes).
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.4.4.1" class="ltx_text ltx_font_typewriter">Umada and Aono [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.6.1" class="ltx_text ltx_font_typewriter">
Umada, H., Aono, M., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text ltx_font_typewriter">kdevqa at VQA-Med 2020: focusing on GLU-based
classification, in: CLEF 2020 Working Notes.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text ltx_font_typewriter">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text ltx_font_typewriter">
Vaswani, A., Shazeer, N.,
Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text ltx_font_typewriter">Attention is all you need, in:
Advances in Neural Information Processing Systems, pp.
5998--6008.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.5.5.1" class="ltx_text ltx_font_typewriter">Vu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text ltx_font_typewriter">
Vu, M., Sznitman, R.,
Nyholm, T., Löfstedt, T.,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text ltx_font_typewriter">Ensemble of streamlined bilinear visual question
answering models for the ImageCLEF 2019 challenge in the medical domain,
in: CLEF 2019.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text ltx_font_typewriter">Vu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text ltx_font_typewriter">
Vu, M.H., Löfstedt, T.,
Nyholm, T., Sznitman, R.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text ltx_font_typewriter">A question-centric model for visual question
answering in medical imaging.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.9.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Medical Imaging
39, 2856--2868.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.10.1" class="ltx_text ltx_font_typewriter">doi:</span><a target="_blank" href="https:/doi.org/10.1109/TMI.2020.2978284" title="" class="ltx_ref ltx_font_typewriter">10.1109/TMI.2020.2978284</a><span id="bib.bib94.11.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text ltx_font_typewriter">Wang et al. [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text ltx_font_typewriter">
Wang, P., Wu, Q., Shen,
C., Dick, A., van den Hengel, A.,
2017a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text ltx_font_typewriter">Explicit knowledge-based reasoning for visual
question answering, in: Proceedings of the Twenty-Sixth
International Joint Conference on Artificial Intelligence, IJCAI-17, pp.
1290--1296.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.5.5.1" class="ltx_text ltx_font_typewriter">Wang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text ltx_font_typewriter">
Wang, P., Wu, Q., Shen,
C., Dick, A., van den Hengel, A.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.8.1" class="ltx_text ltx_font_typewriter">FVQA: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.9.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Pattern Analysis &amp; Machine
Intelligence 40, 2413--2427.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text ltx_font_typewriter">Wang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text ltx_font_typewriter">
Wang, S., Zhao, Z.,
Ouyang, X., Wang, Q.,
Shen, D., 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text ltx_font_typewriter">ChatCAD: Interactive computer-aided diagnosis on
medical image using large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:2302.07257 .
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.5.5.1" class="ltx_text ltx_font_typewriter">Wang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text ltx_font_typewriter">
Wang, X., Liu, Y., Shen,
C., Ng, C., Luo, C.,
Jin, L., Chan, C.,
van den Hengel, A., Wang, L.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text ltx_font_typewriter">On the general value of evidence, and bilingual
scene-text visual question answering, in: 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR),
IEEE Computer Society, Los Alamitos,
CA, USA. pp. 10123--10132.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib99.5.5.1" class="ltx_text ltx_font_typewriter">Wang et al. [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib99.7.1" class="ltx_text ltx_font_typewriter">
Wang, X., Peng, Y., Lu,
L., Lu, Z., Bagheri, M.,
Summers, R.M., 2017b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.8.1" class="ltx_text ltx_font_typewriter">ChestX-Ray8: Hospital-scale chest X-Ray database
and benchmarks on weakly-supervised classification and localization of common
thorax diseases, in: 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), IEEE Computer
Society, Los Alamitos, CA, USA. pp.
3462--3471.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.9.1" class="ltx_text ltx_font_typewriter">doi:</span><a target="_blank" href="https:/doi.org/10.1109/CVPR.2017.369" title="" class="ltx_ref ltx_font_typewriter">10.1109/CVPR.2017.369</a><span id="bib.bib99.10.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib100.5.5.1" class="ltx_text ltx_font_typewriter">Wu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib100.7.1" class="ltx_text ltx_font_typewriter">
Wu, Q., Teney, D., Wang,
P., Shen, C., Dick, A.,
van den Hengel, A., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.8.1" class="ltx_text ltx_font_typewriter">Visual question answering: A survey of methods and
datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.9.1" class="ltx_text ltx_font_typewriter">Computer Vision and Image Understanding
163, 21--40.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.10.1" class="ltx_text ltx_font_typewriter">Language in Vision.
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib101.5.5.1" class="ltx_text ltx_font_typewriter">Xiao et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib101.7.1" class="ltx_text ltx_font_typewriter">
Xiao, Q., Zhou, X., Xiao,
Y., Zhao, K., 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.8.1" class="ltx_text ltx_font_typewriter">Yunnan university at VQA-Med 2021: Pretrained
BioBERT for medical domain visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.9.1" class="ltx_text ltx_font_typewriter">Working Notes of CLEF 201.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib102.5.5.1" class="ltx_text ltx_font_typewriter">Yan et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib102.7.1" class="ltx_text ltx_font_typewriter">
Yan, X., Li, L., Xie, C.,
Xiao, J., Gu, L., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib102.8.1" class="ltx_text ltx_font_typewriter">Zhejiang University at ImageCLEF 2019 visual
question answering in the medical domain, in: CLEF
(Working Notes).
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib103.5.5.1" class="ltx_text ltx_font_typewriter">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib103.7.1" class="ltx_text ltx_font_typewriter">
Yang, Y., Panagopoulou, A.,
Zhou, S., Jin, D.,
Callison-Burch, C., Yatskar, M.,
2022.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.8.1" class="ltx_text ltx_font_typewriter">Language in a bottle: Language model guided concept
bottlenecks for interpretable image classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:2211.11158 .
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib104.5.5.1" class="ltx_text ltx_font_typewriter">Yang et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib104.7.1" class="ltx_text ltx_font_typewriter">
Yang, Z., He, X., Gao,
J., Deng, L., Smola, A.,
2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.8.1" class="ltx_text ltx_font_typewriter">Stacked attention networks for image question
answering, in: 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), IEEE Computer Society,
Los Alamitos, CA, USA. pp. 21--29.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib105.5.5.1" class="ltx_text ltx_font_typewriter">Yu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib105.7.1" class="ltx_text ltx_font_typewriter">
Yu, Z., Yu, J., Cui, Y.,
Tao, D., Tian, Q., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib105.8.1" class="ltx_text ltx_font_typewriter">Deep modular co-attention networks for visual
question answering, in: 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), IEEE
Computer Society, Los Alamitos, CA, USA. pp.
6274--6283.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib106.5.5.1" class="ltx_text ltx_font_typewriter">Yu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib106.7.1" class="ltx_text ltx_font_typewriter">
Yu, Z., Yu, J., Fan, J.,
Tao, D., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.8.1" class="ltx_text ltx_font_typewriter">Multi-modal factorized bilinear pooling with
co-attention learning for visual question answering, in:
2017 IEEE International Conference on Computer Vision
(ICCV), IEEE Computer Society, Los
Alamitos, CA, USA. pp. 1839--1848.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib107.5.5.1" class="ltx_text ltx_font_typewriter">Yu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib107.7.1" class="ltx_text ltx_font_typewriter">
Yu, Z., Yu, J., Xiang,
C., Fan, J., Tao, D.,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.8.1" class="ltx_text ltx_font_typewriter">Beyond bilinear: Generalized multimodal factorized
high-order pooling for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.9.1" class="ltx_text ltx_font_typewriter">IEEE Transactions on Neural Networks and Learning
Systems 29, 5947--5959.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.10.1" class="ltx_text ltx_font_typewriter">doi:</span><a target="_blank" href="https:/doi.org/10.1109/TNNLS.2018.2817340" title="" class="ltx_ref ltx_font_typewriter">10.1109/TNNLS.2018.2817340</a><span id="bib.bib107.11.2" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib108.5.5.1" class="ltx_text ltx_font_typewriter">Zhan et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib108.7.1" class="ltx_text ltx_font_typewriter">
Zhan, L.M., Liu, B., Fan,
L., Chen, J., Wu, X.M.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.8.1" class="ltx_text ltx_font_typewriter">Medical visual question answering via conditional
reasoning, in: Proceedings of the 28th ACM International
Conference on Multimedia (MM ’20), ACM.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib109.5.5.1" class="ltx_text ltx_font_typewriter">Zheng et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib109.7.1" class="ltx_text ltx_font_typewriter">
Zheng, W., Yan, L., Wang,
F.Y., Gou, C., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib109.8.1" class="ltx_text ltx_font_typewriter">Learning from the guidance: Knowledge embedded
meta-learning for medical visual question answering, in:
Yang, H., Pasupa, K.,
Leung, A.C.S., Kwok, J.T.,
Chan, J.H., King, I. (Eds.),
Neural Information Processing,
Springer International Publishing,
Cham. pp. 194--202.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib110.5.5.1" class="ltx_text ltx_font_typewriter">Zhou et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib110.7.1" class="ltx_text ltx_font_typewriter">
Zhou, B., Cui, Q., Wei,
X.S., Chen, Z.M., 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.8.1" class="ltx_text ltx_font_typewriter">BBN: Bilateral-branch network with cumulative
learning for long-tailed visual recognition , 1--8.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib111.5.5.1" class="ltx_text ltx_font_typewriter">Zhou et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib111.7.1" class="ltx_text ltx_font_typewriter">
Zhou, Y., Kang, X., Ren,
F., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.8.1" class="ltx_text ltx_font_typewriter">Employing Inception-Resnet-v2 and Bi-LSTM for
medical domain visual question answering, in: CLEF
(Working Notes).
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib112.5.5.1" class="ltx_text ltx_font_typewriter">Zhou et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib112.7.1" class="ltx_text ltx_font_typewriter">
Zhou, Y., Kang, X., Ren,
F., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.8.1" class="ltx_text ltx_font_typewriter">TUA1 at ImageCLEF 2019 vqa-med: a classification
and generation model based on transfer learning, in:
CLEF (Working Notes).
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.10055" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.10056" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.10056">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.10056" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.10057" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 00:23:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
