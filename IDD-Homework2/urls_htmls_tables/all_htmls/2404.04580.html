<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.04580] SDFR: Synthetic Data for Face Recognition Competition</title><meta property="og:description" content="Large-scale face recognition datasets are collected by crawling the Internet and without individuals’ consent, raising legal, ethical, and privacy concerns. With the recent advances in generative models, recently sever…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SDFR: Synthetic Data for Face Recognition Competition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SDFR: Synthetic Data for Face Recognition Competition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.04580">

<!--Generated on Sun May  5 16:13:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">SDFR: Synthetic Data for Face Recognition Competition
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id44.44.44" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:455.2pt;">
<span id="id32.32.32.32" class="ltx_p"><span id="id28.28.28.28.28" class="ltx_text" style="font-size:120%;">Hatef Otroshi Shahreza<sup id="id28.28.28.28.28.1" class="ltx_sup"><span id="id28.28.28.28.28.1.1" class="ltx_text ltx_font_italic">1,2,†</span></sup>,
Christophe Ecabert<sup id="id28.28.28.28.28.2" class="ltx_sup"><span id="id28.28.28.28.28.2.1" class="ltx_text ltx_font_italic">1,†,∗</span></sup>,
Anjith George<sup id="id28.28.28.28.28.3" class="ltx_sup"><span id="id28.28.28.28.28.3.1" class="ltx_text ltx_font_italic">1,†,∗</span></sup>,
<br class="ltx_break">Alexander Unnervik<sup id="id28.28.28.28.28.4" class="ltx_sup"><span id="id28.28.28.28.28.4.1" class="ltx_text ltx_font_italic">1,2,†,∗</span></sup>, Sébastien Marcel<sup id="id28.28.28.28.28.5" class="ltx_sup"><span id="id28.28.28.28.28.5.1" class="ltx_text ltx_font_italic">1,3,†</span></sup>,
Nicolò Di Domenico<sup id="id28.28.28.28.28.6" class="ltx_sup"><span id="id28.28.28.28.28.6.1" class="ltx_text ltx_font_italic">4,‡</span></sup>, 
<br class="ltx_break">Guido Borghi<sup id="id28.28.28.28.28.7" class="ltx_sup"><span id="id28.28.28.28.28.7.1" class="ltx_text ltx_font_italic">4,‡</span></sup>,
Davide Maltoni<sup id="id28.28.28.28.28.8" class="ltx_sup"><span id="id28.28.28.28.28.8.1" class="ltx_text ltx_font_italic">4,‡</span></sup>,
Fadi Boutros<sup id="id28.28.28.28.28.9" class="ltx_sup"><span id="id28.28.28.28.28.9.1" class="ltx_text ltx_font_italic">5,‡</span></sup>,
Julia Vogel<sup id="id28.28.28.28.28.10" class="ltx_sup"><span id="id28.28.28.28.28.10.1" class="ltx_text ltx_font_italic">5,‡</span></sup>, 
<br class="ltx_break">Naser Damer<sup id="id28.28.28.28.28.11" class="ltx_sup"><span id="id28.28.28.28.28.11.1" class="ltx_text ltx_font_italic">5,6,‡</span></sup>,
Ángela Sánchez-Pérez<sup id="id28.28.28.28.28.12" class="ltx_sup"><span id="id28.28.28.28.28.12.1" class="ltx_text ltx_font_italic">7,8,‡</span></sup>,
Enrique Mas-Candela<sup id="id28.28.28.28.28.13" class="ltx_sup"><span id="id28.28.28.28.28.13.1" class="ltx_text ltx_font_italic">7,8,‡</span></sup>, 
<br class="ltx_break">Jorge Calvo-Zaragoza<sup id="id28.28.28.28.28.14" class="ltx_sup"><span id="id28.28.28.28.28.14.1" class="ltx_text ltx_font_italic">7,‡</span></sup>,
Bernardo Biesseck<sup id="id28.28.28.28.28.15" class="ltx_sup"><span id="id28.28.28.28.28.15.1" class="ltx_text ltx_font_italic">9,10,‡</span></sup>,
Pedro Vidal<sup id="id28.28.28.28.28.16" class="ltx_sup"><span id="id28.28.28.28.28.16.1" class="ltx_text ltx_font_italic">9,‡</span></sup>,
Roger Granada<sup id="id28.28.28.28.28.17" class="ltx_sup"><span id="id28.28.28.28.28.17.1" class="ltx_text ltx_font_italic">11,‡</span></sup>, 
<br class="ltx_break">David Menotti<sup id="id28.28.28.28.28.18" class="ltx_sup"><span id="id28.28.28.28.28.18.1" class="ltx_text ltx_font_italic">9,‡</span></sup>,
Ivan DeAndres-Tame<sup id="id28.28.28.28.28.19" class="ltx_sup"><span id="id28.28.28.28.28.19.1" class="ltx_text ltx_font_italic">12,‡</span></sup>,
Simone Maurizio La Cava<sup id="id28.28.28.28.28.20" class="ltx_sup"><span id="id28.28.28.28.28.20.1" class="ltx_text ltx_font_italic">13,‡</span></sup>,
Sara Concas<sup id="id28.28.28.28.28.21" class="ltx_sup"><span id="id28.28.28.28.28.21.1" class="ltx_text ltx_font_italic">13,‡</span></sup>,
Pietro Melzi<sup id="id28.28.28.28.28.22" class="ltx_sup"><span id="id28.28.28.28.28.22.1" class="ltx_text ltx_font_italic">12,‡</span></sup>,
Ruben Tolosana<sup id="id28.28.28.28.28.23" class="ltx_sup"><span id="id28.28.28.28.28.23.1" class="ltx_text ltx_font_italic">12,‡</span></sup>,
Ruben Vera-Rodriguez<sup id="id28.28.28.28.28.24" class="ltx_sup"><span id="id28.28.28.28.28.24.1" class="ltx_text ltx_font_italic">12,‡</span></sup>,
Gianpaolo Perelli<sup id="id28.28.28.28.28.25" class="ltx_sup"><span id="id28.28.28.28.28.25.1" class="ltx_text ltx_font_italic">13,‡</span></sup>, 
<br class="ltx_break">Giulia Orrù<sup id="id28.28.28.28.28.26" class="ltx_sup"><span id="id28.28.28.28.28.26.1" class="ltx_text ltx_font_italic">13,‡</span></sup>,
Gian Luca Marcialis<sup id="id28.28.28.28.28.27" class="ltx_sup"><span id="id28.28.28.28.28.27.1" class="ltx_text ltx_font_italic">13,‡</span></sup>,
Julian Fierrez<sup id="id28.28.28.28.28.28" class="ltx_sup"><span id="id28.28.28.28.28.28.1" class="ltx_text ltx_font_italic">12,‡</span></sup>

<br class="ltx_break"></span>
<sup id="id32.32.32.32.29" class="ltx_sup"><span id="id32.32.32.32.29.1" class="ltx_text ltx_font_italic">1</span></sup>Idiap Research Institute, Switzerland
<sup id="id32.32.32.32.30" class="ltx_sup"><span id="id32.32.32.32.30.1" class="ltx_text ltx_font_italic">2</span></sup>École Polytechnique Fédérale de Lausanne (EPFL), Switzerland
<sup id="id32.32.32.32.31" class="ltx_sup"><span id="id32.32.32.32.31.1" class="ltx_text ltx_font_italic">3</span></sup>Université de Lausanne (UNIL), Switzerland
<sup id="id32.32.32.32.32" class="ltx_sup"><span id="id32.32.32.32.32.1" class="ltx_text ltx_font_italic">4</span></sup>University of Bologna, Italy</span>
<span id="id34.34.34.34" class="ltx_p ltx_align_center"><sup id="id34.34.34.34.1" class="ltx_sup"><span id="id34.34.34.34.1.1" class="ltx_text ltx_font_italic">5</span></sup>Fraunhofer Institute for Computer Graphics Research (IGD), Germany
<sup id="id34.34.34.34.2" class="ltx_sup"><span id="id34.34.34.34.2.1" class="ltx_text ltx_font_italic">6</span></sup>TU Darmstadt, Germany</span>
<span id="id36.36.36.36" class="ltx_p ltx_align_center"><sup id="id36.36.36.36.1" class="ltx_sup"><span id="id36.36.36.36.1.1" class="ltx_text ltx_font_italic">7</span></sup>Universidad de Alicante, Spain,
<sup id="id36.36.36.36.2" class="ltx_sup"><span id="id36.36.36.36.2.1" class="ltx_text ltx_font_italic">8</span></sup>Facephi Biometria SA, R&amp;D Centre, Spain,</span>
<span id="id37.37.37.37" class="ltx_p ltx_align_center"><sup id="id37.37.37.37.1" class="ltx_sup"><span id="id37.37.37.37.1.1" class="ltx_text ltx_font_italic">9</span></sup>Federal University of Paraná (UFPR), Curitiba, PR, Brazil</span>
<span id="id39.39.39.39" class="ltx_p ltx_align_center"><sup id="id39.39.39.39.1" class="ltx_sup"><span id="id39.39.39.39.1.1" class="ltx_text ltx_font_italic">10</span></sup>Federal Institute of Mato Grosso (IFMT), Pontes e Lacerda, Brazil
<sup id="id39.39.39.39.2" class="ltx_sup"><span id="id39.39.39.39.2.1" class="ltx_text ltx_font_italic">11</span></sup>unico - idTech, Brazil</span>
<span id="id41.41.41.41" class="ltx_p ltx_align_center"><sup id="id41.41.41.41.1" class="ltx_sup"><span id="id41.41.41.41.1.1" class="ltx_text ltx_font_italic">12</span></sup>Universidad Autonoma de Madrid (UAM), Spain
<sup id="id41.41.41.41.2" class="ltx_sup"><span id="id41.41.41.41.2.1" class="ltx_text ltx_font_italic">13</span></sup>Università degli Studi di Cagliari (UNICA), Italy</span>
<span id="id43.43.43.43" class="ltx_p ltx_align_center"><sup id="id43.43.43.43.1" class="ltx_sup"><span id="id43.43.43.43.1.1" class="ltx_text ltx_font_italic">†</span></sup>Competition organizer  <sup id="id43.43.43.43.2" class="ltx_sup"><span id="id43.43.43.43.2.1" class="ltx_text ltx_font_italic">‡</span></sup>Competition participant</span>
<span id="id44.44.44.44" class="ltx_p ltx_align_center"><sup id="id44.44.44.44.1" class="ltx_sup"><span id="id44.44.44.44.1.1" class="ltx_text ltx_font_italic">∗</span></sup>Equal Contribution</span>
</span>
</span><span class="ltx_author_notes"> Competition Website: <a target="_blank" href="http://www.idiap.ch/challenge/sdfr/" title="" class="ltx_ref ltx_href">www.idiap.ch/challenge/sdfr</a></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id45.id1" class="ltx_p">Large-scale face recognition datasets are collected by crawling the Internet and without individuals’ consent, raising legal, ethical, and privacy concerns. With the recent advances in generative models, recently several works proposed generating synthetic face recognition datasets to mitigate concerns in web-crawled face recognition datasets. This paper presents the summary of the Synthetic Data for Face Recognition (SDFR) Competition held in conjunction with the 18th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2024) and established to investigate the use of synthetic data for training face recognition models. The SDFR competition was split into two tasks, allowing participants to train face recognition systems using new synthetic datasets and/or existing ones. In the first task, the face recognition backbone was fixed and the dataset size was limited, while the second task provided almost complete freedom on the model backbone, the dataset, and the training pipeline. The submitted models were trained on existing and also new synthetic datasets and used clever methods to improve training with synthetic data. The submissions were evaluated and ranked on a diverse set of seven benchmarking datasets. The paper gives an overview of the submitted face recognition models and reports achieved performance compared to baseline models trained on real and synthetic datasets. Furthermore, the evaluation of submissions is extended to bias assessment across different demography groups. Lastly, an outlook on the current state of the research in training face recognition models using synthetic data is presented, and existing problems as well as potential future directions are also discussed.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in state-of-the-art face recognition models are driven in part by the availability of large-scale datasets and deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Meanwhile, large-scale face recognition datasets, such as VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, WebFace260M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, etc., were collected by crawling images from the Internet, raising legal, ethical, and privacy concerns. In light of these concerns and the fact that these datasets are collected without the individuals’ consent,
some dataset owners decided to take their datasets down; for example, Microsoft retracted MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
To address such concerns, recently, several studies have proposed generating synthetic face recognition datasets and using synthetic face images for training face recognition models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The generated synthetic face recognition datasets should be composed of different synthetic subjects with several samples per subject.
As a face recognition dataset, each generated synthetic face dataset should include sufficient <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">intra-class</span> variations to model challenging scenarios in the task of face recognition including variations caused by pose, aging, expressions, occlusions, environmental conditions, etc. Moreover, the generated dataset needs to have adequate <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">inter-class</span> variation so that the trained face recognition models can have generalization for unseen subjects when used in a face recognition system.
Given these challenges, generating synthetic face datasets with sufficient inter-class and intra-class variations is an active area of research.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As the main motivation for generating synthetic face recognition datasets, the generation of synthetic face recognition datasets should not rely on large-scale web-crawled datasets which have privacy concerns. In addition, since the synthetic face recognition datasets should eventually replace large-scale real face recognition datasets, face recognition models trained using synthetic datasets should lead to a high recognition accuracy.
However, there is still a gap in the recognition accuracy between training using large-scale real face recognition datasets and existing synthetic face recognition datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The Synthetic Data for Face Recognition (SDFR) competition held in conjunction with the 18th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2024) was organized to accelerate research in synthetic data generation for privacy-friendly face recognition models and to bridge the gap between real and synthetic face datasets.
Teams were invited to propose clever ways to use synthetic face recognition datasets (either existing or new synthetic face datasets) to train face recognition models.
The competition was split into two tasks, where the first task involved a predefined face recognition backbone and a limit on the dataset size to focus on the quality of synthesized face datasets, while the second task provided almost complete freedom on the model backbone, the dataset, and the training pipeline.
The submitted trained face recognition models using synthetic data were evaluated based on a diverse set of seven benchmarking datasets.
The SDFR competition is the first competition which is specifically focused on generating synthetic face recognition datasets and allows training face recognition models using new methods or existing datasets from the literature. The competition rules were also designed to allow exploring ideas for generating privacy-friendly datasets, while preventing the application of large-scale web-crawled datasets.
This paper presents a summary of the SDFR competition and discusses submissions and findings in this competition.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The remainder of the paper is organized as follows. Section <a href="#S2" title="II SDFR Competition ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides details about the SDFR competition, including the definition of tasks, and evaluation metrics used in the competition.
In Section <a href="#S3" title="III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we provide a description of the final submitted face recognition systems proposed in the SDFR competition for each task. Section <a href="#S4" title="IV Leaderboards ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the results achieved in the different tasks and compare them with several baselines. Section <a href="#S5" title="V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> extends the analyses for final submission and evaluates their bias on different demography groups. Section <a href="#S5" title="V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> also discusses submissions with not qualified datasets, which are excluded from the leaderboard, and discusses their limitations and findings.
Furthermore, Section <a href="#S5" title="V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents a further discussion on training face recognition using synthetic data and highlights potential future research directions in the field.
Finally, in Section <a href="#S6" title="VI Conclusion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we draw the conclusions from the SDFR competition.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">SDFR Competition</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the SDFR competition, participants were expected to train face recognition models using synthetic data and submit their trained models (in ONNX format) through a submission platform. The organizers regularly evaluated submitted models on benchmarking datasets, and the results appeared in the leaderboard on the competition website throughout the competition.
In the following, a brief overview of the definition of competition tasks, rules, and evaluation is presented. Finally, the novelties of the SDFR competition compared to previous competitions on face recognition are discussed.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Tasks</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The SDFR competition was divided into two tasks, and each team could use existing synthetic datasets or a newly generated dataset to participate in either or both tasks:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Task 1 [Constrained]:</span> In this task, the generated synthetic dataset could have up to one million synthesized images (for example, 10,000 identities and 100 images per identity). The backbone is also fixed to iResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Task 2 [Unconstrained]:</span> In this task, participants could use synthetic data with no limit on the number of synthesized images. Participants were also allowed to use any network architecture and train their best model with state-of-the-art techniques, but only using synthetic data.</p>
</div>
</li>
</ul>
<p id="S2.SS1.p1.2" class="ltx_p">Submitted models in each task were evaluated and ranked in the leaderboard of the corresponding task.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Rules</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The competition was organized with several rules which allowed participants to generate new synthetic face recognition datasets and/or use (a subset, the complete set, or an extension of) existing ones to train face recognition models.
Participants were not allowed to use a real dataset with identity labels for training the face generator model and generate synthetic images. As a result, some of the existing synthetic datasets for the literature, such as DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, were not qualified for the competition. Further details about such synthetic datasets are discussed in Section <a href="#S5" title="V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
However, the rules allowed to use real datasets without identity labels (such as FFHQ<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The FFHQ dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> contains 70,000 images without identity labels with variation in terms of age, ethnicity, etc. Especially, each individual image in this dataset was published on Flickr by their respective authors under licenses that allow free use, redistribution, and adaptation for non-commercial purposes.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>) to train the face generator model. This could be extended to real datasets with identity labels if the identity labels were not used in training the face generator model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Given the limitations in available datasets without identity labels, this rule relaxed the restriction in available datasets and allowed participants to use more available datasets. Indeed, this relaxation in the rules was considered to provide the opportunity for participants to investigate limitations in existing datasets without identity labels (such as FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>). However, outside of the scope of the competition, for a responsible and privacy-friendly synthetic face recognition dataset, large-scale web-crawled datasets (even without labels) should not be used to train generative models.</span></span></span>.
Participants were also allowed to use a pre-trained face recognition model for controlling and generating synthetic datasets. However, they were not allowed to directly learn embeddings of a pre-trained face recognition model.
The complete definition of the rules is available on the competition website<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://www.idiap.ch/challenge/sdfr/" title="" class="ltx_ref ltx_href">www.idiap.ch/challenge/sdfr</a></span></span></span>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Evaluation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The submitted models were evaluated on a diverse set of seven benchmarking face recognition datasets.
The benchmarking datasets include:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">one high-quality unconstrained dataset:</span> Labeled Faces in the Wild (LFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">two cross-pose datasets:</span> Celebrities in Frontal-Profile in the Wild (CFP-FP) and Cross-Pose LFW (CPLFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>,</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">two cross-age datasets</span>: AgeDB-30 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> (30 years age gap) and Cross-age LFW (CALFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>,</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p"><span id="S2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">two challenging mixed-quality datasets:</span> IARPA Janus Benchmark–B (IJB-B) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and IARPA Janus Benchmark–C (IJB-C) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</li>
</ul>
<p id="S2.SS3.p1.2" class="ltx_p">The submitted models were ranked on each dataset separately, and the final ranking was based on the Borda count.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Novelties</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Considering the wide application of face recognition systems and the increasing demand for high-performing face recognition models, several competitions have been organized in conjunction with different conferences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
However, most of these competitions were based on large-scale web-crawled face recognition datasets, which raise privacy concerns.
The FRCSyn challenge held in WACVW 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> was the first competition on training face recognition models based on synthetic data; however, participants were limited to using two existing synthetic datasets from the literature (i.e., DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>).
The SDFR competition is the first competition focused on generating synthetic face recognition datasets, and allows training face recognition models using new methods or existing datasets from the literature (as long as they follow competition rules). The rules of the SDFR competition were also designed to allow exploring ideas for generating privacy-friendly datasets, while preventing the application of large-scale web-crawled datasets.
Furthermore, the SDFR competition is based on a thorough evaluation using seven benchmarking datasets.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Description of Submissions</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The SDFR competition received significant interest, with 12 international teams registered.
In total, we received 17 submissions from 5 teams.
Table <a href="#S3.T1" title="TABLE I ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> provides a summary overview of the participating teams, including the team name, their affiliation, and the tasks they participated. In the following section, we briefly describe the proposed method used by each team.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">BioLab (Task 1 and Task 2):</span>
The BioLab team used iResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for task 1 and an iResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for task 2. Both models were trained using the margin-based AdaFace loss<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, chosen for its state-of-the-art results and robustness during training, especially when employing low-quality images.
For task 1, they trained their model on the IDiff-Face (Uniform) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset, which contains about 500K images across 10K identities; for task 2, they extended their training dataset in task 1 with the DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset, bringing the total up to 1.7M images spanning 120k classes.
To improve the performance of the models, they applied data augmentation techniques to the images during training. In particular, they sequentially applied random horizontal flips, color jittering, random crops by blacking out the rest of the image, random JPEG compression, random grayscale, and random down-and-upsampling with different interpolation algorithms (i.e. nearest neighbor, bilinear, bicubic, area, Lanczos).
When employing images from DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, they applied a more aggressive data augmentation pipeline to bridge the quality gap between synthetic and real images; as suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, they extended the above-mentioned pipeline by adding more photometric augmentation steps i.e., Gaussian blur (20%), random noise (20%), and motion blur (10%).
They built the validation set by generating random matching and non-matching pairs by selecting images from the first classes of IDiff-Face and DigiFace-1M, generating 12K and 84K pairs, respectively, for task 1 and task 2.
The classes used to generate such pairs are excluded from training. With this validation set, they monitored the verification accuracy during training and applied an early stopping policy with patience of 10 epochs to prevent overfitting. Both models are optimized with SGD with a batch size of 512, and the initial learning rate of 0.1 divided by a factor of 10 at prefixed epochs to ensure better training stability.
Code is available: <a target="_blank" href="https://github.com/ndido98/sdfr" title="" class="ltx_ref ltx_href">https://github.com/ndido98/sdfr</a></p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.4.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.5.2" class="ltx_text" style="font-size:90%;">A summary of the participant teams, their affiliations, and their participating tasks.</span></figcaption>
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">Team Name</span></th>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Affiliations<sup id="S3.T1.1.1.1.1.1" class="ltx_sup"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">Country</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_bold">Tasks</span></td>
</tr>
<tr id="S3.T1.2.3.1" class="ltx_tr">
<th id="S3.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BioLab</th>
<td id="S3.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">4</td>
<td id="S3.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">Italy</td>
<td id="S3.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
</tr>
<tr id="S3.T1.2.4.2" class="ltx_tr">
<th id="S3.T1.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">IGD-IDiff-Face</th>
<td id="S3.T1.2.4.2.2" class="ltx_td ltx_align_center">5,6</td>
<td id="S3.T1.2.4.2.3" class="ltx_td ltx_align_center">Germany</td>
<td id="S3.T1.2.4.2.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S3.T1.2.5.3" class="ltx_tr">
<th id="S3.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">APhi</th>
<td id="S3.T1.2.5.3.2" class="ltx_td ltx_align_center">7,8</td>
<td id="S3.T1.2.5.3.3" class="ltx_td ltx_align_center">Spain</td>
<td id="S3.T1.2.5.3.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S3.T1.2.6.4" class="ltx_tr">
<th id="S3.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BOVIFOCR-UFPR</th>
<td id="S3.T1.2.6.4.2" class="ltx_td ltx_align_center">9-11</td>
<td id="S3.T1.2.6.4.3" class="ltx_td ltx_align_center">Brazil</td>
<td id="S3.T1.2.6.4.4" class="ltx_td ltx_align_center">1,2</td>
</tr>
<tr id="S3.T1.2.7.5" class="ltx_tr">
<th id="S3.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BiDA-PRA</th>
<td id="S3.T1.2.7.5.2" class="ltx_td ltx_align_center">12,13</td>
<td id="S3.T1.2.7.5.3" class="ltx_td ltx_align_center">Spain, Italy</td>
<td id="S3.T1.2.7.5.4" class="ltx_td ltx_align_center">1,2</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4">
<sup id="S3.T1.2.2.1.1" class="ltx_sup"><span id="S3.T1.2.2.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>The numbers reported in the “Affiliations” column are provided</th>
</tr>
<tr id="S3.T1.2.8.6" class="ltx_tr">
<th id="S3.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">in the title page.</th>
<td id="S3.T1.2.8.6.2" class="ltx_td"></td>
<td id="S3.T1.2.8.6.3" class="ltx_td"></td>
<td id="S3.T1.2.8.6.4" class="ltx_td"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">IGD-IDiff-Face (Task 1):</span>
The submitted solution of the IGD-IDiff-Face team was based on IDiff-Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The training dataset was obtained by combining two subsets, IDiff-Face CPD25 (Uniform) and IDiff-Face CPD50 (Two-Stage), of the IDiff-Face dataset.
These subsets contain 10,049 and 10,050 identities, respectively, and 50 images per identity. They selected the first exact 10k from each subset. Therefore, the total number of identities is 20K, each contains 50 images.
They trained the face recognition model using CosFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> with a margin of 0.35 and a scale factor of 64.
During the training, they augmented the training data using RandAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> with parameters (operation is 16 and magnitude is 4) described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
The model is trained using the SGD optimizer with a batch size of 512. The model is trained using a plateau-based learning scheduler. The initial learning rate is set to 0.1 and divided by 10 when the average validation accuracy does not improve for 10 consecutive epochs where the patience parameter is set to 10 and the factor to 0.4. The total number of epochs is set to 200 with an early stopping parameter of 10 epochs, i.e., the training will stop if the average validation accuracy does not improve for 10 epochs. Code is available: <a target="_blank" href="https://github.com/fdbtrs/IDiff-Face" title="" class="ltx_ref ltx_href">https://github.com/fdbtrs/IDiff-Face</a></p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">APhi (Task 1):</span>
The APhi team used iResNet-50 as the backbone of their model and trained it with MagFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> loss function.
As their training dataset, they used the IDiff-Face (Uniform) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which consists of about 550K images and 10K identities.
They used data augmentation techniques, including random scale and then resize, Gaussian blur, Hue saturation, and horizontal flip.
They trained their model for 25 epochs using the SGD optimizer with a batch size of 512 and a learning rate initialized from 0.1 and divided by 10 at epochs 10, 18, and 22. The weight decay was set to <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mn id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p4.1.m1.1.1.1" xref="S3.p4.1.m1.1.1.1.cmml">×</mo><msup id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml"><mn id="S3.p4.1.m1.1.1.3.2" xref="S3.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.p4.1.m1.1.1.3.3" xref="S3.p4.1.m1.1.1.3.3.cmml"><mo id="S3.p4.1.m1.1.1.3.3a" xref="S3.p4.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.p4.1.m1.1.1.3.3.2" xref="S3.p4.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><times id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">5</cn><apply id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.3.1.cmml" xref="S3.p4.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.p4.1.m1.1.1.3.2.cmml" xref="S3.p4.1.m1.1.1.3.2">10</cn><apply id="S3.p4.1.m1.1.1.3.3.cmml" xref="S3.p4.1.m1.1.1.3.3"><minus id="S3.p4.1.m1.1.1.3.3.1.cmml" xref="S3.p4.1.m1.1.1.3.3"></minus><cn type="integer" id="S3.p4.1.m1.1.1.3.3.2.cmml" xref="S3.p4.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">5\times 10^{-4}</annotation></semantics></math> and the momentum is 0.9. In addition, 200 users have been taken from the training set and used for validation (a total of 10K images). The validation was performed by evaluating the equal error rate (EER) over all the possible pairs in the validation set. They validated the trained model every 1,000 iterations, and stored the model with the lowest EER.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.7" class="ltx_p"><span id="S3.p5.7.1" class="ltx_text ltx_font_bold">BOVIFOCR-UFPR (Task 1 and Task 2):</span>
The BOVIFOCR-UFPR team used iResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for both task 1 and task 2 and trained their models using ArcFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, chosen for its performance for deep face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
For task 1, they used the uniform version of the IDiff-Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> as the training dataset containing <math id="S3.p5.1.m1.2" class="ltx_Math" alttext="10,049" display="inline"><semantics id="S3.p5.1.m1.2a"><mrow id="S3.p5.1.m1.2.3.2" xref="S3.p5.1.m1.2.3.1.cmml"><mn id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">10</mn><mo id="S3.p5.1.m1.2.3.2.1" xref="S3.p5.1.m1.2.3.1.cmml">,</mo><mn id="S3.p5.1.m1.2.2" xref="S3.p5.1.m1.2.2.cmml">049</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.2b"><list id="S3.p5.1.m1.2.3.1.cmml" xref="S3.p5.1.m1.2.3.2"><cn type="integer" id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">10</cn><cn type="integer" id="S3.p5.1.m1.2.2.cmml" xref="S3.p5.1.m1.2.2">049</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.2c">10,049</annotation></semantics></math> identities, with <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="55" display="inline"><semantics id="S3.p5.2.m2.1a"><mn id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><cn type="integer" id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">55</annotation></semantics></math> images per identity (<math id="S3.p5.3.m3.2" class="ltx_Math" alttext="502,450" display="inline"><semantics id="S3.p5.3.m3.2a"><mrow id="S3.p5.3.m3.2.3.2" xref="S3.p5.3.m3.2.3.1.cmml"><mn id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">502</mn><mo id="S3.p5.3.m3.2.3.2.1" xref="S3.p5.3.m3.2.3.1.cmml">,</mo><mn id="S3.p5.3.m3.2.2" xref="S3.p5.3.m3.2.2.cmml">450</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.2b"><list id="S3.p5.3.m3.2.3.1.cmml" xref="S3.p5.3.m3.2.3.2"><cn type="integer" id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">502</cn><cn type="integer" id="S3.p5.3.m3.2.2.cmml" xref="S3.p5.3.m3.2.2">450</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.2c">502,450</annotation></semantics></math> images in total).
The training images were augmented using Random Flip with a probability of
<math id="S3.p5.4.m4.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S3.p5.4.m4.1a"><mrow id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml"><mn id="S3.p5.4.m4.1.1.2" xref="S3.p5.4.m4.1.1.2.cmml">50</mn><mo id="S3.p5.4.m4.1.1.1" xref="S3.p5.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><apply id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1"><csymbol cd="latexml" id="S3.p5.4.m4.1.1.1.cmml" xref="S3.p5.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.p5.4.m4.1.1.2.cmml" xref="S3.p5.4.m4.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">50\%</annotation></semantics></math>.
They also applied random erasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> and randaugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> as additional augmentation.
They used the SGD optimizer and set the momentum to <math id="S3.p5.5.m5.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S3.p5.5.m5.1a"><mn id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.1b"><cn type="float" id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.1c">0.9</annotation></semantics></math> and weight decay to <math id="S3.p5.6.m6.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S3.p5.6.m6.1a"><mrow id="S3.p5.6.m6.1.1" xref="S3.p5.6.m6.1.1.cmml"><mn id="S3.p5.6.m6.1.1.2" xref="S3.p5.6.m6.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p5.6.m6.1.1.1" xref="S3.p5.6.m6.1.1.1.cmml">×</mo><msup id="S3.p5.6.m6.1.1.3" xref="S3.p5.6.m6.1.1.3.cmml"><mn id="S3.p5.6.m6.1.1.3.2" xref="S3.p5.6.m6.1.1.3.2.cmml">10</mn><mrow id="S3.p5.6.m6.1.1.3.3" xref="S3.p5.6.m6.1.1.3.3.cmml"><mo id="S3.p5.6.m6.1.1.3.3a" xref="S3.p5.6.m6.1.1.3.3.cmml">−</mo><mn id="S3.p5.6.m6.1.1.3.3.2" xref="S3.p5.6.m6.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.6.m6.1b"><apply id="S3.p5.6.m6.1.1.cmml" xref="S3.p5.6.m6.1.1"><times id="S3.p5.6.m6.1.1.1.cmml" xref="S3.p5.6.m6.1.1.1"></times><cn type="integer" id="S3.p5.6.m6.1.1.2.cmml" xref="S3.p5.6.m6.1.1.2">5</cn><apply id="S3.p5.6.m6.1.1.3.cmml" xref="S3.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.p5.6.m6.1.1.3.1.cmml" xref="S3.p5.6.m6.1.1.3">superscript</csymbol><cn type="integer" id="S3.p5.6.m6.1.1.3.2.cmml" xref="S3.p5.6.m6.1.1.3.2">10</cn><apply id="S3.p5.6.m6.1.1.3.3.cmml" xref="S3.p5.6.m6.1.1.3.3"><minus id="S3.p5.6.m6.1.1.3.3.1.cmml" xref="S3.p5.6.m6.1.1.3.3"></minus><cn type="integer" id="S3.p5.6.m6.1.1.3.3.2.cmml" xref="S3.p5.6.m6.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.6.m6.1c">5\times 10^{-4}</annotation></semantics></math>. The learning rate was set to <math id="S3.p5.7.m7.1" class="ltx_Math" alttext="0.02" display="inline"><semantics id="S3.p5.7.m7.1a"><mn id="S3.p5.7.m7.1.1" xref="S3.p5.7.m7.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S3.p5.7.m7.1b"><cn type="float" id="S3.p5.7.m7.1.1.cmml" xref="S3.p5.7.m7.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.7.m7.1c">0.02</annotation></semantics></math> and reduced at each iteration following the equation:</p>
</div>
<div id="S3.p6" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\left(\frac{1.0-\frac{l}{t}}{1.0-\frac{l-1}{t}}\right)," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1"><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mn id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">1.0</mn><mo id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">−</mo><mfrac id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">l</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">t</mi></mfrac></mrow><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">1.0</mn><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">−</mo><mfrac id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mrow id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">l</mi><mo id="S3.E1.m1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.3.3.2.1.cmml">−</mo><mn id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">t</mi></mfrac></mrow></mfrac><mo id="S3.E1.m1.2.2.1.1.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><minus id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></minus><cn type="float" id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">1.0</cn><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><divide id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3"></divide><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝑙</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><cn type="float" id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">1.0</cn><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><divide id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3"></divide><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><minus id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.1"></minus><ci id="S3.E1.m1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.3.3.2.2">𝑙</ci><cn type="integer" id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3">1</cn></apply><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\left(\frac{1.0-\frac{l}{t}}{1.0-\frac{l-1}{t}}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.2" class="ltx_p">where <math id="S3.p7.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.p7.1.m1.1a"><mi id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><ci id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">l</annotation></semantics></math> and <math id="S3.p7.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p7.2.m2.1a"><mi id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><ci id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">t</annotation></semantics></math> represent the current iteration and the total number of iterations, respectively.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.3" class="ltx_p">The model was trained for <math id="S3.p8.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.p8.1.m1.1a"><mn id="S3.p8.1.m1.1.1" xref="S3.p8.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.1b"><cn type="integer" id="S3.p8.1.m1.1.1.cmml" xref="S3.p8.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.1c">20</annotation></semantics></math> epochs within a batch size of <math id="S3.p8.2.m2.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.p8.2.m2.1a"><mn id="S3.p8.2.m2.1.1" xref="S3.p8.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.p8.2.m2.1b"><cn type="integer" id="S3.p8.2.m2.1.1.cmml" xref="S3.p8.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.2.m2.1c">128</annotation></semantics></math>, running for approximately <math id="S3.p8.3.m3.1" class="ltx_Math" alttext="78" display="inline"><semantics id="S3.p8.3.m3.1a"><mn id="S3.p8.3.m3.1.1" xref="S3.p8.3.m3.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S3.p8.3.m3.1b"><cn type="integer" id="S3.p8.3.m3.1.1.cmml" xref="S3.p8.3.m3.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.3.m3.1c">78</annotation></semantics></math>K iterations using the Insightface library<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/deepinsight/insightface" title="" class="ltx_ref ltx_href">https://github.com/deepinsight/insightface</a></span></span></span>. Code for task 1 is available: <a target="_blank" href="https://github.com/PedroBVidal/insightface" title="" class="ltx_ref ltx_href">https://github.com/PedroBVidal/insightface</a></p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.10" class="ltx_p">For task 2, they also used IDiff-Face (Uniform) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and applied random flip and also random face pose augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> based on 3D face reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to manipulate face pose in yaw axis. About <math id="S3.p9.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.p9.1.m1.1a"><mn id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><cn type="integer" id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">40</annotation></semantics></math>K faces were randomly profiled in left (<math id="S3.p9.2.m2.1" class="ltx_Math" alttext="-60^{\circ}" display="inline"><semantics id="S3.p9.2.m2.1a"><mrow id="S3.p9.2.m2.1.1" xref="S3.p9.2.m2.1.1.cmml"><mo id="S3.p9.2.m2.1.1a" xref="S3.p9.2.m2.1.1.cmml">−</mo><msup id="S3.p9.2.m2.1.1.2" xref="S3.p9.2.m2.1.1.2.cmml"><mn id="S3.p9.2.m2.1.1.2.2" xref="S3.p9.2.m2.1.1.2.2.cmml">60</mn><mo id="S3.p9.2.m2.1.1.2.3" xref="S3.p9.2.m2.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.2.m2.1b"><apply id="S3.p9.2.m2.1.1.cmml" xref="S3.p9.2.m2.1.1"><minus id="S3.p9.2.m2.1.1.1.cmml" xref="S3.p9.2.m2.1.1"></minus><apply id="S3.p9.2.m2.1.1.2.cmml" xref="S3.p9.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p9.2.m2.1.1.2.1.cmml" xref="S3.p9.2.m2.1.1.2">superscript</csymbol><cn type="integer" id="S3.p9.2.m2.1.1.2.2.cmml" xref="S3.p9.2.m2.1.1.2.2">60</cn><compose id="S3.p9.2.m2.1.1.2.3.cmml" xref="S3.p9.2.m2.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.2.m2.1c">-60^{\circ}</annotation></semantics></math>) or right (<math id="S3.p9.3.m3.1" class="ltx_Math" alttext="+60^{\circ}" display="inline"><semantics id="S3.p9.3.m3.1a"><mrow id="S3.p9.3.m3.1.1" xref="S3.p9.3.m3.1.1.cmml"><mo id="S3.p9.3.m3.1.1a" xref="S3.p9.3.m3.1.1.cmml">+</mo><msup id="S3.p9.3.m3.1.1.2" xref="S3.p9.3.m3.1.1.2.cmml"><mn id="S3.p9.3.m3.1.1.2.2" xref="S3.p9.3.m3.1.1.2.2.cmml">60</mn><mo id="S3.p9.3.m3.1.1.2.3" xref="S3.p9.3.m3.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.3.m3.1b"><apply id="S3.p9.3.m3.1.1.cmml" xref="S3.p9.3.m3.1.1"><plus id="S3.p9.3.m3.1.1.1.cmml" xref="S3.p9.3.m3.1.1"></plus><apply id="S3.p9.3.m3.1.1.2.cmml" xref="S3.p9.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.p9.3.m3.1.1.2.1.cmml" xref="S3.p9.3.m3.1.1.2">superscript</csymbol><cn type="integer" id="S3.p9.3.m3.1.1.2.2.cmml" xref="S3.p9.3.m3.1.1.2.2">60</cn><compose id="S3.p9.3.m3.1.1.2.3.cmml" xref="S3.p9.3.m3.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.3.m3.1c">+60^{\circ}</annotation></semantics></math>) direction, corresponding to <math id="S3.p9.4.m4.1" class="ltx_Math" alttext="\mathtt{\sim}8\%" display="inline"><semantics id="S3.p9.4.m4.1a"><mrow id="S3.p9.4.m4.1.1" xref="S3.p9.4.m4.1.1.cmml"><mi id="S3.p9.4.m4.1.1.2" xref="S3.p9.4.m4.1.1.2.cmml"></mi><mo id="S3.p9.4.m4.1.1.1" xref="S3.p9.4.m4.1.1.1.cmml">∼</mo><mrow id="S3.p9.4.m4.1.1.3" xref="S3.p9.4.m4.1.1.3.cmml"><mn id="S3.p9.4.m4.1.1.3.2" xref="S3.p9.4.m4.1.1.3.2.cmml">8</mn><mo id="S3.p9.4.m4.1.1.3.1" xref="S3.p9.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.4.m4.1b"><apply id="S3.p9.4.m4.1.1.cmml" xref="S3.p9.4.m4.1.1"><csymbol cd="latexml" id="S3.p9.4.m4.1.1.1.cmml" xref="S3.p9.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.p9.4.m4.1.1.2.cmml" xref="S3.p9.4.m4.1.1.2">absent</csymbol><apply id="S3.p9.4.m4.1.1.3.cmml" xref="S3.p9.4.m4.1.1.3"><csymbol cd="latexml" id="S3.p9.4.m4.1.1.3.1.cmml" xref="S3.p9.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S3.p9.4.m4.1.1.3.2.cmml" xref="S3.p9.4.m4.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.4.m4.1c">\mathtt{\sim}8\%</annotation></semantics></math> from the whole dataset. Some sample augmented face images are shown in Fig. <a href="#S3.F1" title="Figure 1 ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The used SGD optimizer with learning rate <math id="S3.p9.5.m5.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.p9.5.m5.1a"><mn id="S3.p9.5.m5.1.1" xref="S3.p9.5.m5.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.p9.5.m5.1b"><cn type="float" id="S3.p9.5.m5.1.1.cmml" xref="S3.p9.5.m5.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.5.m5.1c">0.1</annotation></semantics></math>, momentum <math id="S3.p9.6.m6.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S3.p9.6.m6.1a"><mn id="S3.p9.6.m6.1.1" xref="S3.p9.6.m6.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S3.p9.6.m6.1b"><cn type="float" id="S3.p9.6.m6.1.1.cmml" xref="S3.p9.6.m6.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.6.m6.1c">0.9</annotation></semantics></math>, batch size <math id="S3.p9.7.m7.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S3.p9.7.m7.1a"><mn id="S3.p9.7.m7.1.1" xref="S3.p9.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.p9.7.m7.1b"><cn type="integer" id="S3.p9.7.m7.1.1.cmml" xref="S3.p9.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.7.m7.1c">256</annotation></semantics></math>, and weight decay of <math id="S3.p9.8.m8.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S3.p9.8.m8.1a"><mrow id="S3.p9.8.m8.1.1" xref="S3.p9.8.m8.1.1.cmml"><mn id="S3.p9.8.m8.1.1.2" xref="S3.p9.8.m8.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p9.8.m8.1.1.1" xref="S3.p9.8.m8.1.1.1.cmml">×</mo><msup id="S3.p9.8.m8.1.1.3" xref="S3.p9.8.m8.1.1.3.cmml"><mn id="S3.p9.8.m8.1.1.3.2" xref="S3.p9.8.m8.1.1.3.2.cmml">10</mn><mrow id="S3.p9.8.m8.1.1.3.3" xref="S3.p9.8.m8.1.1.3.3.cmml"><mo id="S3.p9.8.m8.1.1.3.3a" xref="S3.p9.8.m8.1.1.3.3.cmml">−</mo><mn id="S3.p9.8.m8.1.1.3.3.2" xref="S3.p9.8.m8.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.8.m8.1b"><apply id="S3.p9.8.m8.1.1.cmml" xref="S3.p9.8.m8.1.1"><times id="S3.p9.8.m8.1.1.1.cmml" xref="S3.p9.8.m8.1.1.1"></times><cn type="integer" id="S3.p9.8.m8.1.1.2.cmml" xref="S3.p9.8.m8.1.1.2">5</cn><apply id="S3.p9.8.m8.1.1.3.cmml" xref="S3.p9.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.p9.8.m8.1.1.3.1.cmml" xref="S3.p9.8.m8.1.1.3">superscript</csymbol><cn type="integer" id="S3.p9.8.m8.1.1.3.2.cmml" xref="S3.p9.8.m8.1.1.3.2">10</cn><apply id="S3.p9.8.m8.1.1.3.3.cmml" xref="S3.p9.8.m8.1.1.3.3"><minus id="S3.p9.8.m8.1.1.3.3.1.cmml" xref="S3.p9.8.m8.1.1.3.3"></minus><cn type="integer" id="S3.p9.8.m8.1.1.3.3.2.cmml" xref="S3.p9.8.m8.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.8.m8.1c">5\times 10^{-4}</annotation></semantics></math>.
The model was trained during <math id="S3.p9.9.m9.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.p9.9.m9.1a"><mn id="S3.p9.9.m9.1.1" xref="S3.p9.9.m9.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.p9.9.m9.1b"><cn type="integer" id="S3.p9.9.m9.1.1.cmml" xref="S3.p9.9.m9.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.9.m9.1c">20</annotation></semantics></math> epochs, corresponding to <math id="S3.p9.10.m10.1" class="ltx_Math" alttext="84" display="inline"><semantics id="S3.p9.10.m10.1a"><mn id="S3.p9.10.m10.1.1" xref="S3.p9.10.m10.1.1.cmml">84</mn><annotation-xml encoding="MathML-Content" id="S3.p9.10.m10.1b"><cn type="integer" id="S3.p9.10.m10.1.1.cmml" xref="S3.p9.10.m10.1.1">84</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.10.m10.1c">84</annotation></semantics></math>k iterations.
Code for task 2 is also available: <a target="_blank" href="https://github.com/BOVIFOCR/insightface_SDFR_at_FG2024" title="" class="ltx_ref ltx_href">https://github.com/BOVIFOCR/insightface_SDFR_at_FG2024</a></p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.04580/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="90" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Sample original and augmented face images from the IDiff-Face (Uniform) dataset obtained by synthetically rotating face pose in the yaw axis used by the BOVIFOCR-UFPR team in task 2.</span></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S3.T2.6.2" class="ltx_text" style="font-size:90%;">Leaderboard of Task 1 and Task 2 compared to several baselines.</span></figcaption>
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:240.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-140.9pt,77.9pt) scale(0.606145007533588,0.606145007533588) ;">
<table id="S3.T2.3.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.3.3.4.1" class="ltx_tr">
<td id="S3.T2.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Task / Baseline</span></td>
<td id="S3.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.2.1" class="ltx_text ltx_font_bold">Team / Method</span></td>
<td id="S3.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.3.1" class="ltx_text ltx_font_bold">No. Images</span></td>
<td id="S3.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.4.1" class="ltx_text ltx_font_bold">LFW</span></td>
<td id="S3.T2.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.5.1" class="ltx_text ltx_font_bold">CALFW</span></td>
<td id="S3.T2.3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.6.1" class="ltx_text ltx_font_bold">CPLFW</span></td>
<td id="S3.T2.3.3.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.7.1" class="ltx_text ltx_font_bold">AgeDB30</span></td>
<td id="S3.T2.3.3.4.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.8.1" class="ltx_text ltx_font_bold">CFP-FP</span></td>
<td id="S3.T2.3.3.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.4.1.9.1" class="ltx_text ltx_font_bold">IJB-B</span></td>
<td id="S3.T2.3.3.4.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.4.1.10.1" class="ltx_text ltx_font_bold">IJB-C</span></td>
<td id="S3.T2.3.3.4.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.4.1.11.1" class="ltx_text ltx_font_bold">Rank</span></td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<div id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:46.3pt;height:9.699999999999999pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.6pt,0.4pt) scale(0.9,0.9) ;">
<p id="S3.T2.1.1.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">(TAR@<math id="S3.T2.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{10^{-4}}" display="inline"><semantics id="S3.T2.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T2.1.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T2.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.2.cmml">𝟏𝟎</mn><mrow id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mo id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3a" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.cmml">−</mo><mn id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">𝟒</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T2.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.2">10</cn><apply id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3"><minus id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3"></minus><cn type="integer" id="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.1.m1.1c">\mathbf{10^{-4}}</annotation></semantics></math>)</span></p>
</span></div>
</td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;">
<div id="S3.T2.2.2.2.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:46.3pt;height:9.699999999999999pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.6pt,0.4pt) scale(0.9,0.9) ;">
<p id="S3.T2.2.2.2.2.1.1" class="ltx_p"><span id="S3.T2.2.2.2.2.1.1.1" class="ltx_text ltx_font_bold">(TAR@<math id="S3.T2.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{10^{-4}}" display="inline"><semantics id="S3.T2.2.2.2.2.1.1.1.m1.1a"><msup id="S3.T2.2.2.2.2.1.1.1.m1.1.1" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.cmml"><mn id="S3.T2.2.2.2.2.1.1.1.m1.1.1.2" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.2.cmml">𝟏𝟎</mn><mrow id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.cmml"><mo id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3a" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.cmml">−</mo><mn id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.2" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.2.cmml">𝟒</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.1.1.1.m1.1b"><apply id="S3.T2.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.2.2.2.2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T2.2.2.2.2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.2">10</cn><apply id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3"><minus id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3"></minus><cn type="integer" id="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.1.1.1.m1.1c">\mathbf{10^{-4}}</annotation></semantics></math>)</span></p>
</span></div>
</td>
</tr>
<tr id="S3.T2.3.3.5.2" class="ltx_tr">
<td id="S3.T2.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.5.2.1.1" class="ltx_text">Baselines</span></td>
<td id="S3.T2.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">MS-Celeb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S3.T2.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">5.8M</td>
<td id="S3.T2.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">99.82</td>
<td id="S3.T2.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">95.92</td>
<td id="S3.T2.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">92.52</td>
<td id="S3.T2.3.3.5.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.62</td>
<td id="S3.T2.3.3.5.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.01</td>
<td id="S3.T2.3.3.5.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">94.88</td>
<td id="S3.T2.3.3.5.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.23</td>
<td id="S3.T2.3.3.5.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.6.3" class="ltx_tr">
<td id="S3.T2.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.6.3.1.1" class="ltx_text">(real)</span></td>
<td id="S3.T2.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">WebFace-4M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T2.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">4M</td>
<td id="S3.T2.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">99.78</td>
<td id="S3.T2.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.02</td>
<td id="S3.T2.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">93.90</td>
<td id="S3.T2.3.3.6.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.52</td>
<td id="S3.T2.3.3.6.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.19</td>
<td id="S3.T2.3.3.6.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">95.52</td>
<td id="S3.T2.3.3.6.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.02</td>
<td id="S3.T2.3.3.6.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.7.4" class="ltx_tr">
<td id="S3.T2.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">Casia-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S3.T2.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">490K</td>
<td id="S3.T2.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">99.08</td>
<td id="S3.T2.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">92.88</td>
<td id="S3.T2.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">89.23</td>
<td id="S3.T2.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">93.72</td>
<td id="S3.T2.3.3.7.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">94.67</td>
<td id="S3.T2.3.3.7.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">45.91</td>
<td id="S3.T2.3.3.7.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">52.43</td>
<td id="S3.T2.3.3.7.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.8.5" class="ltx_tr">
<td id="S3.T2.3.3.8.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.8.5.1.1" class="ltx_text">Baselines</span></td>
<td id="S3.T2.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T2.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">1M</td>
<td id="S3.T2.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">85.55</td>
<td id="S3.T2.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">69.85</td>
<td id="S3.T2.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">59.42</td>
<td id="S3.T2.3.3.8.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.38</td>
<td id="S3.T2.3.3.8.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">62.11</td>
<td id="S3.T2.3.3.8.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">14.62</td>
<td id="S3.T2.3.3.8.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">12.85</td>
<td id="S3.T2.3.3.8.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.9.6" class="ltx_tr">
<td id="S3.T2.3.3.9.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.9.6.1.1" class="ltx_text">(synthetic)</span></td>
<td id="S3.T2.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T2.3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">1.2M</td>
<td id="S3.T2.3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">90.63</td>
<td id="S3.T2.3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">74.02</td>
<td id="S3.T2.3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">71.38</td>
<td id="S3.T2.3.3.9.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">65.03</td>
<td id="S3.T2.3.3.9.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.11</td>
<td id="S3.T2.3.3.9.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">38.89</td>
<td id="S3.T2.3.3.9.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">45.09</td>
<td id="S3.T2.3.3.9.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.10.7" class="ltx_tr">
<td id="S3.T2.3.3.10.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">IDNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S3.T2.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">1.05M</td>
<td id="S3.T2.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">84.90</td>
<td id="S3.T2.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">70.28</td>
<td id="S3.T2.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">68.32</td>
<td id="S3.T2.3.3.10.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">63.88</td>
<td id="S3.T2.3.3.10.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">70.34</td>
<td id="S3.T2.3.3.10.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">27.57</td>
<td id="S3.T2.3.3.10.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">32.28</td>
<td id="S3.T2.3.3.10.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">-</td>
</tr>
<tr id="S3.T2.3.3.11.8" class="ltx_tr">
<td id="S3.T2.3.3.11.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.11.8.1.1" class="ltx_text">Task 1</span></td>
<td id="S3.T2.3.3.11.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">IGD-IDiff-Face</td>
<td id="S3.T2.3.3.11.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">1M</td>
<td id="S3.T2.3.3.11.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">98.07</td>
<td id="S3.T2.3.3.11.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">90.60</td>
<td id="S3.T2.3.3.11.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">81.28</td>
<td id="S3.T2.3.3.11.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">87.60</td>
<td id="S3.T2.3.3.11.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">84.76</td>
<td id="S3.T2.3.3.11.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">64.36</td>
<td id="S3.T2.3.3.11.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">68.04</td>
<td id="S3.T2.3.3.11.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">1</td>
</tr>
<tr id="S3.T2.3.3.12.9" class="ltx_tr">
<td id="S3.T2.3.3.12.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="4"><span id="S3.T2.3.3.12.9.1.1" class="ltx_text">(leaderboard)</span></td>
<td id="S3.T2.3.3.12.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">APhi</td>
<td id="S3.T2.3.3.12.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">500K</td>
<td id="S3.T2.3.3.12.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.45</td>
<td id="S3.T2.3.3.12.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">89.95</td>
<td id="S3.T2.3.3.12.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.03</td>
<td id="S3.T2.3.3.12.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">84.75</td>
<td id="S3.T2.3.3.12.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">80.04</td>
<td id="S3.T2.3.3.12.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.18</td>
<td id="S3.T2.3.3.12.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">60.85</td>
<td id="S3.T2.3.3.12.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">2</td>
</tr>
<tr id="S3.T2.3.3.13.10" class="ltx_tr">
<td id="S3.T2.3.3.13.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">BOVIFOCR-UFPR</td>
<td id="S3.T2.3.3.13.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">500K</td>
<td id="S3.T2.3.3.13.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">97.53</td>
<td id="S3.T2.3.3.13.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">89.38</td>
<td id="S3.T2.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">80.07</td>
<td id="S3.T2.3.3.13.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">83.90</td>
<td id="S3.T2.3.3.13.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">84.37</td>
<td id="S3.T2.3.3.13.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">12.70</td>
<td id="S3.T2.3.3.13.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">13.71</td>
<td id="S3.T2.3.3.13.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">3</td>
</tr>
<tr id="S3.T2.3.3.14.11" class="ltx_tr">
<td id="S3.T2.3.3.14.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">BioLab</td>
<td id="S3.T2.3.3.14.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">500K</td>
<td id="S3.T2.3.3.14.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.97</td>
<td id="S3.T2.3.3.14.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">89.12</td>
<td id="S3.T2.3.3.14.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">76.80</td>
<td id="S3.T2.3.3.14.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">83.77</td>
<td id="S3.T2.3.3.14.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">77.34</td>
<td id="S3.T2.3.3.14.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">60.21</td>
<td id="S3.T2.3.3.14.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">63.56</td>
<td id="S3.T2.3.3.14.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">4</td>
</tr>
<tr id="S3.T2.3.3.15.12" class="ltx_tr">
<td id="S3.T2.3.3.15.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">BiDA-PRA</td>
<td id="S3.T2.3.3.15.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">1M</td>
<td id="S3.T2.3.3.15.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.88</td>
<td id="S3.T2.3.3.15.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">87.95</td>
<td id="S3.T2.3.3.15.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.13</td>
<td id="S3.T2.3.3.15.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">83.85</td>
<td id="S3.T2.3.3.15.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.90</td>
<td id="S3.T2.3.3.15.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.08</td>
<td id="S3.T2.3.3.15.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.33</td>
<td id="S3.T2.3.3.15.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">5</td>
</tr>
<tr id="S3.T2.3.3.16.13" class="ltx_tr">
<td id="S3.T2.3.3.16.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;"><span id="S3.T2.3.3.16.13.1.1" class="ltx_text">Task 2</span></td>
<td id="S3.T2.3.3.16.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">BioLab</td>
<td id="S3.T2.3.3.16.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">1.7M</td>
<td id="S3.T2.3.3.16.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">98.33</td>
<td id="S3.T2.3.3.16.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">90.87</td>
<td id="S3.T2.3.3.16.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">84.45</td>
<td id="S3.T2.3.3.16.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">87.85</td>
<td id="S3.T2.3.3.16.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">88.11</td>
<td id="S3.T2.3.3.16.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">76.94</td>
<td id="S3.T2.3.3.16.13.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">81.25</td>
<td id="S3.T2.3.3.16.13.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.35pt;padding-bottom:1.35pt;">1</td>
</tr>
<tr id="S3.T2.3.3.17.14" class="ltx_tr">
<td id="S3.T2.3.3.17.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1.35pt;padding-bottom:1.35pt;" rowspan="2"><span id="S3.T2.3.3.17.14.1.1" class="ltx_text">(leaderboard)</span></td>
<td id="S3.T2.3.3.17.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">BiDA-PRA</td>
<td id="S3.T2.3.3.17.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">1M</td>
<td id="S3.T2.3.3.17.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.88</td>
<td id="S3.T2.3.3.17.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">87.95</td>
<td id="S3.T2.3.3.17.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.13</td>
<td id="S3.T2.3.3.17.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">83.85</td>
<td id="S3.T2.3.3.17.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">78.90</td>
<td id="S3.T2.3.3.17.14.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.08</td>
<td id="S3.T2.3.3.17.14.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">58.33</td>
<td id="S3.T2.3.3.17.14.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">2</td>
</tr>
<tr id="S3.T2.3.3.18.15" class="ltx_tr">
<td id="S3.T2.3.3.18.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">BOVIFOCR-UFPR</td>
<td id="S3.T2.3.3.18.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">500K</td>
<td id="S3.T2.3.3.18.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">96.38</td>
<td id="S3.T2.3.3.18.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">88.38</td>
<td id="S3.T2.3.3.18.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">75.98</td>
<td id="S3.T2.3.3.18.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">81.55</td>
<td id="S3.T2.3.3.18.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">76.97</td>
<td id="S3.T2.3.3.18.15.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">40.97</td>
<td id="S3.T2.3.3.18.15.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">45.93</td>
<td id="S3.T2.3.3.18.15.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;">3</td>
</tr>
<tr id="S3.T2.3.3.19.16" class="ltx_tr">
<td id="S3.T2.3.3.19.16.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1.35pt;padding-bottom:1.35pt;" colspan="11">The values reported in the table for each benchmarking dataset are in percentage. The values reported on the LFW, CALFW, CPLFW, AgeDB30, and CFP-FP</td>
</tr>
<tr id="S3.T2.3.3.3" class="ltx_tr">
<td id="S3.T2.3.3.3.1" class="ltx_td ltx_align_left" style="padding-top:1.35pt;padding-bottom:1.35pt;" colspan="11">datasets are accuracy. The values reported on the IJB-B and IJB-C datasets are True Accept Rate (TAR) at the False Accept Rate (FAR) of <math id="S3.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.T2.3.3.3.1.m1.1a"><msup id="S3.T2.3.3.3.1.m1.1.1" xref="S3.T2.3.3.3.1.m1.1.1.cmml"><mn id="S3.T2.3.3.3.1.m1.1.1.2" xref="S3.T2.3.3.3.1.m1.1.1.2.cmml">10</mn><mrow id="S3.T2.3.3.3.1.m1.1.1.3" xref="S3.T2.3.3.3.1.m1.1.1.3.cmml"><mo id="S3.T2.3.3.3.1.m1.1.1.3a" xref="S3.T2.3.3.3.1.m1.1.1.3.cmml">−</mo><mn id="S3.T2.3.3.3.1.m1.1.1.3.2" xref="S3.T2.3.3.3.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.m1.1b"><apply id="S3.T2.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.3.3.3.1.m1.1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T2.3.3.3.1.m1.1.1.2.cmml" xref="S3.T2.3.3.3.1.m1.1.1.2">10</cn><apply id="S3.T2.3.3.3.1.m1.1.1.3.cmml" xref="S3.T2.3.3.3.1.m1.1.1.3"><minus id="S3.T2.3.3.3.1.m1.1.1.3.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1.3"></minus><cn type="integer" id="S3.T2.3.3.3.1.m1.1.1.3.2.cmml" xref="S3.T2.3.3.3.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.m1.1c">10^{-4}</annotation></semantics></math>.</td>
</tr>
<tr id="S3.T2.3.3.20.17" class="ltx_tr">
<td id="S3.T2.3.3.20.17.1" class="ltx_td ltx_align_left" style="padding-top:1.35pt;padding-bottom:1.35pt;" colspan="11">All baseline models are trained with iResNet-50 and AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> loss function.
Baselines with real data are pretrained models available on the AdaFace</td>
</tr>
<tr id="S3.T2.3.3.21.18" class="ltx_tr">
<td id="S3.T2.3.3.21.18.1" class="ltx_td ltx_align_left" style="padding-top:1.35pt;padding-bottom:1.35pt;" colspan="11">GitHub repository, and baseline models with synthetic data are trained by organizers using the same hyperparamters.</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p"><span id="S3.p10.1.1" class="ltx_text ltx_font_bold">BiDA-PRA (Task 1 and Task 2):</span>
The BiDA-PRA team submitted the same model for both tasks. They used iResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and trained using the AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> loss function.
For preparing their training dataset, they used the information about demographic distribution across the competition evaluation datasets, focusing on ethnicity, gender, and age, available from the datasets meta-data or from the analyses reported in previous studies, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
For the training dataset, they used pre-generated IDiff-Face synthetic dataset (both Uniform and Two-stage subsets, containing about 1M images in total), and created a “cleaned” version of the dataset, discarding each identity image whose labels were different from the average ethnicity and gender predictions.
These attributes were automatically extracted for each face image in the IDiff-Face dataset using the FairFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> model.
Finally, after the cleaning step, they considered 600K total face images of the IDiff-Face dataset. In addition, to generate more training samples, they trained<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The BiDA-PRA team considered that pre-trained StyleGAN3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> with the FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> dataset exhibit superior quality compared to the images in the evaluation datasets. Therefore, they trained StyleGAN3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> on the CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> dataset, which consists of images with more similar quality compared to evaluation datasets.</span></span></span> StyleGAN3 on CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> (without using its identity labels) and used it to generate 20K images of unique identities. They applied the method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to generate pose variations in the latent space of those images, resulting in 20 images per identity, and in total 400K face images generated by StyleGAN3. However, they did not perform an analysis of the demographic distribution for the images generated by StyleGAN3.
During training, they augmented data using random histogram equalization (10%), random horizontal flip (50%), random grayscale conversion (20%), and Gaussian blur (80% with kernel 1, and 20% with kernel 5).
In addition, they applied a custom augmentation technique involved cutting off half of the face, either the left or the right side, from some of the frontal or near-frontal images<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>This custom augmentation technique was intended to train the network to extract useful information from half of the face, as in the case of profile images.</span></span></span>.
In particular, for each selected image, they checked if the face could be considered frontal or not through a landmark-based estimation. If the image was considered frontal, they randomly selected the left or the right side to be removed. They detected the landmarks of the nose and the eyes using MTCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, and then they computed the difference between the angles on the eyes from the triangle composed of these three landmarks. Specifically, they selected a maximum angle difference of 15 degrees to consider the face as frontal or near-frontal, while setting the image selection probability to 5%. Code is available: <a target="_blank" href="https://github.com/BiDAlab/SDFR-FRModels" title="" class="ltx_ref ltx_href">https://github.com/BiDAlab/SDFR-FRModels</a></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Leaderboards</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As mentioned in Section <a href="#S2" title="II SDFR Competition ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, submitted models are evaluated on a diverse set of benchmarking datasets. Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reports the leaderboard of each task and also compares the results of submissions with some baselines from the literature. The values in the table for each benchmarking dataset are in percentage. The values reported on the LFW, CALFW, CPLFW, AgeDB30, and CFP-FP datasets are accuracy and the values reported on the IJB-B and IJB-C datasets are True Accept Rate (TAR) at the False Accept Rate (FAR) of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.p1.1.m1.1a"><msup id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mn id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mo id="S4.p1.1.m1.1.1.3a" xref="S4.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">10</cn><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><minus id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">10^{-4}</annotation></semantics></math>.
The baselines were trained by the corresponding dataset mentioned in the table using iResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> network with AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> loss function.
As can be seen in Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, there is a gap in the recognition performance of baselines with real datasets and baselines with synthetic datasets. This difference is more significant for challenging datasets such as IJB-B, and IJB-C. However, the submitted models could improve the performance compared to baselines with synthetic datasets. In particular, the submitted model by the BioLab team for task 2 achieved considerable performance on all datasets, even on challenging datasets such as IJB-B and IJB-C. This submission used two different datasets, DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (a synthetic dataset based on computer graphic pipeline) and IDiff-Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (a synthetic dataset based on diffusion models), showing potentials in improving performance using synthetic datasets with different types.
While task 1 received significant interest from the participating teams, task 2 received fewer submissions, indicating the difficulty of task 2 which allowed unconstrained training with synthetic data.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In task 1, the IGD-IDiff-Face team used two subsets of IDiff-Face (Uniform and Two-stage), resulting in larger training data, compared to the APhi, BOVIFOCR-UFPR, and BioLab teams, who used only the Uniform subset of IDiff-Face with about 500K images. Meanwhile, the submission of BiDA-PRA also benefits from both subsets of IDiff-Face and in addition used StyleGAN generated images, using the complete limit of 1M images in task 1. While using 1M training samples helped the performance of their submission by the IGD-IDiff-Face team, it could not help the performance of the BiDA-PRA team. This might be caused by not enough variation in their StyleGAN-generated images, which only had pose variation.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">All the teams also used different data augmentation techniques, to further improve their trained models. The data augmentation transformations used by participants contain various techniques and include traditional methods (such as random flip, Gaussian blur, etc), state-of-the-art techniques (such as random erasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, randaugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and pose augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>), as well as custom methods (such as cutting off half of the face). This highlights the importance of data augmentation in training with synthetic data, which lack intra-class variations.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Last but not least, all final submissions have used the IDiff-Face dataset as part of their training, with different sampling strategies and training approaches. In fact, the IDiff-Face dataset is the best-performing synthetic dataset in the literature which is aligned with the competition rules. We should note that participating teams also submitted other models, which had competitive performance with the final leaderboard but had a conflict with the competition rules. Such submissions with not qualified datasets are discussed in detail in Section <a href="#S5" title="V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
To facilitate reproducibility of results in the SDFR competition, all final submission which appeared on the final leaderboard are publicly available on the competition website: <a target="_blank" href="http://www.idiap.ch/challenge/sdfr/" title="" class="ltx_ref ltx_href">www.idiap.ch/challenge/sdfr</a></p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In addition to our evaluation of submitted models in Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we evaluated each of the models in the leaderboard on the Racial Faces in-the-Wild (RFW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> dataset, to investigate their performance across different demography groups. The RFW dataset contains four testing subsets corresponding to Caucasian, Asian, Indian, and African groups, and each is composed of about 3,000 subjects with 6,000 image pairs for the face verification task.
Table <a href="#S5.T3" title="TABLE III ‣ V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> reports the performance of the final submissions in the leaderboard as well as baseline models trained on real and synthetic datasets on the different subsets of the RFW dataset. As the results in this table show, all final submissions from the leaderboard achieved their best performance in the Caucasian group and worst performance in the African group. In each task, the best submission in the leaderboard in Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> also achieves the highest average and lowest standard deviation across different demography groups on the RFW dataset. The results in Table <a href="#S5.T3" title="TABLE III ‣ V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> also show that there is similarly bias in the baselines trained on real and synthetic datasets.
Hence, the results in this table motivates future research on generating responsible synthetic face recognition datasets to alleviate bias in recognition accuracy across different demography groups.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Evaluation results of the final submissions on the RFW dataset.</span></figcaption>
<div id="S5.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:287.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.3pt,0.9pt) scale(0.993855716244508,0.993855716244508) ;">
<table id="S5.T3.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.4.1.1.1" class="ltx_tr">
<td id="S5.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:60.0pt;height:9.1pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.3pt,0.4pt) scale(0.9,0.9) ;">
<p id="S5.T3.4.1.1.1.1.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task
<span id="S5.T3.4.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.8pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.3pt,0.0pt) scale(0.25,0.25) ;">
</span></span>/
<span id="S5.T3.4.1.1.1.1.1.1.1.2" class="ltx_inline-block ltx_transformed_outer" style="width:0.8pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.3pt,0.0pt) scale(0.25,0.25) ;">
</span></span>Baseline</span></p>
</span></div>
</td>
<td id="S5.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:64.7pt;height:9.699999999999999pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.8pt,0.1pt) scale(0.975,0.975) ;">
<p id="S5.T3.4.1.1.1.2.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Team
<span id="S5.T3.4.1.1.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:1.7pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.8pt,0.0pt) scale(0.5,0.5) ;">
</span></span>/
<span id="S5.T3.4.1.1.1.2.1.1.1.2" class="ltx_inline-block ltx_transformed_outer" style="width:1.7pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.8pt,0.0pt) scale(0.5,0.5) ;">
</span></span>Method</span></p>
</span></div>
</td>
<td id="S5.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:40.1pt;height:6.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.2pt,0.3pt) scale(0.9,0.9) ;">
<p id="S5.T3.4.1.1.1.3.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Caucasian</span></p>
</span></div>
</td>
<td id="S5.T3.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;"><span id="S5.T3.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Asian</span></td>
<td id="S5.T3.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:27.4pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.4pt,0.1pt) scale(0.975,0.975) ;">
<p id="S5.T3.4.1.1.1.5.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Indian</span></p>
</span></div>
</td>
<td id="S5.T3.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:30.6pt;height:6.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.8pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T3.4.1.1.1.6.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">African</span></p>
</span></div>
</td>
<td id="S5.T3.4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;"><span id="S5.T3.4.1.1.1.7.1" class="ltx_text ltx_font_bold">Avg.</span></td>
<td id="S5.T3.4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:17.3pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.1pt) scale(0.975,0.975) ;">
<p id="S5.T3.4.1.1.1.8.1.1" class="ltx_p"><span id="S5.T3.4.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold">Std.</span></p>
</span></div>
</td>
</tr>
<tr id="S5.T3.4.1.2.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;"><span id="S5.T3.4.1.2.2.1.1" class="ltx_text">Baselines</span></td>
<td id="S5.T3.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">MS-Celeb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T3.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">99.33</td>
<td id="S5.T3.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">97.73</td>
<td id="S5.T3.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">98.23</td>
<td id="S5.T3.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">98.32</td>
<td id="S5.T3.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">98.40</td>
<td id="S5.T3.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">0.68</td>
</tr>
<tr id="S5.T3.4.1.3.3" class="ltx_tr">
<td id="S5.T3.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding:1.5pt 2.4pt;" rowspan="2"><span id="S5.T3.4.1.3.3.1.1" class="ltx_text">(real)</span></td>
<td id="S5.T3.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">WebFace-4M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S5.T3.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">98.98</td>
<td id="S5.T3.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">97.10</td>
<td id="S5.T3.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">97.72</td>
<td id="S5.T3.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">97.63</td>
<td id="S5.T3.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">97.86</td>
<td id="S5.T3.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">0.80</td>
</tr>
<tr id="S5.T3.4.1.4.4" class="ltx_tr">
<td id="S5.T3.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.4.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:146.1pt;height:9.5pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T3.4.1.4.4.1.1.1" class="ltx_p">Casia-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite></p>
</span></div>
</td>
<td id="S5.T3.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">93.05</td>
<td id="S5.T3.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">85.07</td>
<td id="S5.T3.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">88.20</td>
<td id="S5.T3.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">85.57</td>
<td id="S5.T3.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">87.97</td>
<td id="S5.T3.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">3.66</td>
</tr>
<tr id="S5.T3.4.1.5.5" class="ltx_tr">
<td id="S5.T3.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;"><span id="S5.T3.4.1.5.5.1.1" class="ltx_text">Baselines</span></td>
<td id="S5.T3.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S5.T3.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">65.60</td>
<td id="S5.T3.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">64.48</td>
<td id="S5.T3.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">61.48</td>
<td id="S5.T3.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">57.27</td>
<td id="S5.T3.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">62.21</td>
<td id="S5.T3.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">3.73</td>
</tr>
<tr id="S5.T3.4.1.6.6" class="ltx_tr">
<td id="S5.T3.4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding:1.5pt 2.4pt;" rowspan="2"><span id="S5.T3.4.1.6.6.1.1" class="ltx_text">(synthetic)</span></td>
<td id="S5.T3.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S5.T3.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">71.93</td>
<td id="S5.T3.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">68.30</td>
<td id="S5.T3.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">69.02</td>
<td id="S5.T3.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">64.80</td>
<td id="S5.T3.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">68.52</td>
<td id="S5.T3.4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">2.94</td>
</tr>
<tr id="S5.T3.4.1.7.7" class="ltx_tr">
<td id="S5.T3.4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">IDNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T3.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">70.03</td>
<td id="S5.T3.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">64.22</td>
<td id="S5.T3.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">65.77</td>
<td id="S5.T3.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">59.30</td>
<td id="S5.T3.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">64.83</td>
<td id="S5.T3.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">4.44</td>
</tr>
<tr id="S5.T3.4.1.8.8" class="ltx_tr">
<td id="S5.T3.4.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;" rowspan="5"><span id="S5.T3.4.1.8.8.1.1" class="ltx_text">Task 1</span></td>
<td id="S5.T3.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">IGD-IDiff-Face</td>
<td id="S5.T3.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">85.98</td>
<td id="S5.T3.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">80.98</td>
<td id="S5.T3.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">82.45</td>
<td id="S5.T3.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">76.85</td>
<td id="S5.T3.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">81.57</td>
<td id="S5.T3.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">3.78</td>
</tr>
<tr id="S5.T3.4.1.9.9" class="ltx_tr">
<td id="S5.T3.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">APhi</td>
<td id="S5.T3.4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">84.55</td>
<td id="S5.T3.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">79.45</td>
<td id="S5.T3.4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">80.70</td>
<td id="S5.T3.4.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">73.85</td>
<td id="S5.T3.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">79.64</td>
<td id="S5.T3.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">4.43</td>
</tr>
<tr id="S5.T3.4.1.10.10" class="ltx_tr">
<td id="S5.T3.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.10.10.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:81.5pt;height:6.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T3.4.1.10.10.1.1.1" class="ltx_p">BOVIFOCR-UFPR</p>
</span></div>
</td>
<td id="S5.T3.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">83.65</td>
<td id="S5.T3.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">78.92</td>
<td id="S5.T3.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">80.18</td>
<td id="S5.T3.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">74.72</td>
<td id="S5.T3.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">79.37</td>
<td id="S5.T3.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">3.69</td>
</tr>
<tr id="S5.T3.4.1.11.11" class="ltx_tr">
<td id="S5.T3.4.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">BioLab</td>
<td id="S5.T3.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">81.70</td>
<td id="S5.T3.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">77.22</td>
<td id="S5.T3.4.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">78.97</td>
<td id="S5.T3.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">71.20</td>
<td id="S5.T3.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">77.27</td>
<td id="S5.T3.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">4.45</td>
</tr>
<tr id="S5.T3.4.1.12.12" class="ltx_tr">
<td id="S5.T3.4.1.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">BiDA-PRA</td>
<td id="S5.T3.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">82.12</td>
<td id="S5.T3.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">75.87</td>
<td id="S5.T3.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">77.88</td>
<td id="S5.T3.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">70.00</td>
<td id="S5.T3.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">76.47</td>
<td id="S5.T3.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">5.04</td>
</tr>
<tr id="S5.T3.4.1.13.13" class="ltx_tr">
<td id="S5.T3.4.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;" rowspan="3"><span id="S5.T3.4.1.13.13.1.1" class="ltx_text">Task 2</span></td>
<td id="S5.T3.4.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">BioLab</td>
<td id="S5.T3.4.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">86.32</td>
<td id="S5.T3.4.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">81.47</td>
<td id="S5.T3.4.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">82.22</td>
<td id="S5.T3.4.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">77.30</td>
<td id="S5.T3.4.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">81.82</td>
<td id="S5.T3.4.1.13.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 2.4pt;">3.70</td>
</tr>
<tr id="S5.T3.4.1.14.14" class="ltx_tr">
<td id="S5.T3.4.1.14.14.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">BiDA-PRA</td>
<td id="S5.T3.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">82.12</td>
<td id="S5.T3.4.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">75.87</td>
<td id="S5.T3.4.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">77.88</td>
<td id="S5.T3.4.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">70.00</td>
<td id="S5.T3.4.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">76.47</td>
<td id="S5.T3.4.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">3.48</td>
</tr>
<tr id="S5.T3.4.1.15.15" class="ltx_tr">
<td id="S5.T3.4.1.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">
<div id="S5.T3.4.1.15.15.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:81.5pt;height:6.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T3.4.1.15.15.1.1.1" class="ltx_p">BOVIFOCR-UFPR</p>
</span></div>
</td>
<td id="S5.T3.4.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">80.37</td>
<td id="S5.T3.4.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">76.47</td>
<td id="S5.T3.4.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">77.65</td>
<td id="S5.T3.4.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">71.08</td>
<td id="S5.T3.4.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">76.39</td>
<td id="S5.T3.4.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 2.4pt;">3.91</td>
</tr>
<tr id="S5.T3.4.1.16.16" class="ltx_tr">
<td id="S5.T3.4.1.16.16.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 2.4pt;" colspan="8">The values reported in the table are accuracy and in percentage.</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Some of the synthetic datasets in the literature, such as DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, have a conflict with the competition rules, and therefore participants were not allowed to use them for their final submission.
DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> used CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to train the dual condition generator, where training the generator requires two samples of the same subject, and thus it requires an identity-labeled dataset. SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> also used CASIA-WebFace with identity labels to train a conditional version of StyleGAN2-ADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (conditioned on identity). Therefore, DCFace and SFace rely on CASIA-WebFace, as a large-scale web-crawled dataset.
GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> used StyleGAN to generate different synthetic images and then used DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to generate different samples of images that were generated by StyleGAN in the first place. However, DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is based on Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which is trained on the LAION <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset. The LAION dataset is a large-scale web-crawled dataset, which includes the identity label of famous people in the caption of images. As a result, Stable Diffusion (a text-to-image model) can generate images of famous people given the name of the person in the input text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, raising privacy and ethical issues. Therefore, since the generator in GANDiffFace is trained and benefits from identity labels in the LAION dataset, the GANDiffFace dataset was not allowed in the competition.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x2.png" id="S5.F2.1.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x3.png" id="S5.F2.2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x4.png" id="S5.F2.3.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x5.png" id="S5.F2.4.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x6.png" id="S5.F2.5.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x7.png" id="S5.F2.6.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x8.png" id="S5.F2.7.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x9.png" id="S5.F2.8.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x10.png" id="S5.F2.9.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x11.png" id="S5.F2.10.g1" class="ltx_graphics ltx_img_square" width="631" height="651" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x12.png" id="S5.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x13.png" id="S5.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x14.png" id="S5.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x15.png" id="S5.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.04580/assets/x16.png" id="S5.F2.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="631" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.12.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S5.F2.13.2" class="ltx_text" style="font-size:90%;">Sample face images from the new custom generated dataset by the BioLab team generated for different famous people:
(a) Morgan Freeman, (b) Taylor Swift, (c) Bill Gates, (d) Jennifer Aniston, (e) Gary Oldman.
</span></figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In addition to the submissions reported in the leaderboard, some participating teams trained models with datasets which had a conflict with the competition rules.
The BOVIFOCR-UFPR team also used DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> with 1.2M images, and dropped 5,454 identities with 55 images per id, reducing the total number of images to 999,975 to fulfill the maximum allowed number of images in task 1. Then, they trained iResNet-50 with the same data augmentation and loss function as their final submission for task 1 described in Section <a href="#S3" title="III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
The APhi team submitted two models for task 1 trained on DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In the first model, they selected arbitrary 1M images with 59,968 identities and trained the iResNet-50 network with AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> loss function. In their second model, they selected 1M based on the face quality. To this end, they extracted the MagFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> score of each image, and then they selected the best quality images, resulting in 1M images and 59,894 identities. Then, they trained the iResNet-50 network with ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> loss function.
The BioLab team submitted a model trained with a newly generated dataset based on Stable Diffusion as well as with DCFace.
Their newly generated dataset based on Stable Diffusion, was built by querying Wikidata<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://www.wikidata.org/" title="" class="ltx_ref ltx_href">www.wikidata.org</a></span></span></span> for all people born between 1900 and 2004 that have an associated picture. To remove subjects that may be difficult to generate with Stable Diffusion, they filtered the list by keeping only the ones with at least 30 translations of their Wikipedia page. After gathering the list, they generated 64 images per subject by employing the Realistic Vision 5.1 model, fine-tuned from Stable Diffusion 1.5. Then, they filtered the generated images by computing ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> features for each image, computing the distance between each pair of images of each subject, and finally keeping the largest subset so that the cosine distance between all feature vectors is less than a given threshold. Moreover, if after this filtering step, there were not enough images for a given subject, that subject was removed entirely from the dataset.
This process led to a dataset with around 825K images across 18K identities. Fig. <a href="#S5.F2" title="Figure 2 ‣ V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates sample images from their generated dataset.
For task 1, they used their newly generated dataset<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>As this approach relies on Stable Diffusion (trained on the LAION <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset) for image generation, this submission had a conflict with competition rules, which prevent the use of large-scale datasets with identity labels to train face generator network.</span></span></span> and trained iResNet-50 with the same data augmentation and loss function as their final submission described in Section <a href="#S3" title="III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. For task 2, they also used the same training approach to train the iResNet-100 network, but using the DCFace dataset with 1.2M images.
Table <a href="#S5.T4" title="TABLE IV ‣ V Discussion ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> reports the performance of these submissions on benchmarking datasets. As the results in this table show, these submissions achieve competitive performance with the final submissions on the leaderboards in Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S5.T4.5.2" class="ltx_text" style="font-size:90%;">Submissions with not qualified datasets.</span></figcaption>
<div id="S5.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:180.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(43.5pt,-18.1pt) scale(1.25124215120489,1.25124215120489) ;">
<table id="S5.T4.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2.3.1" class="ltx_tr">
<th id="S5.T4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Team (method)</span></th>
<td id="S5.T4.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2">
<div id="S5.T4.2.2.3.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:19.6pt;height:5.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.5pt) scale(0.85,0.85) ;">
<p id="S5.T4.2.2.3.1.2.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.2.1.1.1" class="ltx_text ltx_font_bold">LFW</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2">
<div id="S5.T4.2.2.3.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:31.2pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.3pt,0.6pt) scale(0.825,0.825) ;">
<p id="S5.T4.2.2.3.1.3.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.3.1.1.1" class="ltx_text ltx_font_bold">CALFW</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2">
<div id="S5.T4.2.2.3.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:33.3pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.5pt,0.6pt) scale(0.825,0.825) ;">
<p id="S5.T4.2.2.3.1.4.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.4.1.1.1" class="ltx_text ltx_font_bold">CPLFW</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2">
<div id="S5.T4.2.2.3.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:34.4pt;height:7.199999999999999pt;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.6pt,0.6pt) scale(0.825,0.825) ;">
<p id="S5.T4.2.2.3.1.5.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.5.1.1.1" class="ltx_text ltx_font_bold">AgeDB30</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2">
<div id="S5.T4.2.2.3.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:30.7pt;height:5.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.3pt,0.6pt) scale(0.825,0.825) ;">
<p id="S5.T4.2.2.3.1.6.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.6.1.1.1" class="ltx_text ltx_font_bold">CFP-FP</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.2.2.3.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:22.3pt;height:5.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.0pt,0.5pt) scale(0.85,0.85) ;">
<p id="S5.T4.2.2.3.1.7.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.7.1.1.1" class="ltx_text ltx_font_bold">IJB-B</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.2.2.3.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:22.4pt;height:5.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.0pt,0.5pt) scale(0.85,0.85) ;">
<p id="S5.T4.2.2.3.1.8.1.1" class="ltx_p"><span id="S5.T4.2.2.3.1.8.1.1.1" class="ltx_text ltx_font_bold">IJB-C</span></p>
</span></div>
</td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:35.3pt;height:7.4pt;vertical-align:-1.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,1.3pt) scale(0.685,0.685) ;">
<p id="S5.T4.1.1.1.1.1.1" class="ltx_p"><span id="S5.T4.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">(TAR@<math id="S5.T4.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{10^{-4}}" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.1.m1.1a"><msup id="S5.T4.1.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S5.T4.1.1.1.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.2.cmml">𝟏𝟎</mn><mrow id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mo id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3a" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.cmml">−</mo><mn id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">𝟒</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.2">10</cn><apply id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3"><minus id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3"></minus><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.1.m1.1c">\mathbf{10^{-4}}</annotation></semantics></math>)</span></p>
</span></div>
</td>
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.2.2.2.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:35.3pt;height:7.4pt;vertical-align:-1.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,1.3pt) scale(0.685,0.685) ;">
<p id="S5.T4.2.2.2.2.1.1" class="ltx_p"><span id="S5.T4.2.2.2.2.1.1.1" class="ltx_text ltx_font_bold">(TAR@<math id="S5.T4.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{10^{-4}}" display="inline"><semantics id="S5.T4.2.2.2.2.1.1.1.m1.1a"><msup id="S5.T4.2.2.2.2.1.1.1.m1.1.1" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.cmml"><mn id="S5.T4.2.2.2.2.1.1.1.m1.1.1.2" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.2.cmml">𝟏𝟎</mn><mrow id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.cmml"><mo id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3a" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.cmml">−</mo><mn id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.2" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.2.cmml">𝟒</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.1.1.1.m1.1b"><apply id="S5.T4.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.2.2.2.2.1.1.1.m1.1.1.1.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.T4.2.2.2.2.1.1.1.m1.1.1.2.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.2">10</cn><apply id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3"><minus id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3"></minus><cn type="integer" id="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.2.2.2.2.1.1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.1.1.1.m1.1c">\mathbf{10^{-4}}</annotation></semantics></math>)</span></p>
</span></div>
</td>
</tr>
<tr id="S5.T4.2.2.4.2" class="ltx_tr">
<th id="S5.T4.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.2.2.4.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:79.4pt;height:6.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.2pt,0.3pt) scale(0.925,0.925) ;">
<p id="S5.T4.2.2.4.2.1.1.1" class="ltx_p">BOVIFOCR-UFPR</p>
</span></div>
</th>
<td id="S5.T4.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">98.98</td>
<td id="S5.T4.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">92.42</td>
<td id="S5.T4.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">86.38</td>
<td id="S5.T4.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">91.38</td>
<td id="S5.T4.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">90.89</td>
<td id="S5.T4.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">79.62</td>
<td id="S5.T4.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">84.11</td>
</tr>
<tr id="S5.T4.2.2.5.3" class="ltx_tr">
<th id="S5.T4.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">APhi 
<div id="S5.T4.2.2.5.3.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:43.5pt;height:9.5pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.1pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T4.2.2.5.3.1.1.1" class="ltx_p">(model 1)</p>
</span></div>
</th>
<td id="S5.T4.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">98.83</td>
<td id="S5.T4.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">92.25</td>
<td id="S5.T4.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">82.93</td>
<td id="S5.T4.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">90.95</td>
<td id="S5.T4.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">87.36</td>
<td id="S5.T4.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">78.41</td>
<td id="S5.T4.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">82.38</td>
</tr>
<tr id="S5.T4.2.2.6.4" class="ltx_tr">
<th id="S5.T4.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">APhi 
<div id="S5.T4.2.2.6.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:43.5pt;height:9.5pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.1pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T4.2.2.6.4.1.1.1" class="ltx_p">(model 2)</p>
</span></div>
</th>
<td id="S5.T4.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">98.38</td>
<td id="S5.T4.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">92.22</td>
<td id="S5.T4.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">84.27</td>
<td id="S5.T4.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">90.88</td>
<td id="S5.T4.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">87.57</td>
<td id="S5.T4.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">79.48</td>
<td id="S5.T4.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">83.61</td>
</tr>
<tr id="S5.T4.2.2.7.5" class="ltx_tr">
<th id="S5.T4.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;"><span id="S5.T4.2.2.7.5.1.1" class="ltx_text">BioLab</span>
</th>
<td id="S5.T4.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.2.1" class="ltx_text">93.90</span></td>
<td id="S5.T4.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.3.1" class="ltx_text">82.30</span></td>
<td id="S5.T4.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.4.1" class="ltx_text">71.23</span></td>
<td id="S5.T4.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.5.1" class="ltx_text">74.42</span></td>
<td id="S5.T4.2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.6.1" class="ltx_text">72.04</span></td>
<td id="S5.T4.2.2.7.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.7.1" class="ltx_text">49.87</span></td>
<td id="S5.T4.2.2.7.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;" rowspan="2"><span id="S5.T4.2.2.7.5.8.1" class="ltx_text">52.76</span></td>
</tr>
<tr id="S5.T4.2.2.8.6" class="ltx_tr">
<th id="S5.T4.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding:1.25pt 2.4pt;">
<div id="S5.T4.2.2.8.6.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:70.3pt;height:9.5pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.9pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T4.2.2.8.6.1.1.1" class="ltx_p">(custom dataset)</p>
</span></div>
</th>
</tr>
<tr id="S5.T4.2.2.9.7" class="ltx_tr">
<th id="S5.T4.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">BioLab 
<div id="S5.T4.2.2.9.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:40.1pt;height:9.5pt;vertical-align:-2.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.1pt,0.2pt) scale(0.95,0.95) ;">
<p id="S5.T4.2.2.9.7.1.1.1" class="ltx_p">(DCFace)</p>
</span></div>
</th>
<td id="S5.T4.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">98.52</td>
<td id="S5.T4.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">92.48</td>
<td id="S5.T4.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">83.35</td>
<td id="S5.T4.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">91.12</td>
<td id="S5.T4.2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">87.63</td>
<td id="S5.T4.2.2.9.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">30.17</td>
<td id="S5.T4.2.2.9.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding:1.25pt 2.4pt;">24.38</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In drawing our discussion to a close, the submissions in this competition used creative approaches for training face recognition using synthetic data and could achieve improvements compared to baselines based on synthetic datasets. However, as the results in Table <a href="#S3.T2" title="TABLE II ‣ III Description of Submissions ‣ SDFR: Synthetic Data for Face Recognition Competition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show, we still observe a significant gap between models trained with synthetic data and models trained with large-scale web-crawled datasets, demanding more research on generation and training face recognition with synthetic data. In fact, one aspect of this challenge is to increase inter-class and intra-class variation in generating synthetic face datasets. Compared to large-scale datasets, such as MS-Celeb which has 5.8M images, the largest synthetic face recognition dataset in the literature has 1.2M images (DigiFace). Therefore, another potential aspect of the topic for future work can be to explore if we can scale synthetic face datasets to generate more images and investigate if it can contribute to the recognition performance of face recognition models. In fact, scaling synthetic face recognition datasets may not be straightforward, as we may achieve the generation capacity of face generator networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which also requires further studies.
We should also note that in this competition and in most methods for generating synthetic face recognition datasets, still a pre-trained face recognition model is used in the dataset generation process. Therefore, if we agree on using a pre-trained face recognition model in data generation, it has been shown that directly learning embeddings of a pre-trained face recognition model using synthetic data, without any identity class, achieves superior performance than training using existing synthetic face recognition datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
However,
in a <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">fully</span> privacy-friendly and responsible synthetic face recognition dataset, no pre-trained face recognition model based on large-scale web-crawled face datasets should be used, otherwise, the generated dataset still relies on large-scale web-crawled datasets. This is in fact an important future research direction that requires more attention from the research community.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Existing large-scale face recognition datasets are collected without individuals’ consent and by crawling the Internet, raising legal, ethical, and privacy concerns. While generating a synthetic face recognition dataset can be a potential solution, it is challenging to generate a synthetic dataset with sufficient inter-class and intra-class variations. As a result, there is still a gap between training face recognition models with real and synthetic datasets. The submitted models in the SDFR competition deployed different techniques using newly generated and existing datasets to improve the performance of face recognition models trained with synthetic datasets.
In this paper, different submissions from participating teams in the SDFR competition were presented and their trained models were evaluated on a diverse set of benchmarking datasets. We also outlined the current state of research on this topic and discussed open problems, such as scaling synthetic datasets as well as increasing the variations in generated images, which require more research in the future.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The organization of this competition was supported by the H2020 TReSPAsS-ETN Marie Skłodowska-Curie early training network (grant agreement 860813) as well as the Hasler foundation through the “Responsible Face Recognition” (SAFER) project.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">The work of BioLab team received funding from the European Union’s Horizon 2020 research and innovation program under Grant Agreement No. 883356 (Disclaimer: the text reflects only the author’s views, and the Commission is not liable for any use that may be made of the information contained therein). The BioLab team would like to thank Andrea Pilzer, NVIDIA AI Technology Center, EMEA, for his support. The BioLab team also acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">The submitted solution by the IGD-IDiff-Face team has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">The work of BiDA-PRA team was supported by INTER-ACTION (PID2021-126521OB-I00 MICINN/FEDER), Cátedra ENIA UAM-VERIDAS en IA Responsable (NextGenerationEU PRTR TSI-100927-2023-2), and R&amp;D Agreement DGGC/UAM/FUAM for Biometrics and Cybersecurity.
The work of BiDA-PRA team was also supported by the European Union – Next Generation EU through the Italian Ministry of University and Research (MUR) within the PRIN PNRR 2022 – BullyBuster 2 - the ongoing fight against bullying and cyberbullying with the help of artificial intelligence for the human wellbeing (CUP: F53D23009240001).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
The rise and fall (and rise) of datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Machine Intelligence</span><span id="bib.bib1.3.2" class="ltx_text" style="font-size:90%;">, 4(1):1–2, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
G. Bae, M. de La Gorce, T. Baltrušaitis, C. Hewitt, D. Chen, J. Valentin, R. Cipolla, and J. Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">DigiFace-1M: 1 million digital face images for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 3526–3535, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
V. N. Boddeti, G. Sreekumar, and A. Ross.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">On the biometric capacity of generative face models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 1–10. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
F. Boutros, N. Damer, J. N. Kolf, K. Raja, F. Kirchbuchner, R. Ramachandra, A. Kuijper, P. Fang, C. Zhang, F. Wang, D. Montero, N. Aginako, B. Sierra, M. Nieto, M. E. Erakin, U. Demir, H. K. Ekenel, A. Kataoka, K. Ichikawa, S. Kubo, J. Zhang, M. He, D. Han, S. Shan, K. Grm, V. Štruc, S. Seneviratne, N. Kasthuriarachchi, S. Rasnayaka, P. C. Neto, A. F. Sequeira, J. R. Pinto, M. Saffari, and J. S. Cardoso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">MFR 2021: Masked face recognition competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International joint conference on biometrics (IJCB)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1–10. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
F. Boutros, J. H. Grebe, A. Kuijper, and N. Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">IDiff-Face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 19650–19661, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
F. Boutros, M. Huber, P. Siebke, T. Rieber, and N. Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">SFace: Privacy-friendly and accurate face recognition using synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1–11. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
F. Boutros, M. Klemt, M. Fang, A. Kuijper, and N. Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Unsupervised face recognition using unlabeled synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
F. Boutros, V. Struc, J. Fierrez, and N. Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Synthetic data for face recognition: Current state and future prospects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Image and Vision Computing</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, page 104688, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">VGGFace2: A dataset for recognising faces across pose and age.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 67–74. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
S. Cheng, P. Ma, G. Tzimiropoulos, S. Petridis, A. Bulat, J. Shen, and M. Pantic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Towards pose-invariant lip-reading.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4357–4361. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
L. Colbois, T. de Freitas Pereira, and S. Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">On the use of automatically generated synthetic image datasets for benchmarking face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Randaugment: Practical automated data augmentation with a reduced search space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 702–703, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, J. Guo, X. An, Z. Zhu, and S. Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Masked face recognition challenge: The insightface track report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 1437–1444, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, J. Guo, N. Xue, and S. Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">ArcFace: Additive angular margin loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 4690–4699, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, and S. Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Lightweight face recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
A. George, C. Ecabert, H. O. Shahreza, K. Kotwal, and S. Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">EdgeFace: Efficient face recognition model for edge devices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Biometrics, Behavior, and Identity Science</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 6(2):158–168, 2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 87–102. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop on faces in’Real-Life’Images: detection, alignment, and recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
K. Karkkainen and J. Joo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">FairFace: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 1548–1558, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Training generative adversarial networks with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 33:12104–12114, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen, and T. Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Alias-free generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 34:852–863, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
T. Karras, S. Laine, and T. Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">A style-based generator architecture for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 4401–4410, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Kim, A. K. Jain, and X. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">AdaFace: Quality adaptive margin for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 18750–18759, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
M. Kim, F. Liu, A. Jain, and X. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">DCFace: Synthetic face generation with dual condition diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 12715–12725, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
J. N. Kolf, F. Boutros, J. Elliesen, M. Theuerkauf, N. Damer, M. Alansari, O. A. Hay, S. Alansari, S. Javed, N. Werghi, K. Grm, V. Štruc, F. Alonso-Fernandez, K. H. Diaz, J. Bigun, A. George, C. Ecabert, H. O. Shahreza, K. Kotwal, S. Marcel, I. Medvedev, B. Jin, D. Nunes, A. Hassanpour, P. Khatiwada, A. A. Toor, and B. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">EFaR 2023: Efficient face recognition competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 1–12. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
J. N. Kolf, T. Rieber, J. Elliesen, F. Boutros, A. Kuijper, and N. Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Identity-driven three-player generative adversarial network for synthetic-based face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 806–816, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
C. Kong, Q. Luo, and G. Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">RSFAD: A large-scale real scenario face age dataset in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 1–7. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Cheney, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">IARPA janus benchmark-c: Face dataset and protocol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Biometrics (ICB)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 158–165. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
P. Melzi, C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, D. Lawatsch, F. Domin, and M. Schaubert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">GANDiffFace: Controllable generation of synthetic datasets for face recognition with realistic variations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 3086–3095, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
P. Melzi, R. Tolosana, R. Vera-Rodriguez, M. Kim, C. Rathgeb, X. Liu, I. DeAndres-Tame, A. Morales, J. Fierrez, J. Ortega-Garcia, W. Zhao, X. Zhu, Z. Yan, X.-Y. Zhang, J. Wu, Z. Lei, S. Tripathi, M. Kothari, M. H. Zama, D. Deb, B. Biesseck, P. Vidal, R. Granada, G. Fickel, G. Führ, D. Menotti, A. Unnervik, A. George, C. Ecabert, H. O. Shahreza, P. Rahimi, S. Marcel, I. Sarridis, C. Koutlis, G. Baltsou, S. Papadopoulos, C. Diou, N. D. Domenico, G. Borghi, L. Pellegrini, E. Mas-Candela, Ángela Sánchez-Pérez, A. Atzori, F. Boutros, N. Damer, G. Fenu, and M. Marras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">FRCSyn-onGoing: Benchmarking and comprehensive evaluation of real and synthetic data to improve face recognition systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 107:102322, 2024.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
P. Melzi, R. Tolosana, R. Vera-Rodriguez, M. Kim, C. Rathgeb, X. Liu, I. DeAndres-Tame, A. Morales, J. Fierrez, J. Ortega-Garcia, W. Zhao, X. Zhu, Z. Yan, X.-Y. Zhang, J. Wu, Z. Lei, S. Tripathi, M. Kothari, M. H. Zama, D. Deb, B. Biesseck, P. Vidal, R. Granada, G. Fickel, G. Führ, D. Menotti, A. Unnervik, A. George, C. Ecabert, H. O. Shahreza, P. Rahimi, S. Marcel, I. Sarridis, C. Koutlis, G. Baltsou, S. Papadopoulos, C. Diou, N. D. Domenico, G. Borghi, L. Pellegrini, E. Mas-Candela, Ángela Sánchez-Pérez, A. Atzori, F. Boutros, N. Damer, G. Fenu, and M. Marras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">FRCSyn challenge at WACV 2024: Face recognition challenge in the era of synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 892–901, 2024.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Q. Meng, S. Zhao, Z. Huang, and F. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">MagFace: A universal representation for face recognition and quality assessment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vsion and Pattern Recognition (CVPR)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 14225–14234, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">AgeDB: the first manually collected, in-the-wild age database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 51–59, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, and D. Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">SynFace: Face recognition with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 10880–10890, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 22500–22510, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">LAION-5B: An open large-scale dataset for training next generation image-text models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 35:25278–25294, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
H. O. Shahreza, A. George, and S. Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">SynthDistill: Face recognition with knowledge distillation from synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 1–10. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">CosFace: Large margin cosine loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 5265–5274, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
M. Wang, W. Deng, J. Hu, X. Tao, and Y. Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Racial faces in the wild: Reducing racial bias by information maximization adaptation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 692–702, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. Adams, T. Miller, N. Kalka, A. K. Jain, J. A. Duncan, K. Allen, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">IARPA janus benchmark-b face dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 90–98, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Y. Yang, Y. lin, H. Liu, W. Shao, R. Chen, H. Shang, Y. Wang, Y. Qiao, K. Zhang, and P. Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Towards implicit prompt for text-to-image models, 2024.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
D. Yi, Z. Lei, S. Liao, and S. Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Learning face representation from scratch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.7923</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
K. Zhang, Z. Zhang, Z. Li, and Y. Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Joint face detection and alignment using multitask cascaded convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Letters</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 23(10):1499–1503, 2016.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
T. Zheng and W. Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Cross-Pose LFW: A database for studying cross-pose face recognition in unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Beijing University of Posts and Telecommunications, Tech. Rep</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 5(7), 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
T. Zheng, W. Deng, and J. Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Cross-Age LFW: A database for studying cross-age face recognition in unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1708.08197</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Random erasing data augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on Artificial Intelligence</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, volume 34, pages 13001–13008, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
X. Zhu, X. Liu, Z. Lei, and S. Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Face alignment in full pose range: A 3d total solution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 41(1):78–92, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Z. Zhu, G. Huang, J. Deng, Y. Ye, J. Huang, X. Chen, J. Zhu, T. Yang, J. Lu, D. Du, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">WebFace260M: A benchmark unveiling the power of million-scale deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 10492–10502, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.04579" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.04580" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.04580">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.04580" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.04581" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 16:13:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
