<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1909.01960] Beyond Photo Realism for Domain Adaptation from Synthetic Data</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Photo Realism for Domain Adaptation from Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Photo Realism for Domain Adaptation from Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1909.01960">

<!--Generated on Tue Mar 19 18:21:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Beyond Photo Realism for Domain Adaptation from Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kristofer Schlachter
<br class="ltx_break">NYU
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">schlacht@cims.nyu.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Connor DeFanti
<br class="ltx_break">NYU
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">cdefanti@cims.nyu.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastian Herscher
<br class="ltx_break">NYU
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">herscher@nyu.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ken Perlin
<br class="ltx_break">NYU
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">perlin@nyu.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonathan Tompson
<br class="ltx_break">Google Brain
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">tompson@google.com</span>
</span></span>
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Abstract</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As synthetic imagery is used more frequently in training deep models, it is important to understand how different synthesis techniques impact the performance of such models. In this work, we perform a thorough evaluation of the effectiveness of several different synthesis techniques and their impact on the complexity of classifier domain adaptation to the “real” underlying data distribution that they seek to replicate. In addition, we propose a novel learned synthesis technique to better train classifier models than state-of-the-art offline graphical methods, while using significantly less computational resources. We accomplish this by learning a generative model to perform shading of synthetic geometry conditioned on a “g-buffer” representation of the scene to render, as well as a low sample Monte Carlo rendered image. The major contributions are (i) a dataset that allows comparison of real and synthetic versions of the same scene, (ii) an augmented data representation that boosts the stability of learning and improves the datasets accuracy, (iii) three different partially differentiable rendering techniques where lighting, denoising and shading are learned, and (iv) we improve a state of the art generative adversarial network (GAN) approach by using an ensemble of trained models to generate datasets that approach the performance of training on real data and surpass the performance of the full global illumination rendering.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Introduction</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Applying deep learning to supervised computer vision tasks commonly requires large labeled datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which can be time consuming, expensive or impractical to collect. As a result, increasingly more synthetic data has been used to overcome these scalability limitations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. However, using synthetic data can introduce problems that arise from the differences in the distributions of the real and synthesized data domains<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Usually the best way to reduce the differences is to use the most realistic simulation method available for synthesizing images, in this case a Global Illumination (GI) based renderer such as Mitsuba<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Unfortunately, the computational cost of most GI renderers can be prohibitive. Recently, there has been research into boosting the effectiveness of existing synthetic data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. However, these new techniques were not benchmarked against real RGB data or against other rendering techniques.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">This work explores the effectiveness of various rendering methods, which range from very low to very high computational cost. We compare multiple synthetic rendering methods with the performance obtained using the ”real” data distribution as an optimal baseline. This is made possible since we annotate an existing image classification dataset (NORB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) with pixel aligned geometry labels obtained using a high-quality photometric scanner. We represent the geometry and material information of the RGB scene using a ”Geometry Buffer” or ”g-buffer”, which gives us a standard set of 3D scenes where geometric transformations are frozen in place. By using a g-buffer, we isolate the physical process of shading a surface from the geometric process of geometry pose and projection. The shading process is also decoupled from camera parameters, including location, orientation and field of view. This allows us to directly compare different rendering methods and enables us to measure any performance difference due to shading alone.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Capitalizing on this standard 3D scene representation, we setup a number of experiments to compare rendering methods of increasing sophistication and computational cost. We then measure the benefit of adding these rendering techniques and see if the computational cost can be justified. These experiments measure the relative importance of image rendering features. We use a full GI rendering of Mitsuba<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as the method whose performance we sought to match or surpass.
We are able to do this by using non-standard rendering techniques that use convolutional networks to learn a domain optimal shading function. We use this network as a form of rendering where the shading is differentiable. Effectively, we use learned shading to denoise low sample count GI rendering and compared it to high sample count images. We also explore the use of Generative Adversarial Networks to learn shading functions that could render images in unique ways that even outperform the accurate, but extremely expensive GI output.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Contributions</h3>

<div id="S2.SS1.p1" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">We provide a dataset (which we will make public) that allows the direct comparison of training on real data versus synthetic by including 3D scans of the objects that are in the photographic dataset.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">We introduce a novel method of using an ensemble of GAN trained generative models to create an augmented dataset.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">We contribute a thorough comparison of the performance of all the rendering methods with real data.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Previous papers have used g-buffers to condition a learned generative model for shading. Nalbach et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> uses a similar model architecture as the denoiser we employ as well as using a g-buffer as input. The authors use it to learn to approximate screen space shading techniques with a data-driven approach, to see if data driven methods can replicate the standard shading methods used in games. They differ in their approach in that they are optimizing for images looking real to humans and not optimizing the images’ utility for training classifiers or to improve domain adaptation.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">There have been recent advances in denoising low sample count images calculated by Global Illumination (GI) based renderers.
Shied et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> denoises low sample count GI images by combining neighboring frames and feeding them into a fixed spatio-temporal filter. The authors emphasis is on temporal stability, efficient computation and denoising single sample frames in real-time. Their authors do not employ machine learning in their method.
Chaitanya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> along with
Bako et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and
Kalantari et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> all use machine learning to denoise Monte Carlo (GI) renderings for graphics applications. This work uses a modified form of the model presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in order to denoise images used for training classifiers with the goal of synthesizing training data faster.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Further use of global illumination for training classifiers has been explored.
Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> use a custom created large 3D indoor dataset to conduct experiments relating the performance of different renderers and lighting for various machine learning tasks, such as normal estimation, semantic segmentation, and object boundary detection. The approach presented in this work, in contrast, focuses mainly on shading and its effect on training data quality.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Using a video game engine for dataset creation has been explored by Richter et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and Shafaei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. They explored the use of video game snapshots to show that it could train image segmentation models. We build upon their approach by using learned rendering techniques to improve the synthesis quality and to decrease rendering computational cost.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">To learn shading, we used Generative Adversarial Networks (GANs) which were first introduced by Goodfellow et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to learn complex multi-modal distributions. Since then many works have used them to synthesize images.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Learning, partially differentiable renderers was proposed by Sixt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. In their paper they train a differentiable 3D renderer, except their model is scene and application specific. The differentiable rendering model architecture we implemented is not restricted to a specific shading model, geometric input or lighting condition.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">A GAN based method that uses unlabeled data to refine realistic rendered images was pioneered by
Shrivastava et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The paper proposes an architecture that takes in rendered images and makes them more realistic by using an adversarial loss to tune a refiner model. We use this paper as a starting point for our GAN experiments; however, we modify many aspects of it to suit our input, which is a g-buffer (not a pre-rendered image).</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">A GAN-based method related to Shrivastava et al. is Bousmalis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This paper proposes a very similar method, but conditions the input with a noise vector and a depth buffer. The depth buffer is used to mask out the background so the refiner network can concentrate upon pixels containing the object to be classified. Our method conditions on more data including normals and albedo and does not mask out parts of the image which allows for shadowing to be modeled. We also improve upon these two papers by using an ensemble of trained GAN generative models to boost training data performance.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Rendering Methods and Techniques</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>G-Buffer</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">G-Buffers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> separate the geometric processes such as transformation, projection and depth testing, from physical processes such as shading and texture mapping. This is accomplished by the buffers accumulating the geometry information in the first pass and then lighting each pixel of the output on a second pass (a process that is linear in both model and lighting complexity). This is in comparison to a traditional “forward renderer” which calculates geometry information and performs lighting and shadowing in one pass, followed by and writing the result to the output buffer (with quadratic complexity). The technique of using G-Buffers and screen-space rendering is known as “deferred shading,” where expensive lighting and shadowing computation is done after all of the visible geometry is encoded into its buffers.
As an added benefit for this work, the encoding of the geometry state into G-Buffers allows for simplified testing of different shading techniques on the identical 3D scenes.
In this work’s experiments, the G-Buffer consists of the scene encoded geometry normals, geometry diffuse color (also referred to as albedo), and the linear depth from the camera to the visible geometry. See figure <a href="#S3.F1" title="Figure 1 ‣ 3.1.1 G-Buffer ‣ 3.1 Rendering Methods and Techniques ‣ 3 Related Work ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, for examples. We describe how the datasets are produced in <a href="#S4" title="4 Training Data Creation ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1909.01960/assets/assets/gbuffer-onerow.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">G-Buffer. Left is Albedo, middle is Depth, right is world space Normals.</span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Spherical Harmonics</h4>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1909.01960/assets/assets/SH_Fig.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Visualization of first 3 bands of the spherical harmonics functions. Red are negative values and blue are positive. These are the corresponding 9 functions to the 9 coefficients we learn for lighting.</span></figcaption>
</figure>
<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In this work’s experiments, we use a set of functions called Spherical Harmonics (SH) to learn the lighting of an environment. Spherical Harmonics are a set of continuous and rotationally invariant functions that approximate an arbitrary spherical function, such as the distribution of light reflected off a surface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, also known as a bidirectional reflectance distribution function (BRDF). The orthogonal functions are grouped into levels of increasing detail, resolution, or frequency. SH are grouped into levels, where each level is a set of orthogonal functions of finer detail, resolution, or frequency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. To represent the smooth lighting, we only need the first three levels which are a constant, linear polynomials, and quadratic polynomials of the surface normal. See Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1.2 Spherical Harmonics ‣ 3.1 Rendering Methods and Techniques ‣ 3 Related Work ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for a visualization of the first three levels.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">An advantage to using Spherical Harmonics for lighting is that you can represent a smooth lighting environment with just 9 floats. This allows us to parameterize a lighting function with far fewer parameters compared to other representations. This has the effect of drastically reducing the search space when trying to learn a parameterized lighting model.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">We define a trainable module that calculates three levels of SH basis functions. The input to the module consists of the surface normal at every pixel, which is conveniently stored in the G-Buffer. The network outputs a shaded image which can be composited with shadowing and albedo.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Ambient Occlusion</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">In an environment with only large area light sources, there exists a simple first order approximation to global illumination called Ambient Occlusion. It is used as a form of simplified global illumination because it takes into account only screen-space visible scene geometry<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Each view-ray from the camera’s pixels defines a single point on the surface of the scene geometry. The algorithm uses these local screen-space geometry samples to approximate how exposed each surface fragment is to incoming ambient light sources by estimating how much of the hemisphere around that point is blocked by nearby surfaces. It is an approximation since the geometry visible to the camera may not represent all local geometry around the surface patch. However, for lighting conditions that are close to a uniformly bright hemisphere, like an overcast day, and for simple geometry, then ambient occlusion is a good approximation of global illumination.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Global Illumination</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Global illumination (GI) refers to rendering methods that take into account inter-surface reflection, to significantly improved visual fidelity in comparison to faster approximate methods that only simulate direct lighting (or limited bounce counts). GI derives its accuracy from its ability to take into account the light that arrives at a surface not just directly from a light source, but from light that has been reflected off of other surfaces as well. Different surface materials have different reflection characteristics that effect how much sampling must be done in order to converge to the correct rendering solution. Each dataset required different sample counts to converge due to the different material distributions.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Training Data Creation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Direct comparisons between rendering techniques and real images could not be done without creating a dataset that has approximately 1:1 correspondence between domains. To this end we identified the NYU Object Recognition Benchmark (NORB) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as a good starting point. It is small enough to allow for recreation of each photograph with high accuracy in simulation. A characteristic that makes NORB even more attractive is that the object location, rotation, camera placement and lighting condition are known for every image. Furthermore, each object was modified so that the BRDF of each object was approximately homogeneous across the dataset.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">However, the NORB dataset is comprised of grayscale images, which is less-relevant for modern deep-learning architectures. As such, we composed an additional dataset of synthetic objects from ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (choosing a disjoint set of object classes), where the ground-truth RGB image is the result of our most expensive offline rendering method available. This dataset was digital-only and did not have physical models that we could scan like the NORB dataset. So, we used synthetic renders for both training the learning model and testing its accuracy.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">While this is not sufficient on its own to make conclusions about the learning model used in this work on real image data, we use this additional synthetic dataset to validate the grayscale NORB results on a RGB dataset of similar complexity and composition.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>NORB Dataset</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/1909.01960/assets/assets/NORBTestSmall.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="76" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">A sample of the NORB test dataset. Note the dark shadows under some of the models, animals in particular.</span></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The NORB dataset consists of a set of 50 toys split into a 25 object training set and a 25 object test set. For each object, stereo photographs were taken with the object on a turntable in 6 controlled lighting conditions and with 18 camera azimuth angles and 9 elevation angles. This results in a dataset with 48,600 stereo pictures. We limited the dataset to just the 25 objects in the training set. We also confined the dataset to one lighting condition, reducing the image count in the training dataset to 4,050 images. The toys were painted a uniform color with a matte paint. This was intended to prevent texture from being used in classification.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">All of the properties listed above facilitate a near-perfect recreation of the original scenes through 3D rendering (whereas it would be significantly more complicated to recreate natural outdoor scenes with perfect 1:1 geometry correspondence). We know the pose, location and surface of the objects as well as the location of the camera. We set the material properties based on the paint used for the physical toy models.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We used the HP 3D Structured Light Scanner Pro S3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to scan in the objects. The models were then uniformly scaled to fit into a unit cube with the bottom center at the origin. We then rendered a matching g-buffer for every image in the training dataset.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Mitsuba produced the G-Buffers and shaded images at a resolution of 768x768 for each image. In order to compare results based on image quality, we generated 128, 4, and 1 sample count images, which are later referred to as the high, medium, and low sample count images. The images and buffers were then cropped and resized such that the bounding box of the mesh pixels fit into an 80x80 pixel square, with the final image itself being a 96x96 pixel square. This mirrors the procedure done for the source NORB dataset. This creation procedure results in a one-to-one mapping between each image in the synthetic training dataset with the corresponding “real” image in the NORB training dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>ShapeNet Dataset</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1909.01960/assets/assets/piano.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1909.01960/assets/assets/mug.jpg" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1909.01960/assets/assets/car.jpg" id="S4.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1909.01960/assets/assets/chair.jpg" id="S4.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="72" height="72" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">A sample of the ShapeNet train dataset.</span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The ShapeNet dataset consists of tens of thousands of models. We use a subset of ShapeNet: 10 different models from 10 categories each. The dataset size was chosen to match similar popular, small datasets CFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and since we want to measure the relative performance of our domain adaptation techniques in the regime of limited training data (which more closely matches an intended use case). The categories we chose were airplanes, boats, cars, chairs, motorcycles, mugs, pianos, planters, tables, and trains. This provided a wide variety of models that could be identified from many different camera angles.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.6" class="ltx_p">After each model was changed to fit the above conditions, they were then rendered using the Mitsuba renderer. An infinitely large, matte ground plane was created below the objects, and two large, spherical area lights were created above the object to create shadows. In a specific experiment, detailed later, these lights were replaced by a single directional light to create harder shadows. For each model, we used 162 randomly sampled camera angles. The camera angles were chosen such that the elevation angles were between 30 and 70 degrees, and the azimuth could be any angle (between 0 and 2<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\pi</annotation></semantics></math> radians). These bounds and the number of samples were chosen to best match the NORB dataset. The randomization was chosen uniformly, such that if <math id="S4.SS2.p2.2.m2.6" class="ltx_Math" alttext="\zeta_{1}\in[\cos(70^{\circ}),\cos(30^{\circ})],\zeta_{2}\in[0,1]" display="inline"><semantics id="S4.SS2.p2.2.m2.6a"><mrow id="S4.SS2.p2.2.m2.6.6.2" xref="S4.SS2.p2.2.m2.6.6.3.cmml"><mrow id="S4.SS2.p2.2.m2.5.5.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.cmml"><msub id="S4.SS2.p2.2.m2.5.5.1.1.4" xref="S4.SS2.p2.2.m2.5.5.1.1.4.cmml"><mi id="S4.SS2.p2.2.m2.5.5.1.1.4.2" xref="S4.SS2.p2.2.m2.5.5.1.1.4.2.cmml">ζ</mi><mn id="S4.SS2.p2.2.m2.5.5.1.1.4.3" xref="S4.SS2.p2.2.m2.5.5.1.1.4.3.cmml">1</mn></msub><mo id="S4.SS2.p2.2.m2.5.5.1.1.3" xref="S4.SS2.p2.2.m2.5.5.1.1.3.cmml">∈</mo><mrow id="S4.SS2.p2.2.m2.5.5.1.1.2.2" xref="S4.SS2.p2.2.m2.5.5.1.1.2.3.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.3" xref="S4.SS2.p2.2.m2.5.5.1.1.2.3.cmml">[</mo><mrow id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">cos</mi><mo id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1a" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.2" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml">(</mo><msup id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.cmml"><mn id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.2" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.2.cmml">70</mn><mo id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.3" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.3.cmml">∘</mo></msup><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.3" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.2.m2.5.5.1.1.2.2.4" xref="S4.SS2.p2.2.m2.5.5.1.1.2.3.cmml">,</mo><mrow id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml"><mi id="S4.SS2.p2.2.m2.2.2" xref="S4.SS2.p2.2.m2.2.2.cmml">cos</mi><mo id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1a" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml">⁡</mo><mrow id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.2" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml">(</mo><msup id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.cmml"><mn id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.2" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.2.cmml">30</mn><mo id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.3" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.3.cmml">∘</mo></msup><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.3" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.5" xref="S4.SS2.p2.2.m2.5.5.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S4.SS2.p2.2.m2.6.6.2.3" xref="S4.SS2.p2.2.m2.6.6.3a.cmml">,</mo><mrow id="S4.SS2.p2.2.m2.6.6.2.2" xref="S4.SS2.p2.2.m2.6.6.2.2.cmml"><msub id="S4.SS2.p2.2.m2.6.6.2.2.2" xref="S4.SS2.p2.2.m2.6.6.2.2.2.cmml"><mi id="S4.SS2.p2.2.m2.6.6.2.2.2.2" xref="S4.SS2.p2.2.m2.6.6.2.2.2.2.cmml">ζ</mi><mn id="S4.SS2.p2.2.m2.6.6.2.2.2.3" xref="S4.SS2.p2.2.m2.6.6.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.2.m2.6.6.2.2.1" xref="S4.SS2.p2.2.m2.6.6.2.2.1.cmml">∈</mo><mrow id="S4.SS2.p2.2.m2.6.6.2.2.3.2" xref="S4.SS2.p2.2.m2.6.6.2.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.6.6.2.2.3.2.1" xref="S4.SS2.p2.2.m2.6.6.2.2.3.1.cmml">[</mo><mn id="S4.SS2.p2.2.m2.3.3" xref="S4.SS2.p2.2.m2.3.3.cmml">0</mn><mo id="S4.SS2.p2.2.m2.6.6.2.2.3.2.2" xref="S4.SS2.p2.2.m2.6.6.2.2.3.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.4.4" xref="S4.SS2.p2.2.m2.4.4.cmml">1</mn><mo stretchy="false" id="S4.SS2.p2.2.m2.6.6.2.2.3.2.3" xref="S4.SS2.p2.2.m2.6.6.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.6b"><apply id="S4.SS2.p2.2.m2.6.6.3.cmml" xref="S4.SS2.p2.2.m2.6.6.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.6.6.3a.cmml" xref="S4.SS2.p2.2.m2.6.6.2.3">formulae-sequence</csymbol><apply id="S4.SS2.p2.2.m2.5.5.1.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1"><in id="S4.SS2.p2.2.m2.5.5.1.1.3.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.3"></in><apply id="S4.SS2.p2.2.m2.5.5.1.1.4.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.5.5.1.1.4.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.4">subscript</csymbol><ci id="S4.SS2.p2.2.m2.5.5.1.1.4.2.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.4.2">𝜁</ci><cn type="integer" id="S4.SS2.p2.2.m2.5.5.1.1.4.3.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.4.3">1</cn></apply><interval closure="closed" id="S4.SS2.p2.2.m2.5.5.1.1.2.3.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2"><apply id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1"><cos id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"></cos><apply id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.2">70</cn><compose id="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.1.1.1.1.1.1.3"></compose></apply></apply><apply id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.2.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1"><cos id="S4.SS2.p2.2.m2.2.2.cmml" xref="S4.SS2.p2.2.m2.2.2"></cos><apply id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.2">30</cn><compose id="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.3.cmml" xref="S4.SS2.p2.2.m2.5.5.1.1.2.2.2.1.1.1.3"></compose></apply></apply></interval></apply><apply id="S4.SS2.p2.2.m2.6.6.2.2.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2"><in id="S4.SS2.p2.2.m2.6.6.2.2.1.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.1"></in><apply id="S4.SS2.p2.2.m2.6.6.2.2.2.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.6.6.2.2.2.1.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.2.m2.6.6.2.2.2.2.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.2.2">𝜁</ci><cn type="integer" id="S4.SS2.p2.2.m2.6.6.2.2.2.3.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.2.3">2</cn></apply><interval closure="closed" id="S4.SS2.p2.2.m2.6.6.2.2.3.1.cmml" xref="S4.SS2.p2.2.m2.6.6.2.2.3.2"><cn type="integer" id="S4.SS2.p2.2.m2.3.3.cmml" xref="S4.SS2.p2.2.m2.3.3">0</cn><cn type="integer" id="S4.SS2.p2.2.m2.4.4.cmml" xref="S4.SS2.p2.2.m2.4.4">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.6c">\zeta_{1}\in[\cos(70^{\circ}),\cos(30^{\circ})],\zeta_{2}\in[0,1]</annotation></semantics></math> are uniformly distributed random numbers, then the elevation angle <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\theta</annotation></semantics></math> and azimuth angle <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\phi</annotation></semantics></math> are <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="\theta=\cos^{-1}\zeta_{1}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">θ</mi><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml"><msup id="S4.SS2.p2.5.m5.1.1.3.1" xref="S4.SS2.p2.5.m5.1.1.3.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.1.2" xref="S4.SS2.p2.5.m5.1.1.3.1.2.cmml">cos</mi><mrow id="S4.SS2.p2.5.m5.1.1.3.1.3" xref="S4.SS2.p2.5.m5.1.1.3.1.3.cmml"><mo id="S4.SS2.p2.5.m5.1.1.3.1.3a" xref="S4.SS2.p2.5.m5.1.1.3.1.3.cmml">−</mo><mn id="S4.SS2.p2.5.m5.1.1.3.1.3.2" xref="S4.SS2.p2.5.m5.1.1.3.1.3.2.cmml">1</mn></mrow></msup><mo lspace="0.167em" id="S4.SS2.p2.5.m5.1.1.3a" xref="S4.SS2.p2.5.m5.1.1.3.cmml">⁡</mo><msub id="S4.SS2.p2.5.m5.1.1.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.2.2" xref="S4.SS2.p2.5.m5.1.1.3.2.2.cmml">ζ</mi><mn id="S4.SS2.p2.5.m5.1.1.3.2.3" xref="S4.SS2.p2.5.m5.1.1.3.2.3.cmml">1</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><eq id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></eq><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">𝜃</ci><apply id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3"><apply id="S4.SS2.p2.5.m5.1.1.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1">superscript</csymbol><cos id="S4.SS2.p2.5.m5.1.1.3.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.2"></cos><apply id="S4.SS2.p2.5.m5.1.1.3.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.3"><minus id="S4.SS2.p2.5.m5.1.1.3.1.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.3"></minus><cn type="integer" id="S4.SS2.p2.5.m5.1.1.3.1.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1.3.2">1</cn></apply></apply><apply id="S4.SS2.p2.5.m5.1.1.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.3.2.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.3.2.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.2">𝜁</ci><cn type="integer" id="S4.SS2.p2.5.m5.1.1.3.2.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\theta=\cos^{-1}\zeta_{1}</annotation></semantics></math>, <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="\phi=2\pi\zeta_{2}" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">ϕ</mi><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml"><mn id="S4.SS2.p2.6.m6.1.1.3.2" xref="S4.SS2.p2.6.m6.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m6.1.1.3.1" xref="S4.SS2.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.6.m6.1.1.3.3" xref="S4.SS2.p2.6.m6.1.1.3.3.cmml">π</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m6.1.1.3.1a" xref="S4.SS2.p2.6.m6.1.1.3.1.cmml">​</mo><msub id="S4.SS2.p2.6.m6.1.1.3.4" xref="S4.SS2.p2.6.m6.1.1.3.4.cmml"><mi id="S4.SS2.p2.6.m6.1.1.3.4.2" xref="S4.SS2.p2.6.m6.1.1.3.4.2.cmml">ζ</mi><mn id="S4.SS2.p2.6.m6.1.1.3.4.3" xref="S4.SS2.p2.6.m6.1.1.3.4.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><eq id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></eq><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">italic-ϕ</ci><apply id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3"><times id="S4.SS2.p2.6.m6.1.1.3.1.cmml" xref="S4.SS2.p2.6.m6.1.1.3.1"></times><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.2.cmml" xref="S4.SS2.p2.6.m6.1.1.3.2">2</cn><ci id="S4.SS2.p2.6.m6.1.1.3.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3.3">𝜋</ci><apply id="S4.SS2.p2.6.m6.1.1.3.4.cmml" xref="S4.SS2.p2.6.m6.1.1.3.4"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.1.1.3.4.1.cmml" xref="S4.SS2.p2.6.m6.1.1.3.4">subscript</csymbol><ci id="S4.SS2.p2.6.m6.1.1.3.4.2.cmml" xref="S4.SS2.p2.6.m6.1.1.3.4.2">𝜁</ci><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.4.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3.4.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">\phi=2\pi\zeta_{2}</annotation></semantics></math>. This follows the formula given by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The images were rendered at 96x96, outputting the G-buffer of each image, including the image albedo, normals, occlusion, distance field, and GI image data. For this GI rendered dataset, we generated 1024 (high), 32 (medium), and 10 (low) sample count images.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the following experiments, we use increasingly complex simulation methods for rendering the images. We start with just a silhouette as training data and we then incrementally increase the sophistication level of the rendering. We then add complexity such as shading, shadowing and global illumination. Finally, we use a form of a denoising autoencoder to learn shading, cleanup and refine GI based images.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In order to principally measure the impact of these various techniques, we use a single classifier architecture across all experiments; a simplification of the VGG network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Given the small size of the datasets, we reduce the standard VGG network to 8 learnable layers, including 5 convolutional and 3 fully-connected layers along with a cross-entropy loss (negative log likelihood). See Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for an overview of this architecture.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1909.01960/assets/x1.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Architecture of VGG08 Batch Norm Classifier</span></figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The first experiment done for a dataset is to train a classifier with data that is from the same domain as the test set. In NORB, this is using the real photographs for training and in ShapeNet, it is the highest sample count Mitsuba rendered images. </p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">This establishes a target classifier accuracy to be used for measuring the accuracy from using the training data created by the various rendering methods below. Next, we conduct multiple experiments by varying rendering techniques and measuring their quantitative impact on classifier performance.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Firstly, we use renders that use <span id="S5.p5.1.1" class="ltx_text ltx_font_italic">no shading</span> by taking the albedo maps of each image generated in the g-buffer and using this to train the dataset. We then test this against the baseline generated in the first experiment and compare the results. The output images can be seen in figure <a href="#S3.F1" title="Figure 1 ‣ 3.1.1 G-Buffer ‣ 3.1 Rendering Methods and Techniques ‣ 3 Related Work ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/1909.01960/assets/assets/SH_model.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Spherical Harmonics network architecture.</span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/1909.01960/assets/assets/sh-comp-onerow.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Left: Learned SH. Middle: SH x Albedo. Right: SH x Albedo x Ambient Occlusion.</span></figcaption>
</figure>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">The next step beyond albedo-only rendering is shading the models in the image, but not rendering shadows. In order to do this, we learn the shading using the learnable spherical harmonics module (the advantage of learning the diffuse shader is that we can more closely approximate the statistics of the “real” image domain), as seen in figure <a href="#S5.F6" title="Figure 6 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We then take these simple shaded renderings and composite them with ambient occlusion (AO) present in the g-buffer as an approximation of lighting condition independent shadowing. An example output image can be seen in figure <a href="#S5.F7" title="Figure 7 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. As this is the first experiment that uses shadows, this experiment determines whether or not shadows are important in image classification.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/1909.01960/assets/assets/2bounce_small.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="76" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Two Bounce Rendering. Note that it is the equivalent to a shadow mapped scene and differs from the AO composited scenes by the sharpness of the shadows. This is due to the scene using a single directional light.</span></figcaption>
</figure>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">While AO makes a good, fast approximation of shadows, they are not very realistic. Therefore, we next test more realistic shadows. We wish to measure the impact of inter-reflections and global illumination in a later experiment, and so we limit the renderer to do 2-bounce raytracing with a directional light. This allows us to create very hard shadows, as seen in figure <a href="#S5.F8" title="Figure 8 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/1909.01960/assets/assets/mitsuba-onerow.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.3.2" class="ltx_text" style="font-size:90%;">
Mitsuba renders at different sample counts. Left: 1 sample per pixel. Middle: 4. Right: 128.</span></figcaption>
</figure>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">Finally, we use a full (GI) renderer to simulate a realistic scene. Using Mitsuba, this is the closest we can get to the baseline data. As noted earlier, Mitsuba is a Monte-Carlo raytrace-based renderer, and therefore allowing for more samples-per-pixel will produce a more realistic image. However, as a drawback, higher sample counts take linearly more time to render (on the order of seconds for low sample count data and minutes for the high sample count data). We test on three different sample counts for each of the datasets to determine how much is gained from using higher values. The difference in sample counts can be seen in figure <a href="#S5.F9" title="Figure 9 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/1909.01960/assets/x2.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S5.F10.3.2" class="ltx_text" style="font-size:90%;">Architecture of Denoising Model</span></figcaption>
</figure>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">Given that the low sample GI renders take far less time than the high sample renders, we next experiment to see if the lower sample count images can be denoised using a simplified version of the denoising auto-encoder with skip connections inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The network architecture is shown in <a href="#S5.F10" title="Figure 10 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. For this network, we use the Structured Similarity (SSIM) loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, a loss function that has proven to be useful for both image-compression measurement and also image comparison.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.1" class="ltx_p">This denoising process takes milliseconds on the modern GPU, which is a far smaller cost than rendering the high-sample GI renders.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Train on GAN Images</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Another synthetic data generation technique that we explored involved the use of Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We use a GAN architecture inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This work differs from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> by conditioning on a g-buffer and rendered images with and without the presence of sampling noise. Furthermore, this work doesn’t require masking the input images or using local loss functions. This is due to an interesting property of the rendering architecture whereby conditioning the generative model on a static g-buffer, and by ensuring that the shading function is dependent on geometry normals (which are not modified by the generative model and is physically motivated by the rendering equation)
prevents the need for a local loss function and eliminates the problem of a GAN altering the semantic content (i.e. labels) of the objects being rendered.
The GAN architecture we employed was the following:
The generative network was the same model used for denoising shown in figure <a href="#S5.F10" title="Figure 10 ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. The inputs were normals, depth and either the low, medium and high sample global illumination rendered image. Albedo was used for only ShapeNet data based experiments. The regularization loss was SSIM.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Pre-training the generative network against the high sample Mitsuba rendering before starting adversarial training produced a partially differentiable renderer that learned to denoise and shade the scene conditioned on the g-buffer and image input. Unlike in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the conditional data distribution in this work is visually very similar to the target generated distribution. As such, we propose a more principled way to validate the performance of the GAN during training (the proposed method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> was to “visually inspect” the GAN output and early-stop training when the output visually resembled the target dataset). To choose a GAN to generate training data we train a classifier on real training and validation data. No test data is used in testing or training this classifier. We will call this a clean classifier. For every saved GAN model, we run the synthetic training data through it and establish how well the clean classifier can classify the refined images. We use the classification accuracy to rank each GAN. We then take the top 10 GAN refiner models that were ranked from the clean classifier and for each one train a classifier on the full dataset. For an ensemble of GAN models (described below) we take the set of top 10 ranked refiners from the clean classifier and train a classifier with a subset of the refiners synthesizing a new expanded dataset.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.4" class="ltx_p">The expanded dataset is created by the following method. Firstly, we create a new empty training set. For each generative model in the set of refiners we refine each image in the original full training set and append it to the new training set. The new dataset is comprised of <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="D*N" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">∗</mo><mi id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><times id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></times><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝐷</ci><ci id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">D*N</annotation></semantics></math> images where <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">N</annotation></semantics></math> is the number of refiners and <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">D</annotation></semantics></math> is the dataset size.
This effectively generates <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mi id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><ci id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">N</annotation></semantics></math> copies of the training data where each copy is unique and without the computational overhead of a full render to synthesize the images.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Taking the top 10 models ranked by this method, some models outperform just using a raw low sample image or the result of denoising it. This method of choosing a GAN allows for selecting a model that outputs non-photorealistic images and represents a principled way to choose models independent from the realism of their generated input. When evaluating performance of using a single GAN, we took each saved GAN and trained a classifier. We repeated this three times and then kept the top performing model. For every sample count we found a GAN that could outperform directly training on the original image or denoising it.</p>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/1909.01960/assets/assets/4sampleGAN-onerow.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S5.F11.3.2" class="ltx_text" style="font-size:90%;">Right: 4 sample Mitsuba image passed into GAN, Middle: Output of refiner. Left: Absolute value difference between the full sample rendered image and the refined image. Notice that the image is no longer photo-realistic.</span></figcaption>
</figure>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Training Using an Ensemble of GANs</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">The next experiment is a little different from the rest, as more training data is used and is therefore not quite comparable. However, the amount of training data that would have to be generated in a GI renderer remains the same. The major change is the use of the top ten ranked ensemble of GAN models to create multiple refined version of the data.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">The new dataset is an aggregation of the different refined datasets. The costly pre-rendering of the input buffer and images is not changed and the run-time rendering refinement is longer but insignificant compared to GI rendering time. In our experiments, it took a minute longer to generate 10 times as much data by running the dataset through ten GANS on an NVIDIA Titan X Maxwell GPU. The added time for using this method is negligible except for extra time required to train on a larger dataset. This ensemble of GANs is another method in this work that distinguishes it from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We now examine the results from our previous experiments. As the patterns in results between the NORB and ShapeNet datasets were very similar, we will first detail the NORB results, and then use the ShapeNet results to validate these findings on RGB.</p>
</div>
<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T1.2.1.1" class="ltx_tr">
<th id="S6.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T1.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></th>
<td id="S6.T1.2.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S6.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S6.T1.2.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S6.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">Baseline %</span></td>
</tr>
<tr id="S6.T1.2.2.2" class="ltx_tr">
<th id="S6.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Albedo Only</th>
<td id="S6.T1.2.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">53.26%</td>
<td id="S6.T1.2.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">56.06%</td>
</tr>
<tr id="S6.T1.2.3.3" class="ltx_tr">
<th id="S6.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Learned SH Shading</th>
<td id="S6.T1.2.3.3.2" class="ltx_td ltx_align_right ltx_border_r">53.06%</td>
<td id="S6.T1.2.3.3.3" class="ltx_td ltx_align_right ltx_border_r">55.85%</td>
</tr>
<tr id="S6.T1.2.4.4" class="ltx_tr">
<th id="S6.T1.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">SH Shading + AO</th>
<td id="S6.T1.2.4.4.2" class="ltx_td ltx_align_right ltx_border_r">60.89%</td>
<td id="S6.T1.2.4.4.3" class="ltx_td ltx_align_right ltx_border_r">64.09%</td>
</tr>
<tr id="S6.T1.2.5.5" class="ltx_tr">
<th id="S6.T1.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">2 Bounce (Direct Lighting)</th>
<td id="S6.T1.2.5.5.2" class="ltx_td ltx_align_right ltx_border_r">66.49%</td>
<td id="S6.T1.2.5.5.3" class="ltx_td ltx_align_right ltx_border_r">69.98%</td>
</tr>
<tr id="S6.T1.2.6.6" class="ltx_tr">
<th id="S6.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T1.2.6.6.1.1" class="ltx_text ltx_font_bold">Real Images</span></th>
<td id="S6.T1.2.6.6.2" class="ltx_td ltx_align_right ltx_border_r">95.01%</td>
<td id="S6.T1.2.6.6.3" class="ltx_td ltx_align_right ltx_border_r">100.00%</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S6.T1.2.7.1" class="ltx_tr">
<th id="S6.T1.2.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S6.T1.2.7.1.1.1" class="ltx_text ltx_font_bold">NORB Performance of Basic Rendering Methods</span></th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S6.T1.4.2" class="ltx_text" style="font-size:90%;">Performance of real-time (rasterization-based) rendering methods on the NORB dataset. Accuracy is the classification accuracy of the method, and Baseline % is how good the method was compared to the baseline render.</span></figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>NORB Results</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The results for the NORB dataset are shown in table <a href="#S6.T1" title="Table 1 ‣ 6 Results ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Albedo Only (unshaded renders) yielded only 53.26% accuracy. This tells us that while the shape is helpful in classifying an object (compare 53.26% to 20% from randomly guessing), it is not nearly enough information to be consistently accurate. This is also the case for the learned SH shading, which scored slightly under the unshaded renders at 53.06%. We can see that in this case, the fake shading was no more helpful than no shading at all.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">We observe slight improvements once shadows are added. AO, a first approximation to shadows, scored at 60.89% accuracy, a significant jump from renders without shadows. Then, once we improved the quality of shadows with the 2-bounce rendering method, we saw another significant jump to 66.89% accuracy. However, this is still far less than the target baseline.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.2.1.1" class="ltx_tr">
<th id="S6.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></th>
<td id="S6.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">Low</span></td>
<td id="S6.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">Medium</span></td>
<td id="S6.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.4.1" class="ltx_text ltx_font_bold">High</span></td>
<td id="S6.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.2.1.1.5.1" class="ltx_text ltx_font_bold">Baseline</span></td>
</tr>
<tr id="S6.T2.2.2.2" class="ltx_tr">
<th id="S6.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Mitsuba</th>
<td id="S6.T2.2.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">64.44%</td>
<td id="S6.T2.2.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">73.13%</td>
<td id="S6.T2.2.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">74.74%</td>
<td id="S6.T2.2.2.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78.67%</td>
</tr>
<tr id="S6.T2.2.3.3" class="ltx_tr">
<th id="S6.T2.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Denoised</th>
<td id="S6.T2.2.3.3.2" class="ltx_td ltx_align_right ltx_border_r">71.48%</td>
<td id="S6.T2.2.3.3.3" class="ltx_td ltx_align_right ltx_border_r">73.56%</td>
<td id="S6.T2.2.3.3.4" class="ltx_td ltx_align_right ltx_border_r">N/A</td>
<td id="S6.T2.2.3.3.5" class="ltx_td ltx_align_right ltx_border_r">77.42%</td>
</tr>
<tr id="S6.T2.2.4.4" class="ltx_tr">
<th id="S6.T2.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">GAN</th>
<td id="S6.T2.2.4.4.2" class="ltx_td ltx_align_right ltx_border_r">72.84%</td>
<td id="S6.T2.2.4.4.3" class="ltx_td ltx_align_right ltx_border_r">77.65%</td>
<td id="S6.T2.2.4.4.4" class="ltx_td ltx_align_right ltx_border_r">75.26%</td>
<td id="S6.T2.2.4.4.5" class="ltx_td ltx_align_right ltx_border_r">81.73%</td>
</tr>
<tr id="S6.T2.2.5.5" class="ltx_tr">
<th id="S6.T2.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Ensemble</th>
<td id="S6.T2.2.5.5.2" class="ltx_td ltx_align_right ltx_border_r">85.23%</td>
<td id="S6.T2.2.5.5.3" class="ltx_td ltx_align_right ltx_border_r">84.37%</td>
<td id="S6.T2.2.5.5.4" class="ltx_td ltx_align_right ltx_border_r">87.33%</td>
<td id="S6.T2.2.5.5.5" class="ltx_td ltx_align_right ltx_border_r">91.92%</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S6.T2.2.6.1" class="ltx_tr">
<th id="S6.T2.2.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S6.T2.2.6.1.1.1" class="ltx_text ltx_font_bold">Performance of GI Rendered Refined Images</span></th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of GI Rendered Refined Image Performance. The GAN output refers to if only one GAN was used to refine the dataset or if the dataset was expanded by multiple GANs.</span></figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">We next observe the results of the more advanced techniques, such as using GI rendering at a high sample count, denoising a low sample count GI rendering, and learning to render using GANs. These results can be observed in table <a href="#S6.T2" title="Table 2 ‣ 6.1 NORB Results ‣ 6 Results ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. While noisy (low sample) GI renders had poor performance, ranking close to the 2-bounce condition we observed earlier, the highest sample GI render achieved 74.74% accuracy, almost 10% higher. However, the medium sample count renders performed almost as well as the high sample count renders, demonstrating that beyond a certain quality threshold, the increase in accuracy becomes marginal.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">We can further close the gap between the high sample count renders and the lower sample count renders with the denoising auto-encoder, which brought the low sample count images to 71.48% and the medium sample count images to 73.56% accuracy, less than 1% difference to the high sample count render. We can therefore conclude that images could be rendered much faster at low samples, processed with a denoising auto-encoder, and achieve very comparable classification accuracies with a much lower render time.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">Finally, we can see that the images generated using the GANs detailed above had a better performance than any other result. With a single GAN, we achieve 75.26% classification accuracy, which is only a slight increase over the best GI render results. However, with an ensemble of GANs, we were able to achieve an accuracy of 87.33%, over 90% of what one can achieve training with real photographic data. <span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_bold">A surprising detail concerning the GAN-based images is that the best performing generated images were not necessarily visually realistic as compared to the test dataset.</span> This result demonstrations that conventional measures of photorealism does not necessarily equate to improved domain adaptation performance via a Convolutional Network classifier. See Figure <a href="#S5.F11" title="Figure 11 ‣ 5.1 Train on GAN Images ‣ 5 Experiments ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> where the refined images have exaggerated edges, shadows and reflective properties.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>ShapeNet Results</h3>

<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.2.1.1" class="ltx_tr">
<th id="S6.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></th>
<td id="S6.T3.2.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S6.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S6.T3.2.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S6.T3.2.1.1.3.1" class="ltx_text ltx_font_bold">Baseline %</span></td>
</tr>
<tr id="S6.T3.2.2.2" class="ltx_tr">
<th id="S6.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Albedo Only</th>
<td id="S6.T3.2.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">44.44%</td>
<td id="S6.T3.2.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">52.91%</td>
</tr>
<tr id="S6.T3.2.3.3" class="ltx_tr">
<th id="S6.T3.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Learned SH Shading</th>
<td id="S6.T3.2.3.3.2" class="ltx_td ltx_align_right ltx_border_r">47.80%</td>
<td id="S6.T3.2.3.3.3" class="ltx_td ltx_align_right ltx_border_r">56.91%</td>
</tr>
<tr id="S6.T3.2.4.4" class="ltx_tr">
<th id="S6.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">2 Bounce (Direct Lighting)</th>
<td id="S6.T3.2.4.4.2" class="ltx_td ltx_align_right ltx_border_r">70.86%</td>
<td id="S6.T3.2.4.4.3" class="ltx_td ltx_align_right ltx_border_r">84.37%</td>
</tr>
<tr id="S6.T3.2.5.5" class="ltx_tr">
<th id="S6.T3.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S6.T3.2.5.5.1.1" class="ltx_text ltx_font_bold">1024 Sample Renderings</span></th>
<td id="S6.T3.2.5.5.2" class="ltx_td ltx_align_right ltx_border_r">83.99%</td>
<td id="S6.T3.2.5.5.3" class="ltx_td ltx_align_right ltx_border_r">100.00%</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S6.T3.2.6.1" class="ltx_tr">
<th id="S6.T3.2.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="S6.T3.2.6.1.1.1" class="ltx_text ltx_font_bold">ShapeNet Performance of Basic Rendering Methods</span></th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S6.T3.4.2" class="ltx_text" style="font-size:90%;">Performance of real-time (rasterization-based) rendering methods on the ShapeNet dataset.</span></figcaption>
</figure>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.1.1" class="ltx_tr">
<th id="S6.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></th>
<td id="S6.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.2.1.1.2.1" class="ltx_text ltx_font_bold">Low</span></td>
<td id="S6.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.2.1.1.3.1" class="ltx_text ltx_font_bold">Medium</span></td>
<td id="S6.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.2.1.1.4.1" class="ltx_text ltx_font_bold">Baseline %</span></td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Mitsuba</th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">81.68%</td>
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">82.30%</td>
<td id="S6.T4.2.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">97.99%</td>
</tr>
<tr id="S6.T4.2.3.3" class="ltx_tr">
<th id="S6.T4.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Denoised</th>
<td id="S6.T4.2.3.3.2" class="ltx_td ltx_align_right ltx_border_r">78.06%</td>
<td id="S6.T4.2.3.3.3" class="ltx_td ltx_align_right ltx_border_r">79.12%</td>
<td id="S6.T4.2.3.3.4" class="ltx_td ltx_align_right ltx_border_r">96.25%</td>
</tr>
<tr id="S6.T4.2.4.4" class="ltx_tr">
<th id="S6.T4.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">GAN</th>
<td id="S6.T4.2.4.4.2" class="ltx_td ltx_align_right ltx_border_r">83.48%</td>
<td id="S6.T4.2.4.4.3" class="ltx_td ltx_align_right ltx_border_r">83.59%</td>
<td id="S6.T4.2.4.4.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T4.2.4.4.4.1" class="ltx_text ltx_font_bold">99.52%</span></td>
</tr>
<tr id="S6.T4.2.5.5" class="ltx_tr">
<th id="S6.T4.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Ensemble</th>
<td id="S6.T4.2.5.5.2" class="ltx_td ltx_align_right ltx_border_r">86.48%</td>
<td id="S6.T4.2.5.5.3" class="ltx_td ltx_align_right ltx_border_r">86.65%</td>
<td id="S6.T4.2.5.5.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T4.2.5.5.4.1" class="ltx_text ltx_font_bold">103.17%</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S6.T4.2.6.1" class="ltx_tr">
<th id="S6.T4.2.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S6.T4.2.6.1.1.1" class="ltx_text ltx_font_bold">Performance of GI Rendered Refined Images</span></th>
</tr>
</tfoot>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.4.2" class="ltx_text" style="font-size:90%;">Scores using the ShapeNet dataset. Note that the Multiple GAN method performs better than the highest sample render.
</span></figcaption>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Our ShapeNet experiments resulted in similar performance characteristics as on the greyscale NORB dataset, as shown in table <a href="#S6.T3" title="Table 3 ‣ 6.2 ShapeNet Results ‣ 6 Results ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We can see that with the unshaded or learned SH shading models the classification accuracies are very poor, approximately half of the baseline. Once we move to 2-bounce lighting with shadows, this becomes much closer. We can account for the relatively larger jump in performance when adding shadows compared to NORB because the lighting conditions between the 2-bounce render for ShapeNet were much more similar to the target renders than in the case with the NORB 2-bounce renders and their target.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Similarly, due to the nearly identical lighting conditions, we can see in table <a href="#S6.T4" title="Table 4 ‣ 6.2 ShapeNet Results ‣ 6 Results ‣ Beyond Photo Realism for Domain Adaptation from Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the lower sample renders performed nearly as well as the 1024 sample renders that were used as the baseline. Interestingly, the denoiser reduced classification accuracy. This suggests that the denoiser may have modified image statistics that were present in all sample count renders.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">However, where we see the best performance is when we render with GANs, much like NORB. With a single GAN, we achieve 83.59% classification accuracy, or 99.52% of the baseline. When we used an ensemble of GANs, we achieve 86.65% accuracy, which is even more accurate than the baseline. This means that the GAN-rendered models are as good or better for image classification than the high sample GI renders and are an effective regularizer for classifier training.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">By constructing a novel dataset consisting of synthetic and real images with extremely high levels of geometry correspondence, we are able to directly compare various image synthesis techniques and their impact on domain-adaptation. We do so by measuring the domain-transfer classification accuracy of a multi-class image classifier as a proxy measure for “image-realism” as perceived by a Convolutional Network classifier architecture. We have shown that diffuse shading alone gives similar performance to a simple silhouette image, and that shadows are important for domain transfer. Most importantly, we proposed two methods to approximate the effectiveness of offline global-illumination rendering, and whose domain adaptation performance actually exceeds it through the use of a learned shading function. The two learned methods optimized for image statistics that are not captured in standard rendering methods which do not take into account the discriminative capacity of convolutional networks.
Furthermore, we have also shown that by using an ensemble of GAN models you can outperform GI simulation and approach the performance of using real data.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Steve Bako, Thijs Vogels, Brian McWilliams, Mark Meyer, Jan Novák, Alex
Harvill, Pradeep Sen, Tony DeRose, and Fabrice Rousselle.

</span>
<span class="ltx_bibblock">Kernel-predicting convolutional networks for denoising monte carlo
renderings.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG) (Proceedings of SIGGRAPH
2017)</span>, 36(4), July 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip
Krishnan.

</span>
<span class="ltx_bibblock">Unsupervised pixel-level domain adaptation with generative
adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1612.05424, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, Christoph Schied, Marco
Salvi, Aaron Lefohn, Derek Nowrouzezahrai, and Timo Aila.

</span>
<span class="ltx_bibblock">Interactive reconstruction of monte carlo image sequences using a
recurrent denoising autoencoder.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ACM Trans. Graph.</span>, 36(4):98:1–98:12, July 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan,
Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao
Su, Jianxiong Xiao, Li Yi, and Fisher Yu.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1512.03012, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on</span>, pages 248–255. IEEE, 2009.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Ganin and V. Lempitsky.

</span>
<span class="ltx_bibblock">Unsupervised Domain Adaptation by Backpropagation.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ArXiv e-prints</span>, September 2014.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock">In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
27</span>, pages 2672–2680. Curran Associates, Inc., 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
HP.

</span>
<span class="ltx_bibblock">Hp 3d scan: Hp official website.

</span>
<span class="ltx_bibblock">www8.hp.com/us/en/campaign/3Dscanner/overview.html, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Wenzel Jakob.

</span>
<span class="ltx_bibblock">Mitsuba renderer, 2010.

</span>
<span class="ltx_bibblock">http://www.mitsuba-renderer.org.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Nima Khademi Kalantari, Steve Bako, and Pradeep Sen.

</span>
<span class="ltx_bibblock">A machine learning approach for filtering monte carlo noise.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ACM Trans. Graph.</span>, 34(4):122:1–122:12, July 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Alex Krizhevsky.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">Technical report, 2009.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 86(11):2278–2324, Nov 1998.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yann LeCun, Fu Jie Huang, and Léon Bottou.

</span>
<span class="ltx_bibblock">Learning methods for generic object recognition with invariance to
pose and lighting.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2004 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition</span>, CVPR’04, pages 97–104, Washington,
DC, USA, 2004. IEEE Computer Society.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B.
Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: common objects in context.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1405.0312, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gavin Miller.

</span>
<span class="ltx_bibblock">Efficient algorithms for local and global accessibility shading.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the 21st Annual Conference on Computer
Graphics and Interactive Techniques</span>, SIGGRAPH ’94, pages 319–326, New York,
NY, USA, 1994. ACM.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Oliver Nalbach, Elena Arabadzhiyska, Dushyant Mehta, Hans-Peter Seidel, and
Tobias Ritschel.

</span>
<span class="ltx_bibblock">Deep shading: Convolutional neural networks for screen-space shading.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1603.06078, 2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Matt Pharr and Greg Humphreys.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Physically Based Rendering, Second Edition: From Theory To
Implementation</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2nd edition,
2010.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ravi Ramamoorthi and Pat Hanrahan.

</span>
<span class="ltx_bibblock">An efficient representation for irradiance environment maps.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th Annual Conference on Computer
Graphics and Interactive Techniques</span>, SIGGRAPH ’01, pages 497–500, New York,
NY, USA, 2001. ACM.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1608.02192, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Takafumi Saito and Tokiichiro Takahashi.

</span>
<span class="ltx_bibblock">Comprehensible rendering of 3-d shapes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 17th Annual Conference on Computer
Graphics and Interactive Techniques</span>, SIGGRAPH ’90, pages 197–206, New York,
NY, USA, 1990. ACM.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Christoph Schied, Anton Kaplanyan, Chris Wyman, Anjul Patney, Chakravarty
R. Alla Chaitanya, John Burgess, Shiqiu Liu, Carsten Dachsbacher, Aaron
Lefohn, and Marco Salvi.

</span>
<span class="ltx_bibblock">Spatiotemporal variance-guided filtering: Real-time reconstruction
for path-traced global illumination.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of High Performance Graphics</span>, HPG ’17, pages
2:1–2:12, New York, NY, USA, 2017. ACM.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Alireza Shafaei, James J. Little, and Mark Schmidt.

</span>
<span class="ltx_bibblock">Play and learn: Using video games to train computer vision models.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1608.01745, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Dave Shreiner, Graham Sellers, John M. Kessenich, and Bill M. Licea-Kane.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">OpenGL Programming Guide: The Official Guide to Learning OpenGL,
Version 4.3</span>.

</span>
<span class="ltx_bibblock">Addison-Wesley Professional, 8th edition, 2013.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and
Russ Webb.

</span>
<span class="ltx_bibblock">Learning from simulated and unsupervised images through adversarial
training.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1612.07828, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1409.1556, 2014.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Leon Sixt, Benjamin Wild, and Tim Landgraf.

</span>
<span class="ltx_bibblock">Rendergan: Generating realistic labeled data.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1611.01331, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli.

</span>
<span class="ltx_bibblock">Image quality assessment: From error visibility to structural
similarity.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE TRANSACTIONS ON IMAGE PROCESSING</span>, 13(4):600–612, 2004.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin
Jin, and Thomas A. Funkhouser.

</span>
<span class="ltx_bibblock">Physically-based rendering for indoor scene understanding using
convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1612.07429, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1909.01959" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1909.01960" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1909.01960">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1909.01960" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1909.01961" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 18:21:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
