<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</title>
<!--Generated on Fri Sep 27 13:37:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.19603v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S1" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S2" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S2.SS1" title="In 2 Related Work ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Video Object Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S2.SS2" title="In 2 Related Work ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multimodal Large Language Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS1" title="In 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS2" title="In 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Sparse Dense Sampling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS3" title="In 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>One Token Seg All</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS4" title="In 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Training and Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S4" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS1" title="In 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS2" title="In 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation on Video Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS2.SSS1" title="In 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Referring Video Object Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS2.SSS2" title="In 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Motion-guided Video Object Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS2.SSS3" title="In 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.3 </span>Reasoning Video Object Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS3" title="In 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation on Image Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.SS4" title="In 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S6" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitation and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S7" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS1" title="In Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Evaluation on Image Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS2" title="In Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS2.SSS1" title="In A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Temporal Learning Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS2.SSS2" title="In A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Temporal Association Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS2.SSS3" title="In A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.3 </span>Ablation on Training Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS3" title="In Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Qualitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.SS4" title="In Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Failure Cases</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A2" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A3" title="In One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Broader Impact</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zechen Bai<sup class="ltx_sup" id="id12.12.id1"><span class="ltx_text ltx_font_italic" id="id12.12.id1.1">1</span></sup>  Tong He<sup class="ltx_sup" id="id13.13.id2"><span class="ltx_text ltx_font_italic" id="id13.13.id2.1">2</span></sup>  Haiyang Mei<sup class="ltx_sup" id="id14.14.id3"><span class="ltx_text ltx_font_italic" id="id14.14.id3.1">1</span></sup>  Pichao Wang<sup class="ltx_sup" id="id15.15.id4"><span class="ltx_text ltx_font_italic" id="id15.15.id4.1">2</span></sup>  Ziteng Gao<sup class="ltx_sup" id="id16.16.id5"><span class="ltx_text ltx_font_italic" id="id16.16.id5.1">1</span></sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id9.9.4">
Joya Chen<sup class="ltx_sup" id="id9.9.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.1.1">1</span></sup> Lei Liu<sup class="ltx_sup" id="id9.9.4.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.2.1">2</span></sup> Zheng Zhang<sup class="ltx_sup" id="id9.9.4.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.3.1">2</span></sup> Mike Zheng Shou<sup class="ltx_sup" id="id9.9.4.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.9.4.4.1">1</span></sup>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id6">1</sup>Show Lab, National University of Singapore  <sup class="ltx_sup" id="id18.18.id7">2</sup>Amazon 
<br class="ltx_break"/>
</span><span class="ltx_author_notes"><span class="ltx_text ltx_font_bold" id="id19.19.id1">Corresponding Author</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos.
Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions.
Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames.
VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints.
Additionally, we propose a One-Token-Seg-All approach using a specially designed <span class="ltx_text ltx_font_typewriter" id="id20.id1.1">&lt;TRK&gt;</span> token, enabling the model to segment and track objects across multiple frames.
Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA’s superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking.
While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation.
Code and model will be available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/showlab/VideoLISA" style="color:#BF0040;" title="">https://github.com/showlab/VideoLISA</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We live in a dynamic world.
Localizing objects of interest in videos according to human intent is a crucial task for intelligent models and systems.
Language, as a natural interface, serves as the primary reference for identifying target objects.
However, language expressions vary widely across different scenarios, presenting varying levels of difficulty.
While category names are straightforward references, detailed text descriptions from tasks like referring segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite> introduce greater complexity.
In real-world applications, these expressions can be more complex, involving intent understanding, reasoning, and world knowledge, making them more user-friendly yet significantly more challenging for models to understand and act upon.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent advancements in the image domain have shown progress in language-instructed reasoning for detection and segmentation tasks.
Models leveraging multimodal large language models (MLLMs), such as those in DetGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib51" title="">51</a>]</cite> and LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>, have demonstrated the ability to localize target objects by harnessing the implicit reasoning capabilities and world knowledge embedded in large language models (LLMs).
However, these advancements have not seamlessly translated to video tasks, particularly video object segmentation (VOS).
The primary challenge in VOS stems from the additional temporal dimension, which introduces complexities absent in static images.
VOS requires models to 1) on the input side, capture and comprehend the temporal dynamics present in the video; and 2) on the output side, predict temporally consistent segmentation masks across frames.
These challenges render existing image-based methods inadequate for handling video tasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we introduce VideoLISA, a video-based MLLM designed to address language-instructed reasoning segmentation in videos.
Our goal is to segment target objects throughout the entire video based on diverse language queries that necessitate scene understanding, temporal comprehension, and implicit reasoning.
Drawing inspiration from previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>, we employ an LLM to inherit its complex reasoning capabilities and adopt the Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib29" title="">29</a>]</cite> to produce segmentation masks.
To overcome the unique challenges presented by video data, we propose two key innovations: a Sparse Dense Sampling strategy and a One-Token-Seg-All approach.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To equip the model with video temporal understanding ability, it is necessary to involve multiple frames.
Processing visual features from all sampled frames in full feature resolution is computationally prohibitive due to the large number of tokens.
In pursuit of efficiency, reducing the frame number would limit the perception of temporal dynamics while down-sampling frame features would lose visual details that are essential for dense prediction tasks exemplified by segmentation.
Our intuition is that adjacent frames in videos usually share similar visual contents and features.
Therefore, we leverage this inherent <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">temporal redundancy</span> in videos and propose the Sparse Dense Sampling strategy.
It uniformly samples a set of dense frames, preserving full-resolution features (<span class="ltx_text ltx_font_italic" id="S1.p4.1.2">dense</span> tokens), and down-samples the remaining interleaved frames to lower resolution (<span class="ltx_text ltx_font_italic" id="S1.p4.1.3">sparse</span> tokens).
Dense tokens provide detailed visual information needed for accurate segmentation, while sparse tokens capture the temporal context, ensuring that the model remains aware of motion and changes over time.
This balance allows the model to construct a coherent spatiotemporal narrative without excessive computational demands.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">For achieving temporal consistency in segmentation, instead of handling separate representations for each frame, we propose a One-Token-Seg-All approach.
Prior arts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib7" title="">7</a>]</cite> reveal that one compact representation can potentially associate the same object across video frames.
In this work, we design a special <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.1">&lt;TRK&gt;</span> token to segment and track target objects across multiple frames.
Specifically, we incorporate the <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.2">&lt;TRK&gt;</span> token into the model’s vocabulary and utilize its last hidden embedding in the LLM to prompt the mask decoder to produce segmentation masks.
We improve the temporal consistency from two aspects.
First, when generating the <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.3">&lt;TRK&gt;</span> token, the model ‘sees’ the video content through the temporal module, which serves as the information foundation for cross-frame association.
In addition, during training, the <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.4">&lt;TRK&gt;</span> token is intentionally trained to segment multiple frames simultaneously, preventing the model from learning shortcuts that focus only on spatial information of a certain frame.
During inference, a single <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.5">&lt;TRK&gt;</span> token can segment and track objects across an entire video.
The <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.6">&lt;TRK&gt;</span> token acts as a unified spatiotemporal representation, encapsulating object information across multiple frames and reducing the complexity of handling multiple prompts.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We evaluate our model on a comprehensive range of public benchmarks, including standard video/image referring segmentation, motion-guided video segmentation, and image reasoning segmentation.
To further assess the model’s capabilities in complex reasoning, temporal understanding, and object tracking, we introduce the ReasonVOS benchmark.
Extensive experiments and ablation studies demonstrate the effectiveness of our approach.
Although our model is particularly designed for videos, experiments show that it generalizes well on images, making it a potential foundation model for unified language instructed object segmentation. Our contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Sparse Dense Sampling Strategy: We devise a sampling strategy for video-LLM training that achieves a balance between temporal context length and spatial visual detail under computational constraints. This strategyis shown to be effective for spatiotemporal dense prediction tasks, exemplified by video object segmentation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">One-Token-Seg-All Approach: We design an effective approach for temporal consistent object segmentation in videos by utilizing a special <span class="ltx_text ltx_font_typewriter" id="S1.I1.i2.p1.1.1">&lt;TRK&gt;</span> token. This strategy demonstrates robust performance in video object segmentation, leveraging the video-LLM learning module and a specially designed training objective.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">VideoLISA Model: We propose VideoLISA, a video-LLM that democratizes reasoning segmentation to videos. Additionally, we introduce the ReasonVOS benchmark, focusing on complex reasoning, temporal understanding, and object movements. This benchmark, along with a range of public benchmarks, comprehensively validates our model’s performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Video Object Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In computer vision, video object segmentation is a well-studied task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib65" title="">65</a>]</cite>.
Specifically, referring video object segmentation (RVOS) aims to segment the target object mentioned in a natural language expression in a video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib47" title="">47</a>]</cite>.
Compared with image segmentation, RVOS is more challenging since both the action and appearance of the referred object must be segmented in a video.
Gavrilyuk et al. (2018) were the first to propose the RVOS task and the A2D-Sentences benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib23" title="">23</a>]</cite>.
This field continues to evolve with new benchmarks emerge such as Ref-DAVIS-17 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib28" title="">28</a>]</cite>, Ref-YouTube-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite>, and MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite>.
Many previous studies have primarily adapted referring image segmentation approaches for frame-by-frame object segmentation.
For example, URVOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite> and RefVOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib6" title="">6</a>]</cite> utilize cross-modal attention for per-frame segmentation.
Some recent works, such as ReferFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>]</cite> and MTTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib8" title="">8</a>]</cite>, employ a DETR-like structure, which simplifies the referring pipeline and achieves impressive performance.
R2VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib34" title="">34</a>]</cite> enhances multi-modal alignment through text reconstruction.
OnlineRefer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib60" title="">60</a>]</cite> proposes an online model with explicit query propagation.
SgMg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib47" title="">47</a>]</cite> proposes a segment-and-optimize paradigm to solve the feature drift issue.
Despite the impressive results achieved by these methods, several challenges remain.
First, most existing methods are deficient in comprehending the motion information in videos and languages, as revealed by the recent MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite> benchmark.
Second, there are few studies on complex reasoning-based segmentation in the video domain, both methodologically and benchmark-wise.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multimodal Large Language Model</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The remarkable advancements of large language models (LLMs) motivate the research community to extend the foundational capabilities of LLMs to the visual domain, leading to multimodal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib4" title="">4</a>]</cite>.
The pioneering works of MLLMs, such as LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib39" title="">39</a>]</cite>, MiniGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib71" title="">71</a>]</cite>, and InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib13" title="">13</a>]</cite>, exhibit impressive visual understanding capabilities, including image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib3" title="">3</a>]</cite> and visual question answering.
When extending into the video domain, a prominent issue is handling the temporal dimension.
One straightforward approach is to concatenate the tokens from multiple frames <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib37" title="">37</a>]</cite>, though the temporal length might be limited by computational resources.
To address this, one line of work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib25" title="">25</a>]</cite> explores pooling (merging) strategies to reduce the number of tokens, such as pooling along the spatial and temporal dimensions separately <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib45" title="">45</a>]</cite>, token merging based on similarity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib26" title="">26</a>]</cite>, and pooling with different strengths at a slow-fast pace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib25" title="">25</a>]</cite>.
Another line of work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib68" title="">68</a>]</cite> utilizes the Q-former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib32" title="">32</a>]</cite> architecture to extract abstracted features, which greatly reduces the number of tokens.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">More recently, some studies have further integrated region-level image understanding and grounding abilities into MLLMs.
Kosmos-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib50" title="">50</a>]</cite> and Shikra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib10" title="">10</a>]</cite> directly quantize bounding boxes into discrete location tokens or numeric representations of positions.
GPT4RoI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib69" title="">69</a>]</cite> uses a simple pooling operation to extract features within boxes or masks as the region representations.
Another line of work leverages the reasoning ability of MLLMs and resorts to off-the-shelf models for localization.
For example, DetGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib51" title="">51</a>]</cite> utilizes a pre-trained LLM and an open-vocabulary object detector to detect the target object based on human intent described in natural language.
LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite> connects an MLLM and the Segment Anything (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib29" title="">29</a>]</cite> model using a special token to produce fine-grained segmentation masks.
Although these works have achieved impressive performance on image tasks, they are still incapable of processing videos.
For object segmentation in videos, very few studies have leveraged the reasoning ability of LLMs to overcome current limitations.
PG-Video-LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib49" title="">49</a>]</cite> utilizes off-the-shelf object detector and tracker to obtain the target objects first and then match it with the entities mentioned in the generated text.
TrackGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib72" title="">72</a>]</cite> makes a straightforward extension of LISA by iteratively updating the special token with video progresses.
However, the absence of video learning module significantly limits its perception and reasoning of temporal dynamics.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S3.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework of our approach.
</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">The task of language-instructed reasoning segmentation in videos can be formally defined as follows.
Given a video <math alttext="\mathcal{X}_{\rm vid}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">𝒳</mi><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">vid</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝒳</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">vid</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathcal{X}_{\rm vid}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">caligraphic_X start_POSTSUBSCRIPT roman_vid end_POSTSUBSCRIPT</annotation></semantics></math> and a language expression <math alttext="\mathcal{X}_{\rm txt}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">𝒳</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">txt</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝒳</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">txt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathcal{X}_{\rm txt}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">caligraphic_X start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPT</annotation></semantics></math>, the model takes both as input and outputs the pixel-level segmentation masks <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">caligraphic_M</annotation></semantics></math> for all frames.
<math alttext="\mathcal{X}_{\rm txt}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">𝒳</mi><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">txt</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝒳</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">txt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathcal{X}_{\rm txt}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">caligraphic_X start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPT</annotation></semantics></math> is a free-form text that particularly emphasizes implicit intent reasoning, world knowledge, and video temporal dynamics.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.F1" title="Figure 1 ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the model architecture.
It consists of a visual tokenizer, an LLM, a vision encoder, and a promptable mask decoder.
We omit the text tokenizer in the LLM for simplicity.
The visual tokenizer and LLM are initialized from LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib53" title="">53</a>]</cite>.
The vision encoder and mask decoder are initialized from SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib29" title="">29</a>]</cite>.
Given a video, we first uniformly sample <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> frames and encode them into visual tokens via the visual tokenizer, resulting in <math alttext="T_{\rm sparse}\times L" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><msub id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">T</mi><mi id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml">sparse</mi></msub><mo id="S3.SS1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">𝑇</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">sparse</ci></apply><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">T_{\rm sparse}\times L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT × italic_L</annotation></semantics></math> tokens in total.
Ideally, larger <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑇</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> would be better for capturing temporal dynamics.
However, it is prohibitive to let the LLM process such a large number of tokens.
Thus, we develop the Sparse Dense Sampling strategy to reduce the number of tokens, which will be elaborated in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS2" title="3.2 Sparse Dense Sampling ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
After that, the visual tokens are concatenated with text tokens and fed into the LLM.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To equip the LLM with segmentation capabilities, following previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>, we extend the vocabulary of the LLM with a special token <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">&lt;TRK&gt;</span>.
During generation, this special token carries rich semantic information from the text prompt and video content, providing signals for decoding pixel-level segmentation masks.
Specifically, we extract the last layer embedding corresponding to the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">&lt;TRK&gt;</span> token and transform it into a prompt embedding with a multi-layer perceptron (MLP).
At the same time, the vision encoder extracts per-frame features from the video.
Finally, the prompt embedding and the visual features are processed by the mask decoder to produce the segmentation masks.
Note that for one video, there is only one prompt embedding that is in charge of all the frames.
The One-Token-Seg-All approach will be introduced in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.SS3" title="3.3 One Token Seg All ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Sparse Dense Sampling</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.7">Given the <math alttext="T_{\rm sparse}\times L" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">sparse</mi></msub><mo id="S3.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝑇</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">sparse</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">T_{\rm sparse}\times L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT × italic_L</annotation></semantics></math> tokens, we aim to reduce the number of tokens while preserving enough spatial details and temporal dynamics.
Therefore, we further sample <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑇</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames out of <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑇</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> frames.
The visual tokens of the <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝑇</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames are all preserved in full resolution, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.7.1">i.e., dense</span> tokens.
Then, we apply global average pooling on the <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">T</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑇</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> frames to reduce them to low resolution, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.7.2">i.e., sparse</span> tokens.
In our implementation, each frame is represented by only one token.
Finally, the total number of tokens is reduced to <math alttext="T_{\rm sparse}+T_{\rm dense}\times L" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><msub id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.2" xref="S3.SS2.p1.6.m6.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p1.6.m6.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.2.3.cmml">sparse</mi></msub><mo id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">+</mo><mrow id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><msub id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2.2" xref="S3.SS2.p1.6.m6.1.1.3.2.2.cmml">T</mi><mi id="S3.SS2.p1.6.m6.1.1.3.2.3" xref="S3.SS2.p1.6.m6.1.1.3.2.3.cmml">dense</mi></msub><mo id="S3.SS2.p1.6.m6.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.6.m6.1.1.3.1.cmml">×</mo><mi id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml">L</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><plus id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1"></plus><apply id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.2">𝑇</ci><ci id="S3.SS2.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.2.3">sparse</ci></apply><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><times id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.1"></times><apply id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2.3">dense</ci></apply><ci id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">T_{\rm sparse}+T_{\rm dense}\times L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT × italic_L</annotation></semantics></math>, which is significantly smaller than <math alttext="T_{\rm sparse}\times L" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mrow id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><msub id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2.2" xref="S3.SS2.p1.7.m7.1.1.2.2.cmml">T</mi><mi id="S3.SS2.p1.7.m7.1.1.2.3" xref="S3.SS2.p1.7.m7.1.1.2.3.cmml">sparse</mi></msub><mo id="S3.SS2.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.7.m7.1.1.1.cmml">×</mo><mi id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><times id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1.1"></times><apply id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.p1.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2.2">𝑇</ci><ci id="S3.SS2.p1.7.m7.1.1.2.3.cmml" xref="S3.SS2.p1.7.m7.1.1.2.3">sparse</ci></apply><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">T_{\rm sparse}\times L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT × italic_L</annotation></semantics></math>.
The rationale behind this strategy is the inherent temporal redundancy in video data.
By exploiting this, we reduce the computational burden without losing critical information.
The dense tokens provide visual details for their adjacent sparse frames, while the sparse tokens capture the temporal dynamics for the dense frames.
In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S2.SS2" title="2.2 Multimodal Large Language Model ‣ 2 Related Work ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we have discussed several popular temporal learning strategies in video-LLM.
Although they exhibit remarkable performance in general video understanding tasks, our empirical studies (see Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T6" title="Table 6 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>) demonstrate that these popular strategies are not seamlessly transferable to video object segmentation.
This is likely because they either lose spatial details or temporal information, both of which are essential in dense prediction tasks in videos.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>One Token Seg All</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S3.F2.g1" src="x2.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Exploration of One-Token-Seg-All approach.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.F1" title="Figure 1 ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">1</span></a>, throughout the video, we use a single special <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">&lt;TRK&gt;</span> token for segmenting all the frames.
We provide an in-depth analysis of the rationale behind this approach.
In our model, the promptable segmentation model is initialized from SAM, in which the decoder takes the prompt embedding and visual features as inputs and outputs masks.
Our intuition is that <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">segmenting one object in multiple frames can be regarded as segmenting multiple regions (instances) in one image grid</span>.
From this perspective, SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib29" title="">29</a>]</cite> itself already has the potential to segment objects across multiple frames, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">if the prompt is properly given</span>.
Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib20" title="">20</a>]</cite> suggest that one compact representation has the potential to associate the same entity across video frames.
For example, from the perspective of object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib7" title="">7</a>]</cite>, the prompt embedding can be regarded as a semantic kernel while the visual features are the context to be contrasted.
This motivates us to explore whether one prompt embedding is capable of tracking under the promptable decoding paradigm of SAM.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To answer this question, one key problem is whether the prompt embedding contains enough semantic information to serve as the kernel.
In SAM, its own prompt encoder mainly accepts visual prompts, such as points, boxes, and masks.
In videos, the object moves dynamically.
Our pilot study in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.F2" title="Figure 2 ‣ 3.3 One Token Seg All ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">2</span></a> shows that visual prompts quickly fail in the presence of object motion.
This is expected since these visual prompts heavily rely on the object’s spatial location.
We then explore the prompt embedding produced by an image reasoning segmentation model, LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>, which employs a LLM and is trained with segmentation data.
It can be expected that its prompt embedding should contain more semantic information, at least significantly more than that of the visually instructed prompt.
The second row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.F2" title="Figure 2 ‣ 3.3 One Token Seg All ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">2</span></a> validates this hypothesis by applying one prompt embedding to multiple frames.
Compared to box prompts, the prompt embedding from LISA shows improved resilience to object movement, as demonstrated in the first three frames.
However, when the object’s motion becomes larger and a distractor object appears, the segmentation fails again, drifting to another object nearby.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">We identify two primary factors that account for the failure.
Firstly, the input of LISA model only has one frame, which contains very limited temporal information.
Therefore, the generated prompt embedding lacks the information required for cross-frame association.
Secondly, during the training of LISA, the prompt embedding is trained to segment only one frame.
This potentially allows it to learn a shortcut that merely encompasses positional information, rather than learning the semantic information that generalizes across frames.
In our work, the approach of using one token to segment multiple frames has been developed by addressing these issues accordingly.
Firstly, the Sparse Dense Sampling-based temporal learning module provides spatiotemporal information of the video.
The model ‘sees’ the video content, which is the foundation of mask association.
Furthermore, during training, we intentionally train the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">&lt;TRK&gt;</span> token to segment multiple frames.
This objective would enforce the token to learn more ‘semantic’ information that can be used as the semantic kernel and segment the target object across frames.
The last row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S3.F2" title="Figure 2 ‣ 3.3 One Token Seg All ‣ 3 Method ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">2</span></a> presents the segmentation and tracking produced by the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.2">&lt;TRK&gt;</span> token in our VideoLISA.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training and Inference</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS4.p1.3.1">Training Data.</span>
The training data for our model mainly consists of two parts: 1) image segmentation and 2) video segmentation.
For the image part, we follow the setting of LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>.
For the video data, we employ video object segmentation (VOS) and referring video segmentation data (RVOS).
During pre-processing, we fill the original category name or referring expression in the dataset into a template.
For example: “<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.SS4.p1.3.2">USER<span class="ltx_text ltx_font_medium" id="S3.SS4.p1.3.2.1">: &lt;VIDEO&gt; Can you segment {description} in this scene? </span>ASSISTANT<span class="ltx_text ltx_font_medium" id="S3.SS4.p1.3.2.2">: Sure, it is &lt;TRK&gt;.</span></span>”, where {<span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.3.3">description</span>} is the placeholder to fill.
For VOS data that contain videos with multi-class labels, we randomly choose one class and merge all the masks belonging to this class into one binary mask.
<span class="ltx_text ltx_font_bold" id="S3.SS4.p1.3.4">Training Objective.</span>
The model is trained end-to-end using the text generation loss <math alttext="\mathcal{L}_{\rm txt}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">txt</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">txt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{L}_{\rm txt}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPT</annotation></semantics></math> and segmentation loss <math alttext="\mathcal{L}_{\rm seg}" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">seg</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">ℒ</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">seg</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\mathcal{L}_{\rm seg}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT roman_seg end_POSTSUBSCRIPT</annotation></semantics></math>.
The segmentation loss consists of per-pixel binary cross-entropy (BCE) loss and DICE loss.
The final loss is computed as the weighted sum of the three losses.
For video training, we compute the segmentation loss on the sampled <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑇</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames in parallel and average them.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.6.1">Inference.</span>
During inference, given a video, <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑇</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">𝑇</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames are sampled similarly to training, except that the <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m3.1"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">𝑇</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.3.m3.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames are uniformly sampled from <math alttext="T_{\rm sparse}" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m4.1"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">sparse</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">𝑇</ci><ci id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">sparse</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">T_{\rm sparse}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.4.m4.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT</annotation></semantics></math> rather than randomly.
After obtaining the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.6.2">&lt;TRK&gt;</span> token from the LLM, we feed all the frames of the video into the mask decoder one by one, using the same <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.6.3">&lt;TRK&gt;</span> token to segment each frame, yielding a list of masks.
<span class="ltx_text ltx_font_bold" id="S3.SS4.p2.6.4">Post optimization.</span>
Among these frames, the <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS4.p2.5.m5.1"><semantics id="S3.SS4.p2.5.m5.1a"><msub id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">𝑇</ci><ci id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.5.m5.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames are seen in full resolution by the model, making their segmentation masks more reliable and accurate.
For the remaining frames, although the One-Token-Seg-All strategy exhibits impressive cross-frame segmentation performance, our empirical observations indicate it inevitably suffers from low mask quality, likely limited by the inherent capability of the SAM model.
Thus, we employ post-optimization as an optional step to further enhance mask quality.
Specifically, we take XMem++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib5" title="">5</a>]</cite> as the post-optimization approach.
Compared to XMem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib12" title="">12</a>]</cite>, which propagates one mask through the video, XMem++ distinguishes itself by taking multiple ‘reliable’ masks as reference and inferring the masks of the remaining frames.
This paradigm is naturally suitable for our method since the <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="S3.SS4.p2.6.m6.1"><semantics id="S3.SS4.p2.6.m6.1a"><msub id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi id="S3.SS4.p2.6.m6.1.1.2" xref="S3.SS4.p2.6.m6.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2">𝑇</ci><ci id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.6.m6.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> frames span uniformly across the video, providing long-range yet diverse masks as references.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmark</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The versatile abilities of our model can be evaluated using public benchmarks that assess various aspects.
RVOS benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite> evaluate temporal-related abilities, involving referring expression comprehension, video temporal understanding, and temporal consistent segmentation.
Complex reasoning abilities can be assessed by the image-based reasoning segmentation benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>.
However, there is still a lack of a benchmark that comprehensively evaluates the reasoning segmentation abilities of videos.
Towards this goal, we have organized the <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">ReasonVOS</span> benchmark.
Specifically, we annotate language expressions based on the videos and mask annotations from existing datasets, including MOSE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib15" title="">15</a>]</cite>, MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite>, VIPSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib48" title="">48</a>]</cite>, and BURST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib2" title="">2</a>]</cite>.
The criteria for data collection and annotation processes are as follows.
Each language expression should encompass at least one of the following aspects: 1) complex reasoning, 2) world knowledge, 3) temporal dynamics.
For the video and mask selection, objects with explicit movement are highly prioritized to evaluate the temporal consistency of masks.
As a result, we manually annotated 105 samples as initial seed data.
Following previous practices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib72" title="">72</a>]</cite>, we use a LLM to rephrase the language expressions for augmentation and perform another round of human checking.
The resulting ReasonVOS benchmark comprises 458 video-instruction-mask data samples.
This benchmark is specifically designed for zero-shot evaluation purposes, as the reasoning ability is embedded in the LLM and can be triggered by existing image-based reasoning segmentation data.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setting</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets</span>
Our model is trained on a variety of segmentation datasets.
The image-based datasets include
1) semantic segmentation: ADE20K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib70" title="">70</a>]</cite>, COCO-Stuff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib9" title="">9</a>]</cite>, PACO-LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib52" title="">52</a>]</cite>, and PASCAL-Part <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib11" title="">11</a>]</cite>;
2) referring segmentation: refCLEF, refCOCO, refCOCO+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib27" title="">27</a>]</cite>, and refCOCOg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib46" title="">46</a>]</cite>;
3) reason segmentation: 239 ReasonSeg samples from LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>.
The video-based datasets we use include:
1) semantic VOS: YouTube-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib63" title="">63</a>]</cite>;
2) referring VOS: Refer-YouTube-VOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite> and MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite>.
The evaluation benchmarks will be elaborated in the corresponding experiment sections.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.8"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.8.1">Implementation Details</span>
We implement our model with LLaVA-Phi-3-V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib53" title="">53</a>]</cite>, a multimodal LLM based on Phi-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib1" title="">1</a>]</cite> with 3.8B parameters.
We adopt the vision encoder and mask decoder from SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib29" title="">29</a>]</cite>.
We conduct joint training using both image and video datasets.
For video data, we set <math alttext="T_{\rm sparse}=32" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><msub id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2.2" xref="S5.SS1.p2.1.m1.1.1.2.2.cmml">T</mi><mi id="S5.SS1.p2.1.m1.1.1.2.3" xref="S5.SS1.p2.1.m1.1.1.2.3.cmml">sparse</mi></msub><mo id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><eq id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></eq><apply id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.2.1.cmml" xref="S5.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2.2">𝑇</ci><ci id="S5.SS1.p2.1.m1.1.1.2.3.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3">sparse</ci></apply><cn id="S5.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">T_{\rm sparse}=32</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_T start_POSTSUBSCRIPT roman_sparse end_POSTSUBSCRIPT = 32</annotation></semantics></math> and <math alttext="T_{\rm dense}=4" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><msub id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2.2" xref="S5.SS1.p2.2.m2.1.1.2.2.cmml">T</mi><mi id="S5.SS1.p2.2.m2.1.1.2.3" xref="S5.SS1.p2.2.m2.1.1.2.3.cmml">dense</mi></msub><mo id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><eq id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></eq><apply id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.2.1.cmml" xref="S5.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2.2">𝑇</ci><ci id="S5.SS1.p2.2.m2.1.1.2.3.cmml" xref="S5.SS1.p2.2.m2.1.1.2.3">dense</ci></apply><cn id="S5.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">T_{\rm dense}=4</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT = 4</annotation></semantics></math> according to our GPU memory.
For image data, we duplicate the images as pseudo video data.
We train our model using 64 NVIDIA 24G A10 GPUs with a distributed training script based on DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib54" title="">54</a>]</cite>.
We use the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib42" title="">42</a>]</cite> optimizer with the learning rate and weight decay set to 0.0003 and 0, respectively.
We also adopt WarmupDecayLR as the learning rate scheduler, with the warmup iterations set to 100.
The weights of the text generation loss (<math alttext="\lambda_{\rm txt}" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><msub id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">λ</mi><mi id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">txt</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝜆</ci><ci id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">txt</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\lambda_{\rm txt}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_λ start_POSTSUBSCRIPT roman_txt end_POSTSUBSCRIPT</annotation></semantics></math>) and the mask loss (<math alttext="\lambda_{\rm seg}" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><msub id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml"><mi id="S5.SS1.p2.4.m4.1.1.2" xref="S5.SS1.p2.4.m4.1.1.2.cmml">λ</mi><mi id="S5.SS1.p2.4.m4.1.1.3" xref="S5.SS1.p2.4.m4.1.1.3.cmml">seg</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><apply id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.4.m4.1.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p2.4.m4.1.1.2.cmml" xref="S5.SS1.p2.4.m4.1.1.2">𝜆</ci><ci id="S5.SS1.p2.4.m4.1.1.3.cmml" xref="S5.SS1.p2.4.m4.1.1.3">seg</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">\lambda_{\rm seg}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">italic_λ start_POSTSUBSCRIPT roman_seg end_POSTSUBSCRIPT</annotation></semantics></math>) are both set to 1.0.
The weights of the BCE loss (<math alttext="\lambda_{\rm bce}" class="ltx_Math" display="inline" id="S5.SS1.p2.5.m5.1"><semantics id="S5.SS1.p2.5.m5.1a"><msub id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml"><mi id="S5.SS1.p2.5.m5.1.1.2" xref="S5.SS1.p2.5.m5.1.1.2.cmml">λ</mi><mi id="S5.SS1.p2.5.m5.1.1.3" xref="S5.SS1.p2.5.m5.1.1.3.cmml">bce</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><apply id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.5.m5.1.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS1.p2.5.m5.1.1.2.cmml" xref="S5.SS1.p2.5.m5.1.1.2">𝜆</ci><ci id="S5.SS1.p2.5.m5.1.1.3.cmml" xref="S5.SS1.p2.5.m5.1.1.3">bce</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">\lambda_{\rm bce}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.5.m5.1d">italic_λ start_POSTSUBSCRIPT roman_bce end_POSTSUBSCRIPT</annotation></semantics></math>) and the DICE loss (<math alttext="\lambda_{\rm dice}" class="ltx_Math" display="inline" id="S5.SS1.p2.6.m6.1"><semantics id="S5.SS1.p2.6.m6.1a"><msub id="S5.SS1.p2.6.m6.1.1" xref="S5.SS1.p2.6.m6.1.1.cmml"><mi id="S5.SS1.p2.6.m6.1.1.2" xref="S5.SS1.p2.6.m6.1.1.2.cmml">λ</mi><mi id="S5.SS1.p2.6.m6.1.1.3" xref="S5.SS1.p2.6.m6.1.1.3.cmml">dice</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.6.m6.1b"><apply id="S5.SS1.p2.6.m6.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.6.m6.1.1.1.cmml" xref="S5.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S5.SS1.p2.6.m6.1.1.2.cmml" xref="S5.SS1.p2.6.m6.1.1.2">𝜆</ci><ci id="S5.SS1.p2.6.m6.1.1.3.cmml" xref="S5.SS1.p2.6.m6.1.1.3">dice</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.6.m6.1c">\lambda_{\rm dice}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.6.m6.1d">italic_λ start_POSTSUBSCRIPT roman_dice end_POSTSUBSCRIPT</annotation></semantics></math>) are set to 2.0 and 0.5, respectively.
The per-device batch size is set to 2.
For ablation studies, the total number of iterations is <math alttext="3,000" class="ltx_Math" display="inline" id="S5.SS1.p2.7.m7.2"><semantics id="S5.SS1.p2.7.m7.2a"><mrow id="S5.SS1.p2.7.m7.2.3.2" xref="S5.SS1.p2.7.m7.2.3.1.cmml"><mn id="S5.SS1.p2.7.m7.1.1" xref="S5.SS1.p2.7.m7.1.1.cmml">3</mn><mo id="S5.SS1.p2.7.m7.2.3.2.1" xref="S5.SS1.p2.7.m7.2.3.1.cmml">,</mo><mn id="S5.SS1.p2.7.m7.2.2" xref="S5.SS1.p2.7.m7.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.7.m7.2b"><list id="S5.SS1.p2.7.m7.2.3.1.cmml" xref="S5.SS1.p2.7.m7.2.3.2"><cn id="S5.SS1.p2.7.m7.1.1.cmml" type="integer" xref="S5.SS1.p2.7.m7.1.1">3</cn><cn id="S5.SS1.p2.7.m7.2.2.cmml" type="integer" xref="S5.SS1.p2.7.m7.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.7.m7.2c">3,000</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.7.m7.2d">3 , 000</annotation></semantics></math> and each experiment takes around 10 hours.
For the final model used for comparison, we scale up the training to <math alttext="6,000" class="ltx_Math" display="inline" id="S5.SS1.p2.8.m8.2"><semantics id="S5.SS1.p2.8.m8.2a"><mrow id="S5.SS1.p2.8.m8.2.3.2" xref="S5.SS1.p2.8.m8.2.3.1.cmml"><mn id="S5.SS1.p2.8.m8.1.1" xref="S5.SS1.p2.8.m8.1.1.cmml">6</mn><mo id="S5.SS1.p2.8.m8.2.3.2.1" xref="S5.SS1.p2.8.m8.2.3.1.cmml">,</mo><mn id="S5.SS1.p2.8.m8.2.2" xref="S5.SS1.p2.8.m8.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.8.m8.2b"><list id="S5.SS1.p2.8.m8.2.3.1.cmml" xref="S5.SS1.p2.8.m8.2.3.2"><cn id="S5.SS1.p2.8.m8.1.1.cmml" type="integer" xref="S5.SS1.p2.8.m8.1.1">6</cn><cn id="S5.SS1.p2.8.m8.2.2.cmml" type="integer" xref="S5.SS1.p2.8.m8.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.8.m8.2c">6,000</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.8.m8.2d">6 , 000</annotation></semantics></math> iterations, which takes 20 hours.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Evaluation Metrics</span>
For image-based evaluation, we adopt two metrics commonly used in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>: gIoU and cIoU.
gIoU is defined by the average of all per-image Intersection-over-Unions (IoUs), while cIoU is defined by the cumulative intersection over the cumulative union.
For video-based evaluation, we follow previous practices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib60" title="">60</a>]</cite> and use region similarity (J), contour accuracy (F), and their average value (J&amp;F).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation on Video Tasks</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Referring Video Object Segmentation</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">We adopt two benchmarks of standard referring video object segmentation.
Ref-Youtube-VOS is evaluated on the official challenge server <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://codalab.lisn.upsaclay.fr/competitions/3282" title="">https://codalab.lisn.upsaclay.fr/competitions/3282</a></span></span></span>.
Ref-DAVIS-17 is evaluated by the official evaluation code <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/davisvideochallenge/davis2017-evaluation" title="">https://github.com/davisvideochallenge/davis2017-evaluation</a></span></span></span>.
The evaluation results are shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T1" title="Table 1 ‣ 5.2.1 Referring Video Object Segmentation ‣ 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our method demonstrates competitive performance on both benchmarks, achieving comparable or superior results to existing methods.
For Refer-DAVIS-17, our method achieves state-of-the-art performance, outperforming all the other methods by a considerable margin.
In Refer-YouTube-VOS, our method performs well compared to traditional RVOS methods, achieving a high rank.
State-of-the-art methods, such as SgMg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib47" title="">47</a>]</cite>, achieve remarkable performance, thanks to its dedicated video backbones, such as Video-Swin <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib41" title="">41</a>]</cite>.
However, among LLM-based methods with reasoning ability, our model, despite having only 3.8B parameters, outperforms other methods with much larger LLMs, such as LISA-13B and TrackGPT-13B.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>The quantitative evaluation results on Refer-Youtube-VOS and Refer-DAVIS-17.
In the table, <span class="ltx_text ltx_font_bold" id="S5.T1.16.1">bold</span> denotes the best scores; <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.17.2">underline</span> denotes the second place.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.9" style="width:346.9pt;height:324pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.6pt,36.0pt) scale(0.818062352372465,0.818062352372465) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.9.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.9.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.10.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.10.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.10.1.2" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.10.1.2.1" style="font-size:90%;">Year</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T1.9.9.10.1.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.10.1.3.1" style="font-size:90%;">Refer-Youtube-VOS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S5.T1.9.9.10.1.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.10.1.4.1" style="font-size:90%;">Refer-DAVIS-17</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.1.1.1.1.m1.1.1" mathsize="90%" xref="S5.T1.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math><span class="ltx_text" id="S5.T1.2.2.2.2.1" style="font-size:90%;">&amp;</span><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.m2.1"><semantics id="S5.T1.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.2.2.2.2.m2.1.1" mathsize="90%" xref="S5.T1.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.m2.1b"><ci id="S5.T1.2.2.2.2.m2.1.1.cmml" xref="S5.T1.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.3.3.3.3.m1.1.1" mathsize="90%" xref="S5.T1.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.4.4.4.4.m1.1.1" mathsize="90%" xref="S5.T1.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.6.6.6.6" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T1.5.5.5.5.m1.1"><semantics id="S5.T1.5.5.5.5.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.5.5.5.5.m1.1.1" mathsize="90%" xref="S5.T1.5.5.5.5.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.m1.1b"><ci id="S5.T1.5.5.5.5.m1.1.1.cmml" xref="S5.T1.5.5.5.5.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.5.m1.1d">caligraphic_J</annotation></semantics></math><span class="ltx_text" id="S5.T1.6.6.6.6.1" style="font-size:90%;">&amp;</span><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T1.6.6.6.6.m2.1"><semantics id="S5.T1.6.6.6.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.6.6.6.6.m2.1.1" mathsize="90%" xref="S5.T1.6.6.6.6.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.6.m2.1b"><ci id="S5.T1.6.6.6.6.m2.1.1.cmml" xref="S5.T1.6.6.6.6.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.6.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.6.6.m2.1d">caligraphic_F</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.7.7" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T1.7.7.7.7.m1.1"><semantics id="S5.T1.7.7.7.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.7.7.7.7.m1.1.1" mathsize="90%" xref="S5.T1.7.7.7.7.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.7.m1.1b"><ci id="S5.T1.7.7.7.7.m1.1.1.cmml" xref="S5.T1.7.7.7.7.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.7.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.7.7.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.8.8.8" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T1.8.8.8.8.m1.1"><semantics id="S5.T1.8.8.8.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T1.8.8.8.8.m1.1.1" mathsize="90%" xref="S5.T1.8.8.8.8.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.8.m1.1b"><ci id="S5.T1.8.8.8.8.m1.1.1.cmml" xref="S5.T1.8.8.8.8.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.8.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.8.8.m1.1d">caligraphic_F</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.11.2" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="8" id="S5.T1.9.9.11.2.1" style="padding-left:8.0pt;padding-right:8.0pt;"><em class="ltx_emph ltx_font_italic" id="S5.T1.9.9.11.2.1.1" style="font-size:90%;background-color:#F2F2F2;">Traditional methods without reasoning ability</em></th>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.12.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.12.3.1.1" style="font-size:90%;">URVOS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.12.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a><span class="ltx_text" id="S5.T1.9.9.12.3.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.12.3.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.2.1" style="font-size:90%;">2020</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.12.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.3.1" style="font-size:90%;">47.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.12.3.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.4.1" style="font-size:90%;">45.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.9.12.3.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.5.1" style="font-size:90%;">49.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.12.3.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.6.1" style="font-size:90%;">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.12.3.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.7.1" style="font-size:90%;">47.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.12.3.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.12.3.8.1" style="font-size:90%;">55.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.13.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.13.4.1.1" style="font-size:90%;">CMPC-V </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.13.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib40" title="">40</a><span class="ltx_text" id="S5.T1.9.9.13.4.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.13.4.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.2.1" style="font-size:90%;">2021</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.13.4.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.3.1" style="font-size:90%;">47.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.13.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.4.1" style="font-size:90%;">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.13.4.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.5.1" style="font-size:90%;">49.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.13.4.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.13.4.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.13.4.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.13.4.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.14.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.14.5.1.1" style="font-size:90%;">YOFO </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.14.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib31" title="">31</a><span class="ltx_text" id="S5.T1.9.9.14.5.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.14.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.2.1" style="font-size:90%;">2022</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.14.5.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.3.1" style="font-size:90%;">48.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.14.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.4.1" style="font-size:90%;">47.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.14.5.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.5.1" style="font-size:90%;">49.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.14.5.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.6.1" style="font-size:90%;">53.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.14.5.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.7.1" style="font-size:90%;">48.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.14.5.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.14.5.8.1" style="font-size:90%;">57.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.15.6.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.15.6.1.1" style="font-size:90%;">LBDT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.15.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib18" title="">18</a><span class="ltx_text" id="S5.T1.9.9.15.6.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.15.6.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.2.1" style="font-size:90%;">2022</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.15.6.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.3.1" style="font-size:90%;">49.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.15.6.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.4.1" style="font-size:90%;">48.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.15.6.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.5.1" style="font-size:90%;">50.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.15.6.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.6.1" style="font-size:90%;">54.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.15.6.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.15.6.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.15.6.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.16.7.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.16.7.1.1" style="font-size:90%;">MLSA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.16.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib59" title="">59</a><span class="ltx_text" id="S5.T1.9.9.16.7.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.16.7.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.2.1" style="font-size:90%;">2022</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.16.7.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.3.1" style="font-size:90%;">49.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.16.7.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.4.1" style="font-size:90%;">48.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.16.7.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.5.1" style="font-size:90%;">50.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.16.7.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.6.1" style="font-size:90%;">57.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.16.7.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.7.1" style="font-size:90%;">53.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.16.7.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.16.7.8.1" style="font-size:90%;">62.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.17.8.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.17.8.1.1" style="font-size:90%;">PMINet + CFBI </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.17.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib19" title="">19</a><span class="ltx_text" id="S5.T1.9.9.17.8.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.17.8.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.2.1" style="font-size:90%;">2021</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.17.8.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.3.1" style="font-size:90%;">54.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.17.8.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.4.1" style="font-size:90%;">53.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.17.8.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.5.1" style="font-size:90%;">55.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.17.8.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.17.8.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.17.8.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.17.8.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.18.9.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.18.9.1.1" style="font-size:90%;">MTTR </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.18.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib8" title="">8</a><span class="ltx_text" id="S5.T1.9.9.18.9.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.18.9.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.2.1" style="font-size:90%;">2022</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.18.9.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.3.1" style="font-size:90%;">55.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.18.9.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.4.1" style="font-size:90%;">54.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.18.9.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.5.1" style="font-size:90%;">56.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.18.9.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.18.9.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.18.9.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.18.9.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.19.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.19.10.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.19.10.1.1" style="font-size:90%;">CITD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.19.10.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib35" title="">35</a><span class="ltx_text" id="S5.T1.9.9.19.10.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.19.10.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.2.1" style="font-size:90%;">2021</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.19.10.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.3.1" style="font-size:90%;">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.19.10.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.4.1" style="font-size:90%;">60.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.19.10.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.5.1" style="font-size:90%;">62.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.19.10.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.19.10.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.19.10.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.19.10.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.20.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.20.11.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.20.11.1.1" style="font-size:90%;">ReferFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.20.11.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a><span class="ltx_text" id="S5.T1.9.9.20.11.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.20.11.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.2.1" style="font-size:90%;">2022</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.20.11.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.3.1" style="font-size:90%;">62.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.20.11.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.4.1" style="font-size:90%;">61.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.20.11.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.5.1" style="font-size:90%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.20.11.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.6.1" style="font-size:90%;">61.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.20.11.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.7.1" style="font-size:90%;">58.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.20.11.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.20.11.8.1" style="font-size:90%;">64.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.9.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.9.1.1" style="font-size:90%;">R</span><sup class="ltx_sup" id="S5.T1.9.9.9.1.2"><span class="ltx_text" id="S5.T1.9.9.9.1.2.1" style="font-size:90%;">2</span></sup><span class="ltx_text" id="S5.T1.9.9.9.1.3" style="font-size:90%;">-VOS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.9.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib34" title="">34</a><span class="ltx_text" id="S5.T1.9.9.9.1.5.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.9.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.3.1" style="font-size:90%;">61.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.4.1" style="font-size:90%;">59.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.9.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.5.1" style="font-size:90%;">63.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.9.8.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.21.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.21.12.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.21.12.1.1" style="font-size:90%;">SgMg </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.21.12.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib47" title="">47</a><span class="ltx_text" id="S5.T1.9.9.21.12.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.21.12.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.21.12.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.21.12.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.21.12.3.1" style="font-size:90%;">65.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.21.12.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.21.12.4.1" style="font-size:90%;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.21.12.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.21.12.5.1" style="font-size:90%;">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.21.12.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.21.12.6.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.21.12.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.21.12.7.1" style="font-size:90%;">60.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.21.12.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.21.12.8.1" style="font-size:90%;">66.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.22.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.22.13.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.22.13.1.1" style="font-size:90%;">OnlineRefer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.22.13.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib60" title="">60</a><span class="ltx_text" id="S5.T1.9.9.22.13.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.22.13.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.22.13.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.3.1" style="font-size:90%;">63.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.22.13.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.4.1" style="font-size:90%;">61.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.22.13.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.5.1" style="font-size:90%;">65.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.22.13.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.6.1" style="font-size:90%;">64.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.22.13.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.7.1" style="font-size:90%;">61.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.22.13.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.22.13.8.1" style="font-size:90%;">67.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.23.14" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="8" id="S5.T1.9.9.23.14.1" style="padding-left:8.0pt;padding-right:8.0pt;"><em class="ltx_emph ltx_font_italic" id="S5.T1.9.9.23.14.1.1" style="font-size:90%;background-color:#F2F2F2;">LLM-based methods with reasoning ability</em></th>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.24.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.24.15.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.24.15.1.1" style="font-size:90%;">LISA-7B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.24.15.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a><span class="ltx_text" id="S5.T1.9.9.24.15.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.9.9.24.15.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.24.15.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.3.1" style="font-size:90%;">50.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.24.15.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.4.1" style="font-size:90%;">49.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.9.24.15.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.5.1" style="font-size:90%;">50.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.24.15.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.6.1" style="font-size:90%;">58.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.24.15.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.7.1" style="font-size:90%;">54.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.24.15.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.24.15.8.1" style="font-size:90%;">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.25.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.25.16.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.25.16.1.1" style="font-size:90%;">LISA-13B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.25.16.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a><span class="ltx_text" id="S5.T1.9.9.25.16.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.25.16.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.25.16.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.3.1" style="font-size:90%;">52.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.25.16.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.4.1" style="font-size:90%;">52.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.25.16.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.5.1" style="font-size:90%;">53.0</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.25.16.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.6.1" style="font-size:90%;">60.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.25.16.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.7.1" style="font-size:90%;">56.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.25.16.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.25.16.8.1" style="font-size:90%;">64.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.26.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.26.17.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.26.17.1.1" style="font-size:90%;">TrackGPT-7B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.26.17.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib72" title="">72</a><span class="ltx_text" id="S5.T1.9.9.26.17.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.26.17.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.26.17.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.3.1" style="font-size:90%;">56.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.26.17.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.4.1" style="font-size:90%;">55.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.26.17.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.5.1" style="font-size:90%;">57.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.26.17.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.6.1" style="font-size:90%;">63.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.26.17.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.7.1" style="font-size:90%;">59.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.26.17.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.26.17.8.1" style="font-size:90%;">67.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.27.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.27.18.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S5.T1.9.9.27.18.1.1" style="font-size:90%;">TrackGPT-13B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.T1.9.9.27.18.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib72" title="">72</a><span class="ltx_text" id="S5.T1.9.9.27.18.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.27.18.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.2.1" style="font-size:90%;">2023</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.27.18.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.3.1" style="font-size:90%;">59.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.27.18.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.4.1" style="font-size:90%;">58.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.27.18.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.5.1" style="font-size:90%;">60.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.27.18.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.6.1" style="font-size:90%;">66.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.27.18.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.7.1" style="font-size:90%;">62.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.27.18.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.27.18.8.1" style="font-size:90%;">70.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.28.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.28.19.1" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.28.19.1.1" style="font-size:90%;">VideoLISA-3.8B (One-Token-Seg-All)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.9.9.28.19.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.28.19.2.1" style="font-size:90%;">2024</span></th>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.28.19.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.28.19.3.1" style="font-size:90%;">61.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.28.19.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.28.19.4.1" style="font-size:90%;">60.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.9.9.28.19.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.28.19.5.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.28.19.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.28.19.6.1" style="font-size:90%;">67.7</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.28.19.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.28.19.7.1" style="font-size:90%;">63.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.9.9.28.19.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.28.19.8.1" style="font-size:90%;">71.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.9.9.29.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T1.9.9.29.20.1" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.29.20.1.1" style="font-size:90%;">VideoLISA-3.8B (Post-optimization)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T1.9.9.29.20.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T1.9.9.29.20.2.1" style="font-size:90%;">2024</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.9.9.29.20.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.29.20.3.1" style="font-size:90%;">63.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.9.9.29.20.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.29.20.4.1" style="font-size:90%;">61.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.9.9.29.20.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.9.9.29.20.5.1" style="font-size:90%;">65.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.9.9.29.20.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.29.20.6.1" style="font-size:90%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.9.9.29.20.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.29.20.7.1" style="font-size:90%;">64.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.9.9.29.20.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.9.9.29.20.8.1" style="font-size:90%;">72.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T3.3" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Results on MeViS benchmark.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.3.3" style="width:179.8pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.5pt,24.3pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.3.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.3.3.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.3.4" style="padding-left:1.7pt;padding-right:1.7pt;">Methods</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.3.5" style="padding-left:1.7pt;padding-right:1.7pt;">Year</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.1.1.1.1.1" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{J\&amp;F}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T3.1.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.1.m1.1.1.2.cmml">𝒥</mi><mo id="S5.T3.1.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.T3.1.1.1.1.1.m1.1.1.1.cmml">&amp;</mo><mi class="ltx_font_mathcaligraphic" id="S5.T3.1.1.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.1.1.m1.1.1.3.cmml">ℱ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1"><and id="S5.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1"></and><ci id="S5.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.2">𝒥</ci><ci id="S5.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3">ℱ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\mathcal{J\&amp;F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">caligraphic_J &amp; caligraphic_F</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.2.2.2.2.2" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.2.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T3.2.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.2.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.2.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.2.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.2.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.3.3" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T3.3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.3.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.3.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.3.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.3.m1.1d">caligraphic_F</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.4.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.4.1.1" style="padding-left:1.7pt;padding-right:1.7pt;">URVOS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib56" title="">56</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.4.1.2" style="padding-left:1.7pt;padding-right:1.7pt;">2020</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.4.1.3" style="padding-left:1.7pt;padding-right:1.7pt;">27.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.4.1.4" style="padding-left:1.7pt;padding-right:1.7pt;">25.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.4.1.5" style="padding-left:1.7pt;padding-right:1.7pt;">29.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.5.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.5.2.1" style="padding-left:1.7pt;padding-right:1.7pt;">LBDT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib18" title="">18</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.5.2.2" style="padding-left:1.7pt;padding-right:1.7pt;">2022</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.5.2.3" style="padding-left:1.7pt;padding-right:1.7pt;">29.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.5.2.4" style="padding-left:1.7pt;padding-right:1.7pt;">27.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.5.2.5" style="padding-left:1.7pt;padding-right:1.7pt;">30.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.6.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.6.3.1" style="padding-left:1.7pt;padding-right:1.7pt;">MTTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib8" title="">8</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.6.3.2" style="padding-left:1.7pt;padding-right:1.7pt;">2022</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.6.3.3" style="padding-left:1.7pt;padding-right:1.7pt;">30.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.6.3.4" style="padding-left:1.7pt;padding-right:1.7pt;">28.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.6.3.5" style="padding-left:1.7pt;padding-right:1.7pt;">31.2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.7.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.7.4.1" style="padding-left:1.7pt;padding-right:1.7pt;">ReferFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.7.4.2" style="padding-left:1.7pt;padding-right:1.7pt;">2022</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.7.4.3" style="padding-left:1.7pt;padding-right:1.7pt;">31.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.7.4.4" style="padding-left:1.7pt;padding-right:1.7pt;">29.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.7.4.5" style="padding-left:1.7pt;padding-right:1.7pt;">32.2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.8.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.8.5.1" style="padding-left:1.7pt;padding-right:1.7pt;">VLT+TC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib16" title="">16</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.8.5.2" style="padding-left:1.7pt;padding-right:1.7pt;">2021</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.8.5.3" style="padding-left:1.7pt;padding-right:1.7pt;">35.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.8.5.4" style="padding-left:1.7pt;padding-right:1.7pt;">33.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.8.5.5" style="padding-left:1.7pt;padding-right:1.7pt;">37.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.9.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.9.6.1" style="padding-left:1.7pt;padding-right:1.7pt;">LMPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.3.9.6.2" style="padding-left:1.7pt;padding-right:1.7pt;">2023</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.9.6.3" style="padding-left:1.7pt;padding-right:1.7pt;">37.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.9.6.4" style="padding-left:1.7pt;padding-right:1.7pt;">34.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.3.3.3.9.6.5" style="padding-left:1.7pt;padding-right:1.7pt;">40.2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.10.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.10.7.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.10.7.1.1">VideoLISA-3.8B (One-Token-Seg-All)</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.3.3.10.7.2" style="padding-left:1.7pt;padding-right:1.7pt;">2024</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.10.7.3" style="padding-left:1.7pt;padding-right:1.7pt;">42.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.10.7.4" style="padding-left:1.7pt;padding-right:1.7pt;">39.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.3.3.3.10.7.5" style="padding-left:1.7pt;padding-right:1.7pt;">45.2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3.11.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.3.3.3.11.8.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.11.8.1.1">VideoLISA-3.8B (Post-optimization)</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.3.3.3.11.8.2" style="padding-left:1.7pt;padding-right:1.7pt;">2024</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.3.3.3.11.8.3" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.11.8.3.1">44.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.3.3.3.11.8.4" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.11.8.4.1">41.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.3.3.3.11.8.5" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.11.8.5.1">47.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T3.6" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 3: </span>
Results on ReasonVOS benchmark.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.6.3" style="width:179.8pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.5pt,24.3pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.6.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.3.3.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.3.4" style="padding-left:1.7pt;padding-right:1.7pt;">Methods</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.3.5" style="padding-left:1.7pt;padding-right:1.7pt;">Year</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.4.1.1.1.1" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{J\&amp;F}" class="ltx_Math" display="inline" id="S5.T3.4.1.1.1.1.m1.1"><semantics id="S5.T3.4.1.1.1.1.m1.1a"><mrow id="S5.T3.4.1.1.1.1.m1.1.1" xref="S5.T3.4.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T3.4.1.1.1.1.m1.1.1.2" xref="S5.T3.4.1.1.1.1.m1.1.1.2.cmml">𝒥</mi><mo id="S5.T3.4.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.T3.4.1.1.1.1.m1.1.1.1.cmml">&amp;</mo><mi class="ltx_font_mathcaligraphic" id="S5.T3.4.1.1.1.1.m1.1.1.3" xref="S5.T3.4.1.1.1.1.m1.1.1.3.cmml">ℱ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.1.1.1.1.m1.1b"><apply id="S5.T3.4.1.1.1.1.m1.1.1.cmml" xref="S5.T3.4.1.1.1.1.m1.1.1"><and id="S5.T3.4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.4.1.1.1.1.m1.1.1.1"></and><ci id="S5.T3.4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.4.1.1.1.1.m1.1.1.2">𝒥</ci><ci id="S5.T3.4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.4.1.1.1.1.m1.1.1.3">ℱ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.1.1.1.1.m1.1c">\mathcal{J\&amp;F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.1.1.1.1.m1.1d">caligraphic_J &amp; caligraphic_F</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.5.2.2.2.2" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T3.5.2.2.2.2.m1.1"><semantics id="S5.T3.5.2.2.2.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T3.5.2.2.2.2.m1.1.1" xref="S5.T3.5.2.2.2.2.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T3.5.2.2.2.2.m1.1b"><ci id="S5.T3.5.2.2.2.2.m1.1.1.cmml" xref="S5.T3.5.2.2.2.2.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.2.2.2.2.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.2.2.2.2.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.3.3" style="padding-left:1.7pt;padding-right:1.7pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T3.6.3.3.3.3.m1.1"><semantics id="S5.T3.6.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T3.6.3.3.3.3.m1.1.1" xref="S5.T3.6.3.3.3.3.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.6.3.3.3.3.m1.1b"><ci id="S5.T3.6.3.3.3.3.m1.1.1.cmml" xref="S5.T3.6.3.3.3.3.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.3.3.3.3.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.3.3.3.3.m1.1d">caligraphic_F</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.4.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.4.1.1" style="padding-left:1.7pt;padding-right:1.7pt;">MTTR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib8" title="">8</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.4.1.2" style="padding-left:1.7pt;padding-right:1.7pt;">2022</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.4.1.3" style="padding-left:1.7pt;padding-right:1.7pt;">31.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.4.1.4" style="padding-left:1.7pt;padding-right:1.7pt;">29.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.4.1.5" style="padding-left:1.7pt;padding-right:1.7pt;">33.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.5.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.5.2.1" style="padding-left:1.7pt;padding-right:1.7pt;">ReferFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.5.2.2" style="padding-left:1.7pt;padding-right:1.7pt;">2022</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.5.2.3" style="padding-left:1.7pt;padding-right:1.7pt;">32.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.5.2.4" style="padding-left:1.7pt;padding-right:1.7pt;">30.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.5.2.5" style="padding-left:1.7pt;padding-right:1.7pt;">35.6</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.6.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.6.3.1" style="padding-left:1.7pt;padding-right:1.7pt;">SOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib44" title="">44</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.6.3.2" style="padding-left:1.7pt;padding-right:1.7pt;">2023</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.6.3.3" style="padding-left:1.7pt;padding-right:1.7pt;">35.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.6.3.4" style="padding-left:1.7pt;padding-right:1.7pt;">33.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.6.3.5" style="padding-left:1.7pt;padding-right:1.7pt;">38.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.7.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.7.4.1" style="padding-left:1.7pt;padding-right:1.7pt;">OnlineRefer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib60" title="">60</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.7.4.2" style="padding-left:1.7pt;padding-right:1.7pt;">2023</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.7.4.3" style="padding-left:1.7pt;padding-right:1.7pt;">38.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.7.4.4" style="padding-left:1.7pt;padding-right:1.7pt;">34.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.7.4.5" style="padding-left:1.7pt;padding-right:1.7pt;">42.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.8.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.8.5.1" style="padding-left:1.7pt;padding-right:1.7pt;">SgMg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib47" title="">47</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.8.5.2" style="padding-left:1.7pt;padding-right:1.7pt;">2023</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.8.5.3" style="padding-left:1.7pt;padding-right:1.7pt;">36.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.8.5.4" style="padding-left:1.7pt;padding-right:1.7pt;">33.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.8.5.5" style="padding-left:1.7pt;padding-right:1.7pt;">38.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.9.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.9.6.1" style="padding-left:1.7pt;padding-right:1.7pt;">LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.6.3.3.9.6.2" style="padding-left:1.7pt;padding-right:1.7pt;">2023</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.9.6.3" style="padding-left:1.7pt;padding-right:1.7pt;">31.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.9.6.4" style="padding-left:1.7pt;padding-right:1.7pt;">29.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T3.6.3.3.9.6.5" style="padding-left:1.7pt;padding-right:1.7pt;">33.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.10.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.10.7.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.6.3.3.10.7.1.1">VideoLISA-3.8B (One-Token-Seg-All)</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.6.3.3.10.7.2" style="padding-left:1.7pt;padding-right:1.7pt;">2024</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.10.7.3" style="padding-left:1.7pt;padding-right:1.7pt;">45.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.10.7.4" style="padding-left:1.7pt;padding-right:1.7pt;">43.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T3.6.3.3.10.7.5" style="padding-left:1.7pt;padding-right:1.7pt;">47.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.3.3.11.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.6.3.3.11.8.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.6.3.3.11.8.1.1">VideoLISA-3.8B (Post-optimization)</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.6.3.3.11.8.2" style="padding-left:1.7pt;padding-right:1.7pt;">2024</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.6.3.3.11.8.3" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.6.3.3.11.8.3.1">47.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.6.3.3.11.8.4" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.6.3.3.11.8.4.1">45.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" id="S5.T3.6.3.3.11.8.5" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.6.3.3.11.8.5.1">49.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Motion-guided Video Object Segmentation</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">We further evaluate our model on motion-guided VOS using the MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite> benchmark.
Consistent with previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib24" title="">24</a>]</cite>, we evaluate our model’s performance on the validation set of the MeViS benchmark.
The results in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T3" title="Table 3 ‣ 5.2.1 Referring Video Object Segmentation ‣ 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate that our method achieves state-of-the-art performance in this benchmark, outperforming previous methods by a large margin.
We attribute this performance gap to our model’s adeptness in capturing temporal dynamics and cross-modal interaction, facilitated by the Sparse Dense Sampling-based temporal module and the One-Token-Seg-All training paradigm.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Reasoning Video Object Segmentation</h4>
<div class="ltx_para" id="S5.SS2.SSS3.p1">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">In Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T3" title="Table 3 ‣ 5.2.1 Referring Video Object Segmentation ‣ 5.2 Evaluation on Video Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare various methods on the newly organized ReasonVOS benchmark.
For traditional VOS methods, the metrics are evaluated using their released checkpoints pre-trained on the Ref-YouTube-VOS dataset.
This benchmark focuses on complex reasoning, temporal understanding, and segmentation temporal consistency, which present significant challenges for existing VOS methods and image-based reasoning segmentation methods.
It can be observed that most previous methods exhibit unsatisfactory performance on this benchmark.
Traditional RVOS methods, such as ReferFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib61" title="">61</a>]</cite>, excel at tracking moving objects but struggle with comprehending complex language expressions, particularly those requiring multi-step reasoning with world knowledge.
On the other hand, LLM-based models, like LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>, have better language understanding and reasoning capabilities.
The main reasons for the poor performance are: 1) incapability to capture temporal dynamics in the video, and 2) difficulty in segmenting temporally consistent masks.
In contrast, our VideoLISA model demonstrates remarkable performance, thanks to the advanced model design that considers all these crucial aspects.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation on Image Tasks</h3>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Reasoning segmentation results among ours and previous related works. ‘ft’ denotes using 239 reasoning segmentation image-instruction pairs to finetune the model.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:303.5pt;height:165.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-63.3pt,34.4pt) scale(0.705693148705681,0.705693148705681) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1.1" rowspan="3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text" id="S5.T4.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T4.1.1.1.1.2" style="padding-left:5.7pt;padding-right:5.7pt;">val</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6" id="S5.T4.1.1.1.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">test</th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" colspan="2" id="S5.T4.1.1.2.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">overall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" colspan="2" id="S5.T4.1.1.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;">short query</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" colspan="2" id="S5.T4.1.1.2.2.3" style="padding-left:5.7pt;padding-right:5.7pt;">long query</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="2" id="S5.T4.1.1.2.2.4" style="padding-left:5.7pt;padding-right:5.7pt;">overall</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">gIoU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.2" style="padding-left:5.7pt;padding-right:5.7pt;">cIoU</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">gIoU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.4" style="padding-left:5.7pt;padding-right:5.7pt;">cIoU</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.5" style="padding-left:5.7pt;padding-right:5.7pt;">gIoU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.6" style="padding-left:5.7pt;padding-right:5.7pt;">cIoU</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.7" style="padding-left:5.7pt;padding-right:5.7pt;">gIoU</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.3.3.8" style="padding-left:5.7pt;padding-right:5.7pt;">cIoU</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">OVSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib36" title="">36</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">18.6</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">18.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">15.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.6" style="padding-left:5.7pt;padding-right:5.7pt;">28.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.7" style="padding-left:5.7pt;padding-right:5.7pt;">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.8" style="padding-left:5.7pt;padding-right:5.7pt;">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="S5.T4.1.1.4.4.9" style="padding-left:5.7pt;padding-right:5.7pt;">20.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.5.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">GRES <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib38" title="">38</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.5.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">19.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">17.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.5.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">15.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.6" style="padding-left:5.7pt;padding-right:5.7pt;">22.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.5.5.7" style="padding-left:5.7pt;padding-right:5.7pt;">23.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.8" style="padding-left:5.7pt;padding-right:5.7pt;">21.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.9" style="padding-left:5.7pt;padding-right:5.7pt;">22.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.6.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">X-Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib73" title="">73</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">22.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.6.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">17.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.4" style="padding-left:5.7pt;padding-right:5.7pt;">20.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.6.6.5" style="padding-left:5.7pt;padding-right:5.7pt;">11.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.6" style="padding-left:5.7pt;padding-right:5.7pt;">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.6.6.7" style="padding-left:5.7pt;padding-right:5.7pt;">17.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.8" style="padding-left:5.7pt;padding-right:5.7pt;">21.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.9" style="padding-left:5.7pt;padding-right:5.7pt;">16.3</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.7.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">SEEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib74" title="">74</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.7.2" style="padding-left:5.7pt;padding-right:5.7pt;">25.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.7.7.3" style="padding-left:5.7pt;padding-right:5.7pt;">21.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.7.4" style="padding-left:5.7pt;padding-right:5.7pt;">20.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.7.7.5" style="padding-left:5.7pt;padding-right:5.7pt;">11.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.7.6" style="padding-left:5.7pt;padding-right:5.7pt;">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.7.7.7" style="padding-left:5.7pt;padding-right:5.7pt;">20.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.7.8" style="padding-left:5.7pt;padding-right:5.7pt;">24.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.7.9" style="padding-left:5.7pt;padding-right:5.7pt;">18.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.8.8.1" style="padding-left:5.7pt;padding-right:5.7pt;">Grounded-SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib55" title="">55</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.8.2" style="padding-left:5.7pt;padding-right:5.7pt;">26.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.8.8.3" style="padding-left:5.7pt;padding-right:5.7pt;">14.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.8.4" style="padding-left:5.7pt;padding-right:5.7pt;">17.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.8.8.5" style="padding-left:5.7pt;padding-right:5.7pt;">10.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.8.6" style="padding-left:5.7pt;padding-right:5.7pt;">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.8.8.7" style="padding-left:5.7pt;padding-right:5.7pt;">18.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.8.8" style="padding-left:5.7pt;padding-right:5.7pt;">21.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.8.9" style="padding-left:5.7pt;padding-right:5.7pt;">16.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.9.9.1" style="padding-left:5.7pt;padding-right:5.7pt;">LISA-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.2" style="padding-left:5.7pt;padding-right:5.7pt;">44.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.9.9.3" style="padding-left:5.7pt;padding-right:5.7pt;">46.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.4" style="padding-left:5.7pt;padding-right:5.7pt;">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.9.9.5" style="padding-left:5.7pt;padding-right:5.7pt;">34.4</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.6" style="padding-left:5.7pt;padding-right:5.7pt;">36.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.9.9.7" style="padding-left:5.7pt;padding-right:5.7pt;">34.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.8" style="padding-left:5.7pt;padding-right:5.7pt;">36.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.9" style="padding-left:5.7pt;padding-right:5.7pt;">34.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.10.10.1" style="padding-left:5.7pt;padding-right:5.7pt;">LISA-7B (ft) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.2" style="padding-left:5.7pt;padding-right:5.7pt;">52.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.10.10.3" style="padding-left:5.7pt;padding-right:5.7pt;">54.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.4" style="padding-left:5.7pt;padding-right:5.7pt;">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.10.10.5" style="padding-left:5.7pt;padding-right:5.7pt;">40.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.6" style="padding-left:5.7pt;padding-right:5.7pt;">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.10.10.7" style="padding-left:5.7pt;padding-right:5.7pt;">51.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.8" style="padding-left:5.7pt;padding-right:5.7pt;">47.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.9" style="padding-left:5.7pt;padding-right:5.7pt;">48.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.11.11.1" style="padding-left:5.7pt;padding-right:5.7pt;">LISA-13B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.11.2" style="padding-left:5.7pt;padding-right:5.7pt;">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.11.11.3" style="padding-left:5.7pt;padding-right:5.7pt;">46.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.11.4" style="padding-left:5.7pt;padding-right:5.7pt;">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.11.11.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.11.11.5.1">43.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.11.6" style="padding-left:5.7pt;padding-right:5.7pt;">46.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.11.11.7" style="padding-left:5.7pt;padding-right:5.7pt;">46.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.11.8" style="padding-left:5.7pt;padding-right:5.7pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.11.9" style="padding-left:5.7pt;padding-right:5.7pt;">45.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.12.12.1" style="padding-left:5.7pt;padding-right:5.7pt;">LISA-13B (ft) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.12.2" style="padding-left:5.7pt;padding-right:5.7pt;">56.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.12.12.3" style="padding-left:5.7pt;padding-right:5.7pt;">62.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.12.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.12.12.4.1">44.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.12.12.5" style="padding-left:5.7pt;padding-right:5.7pt;">42.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.12.6" style="padding-left:5.7pt;padding-right:5.7pt;">54.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.12.12.7" style="padding-left:5.7pt;padding-right:5.7pt;">54.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.12.8" style="padding-left:5.7pt;padding-right:5.7pt;">51.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.12.9" style="padding-left:5.7pt;padding-right:5.7pt;">51.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.1.1">VideoLISA-3.8B (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.2.1">61.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.3.1">67.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.4" style="padding-left:5.7pt;padding-right:5.7pt;">43.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.5" style="padding-left:5.7pt;padding-right:5.7pt;">42.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.6.1">56.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.7.1">57.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.8.1">53.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="S5.T4.1.1.13.13.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.13.13.9.1">54.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We use the image reasoning segmentation benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite> to assess the reasoning capability of our model.
During testing, we duplicate an image into multiple frames as a pseudo video.
The results are shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T4" title="Table 4 ‣ 5.3 Evaluation on Image Tasks ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.
We observe that our VideoLISA achieves state-of-the-art performance on both validation set and test set.
Remarkably, despite our model employing an LLM with significantly fewer parameters, it outperforms larger models, such as LISA-7B and LISA-13B, demonstrating its exceptional reasoning capability.
We attribute the impressive performance to the following aspects.
From a data perspective, VideoLISA benefits from joint training on both image and video datasets, allowing it to learn from more abundant and diverse supervision signals.
On the model aspect, the temporal learning module and the One-Token-Seg-All training encourage the model to leverage multiple frames of video simultaneously to conduct reasoning, rather than focusing on one image.
Even when generalizing to image tasks, where the video is simulated by an image, the model’s reasoning capability remains effective.
We provide more experiment results on image referring segmentation in the appendix.
These experiments demonstrate that our model is capable of image-based tasks, suggesting the potential for unifying image/video referring/reasoning segmentation tasks into a language-instructed object segmentation task solvable by a single VideoLISA model.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Studies</h3>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T6.5" style="width:238.5pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Ablation study on the temporal modeling architecture.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.5.5" style="width:411.9pt;height:134.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.7pt,5.1pt) scale(0.929304583335155,0.929304583335155) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.5.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.5.5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.5.5.5.6.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T6.5.5.5.6.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T6.5.5.5.6.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">ReasonSeg (val)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S5.T6.5.5.5.6.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">MeViS (valid_u)</th>
</tr>
<tr class="ltx_tr" id="S5.T6.4.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.4.4.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">giou</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.4.4.4.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">ciou</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.1.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T6.2.2.2.2.2.m2.1"><semantics id="S5.T6.2.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.2.2.2.2.2.m2.1.1" xref="S5.T6.2.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.2.m2.1b"><ci id="S5.T6.2.2.2.2.2.m2.1.1.cmml" xref="S5.T6.2.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.3.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T6.3.3.3.3.3.m1.1"><semantics id="S5.T6.3.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.3.3.3.3.3.m1.1.1" xref="S5.T6.3.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.3.3.3.m1.1b"><ci id="S5.T6.3.3.3.3.3.m1.1.1.cmml" xref="S5.T6.3.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.3.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.4.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T6.4.4.4.4.4.m1.1"><semantics id="S5.T6.4.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.4.4.4.4.4.m1.1.1" xref="S5.T6.4.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T6.4.4.4.4.4.m1.1b"><ci id="S5.T6.4.4.4.4.4.m1.1.1.cmml" xref="S5.T6.4.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.4.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.4.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.5.5.5.7.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T6.5.5.5.7.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B (Baseline)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.5.7.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">51.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.5.5.5.7.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">56.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.5.7.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.5.7.1.5" style="padding-left:8.0pt;padding-right:8.0pt;">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.5.5.5.7.1.6" style="padding-left:8.0pt;padding-right:8.0pt;">46.5</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.5.8.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.5.5.5.8.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B (Vid. FT)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.8.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.5.5.5.8.2.3" style="padding-left:8.0pt;padding-right:8.0pt;">56.2</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.8.2.4" style="padding-left:8.0pt;padding-right:8.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.8.2.5" style="padding-left:8.0pt;padding-right:8.0pt;">41.1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.8.2.6" style="padding-left:8.0pt;padding-right:8.0pt;">48.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.5.5.5.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (<math alttext="n" class="ltx_Math" display="inline" id="S5.T6.5.5.5.5.1.m1.1"><semantics id="S5.T6.5.5.5.5.1.m1.1a"><mi id="S5.T6.5.5.5.5.1.m1.1.1" xref="S5.T6.5.5.5.5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.T6.5.5.5.5.1.m1.1b"><ci id="S5.T6.5.5.5.5.1.m1.1.1.cmml" xref="S5.T6.5.5.5.5.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.5.5.5.5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.T6.5.5.5.5.1.m1.1d">italic_n</annotation></semantics></math>-frame)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.5.2" style="padding-left:8.0pt;padding-right:8.0pt;">55.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.5.5.5.5.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.5.5.3.1">60.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.5.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.5.5" style="padding-left:8.0pt;padding-right:8.0pt;">46.7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.5.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.5.9.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.5.5.5.9.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (ST Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib45" title="">45</a>]</cite>)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.9.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.5.5.5.9.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">59.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.9.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">50.8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.9.3.5" style="padding-left:8.0pt;padding-right:8.0pt;">47.8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.9.3.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.8</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.5.10.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T6.5.5.5.10.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Slow-Fast Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib25" title="">25</a>]</cite>)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.10.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">54.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.5.5.5.10.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">54.4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.10.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">50.2</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.10.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">47.2</td>
<td class="ltx_td ltx_align_center" id="S5.T6.5.5.5.10.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.1</td>
</tr>
<tr class="ltx_tr" id="S5.T6.5.5.5.11.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T6.5.5.5.11.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Sparse Dense Sampling)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.5.5.5.11.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.5.11.5.2.1">58.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.5.5.5.11.5.3" style="padding-left:8.0pt;padding-right:8.0pt;">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.5.5.5.11.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.5.11.5.4.1">51.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.5.5.5.11.5.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.5.11.5.5.1">48.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.5.5.5.11.5.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.5.5.5.11.5.6.1">54.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.T6.9" style="width:182.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 6: </span>Ablation study on the mask association <span class="ltx_text ltx_font_italic" id="S5.T6.9.6.1">i.e.,</span> tracking architecture.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.9.4" style="width:424.9pt;height:174.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(59.1pt,-24.3pt) scale(1.38518843699848,1.38518843699848) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.9.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.9.4.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.9.4.4.5.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S5.T6.9.4.4.5.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S5.T6.9.4.4.5.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">MeViS (valid_u)</th>
</tr>
<tr class="ltx_tr" id="S5.T6.9.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.7.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T6.6.1.1.1.1.m1.1"><semantics id="S5.T6.6.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.6.1.1.1.1.m1.1.1" xref="S5.T6.6.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T6.6.1.1.1.1.m1.1b"><ci id="S5.T6.6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.6.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.6.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.6.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T6.7.2.2.2.2.m2.1"><semantics id="S5.T6.7.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.7.2.2.2.2.m2.1.1" xref="S5.T6.7.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T6.7.2.2.2.2.m2.1b"><ci id="S5.T6.7.2.2.2.2.m2.1.1.cmml" xref="S5.T6.7.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.7.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.7.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.8.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="S5.T6.8.3.3.3.3.m1.1"><semantics id="S5.T6.8.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.8.3.3.3.3.m1.1.1" xref="S5.T6.8.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="S5.T6.8.3.3.3.3.m1.1b"><ci id="S5.T6.8.3.3.3.3.m1.1.1.cmml" xref="S5.T6.8.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.8.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.8.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.9.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S5.T6.9.4.4.4.4.m1.1"><semantics id="S5.T6.9.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.T6.9.4.4.4.4.m1.1.1" xref="S5.T6.9.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S5.T6.9.4.4.4.4.m1.1b"><ci id="S5.T6.9.4.4.4.4.m1.1.1.cmml" xref="S5.T6.9.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.9.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.9.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.9.4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.9.4.4.6.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B (Baseline)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.4.4.6.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.4.4.6.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.4.4.6.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">46.5</td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T6.9.4.4.7.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B + XMem<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.7.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">45.6</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.7.2.3" style="padding-left:8.0pt;padding-right:8.0pt;">41.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.7.2.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.3</td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T6.9.4.4.8.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (One-Token-Seg-One)</th>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.8.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">46.1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.8.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">42.4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.8.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.8</td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.4.4.9.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T6.9.4.4.9.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (One-Token-Seg-All)</th>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.9.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">51.7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.9.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">48.4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.4.4.9.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">54.9</td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.4.4.10.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T6.9.4.4.10.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Post optimization)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.9.4.4.10.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.9.4.4.10.5.2.1">54.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.9.4.4.10.5.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.9.4.4.10.5.3.1">50.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.9.4.4.10.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.9.4.4.10.5.4.1">58.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We conduct ablation studies on various design choices of our model.
The detailed experiment results are provided in the appendix.
Here, we summarize the main takeaways for each study.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.2.1">Ablation of temporal learning module.</span>
In this study of Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T6" title="Table 6 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>, we compare our Sparse Dense Sampling strategy with various design choices, including LISA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite> finetuned on videos, the most straightforward solution that directly concatenate visual tokens from multiple frames (<math alttext="n" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><mi id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><ci id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">italic_n</annotation></semantics></math>-frame), the strategy that pools along spatial and temporal dimension separately (ST Pooling), the strategy that pools each frame with different strengths in a slow fast pace.
The comparison of the experiment results shows that our Sparse Dense Sampling strategy outperforms other video-LLM training (sampling) strategies.
In addition to demonstrating the effectiveness of our method, this study also reveals the unique properties of the VOS task.
On the one hand, it requires detailed visual information for accurate segmentation, which makes the pooling-based strategies yield inferior results.
On the other hand, temporal information is also necessary for the model to comprehend motions and behaviors, as validated by the comparison between <math alttext="n" class="ltx_Math" display="inline" id="S5.SS4.p2.2.m2.1"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.2.m2.1d">italic_n</annotation></semantics></math>-frame and ours.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Ablation of temporal association module.</span>
The main takeaway of this part, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#S5.T6" title="Table 6 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>, lies in the comparison between our method and extensions of image-based LISA.
Specifically, we upgrade LISA to fit the VOS task by 1) (baseline) using one <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p3.1.2">&lt;SEG&gt;</span> token from the first frame to segment subsequent frames, 2) marrying LISA with an off-the-shelf tracking model.
With the help of the tracker, LISA performs clearly better than the baseline, while still performs worse than our method.
The main issue comes from that without perception of the video, the model is incapable of processing queries that are concerned with the full video content and temporal dynamic.
We further quantify the effect of the One-Token-Seg-All approach by contrasting it with a strawman setting, One-Token-Seg-One.
The comparison clearly validates the effect and necessity of the One-Token-Seg-All approach.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitation and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Despite the remarkable performance shown on various benchmarks, our model still has limitations.
We discuss them in this section to inspire future work.
First, our model exhibits deficiencies in computational efficiency.
Although we have already reduced the size of LLM to 3.8B, which is much smaller than previous models (7B, 13B), it still incurs a relatively high computational cost compared to previous work on video object segmentation.
In other words, introducing a MLLM brings remarkable understanding and reasoning ability to the model, while also inducing computational costs.
Exploring methods to achieve a trade-off between these aspects presents an interesting avenue for future research.
Second, we observe that state-of-the-art approaches to video object segmentation often employ dedicated video backbones to enhance performance.
Intuitively, using vision encoder pre-trained on videos would be beneficial for temporal-related tasks, such as object tracking.
However, integrating a video backbone while ensuring compatibility with LLM and SAM decoder is non-trivial.
In this work, we focus on empowering video segmentation tasks with reasoning capabilities based on LLM.
Exploring the integration of a video backbone represents a potential avenue for future research.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we propose VideoLISA, a video-based LLM designed for language instructed reasoning segmentation in videos.
It leverages the reasoning capabilities of LLM and employs SAM to produce segmentation masks.
To address the unique challenges in marrying LLM with video object segmentation, we propose two key innovations.
Firstly, a Sparse Dense Sampling strategy is designed to enable LLM to capture and understand temporal dynamics in videos.
By leveraging the inherent temporal redundancy property of videos, this strategy achieves a delicate balance between preserving visual details and temporal context, making it favorable for video object segmentation tasks.
Secondly, we propose a One-Token-Seg-All approach to achieve temporally consistent segmentation masks in the promptable mask decoding paradigm.
Based on a dedicated investigation of the potential and challenges associated with using a single unified prompt to segment video frames, we enhance this capability from both input information foundation and training objective perspectives.
Extensive ablation studies have investigated the function and rationale of the design choices of two modules.
Equipped with the two designs above, our VideoLISA model shows impressive capabilities in video object segmentation, particularly emphasizing complex reasoning, temporal understanding, and object tracking, as validated by our newly organized ReasonVOS benchmark.
Furthermore, it demonstrates notable performance on image segmentation tasks, positioning it as a potential unified model for language-instructed object segmentation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Phi-3 technical report: A highly capable language model locally on your phone.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.14219</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Burst: A benchmark for unifying object recognition, segmentation and tracking in video.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, pages 1674–1683, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Zechen Bai, Yuta Nakashima, and Noa Garcia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Explain me the painting: Multi-topic knowledgeable art description generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, pages 5422–5432, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Hallucination of multimodal large language models: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.18930</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and Hao Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Xmem++: Production-level video segmentation from few annotated frames.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pages 635–644, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, and Xavier Giro-i Nieto.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">A closer look at referring expressions for video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">Multimedia Tools and Applications</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 82(3):4419–4438, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Fully-convolutional siamese networks for object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part II 14</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, pages 850–865. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">End-to-end referring video object segmentation with multimodal transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 4985–4995, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Coco-stuff: Thing and stuff classes in context.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, pages 1209–1218, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Shikra: Unleashing multimodal llm’s referential dialogue magic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.15195</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Detect what you can: Detecting and representing objects using holistic models and body parts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib11.5.3" style="font-size:90%;">, pages 1971–1978, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Ho Kei Cheng and Alexander G Schwing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, pages 640–658. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Instructblip: Towards general-purpose vision-language models with instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Mevis: A large-scale benchmark for video segmentation with motion expressions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 2694–2703, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Mose: A new dataset for video object segmentation in complex scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, pages 20224–20234, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Vision-language transformer and query generation for referring segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 16321–16330, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Vision-language transformer and query generation for referring segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 16321–16330, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, and Si Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Language-bridged spatial-temporal interaction for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, pages 4964–4973, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Zihan Ding, Tianrui Hui, Shaofei Huang, Si Liu, Xuan Luo, Junshi Huang, and Xiaoming Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Progressive multimodal interaction network for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">The 3rd Large-scale Video Object Segmentation Challenge</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 8:6, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Ke Fan, Zechen Bai, Tianjun Xiao, Tong He, Max Horn, Yanwei Fu, Francesco Locatello, and Zheng Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Adaptive slot attention: Object discovery with dynamic slot number.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, pages 23062–23071, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Ke Fan, Zechen Bai, Tianjun Xiao, Dominik Zietlow, Max Horn, Zixu Zhao, Carl-Johann Simon-Gabriel, Mike Zheng Shou, Francesco Locatello, Bernt Schiele, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Unsupervised open-vocabulary object localization in videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">, pages 13747–13755, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Slowfast networks for video recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 6202–6211, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees GM Snoek.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Actor and action video segmentation from a sentence.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 5958–5966, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Shuting He and Henghui Ding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Decoupling static and hierarchical motion perception for referring video segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.03645</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Lita: Language instructed temporal-localization assistant.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2403.19046</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Chat-univi: Unified visual representation empowers large language models with image and video understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">arXiv preprint arXiv:2311.08046</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Referitgame: Referring to objects in photographs of natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, pages 787–798, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Anna Khoreva, Anna Rohrbach, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Video object segmentation with language referring expressions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">Computer Vision–ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2–6, 2018, Revised Selected Papers, Part IV 14</span><span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">, pages 123–141. Springer, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, pages 4015–4026, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Lisa: Reasoning segmentation via large language model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">arXiv preprint arXiv:2308.00692</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Dezhuang Li, Ruoqi Li, Lijun Wang, Yifan Wang, Jinqing Qi, Lu Zhang, Ting Liu, Qingquan Xu, and Huchuan Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">You only infer once: Cross-modal meta-transfer for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, volume 36, pages 1297–1305, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, pages 19730–19742. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Videochat: Chat-centric video understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">arXiv preprint arXiv:2305.06355</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, and Yan Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Robust referring video object segmentation with cyclic structural consensus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 22236–22245, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao Wei, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Rethinking cross-modal interaction from a top-down perspective for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">arXiv preprint arXiv:2106.01061</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Open-vocabulary semantic segmentation with mask-adapted clip.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Video-llava: Learning united visual representation by alignment before projection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">arXiv preprint arXiv:2311.10122</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Chang Liu, Henghui Ding, and Xudong Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Gres: Generalized referring expression segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Cross-modal progressive comprehension for referring segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">, 44(9):4761–4775, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Video swin transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">, pages 3202–3211, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.3.1" style="font-size:90%;">arXiv preprint arXiv:1711.05101</span><span class="ltx_text" id="bib.bib42.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Multi-task collaborative network for joint referring expression comprehension and segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib43.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib43.5.3" style="font-size:90%;">, pages 10034–10043, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, and Yujiu Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Soc: Semantic-assisted object cluster for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib44.4.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Video-chatgpt: Towards detailed video understanding via large vision and language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.05424</span><span class="ltx_text" id="bib.bib45.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Generation and comprehension of unambiguous object descriptions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib46.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib46.5.3" style="font-size:90%;">, pages 11–20, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Bo Miao, Mohammed Bennamoun, Yongsheng Gao, and Ajmal Mian.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">Spectrum-guided multi-granularity referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib47.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib47.5.3" style="font-size:90%;">, pages 920–930, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Large-scale video panoptic segmentation in the wild: A benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib48.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib48.5.3" style="font-size:90%;">, pages 21033–21043, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Pg-video-llava: Pixel grounding large video-language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.3.1" style="font-size:90%;">arXiv preprint arXiv:2311.13435</span><span class="ltx_text" id="bib.bib49.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">Kosmos-2: Grounding multimodal large language models to the world.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.14824</span><span class="ltx_text" id="bib.bib50.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Lingpeng Kong Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">Detgpt: Detect what you need via reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.3.1" style="font-size:90%;">arXiv preprint arXiv:2305.14167</span><span class="ltx_text" id="bib.bib51.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">Paco: Parts and attributes of common objects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib52.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib52.5.3" style="font-size:90%;">, pages 7141–7151, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">Llava++: Extending visual capabilities with llama-3 and phi-3, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib54.4.2" style="font-size:90%;">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</span><span class="ltx_text" id="bib.bib54.5.3" style="font-size:90%;">, pages 3505–3506, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">Grounded sam: Assembling open-world models for diverse visual tasks, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
Seonguk Seo, Joon-Young Lee, and Bohyung Han.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">Urvos: Unified referring video object segmentation network with a large-scale benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib56.4.2" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16</span><span class="ltx_text" id="bib.bib56.5.3" style="font-size:90%;">, pages 208–223. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
Li Wang, Zechen Bai, Yonghua Zhang, and Hongtao Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">Show, recall, and tell: Image captioning with recall mechanism.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</span><span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">, volume 34, pages 12176–12183, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">Cris: Clip-driven referring image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib58.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib58.5.3" style="font-size:90%;">, pages 11686–11695, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
Dongming Wu, Xingping Dong, Ling Shao, and Jianbing Shen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">Multi-level representation learning with semantic alignment for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib59.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib59.5.3" style="font-size:90%;">, pages 4996–5005, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">Onlinerefer: A simple online baseline for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib60.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib60.5.3" style="font-size:90%;">, pages 2761–2770, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.1.1" style="font-size:90%;">
Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.2.1" style="font-size:90%;">Language as queries for referring video object segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib61.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib61.5.3" style="font-size:90%;">, pages 4974–4984, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.1.1" style="font-size:90%;">
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.2.1" style="font-size:90%;">Pllava: Parameter-free llava extension from images to videos for video dense captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.3.1" style="font-size:90%;">arXiv preprint arXiv:2404.16994</span><span class="ltx_text" id="bib.bib62.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.1.1" style="font-size:90%;">
Linjie Yang, Yuchen Fan, and Ning Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.2.1" style="font-size:90%;">The 2nd large-scale video object segmentation challenge - video object segmentation track, October 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.1.1" style="font-size:90%;">
Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.2.1" style="font-size:90%;">Lavt: Language-aware vision transformer for referring image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib64.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib64.5.3" style="font-size:90%;">, pages 18155–18165, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.1.1" style="font-size:90%;">
Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.2.1" style="font-size:90%;">Video object segmentation and tracking: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.3.1" style="font-size:90%;">ACM Transactions on Intelligent Systems and Technology (TIST)</span><span class="ltx_text" id="bib.bib65.4.2" style="font-size:90%;">, 11(4):1–47, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.1.1" style="font-size:90%;">
Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.2.1" style="font-size:90%;">Cross-modal self-attention network for referring image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib66.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib66.5.3" style="font-size:90%;">, pages 10502–10511, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.1.1" style="font-size:90%;">
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.2.1" style="font-size:90%;">A survey on multimodal large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.13549</span><span class="ltx_text" id="bib.bib67.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.1.1" style="font-size:90%;">
Hang Zhang, Xin Li, and Lidong Bing.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.2.1" style="font-size:90%;">Video-llama: An instruction-tuned audio-visual language model for video understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.3.1" style="font-size:90%;">arXiv preprint arXiv:2306.02858</span><span class="ltx_text" id="bib.bib68.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.1.1" style="font-size:90%;">
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.2.1" style="font-size:90%;">Gpt4roi: Instruction tuning large language model on region-of-interest.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.3.1" style="font-size:90%;">arXiv preprint arXiv:2307.03601</span><span class="ltx_text" id="bib.bib69.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.1.1" style="font-size:90%;">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.2.1" style="font-size:90%;">Scene parsing through ade20k dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib70.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib70.5.3" style="font-size:90%;">, pages 633–641, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.1.1" style="font-size:90%;">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.2.1" style="font-size:90%;">Minigpt-4: Enhancing vision-language understanding with advanced large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib71.3.1" style="font-size:90%;">arXiv preprint arXiv:2304.10592</span><span class="ltx_text" id="bib.bib71.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.1.1" style="font-size:90%;">
Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.2.1" style="font-size:90%;">Tracking with human-intent reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.17448</span><span class="ltx_text" id="bib.bib72.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.1.1" style="font-size:90%;">
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.2.1" style="font-size:90%;">Generalized decoding for pixel, image, and language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib73.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib73.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.1.1" style="font-size:90%;">
Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.2.1" style="font-size:90%;">Segment everything everywhere all at once.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.3.1" style="font-size:90%;">arXiv:2304.06718</span><span class="ltx_text" id="bib.bib74.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Evaluation on Image Segmentation</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">In this section, we evaluate our VideoLISA model on the referring image segmentation task with three widely adopted benchmarks.
The results are presented in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T7" title="Table 7 ‣ A.1 Evaluation on Image Segmentation ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">7</span></a>.
On the refCOCO and refCOCO+ benchmarks, our VideoLISA achieves comparable performance with the image-based LISA model.
On the refCOCOg benchmark, VideoLISA outperforms previous methods, achieving state-of-the-art performance.
In general, the results of this experiment, along with the image reasoning segmentation results shown in the main paper, effectively demonstrate that our VideoLISA model is a strong competitor in image segmentation tasks.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Referring segmentation results (cIoU) among ours and existing methods.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T7.3.1.1.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.1.1.1.1" style="font-size:80%;">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="A1.T7.3.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.1.1.2.1" style="font-size:80%;">refCOCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="A1.T7.3.1.1.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.1.1.3.1" style="font-size:80%;">refCOCO+</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T7.3.1.1.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.1.1.4.1" style="font-size:80%;">refCOCOg</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.2.2">
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.1.1" style="font-size:80%;">val</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.2.1" style="font-size:80%;">testA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.3.1" style="font-size:80%;">testB</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.4.1" style="font-size:80%;">val</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.5.1" style="font-size:80%;">testA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.6.1" style="font-size:80%;">testB</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.7.1" style="font-size:80%;">val(U)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.2.2.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.2.2.8.1" style="font-size:80%;">test(U)</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.3.3.1.1" style="font-size:80%;">MCN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.3.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib43" title="">43</a><span class="ltx_text" id="A1.T7.3.3.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.2.1" style="font-size:80%;">62.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.3.1" style="font-size:80%;">64.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.4.1" style="font-size:80%;">59.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.5.1" style="font-size:80%;">50.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.6.1" style="font-size:80%;">55.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.7.1" style="font-size:80%;">44.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.8.1" style="font-size:80%;">49.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" id="A1.T7.3.3.3.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.3.3.9.1" style="font-size:80%;">49.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.4.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.4.4.1.1" style="font-size:80%;">VLT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib17" title="">17</a><span class="ltx_text" id="A1.T7.3.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.2.1" style="font-size:80%;">67.5</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.3.1" style="font-size:80%;">70.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.4.4.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.4.1" style="font-size:80%;">65.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.5.1" style="font-size:80%;">56.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.6.1" style="font-size:80%;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.4.4.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.7.1" style="font-size:80%;">50.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.8.1" style="font-size:80%;">55.0</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.4.4.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.4.4.9.1" style="font-size:80%;">57.7</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.5.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.5.5.1.1" style="font-size:80%;">CRIS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.5.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib58" title="">58</a><span class="ltx_text" id="A1.T7.3.5.5.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.2.1" style="font-size:80%;">70.5</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.3.1" style="font-size:80%;">73.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.5.5.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.4.1" style="font-size:80%;">66.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.5.1" style="font-size:80%;">62.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.6.1" style="font-size:80%;">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.5.5.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.7.1" style="font-size:80%;">53.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.8.1" style="font-size:80%;">59.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.5.5.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.5.5.9.1" style="font-size:80%;">60.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.6.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.6.6.1.1" style="font-size:80%;">LAVT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.6.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib64" title="">64</a><span class="ltx_text" id="A1.T7.3.6.6.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.2.1" style="font-size:80%;">72.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.3.1" style="font-size:80%;">75.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.6.6.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.4.1" style="font-size:80%;">68.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.5.1" style="font-size:80%;">62.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.6.1" style="font-size:80%;">68.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.6.6.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.7.1" style="font-size:80%;">55.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.8.1" style="font-size:80%;">61.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.6.6.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.6.6.9.1" style="font-size:80%;">62.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.7.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.7.7.1.1" style="font-size:80%;">ReLA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.7.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib38" title="">38</a><span class="ltx_text" id="A1.T7.3.7.7.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.7.7.2.1" style="font-size:80%;">73.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.7.7.3.1" style="font-size:80%;">76.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.7.7.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.7.7.4.1" style="font-size:80%;">70.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.7.5.1" style="font-size:80%;">66.0</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.7.6.1" style="font-size:80%;">71.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.7.7.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.7.7.7.1" style="font-size:80%;">57.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.7.7.8.1" style="font-size:80%;">65.0</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.7.7.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.7.7.9.1" style="font-size:80%;">66.0</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.8.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.8.8.1.1" style="font-size:80%;">X-Decoder </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.8.8.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib73" title="">73</a><span class="ltx_text" id="A1.T7.3.8.8.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.8.8.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.8.8.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.8.1" style="font-size:80%;">64.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.8.8.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.8.8.9.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.9.9.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.9.9.1.1" style="font-size:80%;">SEEM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.9.9.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib74" title="">74</a><span class="ltx_text" id="A1.T7.3.9.9.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.9.9.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.9.9.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.8.1" style="font-size:80%;">65.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.9.9.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.9.9.9.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T7.3.10.10.1" style="padding-left:8.5pt;padding-right:8.5pt;">
<span class="ltx_text" id="A1.T7.3.10.10.1.1" style="font-size:80%;">LISA-7B </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A1.T7.3.10.10.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a><span class="ltx_text" id="A1.T7.3.10.10.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.10.10.2.1" style="font-size:80%;">74.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.3.1" style="font-size:80%;">76.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.10.10.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.10.10.4.1" style="font-size:80%;">71.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.5.1" style="font-size:80%;">62.4</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.6.1" style="font-size:80%;">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.10.10.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.7.1" style="font-size:80%;">56.5</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.8.1" style="font-size:80%;">66.4</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.10.10.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.10.10.9.1" style="font-size:80%;">68.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.11.11.1.1" style="font-size:80%;">VideoLISA-3.8B (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.11.11.2.1" style="font-size:80%;">73.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.11.11.3.1" style="font-size:80%;">76.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.11.11.4.1" style="font-size:80%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.11.11.5.1" style="font-size:80%;">63.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.6" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.11.11.6.1" style="font-size:80%;">68.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.7" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text" id="A1.T7.3.11.11.7.1" style="font-size:80%;">56.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.8" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.11.11.8.1" style="font-size:80%;">68.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt ltx_border_t" id="A1.T7.3.11.11.9" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.3.11.11.9.1" style="font-size:80%;">68.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Ablation Studies</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this section, we present ablation studies on the temporal learning module (the Sparse Dense Sampling strategy), the temporal mask association module (the One-Token-Seg-All approach), and the training data recipe.
For fair comparisons, unless specified, all VideoLISA variants are uniformly trained with the same training setting: 1) 3k iterations in total, 2) the same training data recipe, 3) the same learning rate scheduler, and 4) the same training objective.
Three benchmarks are used for analysis:
1) ReasonSeg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib30" title="">30</a>]</cite> evaluates the reasoning ability of the model;
2) MeViS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib14" title="">14</a>]</cite> reflects the model’s performance on temporal learning;
and 3) Ref-DAVIS-17 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib28" title="">28</a>]</cite> measures the general RVOS capability of the model.
For evaluation on video benchmarks, the performance metrics of VideoLISA are computed using the simple One-Token-Seg-All approach without post-optimization, revealing the model’s essential capabilities.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Temporal Learning Module</h4>
<figure class="ltx_table" id="A1.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Ablation study on the temporal modeling architecture. *LISA-7B is reproduced using the released codebase.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T8.9" style="width:424.9pt;height:103.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.9pt,20.4pt) scale(0.716874744887984,0.716874744887984) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.9.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.9.9.10.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="A1.T8.9.9.10.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="A1.T8.9.9.10.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">ReasonSeg (val)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="A1.T8.9.9.10.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">MeViS (valid_u)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="A1.T8.9.9.10.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">Ref-DAVIS-17</th>
</tr>
<tr class="ltx_tr" id="A1.T8.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.8.8.8.9" style="padding-left:8.0pt;padding-right:8.0pt;">giou</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.8.8.8.10" style="padding-left:8.0pt;padding-right:8.0pt;">ciou</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T8.1.1.1.1.m1.1"><semantics id="A1.T8.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.1.1.1.1.m1.1.1" xref="A1.T8.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T8.1.1.1.1.m1.1b"><ci id="A1.T8.1.1.1.1.m1.1.1.cmml" xref="A1.T8.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T8.2.2.2.2.m2.1"><semantics id="A1.T8.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.2.2.2.2.m2.1.1" xref="A1.T8.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.2.2.2.2.m2.1b"><ci id="A1.T8.2.2.2.2.m2.1.1.cmml" xref="A1.T8.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T8.3.3.3.3.m1.1"><semantics id="A1.T8.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.3.3.3.3.m1.1.1" xref="A1.T8.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T8.3.3.3.3.m1.1b"><ci id="A1.T8.3.3.3.3.m1.1.1.cmml" xref="A1.T8.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T8.4.4.4.4.m1.1"><semantics id="A1.T8.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.4.4.4.4.m1.1.1" xref="A1.T8.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.4.4.4.4.m1.1b"><ci id="A1.T8.4.4.4.4.m1.1.1.cmml" xref="A1.T8.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.6.6.6.6" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T8.5.5.5.5.m1.1"><semantics id="A1.T8.5.5.5.5.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.5.5.5.5.m1.1.1" xref="A1.T8.5.5.5.5.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T8.5.5.5.5.m1.1b"><ci id="A1.T8.5.5.5.5.m1.1.1.cmml" xref="A1.T8.5.5.5.5.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.5.5.5.5.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.5.5.5.5.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T8.6.6.6.6.m2.1"><semantics id="A1.T8.6.6.6.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.6.6.6.6.m2.1.1" xref="A1.T8.6.6.6.6.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.6.6.6.6.m2.1b"><ci id="A1.T8.6.6.6.6.m2.1.1.cmml" xref="A1.T8.6.6.6.6.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.6.6.6.6.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.6.6.6.6.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.7.7.7.7" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T8.7.7.7.7.m1.1"><semantics id="A1.T8.7.7.7.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.7.7.7.7.m1.1.1" xref="A1.T8.7.7.7.7.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T8.7.7.7.7.m1.1b"><ci id="A1.T8.7.7.7.7.m1.1.1.cmml" xref="A1.T8.7.7.7.7.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.7.7.7.7.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.7.7.7.7.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.8.8.8.8" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T8.8.8.8.8.m1.1"><semantics id="A1.T8.8.8.8.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T8.8.8.8.8.m1.1.1" xref="A1.T8.8.8.8.8.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.8.8.8.8.m1.1b"><ci id="A1.T8.8.8.8.8.m1.1.1.cmml" xref="A1.T8.8.8.8.8.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.8.8.8.8.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.8.8.8.8.m1.1d">caligraphic_F</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.9.9.11.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T8.9.9.11.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B* (Baseline)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">51.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.9.9.11.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">56.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.5" style="padding-left:8.0pt;padding-right:8.0pt;">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.9.9.11.1.6" style="padding-left:8.0pt;padding-right:8.0pt;">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.7" style="padding-left:8.0pt;padding-right:8.0pt;">58.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.8" style="padding-left:8.0pt;padding-right:8.0pt;">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.9.9.11.1.9" style="padding-left:8.0pt;padding-right:8.0pt;">62.5</td>
</tr>
<tr class="ltx_tr" id="A1.T8.9.9.12.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T8.9.9.12.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B* (Vid. FT)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.12.2.3" style="padding-left:8.0pt;padding-right:8.0pt;">56.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.4" style="padding-left:8.0pt;padding-right:8.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.5" style="padding-left:8.0pt;padding-right:8.0pt;">41.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.12.2.6" style="padding-left:8.0pt;padding-right:8.0pt;">48.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.7" style="padding-left:8.0pt;padding-right:8.0pt;">58.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.8" style="padding-left:8.0pt;padding-right:8.0pt;">54.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.12.2.9" style="padding-left:8.0pt;padding-right:8.0pt;">62.5</td>
</tr>
<tr class="ltx_tr" id="A1.T8.9.9.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T8.9.9.9.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (<math alttext="n" class="ltx_Math" display="inline" id="A1.T8.9.9.9.1.m1.1"><semantics id="A1.T8.9.9.9.1.m1.1a"><mi id="A1.T8.9.9.9.1.m1.1.1" xref="A1.T8.9.9.9.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.T8.9.9.9.1.m1.1b"><ci id="A1.T8.9.9.9.1.m1.1.1.cmml" xref="A1.T8.9.9.9.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.9.9.9.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.T8.9.9.9.1.m1.1d">italic_n</annotation></semantics></math>-frame)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.2" style="padding-left:8.0pt;padding-right:8.0pt;">55.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.9.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.9.3.1">60.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.5" style="padding-left:8.0pt;padding-right:8.0pt;">46.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.9.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.0</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.7" style="padding-left:8.0pt;padding-right:8.0pt;">65.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.8" style="padding-left:8.0pt;padding-right:8.0pt;">62.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.9.9" style="padding-left:8.0pt;padding-right:8.0pt;">68.9</td>
</tr>
<tr class="ltx_tr" id="A1.T8.9.9.13.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T8.9.9.13.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Spatial &amp; Temporal Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib45" title="">45</a>]</cite>)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.13.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">59.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">50.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.5" style="padding-left:8.0pt;padding-right:8.0pt;">47.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.13.3.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.7" style="padding-left:8.0pt;padding-right:8.0pt;">62.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.8" style="padding-left:8.0pt;padding-right:8.0pt;">58.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.13.3.9" style="padding-left:8.0pt;padding-right:8.0pt;">66.3</td>
</tr>
<tr class="ltx_tr" id="A1.T8.9.9.14.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T8.9.9.14.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Slow-Fast Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib25" title="">25</a>]</cite>)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">54.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.14.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">54.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">50.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">47.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.9.9.14.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">53.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.7" style="padding-left:8.0pt;padding-right:8.0pt;">65.7</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.8" style="padding-left:8.0pt;padding-right:8.0pt;">62.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.9.9.14.4.9" style="padding-left:8.0pt;padding-right:8.0pt;">69.4</td>
</tr>
<tr class="ltx_tr" id="A1.T8.9.9.15.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="A1.T8.9.9.15.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Sparse Dense Sampling)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.2.1">58.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.9.9.15.5.3" style="padding-left:8.0pt;padding-right:8.0pt;">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.4.1">51.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.5.1">48.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.9.9.15.5.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.6.1">54.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.7.1">67.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.8.1">64.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.9.9.15.5.9" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T8.9.9.15.5.9.1">71.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">In Tab <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T8" title="Table 8 ‣ A.2.1 Temporal Learning Module ‣ A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">8</span></a>, we compare various strategies for temporal learning.
The first row shows the vanilla LISA-7B model, which only focuses on image-based reasoning segmentation.
To infer LISA-7B on video data, we employ a similar One-Token-Seg-All strategy, where the <span class="ltx_text ltx_font_typewriter" id="A1.SS2.SSS1.p1.1.1">&lt;TRK&gt;</span> token (called <span class="ltx_text ltx_font_typewriter" id="A1.SS2.SSS1.p1.1.2">[SEG]</span> in the original LISA) comes from the first frame.
This performance serves as a baseline for comparison.
In the second row, we construct a naive solution to adapt LISA to the video domain.
Specifically, we finetune LISA-7B on the aforementioned video segmentation datasets.
The results show that simply finetuning on video data does not significantly improve video performance and even hurts the performance on image reasoning segmentation.
Although training on video datasets may enhance the model’s ability to understand temporally related text queries, it still lacks temporal modeling ability from video data, resulting in undesirable performance.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS1.p2">
<p class="ltx_p" id="A1.SS2.SSS1.p2.4">Next, we compare various temporal learning strategies within the VideoLISA framework using the One-Token-Seg-All training objective.
We first experiment with a straightforward video training strategy, called <math alttext="n" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.1.m1.1"><semantics id="A1.SS2.SSS1.p2.1.m1.1a"><mi id="A1.SS2.SSS1.p2.1.m1.1.1" xref="A1.SS2.SSS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p2.1.m1.1b"><ci id="A1.SS2.SSS1.p2.1.m1.1.1.cmml" xref="A1.SS2.SSS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p2.1.m1.1d">italic_n</annotation></semantics></math>-frame, which directly concatenates the visual features from <math alttext="n" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.2.m2.1"><semantics id="A1.SS2.SSS1.p2.2.m2.1a"><mi id="A1.SS2.SSS1.p2.2.m2.1.1" xref="A1.SS2.SSS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p2.2.m2.1b"><ci id="A1.SS2.SSS1.p2.2.m2.1.1.cmml" xref="A1.SS2.SSS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p2.2.m2.1d">italic_n</annotation></semantics></math> sampled frames as input to the large language model.
In our implementation, the value of <math alttext="n" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.3.m3.1"><semantics id="A1.SS2.SSS1.p2.3.m3.1a"><mi id="A1.SS2.SSS1.p2.3.m3.1.1" xref="A1.SS2.SSS1.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p2.3.m3.1b"><ci id="A1.SS2.SSS1.p2.3.m3.1.1.cmml" xref="A1.SS2.SSS1.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p2.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p2.3.m3.1d">italic_n</annotation></semantics></math> is set to the same as <math alttext="T_{\rm dense}" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p2.4.m4.1"><semantics id="A1.SS2.SSS1.p2.4.m4.1a"><msub id="A1.SS2.SSS1.p2.4.m4.1.1" xref="A1.SS2.SSS1.p2.4.m4.1.1.cmml"><mi id="A1.SS2.SSS1.p2.4.m4.1.1.2" xref="A1.SS2.SSS1.p2.4.m4.1.1.2.cmml">T</mi><mi id="A1.SS2.SSS1.p2.4.m4.1.1.3" xref="A1.SS2.SSS1.p2.4.m4.1.1.3.cmml">dense</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p2.4.m4.1b"><apply id="A1.SS2.SSS1.p2.4.m4.1.1.cmml" xref="A1.SS2.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS2.SSS1.p2.4.m4.1.1.1.cmml" xref="A1.SS2.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="A1.SS2.SSS1.p2.4.m4.1.1.2.cmml" xref="A1.SS2.SSS1.p2.4.m4.1.1.2">𝑇</ci><ci id="A1.SS2.SSS1.p2.4.m4.1.1.3.cmml" xref="A1.SS2.SSS1.p2.4.m4.1.1.3">dense</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p2.4.m4.1c">T_{\rm dense}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p2.4.m4.1d">italic_T start_POSTSUBSCRIPT roman_dense end_POSTSUBSCRIPT</annotation></semantics></math> for comparison.
As shown in the third row, we observe that with this simple strategy, the model achieves surprisingly good performance across the benchmarks, significantly outperforming LISA-based methods.
Exposure to multiple frames enables the model to perceive temporal dynamics, while the One-Token-Seg-All training objective supervises the model in learning mask association over the temporal dimension, thereby improving multimodal reasoning and temporal consistency in segmentation.
However, due to computational limits, it is prohibitive to include too many frames as it would result in a large number of tokens.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS1.p3">
<p class="ltx_p" id="A1.SS2.SSS1.p3.1">To enable long temporal context perception, we experiment with several pooling strategies, including pooling along the spatial and temporal dimensions separately <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib45" title="">45</a>]</cite>, pooling with different strengths in a slow-fast pace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib25" title="">25</a>]</cite>, and our Sparse Dense Sampling strategy.
The comparison in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T8" title="Table 8 ‣ A.2.1 Temporal Learning Module ‣ A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">8</span></a> reveals that our Sparse Dense Sampling strategy is a more favorable setting among the experiment designs.
The first spatial-temporal pooling strategy eliminates valuable visual details of the video, resulting in inferior performance.
The second slow-fast paced pooling strategy is similar to ours in implementation.
The key difference is that it applies pooling to all frames, albeit with different strengths, while ours preserves the full visual details of the dense frames.
This difference leads to the observed performance gap.
We argue that this difference is significant due to the unique nature of the video object segmentation task.
On one hand, it requires detailed visual information for accurate segmentation, causing pooling-based strategies to yield inferior results.
On the other hand, the temporal dimension is also necessary for the model to comprehend motions and behaviors, as validated by the comparison between the <math alttext="n" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p3.1.m1.1"><semantics id="A1.SS2.SSS1.p3.1.m1.1a"><mi id="A1.SS2.SSS1.p3.1.m1.1.1" xref="A1.SS2.SSS1.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p3.1.m1.1b"><ci id="A1.SS2.SSS1.p3.1.m1.1.1.cmml" xref="A1.SS2.SSS1.p3.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p3.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p3.1.m1.1d">italic_n</annotation></semantics></math>-frame approach and ours.
Although recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib62" title="">62</a>]</cite> show that applying pooling to visual tokens does not affect the performance of VQA tasks, our experiments validate that preserving the full resolution of visual tokens is necessary for dense prediction tasks, and applying pooling leads to sub-optimal results.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Ablation study on the mask association <span class="ltx_text ltx_font_italic" id="A1.T9.10.1">i.e.,</span> tracking architecture. *LISA-7B is reproduced using the released codebase.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T9.8" style="width:424.9pt;height:129.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(5.1pt,-1.5pt) scale(1.02451252297306,1.02451252297306) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T9.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.8.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.8.8.9.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="A1.T9.8.8.9.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="A1.T9.8.8.9.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">MeViS (valid_u)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="A1.T9.8.8.9.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">Ref-DAVIS-17</th>
</tr>
<tr class="ltx_tr" id="A1.T9.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T9.1.1.1.1.m1.1"><semantics id="A1.T9.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.1.1.1.1.m1.1.1" xref="A1.T9.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T9.1.1.1.1.m1.1b"><ci id="A1.T9.1.1.1.1.m1.1.1.cmml" xref="A1.T9.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T9.2.2.2.2.m2.1"><semantics id="A1.T9.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.2.2.2.2.m2.1.1" xref="A1.T9.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.2.2.2.2.m2.1b"><ci id="A1.T9.2.2.2.2.m2.1.1.cmml" xref="A1.T9.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T9.3.3.3.3.m1.1"><semantics id="A1.T9.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.3.3.3.3.m1.1.1" xref="A1.T9.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T9.3.3.3.3.m1.1b"><ci id="A1.T9.3.3.3.3.m1.1.1.cmml" xref="A1.T9.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T9.4.4.4.4.m1.1"><semantics id="A1.T9.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.4.4.4.4.m1.1.1" xref="A1.T9.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.4.4.4.4.m1.1b"><ci id="A1.T9.4.4.4.4.m1.1.1.cmml" xref="A1.T9.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.6.6.6.6" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T9.5.5.5.5.m1.1"><semantics id="A1.T9.5.5.5.5.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.5.5.5.5.m1.1.1" xref="A1.T9.5.5.5.5.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T9.5.5.5.5.m1.1b"><ci id="A1.T9.5.5.5.5.m1.1.1.cmml" xref="A1.T9.5.5.5.5.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.5.5.5.5.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.5.5.5.5.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T9.6.6.6.6.m2.1"><semantics id="A1.T9.6.6.6.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.6.6.6.6.m2.1.1" xref="A1.T9.6.6.6.6.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.6.6.6.6.m2.1b"><ci id="A1.T9.6.6.6.6.m2.1.1.cmml" xref="A1.T9.6.6.6.6.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.6.6.6.6.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.6.6.6.6.m2.1d">caligraphic_F</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.7.7.7.7" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T9.7.7.7.7.m1.1"><semantics id="A1.T9.7.7.7.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.7.7.7.7.m1.1.1" xref="A1.T9.7.7.7.7.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T9.7.7.7.7.m1.1b"><ci id="A1.T9.7.7.7.7.m1.1.1.cmml" xref="A1.T9.7.7.7.7.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.7.7.7.7.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.7.7.7.7.m1.1d">caligraphic_J</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.8.8.8.8" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T9.8.8.8.8.m1.1"><semantics id="A1.T9.8.8.8.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T9.8.8.8.8.m1.1.1" xref="A1.T9.8.8.8.8.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.8.8.8.8.m1.1b"><ci id="A1.T9.8.8.8.8.m1.1.1.cmml" xref="A1.T9.8.8.8.8.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.8.8.8.8.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T9.8.8.8.8.m1.1d">caligraphic_F</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.8.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.8.8.10.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B* (Baseline)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.8.8.10.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.8.8.10.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.8.8.10.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.8.8.10.1.5" style="padding-left:8.0pt;padding-right:8.0pt;">58.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.8.8.10.1.6" style="padding-left:8.0pt;padding-right:8.0pt;">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.8.8.10.1.7" style="padding-left:8.0pt;padding-right:8.0pt;">62.5</td>
</tr>
<tr class="ltx_tr" id="A1.T9.8.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T9.8.8.11.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">LISA-7B* + XMem<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib12" title="">12</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.11.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">45.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.11.2.3" style="padding-left:8.0pt;padding-right:8.0pt;">41.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.8.8.11.2.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.11.2.5" style="padding-left:8.0pt;padding-right:8.0pt;">62.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.11.2.6" style="padding-left:8.0pt;padding-right:8.0pt;">60.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.11.2.7" style="padding-left:8.0pt;padding-right:8.0pt;">65.5</td>
</tr>
<tr class="ltx_tr" id="A1.T9.8.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T9.8.8.12.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (One-Token-Seg-One)</th>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.12.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">46.1</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.12.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.8.8.12.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">49.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.12.3.5" style="padding-left:8.0pt;padding-right:8.0pt;">60.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.12.3.6" style="padding-left:8.0pt;padding-right:8.0pt;">56.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.12.3.7" style="padding-left:8.0pt;padding-right:8.0pt;">63.8</td>
</tr>
<tr class="ltx_tr" id="A1.T9.8.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T9.8.8.13.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (One-Token-Seg-All)</th>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.13.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">51.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.13.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.8.8.13.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">54.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.13.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">67.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.13.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">64.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.13.4.7" style="padding-left:8.0pt;padding-right:8.0pt;">71.3</td>
</tr>
<tr class="ltx_tr" id="A1.T9.8.8.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A1.T9.8.8.14.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">VideoLISA-3.8B (Post optimization)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.8.8.14.5.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.2.1">54.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.8.8.14.5.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.3.1">50.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.8.8.14.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.4.1">58.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.8.8.14.5.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.5.1">68.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.8.8.14.5.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.6.1">65.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.8.8.14.5.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T9.8.8.14.5.7.1">72.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Temporal Association Module</h4>
<div class="ltx_para" id="A1.SS2.SSS2.p1">
<p class="ltx_p" id="A1.SS2.SSS2.p1.1">In Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T9" title="Table 9 ‣ A.2.1 Temporal Learning Module ‣ A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">9</span></a>, we compare the design choices for the temporal association module, <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS2.p1.1.1">i.e.,</span> tracking.
As in previous comparisons, the One-Token-Seg-All strategy in LISA-7B serves as the baseline in the first row.
One straightforward solution based on LISA is to plug an off-the-shelf tracker into the model.
During inference, LISA outputs the segmentation mask of the first frame based on language instruction.
The tracker then tracks the segmented object through the video, yielding segmentation masks for the subsequent frames.
Specifically, we adopt the popular XMem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib12" title="">12</a>]</cite> model as the tracker, as shown in the second row of the table.
Compared to VideoLISA (both One-Token-Seg-All and post-optimization), LISA+XMem achieves worse performance on these benchmarks.
This validates that simply plugging an existing tracker into an image-based reasoning segmentation model does not address the problem of video reasoning segmentation.
The vital issue is that the LLM in charge of perception and reasoning does not capture the entire video content, making its predictions nonsensical.
In contrast, VideoLISA’s temporal learning module and dedicated training objective enrich the <span class="ltx_text ltx_font_typewriter" id="A1.SS2.SSS2.p1.1.2">&lt;TRK&gt;</span> token with semantic information, enabling it to find the target object across all frames.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS2.p2">
<p class="ltx_p" id="A1.SS2.SSS2.p2.1">To quantify the effect of the One-Token-Seg-All training objective, we build a strawman setting named One-Token-Seg-One.
In this setting, the video content is captured with the temporal learning module, but the training only supervises the segmentation of one frame.
The comparison is shown in the third and fourth rows of Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T9" title="Table 9 ‣ A.2.1 Temporal Learning Module ‣ A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">9</span></a>.
We observe that the slight difference in supervision leads to a significant performance gap in the benchmarks.
This indicates that the One-Token-Seg-All training objective is essential for achieving temporally consistent masks.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS2.p3">
<p class="ltx_p" id="A1.SS2.SSS2.p3.1">In the last row, we present post-optimization, which leverages both the reasoning and segmentation abilities of VideoLISA and a mature tracking model.
Specifically, we first use VideoLISA to produce the <span class="ltx_text ltx_font_typewriter" id="A1.SS2.SSS2.p3.1.1">&lt;TRK&gt;</span> token and then use it to segment the sampled dense frames.
Then, the post-optimization model, implemented as XMem++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#bib.bib5" title="">5</a>]</cite>, takes dense frames and their segmentation masks as references in its permanent memory and infers the masks for the remaining frames.
The reasons for choosing the dense frames as the mask reference include: 1) the dense frames are seen by VideoLISA, thus their masks should be more accurate than those of other unseen frames, and 2) the dense frames are intentionally sampled from the video in a uniform manner, naturally providing a long-range yet diverse reference signal.
By leveraging the association ability from the post-optimization step, VideoLISA achieves the best performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span>Ablation on Training Data</h4>
<figure class="ltx_table" id="A1.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Ablation study on the training data recipe.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T10.8" style="width:424.9pt;height:101.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.0pt,12.6pt) scale(0.800440763102594,0.800440763102594) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T10.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T10.8.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4" id="A1.T10.8.8.9.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">Training Data</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="A1.T10.8.8.9.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">ReasonSeg (val)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="A1.T10.8.8.9.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">MeViS (valid_u)</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="A1.T10.8.8.9.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">Ref-DAVIS-17</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.8.9" style="padding-left:8.0pt;padding-right:8.0pt;">Image Seg.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.8.10" style="padding-left:8.0pt;padding-right:8.0pt;">Video Seg.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.8.11" style="padding-left:8.0pt;padding-right:8.0pt;">Image QA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.8.8.8.12" style="padding-left:8.0pt;padding-right:8.0pt;">Video QA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.8.13" style="padding-left:8.0pt;padding-right:8.0pt;">giou</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.8.8.8.14" style="padding-left:8.0pt;padding-right:8.0pt;">ciou</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.2.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T10.1.1.1.1.m1.1"><semantics id="A1.T10.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.1.1.1.1.m1.1.1" xref="A1.T10.1.1.1.1.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T10.1.1.1.1.m1.1b"><ci id="A1.T10.1.1.1.1.m1.1.1.cmml" xref="A1.T10.1.1.1.1.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.1.1.1.1.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.1.1.1.1.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T10.2.2.2.2.m2.1"><semantics id="A1.T10.2.2.2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.2.2.2.2.m2.1.1" xref="A1.T10.2.2.2.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T10.2.2.2.2.m2.1b"><ci id="A1.T10.2.2.2.2.m2.1.1.cmml" xref="A1.T10.2.2.2.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.2.2.2.2.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.2.2.2.2.m2.1d">caligraphic_F</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T10.3.3.3.3.m1.1"><semantics id="A1.T10.3.3.3.3.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.3.3.3.3.m1.1.1" xref="A1.T10.3.3.3.3.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T10.3.3.3.3.m1.1b"><ci id="A1.T10.3.3.3.3.m1.1.1.cmml" xref="A1.T10.3.3.3.3.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.3.3.3.3.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.3.3.3.3.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.4.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T10.4.4.4.4.m1.1"><semantics id="A1.T10.4.4.4.4.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.4.4.4.4.m1.1.1" xref="A1.T10.4.4.4.4.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T10.4.4.4.4.m1.1b"><ci id="A1.T10.4.4.4.4.m1.1.1.cmml" xref="A1.T10.4.4.4.4.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.4.4.4.4.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.4.4.4.4.m1.1d">caligraphic_F</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.6.6.6.6" style="padding-left:8.0pt;padding-right:8.0pt;">
<math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T10.5.5.5.5.m1.1"><semantics id="A1.T10.5.5.5.5.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.5.5.5.5.m1.1.1" xref="A1.T10.5.5.5.5.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T10.5.5.5.5.m1.1b"><ci id="A1.T10.5.5.5.5.m1.1.1.cmml" xref="A1.T10.5.5.5.5.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.5.5.5.5.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.5.5.5.5.m1.1d">caligraphic_J</annotation></semantics></math>&amp;<math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T10.6.6.6.6.m2.1"><semantics id="A1.T10.6.6.6.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.6.6.6.6.m2.1.1" xref="A1.T10.6.6.6.6.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T10.6.6.6.6.m2.1b"><ci id="A1.T10.6.6.6.6.m2.1.1.cmml" xref="A1.T10.6.6.6.6.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.6.6.6.6.m2.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.6.6.6.6.m2.1d">caligraphic_F</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.7.7.7.7" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="A1.T10.7.7.7.7.m1.1"><semantics id="A1.T10.7.7.7.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.7.7.7.7.m1.1.1" xref="A1.T10.7.7.7.7.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="A1.T10.7.7.7.7.m1.1b"><ci id="A1.T10.7.7.7.7.m1.1.1.cmml" xref="A1.T10.7.7.7.7.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.7.7.7.7.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.7.7.7.7.m1.1d">caligraphic_J</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.8.8" style="padding-left:8.0pt;padding-right:8.0pt;"><math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="A1.T10.8.8.8.8.m1.1"><semantics id="A1.T10.8.8.8.8.m1.1a"><mi class="ltx_font_mathcaligraphic" id="A1.T10.8.8.8.8.m1.1.1" xref="A1.T10.8.8.8.8.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="A1.T10.8.8.8.8.m1.1b"><ci id="A1.T10.8.8.8.8.m1.1.1.cmml" xref="A1.T10.8.8.8.8.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T10.8.8.8.8.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="A1.T10.8.8.8.8.m1.1d">caligraphic_F</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.10.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="A1.T10.8.8.10.2.2" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_border_t" id="A1.T10.8.8.10.2.3" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T10.8.8.10.2.4" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.5" style="padding-left:8.0pt;padding-right:8.0pt;">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.8.8.10.2.6" style="padding-left:8.0pt;padding-right:8.0pt;">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.7" style="padding-left:8.0pt;padding-right:8.0pt;">46.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.8" style="padding-left:8.0pt;padding-right:8.0pt;">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.8.8.10.2.9" style="padding-left:8.0pt;padding-right:8.0pt;">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.10" style="padding-left:8.0pt;padding-right:8.0pt;">62.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.11" style="padding-left:8.0pt;padding-right:8.0pt;">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.8.8.10.2.12" style="padding-left:8.0pt;padding-right:8.0pt;">66.3</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.11.3">
<td class="ltx_td" id="A1.T10.8.8.11.3.1" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td" id="A1.T10.8.8.11.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_border_r" id="A1.T10.8.8.11.3.4" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.5" style="padding-left:8.0pt;padding-right:8.0pt;">41.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.11.3.6" style="padding-left:8.0pt;padding-right:8.0pt;">46.5</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.7" style="padding-left:8.0pt;padding-right:8.0pt;">49.3</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.8" style="padding-left:8.0pt;padding-right:8.0pt;">45.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.11.3.9" style="padding-left:8.0pt;padding-right:8.0pt;">52.8</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.10" style="padding-left:8.0pt;padding-right:8.0pt;">66.0</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.11" style="padding-left:8.0pt;padding-right:8.0pt;">62.7</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.11.3.12" style="padding-left:8.0pt;padding-right:8.0pt;">69.3</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.12.4">
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td" id="A1.T10.8.8.12.4.3" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_border_r" id="A1.T10.8.8.12.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.12.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">60.0</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.7" style="padding-left:8.0pt;padding-right:8.0pt;">51.7</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.8" style="padding-left:8.0pt;padding-right:8.0pt;">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.12.4.9" style="padding-left:8.0pt;padding-right:8.0pt;">54.9</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.10" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.12.4.10.1">67.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.11" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.12.4.11.1">64.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.12.4.12" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.12.4.12.1">71.3</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.13.5">
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.2" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.3" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_border_r" id="A1.T10.8.8.13.5.4" style="padding-left:8.0pt;padding-right:8.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.5" style="padding-left:8.0pt;padding-right:8.0pt;">56.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.13.5.6" style="padding-left:8.0pt;padding-right:8.0pt;">65.6</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.7" style="padding-left:8.0pt;padding-right:8.0pt;">49.8</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.8" style="padding-left:8.0pt;padding-right:8.0pt;">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.8.8.13.5.9" style="padding-left:8.0pt;padding-right:8.0pt;">52.9</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.10" style="padding-left:8.0pt;padding-right:8.0pt;">66.8</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.11" style="padding-left:8.0pt;padding-right:8.0pt;">63.4</td>
<td class="ltx_td ltx_align_center" id="A1.T10.8.8.13.5.12" style="padding-left:8.0pt;padding-right:8.0pt;">70.3</td>
</tr>
<tr class="ltx_tr" id="A1.T10.8.8.14.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.1" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.2" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.3" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.8.8.14.6.4" style="padding-left:8.0pt;padding-right:8.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.14.6.5.1">60.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.8.8.14.6.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.14.6.6.1">67.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.7" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.14.6.7.1">52.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.14.6.8.1">49.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.8.8.14.6.9" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T10.8.8.14.6.9.1">54.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.10" style="padding-left:8.0pt;padding-right:8.0pt;">66.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.11" style="padding-left:8.0pt;padding-right:8.0pt;">63.5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.8.8.14.6.12" style="padding-left:8.0pt;padding-right:8.0pt;">70.3</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="A1.SS2.SSS3.p1">
<p class="ltx_p" id="A1.SS2.SSS3.p1.1">Our model undergoes joint training on both image and video datasets.
An investigation of the training data is presented in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.T10" title="Table 10 ‣ A.2.3 Ablation on Training Data ‣ A.2 Ablation Studies ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">10</span></a>.
We first observe that with image-only segmentation datasets, the model achieves decent performance in reasoning segmentation.
However, the performance on video benchmarks is unsatisfactory, possibly due to insufficient temporal information in the training data.
When using video-only segmentation settings, compared to image-only, the performance on video benchmarks increases significantly.
Simultaneously, the model experiences a dramatic drop in performance in reasoning segmentation.
This comparison demonstrates that video training is helpful for the VOS task, while image data is also necessary to exploit the reasoning ability of the model.
When combining the image and video segmentation datasets, the model yields remarkable performance across various benchmarks.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS3.p2">
<p class="ltx_p" id="A1.SS2.SSS3.p2.1">Next, we additionally explore the effect of using visual question answering (VQA) data.
We first observe that after adding Image-QA data into training, the model experiences a slight performance drop in all benchmarks.
Then, with the involvement of Video-QA data, the model achieves much better performance on the reasoning segmentation benchmark.
Among the two video benchmarks, compared to the model trained with segmentation-only data, this model shows slightly better performance on the MeViS offline validation set yet worse performance on Ref-DAVIS-17.
Intuitively, VQA data has the potential to enhance the model’s reasoning ability.
However, it may also make the multi-task training more challenging, as revealed by the performance fluctuation among different benchmarks. Maintaining the compatibility of different types of training data and tasks is left for future work.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Qualitative Results</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F3" title="Figure 3 ‣ A.3 Qualitative Results ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">3</span></a>, we use a representative video to showcase the versatile language-instructed reasoning capabilities of our model.
VideoLISA can do segmentation in videos via language referring, world knowledge reasoning, and video temporal reasoning.
Additionally, the model can discern subtle differences in language instructions and is not biased to salient or moving objects.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F6" title="Figure 6 ‣ A.4 Failure Cases ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">6</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F7" title="Figure 7 ‣ A.4 Failure Cases ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">7</span></a>, we provide more abundant qualitative examples of VideoLISA.
The red text is only for illustration purposes.
No special prompting techniques were employed.
It’s important to note that these examples were generated using the One-Token-Seg-All inference approach without post-optimization.</p>
</div>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="410" id="A1.F3.g1" src="x3.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
VideoLISA is a capable model on video object segmentation with versatile language-instructed reasoning abilities.
Beyond basic language referring, it enables complex reasoning by leveraging world knowledge and videos temporal dynamics.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="535" id="A1.F4.g1" src="x4.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Failure cases of VideoLISA.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Failure Cases</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">To understand the limitations and capability boundaries of our method, we analyze several failure cases as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F4" title="Figure 4 ‣ A.3 Qualitative Results ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.
In the first example, the video shows a car crashing into a grocery store.
We prompt the model to find the unusual object that interrupts the peace of the scene.
Although we try to rephrase the prompt in various ways, the model consistently outputs the object in the bottom left corner.
We hypothesize that the issue stems from the inherent hallucination of the MLLM, which recognizes the object as a stove, a telephone pole, or something else.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1">In the second example, we ask the model to find the kid who loses the game
We humans have the background knowledge to determine the match result.
However, it seems like this game is beyond the knowledge scope of the MLLM, causing it to segment the wrong person.
Consequently, we provide some background information about the game rules in the text prompt and then ask the same question.
As shown in the third example of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F4" title="Figure 4 ‣ A.3 Qualitative Results ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>, with this cue, the model is able to segment the correct person.
These examples demonstrate that the reasoning capabilities of VideoLISA are bounded by the multimodal large language model behind it, yet this can be alleviated by prompt engineering techniques.
The third example also exhibits low-quality segmentation masks in certain frames, leaving room for future improvements.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="A1.F5.g1" src="x5.png" width="623"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
ReasonVOS benchmark. The left part shows the statistics of data samples. The right part shows the source of the videos.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1118" id="A1.F6.g1" src="x6.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
More qualitative examples of VideoLISA.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1051" id="A1.F7.g1" src="x7.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
More qualitative examples of VideoLISA.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Benchmark</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We show the data statistics of our ReasonVOS benchmark in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19603v1#A1.F5" title="Figure 5 ‣ A.4 Failure Cases ‣ Appendix A Appendix ‣ One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos"><span class="ltx_text ltx_ref_tag">5</span></a>.
We select videos and mask annotations from various sources and annotate additional text descriptions.
In total, ReasonVOS consists of 91 videos.
We manually annotate 105 video-instruction-mask samples as seed data and use Claude 3 API to augment the data into 458 samples.
We further categorize the text descriptions into short query and long query.
Short queries are descriptions of specific objects, usually in the format of attributive clauses.
Long queries are instructions that require reasoning, usually in the format of a full sentence.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Broader Impact</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">The development of our reasoning-based video segmentation model holds significant potential for transforming a variety of fields by enhancing the ability to analyze and interpret video content.
In the realm of surveillance, this technology can improve security measures by accurately identifying and tracking suspicious behavior, thereby preventing potential threats.
In educational settings, the model can assist teachers in identifying and addressing student engagement patterns, fostering a more responsive learning environment.
For healthcare, our model can be applied to monitor patient activities, supporting early intervention and personalized care strategies.
Additionally, in everyday scenarios, such as pet care or home organization, this technology can assist individuals in making informed decisions quickly and efficiently.
By leveraging advanced reasoning capabilities, our model not only advances the field of computer vision but also provides practical solutions that enhance safety, learning, health, and daily life.
However, it is crucial to consider ethical implications, such as privacy concerns and the potential for misuse, ensuring that these technologies are implemented responsibly and equitably.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 27 13:37:07 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
